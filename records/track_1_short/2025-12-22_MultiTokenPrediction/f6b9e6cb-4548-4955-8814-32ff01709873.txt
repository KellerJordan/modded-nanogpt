import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 10:50:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    217107      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    217108      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    217109      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    217110      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    217111      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    217112      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    217113      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    217114      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    217108      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    217109      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    217110      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    217111      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    217112      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    217113      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    217114      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8379 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:87ms step_avg:86.77ms
step:2/1920 train_time:109ms step_avg:54.48ms
step:3/1920 train_time:131ms step_avg:43.78ms
step:4/1920 train_time:165ms step_avg:41.29ms
step:5/1920 train_time:199ms step_avg:39.85ms
step:6/1920 train_time:283ms step_avg:47.15ms
step:7/1920 train_time:306ms step_avg:43.77ms
step:8/1920 train_time:341ms step_avg:42.56ms
step:9/1920 train_time:375ms step_avg:41.62ms
step:10/1920 train_time:409ms step_avg:40.85ms
step:11/1920 train_time:443ms step_avg:40.27ms
step:12/1920 train_time:477ms step_avg:39.75ms
step:13/1920 train_time:511ms step_avg:39.34ms
step:14/1920 train_time:545ms step_avg:38.96ms
step:15/1920 train_time:580ms step_avg:38.67ms
step:16/1920 train_time:614ms step_avg:38.39ms
step:17/1920 train_time:648ms step_avg:38.14ms
step:18/1920 train_time:683ms step_avg:37.92ms
step:19/1920 train_time:717ms step_avg:37.75ms
step:20/1920 train_time:751ms step_avg:37.56ms
step:21/1920 train_time:786ms step_avg:37.42ms
step:22/1920 train_time:820ms step_avg:37.27ms
step:23/1920 train_time:855ms step_avg:37.15ms
step:24/1920 train_time:889ms step_avg:37.03ms
step:25/1920 train_time:923ms step_avg:36.92ms
step:26/1920 train_time:957ms step_avg:36.81ms
step:27/1920 train_time:991ms step_avg:36.72ms
step:28/1920 train_time:1026ms step_avg:36.63ms
step:29/1920 train_time:1060ms step_avg:36.55ms
step:30/1920 train_time:1094ms step_avg:36.47ms
step:31/1920 train_time:1129ms step_avg:36.40ms
step:32/1920 train_time:1163ms step_avg:36.33ms
step:33/1920 train_time:1198ms step_avg:36.30ms
step:34/1920 train_time:1232ms step_avg:36.24ms
step:35/1920 train_time:1267ms step_avg:36.20ms
step:36/1920 train_time:1301ms step_avg:36.15ms
step:37/1920 train_time:1337ms step_avg:36.12ms
step:38/1920 train_time:1371ms step_avg:36.08ms
step:39/1920 train_time:1405ms step_avg:36.04ms
step:40/1920 train_time:1440ms step_avg:35.99ms
step:41/1920 train_time:1474ms step_avg:35.96ms
step:42/1920 train_time:1509ms step_avg:35.92ms
step:43/1920 train_time:1543ms step_avg:35.89ms
step:44/1920 train_time:1577ms step_avg:35.85ms
step:45/1920 train_time:1612ms step_avg:35.82ms
step:46/1920 train_time:1646ms step_avg:35.79ms
step:47/1920 train_time:1681ms step_avg:35.77ms
step:48/1920 train_time:1715ms step_avg:35.74ms
step:49/1920 train_time:1750ms step_avg:35.71ms
step:50/1920 train_time:1784ms step_avg:35.68ms
step:51/1920 train_time:1819ms step_avg:35.66ms
step:52/1920 train_time:1853ms step_avg:35.63ms
step:53/1920 train_time:1887ms step_avg:35.60ms
step:54/1920 train_time:1921ms step_avg:35.58ms
step:55/1920 train_time:1956ms step_avg:35.56ms
step:56/1920 train_time:1990ms step_avg:35.53ms
step:57/1920 train_time:2024ms step_avg:35.51ms
step:58/1920 train_time:2058ms step_avg:35.49ms
step:59/1920 train_time:2093ms step_avg:35.47ms
step:60/1920 train_time:2127ms step_avg:35.45ms
step:61/1920 train_time:2162ms step_avg:35.44ms
step:62/1920 train_time:2196ms step_avg:35.42ms
step:63/1920 train_time:2230ms step_avg:35.40ms
step:64/1920 train_time:2265ms step_avg:35.38ms
step:65/1920 train_time:2300ms step_avg:35.38ms
step:66/1920 train_time:2334ms step_avg:35.36ms
step:67/1920 train_time:2368ms step_avg:35.35ms
step:68/1920 train_time:2403ms step_avg:35.33ms
step:69/1920 train_time:2437ms step_avg:35.32ms
step:70/1920 train_time:2472ms step_avg:35.31ms
step:71/1920 train_time:2506ms step_avg:35.30ms
step:72/1920 train_time:2540ms step_avg:35.28ms
step:73/1920 train_time:2575ms step_avg:35.27ms
step:74/1920 train_time:2609ms step_avg:35.26ms
step:75/1920 train_time:2644ms step_avg:35.25ms
step:76/1920 train_time:2678ms step_avg:35.23ms
step:77/1920 train_time:2712ms step_avg:35.23ms
step:78/1920 train_time:2746ms step_avg:35.21ms
step:79/1920 train_time:2781ms step_avg:35.21ms
step:80/1920 train_time:2815ms step_avg:35.19ms
step:81/1920 train_time:2850ms step_avg:35.18ms
step:82/1920 train_time:2884ms step_avg:35.17ms
step:83/1920 train_time:2918ms step_avg:35.16ms
step:84/1920 train_time:2952ms step_avg:35.15ms
step:85/1920 train_time:2987ms step_avg:35.14ms
step:86/1920 train_time:3021ms step_avg:35.13ms
step:87/1920 train_time:3055ms step_avg:35.12ms
step:88/1920 train_time:3089ms step_avg:35.11ms
step:89/1920 train_time:3124ms step_avg:35.10ms
step:90/1920 train_time:3158ms step_avg:35.09ms
step:91/1920 train_time:3192ms step_avg:35.08ms
step:92/1920 train_time:3227ms step_avg:35.07ms
step:93/1920 train_time:3261ms step_avg:35.07ms
step:94/1920 train_time:3296ms step_avg:35.06ms
step:95/1920 train_time:3330ms step_avg:35.06ms
step:96/1920 train_time:3364ms step_avg:35.05ms
step:97/1920 train_time:3400ms step_avg:35.05ms
step:98/1920 train_time:3434ms step_avg:35.04ms
step:99/1920 train_time:3468ms step_avg:35.03ms
step:100/1920 train_time:3502ms step_avg:35.02ms
step:101/1920 train_time:3537ms step_avg:35.02ms
step:102/1920 train_time:3571ms step_avg:35.01ms
step:103/1920 train_time:3605ms step_avg:35.00ms
step:104/1920 train_time:3639ms step_avg:34.99ms
step:105/1920 train_time:3674ms step_avg:34.99ms
step:106/1920 train_time:3708ms step_avg:34.99ms
step:107/1920 train_time:3743ms step_avg:34.98ms
step:108/1920 train_time:3777ms step_avg:34.98ms
step:109/1920 train_time:3812ms step_avg:34.97ms
step:110/1920 train_time:3846ms step_avg:34.96ms
step:111/1920 train_time:3881ms step_avg:34.96ms
step:112/1920 train_time:3915ms step_avg:34.95ms
step:113/1920 train_time:3949ms step_avg:34.95ms
step:114/1920 train_time:3983ms step_avg:34.94ms
step:115/1920 train_time:4018ms step_avg:34.94ms
step:116/1920 train_time:4052ms step_avg:34.93ms
step:117/1920 train_time:4087ms step_avg:34.93ms
step:118/1920 train_time:4121ms step_avg:34.92ms
step:119/1920 train_time:4155ms step_avg:34.92ms
step:120/1920 train_time:4189ms step_avg:34.91ms
step:121/1920 train_time:4224ms step_avg:34.91ms
step:122/1920 train_time:4258ms step_avg:34.90ms
step:123/1920 train_time:4293ms step_avg:34.90ms
step:124/1920 train_time:4327ms step_avg:34.89ms
step:125/1920 train_time:4362ms step_avg:34.90ms
step:126/1920 train_time:4396ms step_avg:34.89ms
step:127/1920 train_time:4431ms step_avg:34.89ms
step:128/1920 train_time:4465ms step_avg:34.88ms
step:129/1920 train_time:4500ms step_avg:34.88ms
step:130/1920 train_time:4534ms step_avg:34.88ms
step:131/1920 train_time:4568ms step_avg:34.87ms
step:132/1920 train_time:4603ms step_avg:34.87ms
step:133/1920 train_time:4637ms step_avg:34.87ms
step:134/1920 train_time:4672ms step_avg:34.86ms
step:135/1920 train_time:4705ms step_avg:34.86ms
step:136/1920 train_time:4740ms step_avg:34.85ms
step:137/1920 train_time:4774ms step_avg:34.85ms
step:138/1920 train_time:4808ms step_avg:34.84ms
step:139/1920 train_time:4843ms step_avg:34.84ms
step:140/1920 train_time:4877ms step_avg:34.83ms
step:141/1920 train_time:4911ms step_avg:34.83ms
step:142/1920 train_time:4945ms step_avg:34.82ms
step:143/1920 train_time:4979ms step_avg:34.82ms
step:144/1920 train_time:5014ms step_avg:34.82ms
step:145/1920 train_time:5048ms step_avg:34.81ms
step:146/1920 train_time:5082ms step_avg:34.81ms
step:147/1920 train_time:5117ms step_avg:34.81ms
step:148/1920 train_time:5151ms step_avg:34.80ms
step:149/1920 train_time:5185ms step_avg:34.80ms
step:150/1920 train_time:5219ms step_avg:34.79ms
step:151/1920 train_time:5253ms step_avg:34.79ms
step:152/1920 train_time:5287ms step_avg:34.79ms
step:153/1920 train_time:5322ms step_avg:34.78ms
step:154/1920 train_time:5356ms step_avg:34.78ms
step:155/1920 train_time:5391ms step_avg:34.78ms
step:156/1920 train_time:5425ms step_avg:34.77ms
step:157/1920 train_time:5459ms step_avg:34.77ms
step:158/1920 train_time:5493ms step_avg:34.77ms
step:159/1920 train_time:5528ms step_avg:34.77ms
step:160/1920 train_time:5562ms step_avg:34.76ms
step:161/1920 train_time:5597ms step_avg:34.76ms
step:162/1920 train_time:5631ms step_avg:34.76ms
step:163/1920 train_time:5665ms step_avg:34.76ms
step:164/1920 train_time:5700ms step_avg:34.75ms
step:165/1920 train_time:5735ms step_avg:34.76ms
step:166/1920 train_time:5769ms step_avg:34.75ms
step:167/1920 train_time:5803ms step_avg:34.75ms
step:168/1920 train_time:5838ms step_avg:34.75ms
step:169/1920 train_time:5872ms step_avg:34.74ms
step:170/1920 train_time:5906ms step_avg:34.74ms
step:171/1920 train_time:5941ms step_avg:34.74ms
step:172/1920 train_time:5975ms step_avg:34.74ms
step:173/1920 train_time:6009ms step_avg:34.73ms
step:174/1920 train_time:6043ms step_avg:34.73ms
step:175/1920 train_time:6078ms step_avg:34.73ms
step:176/1920 train_time:6112ms step_avg:34.73ms
step:177/1920 train_time:6146ms step_avg:34.72ms
step:178/1920 train_time:6180ms step_avg:34.72ms
step:179/1920 train_time:6214ms step_avg:34.72ms
step:180/1920 train_time:6248ms step_avg:34.71ms
step:181/1920 train_time:6283ms step_avg:34.71ms
step:182/1920 train_time:6317ms step_avg:34.71ms
step:183/1920 train_time:6352ms step_avg:34.71ms
step:184/1920 train_time:6386ms step_avg:34.71ms
step:185/1920 train_time:6420ms step_avg:34.70ms
step:186/1920 train_time:6454ms step_avg:34.70ms
step:187/1920 train_time:6489ms step_avg:34.70ms
step:188/1920 train_time:6523ms step_avg:34.70ms
step:189/1920 train_time:6558ms step_avg:34.70ms
step:190/1920 train_time:6592ms step_avg:34.70ms
step:191/1920 train_time:6627ms step_avg:34.70ms
step:192/1920 train_time:6661ms step_avg:34.69ms
step:193/1920 train_time:6696ms step_avg:34.69ms
step:194/1920 train_time:6730ms step_avg:34.69ms
step:195/1920 train_time:6765ms step_avg:34.69ms
step:196/1920 train_time:6799ms step_avg:34.69ms
step:197/1920 train_time:6834ms step_avg:34.69ms
step:198/1920 train_time:6868ms step_avg:34.68ms
step:199/1920 train_time:6902ms step_avg:34.68ms
step:200/1920 train_time:6936ms step_avg:34.68ms
step:201/1920 train_time:6971ms step_avg:34.68ms
step:202/1920 train_time:7005ms step_avg:34.68ms
step:203/1920 train_time:7040ms step_avg:34.68ms
step:204/1920 train_time:7074ms step_avg:34.67ms
step:205/1920 train_time:7108ms step_avg:34.67ms
step:206/1920 train_time:7142ms step_avg:34.67ms
step:207/1920 train_time:7176ms step_avg:34.67ms
step:208/1920 train_time:7210ms step_avg:34.66ms
step:209/1920 train_time:7245ms step_avg:34.66ms
step:210/1920 train_time:7279ms step_avg:34.66ms
step:211/1920 train_time:7313ms step_avg:34.66ms
step:212/1920 train_time:7347ms step_avg:34.66ms
step:213/1920 train_time:7382ms step_avg:34.66ms
step:214/1920 train_time:7416ms step_avg:34.65ms
step:215/1920 train_time:7450ms step_avg:34.65ms
step:216/1920 train_time:7484ms step_avg:34.65ms
step:217/1920 train_time:7519ms step_avg:34.65ms
step:218/1920 train_time:7553ms step_avg:34.65ms
step:219/1920 train_time:7588ms step_avg:34.65ms
step:220/1920 train_time:7622ms step_avg:34.64ms
step:221/1920 train_time:7656ms step_avg:34.64ms
step:222/1920 train_time:7690ms step_avg:34.64ms
step:223/1920 train_time:7725ms step_avg:34.64ms
step:224/1920 train_time:7759ms step_avg:34.64ms
step:225/1920 train_time:7794ms step_avg:34.64ms
step:226/1920 train_time:7828ms step_avg:34.64ms
step:227/1920 train_time:7862ms step_avg:34.63ms
step:228/1920 train_time:7896ms step_avg:34.63ms
step:229/1920 train_time:7931ms step_avg:34.63ms
step:230/1920 train_time:7965ms step_avg:34.63ms
step:231/1920 train_time:7999ms step_avg:34.63ms
step:232/1920 train_time:8033ms step_avg:34.63ms
step:233/1920 train_time:8067ms step_avg:34.62ms
step:234/1920 train_time:8101ms step_avg:34.62ms
step:235/1920 train_time:8136ms step_avg:34.62ms
step:236/1920 train_time:8170ms step_avg:34.62ms
step:237/1920 train_time:8204ms step_avg:34.62ms
step:238/1920 train_time:8238ms step_avg:34.61ms
step:239/1920 train_time:8273ms step_avg:34.61ms
step:240/1920 train_time:8307ms step_avg:34.61ms
step:241/1920 train_time:8341ms step_avg:34.61ms
step:242/1920 train_time:8375ms step_avg:34.61ms
step:243/1920 train_time:8410ms step_avg:34.61ms
step:244/1920 train_time:8444ms step_avg:34.61ms
step:245/1920 train_time:8478ms step_avg:34.60ms
step:246/1920 train_time:8512ms step_avg:34.60ms
step:247/1920 train_time:8547ms step_avg:34.60ms
step:248/1920 train_time:8581ms step_avg:34.60ms
step:249/1920 train_time:8615ms step_avg:34.60ms
step:250/1920 train_time:8650ms step_avg:34.60ms
step:250/1920 val_loss:4.6194 train_time:8687ms step_avg:34.75ms
step:251/1920 train_time:8704ms step_avg:34.68ms
step:252/1920 train_time:8722ms step_avg:34.61ms
step:253/1920 train_time:8756ms step_avg:34.61ms
step:254/1920 train_time:8790ms step_avg:34.61ms
step:255/1920 train_time:8826ms step_avg:34.61ms
step:256/1920 train_time:8862ms step_avg:34.62ms
step:257/1920 train_time:8897ms step_avg:34.62ms
step:258/1920 train_time:8931ms step_avg:34.62ms
step:259/1920 train_time:8965ms step_avg:34.62ms
step:260/1920 train_time:8999ms step_avg:34.61ms
step:261/1920 train_time:9034ms step_avg:34.61ms
step:262/1920 train_time:9068ms step_avg:34.61ms
step:263/1920 train_time:9103ms step_avg:34.61ms
step:264/1920 train_time:9137ms step_avg:34.61ms
step:265/1920 train_time:9171ms step_avg:34.61ms
step:266/1920 train_time:9205ms step_avg:34.61ms
step:267/1920 train_time:9240ms step_avg:34.61ms
step:268/1920 train_time:9274ms step_avg:34.60ms
step:269/1920 train_time:9308ms step_avg:34.60ms
step:270/1920 train_time:9342ms step_avg:34.60ms
step:271/1920 train_time:9376ms step_avg:34.60ms
step:272/1920 train_time:9410ms step_avg:34.60ms
step:273/1920 train_time:9444ms step_avg:34.59ms
step:274/1920 train_time:9478ms step_avg:34.59ms
step:275/1920 train_time:9512ms step_avg:34.59ms
step:276/1920 train_time:9546ms step_avg:34.59ms
step:277/1920 train_time:9581ms step_avg:34.59ms
step:278/1920 train_time:9614ms step_avg:34.58ms
step:279/1920 train_time:9649ms step_avg:34.58ms
step:280/1920 train_time:9683ms step_avg:34.58ms
step:281/1920 train_time:9717ms step_avg:34.58ms
step:282/1920 train_time:9752ms step_avg:34.58ms
step:283/1920 train_time:9786ms step_avg:34.58ms
step:284/1920 train_time:9820ms step_avg:34.58ms
step:285/1920 train_time:9855ms step_avg:34.58ms
step:286/1920 train_time:9890ms step_avg:34.58ms
step:287/1920 train_time:9924ms step_avg:34.58ms
step:288/1920 train_time:9959ms step_avg:34.58ms
step:289/1920 train_time:9993ms step_avg:34.58ms
step:290/1920 train_time:10027ms step_avg:34.58ms
step:291/1920 train_time:10062ms step_avg:34.58ms
step:292/1920 train_time:10096ms step_avg:34.58ms
step:293/1920 train_time:10130ms step_avg:34.58ms
step:294/1920 train_time:10164ms step_avg:34.57ms
step:295/1920 train_time:10199ms step_avg:34.57ms
step:296/1920 train_time:10233ms step_avg:34.57ms
step:297/1920 train_time:10267ms step_avg:34.57ms
step:298/1920 train_time:10301ms step_avg:34.57ms
step:299/1920 train_time:10336ms step_avg:34.57ms
step:300/1920 train_time:10370ms step_avg:34.57ms
step:301/1920 train_time:10404ms step_avg:34.56ms
step:302/1920 train_time:10438ms step_avg:34.56ms
step:303/1920 train_time:10472ms step_avg:34.56ms
step:304/1920 train_time:10506ms step_avg:34.56ms
step:305/1920 train_time:10540ms step_avg:34.56ms
step:306/1920 train_time:10574ms step_avg:34.56ms
step:307/1920 train_time:10608ms step_avg:34.56ms
step:308/1920 train_time:10643ms step_avg:34.55ms
step:309/1920 train_time:10677ms step_avg:34.55ms
step:310/1920 train_time:10711ms step_avg:34.55ms
step:311/1920 train_time:10745ms step_avg:34.55ms
step:312/1920 train_time:10779ms step_avg:34.55ms
step:313/1920 train_time:10814ms step_avg:34.55ms
step:314/1920 train_time:10848ms step_avg:34.55ms
step:315/1920 train_time:10883ms step_avg:34.55ms
step:316/1920 train_time:10917ms step_avg:34.55ms
step:317/1920 train_time:10952ms step_avg:34.55ms
step:318/1920 train_time:10986ms step_avg:34.55ms
step:319/1920 train_time:11021ms step_avg:34.55ms
step:320/1920 train_time:11055ms step_avg:34.55ms
step:321/1920 train_time:11089ms step_avg:34.55ms
step:322/1920 train_time:11123ms step_avg:34.54ms
step:323/1920 train_time:11159ms step_avg:34.55ms
step:324/1920 train_time:11193ms step_avg:34.54ms
step:325/1920 train_time:11227ms step_avg:34.54ms
step:326/1920 train_time:11261ms step_avg:34.54ms
step:327/1920 train_time:11296ms step_avg:34.54ms
step:328/1920 train_time:11330ms step_avg:34.54ms
step:329/1920 train_time:11365ms step_avg:34.54ms
step:330/1920 train_time:11399ms step_avg:34.54ms
step:331/1920 train_time:11433ms step_avg:34.54ms
step:332/1920 train_time:11467ms step_avg:34.54ms
step:333/1920 train_time:11501ms step_avg:34.54ms
step:334/1920 train_time:11536ms step_avg:34.54ms
step:335/1920 train_time:11570ms step_avg:34.54ms
step:336/1920 train_time:11604ms step_avg:34.54ms
step:337/1920 train_time:11638ms step_avg:34.54ms
step:338/1920 train_time:11673ms step_avg:34.53ms
step:339/1920 train_time:11707ms step_avg:34.53ms
step:340/1920 train_time:11741ms step_avg:34.53ms
step:341/1920 train_time:11775ms step_avg:34.53ms
step:342/1920 train_time:11809ms step_avg:34.53ms
step:343/1920 train_time:11843ms step_avg:34.53ms
step:344/1920 train_time:11877ms step_avg:34.53ms
step:345/1920 train_time:11912ms step_avg:34.53ms
step:346/1920 train_time:11946ms step_avg:34.53ms
step:347/1920 train_time:11980ms step_avg:34.53ms
step:348/1920 train_time:12015ms step_avg:34.52ms
step:349/1920 train_time:12049ms step_avg:34.52ms
step:350/1920 train_time:12083ms step_avg:34.52ms
step:351/1920 train_time:12117ms step_avg:34.52ms
step:352/1920 train_time:12151ms step_avg:34.52ms
step:353/1920 train_time:12186ms step_avg:34.52ms
step:354/1920 train_time:12220ms step_avg:34.52ms
step:355/1920 train_time:12254ms step_avg:34.52ms
step:356/1920 train_time:12288ms step_avg:34.52ms
step:357/1920 train_time:12323ms step_avg:34.52ms
step:358/1920 train_time:12357ms step_avg:34.52ms
step:359/1920 train_time:12392ms step_avg:34.52ms
step:360/1920 train_time:12426ms step_avg:34.52ms
step:361/1920 train_time:12460ms step_avg:34.52ms
step:362/1920 train_time:12495ms step_avg:34.52ms
step:363/1920 train_time:12529ms step_avg:34.52ms
step:364/1920 train_time:12563ms step_avg:34.51ms
step:365/1920 train_time:12597ms step_avg:34.51ms
step:366/1920 train_time:12632ms step_avg:34.51ms
step:367/1920 train_time:12666ms step_avg:34.51ms
step:368/1920 train_time:12700ms step_avg:34.51ms
step:369/1920 train_time:12735ms step_avg:34.51ms
step:370/1920 train_time:12769ms step_avg:34.51ms
step:371/1920 train_time:12803ms step_avg:34.51ms
step:372/1920 train_time:12837ms step_avg:34.51ms
step:373/1920 train_time:12871ms step_avg:34.51ms
step:374/1920 train_time:12905ms step_avg:34.51ms
step:375/1920 train_time:12940ms step_avg:34.51ms
step:376/1920 train_time:12974ms step_avg:34.51ms
step:377/1920 train_time:13008ms step_avg:34.50ms
step:378/1920 train_time:13042ms step_avg:34.50ms
step:379/1920 train_time:13076ms step_avg:34.50ms
step:380/1920 train_time:13110ms step_avg:34.50ms
step:381/1920 train_time:13145ms step_avg:34.50ms
step:382/1920 train_time:13179ms step_avg:34.50ms
step:383/1920 train_time:13213ms step_avg:34.50ms
step:384/1920 train_time:13247ms step_avg:34.50ms
step:385/1920 train_time:13282ms step_avg:34.50ms
step:386/1920 train_time:13316ms step_avg:34.50ms
step:387/1920 train_time:13350ms step_avg:34.50ms
step:388/1920 train_time:13384ms step_avg:34.50ms
step:389/1920 train_time:13419ms step_avg:34.50ms
step:390/1920 train_time:13453ms step_avg:34.49ms
step:391/1920 train_time:13487ms step_avg:34.49ms
step:392/1920 train_time:13521ms step_avg:34.49ms
step:393/1920 train_time:13556ms step_avg:34.49ms
step:394/1920 train_time:13590ms step_avg:34.49ms
step:395/1920 train_time:13624ms step_avg:34.49ms
step:396/1920 train_time:13658ms step_avg:34.49ms
step:397/1920 train_time:13692ms step_avg:34.49ms
step:398/1920 train_time:13726ms step_avg:34.49ms
step:399/1920 train_time:13761ms step_avg:34.49ms
step:400/1920 train_time:13795ms step_avg:34.49ms
step:401/1920 train_time:13829ms step_avg:34.49ms
step:402/1920 train_time:13863ms step_avg:34.49ms
step:403/1920 train_time:13898ms step_avg:34.49ms
step:404/1920 train_time:13932ms step_avg:34.48ms
step:405/1920 train_time:13966ms step_avg:34.48ms
step:406/1920 train_time:14000ms step_avg:34.48ms
step:407/1920 train_time:14034ms step_avg:34.48ms
step:408/1920 train_time:14069ms step_avg:34.48ms
step:409/1920 train_time:14103ms step_avg:34.48ms
step:410/1920 train_time:14137ms step_avg:34.48ms
step:411/1920 train_time:14172ms step_avg:34.48ms
step:412/1920 train_time:14206ms step_avg:34.48ms
step:413/1920 train_time:14241ms step_avg:34.48ms
step:414/1920 train_time:14274ms step_avg:34.48ms
step:415/1920 train_time:14309ms step_avg:34.48ms
step:416/1920 train_time:14343ms step_avg:34.48ms
step:417/1920 train_time:14377ms step_avg:34.48ms
step:418/1920 train_time:14411ms step_avg:34.48ms
step:419/1920 train_time:14446ms step_avg:34.48ms
step:420/1920 train_time:14480ms step_avg:34.48ms
step:421/1920 train_time:14514ms step_avg:34.48ms
step:422/1920 train_time:14549ms step_avg:34.48ms
step:423/1920 train_time:14583ms step_avg:34.48ms
step:424/1920 train_time:14617ms step_avg:34.47ms
step:425/1920 train_time:14652ms step_avg:34.48ms
step:426/1920 train_time:14686ms step_avg:34.47ms
step:427/1920 train_time:14720ms step_avg:34.47ms
step:428/1920 train_time:14754ms step_avg:34.47ms
step:429/1920 train_time:14789ms step_avg:34.47ms
step:430/1920 train_time:14823ms step_avg:34.47ms
step:431/1920 train_time:14858ms step_avg:34.47ms
step:432/1920 train_time:14892ms step_avg:34.47ms
step:433/1920 train_time:14926ms step_avg:34.47ms
step:434/1920 train_time:14960ms step_avg:34.47ms
step:435/1920 train_time:14995ms step_avg:34.47ms
step:436/1920 train_time:15029ms step_avg:34.47ms
step:437/1920 train_time:15064ms step_avg:34.47ms
step:438/1920 train_time:15098ms step_avg:34.47ms
step:439/1920 train_time:15133ms step_avg:34.47ms
step:440/1920 train_time:15167ms step_avg:34.47ms
step:441/1920 train_time:15201ms step_avg:34.47ms
step:442/1920 train_time:15236ms step_avg:34.47ms
step:443/1920 train_time:15270ms step_avg:34.47ms
step:444/1920 train_time:15304ms step_avg:34.47ms
step:445/1920 train_time:15339ms step_avg:34.47ms
step:446/1920 train_time:15373ms step_avg:34.47ms
step:447/1920 train_time:15407ms step_avg:34.47ms
step:448/1920 train_time:15441ms step_avg:34.47ms
step:449/1920 train_time:15475ms step_avg:34.47ms
step:450/1920 train_time:15510ms step_avg:34.47ms
step:451/1920 train_time:15544ms step_avg:34.47ms
step:452/1920 train_time:15578ms step_avg:34.46ms
step:453/1920 train_time:15612ms step_avg:34.46ms
step:454/1920 train_time:15646ms step_avg:34.46ms
step:455/1920 train_time:15681ms step_avg:34.46ms
step:456/1920 train_time:15715ms step_avg:34.46ms
step:457/1920 train_time:15749ms step_avg:34.46ms
step:458/1920 train_time:15783ms step_avg:34.46ms
step:459/1920 train_time:15818ms step_avg:34.46ms
step:460/1920 train_time:15852ms step_avg:34.46ms
step:461/1920 train_time:15886ms step_avg:34.46ms
step:462/1920 train_time:15921ms step_avg:34.46ms
step:463/1920 train_time:15955ms step_avg:34.46ms
step:464/1920 train_time:15989ms step_avg:34.46ms
step:465/1920 train_time:16024ms step_avg:34.46ms
step:466/1920 train_time:16058ms step_avg:34.46ms
step:467/1920 train_time:16093ms step_avg:34.46ms
step:468/1920 train_time:16126ms step_avg:34.46ms
step:469/1920 train_time:16161ms step_avg:34.46ms
step:470/1920 train_time:16195ms step_avg:34.46ms
step:471/1920 train_time:16230ms step_avg:34.46ms
step:472/1920 train_time:16264ms step_avg:34.46ms
step:473/1920 train_time:16298ms step_avg:34.46ms
step:474/1920 train_time:16332ms step_avg:34.46ms
step:475/1920 train_time:16366ms step_avg:34.46ms
step:476/1920 train_time:16400ms step_avg:34.45ms
step:477/1920 train_time:16435ms step_avg:34.45ms
step:478/1920 train_time:16469ms step_avg:34.45ms
step:479/1920 train_time:16504ms step_avg:34.45ms
step:480/1920 train_time:16538ms step_avg:34.45ms
step:481/1920 train_time:16573ms step_avg:34.45ms
step:482/1920 train_time:16607ms step_avg:34.45ms
step:483/1920 train_time:16641ms step_avg:34.45ms
step:484/1920 train_time:16676ms step_avg:34.45ms
step:485/1920 train_time:16710ms step_avg:34.45ms
step:486/1920 train_time:16744ms step_avg:34.45ms
step:487/1920 train_time:16779ms step_avg:34.45ms
step:488/1920 train_time:16813ms step_avg:34.45ms
step:489/1920 train_time:16847ms step_avg:34.45ms
step:490/1920 train_time:16881ms step_avg:34.45ms
step:491/1920 train_time:16916ms step_avg:34.45ms
step:492/1920 train_time:16950ms step_avg:34.45ms
step:493/1920 train_time:16984ms step_avg:34.45ms
step:494/1920 train_time:17018ms step_avg:34.45ms
step:495/1920 train_time:17052ms step_avg:34.45ms
step:496/1920 train_time:17086ms step_avg:34.45ms
step:497/1920 train_time:17121ms step_avg:34.45ms
step:498/1920 train_time:17155ms step_avg:34.45ms
step:499/1920 train_time:17189ms step_avg:34.45ms
step:500/1920 train_time:17223ms step_avg:34.45ms
step:500/1920 val_loss:4.2939 train_time:17261ms step_avg:34.52ms
step:501/1920 train_time:17280ms step_avg:34.49ms
step:502/1920 train_time:17297ms step_avg:34.46ms
step:503/1920 train_time:17331ms step_avg:34.45ms
step:504/1920 train_time:17365ms step_avg:34.45ms
step:505/1920 train_time:17401ms step_avg:34.46ms
step:506/1920 train_time:17437ms step_avg:34.46ms
step:507/1920 train_time:17471ms step_avg:34.46ms
step:508/1920 train_time:17506ms step_avg:34.46ms
step:509/1920 train_time:17541ms step_avg:34.46ms
step:510/1920 train_time:17575ms step_avg:34.46ms
step:511/1920 train_time:17609ms step_avg:34.46ms
step:512/1920 train_time:17643ms step_avg:34.46ms
step:513/1920 train_time:17677ms step_avg:34.46ms
step:514/1920 train_time:17711ms step_avg:34.46ms
step:515/1920 train_time:17745ms step_avg:34.46ms
step:516/1920 train_time:17779ms step_avg:34.46ms
step:517/1920 train_time:17813ms step_avg:34.46ms
step:518/1920 train_time:17847ms step_avg:34.45ms
step:519/1920 train_time:17881ms step_avg:34.45ms
step:520/1920 train_time:17915ms step_avg:34.45ms
step:521/1920 train_time:17950ms step_avg:34.45ms
step:522/1920 train_time:17984ms step_avg:34.45ms
step:523/1920 train_time:18018ms step_avg:34.45ms
step:524/1920 train_time:18052ms step_avg:34.45ms
step:525/1920 train_time:18086ms step_avg:34.45ms
step:526/1920 train_time:18120ms step_avg:34.45ms
step:527/1920 train_time:18154ms step_avg:34.45ms
step:528/1920 train_time:18188ms step_avg:34.45ms
step:529/1920 train_time:18223ms step_avg:34.45ms
step:530/1920 train_time:18257ms step_avg:34.45ms
step:531/1920 train_time:18291ms step_avg:34.45ms
step:532/1920 train_time:18325ms step_avg:34.45ms
step:533/1920 train_time:18360ms step_avg:34.45ms
step:534/1920 train_time:18395ms step_avg:34.45ms
step:535/1920 train_time:18429ms step_avg:34.45ms
step:536/1920 train_time:18463ms step_avg:34.45ms
step:537/1920 train_time:18498ms step_avg:34.45ms
step:538/1920 train_time:18532ms step_avg:34.45ms
step:539/1920 train_time:18567ms step_avg:34.45ms
step:540/1920 train_time:18601ms step_avg:34.45ms
step:541/1920 train_time:18635ms step_avg:34.45ms
step:542/1920 train_time:18669ms step_avg:34.44ms
step:543/1920 train_time:18703ms step_avg:34.44ms
step:544/1920 train_time:18738ms step_avg:34.44ms
step:545/1920 train_time:18772ms step_avg:34.44ms
step:546/1920 train_time:18806ms step_avg:34.44ms
step:547/1920 train_time:18840ms step_avg:34.44ms
step:548/1920 train_time:18874ms step_avg:34.44ms
step:549/1920 train_time:18908ms step_avg:34.44ms
step:550/1920 train_time:18942ms step_avg:34.44ms
step:551/1920 train_time:18976ms step_avg:34.44ms
step:552/1920 train_time:19010ms step_avg:34.44ms
step:553/1920 train_time:19044ms step_avg:34.44ms
step:554/1920 train_time:19078ms step_avg:34.44ms
step:555/1920 train_time:19112ms step_avg:34.44ms
step:556/1920 train_time:19146ms step_avg:34.44ms
step:557/1920 train_time:19181ms step_avg:34.44ms
step:558/1920 train_time:19215ms step_avg:34.43ms
step:559/1920 train_time:19249ms step_avg:34.43ms
step:560/1920 train_time:19283ms step_avg:34.43ms
step:561/1920 train_time:19317ms step_avg:34.43ms
step:562/1920 train_time:19351ms step_avg:34.43ms
step:563/1920 train_time:19386ms step_avg:34.43ms
step:564/1920 train_time:19420ms step_avg:34.43ms
step:565/1920 train_time:19455ms step_avg:34.43ms
step:566/1920 train_time:19489ms step_avg:34.43ms
step:567/1920 train_time:19524ms step_avg:34.43ms
step:568/1920 train_time:19558ms step_avg:34.43ms
step:569/1920 train_time:19593ms step_avg:34.43ms
step:570/1920 train_time:19627ms step_avg:34.43ms
step:571/1920 train_time:19661ms step_avg:34.43ms
step:572/1920 train_time:19695ms step_avg:34.43ms
step:573/1920 train_time:19729ms step_avg:34.43ms
step:574/1920 train_time:19763ms step_avg:34.43ms
step:575/1920 train_time:19798ms step_avg:34.43ms
step:576/1920 train_time:19832ms step_avg:34.43ms
step:577/1920 train_time:19867ms step_avg:34.43ms
step:578/1920 train_time:19900ms step_avg:34.43ms
step:579/1920 train_time:19935ms step_avg:34.43ms
step:580/1920 train_time:19969ms step_avg:34.43ms
step:581/1920 train_time:20003ms step_avg:34.43ms
step:582/1920 train_time:20037ms step_avg:34.43ms
step:583/1920 train_time:20072ms step_avg:34.43ms
step:584/1920 train_time:20106ms step_avg:34.43ms
step:585/1920 train_time:20140ms step_avg:34.43ms
step:586/1920 train_time:20174ms step_avg:34.43ms
step:587/1920 train_time:20208ms step_avg:34.43ms
step:588/1920 train_time:20242ms step_avg:34.43ms
step:589/1920 train_time:20277ms step_avg:34.43ms
step:590/1920 train_time:20311ms step_avg:34.42ms
step:591/1920 train_time:20345ms step_avg:34.43ms
step:592/1920 train_time:20379ms step_avg:34.42ms
step:593/1920 train_time:20414ms step_avg:34.43ms
step:594/1920 train_time:20448ms step_avg:34.42ms
step:595/1920 train_time:20483ms step_avg:34.43ms
step:596/1920 train_time:20517ms step_avg:34.42ms
step:597/1920 train_time:20551ms step_avg:34.42ms
step:598/1920 train_time:20586ms step_avg:34.42ms
step:599/1920 train_time:20620ms step_avg:34.42ms
step:600/1920 train_time:20654ms step_avg:34.42ms
step:601/1920 train_time:20688ms step_avg:34.42ms
step:602/1920 train_time:20723ms step_avg:34.42ms
step:603/1920 train_time:20757ms step_avg:34.42ms
step:604/1920 train_time:20791ms step_avg:34.42ms
step:605/1920 train_time:20825ms step_avg:34.42ms
step:606/1920 train_time:20859ms step_avg:34.42ms
step:607/1920 train_time:20894ms step_avg:34.42ms
step:608/1920 train_time:20928ms step_avg:34.42ms
step:609/1920 train_time:20962ms step_avg:34.42ms
step:610/1920 train_time:20997ms step_avg:34.42ms
step:611/1920 train_time:21031ms step_avg:34.42ms
step:612/1920 train_time:21065ms step_avg:34.42ms
step:613/1920 train_time:21099ms step_avg:34.42ms
step:614/1920 train_time:21133ms step_avg:34.42ms
step:615/1920 train_time:21167ms step_avg:34.42ms
step:616/1920 train_time:21202ms step_avg:34.42ms
step:617/1920 train_time:21236ms step_avg:34.42ms
step:618/1920 train_time:21270ms step_avg:34.42ms
step:619/1920 train_time:21304ms step_avg:34.42ms
step:620/1920 train_time:21338ms step_avg:34.42ms
step:621/1920 train_time:21372ms step_avg:34.42ms
step:622/1920 train_time:21406ms step_avg:34.42ms
step:623/1920 train_time:21441ms step_avg:34.42ms
step:624/1920 train_time:21475ms step_avg:34.42ms
step:625/1920 train_time:21510ms step_avg:34.42ms
step:626/1920 train_time:21544ms step_avg:34.41ms
step:627/1920 train_time:21578ms step_avg:34.42ms
step:628/1920 train_time:21613ms step_avg:34.42ms
step:629/1920 train_time:21675ms step_avg:34.46ms
step:630/1920 train_time:21737ms step_avg:34.50ms
step:631/1920 train_time:21800ms step_avg:34.55ms
step:632/1920 train_time:21861ms step_avg:34.59ms
step:633/1920 train_time:21923ms step_avg:34.63ms
step:634/1920 train_time:21985ms step_avg:34.68ms
step:635/1920 train_time:22047ms step_avg:34.72ms
step:636/1920 train_time:22108ms step_avg:34.76ms
step:637/1920 train_time:22171ms step_avg:34.81ms
step:638/1920 train_time:22233ms step_avg:34.85ms
step:639/1920 train_time:22296ms step_avg:34.89ms
step:640/1920 train_time:22357ms step_avg:34.93ms
step:641/1920 train_time:22419ms step_avg:34.98ms
step:642/1920 train_time:22481ms step_avg:35.02ms
step:643/1920 train_time:22543ms step_avg:35.06ms
step:644/1920 train_time:22604ms step_avg:35.10ms
step:645/1920 train_time:22668ms step_avg:35.14ms
step:646/1920 train_time:22730ms step_avg:35.19ms
step:647/1920 train_time:22793ms step_avg:35.23ms
step:648/1920 train_time:22855ms step_avg:35.27ms
step:649/1920 train_time:22917ms step_avg:35.31ms
step:650/1920 train_time:22979ms step_avg:35.35ms
step:651/1920 train_time:23041ms step_avg:35.39ms
step:652/1920 train_time:23102ms step_avg:35.43ms
step:653/1920 train_time:23164ms step_avg:35.47ms
step:654/1920 train_time:23226ms step_avg:35.51ms
step:655/1920 train_time:23288ms step_avg:35.55ms
step:656/1920 train_time:23350ms step_avg:35.60ms
step:657/1920 train_time:23413ms step_avg:35.64ms
step:658/1920 train_time:23475ms step_avg:35.68ms
step:659/1920 train_time:23537ms step_avg:35.72ms
step:660/1920 train_time:23599ms step_avg:35.76ms
step:661/1920 train_time:23661ms step_avg:35.80ms
step:662/1920 train_time:23722ms step_avg:35.83ms
step:663/1920 train_time:23785ms step_avg:35.88ms
step:664/1920 train_time:23847ms step_avg:35.91ms
step:665/1920 train_time:23911ms step_avg:35.96ms
step:666/1920 train_time:23972ms step_avg:35.99ms
step:667/1920 train_time:24035ms step_avg:36.03ms
step:668/1920 train_time:24098ms step_avg:36.07ms
step:669/1920 train_time:24160ms step_avg:36.11ms
step:670/1920 train_time:24221ms step_avg:36.15ms
step:671/1920 train_time:24283ms step_avg:36.19ms
step:672/1920 train_time:24345ms step_avg:36.23ms
step:673/1920 train_time:24408ms step_avg:36.27ms
step:674/1920 train_time:24471ms step_avg:36.31ms
step:675/1920 train_time:24533ms step_avg:36.35ms
step:676/1920 train_time:24595ms step_avg:36.38ms
step:677/1920 train_time:24657ms step_avg:36.42ms
step:678/1920 train_time:24719ms step_avg:36.46ms
step:679/1920 train_time:24781ms step_avg:36.50ms
step:680/1920 train_time:24842ms step_avg:36.53ms
step:681/1920 train_time:24904ms step_avg:36.57ms
step:682/1920 train_time:24966ms step_avg:36.61ms
step:683/1920 train_time:25029ms step_avg:36.65ms
step:684/1920 train_time:25091ms step_avg:36.68ms
step:685/1920 train_time:25154ms step_avg:36.72ms
step:686/1920 train_time:25216ms step_avg:36.76ms
step:687/1920 train_time:25279ms step_avg:36.80ms
step:688/1920 train_time:25341ms step_avg:36.83ms
step:689/1920 train_time:25403ms step_avg:36.87ms
step:690/1920 train_time:25465ms step_avg:36.91ms
step:691/1920 train_time:25528ms step_avg:36.94ms
step:692/1920 train_time:25590ms step_avg:36.98ms
step:693/1920 train_time:25653ms step_avg:37.02ms
step:694/1920 train_time:25715ms step_avg:37.05ms
step:695/1920 train_time:25777ms step_avg:37.09ms
step:696/1920 train_time:25838ms step_avg:37.12ms
step:697/1920 train_time:25900ms step_avg:37.16ms
step:698/1920 train_time:25962ms step_avg:37.19ms
step:699/1920 train_time:26024ms step_avg:37.23ms
step:700/1920 train_time:26086ms step_avg:37.27ms
step:701/1920 train_time:26148ms step_avg:37.30ms
step:702/1920 train_time:26210ms step_avg:37.34ms
step:703/1920 train_time:26274ms step_avg:37.37ms
step:704/1920 train_time:26336ms step_avg:37.41ms
step:705/1920 train_time:26398ms step_avg:37.44ms
step:706/1920 train_time:26459ms step_avg:37.48ms
step:707/1920 train_time:26522ms step_avg:37.51ms
step:708/1920 train_time:26583ms step_avg:37.55ms
step:709/1920 train_time:26646ms step_avg:37.58ms
step:710/1920 train_time:26708ms step_avg:37.62ms
step:711/1920 train_time:26771ms step_avg:37.65ms
step:712/1920 train_time:26834ms step_avg:37.69ms
step:713/1920 train_time:26896ms step_avg:37.72ms
step:714/1920 train_time:26958ms step_avg:37.76ms
step:715/1920 train_time:27019ms step_avg:37.79ms
step:716/1920 train_time:27081ms step_avg:37.82ms
step:717/1920 train_time:27143ms step_avg:37.86ms
step:718/1920 train_time:27205ms step_avg:37.89ms
step:719/1920 train_time:27268ms step_avg:37.92ms
step:720/1920 train_time:27330ms step_avg:37.96ms
step:721/1920 train_time:27393ms step_avg:37.99ms
step:722/1920 train_time:27455ms step_avg:38.03ms
step:723/1920 train_time:27517ms step_avg:38.06ms
step:724/1920 train_time:27580ms step_avg:38.09ms
step:725/1920 train_time:27642ms step_avg:38.13ms
step:726/1920 train_time:27704ms step_avg:38.16ms
step:727/1920 train_time:27767ms step_avg:38.19ms
step:728/1920 train_time:27828ms step_avg:38.23ms
step:729/1920 train_time:27891ms step_avg:38.26ms
step:730/1920 train_time:27953ms step_avg:38.29ms
step:731/1920 train_time:28015ms step_avg:38.32ms
step:732/1920 train_time:28077ms step_avg:38.36ms
step:733/1920 train_time:28139ms step_avg:38.39ms
step:734/1920 train_time:28201ms step_avg:38.42ms
step:735/1920 train_time:28263ms step_avg:38.45ms
step:736/1920 train_time:28326ms step_avg:38.49ms
step:737/1920 train_time:28389ms step_avg:38.52ms
step:738/1920 train_time:28451ms step_avg:38.55ms
step:739/1920 train_time:28514ms step_avg:38.58ms
step:740/1920 train_time:28576ms step_avg:38.62ms
step:741/1920 train_time:28639ms step_avg:38.65ms
step:742/1920 train_time:28700ms step_avg:38.68ms
step:743/1920 train_time:28762ms step_avg:38.71ms
step:744/1920 train_time:28824ms step_avg:38.74ms
step:745/1920 train_time:28886ms step_avg:38.77ms
step:746/1920 train_time:28948ms step_avg:38.80ms
step:747/1920 train_time:29011ms step_avg:38.84ms
step:748/1920 train_time:29073ms step_avg:38.87ms
step:749/1920 train_time:29135ms step_avg:38.90ms
step:750/1920 train_time:29197ms step_avg:38.93ms
step:750/1920 val_loss:4.0469 train_time:29262ms step_avg:39.02ms
step:751/1920 train_time:29282ms step_avg:38.99ms
step:752/1920 train_time:29322ms step_avg:38.99ms
step:753/1920 train_time:29387ms step_avg:39.03ms
step:754/1920 train_time:29452ms step_avg:39.06ms
step:755/1920 train_time:29516ms step_avg:39.09ms
step:756/1920 train_time:29577ms step_avg:39.12ms
step:757/1920 train_time:29639ms step_avg:39.15ms
step:758/1920 train_time:29699ms step_avg:39.18ms
step:759/1920 train_time:29761ms step_avg:39.21ms
step:760/1920 train_time:29822ms step_avg:39.24ms
step:761/1920 train_time:29884ms step_avg:39.27ms
step:762/1920 train_time:29944ms step_avg:39.30ms
step:763/1920 train_time:30006ms step_avg:39.33ms
step:764/1920 train_time:30067ms step_avg:39.35ms
step:765/1920 train_time:30129ms step_avg:39.38ms
step:766/1920 train_time:30196ms step_avg:39.42ms
step:767/1920 train_time:30261ms step_avg:39.45ms
step:768/1920 train_time:30323ms step_avg:39.48ms
step:769/1920 train_time:30386ms step_avg:39.51ms
step:770/1920 train_time:30448ms step_avg:39.54ms
step:771/1920 train_time:30511ms step_avg:39.57ms
step:772/1920 train_time:30573ms step_avg:39.60ms
step:773/1920 train_time:30635ms step_avg:39.63ms
step:774/1920 train_time:30696ms step_avg:39.66ms
step:775/1920 train_time:30758ms step_avg:39.69ms
step:776/1920 train_time:30819ms step_avg:39.72ms
step:777/1920 train_time:30881ms step_avg:39.74ms
step:778/1920 train_time:30942ms step_avg:39.77ms
step:779/1920 train_time:31003ms step_avg:39.80ms
step:780/1920 train_time:31065ms step_avg:39.83ms
step:781/1920 train_time:31128ms step_avg:39.86ms
step:782/1920 train_time:31191ms step_avg:39.89ms
step:783/1920 train_time:31254ms step_avg:39.92ms
step:784/1920 train_time:31316ms step_avg:39.94ms
step:785/1920 train_time:31379ms step_avg:39.97ms
step:786/1920 train_time:31441ms step_avg:40.00ms
step:787/1920 train_time:31504ms step_avg:40.03ms
step:788/1920 train_time:31566ms step_avg:40.06ms
step:789/1920 train_time:31629ms step_avg:40.09ms
step:790/1920 train_time:31691ms step_avg:40.12ms
step:791/1920 train_time:31753ms step_avg:40.14ms
step:792/1920 train_time:31815ms step_avg:40.17ms
step:793/1920 train_time:31878ms step_avg:40.20ms
step:794/1920 train_time:31940ms step_avg:40.23ms
step:795/1920 train_time:32001ms step_avg:40.25ms
step:796/1920 train_time:32063ms step_avg:40.28ms
step:797/1920 train_time:32125ms step_avg:40.31ms
step:798/1920 train_time:32187ms step_avg:40.33ms
step:799/1920 train_time:32250ms step_avg:40.36ms
step:800/1920 train_time:32312ms step_avg:40.39ms
step:801/1920 train_time:32375ms step_avg:40.42ms
step:802/1920 train_time:32437ms step_avg:40.45ms
step:803/1920 train_time:32499ms step_avg:40.47ms
step:804/1920 train_time:32561ms step_avg:40.50ms
step:805/1920 train_time:32623ms step_avg:40.53ms
step:806/1920 train_time:32686ms step_avg:40.55ms
step:807/1920 train_time:32748ms step_avg:40.58ms
step:808/1920 train_time:32811ms step_avg:40.61ms
step:809/1920 train_time:32874ms step_avg:40.64ms
step:810/1920 train_time:32935ms step_avg:40.66ms
step:811/1920 train_time:32998ms step_avg:40.69ms
step:812/1920 train_time:33060ms step_avg:40.71ms
step:813/1920 train_time:33122ms step_avg:40.74ms
step:814/1920 train_time:33183ms step_avg:40.77ms
step:815/1920 train_time:33245ms step_avg:40.79ms
step:816/1920 train_time:33307ms step_avg:40.82ms
step:817/1920 train_time:33370ms step_avg:40.85ms
step:818/1920 train_time:33432ms step_avg:40.87ms
step:819/1920 train_time:33495ms step_avg:40.90ms
step:820/1920 train_time:33557ms step_avg:40.92ms
step:821/1920 train_time:33620ms step_avg:40.95ms
step:822/1920 train_time:33681ms step_avg:40.97ms
step:823/1920 train_time:33744ms step_avg:41.00ms
step:824/1920 train_time:33806ms step_avg:41.03ms
step:825/1920 train_time:33868ms step_avg:41.05ms
step:826/1920 train_time:33931ms step_avg:41.08ms
step:827/1920 train_time:33994ms step_avg:41.10ms
step:828/1920 train_time:34055ms step_avg:41.13ms
step:829/1920 train_time:34118ms step_avg:41.16ms
step:830/1920 train_time:34180ms step_avg:41.18ms
step:831/1920 train_time:34242ms step_avg:41.21ms
step:832/1920 train_time:34304ms step_avg:41.23ms
step:833/1920 train_time:34366ms step_avg:41.26ms
step:834/1920 train_time:34428ms step_avg:41.28ms
step:835/1920 train_time:34491ms step_avg:41.31ms
step:836/1920 train_time:34553ms step_avg:41.33ms
step:837/1920 train_time:34616ms step_avg:41.36ms
step:838/1920 train_time:34678ms step_avg:41.38ms
step:839/1920 train_time:34741ms step_avg:41.41ms
step:840/1920 train_time:34802ms step_avg:41.43ms
step:841/1920 train_time:34865ms step_avg:41.46ms
step:842/1920 train_time:34926ms step_avg:41.48ms
step:843/1920 train_time:34990ms step_avg:41.51ms
step:844/1920 train_time:35051ms step_avg:41.53ms
step:845/1920 train_time:35114ms step_avg:41.56ms
step:846/1920 train_time:35176ms step_avg:41.58ms
step:847/1920 train_time:35239ms step_avg:41.60ms
step:848/1920 train_time:35301ms step_avg:41.63ms
step:849/1920 train_time:35363ms step_avg:41.65ms
step:850/1920 train_time:35425ms step_avg:41.68ms
step:851/1920 train_time:35488ms step_avg:41.70ms
step:852/1920 train_time:35550ms step_avg:41.73ms
step:853/1920 train_time:35613ms step_avg:41.75ms
step:854/1920 train_time:35675ms step_avg:41.77ms
step:855/1920 train_time:35738ms step_avg:41.80ms
step:856/1920 train_time:35799ms step_avg:41.82ms
step:857/1920 train_time:35862ms step_avg:41.85ms
step:858/1920 train_time:35923ms step_avg:41.87ms
step:859/1920 train_time:35985ms step_avg:41.89ms
step:860/1920 train_time:36047ms step_avg:41.92ms
step:861/1920 train_time:36110ms step_avg:41.94ms
step:862/1920 train_time:36172ms step_avg:41.96ms
step:863/1920 train_time:36234ms step_avg:41.99ms
step:864/1920 train_time:36295ms step_avg:42.01ms
step:865/1920 train_time:36358ms step_avg:42.03ms
step:866/1920 train_time:36419ms step_avg:42.05ms
step:867/1920 train_time:36481ms step_avg:42.08ms
step:868/1920 train_time:36543ms step_avg:42.10ms
step:869/1920 train_time:36606ms step_avg:42.12ms
step:870/1920 train_time:36669ms step_avg:42.15ms
step:871/1920 train_time:36732ms step_avg:42.17ms
step:872/1920 train_time:36794ms step_avg:42.20ms
step:873/1920 train_time:36857ms step_avg:42.22ms
step:874/1920 train_time:36918ms step_avg:42.24ms
step:875/1920 train_time:36981ms step_avg:42.26ms
step:876/1920 train_time:37042ms step_avg:42.29ms
step:877/1920 train_time:37105ms step_avg:42.31ms
step:878/1920 train_time:37166ms step_avg:42.33ms
step:879/1920 train_time:37229ms step_avg:42.35ms
step:880/1920 train_time:37292ms step_avg:42.38ms
step:881/1920 train_time:37355ms step_avg:42.40ms
step:882/1920 train_time:37416ms step_avg:42.42ms
step:883/1920 train_time:37478ms step_avg:42.44ms
step:884/1920 train_time:37540ms step_avg:42.47ms
step:885/1920 train_time:37602ms step_avg:42.49ms
step:886/1920 train_time:37664ms step_avg:42.51ms
step:887/1920 train_time:37727ms step_avg:42.53ms
step:888/1920 train_time:37788ms step_avg:42.55ms
step:889/1920 train_time:37852ms step_avg:42.58ms
step:890/1920 train_time:37913ms step_avg:42.60ms
step:891/1920 train_time:37976ms step_avg:42.62ms
step:892/1920 train_time:38038ms step_avg:42.64ms
step:893/1920 train_time:38100ms step_avg:42.67ms
step:894/1920 train_time:38161ms step_avg:42.69ms
step:895/1920 train_time:38223ms step_avg:42.71ms
step:896/1920 train_time:38285ms step_avg:42.73ms
step:897/1920 train_time:38349ms step_avg:42.75ms
step:898/1920 train_time:38411ms step_avg:42.77ms
step:899/1920 train_time:38474ms step_avg:42.80ms
step:900/1920 train_time:38536ms step_avg:42.82ms
step:901/1920 train_time:38599ms step_avg:42.84ms
step:902/1920 train_time:38660ms step_avg:42.86ms
step:903/1920 train_time:38722ms step_avg:42.88ms
step:904/1920 train_time:38784ms step_avg:42.90ms
step:905/1920 train_time:38847ms step_avg:42.92ms
step:906/1920 train_time:38908ms step_avg:42.95ms
step:907/1920 train_time:38972ms step_avg:42.97ms
step:908/1920 train_time:39033ms step_avg:42.99ms
step:909/1920 train_time:39096ms step_avg:43.01ms
step:910/1920 train_time:39158ms step_avg:43.03ms
step:911/1920 train_time:39221ms step_avg:43.05ms
step:912/1920 train_time:39282ms step_avg:43.07ms
step:913/1920 train_time:39344ms step_avg:43.09ms
step:914/1920 train_time:39406ms step_avg:43.11ms
step:915/1920 train_time:39469ms step_avg:43.14ms
step:916/1920 train_time:39532ms step_avg:43.16ms
step:917/1920 train_time:39595ms step_avg:43.18ms
step:918/1920 train_time:39657ms step_avg:43.20ms
step:919/1920 train_time:39719ms step_avg:43.22ms
step:920/1920 train_time:39781ms step_avg:43.24ms
step:921/1920 train_time:39843ms step_avg:43.26ms
step:922/1920 train_time:39905ms step_avg:43.28ms
step:923/1920 train_time:39968ms step_avg:43.30ms
step:924/1920 train_time:40030ms step_avg:43.32ms
step:925/1920 train_time:40094ms step_avg:43.34ms
step:926/1920 train_time:40156ms step_avg:43.36ms
step:927/1920 train_time:40218ms step_avg:43.39ms
step:928/1920 train_time:40280ms step_avg:43.41ms
step:929/1920 train_time:40343ms step_avg:43.43ms
step:930/1920 train_time:40405ms step_avg:43.45ms
step:931/1920 train_time:40467ms step_avg:43.47ms
step:932/1920 train_time:40529ms step_avg:43.49ms
step:933/1920 train_time:40592ms step_avg:43.51ms
step:934/1920 train_time:40654ms step_avg:43.53ms
step:935/1920 train_time:40717ms step_avg:43.55ms
step:936/1920 train_time:40779ms step_avg:43.57ms
step:937/1920 train_time:40841ms step_avg:43.59ms
step:938/1920 train_time:40902ms step_avg:43.61ms
step:939/1920 train_time:40965ms step_avg:43.63ms
step:940/1920 train_time:41027ms step_avg:43.65ms
step:941/1920 train_time:41090ms step_avg:43.67ms
step:942/1920 train_time:41152ms step_avg:43.69ms
step:943/1920 train_time:41215ms step_avg:43.71ms
step:944/1920 train_time:41277ms step_avg:43.73ms
step:945/1920 train_time:41340ms step_avg:43.75ms
step:946/1920 train_time:41401ms step_avg:43.76ms
step:947/1920 train_time:41463ms step_avg:43.78ms
step:948/1920 train_time:41525ms step_avg:43.80ms
step:949/1920 train_time:41588ms step_avg:43.82ms
step:950/1920 train_time:41650ms step_avg:43.84ms
step:951/1920 train_time:41714ms step_avg:43.86ms
step:952/1920 train_time:41776ms step_avg:43.88ms
step:953/1920 train_time:41838ms step_avg:43.90ms
step:954/1920 train_time:41900ms step_avg:43.92ms
step:955/1920 train_time:41962ms step_avg:43.94ms
step:956/1920 train_time:42024ms step_avg:43.96ms
step:957/1920 train_time:42086ms step_avg:43.98ms
step:958/1920 train_time:42149ms step_avg:44.00ms
step:959/1920 train_time:42212ms step_avg:44.02ms
step:960/1920 train_time:42274ms step_avg:44.04ms
step:961/1920 train_time:42337ms step_avg:44.05ms
step:962/1920 train_time:42399ms step_avg:44.07ms
step:963/1920 train_time:42461ms step_avg:44.09ms
step:964/1920 train_time:42523ms step_avg:44.11ms
step:965/1920 train_time:42585ms step_avg:44.13ms
step:966/1920 train_time:42648ms step_avg:44.15ms
step:967/1920 train_time:42711ms step_avg:44.17ms
step:968/1920 train_time:42772ms step_avg:44.19ms
step:969/1920 train_time:42835ms step_avg:44.21ms
step:970/1920 train_time:42897ms step_avg:44.22ms
step:971/1920 train_time:42959ms step_avg:44.24ms
step:972/1920 train_time:43021ms step_avg:44.26ms
step:973/1920 train_time:43083ms step_avg:44.28ms
step:974/1920 train_time:43145ms step_avg:44.30ms
step:975/1920 train_time:43208ms step_avg:44.32ms
step:976/1920 train_time:43270ms step_avg:44.33ms
step:977/1920 train_time:43333ms step_avg:44.35ms
step:978/1920 train_time:43396ms step_avg:44.37ms
step:979/1920 train_time:43458ms step_avg:44.39ms
step:980/1920 train_time:43520ms step_avg:44.41ms
step:981/1920 train_time:43582ms step_avg:44.43ms
step:982/1920 train_time:43643ms step_avg:44.44ms
step:983/1920 train_time:43706ms step_avg:44.46ms
step:984/1920 train_time:43769ms step_avg:44.48ms
step:985/1920 train_time:43832ms step_avg:44.50ms
step:986/1920 train_time:43893ms step_avg:44.52ms
step:987/1920 train_time:43957ms step_avg:44.54ms
step:988/1920 train_time:44018ms step_avg:44.55ms
step:989/1920 train_time:44081ms step_avg:44.57ms
step:990/1920 train_time:44142ms step_avg:44.59ms
step:991/1920 train_time:44205ms step_avg:44.61ms
step:992/1920 train_time:44267ms step_avg:44.62ms
step:993/1920 train_time:44330ms step_avg:44.64ms
step:994/1920 train_time:44392ms step_avg:44.66ms
step:995/1920 train_time:44455ms step_avg:44.68ms
step:996/1920 train_time:44517ms step_avg:44.70ms
step:997/1920 train_time:44580ms step_avg:44.71ms
step:998/1920 train_time:44641ms step_avg:44.73ms
step:999/1920 train_time:44703ms step_avg:44.75ms
step:1000/1920 train_time:44765ms step_avg:44.77ms
step:1000/1920 val_loss:3.7832 train_time:44830ms step_avg:44.83ms
step:1001/1920 train_time:44849ms step_avg:44.80ms
step:1002/1920 train_time:44891ms step_avg:44.80ms
step:1003/1920 train_time:44956ms step_avg:44.82ms
step:1004/1920 train_time:45020ms step_avg:44.84ms
step:1005/1920 train_time:45083ms step_avg:44.86ms
step:1006/1920 train_time:45145ms step_avg:44.88ms
step:1007/1920 train_time:45207ms step_avg:44.89ms
step:1008/1920 train_time:45269ms step_avg:44.91ms
step:1009/1920 train_time:45331ms step_avg:44.93ms
step:1010/1920 train_time:45392ms step_avg:44.94ms
step:1011/1920 train_time:45454ms step_avg:44.96ms
step:1012/1920 train_time:45515ms step_avg:44.98ms
step:1013/1920 train_time:45577ms step_avg:44.99ms
step:1014/1920 train_time:45639ms step_avg:45.01ms
step:1015/1920 train_time:45700ms step_avg:45.03ms
step:1016/1920 train_time:45763ms step_avg:45.04ms
step:1017/1920 train_time:45827ms step_avg:45.06ms
step:1018/1920 train_time:45889ms step_avg:45.08ms
step:1019/1920 train_time:45953ms step_avg:45.10ms
step:1020/1920 train_time:46014ms step_avg:45.11ms
step:1021/1920 train_time:46077ms step_avg:45.13ms
step:1022/1920 train_time:46140ms step_avg:45.15ms
step:1023/1920 train_time:46202ms step_avg:45.16ms
step:1024/1920 train_time:46265ms step_avg:45.18ms
step:1025/1920 train_time:46327ms step_avg:45.20ms
step:1026/1920 train_time:46389ms step_avg:45.21ms
step:1027/1920 train_time:46451ms step_avg:45.23ms
step:1028/1920 train_time:46512ms step_avg:45.25ms
step:1029/1920 train_time:46574ms step_avg:45.26ms
step:1030/1920 train_time:46636ms step_avg:45.28ms
step:1031/1920 train_time:46698ms step_avg:45.29ms
step:1032/1920 train_time:46760ms step_avg:45.31ms
step:1033/1920 train_time:46822ms step_avg:45.33ms
step:1034/1920 train_time:46885ms step_avg:45.34ms
step:1035/1920 train_time:46948ms step_avg:45.36ms
step:1036/1920 train_time:47010ms step_avg:45.38ms
step:1037/1920 train_time:47073ms step_avg:45.39ms
step:1038/1920 train_time:47135ms step_avg:45.41ms
step:1039/1920 train_time:47197ms step_avg:45.43ms
step:1040/1920 train_time:47259ms step_avg:45.44ms
step:1041/1920 train_time:47322ms step_avg:45.46ms
step:1042/1920 train_time:47384ms step_avg:45.47ms
step:1043/1920 train_time:47447ms step_avg:45.49ms
step:1044/1920 train_time:47509ms step_avg:45.51ms
step:1045/1920 train_time:47572ms step_avg:45.52ms
step:1046/1920 train_time:47633ms step_avg:45.54ms
step:1047/1920 train_time:47695ms step_avg:45.55ms
step:1048/1920 train_time:47756ms step_avg:45.57ms
step:1049/1920 train_time:47819ms step_avg:45.59ms
step:1050/1920 train_time:47881ms step_avg:45.60ms
step:1051/1920 train_time:47944ms step_avg:45.62ms
step:1052/1920 train_time:48006ms step_avg:45.63ms
step:1053/1920 train_time:48069ms step_avg:45.65ms
step:1054/1920 train_time:48131ms step_avg:45.66ms
step:1055/1920 train_time:48193ms step_avg:45.68ms
step:1056/1920 train_time:48255ms step_avg:45.70ms
step:1057/1920 train_time:48318ms step_avg:45.71ms
step:1058/1920 train_time:48379ms step_avg:45.73ms
step:1059/1920 train_time:48442ms step_avg:45.74ms
step:1060/1920 train_time:48505ms step_avg:45.76ms
step:1061/1920 train_time:48569ms step_avg:45.78ms
step:1062/1920 train_time:48631ms step_avg:45.79ms
step:1063/1920 train_time:48693ms step_avg:45.81ms
step:1064/1920 train_time:48754ms step_avg:45.82ms
step:1065/1920 train_time:48816ms step_avg:45.84ms
step:1066/1920 train_time:48878ms step_avg:45.85ms
step:1067/1920 train_time:48941ms step_avg:45.87ms
step:1068/1920 train_time:49003ms step_avg:45.88ms
step:1069/1920 train_time:49067ms step_avg:45.90ms
step:1070/1920 train_time:49129ms step_avg:45.91ms
step:1071/1920 train_time:49191ms step_avg:45.93ms
step:1072/1920 train_time:49253ms step_avg:45.94ms
step:1073/1920 train_time:49315ms step_avg:45.96ms
step:1074/1920 train_time:49377ms step_avg:45.97ms
step:1075/1920 train_time:49440ms step_avg:45.99ms
step:1076/1920 train_time:49502ms step_avg:46.01ms
step:1077/1920 train_time:49565ms step_avg:46.02ms
step:1078/1920 train_time:49627ms step_avg:46.04ms
step:1079/1920 train_time:49690ms step_avg:46.05ms
step:1080/1920 train_time:49751ms step_avg:46.07ms
step:1081/1920 train_time:49814ms step_avg:46.08ms
step:1082/1920 train_time:49875ms step_avg:46.10ms
step:1083/1920 train_time:49937ms step_avg:46.11ms
step:1084/1920 train_time:49999ms step_avg:46.12ms
step:1085/1920 train_time:50062ms step_avg:46.14ms
step:1086/1920 train_time:50124ms step_avg:46.16ms
step:1087/1920 train_time:50187ms step_avg:46.17ms
step:1088/1920 train_time:50249ms step_avg:46.18ms
step:1089/1920 train_time:50312ms step_avg:46.20ms
step:1090/1920 train_time:50374ms step_avg:46.21ms
step:1091/1920 train_time:50436ms step_avg:46.23ms
step:1092/1920 train_time:50498ms step_avg:46.24ms
step:1093/1920 train_time:50561ms step_avg:46.26ms
step:1094/1920 train_time:50623ms step_avg:46.27ms
step:1095/1920 train_time:50686ms step_avg:46.29ms
step:1096/1920 train_time:50748ms step_avg:46.30ms
step:1097/1920 train_time:50811ms step_avg:46.32ms
step:1098/1920 train_time:50872ms step_avg:46.33ms
step:1099/1920 train_time:50934ms step_avg:46.35ms
step:1100/1920 train_time:50997ms step_avg:46.36ms
step:1101/1920 train_time:51059ms step_avg:46.38ms
step:1102/1920 train_time:51121ms step_avg:46.39ms
step:1103/1920 train_time:51184ms step_avg:46.40ms
step:1104/1920 train_time:51246ms step_avg:46.42ms
step:1105/1920 train_time:51309ms step_avg:46.43ms
step:1106/1920 train_time:51371ms step_avg:46.45ms
step:1107/1920 train_time:51434ms step_avg:46.46ms
step:1108/1920 train_time:51495ms step_avg:46.48ms
step:1109/1920 train_time:51557ms step_avg:46.49ms
step:1110/1920 train_time:51620ms step_avg:46.50ms
step:1111/1920 train_time:51683ms step_avg:46.52ms
step:1112/1920 train_time:51745ms step_avg:46.53ms
step:1113/1920 train_time:51808ms step_avg:46.55ms
step:1114/1920 train_time:51870ms step_avg:46.56ms
step:1115/1920 train_time:51932ms step_avg:46.58ms
step:1116/1920 train_time:51994ms step_avg:46.59ms
step:1117/1920 train_time:52056ms step_avg:46.60ms
step:1118/1920 train_time:52118ms step_avg:46.62ms
step:1119/1920 train_time:52181ms step_avg:46.63ms
step:1120/1920 train_time:52244ms step_avg:46.65ms
step:1121/1920 train_time:52308ms step_avg:46.66ms
step:1122/1920 train_time:52370ms step_avg:46.68ms
step:1123/1920 train_time:52433ms step_avg:46.69ms
step:1124/1920 train_time:52494ms step_avg:46.70ms
step:1125/1920 train_time:52556ms step_avg:46.72ms
step:1126/1920 train_time:52618ms step_avg:46.73ms
step:1127/1920 train_time:52681ms step_avg:46.74ms
step:1128/1920 train_time:52743ms step_avg:46.76ms
step:1129/1920 train_time:52807ms step_avg:46.77ms
step:1130/1920 train_time:52869ms step_avg:46.79ms
step:1131/1920 train_time:52932ms step_avg:46.80ms
step:1132/1920 train_time:52994ms step_avg:46.81ms
step:1133/1920 train_time:53056ms step_avg:46.83ms
step:1134/1920 train_time:53118ms step_avg:46.84ms
step:1135/1920 train_time:53180ms step_avg:46.85ms
step:1136/1920 train_time:53242ms step_avg:46.87ms
step:1137/1920 train_time:53306ms step_avg:46.88ms
step:1138/1920 train_time:53368ms step_avg:46.90ms
step:1139/1920 train_time:53430ms step_avg:46.91ms
step:1140/1920 train_time:53492ms step_avg:46.92ms
step:1141/1920 train_time:53554ms step_avg:46.94ms
step:1142/1920 train_time:53615ms step_avg:46.95ms
step:1143/1920 train_time:53677ms step_avg:46.96ms
step:1144/1920 train_time:53739ms step_avg:46.97ms
step:1145/1920 train_time:53802ms step_avg:46.99ms
step:1146/1920 train_time:53865ms step_avg:47.00ms
step:1147/1920 train_time:53929ms step_avg:47.02ms
step:1148/1920 train_time:53990ms step_avg:47.03ms
step:1149/1920 train_time:54053ms step_avg:47.04ms
step:1150/1920 train_time:54114ms step_avg:47.06ms
step:1151/1920 train_time:54176ms step_avg:47.07ms
step:1152/1920 train_time:54238ms step_avg:47.08ms
step:1153/1920 train_time:54301ms step_avg:47.10ms
step:1154/1920 train_time:54364ms step_avg:47.11ms
step:1155/1920 train_time:54427ms step_avg:47.12ms
step:1156/1920 train_time:54489ms step_avg:47.14ms
step:1157/1920 train_time:54552ms step_avg:47.15ms
step:1158/1920 train_time:54613ms step_avg:47.16ms
step:1159/1920 train_time:54675ms step_avg:47.17ms
step:1160/1920 train_time:54737ms step_avg:47.19ms
step:1161/1920 train_time:54800ms step_avg:47.20ms
step:1162/1920 train_time:54863ms step_avg:47.21ms
step:1163/1920 train_time:54926ms step_avg:47.23ms
step:1164/1920 train_time:54988ms step_avg:47.24ms
step:1165/1920 train_time:55051ms step_avg:47.25ms
step:1166/1920 train_time:55112ms step_avg:47.27ms
step:1167/1920 train_time:55175ms step_avg:47.28ms
step:1168/1920 train_time:55237ms step_avg:47.29ms
step:1169/1920 train_time:55300ms step_avg:47.30ms
step:1170/1920 train_time:55362ms step_avg:47.32ms
step:1171/1920 train_time:55426ms step_avg:47.33ms
step:1172/1920 train_time:55487ms step_avg:47.34ms
step:1173/1920 train_time:55549ms step_avg:47.36ms
step:1174/1920 train_time:55610ms step_avg:47.37ms
step:1175/1920 train_time:55673ms step_avg:47.38ms
step:1176/1920 train_time:55734ms step_avg:47.39ms
step:1177/1920 train_time:55797ms step_avg:47.41ms
step:1178/1920 train_time:55859ms step_avg:47.42ms
step:1179/1920 train_time:55923ms step_avg:47.43ms
step:1180/1920 train_time:55985ms step_avg:47.44ms
step:1181/1920 train_time:56048ms step_avg:47.46ms
step:1182/1920 train_time:56110ms step_avg:47.47ms
step:1183/1920 train_time:56172ms step_avg:47.48ms
step:1184/1920 train_time:56234ms step_avg:47.49ms
step:1185/1920 train_time:56296ms step_avg:47.51ms
step:1186/1920 train_time:56358ms step_avg:47.52ms
step:1187/1920 train_time:56421ms step_avg:47.53ms
step:1188/1920 train_time:56483ms step_avg:47.54ms
step:1189/1920 train_time:56546ms step_avg:47.56ms
step:1190/1920 train_time:56607ms step_avg:47.57ms
step:1191/1920 train_time:56670ms step_avg:47.58ms
step:1192/1920 train_time:56732ms step_avg:47.59ms
step:1193/1920 train_time:56795ms step_avg:47.61ms
step:1194/1920 train_time:56857ms step_avg:47.62ms
step:1195/1920 train_time:56919ms step_avg:47.63ms
step:1196/1920 train_time:56981ms step_avg:47.64ms
step:1197/1920 train_time:57044ms step_avg:47.66ms
step:1198/1920 train_time:57106ms step_avg:47.67ms
step:1199/1920 train_time:57169ms step_avg:47.68ms
step:1200/1920 train_time:57231ms step_avg:47.69ms
step:1201/1920 train_time:57293ms step_avg:47.70ms
step:1202/1920 train_time:57354ms step_avg:47.72ms
step:1203/1920 train_time:57417ms step_avg:47.73ms
step:1204/1920 train_time:57479ms step_avg:47.74ms
step:1205/1920 train_time:57542ms step_avg:47.75ms
step:1206/1920 train_time:57604ms step_avg:47.76ms
step:1207/1920 train_time:57667ms step_avg:47.78ms
step:1208/1920 train_time:57729ms step_avg:47.79ms
step:1209/1920 train_time:57791ms step_avg:47.80ms
step:1210/1920 train_time:57853ms step_avg:47.81ms
step:1211/1920 train_time:57915ms step_avg:47.82ms
step:1212/1920 train_time:57977ms step_avg:47.84ms
step:1213/1920 train_time:58040ms step_avg:47.85ms
step:1214/1920 train_time:58101ms step_avg:47.86ms
step:1215/1920 train_time:58164ms step_avg:47.87ms
step:1216/1920 train_time:58226ms step_avg:47.88ms
step:1217/1920 train_time:58289ms step_avg:47.90ms
step:1218/1920 train_time:58350ms step_avg:47.91ms
step:1219/1920 train_time:58413ms step_avg:47.92ms
step:1220/1920 train_time:58475ms step_avg:47.93ms
step:1221/1920 train_time:58538ms step_avg:47.94ms
step:1222/1920 train_time:58600ms step_avg:47.95ms
step:1223/1920 train_time:58663ms step_avg:47.97ms
step:1224/1920 train_time:58725ms step_avg:47.98ms
step:1225/1920 train_time:58789ms step_avg:47.99ms
step:1226/1920 train_time:58851ms step_avg:48.00ms
step:1227/1920 train_time:58914ms step_avg:48.01ms
step:1228/1920 train_time:58975ms step_avg:48.03ms
step:1229/1920 train_time:59037ms step_avg:48.04ms
step:1230/1920 train_time:59099ms step_avg:48.05ms
step:1231/1920 train_time:59162ms step_avg:48.06ms
step:1232/1920 train_time:59224ms step_avg:48.07ms
step:1233/1920 train_time:59288ms step_avg:48.08ms
step:1234/1920 train_time:59350ms step_avg:48.10ms
step:1235/1920 train_time:59412ms step_avg:48.11ms
step:1236/1920 train_time:59474ms step_avg:48.12ms
step:1237/1920 train_time:59536ms step_avg:48.13ms
step:1238/1920 train_time:59598ms step_avg:48.14ms
step:1239/1920 train_time:59661ms step_avg:48.15ms
step:1240/1920 train_time:59723ms step_avg:48.16ms
step:1241/1920 train_time:59786ms step_avg:48.18ms
step:1242/1920 train_time:59848ms step_avg:48.19ms
step:1243/1920 train_time:59911ms step_avg:48.20ms
step:1244/1920 train_time:59972ms step_avg:48.21ms
step:1245/1920 train_time:60035ms step_avg:48.22ms
step:1246/1920 train_time:60097ms step_avg:48.23ms
step:1247/1920 train_time:60159ms step_avg:48.24ms
step:1248/1920 train_time:60221ms step_avg:48.25ms
step:1249/1920 train_time:60284ms step_avg:48.27ms
step:1250/1920 train_time:60347ms step_avg:48.28ms
step:1250/1920 val_loss:3.5555 train_time:60412ms step_avg:48.33ms
step:1251/1920 train_time:60431ms step_avg:48.31ms
step:1252/1920 train_time:60473ms step_avg:48.30ms
step:1253/1920 train_time:60536ms step_avg:48.31ms
step:1254/1920 train_time:60600ms step_avg:48.33ms
step:1255/1920 train_time:60663ms step_avg:48.34ms
step:1256/1920 train_time:60750ms step_avg:48.37ms
step:1257/1920 train_time:60838ms step_avg:48.40ms
step:1258/1920 train_time:60925ms step_avg:48.43ms
step:1259/1920 train_time:61012ms step_avg:48.46ms
step:1260/1920 train_time:61099ms step_avg:48.49ms
step:1261/1920 train_time:61187ms step_avg:48.52ms
step:1262/1920 train_time:61274ms step_avg:48.55ms
step:1263/1920 train_time:61365ms step_avg:48.59ms
step:1264/1920 train_time:61454ms step_avg:48.62ms
step:1265/1920 train_time:61546ms step_avg:48.65ms
step:1266/1920 train_time:61635ms step_avg:48.68ms
step:1267/1920 train_time:61724ms step_avg:48.72ms
step:1268/1920 train_time:61812ms step_avg:48.75ms
step:1269/1920 train_time:61901ms step_avg:48.78ms
step:1270/1920 train_time:61987ms step_avg:48.81ms
step:1271/1920 train_time:62075ms step_avg:48.84ms
step:1272/1920 train_time:62162ms step_avg:48.87ms
step:1273/1920 train_time:62250ms step_avg:48.90ms
step:1274/1920 train_time:62338ms step_avg:48.93ms
step:1275/1920 train_time:62429ms step_avg:48.96ms
step:1276/1920 train_time:62517ms step_avg:48.99ms
step:1277/1920 train_time:62607ms step_avg:49.03ms
step:1278/1920 train_time:62697ms step_avg:49.06ms
step:1279/1920 train_time:62786ms step_avg:49.09ms
step:1280/1920 train_time:62874ms step_avg:49.12ms
step:1281/1920 train_time:62962ms step_avg:49.15ms
step:1282/1920 train_time:63049ms step_avg:49.18ms
step:1283/1920 train_time:63137ms step_avg:49.21ms
step:1284/1920 train_time:63225ms step_avg:49.24ms
step:1285/1920 train_time:63314ms step_avg:49.27ms
step:1286/1920 train_time:63402ms step_avg:49.30ms
step:1287/1920 train_time:63491ms step_avg:49.33ms
step:1288/1920 train_time:63580ms step_avg:49.36ms
step:1289/1920 train_time:63670ms step_avg:49.39ms
step:1290/1920 train_time:63758ms step_avg:49.43ms
step:1291/1920 train_time:63848ms step_avg:49.46ms
step:1292/1920 train_time:63935ms step_avg:49.49ms
step:1293/1920 train_time:64023ms step_avg:49.52ms
step:1294/1920 train_time:64110ms step_avg:49.54ms
step:1295/1920 train_time:64198ms step_avg:49.57ms
step:1296/1920 train_time:64286ms step_avg:49.60ms
step:1297/1920 train_time:64374ms step_avg:49.63ms
step:1298/1920 train_time:64463ms step_avg:49.66ms
step:1299/1920 train_time:64552ms step_avg:49.69ms
step:1300/1920 train_time:64640ms step_avg:49.72ms
step:1301/1920 train_time:64729ms step_avg:49.75ms
step:1302/1920 train_time:64818ms step_avg:49.78ms
step:1303/1920 train_time:64906ms step_avg:49.81ms
step:1304/1920 train_time:64994ms step_avg:49.84ms
step:1305/1920 train_time:65082ms step_avg:49.87ms
step:1306/1920 train_time:65169ms step_avg:49.90ms
step:1307/1920 train_time:65257ms step_avg:49.93ms
step:1308/1920 train_time:65345ms step_avg:49.96ms
step:1309/1920 train_time:65434ms step_avg:49.99ms
step:1310/1920 train_time:65523ms step_avg:50.02ms
step:1311/1920 train_time:65611ms step_avg:50.05ms
step:1312/1920 train_time:65700ms step_avg:50.08ms
step:1313/1920 train_time:65788ms step_avg:50.11ms
step:1314/1920 train_time:65876ms step_avg:50.13ms
step:1315/1920 train_time:65967ms step_avg:50.16ms
step:1316/1920 train_time:66056ms step_avg:50.19ms
step:1317/1920 train_time:66144ms step_avg:50.22ms
step:1318/1920 train_time:66232ms step_avg:50.25ms
step:1319/1920 train_time:66320ms step_avg:50.28ms
step:1320/1920 train_time:66408ms step_avg:50.31ms
step:1321/1920 train_time:66497ms step_avg:50.34ms
step:1322/1920 train_time:66586ms step_avg:50.37ms
step:1323/1920 train_time:66673ms step_avg:50.40ms
step:1324/1920 train_time:66763ms step_avg:50.42ms
step:1325/1920 train_time:66851ms step_avg:50.45ms
step:1326/1920 train_time:66939ms step_avg:50.48ms
step:1327/1920 train_time:67028ms step_avg:50.51ms
step:1328/1920 train_time:67116ms step_avg:50.54ms
step:1329/1920 train_time:67205ms step_avg:50.57ms
step:1330/1920 train_time:67292ms step_avg:50.60ms
step:1331/1920 train_time:67381ms step_avg:50.62ms
step:1332/1920 train_time:67468ms step_avg:50.65ms
step:1333/1920 train_time:67557ms step_avg:50.68ms
step:1334/1920 train_time:67646ms step_avg:50.71ms
step:1335/1920 train_time:67734ms step_avg:50.74ms
step:1336/1920 train_time:67823ms step_avg:50.77ms
step:1337/1920 train_time:67911ms step_avg:50.79ms
step:1338/1920 train_time:67999ms step_avg:50.82ms
step:1339/1920 train_time:68088ms step_avg:50.85ms
step:1340/1920 train_time:68177ms step_avg:50.88ms
step:1341/1920 train_time:68266ms step_avg:50.91ms
step:1342/1920 train_time:68353ms step_avg:50.93ms
step:1343/1920 train_time:68442ms step_avg:50.96ms
step:1344/1920 train_time:68529ms step_avg:50.99ms
step:1345/1920 train_time:68619ms step_avg:51.02ms
step:1346/1920 train_time:68707ms step_avg:51.05ms
step:1347/1920 train_time:68795ms step_avg:51.07ms
step:1348/1920 train_time:68884ms step_avg:51.10ms
step:1349/1920 train_time:68973ms step_avg:51.13ms
step:1350/1920 train_time:69061ms step_avg:51.16ms
step:1351/1920 train_time:69150ms step_avg:51.18ms
step:1352/1920 train_time:69239ms step_avg:51.21ms
step:1353/1920 train_time:69327ms step_avg:51.24ms
step:1354/1920 train_time:69415ms step_avg:51.27ms
step:1355/1920 train_time:69505ms step_avg:51.29ms
step:1356/1920 train_time:69593ms step_avg:51.32ms
step:1357/1920 train_time:69682ms step_avg:51.35ms
step:1358/1920 train_time:69769ms step_avg:51.38ms
step:1359/1920 train_time:69858ms step_avg:51.40ms
step:1360/1920 train_time:69946ms step_avg:51.43ms
step:1361/1920 train_time:70034ms step_avg:51.46ms
step:1362/1920 train_time:70123ms step_avg:51.49ms
step:1363/1920 train_time:70211ms step_avg:51.51ms
step:1364/1920 train_time:70300ms step_avg:51.54ms
step:1365/1920 train_time:70387ms step_avg:51.57ms
step:1366/1920 train_time:70476ms step_avg:51.59ms
step:1367/1920 train_time:70566ms step_avg:51.62ms
step:1368/1920 train_time:70655ms step_avg:51.65ms
step:1369/1920 train_time:70744ms step_avg:51.68ms
step:1370/1920 train_time:70833ms step_avg:51.70ms
step:1371/1920 train_time:70921ms step_avg:51.73ms
step:1372/1920 train_time:71008ms step_avg:51.76ms
step:1373/1920 train_time:71096ms step_avg:51.78ms
step:1374/1920 train_time:71184ms step_avg:51.81ms
step:1375/1920 train_time:71273ms step_avg:51.83ms
step:1376/1920 train_time:71361ms step_avg:51.86ms
step:1377/1920 train_time:71450ms step_avg:51.89ms
step:1378/1920 train_time:71539ms step_avg:51.92ms
step:1379/1920 train_time:71629ms step_avg:51.94ms
step:1380/1920 train_time:71719ms step_avg:51.97ms
step:1381/1920 train_time:71809ms step_avg:52.00ms
step:1382/1920 train_time:71897ms step_avg:52.02ms
step:1383/1920 train_time:71986ms step_avg:52.05ms
step:1384/1920 train_time:72075ms step_avg:52.08ms
step:1385/1920 train_time:72163ms step_avg:52.10ms
step:1386/1920 train_time:72251ms step_avg:52.13ms
step:1387/1920 train_time:72340ms step_avg:52.16ms
step:1388/1920 train_time:72427ms step_avg:52.18ms
step:1389/1920 train_time:72516ms step_avg:52.21ms
step:1390/1920 train_time:72604ms step_avg:52.23ms
step:1391/1920 train_time:72693ms step_avg:52.26ms
step:1392/1920 train_time:72781ms step_avg:52.29ms
step:1393/1920 train_time:72870ms step_avg:52.31ms
step:1394/1920 train_time:72958ms step_avg:52.34ms
step:1395/1920 train_time:73046ms step_avg:52.36ms
step:1396/1920 train_time:73135ms step_avg:52.39ms
step:1397/1920 train_time:73224ms step_avg:52.42ms
step:1398/1920 train_time:73312ms step_avg:52.44ms
step:1399/1920 train_time:73400ms step_avg:52.47ms
step:1400/1920 train_time:73488ms step_avg:52.49ms
step:1401/1920 train_time:73576ms step_avg:52.52ms
step:1402/1920 train_time:73664ms step_avg:52.54ms
step:1403/1920 train_time:73753ms step_avg:52.57ms
step:1404/1920 train_time:73842ms step_avg:52.59ms
step:1405/1920 train_time:73930ms step_avg:52.62ms
step:1406/1920 train_time:74018ms step_avg:52.64ms
step:1407/1920 train_time:74107ms step_avg:52.67ms
step:1408/1920 train_time:74194ms step_avg:52.69ms
step:1409/1920 train_time:74283ms step_avg:52.72ms
step:1410/1920 train_time:74370ms step_avg:52.74ms
step:1411/1920 train_time:74459ms step_avg:52.77ms
step:1412/1920 train_time:74547ms step_avg:52.80ms
step:1413/1920 train_time:74635ms step_avg:52.82ms
step:1414/1920 train_time:74723ms step_avg:52.84ms
step:1415/1920 train_time:74811ms step_avg:52.87ms
step:1416/1920 train_time:74900ms step_avg:52.90ms
step:1417/1920 train_time:74989ms step_avg:52.92ms
step:1418/1920 train_time:75078ms step_avg:52.95ms
step:1419/1920 train_time:75167ms step_avg:52.97ms
step:1420/1920 train_time:75255ms step_avg:53.00ms
step:1421/1920 train_time:75345ms step_avg:53.02ms
step:1422/1920 train_time:75433ms step_avg:53.05ms
step:1423/1920 train_time:75522ms step_avg:53.07ms
step:1424/1920 train_time:75609ms step_avg:53.10ms
step:1425/1920 train_time:75698ms step_avg:53.12ms
step:1426/1920 train_time:75785ms step_avg:53.15ms
step:1427/1920 train_time:75873ms step_avg:53.17ms
step:1428/1920 train_time:75962ms step_avg:53.19ms
step:1429/1920 train_time:76050ms step_avg:53.22ms
step:1430/1920 train_time:76138ms step_avg:53.24ms
step:1431/1920 train_time:76227ms step_avg:53.27ms
step:1432/1920 train_time:76315ms step_avg:53.29ms
step:1433/1920 train_time:76404ms step_avg:53.32ms
step:1434/1920 train_time:76492ms step_avg:53.34ms
step:1435/1920 train_time:76580ms step_avg:53.37ms
step:1436/1920 train_time:76667ms step_avg:53.39ms
step:1437/1920 train_time:76755ms step_avg:53.41ms
step:1438/1920 train_time:76843ms step_avg:53.44ms
step:1439/1920 train_time:76931ms step_avg:53.46ms
step:1440/1920 train_time:77019ms step_avg:53.49ms
step:1441/1920 train_time:77109ms step_avg:53.51ms
step:1442/1920 train_time:77197ms step_avg:53.53ms
step:1443/1920 train_time:77286ms step_avg:53.56ms
step:1444/1920 train_time:77373ms step_avg:53.58ms
step:1445/1920 train_time:77462ms step_avg:53.61ms
step:1446/1920 train_time:77549ms step_avg:53.63ms
step:1447/1920 train_time:77638ms step_avg:53.65ms
step:1448/1920 train_time:77726ms step_avg:53.68ms
step:1449/1920 train_time:77814ms step_avg:53.70ms
step:1450/1920 train_time:77902ms step_avg:53.73ms
step:1451/1920 train_time:77990ms step_avg:53.75ms
step:1452/1920 train_time:78078ms step_avg:53.77ms
step:1453/1920 train_time:78168ms step_avg:53.80ms
step:1454/1920 train_time:78257ms step_avg:53.82ms
step:1455/1920 train_time:78347ms step_avg:53.85ms
step:1456/1920 train_time:78435ms step_avg:53.87ms
step:1457/1920 train_time:78524ms step_avg:53.89ms
step:1458/1920 train_time:78612ms step_avg:53.92ms
step:1459/1920 train_time:78701ms step_avg:53.94ms
step:1460/1920 train_time:78788ms step_avg:53.96ms
step:1461/1920 train_time:78877ms step_avg:53.99ms
step:1462/1920 train_time:78965ms step_avg:54.01ms
step:1463/1920 train_time:79053ms step_avg:54.04ms
step:1464/1920 train_time:79142ms step_avg:54.06ms
step:1465/1920 train_time:79230ms step_avg:54.08ms
step:1466/1920 train_time:79319ms step_avg:54.11ms
step:1467/1920 train_time:79408ms step_avg:54.13ms
step:1468/1920 train_time:79495ms step_avg:54.15ms
step:1469/1920 train_time:79584ms step_avg:54.18ms
step:1470/1920 train_time:79672ms step_avg:54.20ms
step:1471/1920 train_time:79761ms step_avg:54.22ms
step:1472/1920 train_time:79848ms step_avg:54.24ms
step:1473/1920 train_time:79938ms step_avg:54.27ms
step:1474/1920 train_time:80025ms step_avg:54.29ms
step:1475/1920 train_time:80114ms step_avg:54.31ms
step:1476/1920 train_time:80202ms step_avg:54.34ms
step:1477/1920 train_time:80290ms step_avg:54.36ms
step:1478/1920 train_time:80378ms step_avg:54.38ms
step:1479/1920 train_time:80467ms step_avg:54.41ms
step:1480/1920 train_time:80555ms step_avg:54.43ms
step:1481/1920 train_time:80645ms step_avg:54.45ms
step:1482/1920 train_time:80733ms step_avg:54.48ms
step:1483/1920 train_time:80822ms step_avg:54.50ms
step:1484/1920 train_time:80910ms step_avg:54.52ms
step:1485/1920 train_time:80999ms step_avg:54.54ms
step:1486/1920 train_time:81087ms step_avg:54.57ms
step:1487/1920 train_time:81174ms step_avg:54.59ms
step:1488/1920 train_time:81263ms step_avg:54.61ms
step:1489/1920 train_time:81351ms step_avg:54.63ms
step:1490/1920 train_time:81439ms step_avg:54.66ms
step:1491/1920 train_time:81528ms step_avg:54.68ms
step:1492/1920 train_time:81617ms step_avg:54.70ms
step:1493/1920 train_time:81705ms step_avg:54.73ms
step:1494/1920 train_time:81793ms step_avg:54.75ms
step:1495/1920 train_time:81882ms step_avg:54.77ms
step:1496/1920 train_time:81969ms step_avg:54.79ms
step:1497/1920 train_time:82059ms step_avg:54.82ms
step:1498/1920 train_time:82146ms step_avg:54.84ms
step:1499/1920 train_time:82235ms step_avg:54.86ms
step:1500/1920 train_time:82323ms step_avg:54.88ms
step:1500/1920 val_loss:3.4146 train_time:82414ms step_avg:54.94ms
step:1501/1920 train_time:82432ms step_avg:54.92ms
step:1502/1920 train_time:82505ms step_avg:54.93ms
step:1503/1920 train_time:82595ms step_avg:54.95ms
step:1504/1920 train_time:82683ms step_avg:54.98ms
step:1505/1920 train_time:82771ms step_avg:55.00ms
step:1506/1920 train_time:82857ms step_avg:55.02ms
step:1507/1920 train_time:82945ms step_avg:55.04ms
step:1508/1920 train_time:83032ms step_avg:55.06ms
step:1509/1920 train_time:83120ms step_avg:55.08ms
step:1510/1920 train_time:83208ms step_avg:55.10ms
step:1511/1920 train_time:83297ms step_avg:55.13ms
step:1512/1920 train_time:83386ms step_avg:55.15ms
step:1513/1920 train_time:83477ms step_avg:55.17ms
step:1514/1920 train_time:83566ms step_avg:55.20ms
step:1515/1920 train_time:83655ms step_avg:55.22ms
step:1516/1920 train_time:83742ms step_avg:55.24ms
step:1517/1920 train_time:83830ms step_avg:55.26ms
step:1518/1920 train_time:83917ms step_avg:55.28ms
step:1519/1920 train_time:84005ms step_avg:55.30ms
step:1520/1920 train_time:84092ms step_avg:55.32ms
step:1521/1920 train_time:84181ms step_avg:55.35ms
step:1522/1920 train_time:84269ms step_avg:55.37ms
step:1523/1920 train_time:84359ms step_avg:55.39ms
step:1524/1920 train_time:84447ms step_avg:55.41ms
step:1525/1920 train_time:84538ms step_avg:55.43ms
step:1526/1920 train_time:84626ms step_avg:55.46ms
step:1527/1920 train_time:84714ms step_avg:55.48ms
step:1528/1920 train_time:84802ms step_avg:55.50ms
step:1529/1920 train_time:84889ms step_avg:55.52ms
step:1530/1920 train_time:84978ms step_avg:55.54ms
step:1531/1920 train_time:85065ms step_avg:55.56ms
step:1532/1920 train_time:85153ms step_avg:55.58ms
step:1533/1920 train_time:85242ms step_avg:55.60ms
step:1534/1920 train_time:85330ms step_avg:55.63ms
step:1535/1920 train_time:85419ms step_avg:55.65ms
step:1536/1920 train_time:85508ms step_avg:55.67ms
step:1537/1920 train_time:85599ms step_avg:55.69ms
step:1538/1920 train_time:85686ms step_avg:55.71ms
step:1539/1920 train_time:85774ms step_avg:55.73ms
step:1540/1920 train_time:85862ms step_avg:55.75ms
step:1541/1920 train_time:85950ms step_avg:55.78ms
step:1542/1920 train_time:86037ms step_avg:55.80ms
step:1543/1920 train_time:86125ms step_avg:55.82ms
step:1544/1920 train_time:86213ms step_avg:55.84ms
step:1545/1920 train_time:86303ms step_avg:55.86ms
step:1546/1920 train_time:86391ms step_avg:55.88ms
step:1547/1920 train_time:86480ms step_avg:55.90ms
step:1548/1920 train_time:86568ms step_avg:55.92ms
step:1549/1920 train_time:86657ms step_avg:55.94ms
step:1550/1920 train_time:86745ms step_avg:55.96ms
step:1551/1920 train_time:86833ms step_avg:55.99ms
step:1552/1920 train_time:86921ms step_avg:56.01ms
step:1553/1920 train_time:87009ms step_avg:56.03ms
step:1554/1920 train_time:87097ms step_avg:56.05ms
step:1555/1920 train_time:87186ms step_avg:56.07ms
step:1556/1920 train_time:87274ms step_avg:56.09ms
step:1557/1920 train_time:87364ms step_avg:56.11ms
step:1558/1920 train_time:87452ms step_avg:56.13ms
step:1559/1920 train_time:87542ms step_avg:56.15ms
step:1560/1920 train_time:87630ms step_avg:56.17ms
step:1561/1920 train_time:87719ms step_avg:56.19ms
step:1562/1920 train_time:87806ms step_avg:56.21ms
step:1563/1920 train_time:87895ms step_avg:56.23ms
step:1564/1920 train_time:87983ms step_avg:56.25ms
step:1565/1920 train_time:88071ms step_avg:56.28ms
step:1566/1920 train_time:88160ms step_avg:56.30ms
step:1567/1920 train_time:88249ms step_avg:56.32ms
step:1568/1920 train_time:88337ms step_avg:56.34ms
step:1569/1920 train_time:88426ms step_avg:56.36ms
step:1570/1920 train_time:88515ms step_avg:56.38ms
step:1571/1920 train_time:88605ms step_avg:56.40ms
step:1572/1920 train_time:88694ms step_avg:56.42ms
step:1573/1920 train_time:88782ms step_avg:56.44ms
step:1574/1920 train_time:88870ms step_avg:56.46ms
step:1575/1920 train_time:88958ms step_avg:56.48ms
step:1576/1920 train_time:89045ms step_avg:56.50ms
step:1577/1920 train_time:89134ms step_avg:56.52ms
step:1578/1920 train_time:89221ms step_avg:56.54ms
step:1579/1920 train_time:89310ms step_avg:56.56ms
step:1580/1920 train_time:89399ms step_avg:56.58ms
step:1581/1920 train_time:89487ms step_avg:56.60ms
step:1582/1920 train_time:89575ms step_avg:56.62ms
step:1583/1920 train_time:89665ms step_avg:56.64ms
step:1584/1920 train_time:89753ms step_avg:56.66ms
step:1585/1920 train_time:89842ms step_avg:56.68ms
step:1586/1920 train_time:89930ms step_avg:56.70ms
step:1587/1920 train_time:90018ms step_avg:56.72ms
step:1588/1920 train_time:90106ms step_avg:56.74ms
step:1589/1920 train_time:90195ms step_avg:56.76ms
step:1590/1920 train_time:90283ms step_avg:56.78ms
step:1591/1920 train_time:90373ms step_avg:56.80ms
step:1592/1920 train_time:90461ms step_avg:56.82ms
step:1593/1920 train_time:90550ms step_avg:56.84ms
step:1594/1920 train_time:90638ms step_avg:56.86ms
step:1595/1920 train_time:90727ms step_avg:56.88ms
step:1596/1920 train_time:90815ms step_avg:56.90ms
step:1597/1920 train_time:90904ms step_avg:56.92ms
step:1598/1920 train_time:90993ms step_avg:56.94ms
step:1599/1920 train_time:91083ms step_avg:56.96ms
step:1600/1920 train_time:91171ms step_avg:56.98ms
step:1601/1920 train_time:91260ms step_avg:57.00ms
step:1602/1920 train_time:91348ms step_avg:57.02ms
step:1603/1920 train_time:91437ms step_avg:57.04ms
step:1604/1920 train_time:91526ms step_avg:57.06ms
step:1605/1920 train_time:91614ms step_avg:57.08ms
step:1606/1920 train_time:91702ms step_avg:57.10ms
step:1607/1920 train_time:91789ms step_avg:57.12ms
step:1608/1920 train_time:91878ms step_avg:57.14ms
step:1609/1920 train_time:91967ms step_avg:57.16ms
step:1610/1920 train_time:92055ms step_avg:57.18ms
step:1611/1920 train_time:92144ms step_avg:57.20ms
step:1612/1920 train_time:92232ms step_avg:57.22ms
step:1613/1920 train_time:92321ms step_avg:57.24ms
step:1614/1920 train_time:92409ms step_avg:57.25ms
step:1615/1920 train_time:92499ms step_avg:57.27ms
step:1616/1920 train_time:92586ms step_avg:57.29ms
step:1617/1920 train_time:92675ms step_avg:57.31ms
step:1618/1920 train_time:92764ms step_avg:57.33ms
step:1619/1920 train_time:92853ms step_avg:57.35ms
step:1620/1920 train_time:92942ms step_avg:57.37ms
step:1621/1920 train_time:93032ms step_avg:57.39ms
step:1622/1920 train_time:93120ms step_avg:57.41ms
step:1623/1920 train_time:93209ms step_avg:57.43ms
step:1624/1920 train_time:93298ms step_avg:57.45ms
step:1625/1920 train_time:93386ms step_avg:57.47ms
step:1626/1920 train_time:93475ms step_avg:57.49ms
step:1627/1920 train_time:93564ms step_avg:57.51ms
step:1628/1920 train_time:93651ms step_avg:57.53ms
step:1629/1920 train_time:93740ms step_avg:57.54ms
step:1630/1920 train_time:93828ms step_avg:57.56ms
step:1631/1920 train_time:93917ms step_avg:57.58ms
step:1632/1920 train_time:94004ms step_avg:57.60ms
step:1633/1920 train_time:94093ms step_avg:57.62ms
step:1634/1920 train_time:94181ms step_avg:57.64ms
step:1635/1920 train_time:94270ms step_avg:57.66ms
step:1636/1920 train_time:94358ms step_avg:57.68ms
step:1637/1920 train_time:94446ms step_avg:57.69ms
step:1638/1920 train_time:94535ms step_avg:57.71ms
step:1639/1920 train_time:94623ms step_avg:57.73ms
step:1640/1920 train_time:94711ms step_avg:57.75ms
step:1641/1920 train_time:94801ms step_avg:57.77ms
step:1642/1920 train_time:94888ms step_avg:57.79ms
step:1643/1920 train_time:94977ms step_avg:57.81ms
step:1644/1920 train_time:95065ms step_avg:57.83ms
step:1645/1920 train_time:95153ms step_avg:57.84ms
step:1646/1920 train_time:95242ms step_avg:57.86ms
step:1647/1920 train_time:95330ms step_avg:57.88ms
step:1648/1920 train_time:95418ms step_avg:57.90ms
step:1649/1920 train_time:95507ms step_avg:57.92ms
step:1650/1920 train_time:95596ms step_avg:57.94ms
step:1651/1920 train_time:95684ms step_avg:57.96ms
step:1652/1920 train_time:95773ms step_avg:57.97ms
step:1653/1920 train_time:95862ms step_avg:57.99ms
step:1654/1920 train_time:95950ms step_avg:58.01ms
step:1655/1920 train_time:96039ms step_avg:58.03ms
step:1656/1920 train_time:96127ms step_avg:58.05ms
step:1657/1920 train_time:96216ms step_avg:58.07ms
step:1658/1920 train_time:96304ms step_avg:58.08ms
step:1659/1920 train_time:96393ms step_avg:58.10ms
step:1660/1920 train_time:96481ms step_avg:58.12ms
step:1661/1920 train_time:96569ms step_avg:58.14ms
step:1662/1920 train_time:96657ms step_avg:58.16ms
step:1663/1920 train_time:96746ms step_avg:58.18ms
step:1664/1920 train_time:96836ms step_avg:58.19ms
step:1665/1920 train_time:96924ms step_avg:58.21ms
step:1666/1920 train_time:97012ms step_avg:58.23ms
step:1667/1920 train_time:97102ms step_avg:58.25ms
step:1668/1920 train_time:97189ms step_avg:58.27ms
step:1669/1920 train_time:97278ms step_avg:58.29ms
step:1670/1920 train_time:97366ms step_avg:58.30ms
step:1671/1920 train_time:97455ms step_avg:58.32ms
step:1672/1920 train_time:97542ms step_avg:58.34ms
step:1673/1920 train_time:97631ms step_avg:58.36ms
step:1674/1920 train_time:97719ms step_avg:58.37ms
step:1675/1920 train_time:97807ms step_avg:58.39ms
step:1676/1920 train_time:97896ms step_avg:58.41ms
step:1677/1920 train_time:97985ms step_avg:58.43ms
step:1678/1920 train_time:98074ms step_avg:58.45ms
step:1679/1920 train_time:98164ms step_avg:58.47ms
step:1680/1920 train_time:98252ms step_avg:58.48ms
step:1681/1920 train_time:98342ms step_avg:58.50ms
step:1682/1920 train_time:98429ms step_avg:58.52ms
step:1683/1920 train_time:98518ms step_avg:58.54ms
step:1684/1920 train_time:98606ms step_avg:58.55ms
step:1685/1920 train_time:98694ms step_avg:58.57ms
step:1686/1920 train_time:98782ms step_avg:58.59ms
step:1687/1920 train_time:98871ms step_avg:58.61ms
step:1688/1920 train_time:98960ms step_avg:58.63ms
step:1689/1920 train_time:99048ms step_avg:58.64ms
step:1690/1920 train_time:99136ms step_avg:58.66ms
step:1691/1920 train_time:99225ms step_avg:58.68ms
step:1692/1920 train_time:99313ms step_avg:58.70ms
step:1693/1920 train_time:99402ms step_avg:58.71ms
step:1694/1920 train_time:99491ms step_avg:58.73ms
step:1695/1920 train_time:99580ms step_avg:58.75ms
step:1696/1920 train_time:99667ms step_avg:58.77ms
step:1697/1920 train_time:99756ms step_avg:58.78ms
step:1698/1920 train_time:99843ms step_avg:58.80ms
step:1699/1920 train_time:99932ms step_avg:58.82ms
step:1700/1920 train_time:100020ms step_avg:58.84ms
step:1701/1920 train_time:100109ms step_avg:58.85ms
step:1702/1920 train_time:100197ms step_avg:58.87ms
step:1703/1920 train_time:100286ms step_avg:58.89ms
step:1704/1920 train_time:100374ms step_avg:58.91ms
step:1705/1920 train_time:100464ms step_avg:58.92ms
step:1706/1920 train_time:100552ms step_avg:58.94ms
step:1707/1920 train_time:100641ms step_avg:58.96ms
step:1708/1920 train_time:100729ms step_avg:58.97ms
step:1709/1920 train_time:100818ms step_avg:58.99ms
step:1710/1920 train_time:100906ms step_avg:59.01ms
step:1711/1920 train_time:100995ms step_avg:59.03ms
step:1712/1920 train_time:101082ms step_avg:59.04ms
step:1713/1920 train_time:101171ms step_avg:59.06ms
step:1714/1920 train_time:101258ms step_avg:59.08ms
step:1715/1920 train_time:101347ms step_avg:59.09ms
step:1716/1920 train_time:101436ms step_avg:59.11ms
step:1717/1920 train_time:101524ms step_avg:59.13ms
step:1718/1920 train_time:101612ms step_avg:59.15ms
step:1719/1920 train_time:101701ms step_avg:59.16ms
step:1720/1920 train_time:101789ms step_avg:59.18ms
step:1721/1920 train_time:101879ms step_avg:59.20ms
step:1722/1920 train_time:101966ms step_avg:59.21ms
step:1723/1920 train_time:102055ms step_avg:59.23ms
step:1724/1920 train_time:102143ms step_avg:59.25ms
step:1725/1920 train_time:102231ms step_avg:59.26ms
step:1726/1920 train_time:102319ms step_avg:59.28ms
step:1727/1920 train_time:102408ms step_avg:59.30ms
step:1728/1920 train_time:102496ms step_avg:59.31ms
step:1729/1920 train_time:102584ms step_avg:59.33ms
step:1730/1920 train_time:102673ms step_avg:59.35ms
step:1731/1920 train_time:102762ms step_avg:59.37ms
step:1732/1920 train_time:102850ms step_avg:59.38ms
step:1733/1920 train_time:102939ms step_avg:59.40ms
step:1734/1920 train_time:103027ms step_avg:59.42ms
step:1735/1920 train_time:103115ms step_avg:59.43ms
step:1736/1920 train_time:103203ms step_avg:59.45ms
step:1737/1920 train_time:103291ms step_avg:59.47ms
step:1738/1920 train_time:103380ms step_avg:59.48ms
step:1739/1920 train_time:103469ms step_avg:59.50ms
step:1740/1920 train_time:103556ms step_avg:59.52ms
step:1741/1920 train_time:103645ms step_avg:59.53ms
step:1742/1920 train_time:103733ms step_avg:59.55ms
step:1743/1920 train_time:103822ms step_avg:59.57ms
step:1744/1920 train_time:103910ms step_avg:59.58ms
step:1745/1920 train_time:103999ms step_avg:59.60ms
step:1746/1920 train_time:104086ms step_avg:59.61ms
step:1747/1920 train_time:104175ms step_avg:59.63ms
step:1748/1920 train_time:104263ms step_avg:59.65ms
step:1749/1920 train_time:104351ms step_avg:59.66ms
step:1750/1920 train_time:104439ms step_avg:59.68ms
step:1750/1920 val_loss:3.3240 train_time:104531ms step_avg:59.73ms
step:1751/1920 train_time:104548ms step_avg:59.71ms
step:1752/1920 train_time:104621ms step_avg:59.71ms
step:1753/1920 train_time:104713ms step_avg:59.73ms
step:1754/1920 train_time:104801ms step_avg:59.75ms
step:1755/1920 train_time:104890ms step_avg:59.77ms
step:1756/1920 train_time:104976ms step_avg:59.78ms
step:1757/1920 train_time:105063ms step_avg:59.80ms
step:1758/1920 train_time:105150ms step_avg:59.81ms
step:1759/1920 train_time:105238ms step_avg:59.83ms
step:1760/1920 train_time:105326ms step_avg:59.84ms
step:1761/1920 train_time:105415ms step_avg:59.86ms
step:1762/1920 train_time:105505ms step_avg:59.88ms
step:1763/1920 train_time:105596ms step_avg:59.90ms
step:1764/1920 train_time:105687ms step_avg:59.91ms
step:1765/1920 train_time:105777ms step_avg:59.93ms
step:1766/1920 train_time:105865ms step_avg:59.95ms
step:1767/1920 train_time:105953ms step_avg:59.96ms
step:1768/1920 train_time:106040ms step_avg:59.98ms
step:1769/1920 train_time:106128ms step_avg:59.99ms
step:1770/1920 train_time:106215ms step_avg:60.01ms
step:1771/1920 train_time:106302ms step_avg:60.02ms
step:1772/1920 train_time:106390ms step_avg:60.04ms
step:1773/1920 train_time:106479ms step_avg:60.06ms
step:1774/1920 train_time:106571ms step_avg:60.07ms
step:1775/1920 train_time:106661ms step_avg:60.09ms
step:1776/1920 train_time:106750ms step_avg:60.11ms
step:1777/1920 train_time:106839ms step_avg:60.12ms
step:1778/1920 train_time:106927ms step_avg:60.14ms
step:1779/1920 train_time:107015ms step_avg:60.15ms
step:1780/1920 train_time:107102ms step_avg:60.17ms
step:1781/1920 train_time:107191ms step_avg:60.19ms
step:1782/1920 train_time:107278ms step_avg:60.20ms
step:1783/1920 train_time:107365ms step_avg:60.22ms
step:1784/1920 train_time:107454ms step_avg:60.23ms
step:1785/1920 train_time:107543ms step_avg:60.25ms
step:1786/1920 train_time:107632ms step_avg:60.26ms
step:1787/1920 train_time:107721ms step_avg:60.28ms
step:1788/1920 train_time:107810ms step_avg:60.30ms
step:1789/1920 train_time:107899ms step_avg:60.31ms
step:1790/1920 train_time:107987ms step_avg:60.33ms
step:1791/1920 train_time:108076ms step_avg:60.34ms
step:1792/1920 train_time:108163ms step_avg:60.36ms
step:1793/1920 train_time:108250ms step_avg:60.37ms
step:1794/1920 train_time:108337ms step_avg:60.39ms
step:1795/1920 train_time:108425ms step_avg:60.40ms
step:1796/1920 train_time:108513ms step_avg:60.42ms
step:1797/1920 train_time:108603ms step_avg:60.44ms
step:1798/1920 train_time:108693ms step_avg:60.45ms
step:1799/1920 train_time:108782ms step_avg:60.47ms
step:1800/1920 train_time:108869ms step_avg:60.48ms
step:1801/1920 train_time:108958ms step_avg:60.50ms
step:1802/1920 train_time:109046ms step_avg:60.51ms
step:1803/1920 train_time:109135ms step_avg:60.53ms
step:1804/1920 train_time:109222ms step_avg:60.54ms
step:1805/1920 train_time:109311ms step_avg:60.56ms
step:1806/1920 train_time:109398ms step_avg:60.57ms
step:1807/1920 train_time:109487ms step_avg:60.59ms
step:1808/1920 train_time:109576ms step_avg:60.61ms
step:1809/1920 train_time:109664ms step_avg:60.62ms
step:1810/1920 train_time:109753ms step_avg:60.64ms
step:1811/1920 train_time:109842ms step_avg:60.65ms
step:1812/1920 train_time:109930ms step_avg:60.67ms
step:1813/1920 train_time:110019ms step_avg:60.68ms
step:1814/1920 train_time:110107ms step_avg:60.70ms
step:1815/1920 train_time:110197ms step_avg:60.71ms
step:1816/1920 train_time:110284ms step_avg:60.73ms
step:1817/1920 train_time:110372ms step_avg:60.74ms
step:1818/1920 train_time:110460ms step_avg:60.76ms
step:1819/1920 train_time:110549ms step_avg:60.77ms
step:1820/1920 train_time:110638ms step_avg:60.79ms
step:1821/1920 train_time:110727ms step_avg:60.81ms
step:1822/1920 train_time:110816ms step_avg:60.82ms
step:1823/1920 train_time:110904ms step_avg:60.84ms
step:1824/1920 train_time:110992ms step_avg:60.85ms
step:1825/1920 train_time:111081ms step_avg:60.87ms
step:1826/1920 train_time:111169ms step_avg:60.88ms
step:1827/1920 train_time:111257ms step_avg:60.90ms
step:1828/1920 train_time:111345ms step_avg:60.91ms
step:1829/1920 train_time:111433ms step_avg:60.93ms
step:1830/1920 train_time:111521ms step_avg:60.94ms
step:1831/1920 train_time:111610ms step_avg:60.96ms
step:1832/1920 train_time:111698ms step_avg:60.97ms
step:1833/1920 train_time:111786ms step_avg:60.99ms
step:1834/1920 train_time:111874ms step_avg:61.00ms
step:1835/1920 train_time:111963ms step_avg:61.02ms
step:1836/1920 train_time:112051ms step_avg:61.03ms
step:1837/1920 train_time:112140ms step_avg:61.05ms
step:1838/1920 train_time:112228ms step_avg:61.06ms
step:1839/1920 train_time:112317ms step_avg:61.08ms
step:1840/1920 train_time:112405ms step_avg:61.09ms
step:1841/1920 train_time:112494ms step_avg:61.11ms
step:1842/1920 train_time:112582ms step_avg:61.12ms
step:1843/1920 train_time:112671ms step_avg:61.13ms
step:1844/1920 train_time:112759ms step_avg:61.15ms
step:1845/1920 train_time:112847ms step_avg:61.16ms
step:1846/1920 train_time:112936ms step_avg:61.18ms
step:1847/1920 train_time:113024ms step_avg:61.19ms
step:1848/1920 train_time:113112ms step_avg:61.21ms
step:1849/1920 train_time:113201ms step_avg:61.22ms
step:1850/1920 train_time:113289ms step_avg:61.24ms
step:1851/1920 train_time:113377ms step_avg:61.25ms
step:1852/1920 train_time:113465ms step_avg:61.27ms
step:1853/1920 train_time:113554ms step_avg:61.28ms
step:1854/1920 train_time:113643ms step_avg:61.30ms
step:1855/1920 train_time:113733ms step_avg:61.31ms
step:1856/1920 train_time:113820ms step_avg:61.33ms
step:1857/1920 train_time:113909ms step_avg:61.34ms
step:1858/1920 train_time:113997ms step_avg:61.35ms
step:1859/1920 train_time:114085ms step_avg:61.37ms
step:1860/1920 train_time:114174ms step_avg:61.38ms
step:1861/1920 train_time:114263ms step_avg:61.40ms
step:1862/1920 train_time:114351ms step_avg:61.41ms
step:1863/1920 train_time:114439ms step_avg:61.43ms
step:1864/1920 train_time:114528ms step_avg:61.44ms
step:1865/1920 train_time:114617ms step_avg:61.46ms
step:1866/1920 train_time:114705ms step_avg:61.47ms
step:1867/1920 train_time:114793ms step_avg:61.49ms
step:1868/1920 train_time:114881ms step_avg:61.50ms
step:1869/1920 train_time:114969ms step_avg:61.51ms
step:1870/1920 train_time:115057ms step_avg:61.53ms
step:1871/1920 train_time:115145ms step_avg:61.54ms
step:1872/1920 train_time:115234ms step_avg:61.56ms
step:1873/1920 train_time:115322ms step_avg:61.57ms
step:1874/1920 train_time:115410ms step_avg:61.58ms
step:1875/1920 train_time:115500ms step_avg:61.60ms
step:1876/1920 train_time:115587ms step_avg:61.61ms
step:1877/1920 train_time:115676ms step_avg:61.63ms
step:1878/1920 train_time:115763ms step_avg:61.64ms
step:1879/1920 train_time:115852ms step_avg:61.66ms
step:1880/1920 train_time:115941ms step_avg:61.67ms
step:1881/1920 train_time:116029ms step_avg:61.68ms
step:1882/1920 train_time:116117ms step_avg:61.70ms
step:1883/1920 train_time:116205ms step_avg:61.71ms
step:1884/1920 train_time:116294ms step_avg:61.73ms
step:1885/1920 train_time:116383ms step_avg:61.74ms
step:1886/1920 train_time:116471ms step_avg:61.76ms
step:1887/1920 train_time:116560ms step_avg:61.77ms
step:1888/1920 train_time:116649ms step_avg:61.78ms
step:1889/1920 train_time:116738ms step_avg:61.80ms
step:1890/1920 train_time:116826ms step_avg:61.81ms
step:1891/1920 train_time:116916ms step_avg:61.83ms
step:1892/1920 train_time:117004ms step_avg:61.84ms
step:1893/1920 train_time:117094ms step_avg:61.86ms
step:1894/1920 train_time:117182ms step_avg:61.87ms
step:1895/1920 train_time:117271ms step_avg:61.88ms
step:1896/1920 train_time:117359ms step_avg:61.90ms
step:1897/1920 train_time:117447ms step_avg:61.91ms
step:1898/1920 train_time:117536ms step_avg:61.93ms
step:1899/1920 train_time:117625ms step_avg:61.94ms
step:1900/1920 train_time:117714ms step_avg:61.95ms
step:1901/1920 train_time:117803ms step_avg:61.97ms
step:1902/1920 train_time:117892ms step_avg:61.98ms
step:1903/1920 train_time:117981ms step_avg:62.00ms
step:1904/1920 train_time:118070ms step_avg:62.01ms
step:1905/1920 train_time:118159ms step_avg:62.03ms
step:1906/1920 train_time:118247ms step_avg:62.04ms
step:1907/1920 train_time:118337ms step_avg:62.05ms
step:1908/1920 train_time:118424ms step_avg:62.07ms
step:1909/1920 train_time:118515ms step_avg:62.08ms
step:1910/1920 train_time:118603ms step_avg:62.10ms
step:1911/1920 train_time:118691ms step_avg:62.11ms
step:1912/1920 train_time:118779ms step_avg:62.12ms
step:1913/1920 train_time:118868ms step_avg:62.14ms
step:1914/1920 train_time:118956ms step_avg:62.15ms
step:1915/1920 train_time:119044ms step_avg:62.16ms
step:1916/1920 train_time:119133ms step_avg:62.18ms
step:1917/1920 train_time:119222ms step_avg:62.19ms
step:1918/1920 train_time:119310ms step_avg:62.21ms
step:1919/1920 train_time:119398ms step_avg:62.22ms
step:1920/1920 train_time:119486ms step_avg:62.23ms
step:1920/1920 val_loss:3.2794 train_time:119578ms step_avg:62.28ms
peak memory allocated: 29817 MiB reserved: 45078 MiB

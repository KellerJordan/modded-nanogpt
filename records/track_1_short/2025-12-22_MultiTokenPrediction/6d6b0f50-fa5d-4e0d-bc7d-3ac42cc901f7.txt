import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:12:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    232691      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    232692      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    232693      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    232694      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    232695      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    232696      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    232697      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    232698      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    232692      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    232693      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    232694      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    232695      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    232696      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    232697      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    232698      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8342 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:89ms step_avg:89.09ms
step:2/1920 train_time:110ms step_avg:55.21ms
step:3/1920 train_time:134ms step_avg:44.74ms
step:4/1920 train_time:168ms step_avg:42.05ms
step:5/1920 train_time:202ms step_avg:40.50ms
step:6/1920 train_time:292ms step_avg:48.68ms
step:7/1920 train_time:309ms step_avg:44.18ms
step:8/1920 train_time:343ms step_avg:42.93ms
step:9/1920 train_time:378ms step_avg:41.95ms
step:10/1920 train_time:412ms step_avg:41.17ms
step:11/1920 train_time:446ms step_avg:40.56ms
step:12/1920 train_time:480ms step_avg:40.03ms
step:13/1920 train_time:515ms step_avg:39.60ms
step:14/1920 train_time:549ms step_avg:39.22ms
step:15/1920 train_time:583ms step_avg:38.90ms
step:16/1920 train_time:618ms step_avg:38.61ms
step:17/1920 train_time:652ms step_avg:38.38ms
step:18/1920 train_time:687ms step_avg:38.15ms
step:19/1920 train_time:721ms step_avg:37.97ms
step:20/1920 train_time:755ms step_avg:37.77ms
step:21/1920 train_time:790ms step_avg:37.62ms
step:22/1920 train_time:824ms step_avg:37.46ms
step:23/1920 train_time:859ms step_avg:37.35ms
step:24/1920 train_time:893ms step_avg:37.23ms
step:25/1920 train_time:928ms step_avg:37.12ms
step:26/1920 train_time:962ms step_avg:37.00ms
step:27/1920 train_time:997ms step_avg:36.91ms
step:28/1920 train_time:1031ms step_avg:36.82ms
step:29/1920 train_time:1065ms step_avg:36.73ms
step:30/1920 train_time:1100ms step_avg:36.65ms
step:31/1920 train_time:1134ms step_avg:36.58ms
step:32/1920 train_time:1168ms step_avg:36.51ms
step:33/1920 train_time:1204ms step_avg:36.49ms
step:34/1920 train_time:1239ms step_avg:36.44ms
step:35/1920 train_time:1274ms step_avg:36.40ms
step:36/1920 train_time:1308ms step_avg:36.34ms
step:37/1920 train_time:1343ms step_avg:36.31ms
step:38/1920 train_time:1378ms step_avg:36.26ms
step:39/1920 train_time:1412ms step_avg:36.21ms
step:40/1920 train_time:1447ms step_avg:36.17ms
step:41/1920 train_time:1482ms step_avg:36.14ms
step:42/1920 train_time:1516ms step_avg:36.10ms
step:43/1920 train_time:1551ms step_avg:36.07ms
step:44/1920 train_time:1585ms step_avg:36.02ms
step:45/1920 train_time:1619ms step_avg:35.99ms
step:46/1920 train_time:1654ms step_avg:35.95ms
step:47/1920 train_time:1688ms step_avg:35.92ms
step:48/1920 train_time:1722ms step_avg:35.88ms
step:49/1920 train_time:1757ms step_avg:35.85ms
step:50/1920 train_time:1791ms step_avg:35.82ms
step:51/1920 train_time:1825ms step_avg:35.79ms
step:52/1920 train_time:1860ms step_avg:35.76ms
step:53/1920 train_time:1894ms step_avg:35.74ms
step:54/1920 train_time:1928ms step_avg:35.71ms
step:55/1920 train_time:1963ms step_avg:35.69ms
step:56/1920 train_time:1997ms step_avg:35.67ms
step:57/1920 train_time:2032ms step_avg:35.65ms
step:58/1920 train_time:2066ms step_avg:35.63ms
step:59/1920 train_time:2101ms step_avg:35.61ms
step:60/1920 train_time:2135ms step_avg:35.58ms
step:61/1920 train_time:2170ms step_avg:35.57ms
step:62/1920 train_time:2204ms step_avg:35.55ms
step:63/1920 train_time:2239ms step_avg:35.54ms
step:64/1920 train_time:2273ms step_avg:35.52ms
step:65/1920 train_time:2308ms step_avg:35.50ms
step:66/1920 train_time:2342ms step_avg:35.49ms
step:67/1920 train_time:2377ms step_avg:35.47ms
step:68/1920 train_time:2411ms step_avg:35.46ms
step:69/1920 train_time:2446ms step_avg:35.45ms
step:70/1920 train_time:2481ms step_avg:35.44ms
step:71/1920 train_time:2516ms step_avg:35.43ms
step:72/1920 train_time:2550ms step_avg:35.42ms
step:73/1920 train_time:2585ms step_avg:35.41ms
step:74/1920 train_time:2619ms step_avg:35.40ms
step:75/1920 train_time:2654ms step_avg:35.38ms
step:76/1920 train_time:2688ms step_avg:35.37ms
step:77/1920 train_time:2723ms step_avg:35.36ms
step:78/1920 train_time:2757ms step_avg:35.34ms
step:79/1920 train_time:2791ms step_avg:35.33ms
step:80/1920 train_time:2826ms step_avg:35.32ms
step:81/1920 train_time:2860ms step_avg:35.31ms
step:82/1920 train_time:2894ms step_avg:35.29ms
step:83/1920 train_time:2928ms step_avg:35.28ms
step:84/1920 train_time:2963ms step_avg:35.27ms
step:85/1920 train_time:2997ms step_avg:35.26ms
step:86/1920 train_time:3031ms step_avg:35.25ms
step:87/1920 train_time:3066ms step_avg:35.24ms
step:88/1920 train_time:3100ms step_avg:35.23ms
step:89/1920 train_time:3135ms step_avg:35.23ms
step:90/1920 train_time:3169ms step_avg:35.21ms
step:91/1920 train_time:3204ms step_avg:35.21ms
step:92/1920 train_time:3238ms step_avg:35.19ms
step:93/1920 train_time:3272ms step_avg:35.19ms
step:94/1920 train_time:3307ms step_avg:35.18ms
step:95/1920 train_time:3341ms step_avg:35.17ms
step:96/1920 train_time:3376ms step_avg:35.17ms
step:97/1920 train_time:3411ms step_avg:35.16ms
step:98/1920 train_time:3445ms step_avg:35.16ms
step:99/1920 train_time:3480ms step_avg:35.15ms
step:100/1920 train_time:3515ms step_avg:35.15ms
step:101/1920 train_time:3549ms step_avg:35.14ms
step:102/1920 train_time:3583ms step_avg:35.13ms
step:103/1920 train_time:3618ms step_avg:35.12ms
step:104/1920 train_time:3652ms step_avg:35.12ms
step:105/1920 train_time:3686ms step_avg:35.11ms
step:106/1920 train_time:3721ms step_avg:35.10ms
step:107/1920 train_time:3755ms step_avg:35.09ms
step:108/1920 train_time:3789ms step_avg:35.08ms
step:109/1920 train_time:3824ms step_avg:35.08ms
step:110/1920 train_time:3858ms step_avg:35.07ms
step:111/1920 train_time:3893ms step_avg:35.07ms
step:112/1920 train_time:3927ms step_avg:35.06ms
step:113/1920 train_time:3962ms step_avg:35.06ms
step:114/1920 train_time:3996ms step_avg:35.05ms
step:115/1920 train_time:4030ms step_avg:35.05ms
step:116/1920 train_time:4065ms step_avg:35.04ms
step:117/1920 train_time:4099ms step_avg:35.04ms
step:118/1920 train_time:4133ms step_avg:35.03ms
step:119/1920 train_time:4168ms step_avg:35.03ms
step:120/1920 train_time:4202ms step_avg:35.02ms
step:121/1920 train_time:4237ms step_avg:35.02ms
step:122/1920 train_time:4271ms step_avg:35.01ms
step:123/1920 train_time:4306ms step_avg:35.01ms
step:124/1920 train_time:4341ms step_avg:35.01ms
step:125/1920 train_time:4375ms step_avg:35.00ms
step:126/1920 train_time:4410ms step_avg:35.00ms
step:127/1920 train_time:4445ms step_avg:35.00ms
step:128/1920 train_time:4479ms step_avg:34.99ms
step:129/1920 train_time:4514ms step_avg:34.99ms
step:130/1920 train_time:4548ms step_avg:34.98ms
step:131/1920 train_time:4582ms step_avg:34.98ms
step:132/1920 train_time:4617ms step_avg:34.98ms
step:133/1920 train_time:4651ms step_avg:34.97ms
step:134/1920 train_time:4686ms step_avg:34.97ms
step:135/1920 train_time:4720ms step_avg:34.97ms
step:136/1920 train_time:4755ms step_avg:34.96ms
step:137/1920 train_time:4789ms step_avg:34.96ms
step:138/1920 train_time:4823ms step_avg:34.95ms
step:139/1920 train_time:4857ms step_avg:34.95ms
step:140/1920 train_time:4892ms step_avg:34.94ms
step:141/1920 train_time:4926ms step_avg:34.94ms
step:142/1920 train_time:4960ms step_avg:34.93ms
step:143/1920 train_time:4994ms step_avg:34.93ms
step:144/1920 train_time:5029ms step_avg:34.92ms
step:145/1920 train_time:5063ms step_avg:34.92ms
step:146/1920 train_time:5097ms step_avg:34.91ms
step:147/1920 train_time:5132ms step_avg:34.91ms
step:148/1920 train_time:5166ms step_avg:34.90ms
step:149/1920 train_time:5200ms step_avg:34.90ms
step:150/1920 train_time:5234ms step_avg:34.89ms
step:151/1920 train_time:5269ms step_avg:34.89ms
step:152/1920 train_time:5303ms step_avg:34.89ms
step:153/1920 train_time:5338ms step_avg:34.89ms
step:154/1920 train_time:5372ms step_avg:34.89ms
step:155/1920 train_time:5407ms step_avg:34.88ms
step:156/1920 train_time:5441ms step_avg:34.88ms
step:157/1920 train_time:5475ms step_avg:34.88ms
step:158/1920 train_time:5510ms step_avg:34.87ms
step:159/1920 train_time:5545ms step_avg:34.87ms
step:160/1920 train_time:5579ms step_avg:34.87ms
step:161/1920 train_time:5613ms step_avg:34.86ms
step:162/1920 train_time:5647ms step_avg:34.86ms
step:163/1920 train_time:5682ms step_avg:34.86ms
step:164/1920 train_time:5716ms step_avg:34.86ms
step:165/1920 train_time:5751ms step_avg:34.86ms
step:166/1920 train_time:5785ms step_avg:34.85ms
step:167/1920 train_time:5820ms step_avg:34.85ms
step:168/1920 train_time:5854ms step_avg:34.85ms
step:169/1920 train_time:5889ms step_avg:34.85ms
step:170/1920 train_time:5923ms step_avg:34.84ms
step:171/1920 train_time:5958ms step_avg:34.84ms
step:172/1920 train_time:5993ms step_avg:34.84ms
step:173/1920 train_time:6027ms step_avg:34.84ms
step:174/1920 train_time:6061ms step_avg:34.83ms
step:175/1920 train_time:6095ms step_avg:34.83ms
step:176/1920 train_time:6129ms step_avg:34.83ms
step:177/1920 train_time:6164ms step_avg:34.82ms
step:178/1920 train_time:6198ms step_avg:34.82ms
step:179/1920 train_time:6232ms step_avg:34.82ms
step:180/1920 train_time:6267ms step_avg:34.81ms
step:181/1920 train_time:6302ms step_avg:34.82ms
step:182/1920 train_time:6336ms step_avg:34.81ms
step:183/1920 train_time:6370ms step_avg:34.81ms
step:184/1920 train_time:6404ms step_avg:34.81ms
step:185/1920 train_time:6439ms step_avg:34.80ms
step:186/1920 train_time:6473ms step_avg:34.80ms
step:187/1920 train_time:6508ms step_avg:34.80ms
step:188/1920 train_time:6542ms step_avg:34.80ms
step:189/1920 train_time:6577ms step_avg:34.80ms
step:190/1920 train_time:6611ms step_avg:34.79ms
step:191/1920 train_time:6645ms step_avg:34.79ms
step:192/1920 train_time:6680ms step_avg:34.79ms
step:193/1920 train_time:6714ms step_avg:34.79ms
step:194/1920 train_time:6748ms step_avg:34.78ms
step:195/1920 train_time:6783ms step_avg:34.78ms
step:196/1920 train_time:6817ms step_avg:34.78ms
step:197/1920 train_time:6852ms step_avg:34.78ms
step:198/1920 train_time:6886ms step_avg:34.78ms
step:199/1920 train_time:6921ms step_avg:34.78ms
step:200/1920 train_time:6955ms step_avg:34.78ms
step:201/1920 train_time:6990ms step_avg:34.77ms
step:202/1920 train_time:7024ms step_avg:34.77ms
step:203/1920 train_time:7058ms step_avg:34.77ms
step:204/1920 train_time:7092ms step_avg:34.77ms
step:205/1920 train_time:7126ms step_avg:34.76ms
step:206/1920 train_time:7161ms step_avg:34.76ms
step:207/1920 train_time:7195ms step_avg:34.76ms
step:208/1920 train_time:7230ms step_avg:34.76ms
step:209/1920 train_time:7264ms step_avg:34.76ms
step:210/1920 train_time:7298ms step_avg:34.75ms
step:211/1920 train_time:7333ms step_avg:34.75ms
step:212/1920 train_time:7367ms step_avg:34.75ms
step:213/1920 train_time:7402ms step_avg:34.75ms
step:214/1920 train_time:7436ms step_avg:34.75ms
step:215/1920 train_time:7470ms step_avg:34.75ms
step:216/1920 train_time:7505ms step_avg:34.74ms
step:217/1920 train_time:7539ms step_avg:34.74ms
step:218/1920 train_time:7573ms step_avg:34.74ms
step:219/1920 train_time:7608ms step_avg:34.74ms
step:220/1920 train_time:7642ms step_avg:34.74ms
step:221/1920 train_time:7677ms step_avg:34.74ms
step:222/1920 train_time:7711ms step_avg:34.74ms
step:223/1920 train_time:7746ms step_avg:34.73ms
step:224/1920 train_time:7780ms step_avg:34.73ms
step:225/1920 train_time:7814ms step_avg:34.73ms
step:226/1920 train_time:7848ms step_avg:34.73ms
step:227/1920 train_time:7883ms step_avg:34.73ms
step:228/1920 train_time:7918ms step_avg:34.73ms
step:229/1920 train_time:7952ms step_avg:34.72ms
step:230/1920 train_time:7986ms step_avg:34.72ms
step:231/1920 train_time:8020ms step_avg:34.72ms
step:232/1920 train_time:8055ms step_avg:34.72ms
step:233/1920 train_time:8089ms step_avg:34.72ms
step:234/1920 train_time:8123ms step_avg:34.72ms
step:235/1920 train_time:8158ms step_avg:34.71ms
step:236/1920 train_time:8192ms step_avg:34.71ms
step:237/1920 train_time:8226ms step_avg:34.71ms
step:238/1920 train_time:8260ms step_avg:34.71ms
step:239/1920 train_time:8295ms step_avg:34.71ms
step:240/1920 train_time:8329ms step_avg:34.70ms
step:241/1920 train_time:8364ms step_avg:34.70ms
step:242/1920 train_time:8398ms step_avg:34.70ms
step:243/1920 train_time:8432ms step_avg:34.70ms
step:244/1920 train_time:8466ms step_avg:34.70ms
step:245/1920 train_time:8501ms step_avg:34.70ms
step:246/1920 train_time:8536ms step_avg:34.70ms
step:247/1920 train_time:8570ms step_avg:34.70ms
step:248/1920 train_time:8604ms step_avg:34.69ms
step:249/1920 train_time:8639ms step_avg:34.69ms
step:250/1920 train_time:8673ms step_avg:34.69ms
step:250/1920 val_loss:4.6138 train_time:8711ms step_avg:34.84ms
step:251/1920 train_time:8728ms step_avg:34.77ms
step:252/1920 train_time:8746ms step_avg:34.71ms
step:253/1920 train_time:8780ms step_avg:34.70ms
step:254/1920 train_time:8815ms step_avg:34.70ms
step:255/1920 train_time:8852ms step_avg:34.71ms
step:256/1920 train_time:8887ms step_avg:34.71ms
step:257/1920 train_time:8922ms step_avg:34.72ms
step:258/1920 train_time:8956ms step_avg:34.72ms
step:259/1920 train_time:8992ms step_avg:34.72ms
step:260/1920 train_time:9026ms step_avg:34.71ms
step:261/1920 train_time:9060ms step_avg:34.71ms
step:262/1920 train_time:9094ms step_avg:34.71ms
step:263/1920 train_time:9129ms step_avg:34.71ms
step:264/1920 train_time:9163ms step_avg:34.71ms
step:265/1920 train_time:9197ms step_avg:34.71ms
step:266/1920 train_time:9231ms step_avg:34.70ms
step:267/1920 train_time:9266ms step_avg:34.70ms
step:268/1920 train_time:9300ms step_avg:34.70ms
step:269/1920 train_time:9334ms step_avg:34.70ms
step:270/1920 train_time:9368ms step_avg:34.70ms
step:271/1920 train_time:9402ms step_avg:34.69ms
step:272/1920 train_time:9436ms step_avg:34.69ms
step:273/1920 train_time:9471ms step_avg:34.69ms
step:274/1920 train_time:9505ms step_avg:34.69ms
step:275/1920 train_time:9539ms step_avg:34.69ms
step:276/1920 train_time:9573ms step_avg:34.69ms
step:277/1920 train_time:9607ms step_avg:34.68ms
step:278/1920 train_time:9641ms step_avg:34.68ms
step:279/1920 train_time:9676ms step_avg:34.68ms
step:280/1920 train_time:9710ms step_avg:34.68ms
step:281/1920 train_time:9745ms step_avg:34.68ms
step:282/1920 train_time:9779ms step_avg:34.68ms
step:283/1920 train_time:9814ms step_avg:34.68ms
step:284/1920 train_time:9849ms step_avg:34.68ms
step:285/1920 train_time:9883ms step_avg:34.68ms
step:286/1920 train_time:9918ms step_avg:34.68ms
step:287/1920 train_time:9953ms step_avg:34.68ms
step:288/1920 train_time:9987ms step_avg:34.68ms
step:289/1920 train_time:10022ms step_avg:34.68ms
step:290/1920 train_time:10057ms step_avg:34.68ms
step:291/1920 train_time:10092ms step_avg:34.68ms
step:292/1920 train_time:10126ms step_avg:34.68ms
step:293/1920 train_time:10160ms step_avg:34.68ms
step:294/1920 train_time:10194ms step_avg:34.67ms
step:295/1920 train_time:10229ms step_avg:34.67ms
step:296/1920 train_time:10263ms step_avg:34.67ms
step:297/1920 train_time:10297ms step_avg:34.67ms
step:298/1920 train_time:10331ms step_avg:34.67ms
step:299/1920 train_time:10366ms step_avg:34.67ms
step:300/1920 train_time:10400ms step_avg:34.67ms
step:301/1920 train_time:10434ms step_avg:34.66ms
step:302/1920 train_time:10468ms step_avg:34.66ms
step:303/1920 train_time:10502ms step_avg:34.66ms
step:304/1920 train_time:10536ms step_avg:34.66ms
step:305/1920 train_time:10571ms step_avg:34.66ms
step:306/1920 train_time:10605ms step_avg:34.66ms
step:307/1920 train_time:10639ms step_avg:34.66ms
step:308/1920 train_time:10673ms step_avg:34.65ms
step:309/1920 train_time:10708ms step_avg:34.65ms
step:310/1920 train_time:10742ms step_avg:34.65ms
step:311/1920 train_time:10776ms step_avg:34.65ms
step:312/1920 train_time:10811ms step_avg:34.65ms
step:313/1920 train_time:10845ms step_avg:34.65ms
step:314/1920 train_time:10880ms step_avg:34.65ms
step:315/1920 train_time:10914ms step_avg:34.65ms
step:316/1920 train_time:10949ms step_avg:34.65ms
step:317/1920 train_time:10983ms step_avg:34.65ms
step:318/1920 train_time:11017ms step_avg:34.65ms
step:319/1920 train_time:11052ms step_avg:34.65ms
step:320/1920 train_time:11086ms step_avg:34.64ms
step:321/1920 train_time:11121ms step_avg:34.64ms
step:322/1920 train_time:11155ms step_avg:34.64ms
step:323/1920 train_time:11189ms step_avg:34.64ms
step:324/1920 train_time:11224ms step_avg:34.64ms
step:325/1920 train_time:11258ms step_avg:34.64ms
step:326/1920 train_time:11292ms step_avg:34.64ms
step:327/1920 train_time:11327ms step_avg:34.64ms
step:328/1920 train_time:11361ms step_avg:34.64ms
step:329/1920 train_time:11395ms step_avg:34.64ms
step:330/1920 train_time:11429ms step_avg:34.63ms
step:331/1920 train_time:11463ms step_avg:34.63ms
step:332/1920 train_time:11498ms step_avg:34.63ms
step:333/1920 train_time:11532ms step_avg:34.63ms
step:334/1920 train_time:11567ms step_avg:34.63ms
step:335/1920 train_time:11601ms step_avg:34.63ms
step:336/1920 train_time:11635ms step_avg:34.63ms
step:337/1920 train_time:11669ms step_avg:34.63ms
step:338/1920 train_time:11703ms step_avg:34.63ms
step:339/1920 train_time:11737ms step_avg:34.62ms
step:340/1920 train_time:11772ms step_avg:34.62ms
step:341/1920 train_time:11806ms step_avg:34.62ms
step:342/1920 train_time:11841ms step_avg:34.62ms
step:343/1920 train_time:11875ms step_avg:34.62ms
step:344/1920 train_time:11910ms step_avg:34.62ms
step:345/1920 train_time:11944ms step_avg:34.62ms
step:346/1920 train_time:11978ms step_avg:34.62ms
step:347/1920 train_time:12013ms step_avg:34.62ms
step:348/1920 train_time:12047ms step_avg:34.62ms
step:349/1920 train_time:12081ms step_avg:34.62ms
step:350/1920 train_time:12116ms step_avg:34.62ms
step:351/1920 train_time:12150ms step_avg:34.61ms
step:352/1920 train_time:12184ms step_avg:34.61ms
step:353/1920 train_time:12218ms step_avg:34.61ms
step:354/1920 train_time:12252ms step_avg:34.61ms
step:355/1920 train_time:12287ms step_avg:34.61ms
step:356/1920 train_time:12321ms step_avg:34.61ms
step:357/1920 train_time:12356ms step_avg:34.61ms
step:358/1920 train_time:12390ms step_avg:34.61ms
step:359/1920 train_time:12424ms step_avg:34.61ms
step:360/1920 train_time:12459ms step_avg:34.61ms
step:361/1920 train_time:12494ms step_avg:34.61ms
step:362/1920 train_time:12528ms step_avg:34.61ms
step:363/1920 train_time:12562ms step_avg:34.61ms
step:364/1920 train_time:12597ms step_avg:34.61ms
step:365/1920 train_time:12632ms step_avg:34.61ms
step:366/1920 train_time:12666ms step_avg:34.61ms
step:367/1920 train_time:12700ms step_avg:34.61ms
step:368/1920 train_time:12734ms step_avg:34.60ms
step:369/1920 train_time:12769ms step_avg:34.60ms
step:370/1920 train_time:12803ms step_avg:34.60ms
step:371/1920 train_time:12837ms step_avg:34.60ms
step:372/1920 train_time:12872ms step_avg:34.60ms
step:373/1920 train_time:12906ms step_avg:34.60ms
step:374/1920 train_time:12940ms step_avg:34.60ms
step:375/1920 train_time:12975ms step_avg:34.60ms
step:376/1920 train_time:13009ms step_avg:34.60ms
step:377/1920 train_time:13043ms step_avg:34.60ms
step:378/1920 train_time:13077ms step_avg:34.60ms
step:379/1920 train_time:13112ms step_avg:34.60ms
step:380/1920 train_time:13146ms step_avg:34.60ms
step:381/1920 train_time:13181ms step_avg:34.60ms
step:382/1920 train_time:13215ms step_avg:34.59ms
step:383/1920 train_time:13250ms step_avg:34.59ms
step:384/1920 train_time:13284ms step_avg:34.59ms
step:385/1920 train_time:13318ms step_avg:34.59ms
step:386/1920 train_time:13352ms step_avg:34.59ms
step:387/1920 train_time:13387ms step_avg:34.59ms
step:388/1920 train_time:13421ms step_avg:34.59ms
step:389/1920 train_time:13456ms step_avg:34.59ms
step:390/1920 train_time:13490ms step_avg:34.59ms
step:391/1920 train_time:13524ms step_avg:34.59ms
step:392/1920 train_time:13558ms step_avg:34.59ms
step:393/1920 train_time:13593ms step_avg:34.59ms
step:394/1920 train_time:13627ms step_avg:34.59ms
step:395/1920 train_time:13661ms step_avg:34.59ms
step:396/1920 train_time:13696ms step_avg:34.58ms
step:397/1920 train_time:13730ms step_avg:34.59ms
step:398/1920 train_time:13765ms step_avg:34.58ms
step:399/1920 train_time:13799ms step_avg:34.58ms
step:400/1920 train_time:13833ms step_avg:34.58ms
step:401/1920 train_time:13868ms step_avg:34.58ms
step:402/1920 train_time:13902ms step_avg:34.58ms
step:403/1920 train_time:13936ms step_avg:34.58ms
step:404/1920 train_time:13971ms step_avg:34.58ms
step:405/1920 train_time:14005ms step_avg:34.58ms
step:406/1920 train_time:14039ms step_avg:34.58ms
step:407/1920 train_time:14074ms step_avg:34.58ms
step:408/1920 train_time:14108ms step_avg:34.58ms
step:409/1920 train_time:14143ms step_avg:34.58ms
step:410/1920 train_time:14177ms step_avg:34.58ms
step:411/1920 train_time:14212ms step_avg:34.58ms
step:412/1920 train_time:14246ms step_avg:34.58ms
step:413/1920 train_time:14281ms step_avg:34.58ms
step:414/1920 train_time:14315ms step_avg:34.58ms
step:415/1920 train_time:14349ms step_avg:34.58ms
step:416/1920 train_time:14383ms step_avg:34.58ms
step:417/1920 train_time:14418ms step_avg:34.58ms
step:418/1920 train_time:14452ms step_avg:34.57ms
step:419/1920 train_time:14486ms step_avg:34.57ms
step:420/1920 train_time:14521ms step_avg:34.57ms
step:421/1920 train_time:14555ms step_avg:34.57ms
step:422/1920 train_time:14589ms step_avg:34.57ms
step:423/1920 train_time:14624ms step_avg:34.57ms
step:424/1920 train_time:14659ms step_avg:34.57ms
step:425/1920 train_time:14694ms step_avg:34.57ms
step:426/1920 train_time:14728ms step_avg:34.57ms
step:427/1920 train_time:14762ms step_avg:34.57ms
step:428/1920 train_time:14796ms step_avg:34.57ms
step:429/1920 train_time:14831ms step_avg:34.57ms
step:430/1920 train_time:14865ms step_avg:34.57ms
step:431/1920 train_time:14899ms step_avg:34.57ms
step:432/1920 train_time:14934ms step_avg:34.57ms
step:433/1920 train_time:14968ms step_avg:34.57ms
step:434/1920 train_time:15002ms step_avg:34.57ms
step:435/1920 train_time:15037ms step_avg:34.57ms
step:436/1920 train_time:15071ms step_avg:34.57ms
step:437/1920 train_time:15106ms step_avg:34.57ms
step:438/1920 train_time:15140ms step_avg:34.57ms
step:439/1920 train_time:15175ms step_avg:34.57ms
step:440/1920 train_time:15209ms step_avg:34.57ms
step:441/1920 train_time:15243ms step_avg:34.57ms
step:442/1920 train_time:15278ms step_avg:34.56ms
step:443/1920 train_time:15313ms step_avg:34.57ms
step:444/1920 train_time:15347ms step_avg:34.57ms
step:445/1920 train_time:15381ms step_avg:34.56ms
step:446/1920 train_time:15416ms step_avg:34.56ms
step:447/1920 train_time:15450ms step_avg:34.56ms
step:448/1920 train_time:15484ms step_avg:34.56ms
step:449/1920 train_time:15519ms step_avg:34.56ms
step:450/1920 train_time:15553ms step_avg:34.56ms
step:451/1920 train_time:15587ms step_avg:34.56ms
step:452/1920 train_time:15621ms step_avg:34.56ms
step:453/1920 train_time:15656ms step_avg:34.56ms
step:454/1920 train_time:15690ms step_avg:34.56ms
step:455/1920 train_time:15725ms step_avg:34.56ms
step:456/1920 train_time:15759ms step_avg:34.56ms
step:457/1920 train_time:15794ms step_avg:34.56ms
step:458/1920 train_time:15828ms step_avg:34.56ms
step:459/1920 train_time:15863ms step_avg:34.56ms
step:460/1920 train_time:15897ms step_avg:34.56ms
step:461/1920 train_time:15932ms step_avg:34.56ms
step:462/1920 train_time:15967ms step_avg:34.56ms
step:463/1920 train_time:16001ms step_avg:34.56ms
step:464/1920 train_time:16035ms step_avg:34.56ms
step:465/1920 train_time:16070ms step_avg:34.56ms
step:466/1920 train_time:16105ms step_avg:34.56ms
step:467/1920 train_time:16139ms step_avg:34.56ms
step:468/1920 train_time:16173ms step_avg:34.56ms
step:469/1920 train_time:16208ms step_avg:34.56ms
step:470/1920 train_time:16242ms step_avg:34.56ms
step:471/1920 train_time:16276ms step_avg:34.56ms
step:472/1920 train_time:16310ms step_avg:34.56ms
step:473/1920 train_time:16345ms step_avg:34.56ms
step:474/1920 train_time:16379ms step_avg:34.55ms
step:475/1920 train_time:16413ms step_avg:34.55ms
step:476/1920 train_time:16448ms step_avg:34.55ms
step:477/1920 train_time:16482ms step_avg:34.55ms
step:478/1920 train_time:16516ms step_avg:34.55ms
step:479/1920 train_time:16551ms step_avg:34.55ms
step:480/1920 train_time:16585ms step_avg:34.55ms
step:481/1920 train_time:16619ms step_avg:34.55ms
step:482/1920 train_time:16653ms step_avg:34.55ms
step:483/1920 train_time:16688ms step_avg:34.55ms
step:484/1920 train_time:16722ms step_avg:34.55ms
step:485/1920 train_time:16756ms step_avg:34.55ms
step:486/1920 train_time:16791ms step_avg:34.55ms
step:487/1920 train_time:16825ms step_avg:34.55ms
step:488/1920 train_time:16859ms step_avg:34.55ms
step:489/1920 train_time:16893ms step_avg:34.55ms
step:490/1920 train_time:16927ms step_avg:34.55ms
step:491/1920 train_time:16962ms step_avg:34.55ms
step:492/1920 train_time:16996ms step_avg:34.55ms
step:493/1920 train_time:17031ms step_avg:34.55ms
step:494/1920 train_time:17066ms step_avg:34.55ms
step:495/1920 train_time:17100ms step_avg:34.55ms
step:496/1920 train_time:17134ms step_avg:34.54ms
step:497/1920 train_time:17168ms step_avg:34.54ms
step:498/1920 train_time:17202ms step_avg:34.54ms
step:499/1920 train_time:17237ms step_avg:34.54ms
step:500/1920 train_time:17271ms step_avg:34.54ms
step:500/1920 val_loss:4.2996 train_time:17308ms step_avg:34.62ms
step:501/1920 train_time:17326ms step_avg:34.58ms
step:502/1920 train_time:17343ms step_avg:34.55ms
step:503/1920 train_time:17377ms step_avg:34.55ms
step:504/1920 train_time:17412ms step_avg:34.55ms
step:505/1920 train_time:17448ms step_avg:34.55ms
step:506/1920 train_time:17483ms step_avg:34.55ms
step:507/1920 train_time:17519ms step_avg:34.55ms
step:508/1920 train_time:17553ms step_avg:34.55ms
step:509/1920 train_time:17588ms step_avg:34.55ms
step:510/1920 train_time:17623ms step_avg:34.55ms
step:511/1920 train_time:17657ms step_avg:34.55ms
step:512/1920 train_time:17691ms step_avg:34.55ms
step:513/1920 train_time:17726ms step_avg:34.55ms
step:514/1920 train_time:17760ms step_avg:34.55ms
step:515/1920 train_time:17794ms step_avg:34.55ms
step:516/1920 train_time:17828ms step_avg:34.55ms
step:517/1920 train_time:17862ms step_avg:34.55ms
step:518/1920 train_time:17896ms step_avg:34.55ms
step:519/1920 train_time:17930ms step_avg:34.55ms
step:520/1920 train_time:17964ms step_avg:34.55ms
step:521/1920 train_time:17998ms step_avg:34.55ms
step:522/1920 train_time:18032ms step_avg:34.54ms
step:523/1920 train_time:18066ms step_avg:34.54ms
step:524/1920 train_time:18101ms step_avg:34.54ms
step:525/1920 train_time:18135ms step_avg:34.54ms
step:526/1920 train_time:18169ms step_avg:34.54ms
step:527/1920 train_time:18203ms step_avg:34.54ms
step:528/1920 train_time:18237ms step_avg:34.54ms
step:529/1920 train_time:18271ms step_avg:34.54ms
step:530/1920 train_time:18305ms step_avg:34.54ms
step:531/1920 train_time:18340ms step_avg:34.54ms
step:532/1920 train_time:18374ms step_avg:34.54ms
step:533/1920 train_time:18409ms step_avg:34.54ms
step:534/1920 train_time:18443ms step_avg:34.54ms
step:535/1920 train_time:18478ms step_avg:34.54ms
step:536/1920 train_time:18513ms step_avg:34.54ms
step:537/1920 train_time:18548ms step_avg:34.54ms
step:538/1920 train_time:18582ms step_avg:34.54ms
step:539/1920 train_time:18616ms step_avg:34.54ms
step:540/1920 train_time:18650ms step_avg:34.54ms
step:541/1920 train_time:18685ms step_avg:34.54ms
step:542/1920 train_time:18719ms step_avg:34.54ms
step:543/1920 train_time:18754ms step_avg:34.54ms
step:544/1920 train_time:18788ms step_avg:34.54ms
step:545/1920 train_time:18822ms step_avg:34.54ms
step:546/1920 train_time:18856ms step_avg:34.54ms
step:547/1920 train_time:18891ms step_avg:34.53ms
step:548/1920 train_time:18925ms step_avg:34.53ms
step:549/1920 train_time:18959ms step_avg:34.53ms
step:550/1920 train_time:18993ms step_avg:34.53ms
step:551/1920 train_time:19027ms step_avg:34.53ms
step:552/1920 train_time:19062ms step_avg:34.53ms
step:553/1920 train_time:19096ms step_avg:34.53ms
step:554/1920 train_time:19130ms step_avg:34.53ms
step:555/1920 train_time:19164ms step_avg:34.53ms
step:556/1920 train_time:19198ms step_avg:34.53ms
step:557/1920 train_time:19232ms step_avg:34.53ms
step:558/1920 train_time:19266ms step_avg:34.53ms
step:559/1920 train_time:19301ms step_avg:34.53ms
step:560/1920 train_time:19335ms step_avg:34.53ms
step:561/1920 train_time:19369ms step_avg:34.53ms
step:562/1920 train_time:19404ms step_avg:34.53ms
step:563/1920 train_time:19438ms step_avg:34.53ms
step:564/1920 train_time:19473ms step_avg:34.53ms
step:565/1920 train_time:19507ms step_avg:34.53ms
step:566/1920 train_time:19541ms step_avg:34.52ms
step:567/1920 train_time:19576ms step_avg:34.52ms
step:568/1920 train_time:19610ms step_avg:34.52ms
step:569/1920 train_time:19645ms step_avg:34.53ms
step:570/1920 train_time:19679ms step_avg:34.53ms
step:571/1920 train_time:19714ms step_avg:34.53ms
step:572/1920 train_time:19748ms step_avg:34.52ms
step:573/1920 train_time:19783ms step_avg:34.53ms
step:574/1920 train_time:19817ms step_avg:34.52ms
step:575/1920 train_time:19852ms step_avg:34.52ms
step:576/1920 train_time:19886ms step_avg:34.52ms
step:577/1920 train_time:19920ms step_avg:34.52ms
step:578/1920 train_time:19954ms step_avg:34.52ms
step:579/1920 train_time:19989ms step_avg:34.52ms
step:580/1920 train_time:20023ms step_avg:34.52ms
step:581/1920 train_time:20057ms step_avg:34.52ms
step:582/1920 train_time:20092ms step_avg:34.52ms
step:583/1920 train_time:20126ms step_avg:34.52ms
step:584/1920 train_time:20160ms step_avg:34.52ms
step:585/1920 train_time:20194ms step_avg:34.52ms
step:586/1920 train_time:20228ms step_avg:34.52ms
step:587/1920 train_time:20263ms step_avg:34.52ms
step:588/1920 train_time:20297ms step_avg:34.52ms
step:589/1920 train_time:20331ms step_avg:34.52ms
step:590/1920 train_time:20365ms step_avg:34.52ms
step:591/1920 train_time:20400ms step_avg:34.52ms
step:592/1920 train_time:20434ms step_avg:34.52ms
step:593/1920 train_time:20468ms step_avg:34.52ms
step:594/1920 train_time:20502ms step_avg:34.52ms
step:595/1920 train_time:20537ms step_avg:34.52ms
step:596/1920 train_time:20571ms step_avg:34.52ms
step:597/1920 train_time:20606ms step_avg:34.52ms
step:598/1920 train_time:20640ms step_avg:34.51ms
step:599/1920 train_time:20675ms step_avg:34.52ms
step:600/1920 train_time:20709ms step_avg:34.51ms
step:601/1920 train_time:20744ms step_avg:34.52ms
step:602/1920 train_time:20778ms step_avg:34.51ms
step:603/1920 train_time:20812ms step_avg:34.51ms
step:604/1920 train_time:20846ms step_avg:34.51ms
step:605/1920 train_time:20881ms step_avg:34.51ms
step:606/1920 train_time:20915ms step_avg:34.51ms
step:607/1920 train_time:20950ms step_avg:34.51ms
step:608/1920 train_time:20984ms step_avg:34.51ms
step:609/1920 train_time:21019ms step_avg:34.51ms
step:610/1920 train_time:21053ms step_avg:34.51ms
step:611/1920 train_time:21087ms step_avg:34.51ms
step:612/1920 train_time:21121ms step_avg:34.51ms
step:613/1920 train_time:21156ms step_avg:34.51ms
step:614/1920 train_time:21190ms step_avg:34.51ms
step:615/1920 train_time:21224ms step_avg:34.51ms
step:616/1920 train_time:21258ms step_avg:34.51ms
step:617/1920 train_time:21293ms step_avg:34.51ms
step:618/1920 train_time:21327ms step_avg:34.51ms
step:619/1920 train_time:21361ms step_avg:34.51ms
step:620/1920 train_time:21395ms step_avg:34.51ms
step:621/1920 train_time:21430ms step_avg:34.51ms
step:622/1920 train_time:21464ms step_avg:34.51ms
step:623/1920 train_time:21498ms step_avg:34.51ms
step:624/1920 train_time:21532ms step_avg:34.51ms
step:625/1920 train_time:21566ms step_avg:34.51ms
step:626/1920 train_time:21601ms step_avg:34.51ms
step:627/1920 train_time:21635ms step_avg:34.51ms
step:628/1920 train_time:21670ms step_avg:34.51ms
step:629/1920 train_time:21731ms step_avg:34.55ms
step:630/1920 train_time:21793ms step_avg:34.59ms
step:631/1920 train_time:21856ms step_avg:34.64ms
step:632/1920 train_time:21918ms step_avg:34.68ms
step:633/1920 train_time:21980ms step_avg:34.72ms
step:634/1920 train_time:22042ms step_avg:34.77ms
step:635/1920 train_time:22104ms step_avg:34.81ms
step:636/1920 train_time:22166ms step_avg:34.85ms
step:637/1920 train_time:22228ms step_avg:34.90ms
step:638/1920 train_time:22290ms step_avg:34.94ms
step:639/1920 train_time:22353ms step_avg:34.98ms
step:640/1920 train_time:22414ms step_avg:35.02ms
step:641/1920 train_time:22477ms step_avg:35.07ms
step:642/1920 train_time:22538ms step_avg:35.11ms
step:643/1920 train_time:22601ms step_avg:35.15ms
step:644/1920 train_time:22662ms step_avg:35.19ms
step:645/1920 train_time:22724ms step_avg:35.23ms
step:646/1920 train_time:22786ms step_avg:35.27ms
step:647/1920 train_time:22848ms step_avg:35.31ms
step:648/1920 train_time:22911ms step_avg:35.36ms
step:649/1920 train_time:22974ms step_avg:35.40ms
step:650/1920 train_time:23035ms step_avg:35.44ms
step:651/1920 train_time:23098ms step_avg:35.48ms
step:652/1920 train_time:23160ms step_avg:35.52ms
step:653/1920 train_time:23223ms step_avg:35.56ms
step:654/1920 train_time:23284ms step_avg:35.60ms
step:655/1920 train_time:23347ms step_avg:35.64ms
step:656/1920 train_time:23408ms step_avg:35.68ms
step:657/1920 train_time:23471ms step_avg:35.72ms
step:658/1920 train_time:23533ms step_avg:35.76ms
step:659/1920 train_time:23596ms step_avg:35.81ms
step:660/1920 train_time:23657ms step_avg:35.84ms
step:661/1920 train_time:23720ms step_avg:35.88ms
step:662/1920 train_time:23781ms step_avg:35.92ms
step:663/1920 train_time:23844ms step_avg:35.96ms
step:664/1920 train_time:23906ms step_avg:36.00ms
step:665/1920 train_time:23968ms step_avg:36.04ms
step:666/1920 train_time:24030ms step_avg:36.08ms
step:667/1920 train_time:24092ms step_avg:36.12ms
step:668/1920 train_time:24155ms step_avg:36.16ms
step:669/1920 train_time:24217ms step_avg:36.20ms
step:670/1920 train_time:24279ms step_avg:36.24ms
step:671/1920 train_time:24342ms step_avg:36.28ms
step:672/1920 train_time:24404ms step_avg:36.32ms
step:673/1920 train_time:24466ms step_avg:36.35ms
step:674/1920 train_time:24528ms step_avg:36.39ms
step:675/1920 train_time:24591ms step_avg:36.43ms
step:676/1920 train_time:24653ms step_avg:36.47ms
step:677/1920 train_time:24716ms step_avg:36.51ms
step:678/1920 train_time:24778ms step_avg:36.55ms
step:679/1920 train_time:24841ms step_avg:36.58ms
step:680/1920 train_time:24902ms step_avg:36.62ms
step:681/1920 train_time:24965ms step_avg:36.66ms
step:682/1920 train_time:25026ms step_avg:36.70ms
step:683/1920 train_time:25089ms step_avg:36.73ms
step:684/1920 train_time:25151ms step_avg:36.77ms
step:685/1920 train_time:25214ms step_avg:36.81ms
step:686/1920 train_time:25277ms step_avg:36.85ms
step:687/1920 train_time:25340ms step_avg:36.88ms
step:688/1920 train_time:25402ms step_avg:36.92ms
step:689/1920 train_time:25464ms step_avg:36.96ms
step:690/1920 train_time:25525ms step_avg:36.99ms
step:691/1920 train_time:25587ms step_avg:37.03ms
step:692/1920 train_time:25649ms step_avg:37.07ms
step:693/1920 train_time:25713ms step_avg:37.10ms
step:694/1920 train_time:25775ms step_avg:37.14ms
step:695/1920 train_time:25838ms step_avg:37.18ms
step:696/1920 train_time:25899ms step_avg:37.21ms
step:697/1920 train_time:25962ms step_avg:37.25ms
step:698/1920 train_time:26024ms step_avg:37.28ms
step:699/1920 train_time:26086ms step_avg:37.32ms
step:700/1920 train_time:26148ms step_avg:37.35ms
step:701/1920 train_time:26210ms step_avg:37.39ms
step:702/1920 train_time:26273ms step_avg:37.43ms
step:703/1920 train_time:26336ms step_avg:37.46ms
step:704/1920 train_time:26398ms step_avg:37.50ms
step:705/1920 train_time:26460ms step_avg:37.53ms
step:706/1920 train_time:26522ms step_avg:37.57ms
step:707/1920 train_time:26584ms step_avg:37.60ms
step:708/1920 train_time:26646ms step_avg:37.64ms
step:709/1920 train_time:26708ms step_avg:37.67ms
step:710/1920 train_time:26770ms step_avg:37.70ms
step:711/1920 train_time:26834ms step_avg:37.74ms
step:712/1920 train_time:26896ms step_avg:37.78ms
step:713/1920 train_time:26959ms step_avg:37.81ms
step:714/1920 train_time:27021ms step_avg:37.84ms
step:715/1920 train_time:27083ms step_avg:37.88ms
step:716/1920 train_time:27144ms step_avg:37.91ms
step:717/1920 train_time:27206ms step_avg:37.94ms
step:718/1920 train_time:27268ms step_avg:37.98ms
step:719/1920 train_time:27331ms step_avg:38.01ms
step:720/1920 train_time:27393ms step_avg:38.05ms
step:721/1920 train_time:27456ms step_avg:38.08ms
step:722/1920 train_time:27518ms step_avg:38.11ms
step:723/1920 train_time:27581ms step_avg:38.15ms
step:724/1920 train_time:27643ms step_avg:38.18ms
step:725/1920 train_time:27705ms step_avg:38.21ms
step:726/1920 train_time:27767ms step_avg:38.25ms
step:727/1920 train_time:27830ms step_avg:38.28ms
step:728/1920 train_time:27892ms step_avg:38.31ms
step:729/1920 train_time:27956ms step_avg:38.35ms
step:730/1920 train_time:28018ms step_avg:38.38ms
step:731/1920 train_time:28081ms step_avg:38.41ms
step:732/1920 train_time:28142ms step_avg:38.45ms
step:733/1920 train_time:28205ms step_avg:38.48ms
step:734/1920 train_time:28266ms step_avg:38.51ms
step:735/1920 train_time:28329ms step_avg:38.54ms
step:736/1920 train_time:28390ms step_avg:38.57ms
step:737/1920 train_time:28454ms step_avg:38.61ms
step:738/1920 train_time:28516ms step_avg:38.64ms
step:739/1920 train_time:28579ms step_avg:38.67ms
step:740/1920 train_time:28641ms step_avg:38.70ms
step:741/1920 train_time:28703ms step_avg:38.74ms
step:742/1920 train_time:28765ms step_avg:38.77ms
step:743/1920 train_time:28827ms step_avg:38.80ms
step:744/1920 train_time:28889ms step_avg:38.83ms
step:745/1920 train_time:28952ms step_avg:38.86ms
step:746/1920 train_time:29014ms step_avg:38.89ms
step:747/1920 train_time:29077ms step_avg:38.92ms
step:748/1920 train_time:29138ms step_avg:38.96ms
step:749/1920 train_time:29201ms step_avg:38.99ms
step:750/1920 train_time:29263ms step_avg:39.02ms
step:750/1920 val_loss:4.0443 train_time:29328ms step_avg:39.10ms
step:751/1920 train_time:29348ms step_avg:39.08ms
step:752/1920 train_time:29388ms step_avg:39.08ms
step:753/1920 train_time:29457ms step_avg:39.12ms
step:754/1920 train_time:29523ms step_avg:39.15ms
step:755/1920 train_time:29587ms step_avg:39.19ms
step:756/1920 train_time:29649ms step_avg:39.22ms
step:757/1920 train_time:29712ms step_avg:39.25ms
step:758/1920 train_time:29773ms step_avg:39.28ms
step:759/1920 train_time:29834ms step_avg:39.31ms
step:760/1920 train_time:29895ms step_avg:39.34ms
step:761/1920 train_time:29957ms step_avg:39.37ms
step:762/1920 train_time:30018ms step_avg:39.39ms
step:763/1920 train_time:30080ms step_avg:39.42ms
step:764/1920 train_time:30140ms step_avg:39.45ms
step:765/1920 train_time:30202ms step_avg:39.48ms
step:766/1920 train_time:30265ms step_avg:39.51ms
step:767/1920 train_time:30328ms step_avg:39.54ms
step:768/1920 train_time:30391ms step_avg:39.57ms
step:769/1920 train_time:30455ms step_avg:39.60ms
step:770/1920 train_time:30518ms step_avg:39.63ms
step:771/1920 train_time:30581ms step_avg:39.66ms
step:772/1920 train_time:30643ms step_avg:39.69ms
step:773/1920 train_time:30706ms step_avg:39.72ms
step:774/1920 train_time:30769ms step_avg:39.75ms
step:775/1920 train_time:30831ms step_avg:39.78ms
step:776/1920 train_time:30892ms step_avg:39.81ms
step:777/1920 train_time:30954ms step_avg:39.84ms
step:778/1920 train_time:31015ms step_avg:39.87ms
step:779/1920 train_time:31077ms step_avg:39.89ms
step:780/1920 train_time:31138ms step_avg:39.92ms
step:781/1920 train_time:31200ms step_avg:39.95ms
step:782/1920 train_time:31262ms step_avg:39.98ms
step:783/1920 train_time:31324ms step_avg:40.01ms
step:784/1920 train_time:31387ms step_avg:40.03ms
step:785/1920 train_time:31450ms step_avg:40.06ms
step:786/1920 train_time:31512ms step_avg:40.09ms
step:787/1920 train_time:31576ms step_avg:40.12ms
step:788/1920 train_time:31638ms step_avg:40.15ms
step:789/1920 train_time:31701ms step_avg:40.18ms
step:790/1920 train_time:31762ms step_avg:40.21ms
step:791/1920 train_time:31825ms step_avg:40.23ms
step:792/1920 train_time:31887ms step_avg:40.26ms
step:793/1920 train_time:31950ms step_avg:40.29ms
step:794/1920 train_time:32011ms step_avg:40.32ms
step:795/1920 train_time:32073ms step_avg:40.34ms
step:796/1920 train_time:32134ms step_avg:40.37ms
step:797/1920 train_time:32197ms step_avg:40.40ms
step:798/1920 train_time:32258ms step_avg:40.42ms
step:799/1920 train_time:32320ms step_avg:40.45ms
step:800/1920 train_time:32382ms step_avg:40.48ms
step:801/1920 train_time:32445ms step_avg:40.51ms
step:802/1920 train_time:32507ms step_avg:40.53ms
step:803/1920 train_time:32571ms step_avg:40.56ms
step:804/1920 train_time:32634ms step_avg:40.59ms
step:805/1920 train_time:32697ms step_avg:40.62ms
step:806/1920 train_time:32759ms step_avg:40.64ms
step:807/1920 train_time:32821ms step_avg:40.67ms
step:808/1920 train_time:32882ms step_avg:40.70ms
step:809/1920 train_time:32945ms step_avg:40.72ms
step:810/1920 train_time:33007ms step_avg:40.75ms
step:811/1920 train_time:33070ms step_avg:40.78ms
step:812/1920 train_time:33131ms step_avg:40.80ms
step:813/1920 train_time:33194ms step_avg:40.83ms
step:814/1920 train_time:33256ms step_avg:40.85ms
step:815/1920 train_time:33318ms step_avg:40.88ms
step:816/1920 train_time:33380ms step_avg:40.91ms
step:817/1920 train_time:33442ms step_avg:40.93ms
step:818/1920 train_time:33504ms step_avg:40.96ms
step:819/1920 train_time:33568ms step_avg:40.99ms
step:820/1920 train_time:33630ms step_avg:41.01ms
step:821/1920 train_time:33693ms step_avg:41.04ms
step:822/1920 train_time:33755ms step_avg:41.06ms
step:823/1920 train_time:33817ms step_avg:41.09ms
step:824/1920 train_time:33879ms step_avg:41.12ms
step:825/1920 train_time:33942ms step_avg:41.14ms
step:826/1920 train_time:34003ms step_avg:41.17ms
step:827/1920 train_time:34066ms step_avg:41.19ms
step:828/1920 train_time:34129ms step_avg:41.22ms
step:829/1920 train_time:34191ms step_avg:41.24ms
step:830/1920 train_time:34253ms step_avg:41.27ms
step:831/1920 train_time:34315ms step_avg:41.29ms
step:832/1920 train_time:34377ms step_avg:41.32ms
step:833/1920 train_time:34439ms step_avg:41.34ms
step:834/1920 train_time:34502ms step_avg:41.37ms
step:835/1920 train_time:34565ms step_avg:41.39ms
step:836/1920 train_time:34627ms step_avg:41.42ms
step:837/1920 train_time:34689ms step_avg:41.45ms
step:838/1920 train_time:34751ms step_avg:41.47ms
step:839/1920 train_time:34814ms step_avg:41.49ms
step:840/1920 train_time:34876ms step_avg:41.52ms
step:841/1920 train_time:34939ms step_avg:41.54ms
step:842/1920 train_time:35001ms step_avg:41.57ms
step:843/1920 train_time:35063ms step_avg:41.59ms
step:844/1920 train_time:35125ms step_avg:41.62ms
step:845/1920 train_time:35188ms step_avg:41.64ms
step:846/1920 train_time:35250ms step_avg:41.67ms
step:847/1920 train_time:35312ms step_avg:41.69ms
step:848/1920 train_time:35374ms step_avg:41.71ms
step:849/1920 train_time:35437ms step_avg:41.74ms
step:850/1920 train_time:35499ms step_avg:41.76ms
step:851/1920 train_time:35562ms step_avg:41.79ms
step:852/1920 train_time:35623ms step_avg:41.81ms
step:853/1920 train_time:35686ms step_avg:41.84ms
step:854/1920 train_time:35749ms step_avg:41.86ms
step:855/1920 train_time:35812ms step_avg:41.89ms
step:856/1920 train_time:35874ms step_avg:41.91ms
step:857/1920 train_time:35936ms step_avg:41.93ms
step:858/1920 train_time:35999ms step_avg:41.96ms
step:859/1920 train_time:36061ms step_avg:41.98ms
step:860/1920 train_time:36123ms step_avg:42.00ms
step:861/1920 train_time:36185ms step_avg:42.03ms
step:862/1920 train_time:36247ms step_avg:42.05ms
step:863/1920 train_time:36310ms step_avg:42.07ms
step:864/1920 train_time:36373ms step_avg:42.10ms
step:865/1920 train_time:36435ms step_avg:42.12ms
step:866/1920 train_time:36497ms step_avg:42.14ms
step:867/1920 train_time:36559ms step_avg:42.17ms
step:868/1920 train_time:36621ms step_avg:42.19ms
step:869/1920 train_time:36685ms step_avg:42.21ms
step:870/1920 train_time:36748ms step_avg:42.24ms
step:871/1920 train_time:36812ms step_avg:42.26ms
step:872/1920 train_time:36873ms step_avg:42.29ms
step:873/1920 train_time:36936ms step_avg:42.31ms
step:874/1920 train_time:36998ms step_avg:42.33ms
step:875/1920 train_time:37060ms step_avg:42.35ms
step:876/1920 train_time:37122ms step_avg:42.38ms
step:877/1920 train_time:37185ms step_avg:42.40ms
step:878/1920 train_time:37247ms step_avg:42.42ms
step:879/1920 train_time:37310ms step_avg:42.45ms
step:880/1920 train_time:37372ms step_avg:42.47ms
step:881/1920 train_time:37435ms step_avg:42.49ms
step:882/1920 train_time:37497ms step_avg:42.51ms
step:883/1920 train_time:37558ms step_avg:42.53ms
step:884/1920 train_time:37621ms step_avg:42.56ms
step:885/1920 train_time:37684ms step_avg:42.58ms
step:886/1920 train_time:37745ms step_avg:42.60ms
step:887/1920 train_time:37809ms step_avg:42.63ms
step:888/1920 train_time:37870ms step_avg:42.65ms
step:889/1920 train_time:37933ms step_avg:42.67ms
step:890/1920 train_time:37995ms step_avg:42.69ms
step:891/1920 train_time:38057ms step_avg:42.71ms
step:892/1920 train_time:38119ms step_avg:42.73ms
step:893/1920 train_time:38182ms step_avg:42.76ms
step:894/1920 train_time:38244ms step_avg:42.78ms
step:895/1920 train_time:38307ms step_avg:42.80ms
step:896/1920 train_time:38369ms step_avg:42.82ms
step:897/1920 train_time:38432ms step_avg:42.84ms
step:898/1920 train_time:38494ms step_avg:42.87ms
step:899/1920 train_time:38557ms step_avg:42.89ms
step:900/1920 train_time:38619ms step_avg:42.91ms
step:901/1920 train_time:38682ms step_avg:42.93ms
step:902/1920 train_time:38744ms step_avg:42.95ms
step:903/1920 train_time:38807ms step_avg:42.98ms
step:904/1920 train_time:38869ms step_avg:43.00ms
step:905/1920 train_time:38932ms step_avg:43.02ms
step:906/1920 train_time:38993ms step_avg:43.04ms
step:907/1920 train_time:39056ms step_avg:43.06ms
step:908/1920 train_time:39118ms step_avg:43.08ms
step:909/1920 train_time:39181ms step_avg:43.10ms
step:910/1920 train_time:39242ms step_avg:43.12ms
step:911/1920 train_time:39305ms step_avg:43.15ms
step:912/1920 train_time:39367ms step_avg:43.17ms
step:913/1920 train_time:39430ms step_avg:43.19ms
step:914/1920 train_time:39492ms step_avg:43.21ms
step:915/1920 train_time:39555ms step_avg:43.23ms
step:916/1920 train_time:39617ms step_avg:43.25ms
step:917/1920 train_time:39679ms step_avg:43.27ms
step:918/1920 train_time:39741ms step_avg:43.29ms
step:919/1920 train_time:39804ms step_avg:43.31ms
step:920/1920 train_time:39866ms step_avg:43.33ms
step:921/1920 train_time:39930ms step_avg:43.35ms
step:922/1920 train_time:39992ms step_avg:43.38ms
step:923/1920 train_time:40055ms step_avg:43.40ms
step:924/1920 train_time:40116ms step_avg:43.42ms
step:925/1920 train_time:40179ms step_avg:43.44ms
step:926/1920 train_time:40241ms step_avg:43.46ms
step:927/1920 train_time:40304ms step_avg:43.48ms
step:928/1920 train_time:40366ms step_avg:43.50ms
step:929/1920 train_time:40429ms step_avg:43.52ms
step:930/1920 train_time:40491ms step_avg:43.54ms
step:931/1920 train_time:40554ms step_avg:43.56ms
step:932/1920 train_time:40616ms step_avg:43.58ms
step:933/1920 train_time:40678ms step_avg:43.60ms
step:934/1920 train_time:40739ms step_avg:43.62ms
step:935/1920 train_time:40802ms step_avg:43.64ms
step:936/1920 train_time:40863ms step_avg:43.66ms
step:937/1920 train_time:40927ms step_avg:43.68ms
step:938/1920 train_time:40989ms step_avg:43.70ms
step:939/1920 train_time:41052ms step_avg:43.72ms
step:940/1920 train_time:41113ms step_avg:43.74ms
step:941/1920 train_time:41177ms step_avg:43.76ms
step:942/1920 train_time:41239ms step_avg:43.78ms
step:943/1920 train_time:41301ms step_avg:43.80ms
step:944/1920 train_time:41363ms step_avg:43.82ms
step:945/1920 train_time:41426ms step_avg:43.84ms
step:946/1920 train_time:41488ms step_avg:43.86ms
step:947/1920 train_time:41552ms step_avg:43.88ms
step:948/1920 train_time:41614ms step_avg:43.90ms
step:949/1920 train_time:41676ms step_avg:43.92ms
step:950/1920 train_time:41738ms step_avg:43.93ms
step:951/1920 train_time:41801ms step_avg:43.95ms
step:952/1920 train_time:41862ms step_avg:43.97ms
step:953/1920 train_time:41925ms step_avg:43.99ms
step:954/1920 train_time:41987ms step_avg:44.01ms
step:955/1920 train_time:42051ms step_avg:44.03ms
step:956/1920 train_time:42112ms step_avg:44.05ms
step:957/1920 train_time:42175ms step_avg:44.07ms
step:958/1920 train_time:42237ms step_avg:44.09ms
step:959/1920 train_time:42299ms step_avg:44.11ms
step:960/1920 train_time:42361ms step_avg:44.13ms
step:961/1920 train_time:42424ms step_avg:44.15ms
step:962/1920 train_time:42486ms step_avg:44.16ms
step:963/1920 train_time:42550ms step_avg:44.18ms
step:964/1920 train_time:42612ms step_avg:44.20ms
step:965/1920 train_time:42675ms step_avg:44.22ms
step:966/1920 train_time:42736ms step_avg:44.24ms
step:967/1920 train_time:42798ms step_avg:44.26ms
step:968/1920 train_time:42861ms step_avg:44.28ms
step:969/1920 train_time:42923ms step_avg:44.30ms
step:970/1920 train_time:42986ms step_avg:44.32ms
step:971/1920 train_time:43050ms step_avg:44.34ms
step:972/1920 train_time:43112ms step_avg:44.35ms
step:973/1920 train_time:43175ms step_avg:44.37ms
step:974/1920 train_time:43236ms step_avg:44.39ms
step:975/1920 train_time:43299ms step_avg:44.41ms
step:976/1920 train_time:43360ms step_avg:44.43ms
step:977/1920 train_time:43423ms step_avg:44.45ms
step:978/1920 train_time:43485ms step_avg:44.46ms
step:979/1920 train_time:43548ms step_avg:44.48ms
step:980/1920 train_time:43610ms step_avg:44.50ms
step:981/1920 train_time:43673ms step_avg:44.52ms
step:982/1920 train_time:43735ms step_avg:44.54ms
step:983/1920 train_time:43798ms step_avg:44.56ms
step:984/1920 train_time:43859ms step_avg:44.57ms
step:985/1920 train_time:43921ms step_avg:44.59ms
step:986/1920 train_time:43983ms step_avg:44.61ms
step:987/1920 train_time:44046ms step_avg:44.63ms
step:988/1920 train_time:44108ms step_avg:44.64ms
step:989/1920 train_time:44172ms step_avg:44.66ms
step:990/1920 train_time:44234ms step_avg:44.68ms
step:991/1920 train_time:44297ms step_avg:44.70ms
step:992/1920 train_time:44359ms step_avg:44.72ms
step:993/1920 train_time:44421ms step_avg:44.73ms
step:994/1920 train_time:44483ms step_avg:44.75ms
step:995/1920 train_time:44545ms step_avg:44.77ms
step:996/1920 train_time:44608ms step_avg:44.79ms
step:997/1920 train_time:44671ms step_avg:44.81ms
step:998/1920 train_time:44733ms step_avg:44.82ms
step:999/1920 train_time:44795ms step_avg:44.84ms
step:1000/1920 train_time:44857ms step_avg:44.86ms
step:1000/1920 val_loss:3.7890 train_time:44922ms step_avg:44.92ms
step:1001/1920 train_time:44939ms step_avg:44.89ms
step:1002/1920 train_time:44985ms step_avg:44.90ms
step:1003/1920 train_time:45053ms step_avg:44.92ms
step:1004/1920 train_time:45117ms step_avg:44.94ms
step:1005/1920 train_time:45180ms step_avg:44.96ms
step:1006/1920 train_time:45242ms step_avg:44.97ms
step:1007/1920 train_time:45304ms step_avg:44.99ms
step:1008/1920 train_time:45366ms step_avg:45.01ms
step:1009/1920 train_time:45427ms step_avg:45.02ms
step:1010/1920 train_time:45488ms step_avg:45.04ms
step:1011/1920 train_time:45550ms step_avg:45.05ms
step:1012/1920 train_time:45611ms step_avg:45.07ms
step:1013/1920 train_time:45673ms step_avg:45.09ms
step:1014/1920 train_time:45735ms step_avg:45.10ms
step:1015/1920 train_time:45796ms step_avg:45.12ms
step:1016/1920 train_time:45858ms step_avg:45.14ms
step:1017/1920 train_time:45922ms step_avg:45.15ms
step:1018/1920 train_time:45985ms step_avg:45.17ms
step:1019/1920 train_time:46050ms step_avg:45.19ms
step:1020/1920 train_time:46112ms step_avg:45.21ms
step:1021/1920 train_time:46176ms step_avg:45.23ms
step:1022/1920 train_time:46239ms step_avg:45.24ms
step:1023/1920 train_time:46301ms step_avg:45.26ms
step:1024/1920 train_time:46364ms step_avg:45.28ms
step:1025/1920 train_time:46426ms step_avg:45.29ms
step:1026/1920 train_time:46488ms step_avg:45.31ms
step:1027/1920 train_time:46550ms step_avg:45.33ms
step:1028/1920 train_time:46611ms step_avg:45.34ms
step:1029/1920 train_time:46673ms step_avg:45.36ms
step:1030/1920 train_time:46734ms step_avg:45.37ms
step:1031/1920 train_time:46796ms step_avg:45.39ms
step:1032/1920 train_time:46858ms step_avg:45.40ms
step:1033/1920 train_time:46921ms step_avg:45.42ms
step:1034/1920 train_time:46984ms step_avg:45.44ms
step:1035/1920 train_time:47049ms step_avg:45.46ms
step:1036/1920 train_time:47111ms step_avg:45.47ms
step:1037/1920 train_time:47174ms step_avg:45.49ms
step:1038/1920 train_time:47237ms step_avg:45.51ms
step:1039/1920 train_time:47300ms step_avg:45.52ms
step:1040/1920 train_time:47362ms step_avg:45.54ms
step:1041/1920 train_time:47425ms step_avg:45.56ms
step:1042/1920 train_time:47487ms step_avg:45.57ms
step:1043/1920 train_time:47549ms step_avg:45.59ms
step:1044/1920 train_time:47611ms step_avg:45.60ms
step:1045/1920 train_time:47673ms step_avg:45.62ms
step:1046/1920 train_time:47734ms step_avg:45.63ms
step:1047/1920 train_time:47797ms step_avg:45.65ms
step:1048/1920 train_time:47858ms step_avg:45.67ms
step:1049/1920 train_time:47922ms step_avg:45.68ms
step:1050/1920 train_time:47985ms step_avg:45.70ms
step:1051/1920 train_time:48048ms step_avg:45.72ms
step:1052/1920 train_time:48110ms step_avg:45.73ms
step:1053/1920 train_time:48174ms step_avg:45.75ms
step:1054/1920 train_time:48236ms step_avg:45.77ms
step:1055/1920 train_time:48299ms step_avg:45.78ms
step:1056/1920 train_time:48361ms step_avg:45.80ms
step:1057/1920 train_time:48424ms step_avg:45.81ms
step:1058/1920 train_time:48485ms step_avg:45.83ms
step:1059/1920 train_time:48548ms step_avg:45.84ms
step:1060/1920 train_time:48610ms step_avg:45.86ms
step:1061/1920 train_time:48672ms step_avg:45.87ms
step:1062/1920 train_time:48734ms step_avg:45.89ms
step:1063/1920 train_time:48796ms step_avg:45.90ms
step:1064/1920 train_time:48858ms step_avg:45.92ms
step:1065/1920 train_time:48921ms step_avg:45.93ms
step:1066/1920 train_time:48983ms step_avg:45.95ms
step:1067/1920 train_time:49046ms step_avg:45.97ms
step:1068/1920 train_time:49108ms step_avg:45.98ms
step:1069/1920 train_time:49171ms step_avg:46.00ms
step:1070/1920 train_time:49234ms step_avg:46.01ms
step:1071/1920 train_time:49297ms step_avg:46.03ms
step:1072/1920 train_time:49359ms step_avg:46.04ms
step:1073/1920 train_time:49422ms step_avg:46.06ms
step:1074/1920 train_time:49484ms step_avg:46.07ms
step:1075/1920 train_time:49548ms step_avg:46.09ms
step:1076/1920 train_time:49609ms step_avg:46.11ms
step:1077/1920 train_time:49672ms step_avg:46.12ms
step:1078/1920 train_time:49734ms step_avg:46.14ms
step:1079/1920 train_time:49796ms step_avg:46.15ms
step:1080/1920 train_time:49857ms step_avg:46.16ms
step:1081/1920 train_time:49920ms step_avg:46.18ms
step:1082/1920 train_time:49982ms step_avg:46.19ms
step:1083/1920 train_time:50046ms step_avg:46.21ms
step:1084/1920 train_time:50108ms step_avg:46.22ms
step:1085/1920 train_time:50171ms step_avg:46.24ms
step:1086/1920 train_time:50233ms step_avg:46.26ms
step:1087/1920 train_time:50295ms step_avg:46.27ms
step:1088/1920 train_time:50358ms step_avg:46.28ms
step:1089/1920 train_time:50421ms step_avg:46.30ms
step:1090/1920 train_time:50483ms step_avg:46.31ms
step:1091/1920 train_time:50546ms step_avg:46.33ms
step:1092/1920 train_time:50608ms step_avg:46.34ms
step:1093/1920 train_time:50671ms step_avg:46.36ms
step:1094/1920 train_time:50733ms step_avg:46.37ms
step:1095/1920 train_time:50795ms step_avg:46.39ms
step:1096/1920 train_time:50857ms step_avg:46.40ms
step:1097/1920 train_time:50919ms step_avg:46.42ms
step:1098/1920 train_time:50981ms step_avg:46.43ms
step:1099/1920 train_time:51045ms step_avg:46.45ms
step:1100/1920 train_time:51106ms step_avg:46.46ms
step:1101/1920 train_time:51169ms step_avg:46.48ms
step:1102/1920 train_time:51231ms step_avg:46.49ms
step:1103/1920 train_time:51293ms step_avg:46.50ms
step:1104/1920 train_time:51355ms step_avg:46.52ms
step:1105/1920 train_time:51419ms step_avg:46.53ms
step:1106/1920 train_time:51481ms step_avg:46.55ms
step:1107/1920 train_time:51544ms step_avg:46.56ms
step:1108/1920 train_time:51605ms step_avg:46.58ms
step:1109/1920 train_time:51668ms step_avg:46.59ms
step:1110/1920 train_time:51729ms step_avg:46.60ms
step:1111/1920 train_time:51792ms step_avg:46.62ms
step:1112/1920 train_time:51854ms step_avg:46.63ms
step:1113/1920 train_time:51917ms step_avg:46.65ms
step:1114/1920 train_time:51979ms step_avg:46.66ms
step:1115/1920 train_time:52042ms step_avg:46.67ms
step:1116/1920 train_time:52104ms step_avg:46.69ms
step:1117/1920 train_time:52167ms step_avg:46.70ms
step:1118/1920 train_time:52229ms step_avg:46.72ms
step:1119/1920 train_time:52292ms step_avg:46.73ms
step:1120/1920 train_time:52354ms step_avg:46.74ms
step:1121/1920 train_time:52417ms step_avg:46.76ms
step:1122/1920 train_time:52479ms step_avg:46.77ms
step:1123/1920 train_time:52541ms step_avg:46.79ms
step:1124/1920 train_time:52604ms step_avg:46.80ms
step:1125/1920 train_time:52667ms step_avg:46.82ms
step:1126/1920 train_time:52728ms step_avg:46.83ms
step:1127/1920 train_time:52791ms step_avg:46.84ms
step:1128/1920 train_time:52853ms step_avg:46.86ms
step:1129/1920 train_time:52915ms step_avg:46.87ms
step:1130/1920 train_time:52977ms step_avg:46.88ms
step:1131/1920 train_time:53040ms step_avg:46.90ms
step:1132/1920 train_time:53102ms step_avg:46.91ms
step:1133/1920 train_time:53166ms step_avg:46.92ms
step:1134/1920 train_time:53228ms step_avg:46.94ms
step:1135/1920 train_time:53290ms step_avg:46.95ms
step:1136/1920 train_time:53352ms step_avg:46.96ms
step:1137/1920 train_time:53415ms step_avg:46.98ms
step:1138/1920 train_time:53477ms step_avg:46.99ms
step:1139/1920 train_time:53540ms step_avg:47.01ms
step:1140/1920 train_time:53602ms step_avg:47.02ms
step:1141/1920 train_time:53666ms step_avg:47.03ms
step:1142/1920 train_time:53728ms step_avg:47.05ms
step:1143/1920 train_time:53790ms step_avg:47.06ms
step:1144/1920 train_time:53853ms step_avg:47.07ms
step:1145/1920 train_time:53915ms step_avg:47.09ms
step:1146/1920 train_time:53977ms step_avg:47.10ms
step:1147/1920 train_time:54039ms step_avg:47.11ms
step:1148/1920 train_time:54101ms step_avg:47.13ms
step:1149/1920 train_time:54165ms step_avg:47.14ms
step:1150/1920 train_time:54227ms step_avg:47.15ms
step:1151/1920 train_time:54290ms step_avg:47.17ms
step:1152/1920 train_time:54352ms step_avg:47.18ms
step:1153/1920 train_time:54414ms step_avg:47.19ms
step:1154/1920 train_time:54477ms step_avg:47.21ms
step:1155/1920 train_time:54539ms step_avg:47.22ms
step:1156/1920 train_time:54601ms step_avg:47.23ms
step:1157/1920 train_time:54664ms step_avg:47.25ms
step:1158/1920 train_time:54726ms step_avg:47.26ms
step:1159/1920 train_time:54789ms step_avg:47.27ms
step:1160/1920 train_time:54851ms step_avg:47.29ms
step:1161/1920 train_time:54914ms step_avg:47.30ms
step:1162/1920 train_time:54975ms step_avg:47.31ms
step:1163/1920 train_time:55038ms step_avg:47.32ms
step:1164/1920 train_time:55100ms step_avg:47.34ms
step:1165/1920 train_time:55164ms step_avg:47.35ms
step:1166/1920 train_time:55226ms step_avg:47.36ms
step:1167/1920 train_time:55289ms step_avg:47.38ms
step:1168/1920 train_time:55351ms step_avg:47.39ms
step:1169/1920 train_time:55413ms step_avg:47.40ms
step:1170/1920 train_time:55475ms step_avg:47.41ms
step:1171/1920 train_time:55538ms step_avg:47.43ms
step:1172/1920 train_time:55600ms step_avg:47.44ms
step:1173/1920 train_time:55663ms step_avg:47.45ms
step:1174/1920 train_time:55725ms step_avg:47.47ms
step:1175/1920 train_time:55788ms step_avg:47.48ms
step:1176/1920 train_time:55850ms step_avg:47.49ms
step:1177/1920 train_time:55913ms step_avg:47.50ms
step:1178/1920 train_time:55975ms step_avg:47.52ms
step:1179/1920 train_time:56037ms step_avg:47.53ms
step:1180/1920 train_time:56099ms step_avg:47.54ms
step:1181/1920 train_time:56162ms step_avg:47.55ms
step:1182/1920 train_time:56224ms step_avg:47.57ms
step:1183/1920 train_time:56287ms step_avg:47.58ms
step:1184/1920 train_time:56349ms step_avg:47.59ms
step:1185/1920 train_time:56412ms step_avg:47.60ms
step:1186/1920 train_time:56474ms step_avg:47.62ms
step:1187/1920 train_time:56536ms step_avg:47.63ms
step:1188/1920 train_time:56598ms step_avg:47.64ms
step:1189/1920 train_time:56661ms step_avg:47.65ms
step:1190/1920 train_time:56724ms step_avg:47.67ms
step:1191/1920 train_time:56786ms step_avg:47.68ms
step:1192/1920 train_time:56848ms step_avg:47.69ms
step:1193/1920 train_time:56911ms step_avg:47.70ms
step:1194/1920 train_time:56973ms step_avg:47.72ms
step:1195/1920 train_time:57035ms step_avg:47.73ms
step:1196/1920 train_time:57097ms step_avg:47.74ms
step:1197/1920 train_time:57160ms step_avg:47.75ms
step:1198/1920 train_time:57222ms step_avg:47.76ms
step:1199/1920 train_time:57286ms step_avg:47.78ms
step:1200/1920 train_time:57348ms step_avg:47.79ms
step:1201/1920 train_time:57411ms step_avg:47.80ms
step:1202/1920 train_time:57473ms step_avg:47.81ms
step:1203/1920 train_time:57535ms step_avg:47.83ms
step:1204/1920 train_time:57597ms step_avg:47.84ms
step:1205/1920 train_time:57660ms step_avg:47.85ms
step:1206/1920 train_time:57722ms step_avg:47.86ms
step:1207/1920 train_time:57785ms step_avg:47.88ms
step:1208/1920 train_time:57847ms step_avg:47.89ms
step:1209/1920 train_time:57910ms step_avg:47.90ms
step:1210/1920 train_time:57971ms step_avg:47.91ms
step:1211/1920 train_time:58034ms step_avg:47.92ms
step:1212/1920 train_time:58096ms step_avg:47.93ms
step:1213/1920 train_time:58158ms step_avg:47.95ms
step:1214/1920 train_time:58220ms step_avg:47.96ms
step:1215/1920 train_time:58283ms step_avg:47.97ms
step:1216/1920 train_time:58345ms step_avg:47.98ms
step:1217/1920 train_time:58408ms step_avg:47.99ms
step:1218/1920 train_time:58470ms step_avg:48.00ms
step:1219/1920 train_time:58532ms step_avg:48.02ms
step:1220/1920 train_time:58594ms step_avg:48.03ms
step:1221/1920 train_time:58657ms step_avg:48.04ms
step:1222/1920 train_time:58718ms step_avg:48.05ms
step:1223/1920 train_time:58782ms step_avg:48.06ms
step:1224/1920 train_time:58844ms step_avg:48.08ms
step:1225/1920 train_time:58908ms step_avg:48.09ms
step:1226/1920 train_time:58970ms step_avg:48.10ms
step:1227/1920 train_time:59032ms step_avg:48.11ms
step:1228/1920 train_time:59094ms step_avg:48.12ms
step:1229/1920 train_time:59156ms step_avg:48.13ms
step:1230/1920 train_time:59218ms step_avg:48.14ms
step:1231/1920 train_time:59281ms step_avg:48.16ms
step:1232/1920 train_time:59343ms step_avg:48.17ms
step:1233/1920 train_time:59407ms step_avg:48.18ms
step:1234/1920 train_time:59469ms step_avg:48.19ms
step:1235/1920 train_time:59531ms step_avg:48.20ms
step:1236/1920 train_time:59593ms step_avg:48.21ms
step:1237/1920 train_time:59655ms step_avg:48.23ms
step:1238/1920 train_time:59716ms step_avg:48.24ms
step:1239/1920 train_time:59780ms step_avg:48.25ms
step:1240/1920 train_time:59842ms step_avg:48.26ms
step:1241/1920 train_time:59905ms step_avg:48.27ms
step:1242/1920 train_time:59967ms step_avg:48.28ms
step:1243/1920 train_time:60029ms step_avg:48.29ms
step:1244/1920 train_time:60091ms step_avg:48.30ms
step:1245/1920 train_time:60155ms step_avg:48.32ms
step:1246/1920 train_time:60217ms step_avg:48.33ms
step:1247/1920 train_time:60280ms step_avg:48.34ms
step:1248/1920 train_time:60342ms step_avg:48.35ms
step:1249/1920 train_time:60405ms step_avg:48.36ms
step:1250/1920 train_time:60467ms step_avg:48.37ms
step:1250/1920 val_loss:3.5530 train_time:60532ms step_avg:48.43ms
step:1251/1920 train_time:60550ms step_avg:48.40ms
step:1252/1920 train_time:60594ms step_avg:48.40ms
step:1253/1920 train_time:60662ms step_avg:48.41ms
step:1254/1920 train_time:60727ms step_avg:48.43ms
step:1255/1920 train_time:60790ms step_avg:48.44ms
step:1256/1920 train_time:60877ms step_avg:48.47ms
step:1257/1920 train_time:60965ms step_avg:48.50ms
step:1258/1920 train_time:61052ms step_avg:48.53ms
step:1259/1920 train_time:61140ms step_avg:48.56ms
step:1260/1920 train_time:61226ms step_avg:48.59ms
step:1261/1920 train_time:61314ms step_avg:48.62ms
step:1262/1920 train_time:61401ms step_avg:48.65ms
step:1263/1920 train_time:61492ms step_avg:48.69ms
step:1264/1920 train_time:61584ms step_avg:48.72ms
step:1265/1920 train_time:61678ms step_avg:48.76ms
step:1266/1920 train_time:61767ms step_avg:48.79ms
step:1267/1920 train_time:61856ms step_avg:48.82ms
step:1268/1920 train_time:61943ms step_avg:48.85ms
step:1269/1920 train_time:62031ms step_avg:48.88ms
step:1270/1920 train_time:62117ms step_avg:48.91ms
step:1271/1920 train_time:62205ms step_avg:48.94ms
step:1272/1920 train_time:62291ms step_avg:48.97ms
step:1273/1920 train_time:62379ms step_avg:49.00ms
step:1274/1920 train_time:62467ms step_avg:49.03ms
step:1275/1920 train_time:62558ms step_avg:49.07ms
step:1276/1920 train_time:62649ms step_avg:49.10ms
step:1277/1920 train_time:62739ms step_avg:49.13ms
step:1278/1920 train_time:62827ms step_avg:49.16ms
step:1279/1920 train_time:62916ms step_avg:49.19ms
step:1280/1920 train_time:63005ms step_avg:49.22ms
step:1281/1920 train_time:63094ms step_avg:49.25ms
step:1282/1920 train_time:63181ms step_avg:49.28ms
step:1283/1920 train_time:63269ms step_avg:49.31ms
step:1284/1920 train_time:63356ms step_avg:49.34ms
step:1285/1920 train_time:63445ms step_avg:49.37ms
step:1286/1920 train_time:63534ms step_avg:49.40ms
step:1287/1920 train_time:63624ms step_avg:49.44ms
step:1288/1920 train_time:63714ms step_avg:49.47ms
step:1289/1920 train_time:63804ms step_avg:49.50ms
step:1290/1920 train_time:63893ms step_avg:49.53ms
step:1291/1920 train_time:63981ms step_avg:49.56ms
step:1292/1920 train_time:64068ms step_avg:49.59ms
step:1293/1920 train_time:64156ms step_avg:49.62ms
step:1294/1920 train_time:64244ms step_avg:49.65ms
step:1295/1920 train_time:64332ms step_avg:49.68ms
step:1296/1920 train_time:64420ms step_avg:49.71ms
step:1297/1920 train_time:64509ms step_avg:49.74ms
step:1298/1920 train_time:64597ms step_avg:49.77ms
step:1299/1920 train_time:64686ms step_avg:49.80ms
step:1300/1920 train_time:64775ms step_avg:49.83ms
step:1301/1920 train_time:64863ms step_avg:49.86ms
step:1302/1920 train_time:64951ms step_avg:49.89ms
step:1303/1920 train_time:65039ms step_avg:49.91ms
step:1304/1920 train_time:65126ms step_avg:49.94ms
step:1305/1920 train_time:65214ms step_avg:49.97ms
step:1306/1920 train_time:65302ms step_avg:50.00ms
step:1307/1920 train_time:65391ms step_avg:50.03ms
step:1308/1920 train_time:65478ms step_avg:50.06ms
step:1309/1920 train_time:65567ms step_avg:50.09ms
step:1310/1920 train_time:65656ms step_avg:50.12ms
step:1311/1920 train_time:65746ms step_avg:50.15ms
step:1312/1920 train_time:65833ms step_avg:50.18ms
step:1313/1920 train_time:65922ms step_avg:50.21ms
step:1314/1920 train_time:66011ms step_avg:50.24ms
step:1315/1920 train_time:66099ms step_avg:50.27ms
step:1316/1920 train_time:66187ms step_avg:50.29ms
step:1317/1920 train_time:66275ms step_avg:50.32ms
step:1318/1920 train_time:66363ms step_avg:50.35ms
step:1319/1920 train_time:66453ms step_avg:50.38ms
step:1320/1920 train_time:66541ms step_avg:50.41ms
step:1321/1920 train_time:66630ms step_avg:50.44ms
step:1322/1920 train_time:66718ms step_avg:50.47ms
step:1323/1920 train_time:66807ms step_avg:50.50ms
step:1324/1920 train_time:66896ms step_avg:50.53ms
step:1325/1920 train_time:66984ms step_avg:50.55ms
step:1326/1920 train_time:67072ms step_avg:50.58ms
step:1327/1920 train_time:67160ms step_avg:50.61ms
step:1328/1920 train_time:67248ms step_avg:50.64ms
step:1329/1920 train_time:67337ms step_avg:50.67ms
step:1330/1920 train_time:67425ms step_avg:50.70ms
step:1331/1920 train_time:67514ms step_avg:50.72ms
step:1332/1920 train_time:67603ms step_avg:50.75ms
step:1333/1920 train_time:67693ms step_avg:50.78ms
step:1334/1920 train_time:67781ms step_avg:50.81ms
step:1335/1920 train_time:67870ms step_avg:50.84ms
step:1336/1920 train_time:67957ms step_avg:50.87ms
step:1337/1920 train_time:68046ms step_avg:50.89ms
step:1338/1920 train_time:68134ms step_avg:50.92ms
step:1339/1920 train_time:68222ms step_avg:50.95ms
step:1340/1920 train_time:68309ms step_avg:50.98ms
step:1341/1920 train_time:68398ms step_avg:51.01ms
step:1342/1920 train_time:68486ms step_avg:51.03ms
step:1343/1920 train_time:68576ms step_avg:51.06ms
step:1344/1920 train_time:68665ms step_avg:51.09ms
step:1345/1920 train_time:68755ms step_avg:51.12ms
step:1346/1920 train_time:68844ms step_avg:51.15ms
step:1347/1920 train_time:68933ms step_avg:51.17ms
step:1348/1920 train_time:69020ms step_avg:51.20ms
step:1349/1920 train_time:69109ms step_avg:51.23ms
step:1350/1920 train_time:69196ms step_avg:51.26ms
step:1351/1920 train_time:69285ms step_avg:51.28ms
step:1352/1920 train_time:69373ms step_avg:51.31ms
step:1353/1920 train_time:69462ms step_avg:51.34ms
step:1354/1920 train_time:69551ms step_avg:51.37ms
step:1355/1920 train_time:69640ms step_avg:51.39ms
step:1356/1920 train_time:69728ms step_avg:51.42ms
step:1357/1920 train_time:69816ms step_avg:51.45ms
step:1358/1920 train_time:69904ms step_avg:51.48ms
step:1359/1920 train_time:69994ms step_avg:51.50ms
step:1360/1920 train_time:70082ms step_avg:51.53ms
step:1361/1920 train_time:70171ms step_avg:51.56ms
step:1362/1920 train_time:70259ms step_avg:51.59ms
step:1363/1920 train_time:70347ms step_avg:51.61ms
step:1364/1920 train_time:70435ms step_avg:51.64ms
step:1365/1920 train_time:70524ms step_avg:51.67ms
step:1366/1920 train_time:70613ms step_avg:51.69ms
step:1367/1920 train_time:70702ms step_avg:51.72ms
step:1368/1920 train_time:70790ms step_avg:51.75ms
step:1369/1920 train_time:70879ms step_avg:51.77ms
step:1370/1920 train_time:70967ms step_avg:51.80ms
step:1371/1920 train_time:71056ms step_avg:51.83ms
step:1372/1920 train_time:71144ms step_avg:51.85ms
step:1373/1920 train_time:71234ms step_avg:51.88ms
step:1374/1920 train_time:71322ms step_avg:51.91ms
step:1375/1920 train_time:71411ms step_avg:51.93ms
step:1376/1920 train_time:71498ms step_avg:51.96ms
step:1377/1920 train_time:71586ms step_avg:51.99ms
step:1378/1920 train_time:71674ms step_avg:52.01ms
step:1379/1920 train_time:71763ms step_avg:52.04ms
step:1380/1920 train_time:71851ms step_avg:52.07ms
step:1381/1920 train_time:71941ms step_avg:52.09ms
step:1382/1920 train_time:72028ms step_avg:52.12ms
step:1383/1920 train_time:72117ms step_avg:52.15ms
step:1384/1920 train_time:72205ms step_avg:52.17ms
step:1385/1920 train_time:72294ms step_avg:52.20ms
step:1386/1920 train_time:72382ms step_avg:52.22ms
step:1387/1920 train_time:72472ms step_avg:52.25ms
step:1388/1920 train_time:72559ms step_avg:52.28ms
step:1389/1920 train_time:72649ms step_avg:52.30ms
step:1390/1920 train_time:72736ms step_avg:52.33ms
step:1391/1920 train_time:72825ms step_avg:52.35ms
step:1392/1920 train_time:72913ms step_avg:52.38ms
step:1393/1920 train_time:73001ms step_avg:52.41ms
step:1394/1920 train_time:73089ms step_avg:52.43ms
step:1395/1920 train_time:73178ms step_avg:52.46ms
step:1396/1920 train_time:73267ms step_avg:52.48ms
step:1397/1920 train_time:73356ms step_avg:52.51ms
step:1398/1920 train_time:73444ms step_avg:52.54ms
step:1399/1920 train_time:73534ms step_avg:52.56ms
step:1400/1920 train_time:73623ms step_avg:52.59ms
step:1401/1920 train_time:73713ms step_avg:52.61ms
step:1402/1920 train_time:73800ms step_avg:52.64ms
step:1403/1920 train_time:73889ms step_avg:52.66ms
step:1404/1920 train_time:73977ms step_avg:52.69ms
step:1405/1920 train_time:74065ms step_avg:52.72ms
step:1406/1920 train_time:74153ms step_avg:52.74ms
step:1407/1920 train_time:74242ms step_avg:52.77ms
step:1408/1920 train_time:74330ms step_avg:52.79ms
step:1409/1920 train_time:74419ms step_avg:52.82ms
step:1410/1920 train_time:74507ms step_avg:52.84ms
step:1411/1920 train_time:74596ms step_avg:52.87ms
step:1412/1920 train_time:74684ms step_avg:52.89ms
step:1413/1920 train_time:74774ms step_avg:52.92ms
step:1414/1920 train_time:74862ms step_avg:52.94ms
step:1415/1920 train_time:74952ms step_avg:52.97ms
step:1416/1920 train_time:75040ms step_avg:52.99ms
step:1417/1920 train_time:75130ms step_avg:53.02ms
step:1418/1920 train_time:75219ms step_avg:53.05ms
step:1419/1920 train_time:75308ms step_avg:53.07ms
step:1420/1920 train_time:75396ms step_avg:53.10ms
step:1421/1920 train_time:75484ms step_avg:53.12ms
step:1422/1920 train_time:75572ms step_avg:53.15ms
step:1423/1920 train_time:75661ms step_avg:53.17ms
step:1424/1920 train_time:75750ms step_avg:53.19ms
step:1425/1920 train_time:75838ms step_avg:53.22ms
step:1426/1920 train_time:75927ms step_avg:53.24ms
step:1427/1920 train_time:76015ms step_avg:53.27ms
step:1428/1920 train_time:76103ms step_avg:53.29ms
step:1429/1920 train_time:76192ms step_avg:53.32ms
step:1430/1920 train_time:76280ms step_avg:53.34ms
step:1431/1920 train_time:76369ms step_avg:53.37ms
step:1432/1920 train_time:76457ms step_avg:53.39ms
step:1433/1920 train_time:76545ms step_avg:53.42ms
step:1434/1920 train_time:76633ms step_avg:53.44ms
step:1435/1920 train_time:76722ms step_avg:53.46ms
step:1436/1920 train_time:76810ms step_avg:53.49ms
step:1437/1920 train_time:76898ms step_avg:53.51ms
step:1438/1920 train_time:76987ms step_avg:53.54ms
step:1439/1920 train_time:77075ms step_avg:53.56ms
step:1440/1920 train_time:77164ms step_avg:53.59ms
step:1441/1920 train_time:77253ms step_avg:53.61ms
step:1442/1920 train_time:77342ms step_avg:53.63ms
step:1443/1920 train_time:77431ms step_avg:53.66ms
step:1444/1920 train_time:77518ms step_avg:53.68ms
step:1445/1920 train_time:77607ms step_avg:53.71ms
step:1446/1920 train_time:77695ms step_avg:53.73ms
step:1447/1920 train_time:77784ms step_avg:53.76ms
step:1448/1920 train_time:77872ms step_avg:53.78ms
step:1449/1920 train_time:77960ms step_avg:53.80ms
step:1450/1920 train_time:78049ms step_avg:53.83ms
step:1451/1920 train_time:78137ms step_avg:53.85ms
step:1452/1920 train_time:78226ms step_avg:53.87ms
step:1453/1920 train_time:78316ms step_avg:53.90ms
step:1454/1920 train_time:78405ms step_avg:53.92ms
step:1455/1920 train_time:78495ms step_avg:53.95ms
step:1456/1920 train_time:78583ms step_avg:53.97ms
step:1457/1920 train_time:78672ms step_avg:54.00ms
step:1458/1920 train_time:78759ms step_avg:54.02ms
step:1459/1920 train_time:78849ms step_avg:54.04ms
step:1460/1920 train_time:78937ms step_avg:54.07ms
step:1461/1920 train_time:79026ms step_avg:54.09ms
step:1462/1920 train_time:79115ms step_avg:54.11ms
step:1463/1920 train_time:79204ms step_avg:54.14ms
step:1464/1920 train_time:79293ms step_avg:54.16ms
step:1465/1920 train_time:79381ms step_avg:54.18ms
step:1466/1920 train_time:79470ms step_avg:54.21ms
step:1467/1920 train_time:79559ms step_avg:54.23ms
step:1468/1920 train_time:79647ms step_avg:54.26ms
step:1469/1920 train_time:79736ms step_avg:54.28ms
step:1470/1920 train_time:79824ms step_avg:54.30ms
step:1471/1920 train_time:79914ms step_avg:54.33ms
step:1472/1920 train_time:80001ms step_avg:54.35ms
step:1473/1920 train_time:80090ms step_avg:54.37ms
step:1474/1920 train_time:80177ms step_avg:54.39ms
step:1475/1920 train_time:80267ms step_avg:54.42ms
step:1476/1920 train_time:80355ms step_avg:54.44ms
step:1477/1920 train_time:80444ms step_avg:54.46ms
step:1478/1920 train_time:80533ms step_avg:54.49ms
step:1479/1920 train_time:80622ms step_avg:54.51ms
step:1480/1920 train_time:80710ms step_avg:54.53ms
step:1481/1920 train_time:80798ms step_avg:54.56ms
step:1482/1920 train_time:80886ms step_avg:54.58ms
step:1483/1920 train_time:80975ms step_avg:54.60ms
step:1484/1920 train_time:81063ms step_avg:54.62ms
step:1485/1920 train_time:81151ms step_avg:54.65ms
step:1486/1920 train_time:81239ms step_avg:54.67ms
step:1487/1920 train_time:81328ms step_avg:54.69ms
step:1488/1920 train_time:81416ms step_avg:54.71ms
step:1489/1920 train_time:81505ms step_avg:54.74ms
step:1490/1920 train_time:81593ms step_avg:54.76ms
step:1491/1920 train_time:81681ms step_avg:54.78ms
step:1492/1920 train_time:81769ms step_avg:54.81ms
step:1493/1920 train_time:81858ms step_avg:54.83ms
step:1494/1920 train_time:81947ms step_avg:54.85ms
step:1495/1920 train_time:82037ms step_avg:54.87ms
step:1496/1920 train_time:82126ms step_avg:54.90ms
step:1497/1920 train_time:82216ms step_avg:54.92ms
step:1498/1920 train_time:82304ms step_avg:54.94ms
step:1499/1920 train_time:82393ms step_avg:54.97ms
step:1500/1920 train_time:82481ms step_avg:54.99ms
step:1500/1920 val_loss:3.4147 train_time:82573ms step_avg:55.05ms
step:1501/1920 train_time:82591ms step_avg:55.02ms
step:1502/1920 train_time:82664ms step_avg:55.04ms
step:1503/1920 train_time:82761ms step_avg:55.06ms
step:1504/1920 train_time:82849ms step_avg:55.09ms
step:1505/1920 train_time:82938ms step_avg:55.11ms
step:1506/1920 train_time:83024ms step_avg:55.13ms
step:1507/1920 train_time:83112ms step_avg:55.15ms
step:1508/1920 train_time:83199ms step_avg:55.17ms
step:1509/1920 train_time:83286ms step_avg:55.19ms
step:1510/1920 train_time:83373ms step_avg:55.21ms
step:1511/1920 train_time:83461ms step_avg:55.24ms
step:1512/1920 train_time:83550ms step_avg:55.26ms
step:1513/1920 train_time:83643ms step_avg:55.28ms
step:1514/1920 train_time:83735ms step_avg:55.31ms
step:1515/1920 train_time:83825ms step_avg:55.33ms
step:1516/1920 train_time:83914ms step_avg:55.35ms
step:1517/1920 train_time:84003ms step_avg:55.37ms
step:1518/1920 train_time:84091ms step_avg:55.40ms
step:1519/1920 train_time:84179ms step_avg:55.42ms
step:1520/1920 train_time:84265ms step_avg:55.44ms
step:1521/1920 train_time:84352ms step_avg:55.46ms
step:1522/1920 train_time:84439ms step_avg:55.48ms
step:1523/1920 train_time:84529ms step_avg:55.50ms
step:1524/1920 train_time:84618ms step_avg:55.52ms
step:1525/1920 train_time:84708ms step_avg:55.55ms
step:1526/1920 train_time:84798ms step_avg:55.57ms
step:1527/1920 train_time:84887ms step_avg:55.59ms
step:1528/1920 train_time:84976ms step_avg:55.61ms
step:1529/1920 train_time:85065ms step_avg:55.63ms
step:1530/1920 train_time:85152ms step_avg:55.66ms
step:1531/1920 train_time:85240ms step_avg:55.68ms
step:1532/1920 train_time:85327ms step_avg:55.70ms
step:1533/1920 train_time:85414ms step_avg:55.72ms
step:1534/1920 train_time:85502ms step_avg:55.74ms
step:1535/1920 train_time:85592ms step_avg:55.76ms
step:1536/1920 train_time:85680ms step_avg:55.78ms
step:1537/1920 train_time:85769ms step_avg:55.80ms
step:1538/1920 train_time:85859ms step_avg:55.83ms
step:1539/1920 train_time:85948ms step_avg:55.85ms
step:1540/1920 train_time:86036ms step_avg:55.87ms
step:1541/1920 train_time:86125ms step_avg:55.89ms
step:1542/1920 train_time:86212ms step_avg:55.91ms
step:1543/1920 train_time:86300ms step_avg:55.93ms
step:1544/1920 train_time:86387ms step_avg:55.95ms
step:1545/1920 train_time:86475ms step_avg:55.97ms
step:1546/1920 train_time:86563ms step_avg:55.99ms
step:1547/1920 train_time:86653ms step_avg:56.01ms
step:1548/1920 train_time:86742ms step_avg:56.03ms
step:1549/1920 train_time:86832ms step_avg:56.06ms
step:1550/1920 train_time:86921ms step_avg:56.08ms
step:1551/1920 train_time:87009ms step_avg:56.10ms
step:1552/1920 train_time:87097ms step_avg:56.12ms
step:1553/1920 train_time:87185ms step_avg:56.14ms
step:1554/1920 train_time:87274ms step_avg:56.16ms
step:1555/1920 train_time:87362ms step_avg:56.18ms
step:1556/1920 train_time:87450ms step_avg:56.20ms
step:1557/1920 train_time:87539ms step_avg:56.22ms
step:1558/1920 train_time:87626ms step_avg:56.24ms
step:1559/1920 train_time:87715ms step_avg:56.26ms
step:1560/1920 train_time:87804ms step_avg:56.28ms
step:1561/1920 train_time:87893ms step_avg:56.31ms
step:1562/1920 train_time:87982ms step_avg:56.33ms
step:1563/1920 train_time:88071ms step_avg:56.35ms
step:1564/1920 train_time:88159ms step_avg:56.37ms
step:1565/1920 train_time:88247ms step_avg:56.39ms
step:1566/1920 train_time:88335ms step_avg:56.41ms
step:1567/1920 train_time:88424ms step_avg:56.43ms
step:1568/1920 train_time:88511ms step_avg:56.45ms
step:1569/1920 train_time:88602ms step_avg:56.47ms
step:1570/1920 train_time:88690ms step_avg:56.49ms
step:1571/1920 train_time:88780ms step_avg:56.51ms
step:1572/1920 train_time:88869ms step_avg:56.53ms
step:1573/1920 train_time:88959ms step_avg:56.55ms
step:1574/1920 train_time:89047ms step_avg:56.57ms
step:1575/1920 train_time:89135ms step_avg:56.59ms
step:1576/1920 train_time:89223ms step_avg:56.61ms
step:1577/1920 train_time:89311ms step_avg:56.63ms
step:1578/1920 train_time:89400ms step_avg:56.65ms
step:1579/1920 train_time:89488ms step_avg:56.67ms
step:1580/1920 train_time:89577ms step_avg:56.69ms
step:1581/1920 train_time:89665ms step_avg:56.71ms
step:1582/1920 train_time:89754ms step_avg:56.73ms
step:1583/1920 train_time:89842ms step_avg:56.75ms
step:1584/1920 train_time:89931ms step_avg:56.77ms
step:1585/1920 train_time:90020ms step_avg:56.80ms
step:1586/1920 train_time:90108ms step_avg:56.81ms
step:1587/1920 train_time:90197ms step_avg:56.83ms
step:1588/1920 train_time:90285ms step_avg:56.85ms
step:1589/1920 train_time:90373ms step_avg:56.87ms
step:1590/1920 train_time:90461ms step_avg:56.89ms
step:1591/1920 train_time:90550ms step_avg:56.91ms
step:1592/1920 train_time:90639ms step_avg:56.93ms
step:1593/1920 train_time:90729ms step_avg:56.95ms
step:1594/1920 train_time:90817ms step_avg:56.97ms
step:1595/1920 train_time:90905ms step_avg:56.99ms
step:1596/1920 train_time:90993ms step_avg:57.01ms
step:1597/1920 train_time:91082ms step_avg:57.03ms
step:1598/1920 train_time:91170ms step_avg:57.05ms
step:1599/1920 train_time:91259ms step_avg:57.07ms
step:1600/1920 train_time:91347ms step_avg:57.09ms
step:1601/1920 train_time:91435ms step_avg:57.11ms
step:1602/1920 train_time:91523ms step_avg:57.13ms
step:1603/1920 train_time:91612ms step_avg:57.15ms
step:1604/1920 train_time:91700ms step_avg:57.17ms
step:1605/1920 train_time:91791ms step_avg:57.19ms
step:1606/1920 train_time:91879ms step_avg:57.21ms
step:1607/1920 train_time:91967ms step_avg:57.23ms
step:1608/1920 train_time:92056ms step_avg:57.25ms
step:1609/1920 train_time:92145ms step_avg:57.27ms
step:1610/1920 train_time:92233ms step_avg:57.29ms
step:1611/1920 train_time:92322ms step_avg:57.31ms
step:1612/1920 train_time:92410ms step_avg:57.33ms
step:1613/1920 train_time:92499ms step_avg:57.35ms
step:1614/1920 train_time:92587ms step_avg:57.37ms
step:1615/1920 train_time:92676ms step_avg:57.38ms
step:1616/1920 train_time:92763ms step_avg:57.40ms
step:1617/1920 train_time:92852ms step_avg:57.42ms
step:1618/1920 train_time:92941ms step_avg:57.44ms
step:1619/1920 train_time:93030ms step_avg:57.46ms
step:1620/1920 train_time:93118ms step_avg:57.48ms
step:1621/1920 train_time:93207ms step_avg:57.50ms
step:1622/1920 train_time:93295ms step_avg:57.52ms
step:1623/1920 train_time:93385ms step_avg:57.54ms
step:1624/1920 train_time:93472ms step_avg:57.56ms
step:1625/1920 train_time:93563ms step_avg:57.58ms
step:1626/1920 train_time:93652ms step_avg:57.60ms
step:1627/1920 train_time:93741ms step_avg:57.62ms
step:1628/1920 train_time:93829ms step_avg:57.63ms
step:1629/1920 train_time:93917ms step_avg:57.65ms
step:1630/1920 train_time:94005ms step_avg:57.67ms
step:1631/1920 train_time:94095ms step_avg:57.69ms
step:1632/1920 train_time:94183ms step_avg:57.71ms
step:1633/1920 train_time:94271ms step_avg:57.73ms
step:1634/1920 train_time:94360ms step_avg:57.75ms
step:1635/1920 train_time:94448ms step_avg:57.77ms
step:1636/1920 train_time:94537ms step_avg:57.79ms
step:1637/1920 train_time:94625ms step_avg:57.80ms
step:1638/1920 train_time:94714ms step_avg:57.82ms
step:1639/1920 train_time:94804ms step_avg:57.84ms
step:1640/1920 train_time:94892ms step_avg:57.86ms
step:1641/1920 train_time:94981ms step_avg:57.88ms
step:1642/1920 train_time:95068ms step_avg:57.90ms
step:1643/1920 train_time:95157ms step_avg:57.92ms
step:1644/1920 train_time:95245ms step_avg:57.93ms
step:1645/1920 train_time:95334ms step_avg:57.95ms
step:1646/1920 train_time:95422ms step_avg:57.97ms
step:1647/1920 train_time:95511ms step_avg:57.99ms
step:1648/1920 train_time:95600ms step_avg:58.01ms
step:1649/1920 train_time:95688ms step_avg:58.03ms
step:1650/1920 train_time:95777ms step_avg:58.05ms
step:1651/1920 train_time:95865ms step_avg:58.06ms
step:1652/1920 train_time:95953ms step_avg:58.08ms
step:1653/1920 train_time:96042ms step_avg:58.10ms
step:1654/1920 train_time:96130ms step_avg:58.12ms
step:1655/1920 train_time:96219ms step_avg:58.14ms
step:1656/1920 train_time:96307ms step_avg:58.16ms
step:1657/1920 train_time:96395ms step_avg:58.17ms
step:1658/1920 train_time:96483ms step_avg:58.19ms
step:1659/1920 train_time:96571ms step_avg:58.21ms
step:1660/1920 train_time:96661ms step_avg:58.23ms
step:1661/1920 train_time:96750ms step_avg:58.25ms
step:1662/1920 train_time:96838ms step_avg:58.27ms
step:1663/1920 train_time:96926ms step_avg:58.28ms
step:1664/1920 train_time:97014ms step_avg:58.30ms
step:1665/1920 train_time:97103ms step_avg:58.32ms
step:1666/1920 train_time:97191ms step_avg:58.34ms
step:1667/1920 train_time:97280ms step_avg:58.36ms
step:1668/1920 train_time:97368ms step_avg:58.37ms
step:1669/1920 train_time:97456ms step_avg:58.39ms
step:1670/1920 train_time:97544ms step_avg:58.41ms
step:1671/1920 train_time:97634ms step_avg:58.43ms
step:1672/1920 train_time:97721ms step_avg:58.45ms
step:1673/1920 train_time:97810ms step_avg:58.46ms
step:1674/1920 train_time:97899ms step_avg:58.48ms
step:1675/1920 train_time:97988ms step_avg:58.50ms
step:1676/1920 train_time:98077ms step_avg:58.52ms
step:1677/1920 train_time:98166ms step_avg:58.54ms
step:1678/1920 train_time:98254ms step_avg:58.55ms
step:1679/1920 train_time:98343ms step_avg:58.57ms
step:1680/1920 train_time:98432ms step_avg:58.59ms
step:1681/1920 train_time:98521ms step_avg:58.61ms
step:1682/1920 train_time:98609ms step_avg:58.63ms
step:1683/1920 train_time:98697ms step_avg:58.64ms
step:1684/1920 train_time:98785ms step_avg:58.66ms
step:1685/1920 train_time:98874ms step_avg:58.68ms
step:1686/1920 train_time:98962ms step_avg:58.70ms
step:1687/1920 train_time:99051ms step_avg:58.71ms
step:1688/1920 train_time:99140ms step_avg:58.73ms
step:1689/1920 train_time:99228ms step_avg:58.75ms
step:1690/1920 train_time:99316ms step_avg:58.77ms
step:1691/1920 train_time:99405ms step_avg:58.78ms
step:1692/1920 train_time:99494ms step_avg:58.80ms
step:1693/1920 train_time:99583ms step_avg:58.82ms
step:1694/1920 train_time:99671ms step_avg:58.84ms
step:1695/1920 train_time:99760ms step_avg:58.86ms
step:1696/1920 train_time:99847ms step_avg:58.87ms
step:1697/1920 train_time:99936ms step_avg:58.89ms
step:1698/1920 train_time:100024ms step_avg:58.91ms
step:1699/1920 train_time:100112ms step_avg:58.92ms
step:1700/1920 train_time:100201ms step_avg:58.94ms
step:1701/1920 train_time:100289ms step_avg:58.96ms
step:1702/1920 train_time:100377ms step_avg:58.98ms
step:1703/1920 train_time:100465ms step_avg:58.99ms
step:1704/1920 train_time:100554ms step_avg:59.01ms
step:1705/1920 train_time:100643ms step_avg:59.03ms
step:1706/1920 train_time:100731ms step_avg:59.05ms
step:1707/1920 train_time:100821ms step_avg:59.06ms
step:1708/1920 train_time:100908ms step_avg:59.08ms
step:1709/1920 train_time:100997ms step_avg:59.10ms
step:1710/1920 train_time:101085ms step_avg:59.11ms
step:1711/1920 train_time:101174ms step_avg:59.13ms
step:1712/1920 train_time:101263ms step_avg:59.15ms
step:1713/1920 train_time:101352ms step_avg:59.17ms
step:1714/1920 train_time:101441ms step_avg:59.18ms
step:1715/1920 train_time:101529ms step_avg:59.20ms
step:1716/1920 train_time:101617ms step_avg:59.22ms
step:1717/1920 train_time:101706ms step_avg:59.23ms
step:1718/1920 train_time:101794ms step_avg:59.25ms
step:1719/1920 train_time:101883ms step_avg:59.27ms
step:1720/1920 train_time:101972ms step_avg:59.29ms
step:1721/1920 train_time:102061ms step_avg:59.30ms
step:1722/1920 train_time:102150ms step_avg:59.32ms
step:1723/1920 train_time:102238ms step_avg:59.34ms
step:1724/1920 train_time:102326ms step_avg:59.35ms
step:1725/1920 train_time:102416ms step_avg:59.37ms
step:1726/1920 train_time:102503ms step_avg:59.39ms
step:1727/1920 train_time:102592ms step_avg:59.40ms
step:1728/1920 train_time:102680ms step_avg:59.42ms
step:1729/1920 train_time:102768ms step_avg:59.44ms
step:1730/1920 train_time:102856ms step_avg:59.45ms
step:1731/1920 train_time:102945ms step_avg:59.47ms
step:1732/1920 train_time:103033ms step_avg:59.49ms
step:1733/1920 train_time:103123ms step_avg:59.51ms
step:1734/1920 train_time:103212ms step_avg:59.52ms
step:1735/1920 train_time:103301ms step_avg:59.54ms
step:1736/1920 train_time:103389ms step_avg:59.56ms
step:1737/1920 train_time:103478ms step_avg:59.57ms
step:1738/1920 train_time:103566ms step_avg:59.59ms
step:1739/1920 train_time:103655ms step_avg:59.61ms
step:1740/1920 train_time:103742ms step_avg:59.62ms
step:1741/1920 train_time:103831ms step_avg:59.64ms
step:1742/1920 train_time:103919ms step_avg:59.65ms
step:1743/1920 train_time:104007ms step_avg:59.67ms
step:1744/1920 train_time:104096ms step_avg:59.69ms
step:1745/1920 train_time:104184ms step_avg:59.70ms
step:1746/1920 train_time:104273ms step_avg:59.72ms
step:1747/1920 train_time:104364ms step_avg:59.74ms
step:1748/1920 train_time:104453ms step_avg:59.76ms
step:1749/1920 train_time:104543ms step_avg:59.77ms
step:1750/1920 train_time:104631ms step_avg:59.79ms
step:1750/1920 val_loss:3.3234 train_time:104723ms step_avg:59.84ms
step:1751/1920 train_time:104742ms step_avg:59.82ms
step:1752/1920 train_time:104813ms step_avg:59.82ms
step:1753/1920 train_time:104907ms step_avg:59.84ms
step:1754/1920 train_time:104996ms step_avg:59.86ms
step:1755/1920 train_time:105085ms step_avg:59.88ms
step:1756/1920 train_time:105172ms step_avg:59.89ms
step:1757/1920 train_time:105259ms step_avg:59.91ms
step:1758/1920 train_time:105346ms step_avg:59.92ms
step:1759/1920 train_time:105433ms step_avg:59.94ms
step:1760/1920 train_time:105520ms step_avg:59.95ms
step:1761/1920 train_time:105608ms step_avg:59.97ms
step:1762/1920 train_time:105699ms step_avg:59.99ms
step:1763/1920 train_time:105793ms step_avg:60.01ms
step:1764/1920 train_time:105887ms step_avg:60.03ms
step:1765/1920 train_time:105977ms step_avg:60.04ms
step:1766/1920 train_time:106064ms step_avg:60.06ms
step:1767/1920 train_time:106153ms step_avg:60.08ms
step:1768/1920 train_time:106240ms step_avg:60.09ms
step:1769/1920 train_time:106327ms step_avg:60.11ms
step:1770/1920 train_time:106414ms step_avg:60.12ms
step:1771/1920 train_time:106501ms step_avg:60.14ms
step:1772/1920 train_time:106588ms step_avg:60.15ms
step:1773/1920 train_time:106678ms step_avg:60.17ms
step:1774/1920 train_time:106768ms step_avg:60.19ms
step:1775/1920 train_time:106859ms step_avg:60.20ms
step:1776/1920 train_time:106950ms step_avg:60.22ms
step:1777/1920 train_time:107038ms step_avg:60.24ms
step:1778/1920 train_time:107126ms step_avg:60.25ms
step:1779/1920 train_time:107213ms step_avg:60.27ms
step:1780/1920 train_time:107301ms step_avg:60.28ms
step:1781/1920 train_time:107389ms step_avg:60.30ms
step:1782/1920 train_time:107476ms step_avg:60.31ms
step:1783/1920 train_time:107565ms step_avg:60.33ms
step:1784/1920 train_time:107653ms step_avg:60.34ms
step:1785/1920 train_time:107743ms step_avg:60.36ms
step:1786/1920 train_time:107833ms step_avg:60.38ms
step:1787/1920 train_time:107923ms step_avg:60.39ms
step:1788/1920 train_time:108012ms step_avg:60.41ms
step:1789/1920 train_time:108100ms step_avg:60.43ms
step:1790/1920 train_time:108188ms step_avg:60.44ms
step:1791/1920 train_time:108277ms step_avg:60.46ms
step:1792/1920 train_time:108364ms step_avg:60.47ms
step:1793/1920 train_time:108452ms step_avg:60.49ms
step:1794/1920 train_time:108540ms step_avg:60.50ms
step:1795/1920 train_time:108629ms step_avg:60.52ms
step:1796/1920 train_time:108718ms step_avg:60.53ms
step:1797/1920 train_time:108808ms step_avg:60.55ms
step:1798/1920 train_time:108898ms step_avg:60.57ms
step:1799/1920 train_time:108989ms step_avg:60.58ms
step:1800/1920 train_time:109078ms step_avg:60.60ms
step:1801/1920 train_time:109166ms step_avg:60.61ms
step:1802/1920 train_time:109254ms step_avg:60.63ms
step:1803/1920 train_time:109343ms step_avg:60.64ms
step:1804/1920 train_time:109430ms step_avg:60.66ms
step:1805/1920 train_time:109518ms step_avg:60.67ms
step:1806/1920 train_time:109605ms step_avg:60.69ms
step:1807/1920 train_time:109696ms step_avg:60.71ms
step:1808/1920 train_time:109784ms step_avg:60.72ms
step:1809/1920 train_time:109875ms step_avg:60.74ms
step:1810/1920 train_time:109963ms step_avg:60.75ms
step:1811/1920 train_time:110052ms step_avg:60.77ms
step:1812/1920 train_time:110141ms step_avg:60.78ms
step:1813/1920 train_time:110229ms step_avg:60.80ms
step:1814/1920 train_time:110317ms step_avg:60.81ms
step:1815/1920 train_time:110406ms step_avg:60.83ms
step:1816/1920 train_time:110494ms step_avg:60.84ms
step:1817/1920 train_time:110585ms step_avg:60.86ms
step:1818/1920 train_time:110673ms step_avg:60.88ms
step:1819/1920 train_time:110762ms step_avg:60.89ms
step:1820/1920 train_time:110851ms step_avg:60.91ms
step:1821/1920 train_time:110940ms step_avg:60.92ms
step:1822/1920 train_time:111028ms step_avg:60.94ms
step:1823/1920 train_time:111117ms step_avg:60.95ms
step:1824/1920 train_time:111205ms step_avg:60.97ms
step:1825/1920 train_time:111293ms step_avg:60.98ms
step:1826/1920 train_time:111381ms step_avg:61.00ms
step:1827/1920 train_time:111470ms step_avg:61.01ms
step:1828/1920 train_time:111558ms step_avg:61.03ms
step:1829/1920 train_time:111647ms step_avg:61.04ms
step:1830/1920 train_time:111736ms step_avg:61.06ms
step:1831/1920 train_time:111826ms step_avg:61.07ms
step:1832/1920 train_time:111914ms step_avg:61.09ms
step:1833/1920 train_time:112005ms step_avg:61.10ms
step:1834/1920 train_time:112093ms step_avg:61.12ms
step:1835/1920 train_time:112182ms step_avg:61.13ms
step:1836/1920 train_time:112269ms step_avg:61.15ms
step:1837/1920 train_time:112358ms step_avg:61.16ms
step:1838/1920 train_time:112446ms step_avg:61.18ms
step:1839/1920 train_time:112535ms step_avg:61.19ms
step:1840/1920 train_time:112623ms step_avg:61.21ms
step:1841/1920 train_time:112712ms step_avg:61.22ms
step:1842/1920 train_time:112801ms step_avg:61.24ms
step:1843/1920 train_time:112889ms step_avg:61.25ms
step:1844/1920 train_time:112979ms step_avg:61.27ms
step:1845/1920 train_time:113068ms step_avg:61.28ms
step:1846/1920 train_time:113156ms step_avg:61.30ms
step:1847/1920 train_time:113245ms step_avg:61.31ms
step:1848/1920 train_time:113332ms step_avg:61.33ms
step:1849/1920 train_time:113421ms step_avg:61.34ms
step:1850/1920 train_time:113508ms step_avg:61.36ms
step:1851/1920 train_time:113597ms step_avg:61.37ms
step:1852/1920 train_time:113685ms step_avg:61.39ms
step:1853/1920 train_time:113775ms step_avg:61.40ms
step:1854/1920 train_time:113863ms step_avg:61.41ms
step:1855/1920 train_time:113951ms step_avg:61.43ms
step:1856/1920 train_time:114039ms step_avg:61.44ms
step:1857/1920 train_time:114128ms step_avg:61.46ms
step:1858/1920 train_time:114216ms step_avg:61.47ms
step:1859/1920 train_time:114305ms step_avg:61.49ms
step:1860/1920 train_time:114393ms step_avg:61.50ms
step:1861/1920 train_time:114483ms step_avg:61.52ms
step:1862/1920 train_time:114571ms step_avg:61.53ms
step:1863/1920 train_time:114659ms step_avg:61.55ms
step:1864/1920 train_time:114747ms step_avg:61.56ms
step:1865/1920 train_time:114836ms step_avg:61.57ms
step:1866/1920 train_time:114925ms step_avg:61.59ms
step:1867/1920 train_time:115013ms step_avg:61.60ms
step:1868/1920 train_time:115103ms step_avg:61.62ms
step:1869/1920 train_time:115191ms step_avg:61.63ms
step:1870/1920 train_time:115279ms step_avg:61.65ms
step:1871/1920 train_time:115367ms step_avg:61.66ms
step:1872/1920 train_time:115455ms step_avg:61.67ms
step:1873/1920 train_time:115545ms step_avg:61.69ms
step:1874/1920 train_time:115634ms step_avg:61.70ms
step:1875/1920 train_time:115723ms step_avg:61.72ms
step:1876/1920 train_time:115811ms step_avg:61.73ms
step:1877/1920 train_time:115900ms step_avg:61.75ms
step:1878/1920 train_time:115987ms step_avg:61.76ms
step:1879/1920 train_time:116076ms step_avg:61.78ms
step:1880/1920 train_time:116164ms step_avg:61.79ms
step:1881/1920 train_time:116253ms step_avg:61.80ms
step:1882/1920 train_time:116341ms step_avg:61.82ms
step:1883/1920 train_time:116429ms step_avg:61.83ms
step:1884/1920 train_time:116518ms step_avg:61.85ms
step:1885/1920 train_time:116608ms step_avg:61.86ms
step:1886/1920 train_time:116696ms step_avg:61.87ms
step:1887/1920 train_time:116785ms step_avg:61.89ms
step:1888/1920 train_time:116875ms step_avg:61.90ms
step:1889/1920 train_time:116964ms step_avg:61.92ms
step:1890/1920 train_time:117052ms step_avg:61.93ms
step:1891/1920 train_time:117142ms step_avg:61.95ms
step:1892/1920 train_time:117230ms step_avg:61.96ms
step:1893/1920 train_time:117319ms step_avg:61.97ms
step:1894/1920 train_time:117406ms step_avg:61.99ms
step:1895/1920 train_time:117495ms step_avg:62.00ms
step:1896/1920 train_time:117584ms step_avg:62.02ms
step:1897/1920 train_time:117673ms step_avg:62.03ms
step:1898/1920 train_time:117762ms step_avg:62.05ms
step:1899/1920 train_time:117852ms step_avg:62.06ms
step:1900/1920 train_time:117941ms step_avg:62.07ms
step:1901/1920 train_time:118029ms step_avg:62.09ms
step:1902/1920 train_time:118118ms step_avg:62.10ms
step:1903/1920 train_time:118208ms step_avg:62.12ms
step:1904/1920 train_time:118297ms step_avg:62.13ms
step:1905/1920 train_time:118386ms step_avg:62.14ms
step:1906/1920 train_time:118474ms step_avg:62.16ms
step:1907/1920 train_time:118564ms step_avg:62.17ms
step:1908/1920 train_time:118653ms step_avg:62.19ms
step:1909/1920 train_time:118744ms step_avg:62.20ms
step:1910/1920 train_time:118831ms step_avg:62.22ms
step:1911/1920 train_time:118921ms step_avg:62.23ms
step:1912/1920 train_time:119008ms step_avg:62.24ms
step:1913/1920 train_time:119098ms step_avg:62.26ms
step:1914/1920 train_time:119186ms step_avg:62.27ms
step:1915/1920 train_time:119276ms step_avg:62.28ms
step:1916/1920 train_time:119364ms step_avg:62.30ms
step:1917/1920 train_time:119453ms step_avg:62.31ms
step:1918/1920 train_time:119540ms step_avg:62.33ms
step:1919/1920 train_time:119629ms step_avg:62.34ms
step:1920/1920 train_time:119718ms step_avg:62.35ms
step:1920/1920 val_loss:3.2787 train_time:119811ms step_avg:62.40ms
peak memory allocated: 29863 MiB reserved: 44658 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:01:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    224874      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    224875      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    224876      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    224877      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    224878      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    224879      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    224880      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    224881      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    224875      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    224876      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    224877      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    224878      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    224879      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    224880      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    224881      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8305 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:68ms step_avg:68.09ms
step:2/1920 train_time:89ms step_avg:44.71ms
step:3/1920 train_time:120ms step_avg:39.92ms
step:4/1920 train_time:154ms step_avg:38.41ms
step:5/1920 train_time:188ms step_avg:37.61ms
step:6/1920 train_time:257ms step_avg:42.91ms
step:7/1920 train_time:294ms step_avg:41.99ms
step:8/1920 train_time:328ms step_avg:40.99ms
step:9/1920 train_time:362ms step_avg:40.24ms
step:10/1920 train_time:396ms step_avg:39.62ms
step:11/1920 train_time:431ms step_avg:39.14ms
step:12/1920 train_time:465ms step_avg:38.72ms
step:13/1920 train_time:499ms step_avg:38.38ms
step:14/1920 train_time:533ms step_avg:38.08ms
step:15/1920 train_time:568ms step_avg:37.85ms
step:16/1920 train_time:602ms step_avg:37.62ms
step:17/1920 train_time:637ms step_avg:37.45ms
step:18/1920 train_time:671ms step_avg:37.26ms
step:19/1920 train_time:705ms step_avg:37.12ms
step:20/1920 train_time:740ms step_avg:36.98ms
step:21/1920 train_time:774ms step_avg:36.85ms
step:22/1920 train_time:808ms step_avg:36.72ms
step:23/1920 train_time:842ms step_avg:36.62ms
step:24/1920 train_time:877ms step_avg:36.52ms
step:25/1920 train_time:911ms step_avg:36.43ms
step:26/1920 train_time:945ms step_avg:36.34ms
step:27/1920 train_time:979ms step_avg:36.27ms
step:28/1920 train_time:1013ms step_avg:36.19ms
step:29/1920 train_time:1048ms step_avg:36.14ms
step:30/1920 train_time:1082ms step_avg:36.07ms
step:31/1920 train_time:1117ms step_avg:36.02ms
step:32/1920 train_time:1151ms step_avg:35.96ms
step:33/1920 train_time:1186ms step_avg:35.94ms
step:34/1920 train_time:1221ms step_avg:35.91ms
step:35/1920 train_time:1256ms step_avg:35.89ms
step:36/1920 train_time:1290ms step_avg:35.85ms
step:37/1920 train_time:1326ms step_avg:35.82ms
step:38/1920 train_time:1360ms step_avg:35.79ms
step:39/1920 train_time:1395ms step_avg:35.76ms
step:40/1920 train_time:1429ms step_avg:35.72ms
step:41/1920 train_time:1464ms step_avg:35.70ms
step:42/1920 train_time:1498ms step_avg:35.67ms
step:43/1920 train_time:1532ms step_avg:35.63ms
step:44/1920 train_time:1566ms step_avg:35.60ms
step:45/1920 train_time:1601ms step_avg:35.58ms
step:46/1920 train_time:1635ms step_avg:35.55ms
step:47/1920 train_time:1670ms step_avg:35.53ms
step:48/1920 train_time:1704ms step_avg:35.50ms
step:49/1920 train_time:1739ms step_avg:35.48ms
step:50/1920 train_time:1773ms step_avg:35.45ms
step:51/1920 train_time:1808ms step_avg:35.44ms
step:52/1920 train_time:1842ms step_avg:35.42ms
step:53/1920 train_time:1876ms step_avg:35.40ms
step:54/1920 train_time:1910ms step_avg:35.38ms
step:55/1920 train_time:1945ms step_avg:35.37ms
step:56/1920 train_time:1979ms step_avg:35.34ms
step:57/1920 train_time:2014ms step_avg:35.33ms
step:58/1920 train_time:2048ms step_avg:35.31ms
step:59/1920 train_time:2082ms step_avg:35.30ms
step:60/1920 train_time:2117ms step_avg:35.28ms
step:61/1920 train_time:2151ms step_avg:35.27ms
step:62/1920 train_time:2185ms step_avg:35.25ms
step:63/1920 train_time:2220ms step_avg:35.24ms
step:64/1920 train_time:2255ms step_avg:35.23ms
step:65/1920 train_time:2289ms step_avg:35.22ms
step:66/1920 train_time:2324ms step_avg:35.21ms
step:67/1920 train_time:2358ms step_avg:35.20ms
step:68/1920 train_time:2393ms step_avg:35.18ms
step:69/1920 train_time:2427ms step_avg:35.18ms
step:70/1920 train_time:2462ms step_avg:35.17ms
step:71/1920 train_time:2496ms step_avg:35.16ms
step:72/1920 train_time:2531ms step_avg:35.15ms
step:73/1920 train_time:2565ms step_avg:35.14ms
step:74/1920 train_time:2600ms step_avg:35.13ms
step:75/1920 train_time:2634ms step_avg:35.12ms
step:76/1920 train_time:2669ms step_avg:35.11ms
step:77/1920 train_time:2704ms step_avg:35.11ms
step:78/1920 train_time:2738ms step_avg:35.10ms
step:79/1920 train_time:2772ms step_avg:35.09ms
step:80/1920 train_time:2806ms step_avg:35.08ms
step:81/1920 train_time:2841ms step_avg:35.07ms
step:82/1920 train_time:2875ms step_avg:35.07ms
step:83/1920 train_time:2910ms step_avg:35.06ms
step:84/1920 train_time:2944ms step_avg:35.05ms
step:85/1920 train_time:2979ms step_avg:35.04ms
step:86/1920 train_time:3013ms step_avg:35.03ms
step:87/1920 train_time:3047ms step_avg:35.03ms
step:88/1920 train_time:3082ms step_avg:35.02ms
step:89/1920 train_time:3116ms step_avg:35.01ms
step:90/1920 train_time:3150ms step_avg:35.01ms
step:91/1920 train_time:3185ms step_avg:35.00ms
step:92/1920 train_time:3219ms step_avg:34.99ms
step:93/1920 train_time:3254ms step_avg:34.99ms
step:94/1920 train_time:3288ms step_avg:34.98ms
step:95/1920 train_time:3323ms step_avg:34.98ms
step:96/1920 train_time:3358ms step_avg:34.97ms
step:97/1920 train_time:3392ms step_avg:34.97ms
step:98/1920 train_time:3426ms step_avg:34.96ms
step:99/1920 train_time:3461ms step_avg:34.96ms
step:100/1920 train_time:3496ms step_avg:34.96ms
step:101/1920 train_time:3530ms step_avg:34.95ms
step:102/1920 train_time:3564ms step_avg:34.94ms
step:103/1920 train_time:3598ms step_avg:34.94ms
step:104/1920 train_time:3633ms step_avg:34.93ms
step:105/1920 train_time:3667ms step_avg:34.93ms
step:106/1920 train_time:3702ms step_avg:34.92ms
step:107/1920 train_time:3737ms step_avg:34.92ms
step:108/1920 train_time:3771ms step_avg:34.91ms
step:109/1920 train_time:3805ms step_avg:34.91ms
step:110/1920 train_time:3839ms step_avg:34.90ms
step:111/1920 train_time:3874ms step_avg:34.90ms
step:112/1920 train_time:3908ms step_avg:34.90ms
step:113/1920 train_time:3943ms step_avg:34.90ms
step:114/1920 train_time:3977ms step_avg:34.89ms
step:115/1920 train_time:4012ms step_avg:34.88ms
step:116/1920 train_time:4046ms step_avg:34.88ms
step:117/1920 train_time:4081ms step_avg:34.88ms
step:118/1920 train_time:4115ms step_avg:34.87ms
step:119/1920 train_time:4149ms step_avg:34.87ms
step:120/1920 train_time:4183ms step_avg:34.86ms
step:121/1920 train_time:4218ms step_avg:34.86ms
step:122/1920 train_time:4252ms step_avg:34.85ms
step:123/1920 train_time:4287ms step_avg:34.85ms
step:124/1920 train_time:4321ms step_avg:34.85ms
step:125/1920 train_time:4356ms step_avg:34.84ms
step:126/1920 train_time:4390ms step_avg:34.84ms
step:127/1920 train_time:4425ms step_avg:34.84ms
step:128/1920 train_time:4459ms step_avg:34.83ms
step:129/1920 train_time:4493ms step_avg:34.83ms
step:130/1920 train_time:4528ms step_avg:34.83ms
step:131/1920 train_time:4562ms step_avg:34.83ms
step:132/1920 train_time:4597ms step_avg:34.82ms
step:133/1920 train_time:4631ms step_avg:34.82ms
step:134/1920 train_time:4665ms step_avg:34.82ms
step:135/1920 train_time:4700ms step_avg:34.82ms
step:136/1920 train_time:4734ms step_avg:34.81ms
step:137/1920 train_time:4769ms step_avg:34.81ms
step:138/1920 train_time:4804ms step_avg:34.81ms
step:139/1920 train_time:4838ms step_avg:34.81ms
step:140/1920 train_time:4872ms step_avg:34.80ms
step:141/1920 train_time:4907ms step_avg:34.80ms
step:142/1920 train_time:4941ms step_avg:34.80ms
step:143/1920 train_time:4975ms step_avg:34.79ms
step:144/1920 train_time:5009ms step_avg:34.79ms
step:145/1920 train_time:5044ms step_avg:34.79ms
step:146/1920 train_time:5078ms step_avg:34.78ms
step:147/1920 train_time:5113ms step_avg:34.78ms
step:148/1920 train_time:5147ms step_avg:34.77ms
step:149/1920 train_time:5182ms step_avg:34.78ms
step:150/1920 train_time:5216ms step_avg:34.77ms
step:151/1920 train_time:5251ms step_avg:34.77ms
step:152/1920 train_time:5285ms step_avg:34.77ms
step:153/1920 train_time:5319ms step_avg:34.77ms
step:154/1920 train_time:5353ms step_avg:34.76ms
step:155/1920 train_time:5388ms step_avg:34.76ms
step:156/1920 train_time:5422ms step_avg:34.76ms
step:157/1920 train_time:5457ms step_avg:34.76ms
step:158/1920 train_time:5491ms step_avg:34.75ms
step:159/1920 train_time:5525ms step_avg:34.75ms
step:160/1920 train_time:5559ms step_avg:34.75ms
step:161/1920 train_time:5594ms step_avg:34.75ms
step:162/1920 train_time:5628ms step_avg:34.74ms
step:163/1920 train_time:5663ms step_avg:34.74ms
step:164/1920 train_time:5697ms step_avg:34.74ms
step:165/1920 train_time:5732ms step_avg:34.74ms
step:166/1920 train_time:5766ms step_avg:34.73ms
step:167/1920 train_time:5801ms step_avg:34.74ms
step:168/1920 train_time:5835ms step_avg:34.73ms
step:169/1920 train_time:5870ms step_avg:34.73ms
step:170/1920 train_time:5904ms step_avg:34.73ms
step:171/1920 train_time:5939ms step_avg:34.73ms
step:172/1920 train_time:5973ms step_avg:34.73ms
step:173/1920 train_time:6007ms step_avg:34.72ms
step:174/1920 train_time:6041ms step_avg:34.72ms
step:175/1920 train_time:6076ms step_avg:34.72ms
step:176/1920 train_time:6110ms step_avg:34.72ms
step:177/1920 train_time:6145ms step_avg:34.71ms
step:178/1920 train_time:6179ms step_avg:34.71ms
step:179/1920 train_time:6213ms step_avg:34.71ms
step:180/1920 train_time:6247ms step_avg:34.71ms
step:181/1920 train_time:6282ms step_avg:34.71ms
step:182/1920 train_time:6316ms step_avg:34.71ms
step:183/1920 train_time:6351ms step_avg:34.71ms
step:184/1920 train_time:6385ms step_avg:34.70ms
step:185/1920 train_time:6420ms step_avg:34.70ms
step:186/1920 train_time:6454ms step_avg:34.70ms
step:187/1920 train_time:6489ms step_avg:34.70ms
step:188/1920 train_time:6523ms step_avg:34.70ms
step:189/1920 train_time:6557ms step_avg:34.69ms
step:190/1920 train_time:6591ms step_avg:34.69ms
step:191/1920 train_time:6626ms step_avg:34.69ms
step:192/1920 train_time:6660ms step_avg:34.69ms
step:193/1920 train_time:6694ms step_avg:34.69ms
step:194/1920 train_time:6729ms step_avg:34.68ms
step:195/1920 train_time:6764ms step_avg:34.69ms
step:196/1920 train_time:6798ms step_avg:34.68ms
step:197/1920 train_time:6832ms step_avg:34.68ms
step:198/1920 train_time:6867ms step_avg:34.68ms
step:199/1920 train_time:6902ms step_avg:34.68ms
step:200/1920 train_time:6936ms step_avg:34.68ms
step:201/1920 train_time:6971ms step_avg:34.68ms
step:202/1920 train_time:7005ms step_avg:34.68ms
step:203/1920 train_time:7040ms step_avg:34.68ms
step:204/1920 train_time:7074ms step_avg:34.68ms
step:205/1920 train_time:7109ms step_avg:34.68ms
step:206/1920 train_time:7143ms step_avg:34.67ms
step:207/1920 train_time:7177ms step_avg:34.67ms
step:208/1920 train_time:7211ms step_avg:34.67ms
step:209/1920 train_time:7246ms step_avg:34.67ms
step:210/1920 train_time:7280ms step_avg:34.67ms
step:211/1920 train_time:7314ms step_avg:34.66ms
step:212/1920 train_time:7349ms step_avg:34.66ms
step:213/1920 train_time:7383ms step_avg:34.66ms
step:214/1920 train_time:7417ms step_avg:34.66ms
step:215/1920 train_time:7451ms step_avg:34.66ms
step:216/1920 train_time:7485ms step_avg:34.65ms
step:217/1920 train_time:7519ms step_avg:34.65ms
step:218/1920 train_time:7554ms step_avg:34.65ms
step:219/1920 train_time:7588ms step_avg:34.65ms
step:220/1920 train_time:7622ms step_avg:34.65ms
step:221/1920 train_time:7656ms step_avg:34.64ms
step:222/1920 train_time:7690ms step_avg:34.64ms
step:223/1920 train_time:7725ms step_avg:34.64ms
step:224/1920 train_time:7759ms step_avg:34.64ms
step:225/1920 train_time:7794ms step_avg:34.64ms
step:226/1920 train_time:7828ms step_avg:34.64ms
step:227/1920 train_time:7863ms step_avg:34.64ms
step:228/1920 train_time:7897ms step_avg:34.64ms
step:229/1920 train_time:7932ms step_avg:34.64ms
step:230/1920 train_time:7966ms step_avg:34.64ms
step:231/1920 train_time:8001ms step_avg:34.64ms
step:232/1920 train_time:8035ms step_avg:34.64ms
step:233/1920 train_time:8070ms step_avg:34.64ms
step:234/1920 train_time:8104ms step_avg:34.63ms
step:235/1920 train_time:8139ms step_avg:34.63ms
step:236/1920 train_time:8173ms step_avg:34.63ms
step:237/1920 train_time:8208ms step_avg:34.63ms
step:238/1920 train_time:8242ms step_avg:34.63ms
step:239/1920 train_time:8276ms step_avg:34.63ms
step:240/1920 train_time:8310ms step_avg:34.63ms
step:241/1920 train_time:8345ms step_avg:34.63ms
step:242/1920 train_time:8379ms step_avg:34.62ms
step:243/1920 train_time:8413ms step_avg:34.62ms
step:244/1920 train_time:8447ms step_avg:34.62ms
step:245/1920 train_time:8482ms step_avg:34.62ms
step:246/1920 train_time:8516ms step_avg:34.62ms
step:247/1920 train_time:8551ms step_avg:34.62ms
step:248/1920 train_time:8585ms step_avg:34.62ms
step:249/1920 train_time:8620ms step_avg:34.62ms
step:250/1920 train_time:8654ms step_avg:34.62ms
step:250/1920 val_loss:4.6060 train_time:8692ms step_avg:34.77ms
step:251/1920 train_time:8709ms step_avg:34.70ms
step:252/1920 train_time:8726ms step_avg:34.63ms
step:253/1920 train_time:8760ms step_avg:34.63ms
step:254/1920 train_time:8795ms step_avg:34.63ms
step:255/1920 train_time:8832ms step_avg:34.63ms
step:256/1920 train_time:8867ms step_avg:34.64ms
step:257/1920 train_time:8902ms step_avg:34.64ms
step:258/1920 train_time:8937ms step_avg:34.64ms
step:259/1920 train_time:8972ms step_avg:34.64ms
step:260/1920 train_time:9006ms step_avg:34.64ms
step:261/1920 train_time:9040ms step_avg:34.64ms
step:262/1920 train_time:9074ms step_avg:34.63ms
step:263/1920 train_time:9109ms step_avg:34.63ms
step:264/1920 train_time:9143ms step_avg:34.63ms
step:265/1920 train_time:9178ms step_avg:34.63ms
step:266/1920 train_time:9212ms step_avg:34.63ms
step:267/1920 train_time:9246ms step_avg:34.63ms
step:268/1920 train_time:9280ms step_avg:34.63ms
step:269/1920 train_time:9314ms step_avg:34.62ms
step:270/1920 train_time:9348ms step_avg:34.62ms
step:271/1920 train_time:9382ms step_avg:34.62ms
step:272/1920 train_time:9416ms step_avg:34.62ms
step:273/1920 train_time:9450ms step_avg:34.62ms
step:274/1920 train_time:9484ms step_avg:34.61ms
step:275/1920 train_time:9519ms step_avg:34.61ms
step:276/1920 train_time:9553ms step_avg:34.61ms
step:277/1920 train_time:9587ms step_avg:34.61ms
step:278/1920 train_time:9621ms step_avg:34.61ms
step:279/1920 train_time:9655ms step_avg:34.61ms
step:280/1920 train_time:9689ms step_avg:34.61ms
step:281/1920 train_time:9724ms step_avg:34.60ms
step:282/1920 train_time:9759ms step_avg:34.61ms
step:283/1920 train_time:9793ms step_avg:34.60ms
step:284/1920 train_time:9828ms step_avg:34.60ms
step:285/1920 train_time:9862ms step_avg:34.60ms
step:286/1920 train_time:9897ms step_avg:34.60ms
step:287/1920 train_time:9932ms step_avg:34.60ms
step:288/1920 train_time:9966ms step_avg:34.60ms
step:289/1920 train_time:10001ms step_avg:34.60ms
step:290/1920 train_time:10035ms step_avg:34.60ms
step:291/1920 train_time:10069ms step_avg:34.60ms
step:292/1920 train_time:10103ms step_avg:34.60ms
step:293/1920 train_time:10138ms step_avg:34.60ms
step:294/1920 train_time:10172ms step_avg:34.60ms
step:295/1920 train_time:10206ms step_avg:34.60ms
step:296/1920 train_time:10240ms step_avg:34.60ms
step:297/1920 train_time:10275ms step_avg:34.60ms
step:298/1920 train_time:10309ms step_avg:34.59ms
step:299/1920 train_time:10343ms step_avg:34.59ms
step:300/1920 train_time:10377ms step_avg:34.59ms
step:301/1920 train_time:10411ms step_avg:34.59ms
step:302/1920 train_time:10446ms step_avg:34.59ms
step:303/1920 train_time:10480ms step_avg:34.59ms
step:304/1920 train_time:10514ms step_avg:34.58ms
step:305/1920 train_time:10548ms step_avg:34.58ms
step:306/1920 train_time:10582ms step_avg:34.58ms
step:307/1920 train_time:10616ms step_avg:34.58ms
step:308/1920 train_time:10650ms step_avg:34.58ms
step:309/1920 train_time:10685ms step_avg:34.58ms
step:310/1920 train_time:10719ms step_avg:34.58ms
step:311/1920 train_time:10753ms step_avg:34.58ms
step:312/1920 train_time:10787ms step_avg:34.58ms
step:313/1920 train_time:10822ms step_avg:34.57ms
step:314/1920 train_time:10856ms step_avg:34.57ms
step:315/1920 train_time:10891ms step_avg:34.57ms
step:316/1920 train_time:10925ms step_avg:34.57ms
step:317/1920 train_time:10959ms step_avg:34.57ms
step:318/1920 train_time:10994ms step_avg:34.57ms
step:319/1920 train_time:11028ms step_avg:34.57ms
step:320/1920 train_time:11062ms step_avg:34.57ms
step:321/1920 train_time:11097ms step_avg:34.57ms
step:322/1920 train_time:11131ms step_avg:34.57ms
step:323/1920 train_time:11165ms step_avg:34.57ms
step:324/1920 train_time:11199ms step_avg:34.57ms
step:325/1920 train_time:11233ms step_avg:34.56ms
step:326/1920 train_time:11268ms step_avg:34.56ms
step:327/1920 train_time:11302ms step_avg:34.56ms
step:328/1920 train_time:11336ms step_avg:34.56ms
step:329/1920 train_time:11370ms step_avg:34.56ms
step:330/1920 train_time:11404ms step_avg:34.56ms
step:331/1920 train_time:11439ms step_avg:34.56ms
step:332/1920 train_time:11473ms step_avg:34.56ms
step:333/1920 train_time:11507ms step_avg:34.56ms
step:334/1920 train_time:11541ms step_avg:34.55ms
step:335/1920 train_time:11575ms step_avg:34.55ms
step:336/1920 train_time:11609ms step_avg:34.55ms
step:337/1920 train_time:11644ms step_avg:34.55ms
step:338/1920 train_time:11678ms step_avg:34.55ms
step:339/1920 train_time:11712ms step_avg:34.55ms
step:340/1920 train_time:11746ms step_avg:34.55ms
step:341/1920 train_time:11781ms step_avg:34.55ms
step:342/1920 train_time:11815ms step_avg:34.55ms
step:343/1920 train_time:11849ms step_avg:34.54ms
step:344/1920 train_time:11883ms step_avg:34.54ms
step:345/1920 train_time:11917ms step_avg:34.54ms
step:346/1920 train_time:11952ms step_avg:34.54ms
step:347/1920 train_time:11986ms step_avg:34.54ms
step:348/1920 train_time:12020ms step_avg:34.54ms
step:349/1920 train_time:12055ms step_avg:34.54ms
step:350/1920 train_time:12089ms step_avg:34.54ms
step:351/1920 train_time:12123ms step_avg:34.54ms
step:352/1920 train_time:12158ms step_avg:34.54ms
step:353/1920 train_time:12192ms step_avg:34.54ms
step:354/1920 train_time:12227ms step_avg:34.54ms
step:355/1920 train_time:12261ms step_avg:34.54ms
step:356/1920 train_time:12295ms step_avg:34.54ms
step:357/1920 train_time:12330ms step_avg:34.54ms
step:358/1920 train_time:12364ms step_avg:34.54ms
step:359/1920 train_time:12399ms step_avg:34.54ms
step:360/1920 train_time:12433ms step_avg:34.54ms
step:361/1920 train_time:12467ms step_avg:34.53ms
step:362/1920 train_time:12501ms step_avg:34.53ms
step:363/1920 train_time:12535ms step_avg:34.53ms
step:364/1920 train_time:12569ms step_avg:34.53ms
step:365/1920 train_time:12603ms step_avg:34.53ms
step:366/1920 train_time:12638ms step_avg:34.53ms
step:367/1920 train_time:12672ms step_avg:34.53ms
step:368/1920 train_time:12706ms step_avg:34.53ms
step:369/1920 train_time:12741ms step_avg:34.53ms
step:370/1920 train_time:12775ms step_avg:34.53ms
step:371/1920 train_time:12810ms step_avg:34.53ms
step:372/1920 train_time:12844ms step_avg:34.53ms
step:373/1920 train_time:12878ms step_avg:34.53ms
step:374/1920 train_time:12913ms step_avg:34.53ms
step:375/1920 train_time:12947ms step_avg:34.53ms
step:376/1920 train_time:12981ms step_avg:34.52ms
step:377/1920 train_time:13015ms step_avg:34.52ms
step:378/1920 train_time:13050ms step_avg:34.52ms
step:379/1920 train_time:13084ms step_avg:34.52ms
step:380/1920 train_time:13119ms step_avg:34.52ms
step:381/1920 train_time:13154ms step_avg:34.52ms
step:382/1920 train_time:13188ms step_avg:34.52ms
step:383/1920 train_time:13222ms step_avg:34.52ms
step:384/1920 train_time:13256ms step_avg:34.52ms
step:385/1920 train_time:13291ms step_avg:34.52ms
step:386/1920 train_time:13325ms step_avg:34.52ms
step:387/1920 train_time:13360ms step_avg:34.52ms
step:388/1920 train_time:13394ms step_avg:34.52ms
step:389/1920 train_time:13428ms step_avg:34.52ms
step:390/1920 train_time:13462ms step_avg:34.52ms
step:391/1920 train_time:13497ms step_avg:34.52ms
step:392/1920 train_time:13531ms step_avg:34.52ms
step:393/1920 train_time:13565ms step_avg:34.52ms
step:394/1920 train_time:13599ms step_avg:34.51ms
step:395/1920 train_time:13633ms step_avg:34.51ms
step:396/1920 train_time:13667ms step_avg:34.51ms
step:397/1920 train_time:13702ms step_avg:34.51ms
step:398/1920 train_time:13736ms step_avg:34.51ms
step:399/1920 train_time:13770ms step_avg:34.51ms
step:400/1920 train_time:13804ms step_avg:34.51ms
step:401/1920 train_time:13839ms step_avg:34.51ms
step:402/1920 train_time:13873ms step_avg:34.51ms
step:403/1920 train_time:13907ms step_avg:34.51ms
step:404/1920 train_time:13941ms step_avg:34.51ms
step:405/1920 train_time:13975ms step_avg:34.51ms
step:406/1920 train_time:14010ms step_avg:34.51ms
step:407/1920 train_time:14044ms step_avg:34.51ms
step:408/1920 train_time:14078ms step_avg:34.51ms
step:409/1920 train_time:14113ms step_avg:34.51ms
step:410/1920 train_time:14147ms step_avg:34.51ms
step:411/1920 train_time:14182ms step_avg:34.50ms
step:412/1920 train_time:14216ms step_avg:34.50ms
step:413/1920 train_time:14250ms step_avg:34.50ms
step:414/1920 train_time:14285ms step_avg:34.50ms
step:415/1920 train_time:14319ms step_avg:34.50ms
step:416/1920 train_time:14353ms step_avg:34.50ms
step:417/1920 train_time:14388ms step_avg:34.50ms
step:418/1920 train_time:14422ms step_avg:34.50ms
step:419/1920 train_time:14456ms step_avg:34.50ms
step:420/1920 train_time:14491ms step_avg:34.50ms
step:421/1920 train_time:14525ms step_avg:34.50ms
step:422/1920 train_time:14559ms step_avg:34.50ms
step:423/1920 train_time:14593ms step_avg:34.50ms
step:424/1920 train_time:14628ms step_avg:34.50ms
step:425/1920 train_time:14662ms step_avg:34.50ms
step:426/1920 train_time:14696ms step_avg:34.50ms
step:427/1920 train_time:14731ms step_avg:34.50ms
step:428/1920 train_time:14765ms step_avg:34.50ms
step:429/1920 train_time:14799ms step_avg:34.50ms
step:430/1920 train_time:14833ms step_avg:34.50ms
step:431/1920 train_time:14868ms step_avg:34.50ms
step:432/1920 train_time:14902ms step_avg:34.50ms
step:433/1920 train_time:14936ms step_avg:34.50ms
step:434/1920 train_time:14971ms step_avg:34.49ms
step:435/1920 train_time:15005ms step_avg:34.49ms
step:436/1920 train_time:15039ms step_avg:34.49ms
step:437/1920 train_time:15074ms step_avg:34.49ms
step:438/1920 train_time:15108ms step_avg:34.49ms
step:439/1920 train_time:15142ms step_avg:34.49ms
step:440/1920 train_time:15176ms step_avg:34.49ms
step:441/1920 train_time:15210ms step_avg:34.49ms
step:442/1920 train_time:15244ms step_avg:34.49ms
step:443/1920 train_time:15279ms step_avg:34.49ms
step:444/1920 train_time:15312ms step_avg:34.49ms
step:445/1920 train_time:15347ms step_avg:34.49ms
step:446/1920 train_time:15381ms step_avg:34.49ms
step:447/1920 train_time:15415ms step_avg:34.49ms
step:448/1920 train_time:15449ms step_avg:34.49ms
step:449/1920 train_time:15484ms step_avg:34.49ms
step:450/1920 train_time:15518ms step_avg:34.48ms
step:451/1920 train_time:15553ms step_avg:34.49ms
step:452/1920 train_time:15587ms step_avg:34.48ms
step:453/1920 train_time:15621ms step_avg:34.48ms
step:454/1920 train_time:15655ms step_avg:34.48ms
step:455/1920 train_time:15690ms step_avg:34.48ms
step:456/1920 train_time:15724ms step_avg:34.48ms
step:457/1920 train_time:15759ms step_avg:34.48ms
step:458/1920 train_time:15793ms step_avg:34.48ms
step:459/1920 train_time:15828ms step_avg:34.48ms
step:460/1920 train_time:15862ms step_avg:34.48ms
step:461/1920 train_time:15896ms step_avg:34.48ms
step:462/1920 train_time:15930ms step_avg:34.48ms
step:463/1920 train_time:15965ms step_avg:34.48ms
step:464/1920 train_time:15999ms step_avg:34.48ms
step:465/1920 train_time:16034ms step_avg:34.48ms
step:466/1920 train_time:16068ms step_avg:34.48ms
step:467/1920 train_time:16103ms step_avg:34.48ms
step:468/1920 train_time:16137ms step_avg:34.48ms
step:469/1920 train_time:16171ms step_avg:34.48ms
step:470/1920 train_time:16205ms step_avg:34.48ms
step:471/1920 train_time:16240ms step_avg:34.48ms
step:472/1920 train_time:16274ms step_avg:34.48ms
step:473/1920 train_time:16308ms step_avg:34.48ms
step:474/1920 train_time:16342ms step_avg:34.48ms
step:475/1920 train_time:16377ms step_avg:34.48ms
step:476/1920 train_time:16411ms step_avg:34.48ms
step:477/1920 train_time:16445ms step_avg:34.48ms
step:478/1920 train_time:16479ms step_avg:34.48ms
step:479/1920 train_time:16514ms step_avg:34.48ms
step:480/1920 train_time:16548ms step_avg:34.48ms
step:481/1920 train_time:16583ms step_avg:34.48ms
step:482/1920 train_time:16617ms step_avg:34.47ms
step:483/1920 train_time:16652ms step_avg:34.48ms
step:484/1920 train_time:16686ms step_avg:34.47ms
step:485/1920 train_time:16720ms step_avg:34.47ms
step:486/1920 train_time:16754ms step_avg:34.47ms
step:487/1920 train_time:16789ms step_avg:34.47ms
step:488/1920 train_time:16823ms step_avg:34.47ms
step:489/1920 train_time:16858ms step_avg:34.47ms
step:490/1920 train_time:16892ms step_avg:34.47ms
step:491/1920 train_time:16926ms step_avg:34.47ms
step:492/1920 train_time:16960ms step_avg:34.47ms
step:493/1920 train_time:16994ms step_avg:34.47ms
step:494/1920 train_time:17029ms step_avg:34.47ms
step:495/1920 train_time:17063ms step_avg:34.47ms
step:496/1920 train_time:17097ms step_avg:34.47ms
step:497/1920 train_time:17131ms step_avg:34.47ms
step:498/1920 train_time:17166ms step_avg:34.47ms
step:499/1920 train_time:17200ms step_avg:34.47ms
step:500/1920 train_time:17234ms step_avg:34.47ms
step:500/1920 val_loss:4.2962 train_time:17272ms step_avg:34.54ms
step:501/1920 train_time:17289ms step_avg:34.51ms
step:502/1920 train_time:17307ms step_avg:34.48ms
step:503/1920 train_time:17341ms step_avg:34.48ms
step:504/1920 train_time:17376ms step_avg:34.48ms
step:505/1920 train_time:17413ms step_avg:34.48ms
step:506/1920 train_time:17449ms step_avg:34.48ms
step:507/1920 train_time:17484ms step_avg:34.49ms
step:508/1920 train_time:17519ms step_avg:34.49ms
step:509/1920 train_time:17553ms step_avg:34.49ms
step:510/1920 train_time:17587ms step_avg:34.49ms
step:511/1920 train_time:17622ms step_avg:34.49ms
step:512/1920 train_time:17656ms step_avg:34.48ms
step:513/1920 train_time:17690ms step_avg:34.48ms
step:514/1920 train_time:17724ms step_avg:34.48ms
step:515/1920 train_time:17758ms step_avg:34.48ms
step:516/1920 train_time:17792ms step_avg:34.48ms
step:517/1920 train_time:17827ms step_avg:34.48ms
step:518/1920 train_time:17860ms step_avg:34.48ms
step:519/1920 train_time:17895ms step_avg:34.48ms
step:520/1920 train_time:17928ms step_avg:34.48ms
step:521/1920 train_time:17963ms step_avg:34.48ms
step:522/1920 train_time:17997ms step_avg:34.48ms
step:523/1920 train_time:18031ms step_avg:34.48ms
step:524/1920 train_time:18065ms step_avg:34.47ms
step:525/1920 train_time:18099ms step_avg:34.47ms
step:526/1920 train_time:18133ms step_avg:34.47ms
step:527/1920 train_time:18168ms step_avg:34.47ms
step:528/1920 train_time:18202ms step_avg:34.47ms
step:529/1920 train_time:18236ms step_avg:34.47ms
step:530/1920 train_time:18270ms step_avg:34.47ms
step:531/1920 train_time:18304ms step_avg:34.47ms
step:532/1920 train_time:18339ms step_avg:34.47ms
step:533/1920 train_time:18373ms step_avg:34.47ms
step:534/1920 train_time:18407ms step_avg:34.47ms
step:535/1920 train_time:18443ms step_avg:34.47ms
step:536/1920 train_time:18477ms step_avg:34.47ms
step:537/1920 train_time:18512ms step_avg:34.47ms
step:538/1920 train_time:18546ms step_avg:34.47ms
step:539/1920 train_time:18581ms step_avg:34.47ms
step:540/1920 train_time:18615ms step_avg:34.47ms
step:541/1920 train_time:18649ms step_avg:34.47ms
step:542/1920 train_time:18683ms step_avg:34.47ms
step:543/1920 train_time:18718ms step_avg:34.47ms
step:544/1920 train_time:18752ms step_avg:34.47ms
step:545/1920 train_time:18786ms step_avg:34.47ms
step:546/1920 train_time:18820ms step_avg:34.47ms
step:547/1920 train_time:18854ms step_avg:34.47ms
step:548/1920 train_time:18888ms step_avg:34.47ms
step:549/1920 train_time:18923ms step_avg:34.47ms
step:550/1920 train_time:18957ms step_avg:34.47ms
step:551/1920 train_time:18991ms step_avg:34.47ms
step:552/1920 train_time:19025ms step_avg:34.47ms
step:553/1920 train_time:19059ms step_avg:34.47ms
step:554/1920 train_time:19093ms step_avg:34.46ms
step:555/1920 train_time:19127ms step_avg:34.46ms
step:556/1920 train_time:19161ms step_avg:34.46ms
step:557/1920 train_time:19195ms step_avg:34.46ms
step:558/1920 train_time:19229ms step_avg:34.46ms
step:559/1920 train_time:19264ms step_avg:34.46ms
step:560/1920 train_time:19298ms step_avg:34.46ms
step:561/1920 train_time:19332ms step_avg:34.46ms
step:562/1920 train_time:19367ms step_avg:34.46ms
step:563/1920 train_time:19402ms step_avg:34.46ms
step:564/1920 train_time:19436ms step_avg:34.46ms
step:565/1920 train_time:19470ms step_avg:34.46ms
step:566/1920 train_time:19504ms step_avg:34.46ms
step:567/1920 train_time:19539ms step_avg:34.46ms
step:568/1920 train_time:19574ms step_avg:34.46ms
step:569/1920 train_time:19608ms step_avg:34.46ms
step:570/1920 train_time:19642ms step_avg:34.46ms
step:571/1920 train_time:19677ms step_avg:34.46ms
step:572/1920 train_time:19711ms step_avg:34.46ms
step:573/1920 train_time:19746ms step_avg:34.46ms
step:574/1920 train_time:19780ms step_avg:34.46ms
step:575/1920 train_time:19814ms step_avg:34.46ms
step:576/1920 train_time:19848ms step_avg:34.46ms
step:577/1920 train_time:19883ms step_avg:34.46ms
step:578/1920 train_time:19917ms step_avg:34.46ms
step:579/1920 train_time:19951ms step_avg:34.46ms
step:580/1920 train_time:19985ms step_avg:34.46ms
step:581/1920 train_time:20019ms step_avg:34.46ms
step:582/1920 train_time:20054ms step_avg:34.46ms
step:583/1920 train_time:20088ms step_avg:34.46ms
step:584/1920 train_time:20122ms step_avg:34.46ms
step:585/1920 train_time:20156ms step_avg:34.45ms
step:586/1920 train_time:20190ms step_avg:34.45ms
step:587/1920 train_time:20224ms step_avg:34.45ms
step:588/1920 train_time:20259ms step_avg:34.45ms
step:589/1920 train_time:20293ms step_avg:34.45ms
step:590/1920 train_time:20327ms step_avg:34.45ms
step:591/1920 train_time:20362ms step_avg:34.45ms
step:592/1920 train_time:20396ms step_avg:34.45ms
step:593/1920 train_time:20431ms step_avg:34.45ms
step:594/1920 train_time:20465ms step_avg:34.45ms
step:595/1920 train_time:20500ms step_avg:34.45ms
step:596/1920 train_time:20534ms step_avg:34.45ms
step:597/1920 train_time:20568ms step_avg:34.45ms
step:598/1920 train_time:20603ms step_avg:34.45ms
step:599/1920 train_time:20637ms step_avg:34.45ms
step:600/1920 train_time:20671ms step_avg:34.45ms
step:601/1920 train_time:20705ms step_avg:34.45ms
step:602/1920 train_time:20739ms step_avg:34.45ms
step:603/1920 train_time:20774ms step_avg:34.45ms
step:604/1920 train_time:20808ms step_avg:34.45ms
step:605/1920 train_time:20842ms step_avg:34.45ms
step:606/1920 train_time:20877ms step_avg:34.45ms
step:607/1920 train_time:20911ms step_avg:34.45ms
step:608/1920 train_time:20945ms step_avg:34.45ms
step:609/1920 train_time:20979ms step_avg:34.45ms
step:610/1920 train_time:21013ms step_avg:34.45ms
step:611/1920 train_time:21048ms step_avg:34.45ms
step:612/1920 train_time:21082ms step_avg:34.45ms
step:613/1920 train_time:21117ms step_avg:34.45ms
step:614/1920 train_time:21151ms step_avg:34.45ms
step:615/1920 train_time:21185ms step_avg:34.45ms
step:616/1920 train_time:21219ms step_avg:34.45ms
step:617/1920 train_time:21254ms step_avg:34.45ms
step:618/1920 train_time:21288ms step_avg:34.45ms
step:619/1920 train_time:21322ms step_avg:34.45ms
step:620/1920 train_time:21356ms step_avg:34.45ms
step:621/1920 train_time:21390ms step_avg:34.45ms
step:622/1920 train_time:21425ms step_avg:34.44ms
step:623/1920 train_time:21459ms step_avg:34.45ms
step:624/1920 train_time:21494ms step_avg:34.45ms
step:625/1920 train_time:21528ms step_avg:34.44ms
step:626/1920 train_time:21562ms step_avg:34.44ms
step:627/1920 train_time:21597ms step_avg:34.44ms
step:628/1920 train_time:21632ms step_avg:34.45ms
step:629/1920 train_time:21694ms step_avg:34.49ms
step:630/1920 train_time:21755ms step_avg:34.53ms
step:631/1920 train_time:21818ms step_avg:34.58ms
step:632/1920 train_time:21879ms step_avg:34.62ms
step:633/1920 train_time:21941ms step_avg:34.66ms
step:634/1920 train_time:22003ms step_avg:34.71ms
step:635/1920 train_time:22065ms step_avg:34.75ms
step:636/1920 train_time:22127ms step_avg:34.79ms
step:637/1920 train_time:22190ms step_avg:34.84ms
step:638/1920 train_time:22252ms step_avg:34.88ms
step:639/1920 train_time:22314ms step_avg:34.92ms
step:640/1920 train_time:22376ms step_avg:34.96ms
step:641/1920 train_time:22439ms step_avg:35.01ms
step:642/1920 train_time:22501ms step_avg:35.05ms
step:643/1920 train_time:22563ms step_avg:35.09ms
step:644/1920 train_time:22624ms step_avg:35.13ms
step:645/1920 train_time:22687ms step_avg:35.17ms
step:646/1920 train_time:22750ms step_avg:35.22ms
step:647/1920 train_time:22813ms step_avg:35.26ms
step:648/1920 train_time:22875ms step_avg:35.30ms
step:649/1920 train_time:22937ms step_avg:35.34ms
step:650/1920 train_time:22999ms step_avg:35.38ms
step:651/1920 train_time:23061ms step_avg:35.42ms
step:652/1920 train_time:23123ms step_avg:35.46ms
step:653/1920 train_time:23185ms step_avg:35.51ms
step:654/1920 train_time:23247ms step_avg:35.55ms
step:655/1920 train_time:23310ms step_avg:35.59ms
step:656/1920 train_time:23372ms step_avg:35.63ms
step:657/1920 train_time:23435ms step_avg:35.67ms
step:658/1920 train_time:23497ms step_avg:35.71ms
step:659/1920 train_time:23559ms step_avg:35.75ms
step:660/1920 train_time:23621ms step_avg:35.79ms
step:661/1920 train_time:23683ms step_avg:35.83ms
step:662/1920 train_time:23745ms step_avg:35.87ms
step:663/1920 train_time:23808ms step_avg:35.91ms
step:664/1920 train_time:23871ms step_avg:35.95ms
step:665/1920 train_time:23934ms step_avg:35.99ms
step:666/1920 train_time:23996ms step_avg:36.03ms
step:667/1920 train_time:24059ms step_avg:36.07ms
step:668/1920 train_time:24121ms step_avg:36.11ms
step:669/1920 train_time:24184ms step_avg:36.15ms
step:670/1920 train_time:24246ms step_avg:36.19ms
step:671/1920 train_time:24308ms step_avg:36.23ms
step:672/1920 train_time:24371ms step_avg:36.27ms
step:673/1920 train_time:24434ms step_avg:36.31ms
step:674/1920 train_time:24495ms step_avg:36.34ms
step:675/1920 train_time:24558ms step_avg:36.38ms
step:676/1920 train_time:24620ms step_avg:36.42ms
step:677/1920 train_time:24682ms step_avg:36.46ms
step:678/1920 train_time:24743ms step_avg:36.49ms
step:679/1920 train_time:24806ms step_avg:36.53ms
step:680/1920 train_time:24868ms step_avg:36.57ms
step:681/1920 train_time:24932ms step_avg:36.61ms
step:682/1920 train_time:24994ms step_avg:36.65ms
step:683/1920 train_time:25057ms step_avg:36.69ms
step:684/1920 train_time:25119ms step_avg:36.72ms
step:685/1920 train_time:25181ms step_avg:36.76ms
step:686/1920 train_time:25243ms step_avg:36.80ms
step:687/1920 train_time:25305ms step_avg:36.83ms
step:688/1920 train_time:25367ms step_avg:36.87ms
step:689/1920 train_time:25429ms step_avg:36.91ms
step:690/1920 train_time:25492ms step_avg:36.94ms
step:691/1920 train_time:25555ms step_avg:36.98ms
step:692/1920 train_time:25617ms step_avg:37.02ms
step:693/1920 train_time:25680ms step_avg:37.06ms
step:694/1920 train_time:25741ms step_avg:37.09ms
step:695/1920 train_time:25804ms step_avg:37.13ms
step:696/1920 train_time:25865ms step_avg:37.16ms
step:697/1920 train_time:25928ms step_avg:37.20ms
step:698/1920 train_time:25990ms step_avg:37.24ms
step:699/1920 train_time:26053ms step_avg:37.27ms
step:700/1920 train_time:26116ms step_avg:37.31ms
step:701/1920 train_time:26179ms step_avg:37.34ms
step:702/1920 train_time:26240ms step_avg:37.38ms
step:703/1920 train_time:26302ms step_avg:37.41ms
step:704/1920 train_time:26364ms step_avg:37.45ms
step:705/1920 train_time:26427ms step_avg:37.48ms
step:706/1920 train_time:26489ms step_avg:37.52ms
step:707/1920 train_time:26552ms step_avg:37.56ms
step:708/1920 train_time:26615ms step_avg:37.59ms
step:709/1920 train_time:26677ms step_avg:37.63ms
step:710/1920 train_time:26739ms step_avg:37.66ms
step:711/1920 train_time:26802ms step_avg:37.70ms
step:712/1920 train_time:26863ms step_avg:37.73ms
step:713/1920 train_time:26926ms step_avg:37.76ms
step:714/1920 train_time:26988ms step_avg:37.80ms
step:715/1920 train_time:27051ms step_avg:37.83ms
step:716/1920 train_time:27114ms step_avg:37.87ms
step:717/1920 train_time:27177ms step_avg:37.90ms
step:718/1920 train_time:27238ms step_avg:37.94ms
step:719/1920 train_time:27301ms step_avg:37.97ms
step:720/1920 train_time:27362ms step_avg:38.00ms
step:721/1920 train_time:27425ms step_avg:38.04ms
step:722/1920 train_time:27487ms step_avg:38.07ms
step:723/1920 train_time:27550ms step_avg:38.11ms
step:724/1920 train_time:27612ms step_avg:38.14ms
step:725/1920 train_time:27675ms step_avg:38.17ms
step:726/1920 train_time:27737ms step_avg:38.20ms
step:727/1920 train_time:27799ms step_avg:38.24ms
step:728/1920 train_time:27860ms step_avg:38.27ms
step:729/1920 train_time:27923ms step_avg:38.30ms
step:730/1920 train_time:27985ms step_avg:38.34ms
step:731/1920 train_time:28048ms step_avg:38.37ms
step:732/1920 train_time:28110ms step_avg:38.40ms
step:733/1920 train_time:28174ms step_avg:38.44ms
step:734/1920 train_time:28237ms step_avg:38.47ms
step:735/1920 train_time:28300ms step_avg:38.50ms
step:736/1920 train_time:28361ms step_avg:38.53ms
step:737/1920 train_time:28423ms step_avg:38.57ms
step:738/1920 train_time:28485ms step_avg:38.60ms
step:739/1920 train_time:28547ms step_avg:38.63ms
step:740/1920 train_time:28609ms step_avg:38.66ms
step:741/1920 train_time:28672ms step_avg:38.69ms
step:742/1920 train_time:28734ms step_avg:38.72ms
step:743/1920 train_time:28797ms step_avg:38.76ms
step:744/1920 train_time:28858ms step_avg:38.79ms
step:745/1920 train_time:28921ms step_avg:38.82ms
step:746/1920 train_time:28983ms step_avg:38.85ms
step:747/1920 train_time:29046ms step_avg:38.88ms
step:748/1920 train_time:29108ms step_avg:38.91ms
step:749/1920 train_time:29171ms step_avg:38.95ms
step:750/1920 train_time:29234ms step_avg:38.98ms
step:750/1920 val_loss:4.0452 train_time:29299ms step_avg:39.07ms
step:751/1920 train_time:29317ms step_avg:39.04ms
step:752/1920 train_time:29360ms step_avg:39.04ms
step:753/1920 train_time:29427ms step_avg:39.08ms
step:754/1920 train_time:29495ms step_avg:39.12ms
step:755/1920 train_time:29559ms step_avg:39.15ms
step:756/1920 train_time:29620ms step_avg:39.18ms
step:757/1920 train_time:29683ms step_avg:39.21ms
step:758/1920 train_time:29744ms step_avg:39.24ms
step:759/1920 train_time:29805ms step_avg:39.27ms
step:760/1920 train_time:29866ms step_avg:39.30ms
step:761/1920 train_time:29927ms step_avg:39.33ms
step:762/1920 train_time:29988ms step_avg:39.35ms
step:763/1920 train_time:30050ms step_avg:39.38ms
step:764/1920 train_time:30111ms step_avg:39.41ms
step:765/1920 train_time:30173ms step_avg:39.44ms
step:766/1920 train_time:30236ms step_avg:39.47ms
step:767/1920 train_time:30300ms step_avg:39.51ms
step:768/1920 train_time:30363ms step_avg:39.54ms
step:769/1920 train_time:30427ms step_avg:39.57ms
step:770/1920 train_time:30491ms step_avg:39.60ms
step:771/1920 train_time:30554ms step_avg:39.63ms
step:772/1920 train_time:30617ms step_avg:39.66ms
step:773/1920 train_time:30680ms step_avg:39.69ms
step:774/1920 train_time:30742ms step_avg:39.72ms
step:775/1920 train_time:30804ms step_avg:39.75ms
step:776/1920 train_time:30865ms step_avg:39.77ms
step:777/1920 train_time:30928ms step_avg:39.80ms
step:778/1920 train_time:30989ms step_avg:39.83ms
step:779/1920 train_time:31051ms step_avg:39.86ms
step:780/1920 train_time:31112ms step_avg:39.89ms
step:781/1920 train_time:31175ms step_avg:39.92ms
step:782/1920 train_time:31237ms step_avg:39.94ms
step:783/1920 train_time:31301ms step_avg:39.98ms
step:784/1920 train_time:31363ms step_avg:40.00ms
step:785/1920 train_time:31426ms step_avg:40.03ms
step:786/1920 train_time:31489ms step_avg:40.06ms
step:787/1920 train_time:31553ms step_avg:40.09ms
step:788/1920 train_time:31616ms step_avg:40.12ms
step:789/1920 train_time:31679ms step_avg:40.15ms
step:790/1920 train_time:31740ms step_avg:40.18ms
step:791/1920 train_time:31803ms step_avg:40.21ms
step:792/1920 train_time:31865ms step_avg:40.23ms
step:793/1920 train_time:31927ms step_avg:40.26ms
step:794/1920 train_time:31988ms step_avg:40.29ms
step:795/1920 train_time:32050ms step_avg:40.31ms
step:796/1920 train_time:32111ms step_avg:40.34ms
step:797/1920 train_time:32173ms step_avg:40.37ms
step:798/1920 train_time:32235ms step_avg:40.39ms
step:799/1920 train_time:32298ms step_avg:40.42ms
step:800/1920 train_time:32360ms step_avg:40.45ms
step:801/1920 train_time:32423ms step_avg:40.48ms
step:802/1920 train_time:32485ms step_avg:40.50ms
step:803/1920 train_time:32547ms step_avg:40.53ms
step:804/1920 train_time:32609ms step_avg:40.56ms
step:805/1920 train_time:32672ms step_avg:40.59ms
step:806/1920 train_time:32735ms step_avg:40.61ms
step:807/1920 train_time:32798ms step_avg:40.64ms
step:808/1920 train_time:32860ms step_avg:40.67ms
step:809/1920 train_time:32922ms step_avg:40.70ms
step:810/1920 train_time:32984ms step_avg:40.72ms
step:811/1920 train_time:33046ms step_avg:40.75ms
step:812/1920 train_time:33108ms step_avg:40.77ms
step:813/1920 train_time:33170ms step_avg:40.80ms
step:814/1920 train_time:33231ms step_avg:40.82ms
step:815/1920 train_time:33294ms step_avg:40.85ms
step:816/1920 train_time:33356ms step_avg:40.88ms
step:817/1920 train_time:33420ms step_avg:40.91ms
step:818/1920 train_time:33482ms step_avg:40.93ms
step:819/1920 train_time:33545ms step_avg:40.96ms
step:820/1920 train_time:33607ms step_avg:40.98ms
step:821/1920 train_time:33669ms step_avg:41.01ms
step:822/1920 train_time:33732ms step_avg:41.04ms
step:823/1920 train_time:33795ms step_avg:41.06ms
step:824/1920 train_time:33856ms step_avg:41.09ms
step:825/1920 train_time:33919ms step_avg:41.11ms
step:826/1920 train_time:33981ms step_avg:41.14ms
step:827/1920 train_time:34043ms step_avg:41.16ms
step:828/1920 train_time:34104ms step_avg:41.19ms
step:829/1920 train_time:34166ms step_avg:41.21ms
step:830/1920 train_time:34228ms step_avg:41.24ms
step:831/1920 train_time:34290ms step_avg:41.26ms
step:832/1920 train_time:34352ms step_avg:41.29ms
step:833/1920 train_time:34415ms step_avg:41.31ms
step:834/1920 train_time:34477ms step_avg:41.34ms
step:835/1920 train_time:34540ms step_avg:41.37ms
step:836/1920 train_time:34602ms step_avg:41.39ms
step:837/1920 train_time:34665ms step_avg:41.42ms
step:838/1920 train_time:34727ms step_avg:41.44ms
step:839/1920 train_time:34790ms step_avg:41.47ms
step:840/1920 train_time:34852ms step_avg:41.49ms
step:841/1920 train_time:34916ms step_avg:41.52ms
step:842/1920 train_time:34978ms step_avg:41.54ms
step:843/1920 train_time:35041ms step_avg:41.57ms
step:844/1920 train_time:35103ms step_avg:41.59ms
step:845/1920 train_time:35165ms step_avg:41.62ms
step:846/1920 train_time:35226ms step_avg:41.64ms
step:847/1920 train_time:35289ms step_avg:41.66ms
step:848/1920 train_time:35350ms step_avg:41.69ms
step:849/1920 train_time:35413ms step_avg:41.71ms
step:850/1920 train_time:35476ms step_avg:41.74ms
step:851/1920 train_time:35540ms step_avg:41.76ms
step:852/1920 train_time:35601ms step_avg:41.79ms
step:853/1920 train_time:35664ms step_avg:41.81ms
step:854/1920 train_time:35726ms step_avg:41.83ms
step:855/1920 train_time:35788ms step_avg:41.86ms
step:856/1920 train_time:35850ms step_avg:41.88ms
step:857/1920 train_time:35913ms step_avg:41.91ms
step:858/1920 train_time:35975ms step_avg:41.93ms
step:859/1920 train_time:36039ms step_avg:41.95ms
step:860/1920 train_time:36101ms step_avg:41.98ms
step:861/1920 train_time:36163ms step_avg:42.00ms
step:862/1920 train_time:36225ms step_avg:42.02ms
step:863/1920 train_time:36286ms step_avg:42.05ms
step:864/1920 train_time:36348ms step_avg:42.07ms
step:865/1920 train_time:36411ms step_avg:42.09ms
step:866/1920 train_time:36472ms step_avg:42.12ms
step:867/1920 train_time:36536ms step_avg:42.14ms
step:868/1920 train_time:36598ms step_avg:42.16ms
step:869/1920 train_time:36661ms step_avg:42.19ms
step:870/1920 train_time:36723ms step_avg:42.21ms
step:871/1920 train_time:36786ms step_avg:42.23ms
step:872/1920 train_time:36847ms step_avg:42.26ms
step:873/1920 train_time:36910ms step_avg:42.28ms
step:874/1920 train_time:36972ms step_avg:42.30ms
step:875/1920 train_time:37037ms step_avg:42.33ms
step:876/1920 train_time:37099ms step_avg:42.35ms
step:877/1920 train_time:37162ms step_avg:42.37ms
step:878/1920 train_time:37224ms step_avg:42.40ms
step:879/1920 train_time:37286ms step_avg:42.42ms
step:880/1920 train_time:37347ms step_avg:42.44ms
step:881/1920 train_time:37410ms step_avg:42.46ms
step:882/1920 train_time:37471ms step_avg:42.48ms
step:883/1920 train_time:37534ms step_avg:42.51ms
step:884/1920 train_time:37596ms step_avg:42.53ms
step:885/1920 train_time:37660ms step_avg:42.55ms
step:886/1920 train_time:37722ms step_avg:42.58ms
step:887/1920 train_time:37784ms step_avg:42.60ms
step:888/1920 train_time:37845ms step_avg:42.62ms
step:889/1920 train_time:37908ms step_avg:42.64ms
step:890/1920 train_time:37970ms step_avg:42.66ms
step:891/1920 train_time:38032ms step_avg:42.69ms
step:892/1920 train_time:38095ms step_avg:42.71ms
step:893/1920 train_time:38158ms step_avg:42.73ms
step:894/1920 train_time:38220ms step_avg:42.75ms
step:895/1920 train_time:38282ms step_avg:42.77ms
step:896/1920 train_time:38344ms step_avg:42.79ms
step:897/1920 train_time:38406ms step_avg:42.82ms
step:898/1920 train_time:38468ms step_avg:42.84ms
step:899/1920 train_time:38531ms step_avg:42.86ms
step:900/1920 train_time:38593ms step_avg:42.88ms
step:901/1920 train_time:38656ms step_avg:42.90ms
step:902/1920 train_time:38718ms step_avg:42.92ms
step:903/1920 train_time:38781ms step_avg:42.95ms
step:904/1920 train_time:38843ms step_avg:42.97ms
step:905/1920 train_time:38906ms step_avg:42.99ms
step:906/1920 train_time:38967ms step_avg:43.01ms
step:907/1920 train_time:39030ms step_avg:43.03ms
step:908/1920 train_time:39091ms step_avg:43.05ms
step:909/1920 train_time:39155ms step_avg:43.07ms
step:910/1920 train_time:39217ms step_avg:43.10ms
step:911/1920 train_time:39280ms step_avg:43.12ms
step:912/1920 train_time:39343ms step_avg:43.14ms
step:913/1920 train_time:39405ms step_avg:43.16ms
step:914/1920 train_time:39467ms step_avg:43.18ms
step:915/1920 train_time:39529ms step_avg:43.20ms
step:916/1920 train_time:39591ms step_avg:43.22ms
step:917/1920 train_time:39654ms step_avg:43.24ms
step:918/1920 train_time:39716ms step_avg:43.26ms
step:919/1920 train_time:39779ms step_avg:43.29ms
step:920/1920 train_time:39841ms step_avg:43.31ms
step:921/1920 train_time:39904ms step_avg:43.33ms
step:922/1920 train_time:39966ms step_avg:43.35ms
step:923/1920 train_time:40028ms step_avg:43.37ms
step:924/1920 train_time:40090ms step_avg:43.39ms
step:925/1920 train_time:40154ms step_avg:43.41ms
step:926/1920 train_time:40216ms step_avg:43.43ms
step:927/1920 train_time:40279ms step_avg:43.45ms
step:928/1920 train_time:40341ms step_avg:43.47ms
step:929/1920 train_time:40403ms step_avg:43.49ms
step:930/1920 train_time:40465ms step_avg:43.51ms
step:931/1920 train_time:40527ms step_avg:43.53ms
step:932/1920 train_time:40590ms step_avg:43.55ms
step:933/1920 train_time:40653ms step_avg:43.57ms
step:934/1920 train_time:40714ms step_avg:43.59ms
step:935/1920 train_time:40778ms step_avg:43.61ms
step:936/1920 train_time:40840ms step_avg:43.63ms
step:937/1920 train_time:40903ms step_avg:43.65ms
step:938/1920 train_time:40964ms step_avg:43.67ms
step:939/1920 train_time:41027ms step_avg:43.69ms
step:940/1920 train_time:41088ms step_avg:43.71ms
step:941/1920 train_time:41151ms step_avg:43.73ms
step:942/1920 train_time:41213ms step_avg:43.75ms
step:943/1920 train_time:41277ms step_avg:43.77ms
step:944/1920 train_time:41339ms step_avg:43.79ms
step:945/1920 train_time:41402ms step_avg:43.81ms
step:946/1920 train_time:41463ms step_avg:43.83ms
step:947/1920 train_time:41526ms step_avg:43.85ms
step:948/1920 train_time:41588ms step_avg:43.87ms
step:949/1920 train_time:41651ms step_avg:43.89ms
step:950/1920 train_time:41713ms step_avg:43.91ms
step:951/1920 train_time:41776ms step_avg:43.93ms
step:952/1920 train_time:41838ms step_avg:43.95ms
step:953/1920 train_time:41901ms step_avg:43.97ms
step:954/1920 train_time:41962ms step_avg:43.99ms
step:955/1920 train_time:42025ms step_avg:44.01ms
step:956/1920 train_time:42087ms step_avg:44.02ms
step:957/1920 train_time:42150ms step_avg:44.04ms
step:958/1920 train_time:42212ms step_avg:44.06ms
step:959/1920 train_time:42275ms step_avg:44.08ms
step:960/1920 train_time:42337ms step_avg:44.10ms
step:961/1920 train_time:42400ms step_avg:44.12ms
step:962/1920 train_time:42462ms step_avg:44.14ms
step:963/1920 train_time:42525ms step_avg:44.16ms
step:964/1920 train_time:42586ms step_avg:44.18ms
step:965/1920 train_time:42649ms step_avg:44.20ms
step:966/1920 train_time:42711ms step_avg:44.21ms
step:967/1920 train_time:42773ms step_avg:44.23ms
step:968/1920 train_time:42835ms step_avg:44.25ms
step:969/1920 train_time:42899ms step_avg:44.27ms
step:970/1920 train_time:42961ms step_avg:44.29ms
step:971/1920 train_time:43023ms step_avg:44.31ms
step:972/1920 train_time:43085ms step_avg:44.33ms
step:973/1920 train_time:43147ms step_avg:44.34ms
step:974/1920 train_time:43209ms step_avg:44.36ms
step:975/1920 train_time:43271ms step_avg:44.38ms
step:976/1920 train_time:43333ms step_avg:44.40ms
step:977/1920 train_time:43396ms step_avg:44.42ms
step:978/1920 train_time:43458ms step_avg:44.44ms
step:979/1920 train_time:43521ms step_avg:44.45ms
step:980/1920 train_time:43583ms step_avg:44.47ms
step:981/1920 train_time:43645ms step_avg:44.49ms
step:982/1920 train_time:43707ms step_avg:44.51ms
step:983/1920 train_time:43769ms step_avg:44.53ms
step:984/1920 train_time:43832ms step_avg:44.54ms
step:985/1920 train_time:43895ms step_avg:44.56ms
step:986/1920 train_time:43957ms step_avg:44.58ms
step:987/1920 train_time:44020ms step_avg:44.60ms
step:988/1920 train_time:44082ms step_avg:44.62ms
step:989/1920 train_time:44145ms step_avg:44.64ms
step:990/1920 train_time:44206ms step_avg:44.65ms
step:991/1920 train_time:44269ms step_avg:44.67ms
step:992/1920 train_time:44331ms step_avg:44.69ms
step:993/1920 train_time:44394ms step_avg:44.71ms
step:994/1920 train_time:44456ms step_avg:44.72ms
step:995/1920 train_time:44519ms step_avg:44.74ms
step:996/1920 train_time:44582ms step_avg:44.76ms
step:997/1920 train_time:44644ms step_avg:44.78ms
step:998/1920 train_time:44706ms step_avg:44.80ms
step:999/1920 train_time:44769ms step_avg:44.81ms
step:1000/1920 train_time:44830ms step_avg:44.83ms
step:1000/1920 val_loss:3.7785 train_time:44896ms step_avg:44.90ms
step:1001/1920 train_time:44914ms step_avg:44.87ms
step:1002/1920 train_time:44960ms step_avg:44.87ms
step:1003/1920 train_time:45026ms step_avg:44.89ms
step:1004/1920 train_time:45089ms step_avg:44.91ms
step:1005/1920 train_time:45154ms step_avg:44.93ms
step:1006/1920 train_time:45216ms step_avg:44.95ms
step:1007/1920 train_time:45278ms step_avg:44.96ms
step:1008/1920 train_time:45340ms step_avg:44.98ms
step:1009/1920 train_time:45402ms step_avg:45.00ms
step:1010/1920 train_time:45463ms step_avg:45.01ms
step:1011/1920 train_time:45526ms step_avg:45.03ms
step:1012/1920 train_time:45587ms step_avg:45.05ms
step:1013/1920 train_time:45649ms step_avg:45.06ms
step:1014/1920 train_time:45710ms step_avg:45.08ms
step:1015/1920 train_time:45772ms step_avg:45.10ms
step:1016/1920 train_time:45835ms step_avg:45.11ms
step:1017/1920 train_time:45899ms step_avg:45.13ms
step:1018/1920 train_time:45961ms step_avg:45.15ms
step:1019/1920 train_time:46026ms step_avg:45.17ms
step:1020/1920 train_time:46088ms step_avg:45.18ms
step:1021/1920 train_time:46151ms step_avg:45.20ms
step:1022/1920 train_time:46214ms step_avg:45.22ms
step:1023/1920 train_time:46277ms step_avg:45.24ms
step:1024/1920 train_time:46339ms step_avg:45.25ms
step:1025/1920 train_time:46401ms step_avg:45.27ms
step:1026/1920 train_time:46463ms step_avg:45.29ms
step:1027/1920 train_time:46525ms step_avg:45.30ms
step:1028/1920 train_time:46587ms step_avg:45.32ms
step:1029/1920 train_time:46648ms step_avg:45.33ms
step:1030/1920 train_time:46709ms step_avg:45.35ms
step:1031/1920 train_time:46772ms step_avg:45.37ms
step:1032/1920 train_time:46835ms step_avg:45.38ms
step:1033/1920 train_time:46898ms step_avg:45.40ms
step:1034/1920 train_time:46960ms step_avg:45.42ms
step:1035/1920 train_time:47023ms step_avg:45.43ms
step:1036/1920 train_time:47086ms step_avg:45.45ms
step:1037/1920 train_time:47148ms step_avg:45.47ms
step:1038/1920 train_time:47211ms step_avg:45.48ms
step:1039/1920 train_time:47274ms step_avg:45.50ms
step:1040/1920 train_time:47336ms step_avg:45.52ms
step:1041/1920 train_time:47398ms step_avg:45.53ms
step:1042/1920 train_time:47460ms step_avg:45.55ms
step:1043/1920 train_time:47523ms step_avg:45.56ms
step:1044/1920 train_time:47585ms step_avg:45.58ms
step:1045/1920 train_time:47647ms step_avg:45.60ms
step:1046/1920 train_time:47709ms step_avg:45.61ms
step:1047/1920 train_time:47772ms step_avg:45.63ms
step:1048/1920 train_time:47834ms step_avg:45.64ms
step:1049/1920 train_time:47897ms step_avg:45.66ms
step:1050/1920 train_time:47959ms step_avg:45.68ms
step:1051/1920 train_time:48022ms step_avg:45.69ms
step:1052/1920 train_time:48084ms step_avg:45.71ms
step:1053/1920 train_time:48147ms step_avg:45.72ms
step:1054/1920 train_time:48209ms step_avg:45.74ms
step:1055/1920 train_time:48272ms step_avg:45.76ms
step:1056/1920 train_time:48334ms step_avg:45.77ms
step:1057/1920 train_time:48398ms step_avg:45.79ms
step:1058/1920 train_time:48460ms step_avg:45.80ms
step:1059/1920 train_time:48523ms step_avg:45.82ms
step:1060/1920 train_time:48584ms step_avg:45.83ms
step:1061/1920 train_time:48646ms step_avg:45.85ms
step:1062/1920 train_time:48707ms step_avg:45.86ms
step:1063/1920 train_time:48770ms step_avg:45.88ms
step:1064/1920 train_time:48832ms step_avg:45.89ms
step:1065/1920 train_time:48895ms step_avg:45.91ms
step:1066/1920 train_time:48957ms step_avg:45.93ms
step:1067/1920 train_time:49020ms step_avg:45.94ms
step:1068/1920 train_time:49082ms step_avg:45.96ms
step:1069/1920 train_time:49144ms step_avg:45.97ms
step:1070/1920 train_time:49206ms step_avg:45.99ms
step:1071/1920 train_time:49268ms step_avg:46.00ms
step:1072/1920 train_time:49331ms step_avg:46.02ms
step:1073/1920 train_time:49394ms step_avg:46.03ms
step:1074/1920 train_time:49457ms step_avg:46.05ms
step:1075/1920 train_time:49520ms step_avg:46.07ms
step:1076/1920 train_time:49582ms step_avg:46.08ms
step:1077/1920 train_time:49644ms step_avg:46.09ms
step:1078/1920 train_time:49706ms step_avg:46.11ms
step:1079/1920 train_time:49768ms step_avg:46.12ms
step:1080/1920 train_time:49830ms step_avg:46.14ms
step:1081/1920 train_time:49893ms step_avg:46.15ms
step:1082/1920 train_time:49955ms step_avg:46.17ms
step:1083/1920 train_time:50019ms step_avg:46.19ms
step:1084/1920 train_time:50080ms step_avg:46.20ms
step:1085/1920 train_time:50143ms step_avg:46.21ms
step:1086/1920 train_time:50205ms step_avg:46.23ms
step:1087/1920 train_time:50268ms step_avg:46.24ms
step:1088/1920 train_time:50330ms step_avg:46.26ms
step:1089/1920 train_time:50393ms step_avg:46.27ms
step:1090/1920 train_time:50456ms step_avg:46.29ms
step:1091/1920 train_time:50519ms step_avg:46.31ms
step:1092/1920 train_time:50581ms step_avg:46.32ms
step:1093/1920 train_time:50643ms step_avg:46.33ms
step:1094/1920 train_time:50706ms step_avg:46.35ms
step:1095/1920 train_time:50768ms step_avg:46.36ms
step:1096/1920 train_time:50829ms step_avg:46.38ms
step:1097/1920 train_time:50891ms step_avg:46.39ms
step:1098/1920 train_time:50954ms step_avg:46.41ms
step:1099/1920 train_time:51017ms step_avg:46.42ms
step:1100/1920 train_time:51079ms step_avg:46.44ms
step:1101/1920 train_time:51142ms step_avg:46.45ms
step:1102/1920 train_time:51203ms step_avg:46.46ms
step:1103/1920 train_time:51266ms step_avg:46.48ms
step:1104/1920 train_time:51328ms step_avg:46.49ms
step:1105/1920 train_time:51391ms step_avg:46.51ms
step:1106/1920 train_time:51454ms step_avg:46.52ms
step:1107/1920 train_time:51518ms step_avg:46.54ms
step:1108/1920 train_time:51580ms step_avg:46.55ms
step:1109/1920 train_time:51642ms step_avg:46.57ms
step:1110/1920 train_time:51704ms step_avg:46.58ms
step:1111/1920 train_time:51766ms step_avg:46.59ms
step:1112/1920 train_time:51828ms step_avg:46.61ms
step:1113/1920 train_time:51890ms step_avg:46.62ms
step:1114/1920 train_time:51953ms step_avg:46.64ms
step:1115/1920 train_time:52016ms step_avg:46.65ms
step:1116/1920 train_time:52078ms step_avg:46.66ms
step:1117/1920 train_time:52141ms step_avg:46.68ms
step:1118/1920 train_time:52202ms step_avg:46.69ms
step:1119/1920 train_time:52265ms step_avg:46.71ms
step:1120/1920 train_time:52327ms step_avg:46.72ms
step:1121/1920 train_time:52390ms step_avg:46.74ms
step:1122/1920 train_time:52453ms step_avg:46.75ms
step:1123/1920 train_time:52516ms step_avg:46.76ms
step:1124/1920 train_time:52578ms step_avg:46.78ms
step:1125/1920 train_time:52641ms step_avg:46.79ms
step:1126/1920 train_time:52702ms step_avg:46.80ms
step:1127/1920 train_time:52765ms step_avg:46.82ms
step:1128/1920 train_time:52827ms step_avg:46.83ms
step:1129/1920 train_time:52889ms step_avg:46.85ms
step:1130/1920 train_time:52951ms step_avg:46.86ms
step:1131/1920 train_time:53014ms step_avg:46.87ms
step:1132/1920 train_time:53076ms step_avg:46.89ms
step:1133/1920 train_time:53139ms step_avg:46.90ms
step:1134/1920 train_time:53201ms step_avg:46.91ms
step:1135/1920 train_time:53265ms step_avg:46.93ms
step:1136/1920 train_time:53326ms step_avg:46.94ms
step:1137/1920 train_time:53388ms step_avg:46.96ms
step:1138/1920 train_time:53451ms step_avg:46.97ms
step:1139/1920 train_time:53514ms step_avg:46.98ms
step:1140/1920 train_time:53577ms step_avg:47.00ms
step:1141/1920 train_time:53639ms step_avg:47.01ms
step:1142/1920 train_time:53701ms step_avg:47.02ms
step:1143/1920 train_time:53764ms step_avg:47.04ms
step:1144/1920 train_time:53825ms step_avg:47.05ms
step:1145/1920 train_time:53887ms step_avg:47.06ms
step:1146/1920 train_time:53949ms step_avg:47.08ms
step:1147/1920 train_time:54012ms step_avg:47.09ms
step:1148/1920 train_time:54074ms step_avg:47.10ms
step:1149/1920 train_time:54138ms step_avg:47.12ms
step:1150/1920 train_time:54200ms step_avg:47.13ms
step:1151/1920 train_time:54263ms step_avg:47.14ms
step:1152/1920 train_time:54324ms step_avg:47.16ms
step:1153/1920 train_time:54387ms step_avg:47.17ms
step:1154/1920 train_time:54449ms step_avg:47.18ms
step:1155/1920 train_time:54512ms step_avg:47.20ms
step:1156/1920 train_time:54574ms step_avg:47.21ms
step:1157/1920 train_time:54638ms step_avg:47.22ms
step:1158/1920 train_time:54700ms step_avg:47.24ms
step:1159/1920 train_time:54762ms step_avg:47.25ms
step:1160/1920 train_time:54824ms step_avg:47.26ms
step:1161/1920 train_time:54886ms step_avg:47.28ms
step:1162/1920 train_time:54948ms step_avg:47.29ms
step:1163/1920 train_time:55011ms step_avg:47.30ms
step:1164/1920 train_time:55073ms step_avg:47.31ms
step:1165/1920 train_time:55137ms step_avg:47.33ms
step:1166/1920 train_time:55199ms step_avg:47.34ms
step:1167/1920 train_time:55262ms step_avg:47.35ms
step:1168/1920 train_time:55324ms step_avg:47.37ms
step:1169/1920 train_time:55386ms step_avg:47.38ms
step:1170/1920 train_time:55448ms step_avg:47.39ms
step:1171/1920 train_time:55511ms step_avg:47.41ms
step:1172/1920 train_time:55574ms step_avg:47.42ms
step:1173/1920 train_time:55637ms step_avg:47.43ms
step:1174/1920 train_time:55699ms step_avg:47.44ms
step:1175/1920 train_time:55762ms step_avg:47.46ms
step:1176/1920 train_time:55824ms step_avg:47.47ms
step:1177/1920 train_time:55887ms step_avg:47.48ms
step:1178/1920 train_time:55948ms step_avg:47.49ms
step:1179/1920 train_time:56010ms step_avg:47.51ms
step:1180/1920 train_time:56072ms step_avg:47.52ms
step:1181/1920 train_time:56135ms step_avg:47.53ms
step:1182/1920 train_time:56197ms step_avg:47.54ms
step:1183/1920 train_time:56260ms step_avg:47.56ms
step:1184/1920 train_time:56321ms step_avg:47.57ms
step:1185/1920 train_time:56384ms step_avg:47.58ms
step:1186/1920 train_time:56446ms step_avg:47.59ms
step:1187/1920 train_time:56508ms step_avg:47.61ms
step:1188/1920 train_time:56571ms step_avg:47.62ms
step:1189/1920 train_time:56633ms step_avg:47.63ms
step:1190/1920 train_time:56696ms step_avg:47.64ms
step:1191/1920 train_time:56759ms step_avg:47.66ms
step:1192/1920 train_time:56822ms step_avg:47.67ms
step:1193/1920 train_time:56885ms step_avg:47.68ms
step:1194/1920 train_time:56947ms step_avg:47.69ms
step:1195/1920 train_time:57009ms step_avg:47.71ms
step:1196/1920 train_time:57071ms step_avg:47.72ms
step:1197/1920 train_time:57134ms step_avg:47.73ms
step:1198/1920 train_time:57197ms step_avg:47.74ms
step:1199/1920 train_time:57260ms step_avg:47.76ms
step:1200/1920 train_time:57322ms step_avg:47.77ms
step:1201/1920 train_time:57384ms step_avg:47.78ms
step:1202/1920 train_time:57445ms step_avg:47.79ms
step:1203/1920 train_time:57508ms step_avg:47.80ms
step:1204/1920 train_time:57570ms step_avg:47.82ms
step:1205/1920 train_time:57633ms step_avg:47.83ms
step:1206/1920 train_time:57695ms step_avg:47.84ms
step:1207/1920 train_time:57758ms step_avg:47.85ms
step:1208/1920 train_time:57820ms step_avg:47.86ms
step:1209/1920 train_time:57883ms step_avg:47.88ms
step:1210/1920 train_time:57944ms step_avg:47.89ms
step:1211/1920 train_time:58007ms step_avg:47.90ms
step:1212/1920 train_time:58069ms step_avg:47.91ms
step:1213/1920 train_time:58132ms step_avg:47.92ms
step:1214/1920 train_time:58194ms step_avg:47.94ms
step:1215/1920 train_time:58257ms step_avg:47.95ms
step:1216/1920 train_time:58320ms step_avg:47.96ms
step:1217/1920 train_time:58382ms step_avg:47.97ms
step:1218/1920 train_time:58444ms step_avg:47.98ms
step:1219/1920 train_time:58506ms step_avg:48.00ms
step:1220/1920 train_time:58568ms step_avg:48.01ms
step:1221/1920 train_time:58631ms step_avg:48.02ms
step:1222/1920 train_time:58693ms step_avg:48.03ms
step:1223/1920 train_time:58757ms step_avg:48.04ms
step:1224/1920 train_time:58819ms step_avg:48.05ms
step:1225/1920 train_time:58882ms step_avg:48.07ms
step:1226/1920 train_time:58943ms step_avg:48.08ms
step:1227/1920 train_time:59006ms step_avg:48.09ms
step:1228/1920 train_time:59068ms step_avg:48.10ms
step:1229/1920 train_time:59130ms step_avg:48.11ms
step:1230/1920 train_time:59193ms step_avg:48.12ms
step:1231/1920 train_time:59256ms step_avg:48.14ms
step:1232/1920 train_time:59319ms step_avg:48.15ms
step:1233/1920 train_time:59382ms step_avg:48.16ms
step:1234/1920 train_time:59444ms step_avg:48.17ms
step:1235/1920 train_time:59506ms step_avg:48.18ms
step:1236/1920 train_time:59568ms step_avg:48.19ms
step:1237/1920 train_time:59631ms step_avg:48.21ms
step:1238/1920 train_time:59693ms step_avg:48.22ms
step:1239/1920 train_time:59756ms step_avg:48.23ms
step:1240/1920 train_time:59818ms step_avg:48.24ms
step:1241/1920 train_time:59881ms step_avg:48.25ms
step:1242/1920 train_time:59943ms step_avg:48.26ms
step:1243/1920 train_time:60005ms step_avg:48.27ms
step:1244/1920 train_time:60067ms step_avg:48.29ms
step:1245/1920 train_time:60129ms step_avg:48.30ms
step:1246/1920 train_time:60191ms step_avg:48.31ms
step:1247/1920 train_time:60254ms step_avg:48.32ms
step:1248/1920 train_time:60316ms step_avg:48.33ms
step:1249/1920 train_time:60379ms step_avg:48.34ms
step:1250/1920 train_time:60441ms step_avg:48.35ms
step:1250/1920 val_loss:3.5529 train_time:60506ms step_avg:48.40ms
step:1251/1920 train_time:60523ms step_avg:48.38ms
step:1252/1920 train_time:60569ms step_avg:48.38ms
step:1253/1920 train_time:60633ms step_avg:48.39ms
step:1254/1920 train_time:60697ms step_avg:48.40ms
step:1255/1920 train_time:60760ms step_avg:48.41ms
step:1256/1920 train_time:60848ms step_avg:48.45ms
step:1257/1920 train_time:60936ms step_avg:48.48ms
step:1258/1920 train_time:61023ms step_avg:48.51ms
step:1259/1920 train_time:61110ms step_avg:48.54ms
step:1260/1920 train_time:61197ms step_avg:48.57ms
step:1261/1920 train_time:61285ms step_avg:48.60ms
step:1262/1920 train_time:61371ms step_avg:48.63ms
step:1263/1920 train_time:61463ms step_avg:48.66ms
step:1264/1920 train_time:61554ms step_avg:48.70ms
step:1265/1920 train_time:61646ms step_avg:48.73ms
step:1266/1920 train_time:61736ms step_avg:48.76ms
step:1267/1920 train_time:61826ms step_avg:48.80ms
step:1268/1920 train_time:61913ms step_avg:48.83ms
step:1269/1920 train_time:62001ms step_avg:48.86ms
step:1270/1920 train_time:62087ms step_avg:48.89ms
step:1271/1920 train_time:62174ms step_avg:48.92ms
step:1272/1920 train_time:62261ms step_avg:48.95ms
step:1273/1920 train_time:62349ms step_avg:48.98ms
step:1274/1920 train_time:62437ms step_avg:49.01ms
step:1275/1920 train_time:62528ms step_avg:49.04ms
step:1276/1920 train_time:62618ms step_avg:49.07ms
step:1277/1920 train_time:62709ms step_avg:49.11ms
step:1278/1920 train_time:62799ms step_avg:49.14ms
step:1279/1920 train_time:62889ms step_avg:49.17ms
step:1280/1920 train_time:62977ms step_avg:49.20ms
step:1281/1920 train_time:63065ms step_avg:49.23ms
step:1282/1920 train_time:63151ms step_avg:49.26ms
step:1283/1920 train_time:63239ms step_avg:49.29ms
step:1284/1920 train_time:63327ms step_avg:49.32ms
step:1285/1920 train_time:63415ms step_avg:49.35ms
step:1286/1920 train_time:63504ms step_avg:49.38ms
step:1287/1920 train_time:63594ms step_avg:49.41ms
step:1288/1920 train_time:63684ms step_avg:49.44ms
step:1289/1920 train_time:63773ms step_avg:49.47ms
step:1290/1920 train_time:63862ms step_avg:49.51ms
step:1291/1920 train_time:63950ms step_avg:49.54ms
step:1292/1920 train_time:64038ms step_avg:49.57ms
step:1293/1920 train_time:64127ms step_avg:49.60ms
step:1294/1920 train_time:64213ms step_avg:49.62ms
step:1295/1920 train_time:64302ms step_avg:49.65ms
step:1296/1920 train_time:64389ms step_avg:49.68ms
step:1297/1920 train_time:64478ms step_avg:49.71ms
step:1298/1920 train_time:64566ms step_avg:49.74ms
step:1299/1920 train_time:64656ms step_avg:49.77ms
step:1300/1920 train_time:64745ms step_avg:49.80ms
step:1301/1920 train_time:64834ms step_avg:49.83ms
step:1302/1920 train_time:64922ms step_avg:49.86ms
step:1303/1920 train_time:65010ms step_avg:49.89ms
step:1304/1920 train_time:65099ms step_avg:49.92ms
step:1305/1920 train_time:65188ms step_avg:49.95ms
step:1306/1920 train_time:65276ms step_avg:49.98ms
step:1307/1920 train_time:65365ms step_avg:50.01ms
step:1308/1920 train_time:65452ms step_avg:50.04ms
step:1309/1920 train_time:65541ms step_avg:50.07ms
step:1310/1920 train_time:65630ms step_avg:50.10ms
step:1311/1920 train_time:65719ms step_avg:50.13ms
step:1312/1920 train_time:65808ms step_avg:50.16ms
step:1313/1920 train_time:65897ms step_avg:50.19ms
step:1314/1920 train_time:65985ms step_avg:50.22ms
step:1315/1920 train_time:66073ms step_avg:50.25ms
step:1316/1920 train_time:66161ms step_avg:50.27ms
step:1317/1920 train_time:66249ms step_avg:50.30ms
step:1318/1920 train_time:66337ms step_avg:50.33ms
step:1319/1920 train_time:66426ms step_avg:50.36ms
step:1320/1920 train_time:66514ms step_avg:50.39ms
step:1321/1920 train_time:66604ms step_avg:50.42ms
step:1322/1920 train_time:66691ms step_avg:50.45ms
step:1323/1920 train_time:66780ms step_avg:50.48ms
step:1324/1920 train_time:66868ms step_avg:50.50ms
step:1325/1920 train_time:66956ms step_avg:50.53ms
step:1326/1920 train_time:67044ms step_avg:50.56ms
step:1327/1920 train_time:67133ms step_avg:50.59ms
step:1328/1920 train_time:67220ms step_avg:50.62ms
step:1329/1920 train_time:67309ms step_avg:50.65ms
step:1330/1920 train_time:67397ms step_avg:50.67ms
step:1331/1920 train_time:67487ms step_avg:50.70ms
step:1332/1920 train_time:67576ms step_avg:50.73ms
step:1333/1920 train_time:67666ms step_avg:50.76ms
step:1334/1920 train_time:67755ms step_avg:50.79ms
step:1335/1920 train_time:67844ms step_avg:50.82ms
step:1336/1920 train_time:67932ms step_avg:50.85ms
step:1337/1920 train_time:68021ms step_avg:50.88ms
step:1338/1920 train_time:68107ms step_avg:50.90ms
step:1339/1920 train_time:68196ms step_avg:50.93ms
step:1340/1920 train_time:68284ms step_avg:50.96ms
step:1341/1920 train_time:68373ms step_avg:50.99ms
step:1342/1920 train_time:68462ms step_avg:51.01ms
step:1343/1920 train_time:68551ms step_avg:51.04ms
step:1344/1920 train_time:68640ms step_avg:51.07ms
step:1345/1920 train_time:68729ms step_avg:51.10ms
step:1346/1920 train_time:68819ms step_avg:51.13ms
step:1347/1920 train_time:68909ms step_avg:51.16ms
step:1348/1920 train_time:68997ms step_avg:51.18ms
step:1349/1920 train_time:69087ms step_avg:51.21ms
step:1350/1920 train_time:69175ms step_avg:51.24ms
step:1351/1920 train_time:69263ms step_avg:51.27ms
step:1352/1920 train_time:69350ms step_avg:51.29ms
step:1353/1920 train_time:69440ms step_avg:51.32ms
step:1354/1920 train_time:69528ms step_avg:51.35ms
step:1355/1920 train_time:69616ms step_avg:51.38ms
step:1356/1920 train_time:69705ms step_avg:51.41ms
step:1357/1920 train_time:69795ms step_avg:51.43ms
step:1358/1920 train_time:69885ms step_avg:51.46ms
step:1359/1920 train_time:69974ms step_avg:51.49ms
step:1360/1920 train_time:70061ms step_avg:51.52ms
step:1361/1920 train_time:70150ms step_avg:51.54ms
step:1362/1920 train_time:70238ms step_avg:51.57ms
step:1363/1920 train_time:70327ms step_avg:51.60ms
step:1364/1920 train_time:70415ms step_avg:51.62ms
step:1365/1920 train_time:70503ms step_avg:51.65ms
step:1366/1920 train_time:70592ms step_avg:51.68ms
step:1367/1920 train_time:70681ms step_avg:51.70ms
step:1368/1920 train_time:70768ms step_avg:51.73ms
step:1369/1920 train_time:70858ms step_avg:51.76ms
step:1370/1920 train_time:70945ms step_avg:51.79ms
step:1371/1920 train_time:71033ms step_avg:51.81ms
step:1372/1920 train_time:71121ms step_avg:51.84ms
step:1373/1920 train_time:71210ms step_avg:51.86ms
step:1374/1920 train_time:71297ms step_avg:51.89ms
step:1375/1920 train_time:71387ms step_avg:51.92ms
step:1376/1920 train_time:71475ms step_avg:51.94ms
step:1377/1920 train_time:71564ms step_avg:51.97ms
step:1378/1920 train_time:71652ms step_avg:52.00ms
step:1379/1920 train_time:71741ms step_avg:52.02ms
step:1380/1920 train_time:71829ms step_avg:52.05ms
step:1381/1920 train_time:71918ms step_avg:52.08ms
step:1382/1920 train_time:72006ms step_avg:52.10ms
step:1383/1920 train_time:72095ms step_avg:52.13ms
step:1384/1920 train_time:72183ms step_avg:52.16ms
step:1385/1920 train_time:72271ms step_avg:52.18ms
step:1386/1920 train_time:72359ms step_avg:52.21ms
step:1387/1920 train_time:72449ms step_avg:52.23ms
step:1388/1920 train_time:72536ms step_avg:52.26ms
step:1389/1920 train_time:72628ms step_avg:52.29ms
step:1390/1920 train_time:72716ms step_avg:52.31ms
step:1391/1920 train_time:72806ms step_avg:52.34ms
step:1392/1920 train_time:72895ms step_avg:52.37ms
step:1393/1920 train_time:72984ms step_avg:52.39ms
step:1394/1920 train_time:73071ms step_avg:52.42ms
step:1395/1920 train_time:73160ms step_avg:52.44ms
step:1396/1920 train_time:73247ms step_avg:52.47ms
step:1397/1920 train_time:73335ms step_avg:52.49ms
step:1398/1920 train_time:73424ms step_avg:52.52ms
step:1399/1920 train_time:73512ms step_avg:52.55ms
step:1400/1920 train_time:73600ms step_avg:52.57ms
step:1401/1920 train_time:73688ms step_avg:52.60ms
step:1402/1920 train_time:73777ms step_avg:52.62ms
step:1403/1920 train_time:73867ms step_avg:52.65ms
step:1404/1920 train_time:73956ms step_avg:52.68ms
step:1405/1920 train_time:74045ms step_avg:52.70ms
step:1406/1920 train_time:74132ms step_avg:52.73ms
step:1407/1920 train_time:74221ms step_avg:52.75ms
step:1408/1920 train_time:74309ms step_avg:52.78ms
step:1409/1920 train_time:74398ms step_avg:52.80ms
step:1410/1920 train_time:74485ms step_avg:52.83ms
step:1411/1920 train_time:74574ms step_avg:52.85ms
step:1412/1920 train_time:74662ms step_avg:52.88ms
step:1413/1920 train_time:74752ms step_avg:52.90ms
step:1414/1920 train_time:74841ms step_avg:52.93ms
step:1415/1920 train_time:74930ms step_avg:52.95ms
step:1416/1920 train_time:75018ms step_avg:52.98ms
step:1417/1920 train_time:75107ms step_avg:53.00ms
step:1418/1920 train_time:75195ms step_avg:53.03ms
step:1419/1920 train_time:75284ms step_avg:53.05ms
step:1420/1920 train_time:75371ms step_avg:53.08ms
step:1421/1920 train_time:75460ms step_avg:53.10ms
step:1422/1920 train_time:75548ms step_avg:53.13ms
step:1423/1920 train_time:75636ms step_avg:53.15ms
step:1424/1920 train_time:75725ms step_avg:53.18ms
step:1425/1920 train_time:75813ms step_avg:53.20ms
step:1426/1920 train_time:75902ms step_avg:53.23ms
step:1427/1920 train_time:75990ms step_avg:53.25ms
step:1428/1920 train_time:76078ms step_avg:53.28ms
step:1429/1920 train_time:76167ms step_avg:53.30ms
step:1430/1920 train_time:76256ms step_avg:53.33ms
step:1431/1920 train_time:76345ms step_avg:53.35ms
step:1432/1920 train_time:76433ms step_avg:53.37ms
step:1433/1920 train_time:76521ms step_avg:53.40ms
step:1434/1920 train_time:76608ms step_avg:53.42ms
step:1435/1920 train_time:76697ms step_avg:53.45ms
step:1436/1920 train_time:76785ms step_avg:53.47ms
step:1437/1920 train_time:76874ms step_avg:53.50ms
step:1438/1920 train_time:76962ms step_avg:53.52ms
step:1439/1920 train_time:77050ms step_avg:53.54ms
step:1440/1920 train_time:77139ms step_avg:53.57ms
step:1441/1920 train_time:77229ms step_avg:53.59ms
step:1442/1920 train_time:77317ms step_avg:53.62ms
step:1443/1920 train_time:77406ms step_avg:53.64ms
step:1444/1920 train_time:77493ms step_avg:53.67ms
step:1445/1920 train_time:77582ms step_avg:53.69ms
step:1446/1920 train_time:77670ms step_avg:53.71ms
step:1447/1920 train_time:77759ms step_avg:53.74ms
step:1448/1920 train_time:77847ms step_avg:53.76ms
step:1449/1920 train_time:77936ms step_avg:53.79ms
step:1450/1920 train_time:78025ms step_avg:53.81ms
step:1451/1920 train_time:78113ms step_avg:53.83ms
step:1452/1920 train_time:78202ms step_avg:53.86ms
step:1453/1920 train_time:78291ms step_avg:53.88ms
step:1454/1920 train_time:78379ms step_avg:53.91ms
step:1455/1920 train_time:78469ms step_avg:53.93ms
step:1456/1920 train_time:78557ms step_avg:53.95ms
step:1457/1920 train_time:78646ms step_avg:53.98ms
step:1458/1920 train_time:78733ms step_avg:54.00ms
step:1459/1920 train_time:78822ms step_avg:54.02ms
step:1460/1920 train_time:78909ms step_avg:54.05ms
step:1461/1920 train_time:78999ms step_avg:54.07ms
step:1462/1920 train_time:79087ms step_avg:54.10ms
step:1463/1920 train_time:79175ms step_avg:54.12ms
step:1464/1920 train_time:79264ms step_avg:54.14ms
step:1465/1920 train_time:79352ms step_avg:54.17ms
step:1466/1920 train_time:79441ms step_avg:54.19ms
step:1467/1920 train_time:79530ms step_avg:54.21ms
step:1468/1920 train_time:79619ms step_avg:54.24ms
step:1469/1920 train_time:79708ms step_avg:54.26ms
step:1470/1920 train_time:79796ms step_avg:54.28ms
step:1471/1920 train_time:79887ms step_avg:54.31ms
step:1472/1920 train_time:79975ms step_avg:54.33ms
step:1473/1920 train_time:80063ms step_avg:54.35ms
step:1474/1920 train_time:80150ms step_avg:54.38ms
step:1475/1920 train_time:80240ms step_avg:54.40ms
step:1476/1920 train_time:80328ms step_avg:54.42ms
step:1477/1920 train_time:80416ms step_avg:54.45ms
step:1478/1920 train_time:80505ms step_avg:54.47ms
step:1479/1920 train_time:80594ms step_avg:54.49ms
step:1480/1920 train_time:80682ms step_avg:54.52ms
step:1481/1920 train_time:80771ms step_avg:54.54ms
step:1482/1920 train_time:80860ms step_avg:54.56ms
step:1483/1920 train_time:80948ms step_avg:54.58ms
step:1484/1920 train_time:81037ms step_avg:54.61ms
step:1485/1920 train_time:81126ms step_avg:54.63ms
step:1486/1920 train_time:81214ms step_avg:54.65ms
step:1487/1920 train_time:81303ms step_avg:54.68ms
step:1488/1920 train_time:81390ms step_avg:54.70ms
step:1489/1920 train_time:81479ms step_avg:54.72ms
step:1490/1920 train_time:81567ms step_avg:54.74ms
step:1491/1920 train_time:81655ms step_avg:54.77ms
step:1492/1920 train_time:81744ms step_avg:54.79ms
step:1493/1920 train_time:81833ms step_avg:54.81ms
step:1494/1920 train_time:81921ms step_avg:54.83ms
step:1495/1920 train_time:82009ms step_avg:54.86ms
step:1496/1920 train_time:82098ms step_avg:54.88ms
step:1497/1920 train_time:82187ms step_avg:54.90ms
step:1498/1920 train_time:82275ms step_avg:54.92ms
step:1499/1920 train_time:82364ms step_avg:54.95ms
step:1500/1920 train_time:82451ms step_avg:54.97ms
step:1500/1920 val_loss:3.4146 train_time:82542ms step_avg:55.03ms
step:1501/1920 train_time:82560ms step_avg:55.00ms
step:1502/1920 train_time:82633ms step_avg:55.02ms
step:1503/1920 train_time:82728ms step_avg:55.04ms
step:1504/1920 train_time:82817ms step_avg:55.06ms
step:1505/1920 train_time:82905ms step_avg:55.09ms
step:1506/1920 train_time:82992ms step_avg:55.11ms
step:1507/1920 train_time:83080ms step_avg:55.13ms
step:1508/1920 train_time:83166ms step_avg:55.15ms
step:1509/1920 train_time:83255ms step_avg:55.17ms
step:1510/1920 train_time:83342ms step_avg:55.19ms
step:1511/1920 train_time:83430ms step_avg:55.21ms
step:1512/1920 train_time:83519ms step_avg:55.24ms
step:1513/1920 train_time:83610ms step_avg:55.26ms
step:1514/1920 train_time:83702ms step_avg:55.29ms
step:1515/1920 train_time:83792ms step_avg:55.31ms
step:1516/1920 train_time:83879ms step_avg:55.33ms
step:1517/1920 train_time:83967ms step_avg:55.35ms
step:1518/1920 train_time:84054ms step_avg:55.37ms
step:1519/1920 train_time:84141ms step_avg:55.39ms
step:1520/1920 train_time:84228ms step_avg:55.41ms
step:1521/1920 train_time:84316ms step_avg:55.43ms
step:1522/1920 train_time:84403ms step_avg:55.46ms
step:1523/1920 train_time:84492ms step_avg:55.48ms
step:1524/1920 train_time:84581ms step_avg:55.50ms
step:1525/1920 train_time:84672ms step_avg:55.52ms
step:1526/1920 train_time:84761ms step_avg:55.54ms
step:1527/1920 train_time:84851ms step_avg:55.57ms
step:1528/1920 train_time:84938ms step_avg:55.59ms
step:1529/1920 train_time:85026ms step_avg:55.61ms
step:1530/1920 train_time:85113ms step_avg:55.63ms
step:1531/1920 train_time:85201ms step_avg:55.65ms
step:1532/1920 train_time:85289ms step_avg:55.67ms
step:1533/1920 train_time:85378ms step_avg:55.69ms
step:1534/1920 train_time:85466ms step_avg:55.71ms
step:1535/1920 train_time:85557ms step_avg:55.74ms
step:1536/1920 train_time:85646ms step_avg:55.76ms
step:1537/1920 train_time:85737ms step_avg:55.78ms
step:1538/1920 train_time:85825ms step_avg:55.80ms
step:1539/1920 train_time:85915ms step_avg:55.83ms
step:1540/1920 train_time:86002ms step_avg:55.85ms
step:1541/1920 train_time:86091ms step_avg:55.87ms
step:1542/1920 train_time:86178ms step_avg:55.89ms
step:1543/1920 train_time:86265ms step_avg:55.91ms
step:1544/1920 train_time:86353ms step_avg:55.93ms
step:1545/1920 train_time:86443ms step_avg:55.95ms
step:1546/1920 train_time:86532ms step_avg:55.97ms
step:1547/1920 train_time:86621ms step_avg:55.99ms
step:1548/1920 train_time:86710ms step_avg:56.01ms
step:1549/1920 train_time:86800ms step_avg:56.04ms
step:1550/1920 train_time:86888ms step_avg:56.06ms
step:1551/1920 train_time:86977ms step_avg:56.08ms
step:1552/1920 train_time:87065ms step_avg:56.10ms
step:1553/1920 train_time:87154ms step_avg:56.12ms
step:1554/1920 train_time:87241ms step_avg:56.14ms
step:1555/1920 train_time:87329ms step_avg:56.16ms
step:1556/1920 train_time:87417ms step_avg:56.18ms
step:1557/1920 train_time:87507ms step_avg:56.20ms
step:1558/1920 train_time:87596ms step_avg:56.22ms
step:1559/1920 train_time:87686ms step_avg:56.24ms
step:1560/1920 train_time:87775ms step_avg:56.27ms
step:1561/1920 train_time:87865ms step_avg:56.29ms
step:1562/1920 train_time:87953ms step_avg:56.31ms
step:1563/1920 train_time:88041ms step_avg:56.33ms
step:1564/1920 train_time:88128ms step_avg:56.35ms
step:1565/1920 train_time:88217ms step_avg:56.37ms
step:1566/1920 train_time:88305ms step_avg:56.39ms
step:1567/1920 train_time:88394ms step_avg:56.41ms
step:1568/1920 train_time:88482ms step_avg:56.43ms
step:1569/1920 train_time:88572ms step_avg:56.45ms
step:1570/1920 train_time:88660ms step_avg:56.47ms
step:1571/1920 train_time:88748ms step_avg:56.49ms
step:1572/1920 train_time:88838ms step_avg:56.51ms
step:1573/1920 train_time:88926ms step_avg:56.53ms
step:1574/1920 train_time:89015ms step_avg:56.55ms
step:1575/1920 train_time:89104ms step_avg:56.57ms
step:1576/1920 train_time:89191ms step_avg:56.59ms
step:1577/1920 train_time:89280ms step_avg:56.61ms
step:1578/1920 train_time:89368ms step_avg:56.63ms
step:1579/1920 train_time:89458ms step_avg:56.66ms
step:1580/1920 train_time:89547ms step_avg:56.68ms
step:1581/1920 train_time:89636ms step_avg:56.70ms
step:1582/1920 train_time:89725ms step_avg:56.72ms
step:1583/1920 train_time:89814ms step_avg:56.74ms
step:1584/1920 train_time:89903ms step_avg:56.76ms
step:1585/1920 train_time:89991ms step_avg:56.78ms
step:1586/1920 train_time:90078ms step_avg:56.80ms
step:1587/1920 train_time:90167ms step_avg:56.82ms
step:1588/1920 train_time:90255ms step_avg:56.84ms
step:1589/1920 train_time:90344ms step_avg:56.86ms
step:1590/1920 train_time:90432ms step_avg:56.88ms
step:1591/1920 train_time:90521ms step_avg:56.90ms
step:1592/1920 train_time:90609ms step_avg:56.92ms
step:1593/1920 train_time:90698ms step_avg:56.94ms
step:1594/1920 train_time:90787ms step_avg:56.96ms
step:1595/1920 train_time:90877ms step_avg:56.98ms
step:1596/1920 train_time:90966ms step_avg:57.00ms
step:1597/1920 train_time:91056ms step_avg:57.02ms
step:1598/1920 train_time:91144ms step_avg:57.04ms
step:1599/1920 train_time:91233ms step_avg:57.06ms
step:1600/1920 train_time:91321ms step_avg:57.08ms
step:1601/1920 train_time:91410ms step_avg:57.10ms
step:1602/1920 train_time:91498ms step_avg:57.11ms
step:1603/1920 train_time:91587ms step_avg:57.13ms
step:1604/1920 train_time:91675ms step_avg:57.15ms
step:1605/1920 train_time:91764ms step_avg:57.17ms
step:1606/1920 train_time:91853ms step_avg:57.19ms
step:1607/1920 train_time:91942ms step_avg:57.21ms
step:1608/1920 train_time:92030ms step_avg:57.23ms
step:1609/1920 train_time:92119ms step_avg:57.25ms
step:1610/1920 train_time:92207ms step_avg:57.27ms
step:1611/1920 train_time:92295ms step_avg:57.29ms
step:1612/1920 train_time:92383ms step_avg:57.31ms
step:1613/1920 train_time:92473ms step_avg:57.33ms
step:1614/1920 train_time:92560ms step_avg:57.35ms
step:1615/1920 train_time:92649ms step_avg:57.37ms
step:1616/1920 train_time:92737ms step_avg:57.39ms
step:1617/1920 train_time:92826ms step_avg:57.41ms
step:1618/1920 train_time:92914ms step_avg:57.43ms
step:1619/1920 train_time:93002ms step_avg:57.44ms
step:1620/1920 train_time:93090ms step_avg:57.46ms
step:1621/1920 train_time:93178ms step_avg:57.48ms
step:1622/1920 train_time:93267ms step_avg:57.50ms
step:1623/1920 train_time:93357ms step_avg:57.52ms
step:1624/1920 train_time:93446ms step_avg:57.54ms
step:1625/1920 train_time:93536ms step_avg:57.56ms
step:1626/1920 train_time:93624ms step_avg:57.58ms
step:1627/1920 train_time:93713ms step_avg:57.60ms
step:1628/1920 train_time:93801ms step_avg:57.62ms
step:1629/1920 train_time:93890ms step_avg:57.64ms
step:1630/1920 train_time:93979ms step_avg:57.66ms
step:1631/1920 train_time:94068ms step_avg:57.68ms
step:1632/1920 train_time:94156ms step_avg:57.69ms
step:1633/1920 train_time:94244ms step_avg:57.71ms
step:1634/1920 train_time:94332ms step_avg:57.73ms
step:1635/1920 train_time:94421ms step_avg:57.75ms
step:1636/1920 train_time:94509ms step_avg:57.77ms
step:1637/1920 train_time:94599ms step_avg:57.79ms
step:1638/1920 train_time:94688ms step_avg:57.81ms
step:1639/1920 train_time:94778ms step_avg:57.83ms
step:1640/1920 train_time:94866ms step_avg:57.85ms
step:1641/1920 train_time:94956ms step_avg:57.86ms
step:1642/1920 train_time:95045ms step_avg:57.88ms
step:1643/1920 train_time:95134ms step_avg:57.90ms
step:1644/1920 train_time:95221ms step_avg:57.92ms
step:1645/1920 train_time:95310ms step_avg:57.94ms
step:1646/1920 train_time:95398ms step_avg:57.96ms
step:1647/1920 train_time:95487ms step_avg:57.98ms
step:1648/1920 train_time:95575ms step_avg:57.99ms
step:1649/1920 train_time:95664ms step_avg:58.01ms
step:1650/1920 train_time:95752ms step_avg:58.03ms
step:1651/1920 train_time:95840ms step_avg:58.05ms
step:1652/1920 train_time:95928ms step_avg:58.07ms
step:1653/1920 train_time:96018ms step_avg:58.09ms
step:1654/1920 train_time:96108ms step_avg:58.11ms
step:1655/1920 train_time:96197ms step_avg:58.13ms
step:1656/1920 train_time:96285ms step_avg:58.14ms
step:1657/1920 train_time:96374ms step_avg:58.16ms
step:1658/1920 train_time:96462ms step_avg:58.18ms
step:1659/1920 train_time:96552ms step_avg:58.20ms
step:1660/1920 train_time:96640ms step_avg:58.22ms
step:1661/1920 train_time:96729ms step_avg:58.24ms
step:1662/1920 train_time:96817ms step_avg:58.25ms
step:1663/1920 train_time:96906ms step_avg:58.27ms
step:1664/1920 train_time:96994ms step_avg:58.29ms
step:1665/1920 train_time:97082ms step_avg:58.31ms
step:1666/1920 train_time:97171ms step_avg:58.33ms
step:1667/1920 train_time:97259ms step_avg:58.34ms
step:1668/1920 train_time:97349ms step_avg:58.36ms
step:1669/1920 train_time:97438ms step_avg:58.38ms
step:1670/1920 train_time:97527ms step_avg:58.40ms
step:1671/1920 train_time:97617ms step_avg:58.42ms
step:1672/1920 train_time:97706ms step_avg:58.44ms
step:1673/1920 train_time:97795ms step_avg:58.46ms
step:1674/1920 train_time:97884ms step_avg:58.47ms
step:1675/1920 train_time:97974ms step_avg:58.49ms
step:1676/1920 train_time:98061ms step_avg:58.51ms
step:1677/1920 train_time:98151ms step_avg:58.53ms
step:1678/1920 train_time:98239ms step_avg:58.55ms
step:1679/1920 train_time:98328ms step_avg:58.56ms
step:1680/1920 train_time:98416ms step_avg:58.58ms
step:1681/1920 train_time:98505ms step_avg:58.60ms
step:1682/1920 train_time:98593ms step_avg:58.62ms
step:1683/1920 train_time:98683ms step_avg:58.64ms
step:1684/1920 train_time:98771ms step_avg:58.65ms
step:1685/1920 train_time:98860ms step_avg:58.67ms
step:1686/1920 train_time:98949ms step_avg:58.69ms
step:1687/1920 train_time:99038ms step_avg:58.71ms
step:1688/1920 train_time:99127ms step_avg:58.72ms
step:1689/1920 train_time:99217ms step_avg:58.74ms
step:1690/1920 train_time:99306ms step_avg:58.76ms
step:1691/1920 train_time:99395ms step_avg:58.78ms
step:1692/1920 train_time:99482ms step_avg:58.80ms
step:1693/1920 train_time:99571ms step_avg:58.81ms
step:1694/1920 train_time:99659ms step_avg:58.83ms
step:1695/1920 train_time:99748ms step_avg:58.85ms
step:1696/1920 train_time:99836ms step_avg:58.87ms
step:1697/1920 train_time:99924ms step_avg:58.88ms
step:1698/1920 train_time:100012ms step_avg:58.90ms
step:1699/1920 train_time:100101ms step_avg:58.92ms
step:1700/1920 train_time:100189ms step_avg:58.93ms
step:1701/1920 train_time:100278ms step_avg:58.95ms
step:1702/1920 train_time:100367ms step_avg:58.97ms
step:1703/1920 train_time:100457ms step_avg:58.99ms
step:1704/1920 train_time:100546ms step_avg:59.01ms
step:1705/1920 train_time:100635ms step_avg:59.02ms
step:1706/1920 train_time:100722ms step_avg:59.04ms
step:1707/1920 train_time:100812ms step_avg:59.06ms
step:1708/1920 train_time:100900ms step_avg:59.07ms
step:1709/1920 train_time:100988ms step_avg:59.09ms
step:1710/1920 train_time:101077ms step_avg:59.11ms
step:1711/1920 train_time:101165ms step_avg:59.13ms
step:1712/1920 train_time:101253ms step_avg:59.14ms
step:1713/1920 train_time:101342ms step_avg:59.16ms
step:1714/1920 train_time:101431ms step_avg:59.18ms
step:1715/1920 train_time:101520ms step_avg:59.20ms
step:1716/1920 train_time:101608ms step_avg:59.21ms
step:1717/1920 train_time:101696ms step_avg:59.23ms
step:1718/1920 train_time:101785ms step_avg:59.25ms
step:1719/1920 train_time:101874ms step_avg:59.26ms
step:1720/1920 train_time:101962ms step_avg:59.28ms
step:1721/1920 train_time:102053ms step_avg:59.30ms
step:1722/1920 train_time:102140ms step_avg:59.31ms
step:1723/1920 train_time:102229ms step_avg:59.33ms
step:1724/1920 train_time:102316ms step_avg:59.35ms
step:1725/1920 train_time:102405ms step_avg:59.36ms
step:1726/1920 train_time:102492ms step_avg:59.38ms
step:1727/1920 train_time:102581ms step_avg:59.40ms
step:1728/1920 train_time:102668ms step_avg:59.41ms
step:1729/1920 train_time:102758ms step_avg:59.43ms
step:1730/1920 train_time:102846ms step_avg:59.45ms
step:1731/1920 train_time:102936ms step_avg:59.47ms
step:1732/1920 train_time:103025ms step_avg:59.48ms
step:1733/1920 train_time:103114ms step_avg:59.50ms
step:1734/1920 train_time:103202ms step_avg:59.52ms
step:1735/1920 train_time:103290ms step_avg:59.53ms
step:1736/1920 train_time:103379ms step_avg:59.55ms
step:1737/1920 train_time:103468ms step_avg:59.57ms
step:1738/1920 train_time:103556ms step_avg:59.58ms
step:1739/1920 train_time:103645ms step_avg:59.60ms
step:1740/1920 train_time:103734ms step_avg:59.62ms
step:1741/1920 train_time:103822ms step_avg:59.63ms
step:1742/1920 train_time:103910ms step_avg:59.65ms
step:1743/1920 train_time:103999ms step_avg:59.67ms
step:1744/1920 train_time:104089ms step_avg:59.68ms
step:1745/1920 train_time:104179ms step_avg:59.70ms
step:1746/1920 train_time:104268ms step_avg:59.72ms
step:1747/1920 train_time:104358ms step_avg:59.74ms
step:1748/1920 train_time:104446ms step_avg:59.75ms
step:1749/1920 train_time:104534ms step_avg:59.77ms
step:1750/1920 train_time:104622ms step_avg:59.78ms
step:1750/1920 val_loss:3.3233 train_time:104714ms step_avg:59.84ms
step:1751/1920 train_time:104732ms step_avg:59.81ms
step:1752/1920 train_time:104803ms step_avg:59.82ms
step:1753/1920 train_time:104897ms step_avg:59.84ms
step:1754/1920 train_time:104987ms step_avg:59.86ms
step:1755/1920 train_time:105075ms step_avg:59.87ms
step:1756/1920 train_time:105163ms step_avg:59.89ms
step:1757/1920 train_time:105250ms step_avg:59.90ms
step:1758/1920 train_time:105338ms step_avg:59.92ms
step:1759/1920 train_time:105425ms step_avg:59.93ms
step:1760/1920 train_time:105513ms step_avg:59.95ms
step:1761/1920 train_time:105601ms step_avg:59.97ms
step:1762/1920 train_time:105689ms step_avg:59.98ms
step:1763/1920 train_time:105781ms step_avg:60.00ms
step:1764/1920 train_time:105872ms step_avg:60.02ms
step:1765/1920 train_time:105963ms step_avg:60.04ms
step:1766/1920 train_time:106051ms step_avg:60.05ms
step:1767/1920 train_time:106139ms step_avg:60.07ms
step:1768/1920 train_time:106226ms step_avg:60.08ms
step:1769/1920 train_time:106314ms step_avg:60.10ms
step:1770/1920 train_time:106402ms step_avg:60.11ms
step:1771/1920 train_time:106488ms step_avg:60.13ms
step:1772/1920 train_time:106575ms step_avg:60.14ms
step:1773/1920 train_time:106664ms step_avg:60.16ms
step:1774/1920 train_time:106754ms step_avg:60.18ms
step:1775/1920 train_time:106846ms step_avg:60.19ms
step:1776/1920 train_time:106935ms step_avg:60.21ms
step:1777/1920 train_time:107026ms step_avg:60.23ms
step:1778/1920 train_time:107115ms step_avg:60.24ms
step:1779/1920 train_time:107204ms step_avg:60.26ms
step:1780/1920 train_time:107292ms step_avg:60.28ms
step:1781/1920 train_time:107381ms step_avg:60.29ms
step:1782/1920 train_time:107467ms step_avg:60.31ms
step:1783/1920 train_time:107555ms step_avg:60.32ms
step:1784/1920 train_time:107643ms step_avg:60.34ms
step:1785/1920 train_time:107732ms step_avg:60.35ms
step:1786/1920 train_time:107821ms step_avg:60.37ms
step:1787/1920 train_time:107911ms step_avg:60.39ms
step:1788/1920 train_time:108001ms step_avg:60.40ms
step:1789/1920 train_time:108089ms step_avg:60.42ms
step:1790/1920 train_time:108177ms step_avg:60.43ms
step:1791/1920 train_time:108266ms step_avg:60.45ms
step:1792/1920 train_time:108354ms step_avg:60.47ms
step:1793/1920 train_time:108443ms step_avg:60.48ms
step:1794/1920 train_time:108530ms step_avg:60.50ms
step:1795/1920 train_time:108618ms step_avg:60.51ms
step:1796/1920 train_time:108706ms step_avg:60.53ms
step:1797/1920 train_time:108795ms step_avg:60.54ms
step:1798/1920 train_time:108883ms step_avg:60.56ms
step:1799/1920 train_time:108973ms step_avg:60.57ms
step:1800/1920 train_time:109063ms step_avg:60.59ms
step:1801/1920 train_time:109151ms step_avg:60.61ms
step:1802/1920 train_time:109240ms step_avg:60.62ms
step:1803/1920 train_time:109328ms step_avg:60.64ms
step:1804/1920 train_time:109416ms step_avg:60.65ms
step:1805/1920 train_time:109505ms step_avg:60.67ms
step:1806/1920 train_time:109593ms step_avg:60.68ms
step:1807/1920 train_time:109682ms step_avg:60.70ms
step:1808/1920 train_time:109769ms step_avg:60.71ms
step:1809/1920 train_time:109860ms step_avg:60.73ms
step:1810/1920 train_time:109948ms step_avg:60.74ms
step:1811/1920 train_time:110038ms step_avg:60.76ms
step:1812/1920 train_time:110126ms step_avg:60.78ms
step:1813/1920 train_time:110214ms step_avg:60.79ms
step:1814/1920 train_time:110302ms step_avg:60.81ms
step:1815/1920 train_time:110390ms step_avg:60.82ms
step:1816/1920 train_time:110478ms step_avg:60.84ms
step:1817/1920 train_time:110566ms step_avg:60.85ms
step:1818/1920 train_time:110655ms step_avg:60.87ms
step:1819/1920 train_time:110745ms step_avg:60.88ms
step:1820/1920 train_time:110834ms step_avg:60.90ms
step:1821/1920 train_time:110923ms step_avg:60.91ms
step:1822/1920 train_time:111011ms step_avg:60.93ms
step:1823/1920 train_time:111100ms step_avg:60.94ms
step:1824/1920 train_time:111188ms step_avg:60.96ms
step:1825/1920 train_time:111277ms step_avg:60.97ms
step:1826/1920 train_time:111364ms step_avg:60.99ms
step:1827/1920 train_time:111452ms step_avg:61.00ms
step:1828/1920 train_time:111541ms step_avg:61.02ms
step:1829/1920 train_time:111629ms step_avg:61.03ms
step:1830/1920 train_time:111717ms step_avg:61.05ms
step:1831/1920 train_time:111806ms step_avg:61.06ms
step:1832/1920 train_time:111894ms step_avg:61.08ms
step:1833/1920 train_time:111984ms step_avg:61.09ms
step:1834/1920 train_time:112073ms step_avg:61.11ms
step:1835/1920 train_time:112161ms step_avg:61.12ms
step:1836/1920 train_time:112249ms step_avg:61.14ms
step:1837/1920 train_time:112337ms step_avg:61.15ms
step:1838/1920 train_time:112425ms step_avg:61.17ms
step:1839/1920 train_time:112514ms step_avg:61.18ms
step:1840/1920 train_time:112602ms step_avg:61.20ms
step:1841/1920 train_time:112691ms step_avg:61.21ms
step:1842/1920 train_time:112779ms step_avg:61.23ms
step:1843/1920 train_time:112868ms step_avg:61.24ms
step:1844/1920 train_time:112957ms step_avg:61.26ms
step:1845/1920 train_time:113047ms step_avg:61.27ms
step:1846/1920 train_time:113136ms step_avg:61.29ms
step:1847/1920 train_time:113225ms step_avg:61.30ms
step:1848/1920 train_time:113313ms step_avg:61.32ms
step:1849/1920 train_time:113403ms step_avg:61.33ms
step:1850/1920 train_time:113490ms step_avg:61.35ms
step:1851/1920 train_time:113578ms step_avg:61.36ms
step:1852/1920 train_time:113666ms step_avg:61.37ms
step:1853/1920 train_time:113756ms step_avg:61.39ms
step:1854/1920 train_time:113844ms step_avg:61.40ms
step:1855/1920 train_time:113933ms step_avg:61.42ms
step:1856/1920 train_time:114021ms step_avg:61.43ms
step:1857/1920 train_time:114110ms step_avg:61.45ms
step:1858/1920 train_time:114197ms step_avg:61.46ms
step:1859/1920 train_time:114286ms step_avg:61.48ms
step:1860/1920 train_time:114374ms step_avg:61.49ms
step:1861/1920 train_time:114464ms step_avg:61.51ms
step:1862/1920 train_time:114552ms step_avg:61.52ms
step:1863/1920 train_time:114641ms step_avg:61.54ms
step:1864/1920 train_time:114729ms step_avg:61.55ms
step:1865/1920 train_time:114819ms step_avg:61.57ms
step:1866/1920 train_time:114907ms step_avg:61.58ms
step:1867/1920 train_time:114996ms step_avg:61.59ms
step:1868/1920 train_time:115084ms step_avg:61.61ms
step:1869/1920 train_time:115172ms step_avg:61.62ms
step:1870/1920 train_time:115260ms step_avg:61.64ms
step:1871/1920 train_time:115349ms step_avg:61.65ms
step:1872/1920 train_time:115438ms step_avg:61.67ms
step:1873/1920 train_time:115527ms step_avg:61.68ms
step:1874/1920 train_time:115616ms step_avg:61.69ms
step:1875/1920 train_time:115704ms step_avg:61.71ms
step:1876/1920 train_time:115792ms step_avg:61.72ms
step:1877/1920 train_time:115881ms step_avg:61.74ms
step:1878/1920 train_time:115969ms step_avg:61.75ms
step:1879/1920 train_time:116059ms step_avg:61.77ms
step:1880/1920 train_time:116146ms step_avg:61.78ms
step:1881/1920 train_time:116235ms step_avg:61.79ms
step:1882/1920 train_time:116323ms step_avg:61.81ms
step:1883/1920 train_time:116413ms step_avg:61.82ms
step:1884/1920 train_time:116501ms step_avg:61.84ms
step:1885/1920 train_time:116590ms step_avg:61.85ms
step:1886/1920 train_time:116678ms step_avg:61.87ms
step:1887/1920 train_time:116767ms step_avg:61.88ms
step:1888/1920 train_time:116856ms step_avg:61.89ms
step:1889/1920 train_time:116946ms step_avg:61.91ms
step:1890/1920 train_time:117034ms step_avg:61.92ms
step:1891/1920 train_time:117124ms step_avg:61.94ms
step:1892/1920 train_time:117212ms step_avg:61.95ms
step:1893/1920 train_time:117301ms step_avg:61.97ms
step:1894/1920 train_time:117389ms step_avg:61.98ms
step:1895/1920 train_time:117479ms step_avg:61.99ms
step:1896/1920 train_time:117567ms step_avg:62.01ms
step:1897/1920 train_time:117656ms step_avg:62.02ms
step:1898/1920 train_time:117744ms step_avg:62.04ms
step:1899/1920 train_time:117833ms step_avg:62.05ms
step:1900/1920 train_time:117923ms step_avg:62.06ms
step:1901/1920 train_time:118013ms step_avg:62.08ms
step:1902/1920 train_time:118102ms step_avg:62.09ms
step:1903/1920 train_time:118192ms step_avg:62.11ms
step:1904/1920 train_time:118280ms step_avg:62.12ms
step:1905/1920 train_time:118369ms step_avg:62.14ms
step:1906/1920 train_time:118456ms step_avg:62.15ms
step:1907/1920 train_time:118547ms step_avg:62.16ms
step:1908/1920 train_time:118636ms step_avg:62.18ms
step:1909/1920 train_time:118726ms step_avg:62.19ms
step:1910/1920 train_time:118814ms step_avg:62.21ms
step:1911/1920 train_time:118904ms step_avg:62.22ms
step:1912/1920 train_time:118992ms step_avg:62.23ms
step:1913/1920 train_time:119084ms step_avg:62.25ms
step:1914/1920 train_time:119173ms step_avg:62.26ms
step:1915/1920 train_time:119261ms step_avg:62.28ms
step:1916/1920 train_time:119349ms step_avg:62.29ms
step:1917/1920 train_time:119440ms step_avg:62.31ms
step:1918/1920 train_time:119527ms step_avg:62.32ms
step:1919/1920 train_time:119616ms step_avg:62.33ms
step:1920/1920 train_time:119704ms step_avg:62.35ms
step:1920/1920 val_loss:3.2783 train_time:119794ms step_avg:62.39ms
peak memory allocated: 29976 MiB reserved: 44378 MiB

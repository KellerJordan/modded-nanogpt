import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 10:54:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   42C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    219710      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    219711      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    219712      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    219713      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    219714      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    219715      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    219716      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    219717      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    219711      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    219712      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    219713      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    219714      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    219715      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    219716      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    219717      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8367 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:67ms step_avg:67.19ms
step:2/1920 train_time:90ms step_avg:45.15ms
step:3/1920 train_time:118ms step_avg:39.25ms
step:4/1920 train_time:152ms step_avg:37.93ms
step:5/1920 train_time:186ms step_avg:37.13ms
step:6/1920 train_time:261ms step_avg:43.47ms
step:7/1920 train_time:284ms step_avg:40.60ms
step:8/1920 train_time:318ms step_avg:39.81ms
step:9/1920 train_time:352ms step_avg:39.16ms
step:10/1920 train_time:386ms step_avg:38.64ms
step:11/1920 train_time:421ms step_avg:38.24ms
step:12/1920 train_time:455ms step_avg:37.89ms
step:13/1920 train_time:489ms step_avg:37.63ms
step:14/1920 train_time:523ms step_avg:37.38ms
step:15/1920 train_time:558ms step_avg:37.18ms
step:16/1920 train_time:592ms step_avg:37.00ms
step:17/1920 train_time:626ms step_avg:36.85ms
step:18/1920 train_time:661ms step_avg:36.70ms
step:19/1920 train_time:695ms step_avg:36.58ms
step:20/1920 train_time:729ms step_avg:36.46ms
step:21/1920 train_time:764ms step_avg:36.36ms
step:22/1920 train_time:798ms step_avg:36.26ms
step:23/1920 train_time:832ms step_avg:36.18ms
step:24/1920 train_time:866ms step_avg:36.10ms
step:25/1920 train_time:900ms step_avg:36.02ms
step:26/1920 train_time:935ms step_avg:35.94ms
step:27/1920 train_time:969ms step_avg:35.89ms
step:28/1920 train_time:1004ms step_avg:35.84ms
step:29/1920 train_time:1038ms step_avg:35.79ms
step:30/1920 train_time:1072ms step_avg:35.74ms
step:31/1920 train_time:1107ms step_avg:35.71ms
step:32/1920 train_time:1141ms step_avg:35.66ms
step:33/1920 train_time:1177ms step_avg:35.65ms
step:34/1920 train_time:1211ms step_avg:35.63ms
step:35/1920 train_time:1247ms step_avg:35.62ms
step:36/1920 train_time:1281ms step_avg:35.60ms
step:37/1920 train_time:1317ms step_avg:35.59ms
step:38/1920 train_time:1351ms step_avg:35.56ms
step:39/1920 train_time:1386ms step_avg:35.53ms
step:40/1920 train_time:1420ms step_avg:35.50ms
step:41/1920 train_time:1455ms step_avg:35.48ms
step:42/1920 train_time:1489ms step_avg:35.45ms
step:43/1920 train_time:1523ms step_avg:35.43ms
step:44/1920 train_time:1558ms step_avg:35.40ms
step:45/1920 train_time:1593ms step_avg:35.39ms
step:46/1920 train_time:1627ms step_avg:35.37ms
step:47/1920 train_time:1661ms step_avg:35.34ms
step:48/1920 train_time:1696ms step_avg:35.32ms
step:49/1920 train_time:1730ms step_avg:35.30ms
step:50/1920 train_time:1764ms step_avg:35.29ms
step:51/1920 train_time:1798ms step_avg:35.26ms
step:52/1920 train_time:1833ms step_avg:35.25ms
step:53/1920 train_time:1867ms step_avg:35.22ms
step:54/1920 train_time:1901ms step_avg:35.20ms
step:55/1920 train_time:1935ms step_avg:35.19ms
step:56/1920 train_time:1970ms step_avg:35.17ms
step:57/1920 train_time:2004ms step_avg:35.16ms
step:58/1920 train_time:2038ms step_avg:35.14ms
step:59/1920 train_time:2073ms step_avg:35.13ms
step:60/1920 train_time:2107ms step_avg:35.12ms
step:61/1920 train_time:2142ms step_avg:35.11ms
step:62/1920 train_time:2177ms step_avg:35.11ms
step:63/1920 train_time:2212ms step_avg:35.11ms
step:64/1920 train_time:2246ms step_avg:35.10ms
step:65/1920 train_time:2281ms step_avg:35.09ms
step:66/1920 train_time:2315ms step_avg:35.08ms
step:67/1920 train_time:2350ms step_avg:35.08ms
step:68/1920 train_time:2385ms step_avg:35.07ms
step:69/1920 train_time:2419ms step_avg:35.06ms
step:70/1920 train_time:2454ms step_avg:35.05ms
step:71/1920 train_time:2488ms step_avg:35.04ms
step:72/1920 train_time:2522ms step_avg:35.03ms
step:73/1920 train_time:2557ms step_avg:35.03ms
step:74/1920 train_time:2591ms step_avg:35.01ms
step:75/1920 train_time:2626ms step_avg:35.01ms
step:76/1920 train_time:2660ms step_avg:35.00ms
step:77/1920 train_time:2695ms step_avg:35.00ms
step:78/1920 train_time:2729ms step_avg:34.99ms
step:79/1920 train_time:2763ms step_avg:34.98ms
step:80/1920 train_time:2798ms step_avg:34.97ms
step:81/1920 train_time:2832ms step_avg:34.97ms
step:82/1920 train_time:2867ms step_avg:34.96ms
step:83/1920 train_time:2901ms step_avg:34.95ms
step:84/1920 train_time:2935ms step_avg:34.94ms
step:85/1920 train_time:2969ms step_avg:34.93ms
step:86/1920 train_time:3004ms step_avg:34.93ms
step:87/1920 train_time:3038ms step_avg:34.92ms
step:88/1920 train_time:3073ms step_avg:34.92ms
step:89/1920 train_time:3107ms step_avg:34.92ms
step:90/1920 train_time:3142ms step_avg:34.91ms
step:91/1920 train_time:3176ms step_avg:34.91ms
step:92/1920 train_time:3211ms step_avg:34.90ms
step:93/1920 train_time:3245ms step_avg:34.89ms
step:94/1920 train_time:3279ms step_avg:34.88ms
step:95/1920 train_time:3314ms step_avg:34.89ms
step:96/1920 train_time:3349ms step_avg:34.88ms
step:97/1920 train_time:3384ms step_avg:34.88ms
step:98/1920 train_time:3418ms step_avg:34.88ms
step:99/1920 train_time:3453ms step_avg:34.87ms
step:100/1920 train_time:3487ms step_avg:34.87ms
step:101/1920 train_time:3521ms step_avg:34.86ms
step:102/1920 train_time:3555ms step_avg:34.86ms
step:103/1920 train_time:3590ms step_avg:34.85ms
step:104/1920 train_time:3624ms step_avg:34.84ms
step:105/1920 train_time:3658ms step_avg:34.84ms
step:106/1920 train_time:3693ms step_avg:34.84ms
step:107/1920 train_time:3727ms step_avg:34.83ms
step:108/1920 train_time:3761ms step_avg:34.83ms
step:109/1920 train_time:3796ms step_avg:34.83ms
step:110/1920 train_time:3830ms step_avg:34.82ms
step:111/1920 train_time:3865ms step_avg:34.82ms
step:112/1920 train_time:3899ms step_avg:34.81ms
step:113/1920 train_time:3934ms step_avg:34.81ms
step:114/1920 train_time:3968ms step_avg:34.81ms
step:115/1920 train_time:4002ms step_avg:34.80ms
step:116/1920 train_time:4037ms step_avg:34.80ms
step:117/1920 train_time:4072ms step_avg:34.80ms
step:118/1920 train_time:4106ms step_avg:34.80ms
step:119/1920 train_time:4140ms step_avg:34.79ms
step:120/1920 train_time:4175ms step_avg:34.79ms
step:121/1920 train_time:4209ms step_avg:34.79ms
step:122/1920 train_time:4244ms step_avg:34.79ms
step:123/1920 train_time:4278ms step_avg:34.78ms
step:124/1920 train_time:4313ms step_avg:34.78ms
step:125/1920 train_time:4347ms step_avg:34.78ms
step:126/1920 train_time:4381ms step_avg:34.77ms
step:127/1920 train_time:4416ms step_avg:34.77ms
step:128/1920 train_time:4451ms step_avg:34.77ms
step:129/1920 train_time:4485ms step_avg:34.77ms
step:130/1920 train_time:4519ms step_avg:34.76ms
step:131/1920 train_time:4554ms step_avg:34.76ms
step:132/1920 train_time:4588ms step_avg:34.76ms
step:133/1920 train_time:4623ms step_avg:34.76ms
step:134/1920 train_time:4657ms step_avg:34.75ms
step:135/1920 train_time:4692ms step_avg:34.75ms
step:136/1920 train_time:4726ms step_avg:34.75ms
step:137/1920 train_time:4760ms step_avg:34.75ms
step:138/1920 train_time:4794ms step_avg:34.74ms
step:139/1920 train_time:4830ms step_avg:34.75ms
step:140/1920 train_time:4864ms step_avg:34.74ms
step:141/1920 train_time:4898ms step_avg:34.74ms
step:142/1920 train_time:4933ms step_avg:34.74ms
step:143/1920 train_time:4967ms step_avg:34.73ms
step:144/1920 train_time:5001ms step_avg:34.73ms
step:145/1920 train_time:5036ms step_avg:34.73ms
step:146/1920 train_time:5070ms step_avg:34.73ms
step:147/1920 train_time:5105ms step_avg:34.73ms
step:148/1920 train_time:5139ms step_avg:34.72ms
step:149/1920 train_time:5174ms step_avg:34.73ms
step:150/1920 train_time:5208ms step_avg:34.72ms
step:151/1920 train_time:5243ms step_avg:34.72ms
step:152/1920 train_time:5277ms step_avg:34.72ms
step:153/1920 train_time:5313ms step_avg:34.72ms
step:154/1920 train_time:5347ms step_avg:34.72ms
step:155/1920 train_time:5381ms step_avg:34.72ms
step:156/1920 train_time:5416ms step_avg:34.72ms
step:157/1920 train_time:5450ms step_avg:34.72ms
step:158/1920 train_time:5485ms step_avg:34.71ms
step:159/1920 train_time:5519ms step_avg:34.71ms
step:160/1920 train_time:5554ms step_avg:34.71ms
step:161/1920 train_time:5588ms step_avg:34.71ms
step:162/1920 train_time:5622ms step_avg:34.70ms
step:163/1920 train_time:5657ms step_avg:34.71ms
step:164/1920 train_time:5691ms step_avg:34.70ms
step:165/1920 train_time:5725ms step_avg:34.70ms
step:166/1920 train_time:5760ms step_avg:34.70ms
step:167/1920 train_time:5794ms step_avg:34.70ms
step:168/1920 train_time:5828ms step_avg:34.69ms
step:169/1920 train_time:5863ms step_avg:34.69ms
step:170/1920 train_time:5897ms step_avg:34.69ms
step:171/1920 train_time:5932ms step_avg:34.69ms
step:172/1920 train_time:5966ms step_avg:34.68ms
step:173/1920 train_time:6000ms step_avg:34.68ms
step:174/1920 train_time:6034ms step_avg:34.68ms
step:175/1920 train_time:6069ms step_avg:34.68ms
step:176/1920 train_time:6103ms step_avg:34.67ms
step:177/1920 train_time:6137ms step_avg:34.67ms
step:178/1920 train_time:6172ms step_avg:34.67ms
step:179/1920 train_time:6207ms step_avg:34.67ms
step:180/1920 train_time:6241ms step_avg:34.67ms
step:181/1920 train_time:6275ms step_avg:34.67ms
step:182/1920 train_time:6309ms step_avg:34.67ms
step:183/1920 train_time:6344ms step_avg:34.67ms
step:184/1920 train_time:6378ms step_avg:34.67ms
step:185/1920 train_time:6413ms step_avg:34.66ms
step:186/1920 train_time:6447ms step_avg:34.66ms
step:187/1920 train_time:6482ms step_avg:34.66ms
step:188/1920 train_time:6516ms step_avg:34.66ms
step:189/1920 train_time:6551ms step_avg:34.66ms
step:190/1920 train_time:6585ms step_avg:34.66ms
step:191/1920 train_time:6620ms step_avg:34.66ms
step:192/1920 train_time:6654ms step_avg:34.66ms
step:193/1920 train_time:6689ms step_avg:34.66ms
step:194/1920 train_time:6723ms step_avg:34.65ms
step:195/1920 train_time:6757ms step_avg:34.65ms
step:196/1920 train_time:6792ms step_avg:34.65ms
step:197/1920 train_time:6826ms step_avg:34.65ms
step:198/1920 train_time:6860ms step_avg:34.65ms
step:199/1920 train_time:6895ms step_avg:34.65ms
step:200/1920 train_time:6929ms step_avg:34.64ms
step:201/1920 train_time:6963ms step_avg:34.64ms
step:202/1920 train_time:6997ms step_avg:34.64ms
step:203/1920 train_time:7032ms step_avg:34.64ms
step:204/1920 train_time:7066ms step_avg:34.64ms
step:205/1920 train_time:7100ms step_avg:34.64ms
step:206/1920 train_time:7135ms step_avg:34.63ms
step:207/1920 train_time:7169ms step_avg:34.63ms
step:208/1920 train_time:7203ms step_avg:34.63ms
step:209/1920 train_time:7238ms step_avg:34.63ms
step:210/1920 train_time:7272ms step_avg:34.63ms
step:211/1920 train_time:7307ms step_avg:34.63ms
step:212/1920 train_time:7341ms step_avg:34.63ms
step:213/1920 train_time:7376ms step_avg:34.63ms
step:214/1920 train_time:7410ms step_avg:34.63ms
step:215/1920 train_time:7445ms step_avg:34.63ms
step:216/1920 train_time:7479ms step_avg:34.62ms
step:217/1920 train_time:7514ms step_avg:34.63ms
step:218/1920 train_time:7548ms step_avg:34.62ms
step:219/1920 train_time:7583ms step_avg:34.62ms
step:220/1920 train_time:7617ms step_avg:34.62ms
step:221/1920 train_time:7651ms step_avg:34.62ms
step:222/1920 train_time:7686ms step_avg:34.62ms
step:223/1920 train_time:7720ms step_avg:34.62ms
step:224/1920 train_time:7755ms step_avg:34.62ms
step:225/1920 train_time:7790ms step_avg:34.62ms
step:226/1920 train_time:7824ms step_avg:34.62ms
step:227/1920 train_time:7858ms step_avg:34.62ms
step:228/1920 train_time:7892ms step_avg:34.62ms
step:229/1920 train_time:7927ms step_avg:34.61ms
step:230/1920 train_time:7961ms step_avg:34.61ms
step:231/1920 train_time:7995ms step_avg:34.61ms
step:232/1920 train_time:8029ms step_avg:34.61ms
step:233/1920 train_time:8063ms step_avg:34.61ms
step:234/1920 train_time:8098ms step_avg:34.61ms
step:235/1920 train_time:8132ms step_avg:34.60ms
step:236/1920 train_time:8166ms step_avg:34.60ms
step:237/1920 train_time:8200ms step_avg:34.60ms
step:238/1920 train_time:8234ms step_avg:34.60ms
step:239/1920 train_time:8269ms step_avg:34.60ms
step:240/1920 train_time:8303ms step_avg:34.60ms
step:241/1920 train_time:8338ms step_avg:34.60ms
step:242/1920 train_time:8372ms step_avg:34.60ms
step:243/1920 train_time:8407ms step_avg:34.60ms
step:244/1920 train_time:8441ms step_avg:34.59ms
step:245/1920 train_time:8476ms step_avg:34.60ms
step:246/1920 train_time:8510ms step_avg:34.59ms
step:247/1920 train_time:8545ms step_avg:34.59ms
step:248/1920 train_time:8579ms step_avg:34.59ms
step:249/1920 train_time:8613ms step_avg:34.59ms
step:250/1920 train_time:8648ms step_avg:34.59ms
step:250/1920 val_loss:4.6268 train_time:8685ms step_avg:34.74ms
step:251/1920 train_time:8702ms step_avg:34.67ms
step:252/1920 train_time:8720ms step_avg:34.60ms
step:253/1920 train_time:8754ms step_avg:34.60ms
step:254/1920 train_time:8789ms step_avg:34.60ms
step:255/1920 train_time:8824ms step_avg:34.60ms
step:256/1920 train_time:8858ms step_avg:34.60ms
step:257/1920 train_time:8893ms step_avg:34.60ms
step:258/1920 train_time:8927ms step_avg:34.60ms
step:259/1920 train_time:8962ms step_avg:34.60ms
step:260/1920 train_time:8996ms step_avg:34.60ms
step:261/1920 train_time:9031ms step_avg:34.60ms
step:262/1920 train_time:9065ms step_avg:34.60ms
step:263/1920 train_time:9099ms step_avg:34.60ms
step:264/1920 train_time:9133ms step_avg:34.60ms
step:265/1920 train_time:9168ms step_avg:34.60ms
step:266/1920 train_time:9202ms step_avg:34.59ms
step:267/1920 train_time:9236ms step_avg:34.59ms
step:268/1920 train_time:9270ms step_avg:34.59ms
step:269/1920 train_time:9304ms step_avg:34.59ms
step:270/1920 train_time:9338ms step_avg:34.59ms
step:271/1920 train_time:9373ms step_avg:34.59ms
step:272/1920 train_time:9407ms step_avg:34.58ms
step:273/1920 train_time:9441ms step_avg:34.58ms
step:274/1920 train_time:9475ms step_avg:34.58ms
step:275/1920 train_time:9510ms step_avg:34.58ms
step:276/1920 train_time:9544ms step_avg:34.58ms
step:277/1920 train_time:9578ms step_avg:34.58ms
step:278/1920 train_time:9613ms step_avg:34.58ms
step:279/1920 train_time:9648ms step_avg:34.58ms
step:280/1920 train_time:9683ms step_avg:34.58ms
step:281/1920 train_time:9717ms step_avg:34.58ms
step:282/1920 train_time:9752ms step_avg:34.58ms
step:283/1920 train_time:9787ms step_avg:34.58ms
step:284/1920 train_time:9821ms step_avg:34.58ms
step:285/1920 train_time:9856ms step_avg:34.58ms
step:286/1920 train_time:9890ms step_avg:34.58ms
step:287/1920 train_time:9925ms step_avg:34.58ms
step:288/1920 train_time:9960ms step_avg:34.58ms
step:289/1920 train_time:9994ms step_avg:34.58ms
step:290/1920 train_time:10028ms step_avg:34.58ms
step:291/1920 train_time:10063ms step_avg:34.58ms
step:292/1920 train_time:10097ms step_avg:34.58ms
step:293/1920 train_time:10132ms step_avg:34.58ms
step:294/1920 train_time:10166ms step_avg:34.58ms
step:295/1920 train_time:10200ms step_avg:34.58ms
step:296/1920 train_time:10234ms step_avg:34.57ms
step:297/1920 train_time:10268ms step_avg:34.57ms
step:298/1920 train_time:10303ms step_avg:34.57ms
step:299/1920 train_time:10337ms step_avg:34.57ms
step:300/1920 train_time:10371ms step_avg:34.57ms
step:301/1920 train_time:10405ms step_avg:34.57ms
step:302/1920 train_time:10440ms step_avg:34.57ms
step:303/1920 train_time:10474ms step_avg:34.57ms
step:304/1920 train_time:10508ms step_avg:34.57ms
step:305/1920 train_time:10542ms step_avg:34.56ms
step:306/1920 train_time:10576ms step_avg:34.56ms
step:307/1920 train_time:10611ms step_avg:34.56ms
step:308/1920 train_time:10645ms step_avg:34.56ms
step:309/1920 train_time:10679ms step_avg:34.56ms
step:310/1920 train_time:10714ms step_avg:34.56ms
step:311/1920 train_time:10748ms step_avg:34.56ms
step:312/1920 train_time:10783ms step_avg:34.56ms
step:313/1920 train_time:10817ms step_avg:34.56ms
step:314/1920 train_time:10851ms step_avg:34.56ms
step:315/1920 train_time:10886ms step_avg:34.56ms
step:316/1920 train_time:10920ms step_avg:34.56ms
step:317/1920 train_time:10955ms step_avg:34.56ms
step:318/1920 train_time:10989ms step_avg:34.56ms
step:319/1920 train_time:11025ms step_avg:34.56ms
step:320/1920 train_time:11059ms step_avg:34.56ms
step:321/1920 train_time:11093ms step_avg:34.56ms
step:322/1920 train_time:11128ms step_avg:34.56ms
step:323/1920 train_time:11163ms step_avg:34.56ms
step:324/1920 train_time:11197ms step_avg:34.56ms
step:325/1920 train_time:11231ms step_avg:34.56ms
step:326/1920 train_time:11266ms step_avg:34.56ms
step:327/1920 train_time:11300ms step_avg:34.56ms
step:328/1920 train_time:11334ms step_avg:34.56ms
step:329/1920 train_time:11369ms step_avg:34.56ms
step:330/1920 train_time:11403ms step_avg:34.55ms
step:331/1920 train_time:11437ms step_avg:34.55ms
step:332/1920 train_time:11472ms step_avg:34.55ms
step:333/1920 train_time:11506ms step_avg:34.55ms
step:334/1920 train_time:11540ms step_avg:34.55ms
step:335/1920 train_time:11575ms step_avg:34.55ms
step:336/1920 train_time:11609ms step_avg:34.55ms
step:337/1920 train_time:11644ms step_avg:34.55ms
step:338/1920 train_time:11679ms step_avg:34.55ms
step:339/1920 train_time:11713ms step_avg:34.55ms
step:340/1920 train_time:11747ms step_avg:34.55ms
step:341/1920 train_time:11781ms step_avg:34.55ms
step:342/1920 train_time:11815ms step_avg:34.55ms
step:343/1920 train_time:11850ms step_avg:34.55ms
step:344/1920 train_time:11884ms step_avg:34.55ms
step:345/1920 train_time:11919ms step_avg:34.55ms
step:346/1920 train_time:11953ms step_avg:34.55ms
step:347/1920 train_time:11988ms step_avg:34.55ms
step:348/1920 train_time:12022ms step_avg:34.55ms
step:349/1920 train_time:12057ms step_avg:34.55ms
step:350/1920 train_time:12091ms step_avg:34.55ms
step:351/1920 train_time:12125ms step_avg:34.54ms
step:352/1920 train_time:12159ms step_avg:34.54ms
step:353/1920 train_time:12194ms step_avg:34.54ms
step:354/1920 train_time:12228ms step_avg:34.54ms
step:355/1920 train_time:12263ms step_avg:34.54ms
step:356/1920 train_time:12297ms step_avg:34.54ms
step:357/1920 train_time:12332ms step_avg:34.54ms
step:358/1920 train_time:12366ms step_avg:34.54ms
step:359/1920 train_time:12400ms step_avg:34.54ms
step:360/1920 train_time:12434ms step_avg:34.54ms
step:361/1920 train_time:12469ms step_avg:34.54ms
step:362/1920 train_time:12503ms step_avg:34.54ms
step:363/1920 train_time:12538ms step_avg:34.54ms
step:364/1920 train_time:12572ms step_avg:34.54ms
step:365/1920 train_time:12606ms step_avg:34.54ms
step:366/1920 train_time:12640ms step_avg:34.54ms
step:367/1920 train_time:12674ms step_avg:34.54ms
step:368/1920 train_time:12709ms step_avg:34.53ms
step:369/1920 train_time:12743ms step_avg:34.54ms
step:370/1920 train_time:12778ms step_avg:34.53ms
step:371/1920 train_time:12812ms step_avg:34.53ms
step:372/1920 train_time:12846ms step_avg:34.53ms
step:373/1920 train_time:12880ms step_avg:34.53ms
step:374/1920 train_time:12915ms step_avg:34.53ms
step:375/1920 train_time:12949ms step_avg:34.53ms
step:376/1920 train_time:12983ms step_avg:34.53ms
step:377/1920 train_time:13018ms step_avg:34.53ms
step:378/1920 train_time:13052ms step_avg:34.53ms
step:379/1920 train_time:13087ms step_avg:34.53ms
step:380/1920 train_time:13121ms step_avg:34.53ms
step:381/1920 train_time:13155ms step_avg:34.53ms
step:382/1920 train_time:13190ms step_avg:34.53ms
step:383/1920 train_time:13225ms step_avg:34.53ms
step:384/1920 train_time:13259ms step_avg:34.53ms
step:385/1920 train_time:13294ms step_avg:34.53ms
step:386/1920 train_time:13328ms step_avg:34.53ms
step:387/1920 train_time:13363ms step_avg:34.53ms
step:388/1920 train_time:13397ms step_avg:34.53ms
step:389/1920 train_time:13432ms step_avg:34.53ms
step:390/1920 train_time:13466ms step_avg:34.53ms
step:391/1920 train_time:13501ms step_avg:34.53ms
step:392/1920 train_time:13535ms step_avg:34.53ms
step:393/1920 train_time:13570ms step_avg:34.53ms
step:394/1920 train_time:13604ms step_avg:34.53ms
step:395/1920 train_time:13639ms step_avg:34.53ms
step:396/1920 train_time:13673ms step_avg:34.53ms
step:397/1920 train_time:13707ms step_avg:34.53ms
step:398/1920 train_time:13742ms step_avg:34.53ms
step:399/1920 train_time:13776ms step_avg:34.53ms
step:400/1920 train_time:13810ms step_avg:34.53ms
step:401/1920 train_time:13845ms step_avg:34.53ms
step:402/1920 train_time:13879ms step_avg:34.52ms
step:403/1920 train_time:13913ms step_avg:34.52ms
step:404/1920 train_time:13948ms step_avg:34.52ms
step:405/1920 train_time:13982ms step_avg:34.52ms
step:406/1920 train_time:14016ms step_avg:34.52ms
step:407/1920 train_time:14051ms step_avg:34.52ms
step:408/1920 train_time:14085ms step_avg:34.52ms
step:409/1920 train_time:14119ms step_avg:34.52ms
step:410/1920 train_time:14153ms step_avg:34.52ms
step:411/1920 train_time:14189ms step_avg:34.52ms
step:412/1920 train_time:14223ms step_avg:34.52ms
step:413/1920 train_time:14258ms step_avg:34.52ms
step:414/1920 train_time:14292ms step_avg:34.52ms
step:415/1920 train_time:14327ms step_avg:34.52ms
step:416/1920 train_time:14361ms step_avg:34.52ms
step:417/1920 train_time:14395ms step_avg:34.52ms
step:418/1920 train_time:14429ms step_avg:34.52ms
step:419/1920 train_time:14464ms step_avg:34.52ms
step:420/1920 train_time:14498ms step_avg:34.52ms
step:421/1920 train_time:14533ms step_avg:34.52ms
step:422/1920 train_time:14567ms step_avg:34.52ms
step:423/1920 train_time:14601ms step_avg:34.52ms
step:424/1920 train_time:14635ms step_avg:34.52ms
step:425/1920 train_time:14671ms step_avg:34.52ms
step:426/1920 train_time:14705ms step_avg:34.52ms
step:427/1920 train_time:14739ms step_avg:34.52ms
step:428/1920 train_time:14773ms step_avg:34.52ms
step:429/1920 train_time:14808ms step_avg:34.52ms
step:430/1920 train_time:14842ms step_avg:34.52ms
step:431/1920 train_time:14877ms step_avg:34.52ms
step:432/1920 train_time:14911ms step_avg:34.52ms
step:433/1920 train_time:14945ms step_avg:34.52ms
step:434/1920 train_time:14979ms step_avg:34.51ms
step:435/1920 train_time:15014ms step_avg:34.51ms
step:436/1920 train_time:15048ms step_avg:34.51ms
step:437/1920 train_time:15082ms step_avg:34.51ms
step:438/1920 train_time:15116ms step_avg:34.51ms
step:439/1920 train_time:15151ms step_avg:34.51ms
step:440/1920 train_time:15185ms step_avg:34.51ms
step:441/1920 train_time:15219ms step_avg:34.51ms
step:442/1920 train_time:15254ms step_avg:34.51ms
step:443/1920 train_time:15288ms step_avg:34.51ms
step:444/1920 train_time:15322ms step_avg:34.51ms
step:445/1920 train_time:15356ms step_avg:34.51ms
step:446/1920 train_time:15391ms step_avg:34.51ms
step:447/1920 train_time:15425ms step_avg:34.51ms
step:448/1920 train_time:15459ms step_avg:34.51ms
step:449/1920 train_time:15494ms step_avg:34.51ms
step:450/1920 train_time:15528ms step_avg:34.51ms
step:451/1920 train_time:15563ms step_avg:34.51ms
step:452/1920 train_time:15597ms step_avg:34.51ms
step:453/1920 train_time:15632ms step_avg:34.51ms
step:454/1920 train_time:15666ms step_avg:34.51ms
step:455/1920 train_time:15700ms step_avg:34.51ms
step:456/1920 train_time:15735ms step_avg:34.51ms
step:457/1920 train_time:15769ms step_avg:34.51ms
step:458/1920 train_time:15804ms step_avg:34.51ms
step:459/1920 train_time:15838ms step_avg:34.51ms
step:460/1920 train_time:15872ms step_avg:34.50ms
step:461/1920 train_time:15906ms step_avg:34.50ms
step:462/1920 train_time:15941ms step_avg:34.50ms
step:463/1920 train_time:15975ms step_avg:34.50ms
step:464/1920 train_time:16009ms step_avg:34.50ms
step:465/1920 train_time:16043ms step_avg:34.50ms
step:466/1920 train_time:16077ms step_avg:34.50ms
step:467/1920 train_time:16112ms step_avg:34.50ms
step:468/1920 train_time:16146ms step_avg:34.50ms
step:469/1920 train_time:16180ms step_avg:34.50ms
step:470/1920 train_time:16214ms step_avg:34.50ms
step:471/1920 train_time:16248ms step_avg:34.50ms
step:472/1920 train_time:16283ms step_avg:34.50ms
step:473/1920 train_time:16317ms step_avg:34.50ms
step:474/1920 train_time:16351ms step_avg:34.50ms
step:475/1920 train_time:16385ms step_avg:34.50ms
step:476/1920 train_time:16420ms step_avg:34.50ms
step:477/1920 train_time:16454ms step_avg:34.49ms
step:478/1920 train_time:16488ms step_avg:34.49ms
step:479/1920 train_time:16523ms step_avg:34.49ms
step:480/1920 train_time:16557ms step_avg:34.49ms
step:481/1920 train_time:16592ms step_avg:34.49ms
step:482/1920 train_time:16626ms step_avg:34.49ms
step:483/1920 train_time:16661ms step_avg:34.49ms
step:484/1920 train_time:16695ms step_avg:34.49ms
step:485/1920 train_time:16729ms step_avg:34.49ms
step:486/1920 train_time:16764ms step_avg:34.49ms
step:487/1920 train_time:16798ms step_avg:34.49ms
step:488/1920 train_time:16832ms step_avg:34.49ms
step:489/1920 train_time:16867ms step_avg:34.49ms
step:490/1920 train_time:16901ms step_avg:34.49ms
step:491/1920 train_time:16935ms step_avg:34.49ms
step:492/1920 train_time:16970ms step_avg:34.49ms
step:493/1920 train_time:17004ms step_avg:34.49ms
step:494/1920 train_time:17038ms step_avg:34.49ms
step:495/1920 train_time:17073ms step_avg:34.49ms
step:496/1920 train_time:17107ms step_avg:34.49ms
step:497/1920 train_time:17141ms step_avg:34.49ms
step:498/1920 train_time:17175ms step_avg:34.49ms
step:499/1920 train_time:17210ms step_avg:34.49ms
step:500/1920 train_time:17244ms step_avg:34.49ms
step:500/1920 val_loss:4.2945 train_time:17281ms step_avg:34.56ms
step:501/1920 train_time:17300ms step_avg:34.53ms
step:502/1920 train_time:17318ms step_avg:34.50ms
step:503/1920 train_time:17352ms step_avg:34.50ms
step:504/1920 train_time:17387ms step_avg:34.50ms
step:505/1920 train_time:17422ms step_avg:34.50ms
step:506/1920 train_time:17456ms step_avg:34.50ms
step:507/1920 train_time:17491ms step_avg:34.50ms
step:508/1920 train_time:17525ms step_avg:34.50ms
step:509/1920 train_time:17560ms step_avg:34.50ms
step:510/1920 train_time:17594ms step_avg:34.50ms
step:511/1920 train_time:17629ms step_avg:34.50ms
step:512/1920 train_time:17663ms step_avg:34.50ms
step:513/1920 train_time:17697ms step_avg:34.50ms
step:514/1920 train_time:17731ms step_avg:34.50ms
step:515/1920 train_time:17765ms step_avg:34.50ms
step:516/1920 train_time:17799ms step_avg:34.49ms
step:517/1920 train_time:17833ms step_avg:34.49ms
step:518/1920 train_time:17867ms step_avg:34.49ms
step:519/1920 train_time:17902ms step_avg:34.49ms
step:520/1920 train_time:17936ms step_avg:34.49ms
step:521/1920 train_time:17970ms step_avg:34.49ms
step:522/1920 train_time:18004ms step_avg:34.49ms
step:523/1920 train_time:18038ms step_avg:34.49ms
step:524/1920 train_time:18072ms step_avg:34.49ms
step:525/1920 train_time:18106ms step_avg:34.49ms
step:526/1920 train_time:18141ms step_avg:34.49ms
step:527/1920 train_time:18175ms step_avg:34.49ms
step:528/1920 train_time:18209ms step_avg:34.49ms
step:529/1920 train_time:18244ms step_avg:34.49ms
step:530/1920 train_time:18278ms step_avg:34.49ms
step:531/1920 train_time:18313ms step_avg:34.49ms
step:532/1920 train_time:18347ms step_avg:34.49ms
step:533/1920 train_time:18382ms step_avg:34.49ms
step:534/1920 train_time:18416ms step_avg:34.49ms
step:535/1920 train_time:18451ms step_avg:34.49ms
step:536/1920 train_time:18485ms step_avg:34.49ms
step:537/1920 train_time:18520ms step_avg:34.49ms
step:538/1920 train_time:18554ms step_avg:34.49ms
step:539/1920 train_time:18588ms step_avg:34.49ms
step:540/1920 train_time:18622ms step_avg:34.49ms
step:541/1920 train_time:18657ms step_avg:34.49ms
step:542/1920 train_time:18691ms step_avg:34.49ms
step:543/1920 train_time:18726ms step_avg:34.49ms
step:544/1920 train_time:18760ms step_avg:34.49ms
step:545/1920 train_time:18794ms step_avg:34.49ms
step:546/1920 train_time:18829ms step_avg:34.48ms
step:547/1920 train_time:18863ms step_avg:34.48ms
step:548/1920 train_time:18897ms step_avg:34.48ms
step:549/1920 train_time:18931ms step_avg:34.48ms
step:550/1920 train_time:18966ms step_avg:34.48ms
step:551/1920 train_time:19000ms step_avg:34.48ms
step:552/1920 train_time:19034ms step_avg:34.48ms
step:553/1920 train_time:19068ms step_avg:34.48ms
step:554/1920 train_time:19103ms step_avg:34.48ms
step:555/1920 train_time:19137ms step_avg:34.48ms
step:556/1920 train_time:19171ms step_avg:34.48ms
step:557/1920 train_time:19205ms step_avg:34.48ms
step:558/1920 train_time:19240ms step_avg:34.48ms
step:559/1920 train_time:19274ms step_avg:34.48ms
step:560/1920 train_time:19308ms step_avg:34.48ms
step:561/1920 train_time:19343ms step_avg:34.48ms
step:562/1920 train_time:19377ms step_avg:34.48ms
step:563/1920 train_time:19412ms step_avg:34.48ms
step:564/1920 train_time:19446ms step_avg:34.48ms
step:565/1920 train_time:19481ms step_avg:34.48ms
step:566/1920 train_time:19515ms step_avg:34.48ms
step:567/1920 train_time:19549ms step_avg:34.48ms
step:568/1920 train_time:19584ms step_avg:34.48ms
step:569/1920 train_time:19618ms step_avg:34.48ms
step:570/1920 train_time:19653ms step_avg:34.48ms
step:571/1920 train_time:19687ms step_avg:34.48ms
step:572/1920 train_time:19722ms step_avg:34.48ms
step:573/1920 train_time:19756ms step_avg:34.48ms
step:574/1920 train_time:19790ms step_avg:34.48ms
step:575/1920 train_time:19825ms step_avg:34.48ms
step:576/1920 train_time:19859ms step_avg:34.48ms
step:577/1920 train_time:19894ms step_avg:34.48ms
step:578/1920 train_time:19928ms step_avg:34.48ms
step:579/1920 train_time:19962ms step_avg:34.48ms
step:580/1920 train_time:19997ms step_avg:34.48ms
step:581/1920 train_time:20031ms step_avg:34.48ms
step:582/1920 train_time:20065ms step_avg:34.48ms
step:583/1920 train_time:20099ms step_avg:34.47ms
step:584/1920 train_time:20133ms step_avg:34.47ms
step:585/1920 train_time:20167ms step_avg:34.47ms
step:586/1920 train_time:20202ms step_avg:34.47ms
step:587/1920 train_time:20236ms step_avg:34.47ms
step:588/1920 train_time:20270ms step_avg:34.47ms
step:589/1920 train_time:20304ms step_avg:34.47ms
step:590/1920 train_time:20339ms step_avg:34.47ms
step:591/1920 train_time:20373ms step_avg:34.47ms
step:592/1920 train_time:20407ms step_avg:34.47ms
step:593/1920 train_time:20442ms step_avg:34.47ms
step:594/1920 train_time:20476ms step_avg:34.47ms
step:595/1920 train_time:20510ms step_avg:34.47ms
step:596/1920 train_time:20545ms step_avg:34.47ms
step:597/1920 train_time:20579ms step_avg:34.47ms
step:598/1920 train_time:20613ms step_avg:34.47ms
step:599/1920 train_time:20648ms step_avg:34.47ms
step:600/1920 train_time:20682ms step_avg:34.47ms
step:601/1920 train_time:20716ms step_avg:34.47ms
step:602/1920 train_time:20751ms step_avg:34.47ms
step:603/1920 train_time:20786ms step_avg:34.47ms
step:604/1920 train_time:20820ms step_avg:34.47ms
step:605/1920 train_time:20855ms step_avg:34.47ms
step:606/1920 train_time:20889ms step_avg:34.47ms
step:607/1920 train_time:20923ms step_avg:34.47ms
step:608/1920 train_time:20957ms step_avg:34.47ms
step:609/1920 train_time:20992ms step_avg:34.47ms
step:610/1920 train_time:21026ms step_avg:34.47ms
step:611/1920 train_time:21060ms step_avg:34.47ms
step:612/1920 train_time:21094ms step_avg:34.47ms
step:613/1920 train_time:21129ms step_avg:34.47ms
step:614/1920 train_time:21163ms step_avg:34.47ms
step:615/1920 train_time:21198ms step_avg:34.47ms
step:616/1920 train_time:21232ms step_avg:34.47ms
step:617/1920 train_time:21266ms step_avg:34.47ms
step:618/1920 train_time:21300ms step_avg:34.47ms
step:619/1920 train_time:21335ms step_avg:34.47ms
step:620/1920 train_time:21369ms step_avg:34.47ms
step:621/1920 train_time:21403ms step_avg:34.47ms
step:622/1920 train_time:21437ms step_avg:34.46ms
step:623/1920 train_time:21472ms step_avg:34.47ms
step:624/1920 train_time:21506ms step_avg:34.46ms
step:625/1920 train_time:21540ms step_avg:34.46ms
step:626/1920 train_time:21575ms step_avg:34.46ms
step:627/1920 train_time:21609ms step_avg:34.46ms
step:628/1920 train_time:21644ms step_avg:34.46ms
step:629/1920 train_time:21705ms step_avg:34.51ms
step:630/1920 train_time:21767ms step_avg:34.55ms
step:631/1920 train_time:21829ms step_avg:34.59ms
step:632/1920 train_time:21890ms step_avg:34.64ms
step:633/1920 train_time:21953ms step_avg:34.68ms
step:634/1920 train_time:22015ms step_avg:34.72ms
step:635/1920 train_time:22078ms step_avg:34.77ms
step:636/1920 train_time:22140ms step_avg:34.81ms
step:637/1920 train_time:22202ms step_avg:34.85ms
step:638/1920 train_time:22263ms step_avg:34.90ms
step:639/1920 train_time:22326ms step_avg:34.94ms
step:640/1920 train_time:22387ms step_avg:34.98ms
step:641/1920 train_time:22450ms step_avg:35.02ms
step:642/1920 train_time:22511ms step_avg:35.06ms
step:643/1920 train_time:22574ms step_avg:35.11ms
step:644/1920 train_time:22635ms step_avg:35.15ms
step:645/1920 train_time:22699ms step_avg:35.19ms
step:646/1920 train_time:22760ms step_avg:35.23ms
step:647/1920 train_time:22823ms step_avg:35.28ms
step:648/1920 train_time:22885ms step_avg:35.32ms
step:649/1920 train_time:22948ms step_avg:35.36ms
step:650/1920 train_time:23009ms step_avg:35.40ms
step:651/1920 train_time:23073ms step_avg:35.44ms
step:652/1920 train_time:23135ms step_avg:35.48ms
step:653/1920 train_time:23198ms step_avg:35.53ms
step:654/1920 train_time:23260ms step_avg:35.57ms
step:655/1920 train_time:23322ms step_avg:35.61ms
step:656/1920 train_time:23383ms step_avg:35.65ms
step:657/1920 train_time:23446ms step_avg:35.69ms
step:658/1920 train_time:23508ms step_avg:35.73ms
step:659/1920 train_time:23570ms step_avg:35.77ms
step:660/1920 train_time:23632ms step_avg:35.81ms
step:661/1920 train_time:23694ms step_avg:35.85ms
step:662/1920 train_time:23757ms step_avg:35.89ms
step:663/1920 train_time:23820ms step_avg:35.93ms
step:664/1920 train_time:23881ms step_avg:35.97ms
step:665/1920 train_time:23944ms step_avg:36.01ms
step:666/1920 train_time:24006ms step_avg:36.04ms
step:667/1920 train_time:24068ms step_avg:36.08ms
step:668/1920 train_time:24131ms step_avg:36.12ms
step:669/1920 train_time:24194ms step_avg:36.16ms
step:670/1920 train_time:24256ms step_avg:36.20ms
step:671/1920 train_time:24319ms step_avg:36.24ms
step:672/1920 train_time:24381ms step_avg:36.28ms
step:673/1920 train_time:24444ms step_avg:36.32ms
step:674/1920 train_time:24505ms step_avg:36.36ms
step:675/1920 train_time:24567ms step_avg:36.40ms
step:676/1920 train_time:24629ms step_avg:36.43ms
step:677/1920 train_time:24691ms step_avg:36.47ms
step:678/1920 train_time:24753ms step_avg:36.51ms
step:679/1920 train_time:24816ms step_avg:36.55ms
step:680/1920 train_time:24878ms step_avg:36.59ms
step:681/1920 train_time:24941ms step_avg:36.62ms
step:682/1920 train_time:25004ms step_avg:36.66ms
step:683/1920 train_time:25066ms step_avg:36.70ms
step:684/1920 train_time:25127ms step_avg:36.74ms
step:685/1920 train_time:25190ms step_avg:36.77ms
step:686/1920 train_time:25252ms step_avg:36.81ms
step:687/1920 train_time:25315ms step_avg:36.85ms
step:688/1920 train_time:25377ms step_avg:36.89ms
step:689/1920 train_time:25440ms step_avg:36.92ms
step:690/1920 train_time:25501ms step_avg:36.96ms
step:691/1920 train_time:25564ms step_avg:37.00ms
step:692/1920 train_time:25626ms step_avg:37.03ms
step:693/1920 train_time:25688ms step_avg:37.07ms
step:694/1920 train_time:25750ms step_avg:37.10ms
step:695/1920 train_time:25813ms step_avg:37.14ms
step:696/1920 train_time:25876ms step_avg:37.18ms
step:697/1920 train_time:25939ms step_avg:37.21ms
step:698/1920 train_time:26001ms step_avg:37.25ms
step:699/1920 train_time:26063ms step_avg:37.29ms
step:700/1920 train_time:26125ms step_avg:37.32ms
step:701/1920 train_time:26188ms step_avg:37.36ms
step:702/1920 train_time:26250ms step_avg:37.39ms
step:703/1920 train_time:26313ms step_avg:37.43ms
step:704/1920 train_time:26375ms step_avg:37.46ms
step:705/1920 train_time:26438ms step_avg:37.50ms
step:706/1920 train_time:26500ms step_avg:37.54ms
step:707/1920 train_time:26563ms step_avg:37.57ms
step:708/1920 train_time:26624ms step_avg:37.60ms
step:709/1920 train_time:26686ms step_avg:37.64ms
step:710/1920 train_time:26748ms step_avg:37.67ms
step:711/1920 train_time:26810ms step_avg:37.71ms
step:712/1920 train_time:26872ms step_avg:37.74ms
step:713/1920 train_time:26936ms step_avg:37.78ms
step:714/1920 train_time:26998ms step_avg:37.81ms
step:715/1920 train_time:27062ms step_avg:37.85ms
step:716/1920 train_time:27123ms step_avg:37.88ms
step:717/1920 train_time:27186ms step_avg:37.92ms
step:718/1920 train_time:27248ms step_avg:37.95ms
step:719/1920 train_time:27310ms step_avg:37.98ms
step:720/1920 train_time:27372ms step_avg:38.02ms
step:721/1920 train_time:27435ms step_avg:38.05ms
step:722/1920 train_time:27497ms step_avg:38.08ms
step:723/1920 train_time:27561ms step_avg:38.12ms
step:724/1920 train_time:27623ms step_avg:38.15ms
step:725/1920 train_time:27685ms step_avg:38.19ms
step:726/1920 train_time:27747ms step_avg:38.22ms
step:727/1920 train_time:27809ms step_avg:38.25ms
step:728/1920 train_time:27871ms step_avg:38.28ms
step:729/1920 train_time:27933ms step_avg:38.32ms
step:730/1920 train_time:27996ms step_avg:38.35ms
step:731/1920 train_time:28059ms step_avg:38.38ms
step:732/1920 train_time:28121ms step_avg:38.42ms
step:733/1920 train_time:28184ms step_avg:38.45ms
step:734/1920 train_time:28246ms step_avg:38.48ms
step:735/1920 train_time:28308ms step_avg:38.51ms
step:736/1920 train_time:28370ms step_avg:38.55ms
step:737/1920 train_time:28433ms step_avg:38.58ms
step:738/1920 train_time:28496ms step_avg:38.61ms
step:739/1920 train_time:28558ms step_avg:38.64ms
step:740/1920 train_time:28620ms step_avg:38.68ms
step:741/1920 train_time:28683ms step_avg:38.71ms
step:742/1920 train_time:28745ms step_avg:38.74ms
step:743/1920 train_time:28807ms step_avg:38.77ms
step:744/1920 train_time:28870ms step_avg:38.80ms
step:745/1920 train_time:28933ms step_avg:38.84ms
step:746/1920 train_time:28995ms step_avg:38.87ms
step:747/1920 train_time:29057ms step_avg:38.90ms
step:748/1920 train_time:29120ms step_avg:38.93ms
step:749/1920 train_time:29183ms step_avg:38.96ms
step:750/1920 train_time:29244ms step_avg:38.99ms
step:750/1920 val_loss:4.0422 train_time:29309ms step_avg:39.08ms
step:751/1920 train_time:29330ms step_avg:39.05ms
step:752/1920 train_time:29370ms step_avg:39.06ms
step:753/1920 train_time:29439ms step_avg:39.10ms
step:754/1920 train_time:29506ms step_avg:39.13ms
step:755/1920 train_time:29569ms step_avg:39.16ms
step:756/1920 train_time:29631ms step_avg:39.19ms
step:757/1920 train_time:29693ms step_avg:39.22ms
step:758/1920 train_time:29754ms step_avg:39.25ms
step:759/1920 train_time:29815ms step_avg:39.28ms
step:760/1920 train_time:29876ms step_avg:39.31ms
step:761/1920 train_time:29938ms step_avg:39.34ms
step:762/1920 train_time:29999ms step_avg:39.37ms
step:763/1920 train_time:30060ms step_avg:39.40ms
step:764/1920 train_time:30121ms step_avg:39.43ms
step:765/1920 train_time:30183ms step_avg:39.45ms
step:766/1920 train_time:30246ms step_avg:39.49ms
step:767/1920 train_time:30309ms step_avg:39.52ms
step:768/1920 train_time:30373ms step_avg:39.55ms
step:769/1920 train_time:30437ms step_avg:39.58ms
step:770/1920 train_time:30500ms step_avg:39.61ms
step:771/1920 train_time:30563ms step_avg:39.64ms
step:772/1920 train_time:30625ms step_avg:39.67ms
step:773/1920 train_time:30688ms step_avg:39.70ms
step:774/1920 train_time:30750ms step_avg:39.73ms
step:775/1920 train_time:30813ms step_avg:39.76ms
step:776/1920 train_time:30874ms step_avg:39.79ms
step:777/1920 train_time:30936ms step_avg:39.82ms
step:778/1920 train_time:30997ms step_avg:39.84ms
step:779/1920 train_time:31060ms step_avg:39.87ms
step:780/1920 train_time:31121ms step_avg:39.90ms
step:781/1920 train_time:31183ms step_avg:39.93ms
step:782/1920 train_time:31244ms step_avg:39.95ms
step:783/1920 train_time:31307ms step_avg:39.98ms
step:784/1920 train_time:31369ms step_avg:40.01ms
step:785/1920 train_time:31432ms step_avg:40.04ms
step:786/1920 train_time:31495ms step_avg:40.07ms
step:787/1920 train_time:31558ms step_avg:40.10ms
step:788/1920 train_time:31619ms step_avg:40.13ms
step:789/1920 train_time:31682ms step_avg:40.16ms
step:790/1920 train_time:31744ms step_avg:40.18ms
step:791/1920 train_time:31807ms step_avg:40.21ms
step:792/1920 train_time:31869ms step_avg:40.24ms
step:793/1920 train_time:31932ms step_avg:40.27ms
step:794/1920 train_time:31994ms step_avg:40.29ms
step:795/1920 train_time:32056ms step_avg:40.32ms
step:796/1920 train_time:32118ms step_avg:40.35ms
step:797/1920 train_time:32180ms step_avg:40.38ms
step:798/1920 train_time:32241ms step_avg:40.40ms
step:799/1920 train_time:32303ms step_avg:40.43ms
step:800/1920 train_time:32365ms step_avg:40.46ms
step:801/1920 train_time:32428ms step_avg:40.48ms
step:802/1920 train_time:32490ms step_avg:40.51ms
step:803/1920 train_time:32554ms step_avg:40.54ms
step:804/1920 train_time:32616ms step_avg:40.57ms
step:805/1920 train_time:32678ms step_avg:40.59ms
step:806/1920 train_time:32740ms step_avg:40.62ms
step:807/1920 train_time:32802ms step_avg:40.65ms
step:808/1920 train_time:32864ms step_avg:40.67ms
step:809/1920 train_time:32927ms step_avg:40.70ms
step:810/1920 train_time:32989ms step_avg:40.73ms
step:811/1920 train_time:33053ms step_avg:40.76ms
step:812/1920 train_time:33115ms step_avg:40.78ms
step:813/1920 train_time:33178ms step_avg:40.81ms
step:814/1920 train_time:33239ms step_avg:40.83ms
step:815/1920 train_time:33302ms step_avg:40.86ms
step:816/1920 train_time:33364ms step_avg:40.89ms
step:817/1920 train_time:33427ms step_avg:40.91ms
step:818/1920 train_time:33488ms step_avg:40.94ms
step:819/1920 train_time:33551ms step_avg:40.97ms
step:820/1920 train_time:33613ms step_avg:40.99ms
step:821/1920 train_time:33676ms step_avg:41.02ms
step:822/1920 train_time:33737ms step_avg:41.04ms
step:823/1920 train_time:33800ms step_avg:41.07ms
step:824/1920 train_time:33862ms step_avg:41.09ms
step:825/1920 train_time:33924ms step_avg:41.12ms
step:826/1920 train_time:33986ms step_avg:41.15ms
step:827/1920 train_time:34050ms step_avg:41.17ms
step:828/1920 train_time:34113ms step_avg:41.20ms
step:829/1920 train_time:34175ms step_avg:41.22ms
step:830/1920 train_time:34237ms step_avg:41.25ms
step:831/1920 train_time:34299ms step_avg:41.27ms
step:832/1920 train_time:34361ms step_avg:41.30ms
step:833/1920 train_time:34424ms step_avg:41.32ms
step:834/1920 train_time:34486ms step_avg:41.35ms
step:835/1920 train_time:34549ms step_avg:41.38ms
step:836/1920 train_time:34610ms step_avg:41.40ms
step:837/1920 train_time:34673ms step_avg:41.43ms
step:838/1920 train_time:34735ms step_avg:41.45ms
step:839/1920 train_time:34797ms step_avg:41.47ms
step:840/1920 train_time:34859ms step_avg:41.50ms
step:841/1920 train_time:34922ms step_avg:41.52ms
step:842/1920 train_time:34984ms step_avg:41.55ms
step:843/1920 train_time:35047ms step_avg:41.57ms
step:844/1920 train_time:35109ms step_avg:41.60ms
step:845/1920 train_time:35172ms step_avg:41.62ms
step:846/1920 train_time:35233ms step_avg:41.65ms
step:847/1920 train_time:35296ms step_avg:41.67ms
step:848/1920 train_time:35358ms step_avg:41.70ms
step:849/1920 train_time:35421ms step_avg:41.72ms
step:850/1920 train_time:35483ms step_avg:41.74ms
step:851/1920 train_time:35546ms step_avg:41.77ms
step:852/1920 train_time:35608ms step_avg:41.79ms
step:853/1920 train_time:35671ms step_avg:41.82ms
step:854/1920 train_time:35733ms step_avg:41.84ms
step:855/1920 train_time:35796ms step_avg:41.87ms
step:856/1920 train_time:35858ms step_avg:41.89ms
step:857/1920 train_time:35920ms step_avg:41.91ms
step:858/1920 train_time:35982ms step_avg:41.94ms
step:859/1920 train_time:36044ms step_avg:41.96ms
step:860/1920 train_time:36106ms step_avg:41.98ms
step:861/1920 train_time:36169ms step_avg:42.01ms
step:862/1920 train_time:36231ms step_avg:42.03ms
step:863/1920 train_time:36294ms step_avg:42.06ms
step:864/1920 train_time:36356ms step_avg:42.08ms
step:865/1920 train_time:36419ms step_avg:42.10ms
step:866/1920 train_time:36481ms step_avg:42.13ms
step:867/1920 train_time:36543ms step_avg:42.15ms
step:868/1920 train_time:36605ms step_avg:42.17ms
step:869/1920 train_time:36668ms step_avg:42.20ms
step:870/1920 train_time:36731ms step_avg:42.22ms
step:871/1920 train_time:36794ms step_avg:42.24ms
step:872/1920 train_time:36856ms step_avg:42.27ms
step:873/1920 train_time:36918ms step_avg:42.29ms
step:874/1920 train_time:36980ms step_avg:42.31ms
step:875/1920 train_time:37042ms step_avg:42.33ms
step:876/1920 train_time:37104ms step_avg:42.36ms
step:877/1920 train_time:37167ms step_avg:42.38ms
step:878/1920 train_time:37229ms step_avg:42.40ms
step:879/1920 train_time:37293ms step_avg:42.43ms
step:880/1920 train_time:37355ms step_avg:42.45ms
step:881/1920 train_time:37418ms step_avg:42.47ms
step:882/1920 train_time:37479ms step_avg:42.49ms
step:883/1920 train_time:37541ms step_avg:42.52ms
step:884/1920 train_time:37603ms step_avg:42.54ms
step:885/1920 train_time:37666ms step_avg:42.56ms
step:886/1920 train_time:37728ms step_avg:42.58ms
step:887/1920 train_time:37791ms step_avg:42.61ms
step:888/1920 train_time:37853ms step_avg:42.63ms
step:889/1920 train_time:37916ms step_avg:42.65ms
step:890/1920 train_time:37978ms step_avg:42.67ms
step:891/1920 train_time:38040ms step_avg:42.69ms
step:892/1920 train_time:38102ms step_avg:42.72ms
step:893/1920 train_time:38164ms step_avg:42.74ms
step:894/1920 train_time:38226ms step_avg:42.76ms
step:895/1920 train_time:38289ms step_avg:42.78ms
step:896/1920 train_time:38352ms step_avg:42.80ms
step:897/1920 train_time:38415ms step_avg:42.83ms
step:898/1920 train_time:38477ms step_avg:42.85ms
step:899/1920 train_time:38540ms step_avg:42.87ms
step:900/1920 train_time:38602ms step_avg:42.89ms
step:901/1920 train_time:38664ms step_avg:42.91ms
step:902/1920 train_time:38726ms step_avg:42.93ms
step:903/1920 train_time:38790ms step_avg:42.96ms
step:904/1920 train_time:38851ms step_avg:42.98ms
step:905/1920 train_time:38915ms step_avg:43.00ms
step:906/1920 train_time:38977ms step_avg:43.02ms
step:907/1920 train_time:39040ms step_avg:43.04ms
step:908/1920 train_time:39101ms step_avg:43.06ms
step:909/1920 train_time:39163ms step_avg:43.08ms
step:910/1920 train_time:39225ms step_avg:43.10ms
step:911/1920 train_time:39288ms step_avg:43.13ms
step:912/1920 train_time:39350ms step_avg:43.15ms
step:913/1920 train_time:39413ms step_avg:43.17ms
step:914/1920 train_time:39475ms step_avg:43.19ms
step:915/1920 train_time:39538ms step_avg:43.21ms
step:916/1920 train_time:39599ms step_avg:43.23ms
step:917/1920 train_time:39661ms step_avg:43.25ms
step:918/1920 train_time:39724ms step_avg:43.27ms
step:919/1920 train_time:39787ms step_avg:43.29ms
step:920/1920 train_time:39849ms step_avg:43.31ms
step:921/1920 train_time:39912ms step_avg:43.34ms
step:922/1920 train_time:39974ms step_avg:43.36ms
step:923/1920 train_time:40037ms step_avg:43.38ms
step:924/1920 train_time:40099ms step_avg:43.40ms
step:925/1920 train_time:40161ms step_avg:43.42ms
step:926/1920 train_time:40223ms step_avg:43.44ms
step:927/1920 train_time:40286ms step_avg:43.46ms
step:928/1920 train_time:40348ms step_avg:43.48ms
step:929/1920 train_time:40412ms step_avg:43.50ms
step:930/1920 train_time:40474ms step_avg:43.52ms
step:931/1920 train_time:40537ms step_avg:43.54ms
step:932/1920 train_time:40598ms step_avg:43.56ms
step:933/1920 train_time:40661ms step_avg:43.58ms
step:934/1920 train_time:40723ms step_avg:43.60ms
step:935/1920 train_time:40785ms step_avg:43.62ms
step:936/1920 train_time:40847ms step_avg:43.64ms
step:937/1920 train_time:40910ms step_avg:43.66ms
step:938/1920 train_time:40972ms step_avg:43.68ms
step:939/1920 train_time:41035ms step_avg:43.70ms
step:940/1920 train_time:41097ms step_avg:43.72ms
step:941/1920 train_time:41160ms step_avg:43.74ms
step:942/1920 train_time:41222ms step_avg:43.76ms
step:943/1920 train_time:41283ms step_avg:43.78ms
step:944/1920 train_time:41346ms step_avg:43.80ms
step:945/1920 train_time:41408ms step_avg:43.82ms
step:946/1920 train_time:41471ms step_avg:43.84ms
step:947/1920 train_time:41534ms step_avg:43.86ms
step:948/1920 train_time:41595ms step_avg:43.88ms
step:949/1920 train_time:41658ms step_avg:43.90ms
step:950/1920 train_time:41719ms step_avg:43.91ms
step:951/1920 train_time:41781ms step_avg:43.93ms
step:952/1920 train_time:41844ms step_avg:43.95ms
step:953/1920 train_time:41907ms step_avg:43.97ms
step:954/1920 train_time:41969ms step_avg:43.99ms
step:955/1920 train_time:42033ms step_avg:44.01ms
step:956/1920 train_time:42094ms step_avg:44.03ms
step:957/1920 train_time:42157ms step_avg:44.05ms
step:958/1920 train_time:42218ms step_avg:44.07ms
step:959/1920 train_time:42280ms step_avg:44.09ms
step:960/1920 train_time:42343ms step_avg:44.11ms
step:961/1920 train_time:42406ms step_avg:44.13ms
step:962/1920 train_time:42468ms step_avg:44.15ms
step:963/1920 train_time:42532ms step_avg:44.17ms
step:964/1920 train_time:42593ms step_avg:44.18ms
step:965/1920 train_time:42656ms step_avg:44.20ms
step:966/1920 train_time:42718ms step_avg:44.22ms
step:967/1920 train_time:42780ms step_avg:44.24ms
step:968/1920 train_time:42842ms step_avg:44.26ms
step:969/1920 train_time:42905ms step_avg:44.28ms
step:970/1920 train_time:42967ms step_avg:44.30ms
step:971/1920 train_time:43030ms step_avg:44.32ms
step:972/1920 train_time:43093ms step_avg:44.33ms
step:973/1920 train_time:43156ms step_avg:44.35ms
step:974/1920 train_time:43217ms step_avg:44.37ms
step:975/1920 train_time:43279ms step_avg:44.39ms
step:976/1920 train_time:43341ms step_avg:44.41ms
step:977/1920 train_time:43403ms step_avg:44.43ms
step:978/1920 train_time:43465ms step_avg:44.44ms
step:979/1920 train_time:43528ms step_avg:44.46ms
step:980/1920 train_time:43591ms step_avg:44.48ms
step:981/1920 train_time:43654ms step_avg:44.50ms
step:982/1920 train_time:43716ms step_avg:44.52ms
step:983/1920 train_time:43778ms step_avg:44.54ms
step:984/1920 train_time:43840ms step_avg:44.55ms
step:985/1920 train_time:43902ms step_avg:44.57ms
step:986/1920 train_time:43964ms step_avg:44.59ms
step:987/1920 train_time:44027ms step_avg:44.61ms
step:988/1920 train_time:44089ms step_avg:44.62ms
step:989/1920 train_time:44152ms step_avg:44.64ms
step:990/1920 train_time:44214ms step_avg:44.66ms
step:991/1920 train_time:44277ms step_avg:44.68ms
step:992/1920 train_time:44338ms step_avg:44.70ms
step:993/1920 train_time:44400ms step_avg:44.71ms
step:994/1920 train_time:44462ms step_avg:44.73ms
step:995/1920 train_time:44525ms step_avg:44.75ms
step:996/1920 train_time:44587ms step_avg:44.77ms
step:997/1920 train_time:44651ms step_avg:44.78ms
step:998/1920 train_time:44713ms step_avg:44.80ms
step:999/1920 train_time:44776ms step_avg:44.82ms
step:1000/1920 train_time:44838ms step_avg:44.84ms
step:1000/1920 val_loss:3.7787 train_time:44902ms step_avg:44.90ms
step:1001/1920 train_time:44924ms step_avg:44.88ms
step:1002/1920 train_time:44964ms step_avg:44.87ms
step:1003/1920 train_time:45027ms step_avg:44.89ms
step:1004/1920 train_time:45090ms step_avg:44.91ms
step:1005/1920 train_time:45153ms step_avg:44.93ms
step:1006/1920 train_time:45216ms step_avg:44.95ms
step:1007/1920 train_time:45278ms step_avg:44.96ms
step:1008/1920 train_time:45339ms step_avg:44.98ms
step:1009/1920 train_time:45402ms step_avg:45.00ms
step:1010/1920 train_time:45463ms step_avg:45.01ms
step:1011/1920 train_time:45525ms step_avg:45.03ms
step:1012/1920 train_time:45586ms step_avg:45.05ms
step:1013/1920 train_time:45648ms step_avg:45.06ms
step:1014/1920 train_time:45710ms step_avg:45.08ms
step:1015/1920 train_time:45773ms step_avg:45.10ms
step:1016/1920 train_time:45837ms step_avg:45.11ms
step:1017/1920 train_time:45901ms step_avg:45.13ms
step:1018/1920 train_time:45964ms step_avg:45.15ms
step:1019/1920 train_time:46027ms step_avg:45.17ms
step:1020/1920 train_time:46089ms step_avg:45.19ms
step:1021/1920 train_time:46152ms step_avg:45.20ms
step:1022/1920 train_time:46214ms step_avg:45.22ms
step:1023/1920 train_time:46277ms step_avg:45.24ms
step:1024/1920 train_time:46340ms step_avg:45.25ms
step:1025/1920 train_time:46403ms step_avg:45.27ms
step:1026/1920 train_time:46464ms step_avg:45.29ms
step:1027/1920 train_time:46526ms step_avg:45.30ms
step:1028/1920 train_time:46587ms step_avg:45.32ms
step:1029/1920 train_time:46649ms step_avg:45.33ms
step:1030/1920 train_time:46711ms step_avg:45.35ms
step:1031/1920 train_time:46774ms step_avg:45.37ms
step:1032/1920 train_time:46836ms step_avg:45.38ms
step:1033/1920 train_time:46900ms step_avg:45.40ms
step:1034/1920 train_time:46962ms step_avg:45.42ms
step:1035/1920 train_time:47026ms step_avg:45.44ms
step:1036/1920 train_time:47088ms step_avg:45.45ms
step:1037/1920 train_time:47151ms step_avg:45.47ms
step:1038/1920 train_time:47213ms step_avg:45.48ms
step:1039/1920 train_time:47277ms step_avg:45.50ms
step:1040/1920 train_time:47339ms step_avg:45.52ms
step:1041/1920 train_time:47402ms step_avg:45.53ms
step:1042/1920 train_time:47463ms step_avg:45.55ms
step:1043/1920 train_time:47526ms step_avg:45.57ms
step:1044/1920 train_time:47587ms step_avg:45.58ms
step:1045/1920 train_time:47649ms step_avg:45.60ms
step:1046/1920 train_time:47710ms step_avg:45.61ms
step:1047/1920 train_time:47773ms step_avg:45.63ms
step:1048/1920 train_time:47836ms step_avg:45.65ms
step:1049/1920 train_time:47899ms step_avg:45.66ms
step:1050/1920 train_time:47961ms step_avg:45.68ms
step:1051/1920 train_time:48024ms step_avg:45.69ms
step:1052/1920 train_time:48086ms step_avg:45.71ms
step:1053/1920 train_time:48150ms step_avg:45.73ms
step:1054/1920 train_time:48213ms step_avg:45.74ms
step:1055/1920 train_time:48277ms step_avg:45.76ms
step:1056/1920 train_time:48339ms step_avg:45.78ms
step:1057/1920 train_time:48402ms step_avg:45.79ms
step:1058/1920 train_time:48464ms step_avg:45.81ms
step:1059/1920 train_time:48526ms step_avg:45.82ms
step:1060/1920 train_time:48587ms step_avg:45.84ms
step:1061/1920 train_time:48649ms step_avg:45.85ms
step:1062/1920 train_time:48711ms step_avg:45.87ms
step:1063/1920 train_time:48774ms step_avg:45.88ms
step:1064/1920 train_time:48836ms step_avg:45.90ms
step:1065/1920 train_time:48899ms step_avg:45.91ms
step:1066/1920 train_time:48961ms step_avg:45.93ms
step:1067/1920 train_time:49024ms step_avg:45.95ms
step:1068/1920 train_time:49086ms step_avg:45.96ms
step:1069/1920 train_time:49148ms step_avg:45.98ms
step:1070/1920 train_time:49211ms step_avg:45.99ms
step:1071/1920 train_time:49273ms step_avg:46.01ms
step:1072/1920 train_time:49336ms step_avg:46.02ms
step:1073/1920 train_time:49399ms step_avg:46.04ms
step:1074/1920 train_time:49461ms step_avg:46.05ms
step:1075/1920 train_time:49524ms step_avg:46.07ms
step:1076/1920 train_time:49585ms step_avg:46.08ms
step:1077/1920 train_time:49647ms step_avg:46.10ms
step:1078/1920 train_time:49709ms step_avg:46.11ms
step:1079/1920 train_time:49772ms step_avg:46.13ms
step:1080/1920 train_time:49834ms step_avg:46.14ms
step:1081/1920 train_time:49898ms step_avg:46.16ms
step:1082/1920 train_time:49960ms step_avg:46.17ms
step:1083/1920 train_time:50023ms step_avg:46.19ms
step:1084/1920 train_time:50085ms step_avg:46.20ms
step:1085/1920 train_time:50147ms step_avg:46.22ms
step:1086/1920 train_time:50210ms step_avg:46.23ms
step:1087/1920 train_time:50273ms step_avg:46.25ms
step:1088/1920 train_time:50335ms step_avg:46.26ms
step:1089/1920 train_time:50399ms step_avg:46.28ms
step:1090/1920 train_time:50461ms step_avg:46.29ms
step:1091/1920 train_time:50524ms step_avg:46.31ms
step:1092/1920 train_time:50585ms step_avg:46.32ms
step:1093/1920 train_time:50647ms step_avg:46.34ms
step:1094/1920 train_time:50709ms step_avg:46.35ms
step:1095/1920 train_time:50771ms step_avg:46.37ms
step:1096/1920 train_time:50833ms step_avg:46.38ms
step:1097/1920 train_time:50897ms step_avg:46.40ms
step:1098/1920 train_time:50959ms step_avg:46.41ms
step:1099/1920 train_time:51022ms step_avg:46.43ms
step:1100/1920 train_time:51083ms step_avg:46.44ms
step:1101/1920 train_time:51146ms step_avg:46.45ms
step:1102/1920 train_time:51208ms step_avg:46.47ms
step:1103/1920 train_time:51271ms step_avg:46.48ms
step:1104/1920 train_time:51334ms step_avg:46.50ms
step:1105/1920 train_time:51398ms step_avg:46.51ms
step:1106/1920 train_time:51460ms step_avg:46.53ms
step:1107/1920 train_time:51523ms step_avg:46.54ms
step:1108/1920 train_time:51584ms step_avg:46.56ms
step:1109/1920 train_time:51647ms step_avg:46.57ms
step:1110/1920 train_time:51708ms step_avg:46.58ms
step:1111/1920 train_time:51771ms step_avg:46.60ms
step:1112/1920 train_time:51833ms step_avg:46.61ms
step:1113/1920 train_time:51896ms step_avg:46.63ms
step:1114/1920 train_time:51959ms step_avg:46.64ms
step:1115/1920 train_time:52022ms step_avg:46.66ms
step:1116/1920 train_time:52085ms step_avg:46.67ms
step:1117/1920 train_time:52147ms step_avg:46.68ms
step:1118/1920 train_time:52209ms step_avg:46.70ms
step:1119/1920 train_time:52272ms step_avg:46.71ms
step:1120/1920 train_time:52334ms step_avg:46.73ms
step:1121/1920 train_time:52398ms step_avg:46.74ms
step:1122/1920 train_time:52460ms step_avg:46.76ms
step:1123/1920 train_time:52523ms step_avg:46.77ms
step:1124/1920 train_time:52585ms step_avg:46.78ms
step:1125/1920 train_time:52647ms step_avg:46.80ms
step:1126/1920 train_time:52708ms step_avg:46.81ms
step:1127/1920 train_time:52771ms step_avg:46.82ms
step:1128/1920 train_time:52833ms step_avg:46.84ms
step:1129/1920 train_time:52897ms step_avg:46.85ms
step:1130/1920 train_time:52959ms step_avg:46.87ms
step:1131/1920 train_time:53023ms step_avg:46.88ms
step:1132/1920 train_time:53085ms step_avg:46.89ms
step:1133/1920 train_time:53147ms step_avg:46.91ms
step:1134/1920 train_time:53209ms step_avg:46.92ms
step:1135/1920 train_time:53272ms step_avg:46.94ms
step:1136/1920 train_time:53334ms step_avg:46.95ms
step:1137/1920 train_time:53397ms step_avg:46.96ms
step:1138/1920 train_time:53459ms step_avg:46.98ms
step:1139/1920 train_time:53522ms step_avg:46.99ms
step:1140/1920 train_time:53584ms step_avg:47.00ms
step:1141/1920 train_time:53646ms step_avg:47.02ms
step:1142/1920 train_time:53708ms step_avg:47.03ms
step:1143/1920 train_time:53771ms step_avg:47.04ms
step:1144/1920 train_time:53833ms step_avg:47.06ms
step:1145/1920 train_time:53896ms step_avg:47.07ms
step:1146/1920 train_time:53958ms step_avg:47.08ms
step:1147/1920 train_time:54021ms step_avg:47.10ms
step:1148/1920 train_time:54083ms step_avg:47.11ms
step:1149/1920 train_time:54146ms step_avg:47.12ms
step:1150/1920 train_time:54208ms step_avg:47.14ms
step:1151/1920 train_time:54270ms step_avg:47.15ms
step:1152/1920 train_time:54333ms step_avg:47.16ms
step:1153/1920 train_time:54397ms step_avg:47.18ms
step:1154/1920 train_time:54459ms step_avg:47.19ms
step:1155/1920 train_time:54522ms step_avg:47.21ms
step:1156/1920 train_time:54584ms step_avg:47.22ms
step:1157/1920 train_time:54647ms step_avg:47.23ms
step:1158/1920 train_time:54708ms step_avg:47.24ms
step:1159/1920 train_time:54771ms step_avg:47.26ms
step:1160/1920 train_time:54833ms step_avg:47.27ms
step:1161/1920 train_time:54897ms step_avg:47.28ms
step:1162/1920 train_time:54959ms step_avg:47.30ms
step:1163/1920 train_time:55022ms step_avg:47.31ms
step:1164/1920 train_time:55084ms step_avg:47.32ms
step:1165/1920 train_time:55147ms step_avg:47.34ms
step:1166/1920 train_time:55208ms step_avg:47.35ms
step:1167/1920 train_time:55271ms step_avg:47.36ms
step:1168/1920 train_time:55334ms step_avg:47.38ms
step:1169/1920 train_time:55397ms step_avg:47.39ms
step:1170/1920 train_time:55459ms step_avg:47.40ms
step:1171/1920 train_time:55522ms step_avg:47.41ms
step:1172/1920 train_time:55585ms step_avg:47.43ms
step:1173/1920 train_time:55647ms step_avg:47.44ms
step:1174/1920 train_time:55709ms step_avg:47.45ms
step:1175/1920 train_time:55771ms step_avg:47.46ms
step:1176/1920 train_time:55833ms step_avg:47.48ms
step:1177/1920 train_time:55897ms step_avg:47.49ms
step:1178/1920 train_time:55959ms step_avg:47.50ms
step:1179/1920 train_time:56022ms step_avg:47.52ms
step:1180/1920 train_time:56084ms step_avg:47.53ms
step:1181/1920 train_time:56147ms step_avg:47.54ms
step:1182/1920 train_time:56209ms step_avg:47.55ms
step:1183/1920 train_time:56271ms step_avg:47.57ms
step:1184/1920 train_time:56333ms step_avg:47.58ms
step:1185/1920 train_time:56397ms step_avg:47.59ms
step:1186/1920 train_time:56459ms step_avg:47.60ms
step:1187/1920 train_time:56522ms step_avg:47.62ms
step:1188/1920 train_time:56583ms step_avg:47.63ms
step:1189/1920 train_time:56646ms step_avg:47.64ms
step:1190/1920 train_time:56708ms step_avg:47.65ms
step:1191/1920 train_time:56771ms step_avg:47.67ms
step:1192/1920 train_time:56833ms step_avg:47.68ms
step:1193/1920 train_time:56896ms step_avg:47.69ms
step:1194/1920 train_time:56958ms step_avg:47.70ms
step:1195/1920 train_time:57021ms step_avg:47.72ms
step:1196/1920 train_time:57083ms step_avg:47.73ms
step:1197/1920 train_time:57145ms step_avg:47.74ms
step:1198/1920 train_time:57207ms step_avg:47.75ms
step:1199/1920 train_time:57270ms step_avg:47.76ms
step:1200/1920 train_time:57332ms step_avg:47.78ms
step:1201/1920 train_time:57395ms step_avg:47.79ms
step:1202/1920 train_time:57457ms step_avg:47.80ms
step:1203/1920 train_time:57520ms step_avg:47.81ms
step:1204/1920 train_time:57582ms step_avg:47.83ms
step:1205/1920 train_time:57644ms step_avg:47.84ms
step:1206/1920 train_time:57706ms step_avg:47.85ms
step:1207/1920 train_time:57769ms step_avg:47.86ms
step:1208/1920 train_time:57831ms step_avg:47.87ms
step:1209/1920 train_time:57894ms step_avg:47.89ms
step:1210/1920 train_time:57956ms step_avg:47.90ms
step:1211/1920 train_time:58019ms step_avg:47.91ms
step:1212/1920 train_time:58081ms step_avg:47.92ms
step:1213/1920 train_time:58144ms step_avg:47.93ms
step:1214/1920 train_time:58206ms step_avg:47.95ms
step:1215/1920 train_time:58269ms step_avg:47.96ms
step:1216/1920 train_time:58331ms step_avg:47.97ms
step:1217/1920 train_time:58395ms step_avg:47.98ms
step:1218/1920 train_time:58457ms step_avg:47.99ms
step:1219/1920 train_time:58521ms step_avg:48.01ms
step:1220/1920 train_time:58583ms step_avg:48.02ms
step:1221/1920 train_time:58645ms step_avg:48.03ms
step:1222/1920 train_time:58707ms step_avg:48.04ms
step:1223/1920 train_time:58770ms step_avg:48.05ms
step:1224/1920 train_time:58831ms step_avg:48.06ms
step:1225/1920 train_time:58894ms step_avg:48.08ms
step:1226/1920 train_time:58956ms step_avg:48.09ms
step:1227/1920 train_time:59019ms step_avg:48.10ms
step:1228/1920 train_time:59081ms step_avg:48.11ms
step:1229/1920 train_time:59143ms step_avg:48.12ms
step:1230/1920 train_time:59205ms step_avg:48.13ms
step:1231/1920 train_time:59267ms step_avg:48.15ms
step:1232/1920 train_time:59330ms step_avg:48.16ms
step:1233/1920 train_time:59392ms step_avg:48.17ms
step:1234/1920 train_time:59454ms step_avg:48.18ms
step:1235/1920 train_time:59518ms step_avg:48.19ms
step:1236/1920 train_time:59580ms step_avg:48.20ms
step:1237/1920 train_time:59642ms step_avg:48.22ms
step:1238/1920 train_time:59704ms step_avg:48.23ms
step:1239/1920 train_time:59766ms step_avg:48.24ms
step:1240/1920 train_time:59828ms step_avg:48.25ms
step:1241/1920 train_time:59891ms step_avg:48.26ms
step:1242/1920 train_time:59954ms step_avg:48.27ms
step:1243/1920 train_time:60017ms step_avg:48.28ms
step:1244/1920 train_time:60079ms step_avg:48.29ms
step:1245/1920 train_time:60141ms step_avg:48.31ms
step:1246/1920 train_time:60203ms step_avg:48.32ms
step:1247/1920 train_time:60266ms step_avg:48.33ms
step:1248/1920 train_time:60328ms step_avg:48.34ms
step:1249/1920 train_time:60390ms step_avg:48.35ms
step:1250/1920 train_time:60452ms step_avg:48.36ms
step:1250/1920 val_loss:3.5539 train_time:60518ms step_avg:48.41ms
step:1251/1920 train_time:60536ms step_avg:48.39ms
step:1252/1920 train_time:60582ms step_avg:48.39ms
step:1253/1920 train_time:60646ms step_avg:48.40ms
step:1254/1920 train_time:60709ms step_avg:48.41ms
step:1255/1920 train_time:60771ms step_avg:48.42ms
step:1256/1920 train_time:60858ms step_avg:48.45ms
step:1257/1920 train_time:60945ms step_avg:48.48ms
step:1258/1920 train_time:61032ms step_avg:48.52ms
step:1259/1920 train_time:61122ms step_avg:48.55ms
step:1260/1920 train_time:61210ms step_avg:48.58ms
step:1261/1920 train_time:61300ms step_avg:48.61ms
step:1262/1920 train_time:61387ms step_avg:48.64ms
step:1263/1920 train_time:61478ms step_avg:48.68ms
step:1264/1920 train_time:61569ms step_avg:48.71ms
step:1265/1920 train_time:61660ms step_avg:48.74ms
step:1266/1920 train_time:61748ms step_avg:48.77ms
step:1267/1920 train_time:61837ms step_avg:48.81ms
step:1268/1920 train_time:61924ms step_avg:48.84ms
step:1269/1920 train_time:62012ms step_avg:48.87ms
step:1270/1920 train_time:62098ms step_avg:48.90ms
step:1271/1920 train_time:62186ms step_avg:48.93ms
step:1272/1920 train_time:62274ms step_avg:48.96ms
step:1273/1920 train_time:62362ms step_avg:48.99ms
step:1274/1920 train_time:62450ms step_avg:49.02ms
step:1275/1920 train_time:62540ms step_avg:49.05ms
step:1276/1920 train_time:62629ms step_avg:49.08ms
step:1277/1920 train_time:62719ms step_avg:49.11ms
step:1278/1920 train_time:62807ms step_avg:49.14ms
step:1279/1920 train_time:62897ms step_avg:49.18ms
step:1280/1920 train_time:62985ms step_avg:49.21ms
step:1281/1920 train_time:63074ms step_avg:49.24ms
step:1282/1920 train_time:63161ms step_avg:49.27ms
step:1283/1920 train_time:63249ms step_avg:49.30ms
step:1284/1920 train_time:63336ms step_avg:49.33ms
step:1285/1920 train_time:63425ms step_avg:49.36ms
step:1286/1920 train_time:63515ms step_avg:49.39ms
step:1287/1920 train_time:63605ms step_avg:49.42ms
step:1288/1920 train_time:63694ms step_avg:49.45ms
step:1289/1920 train_time:63782ms step_avg:49.48ms
step:1290/1920 train_time:63870ms step_avg:49.51ms
step:1291/1920 train_time:63959ms step_avg:49.54ms
step:1292/1920 train_time:64047ms step_avg:49.57ms
step:1293/1920 train_time:64137ms step_avg:49.60ms
step:1294/1920 train_time:64224ms step_avg:49.63ms
step:1295/1920 train_time:64313ms step_avg:49.66ms
step:1296/1920 train_time:64400ms step_avg:49.69ms
step:1297/1920 train_time:64489ms step_avg:49.72ms
step:1298/1920 train_time:64578ms step_avg:49.75ms
step:1299/1920 train_time:64667ms step_avg:49.78ms
step:1300/1920 train_time:64755ms step_avg:49.81ms
step:1301/1920 train_time:64844ms step_avg:49.84ms
step:1302/1920 train_time:64932ms step_avg:49.87ms
step:1303/1920 train_time:65021ms step_avg:49.90ms
step:1304/1920 train_time:65108ms step_avg:49.93ms
step:1305/1920 train_time:65197ms step_avg:49.96ms
step:1306/1920 train_time:65286ms step_avg:49.99ms
step:1307/1920 train_time:65375ms step_avg:50.02ms
step:1308/1920 train_time:65463ms step_avg:50.05ms
step:1309/1920 train_time:65552ms step_avg:50.08ms
step:1310/1920 train_time:65640ms step_avg:50.11ms
step:1311/1920 train_time:65729ms step_avg:50.14ms
step:1312/1920 train_time:65817ms step_avg:50.17ms
step:1313/1920 train_time:65905ms step_avg:50.19ms
step:1314/1920 train_time:65994ms step_avg:50.22ms
step:1315/1920 train_time:66082ms step_avg:50.25ms
step:1316/1920 train_time:66170ms step_avg:50.28ms
step:1317/1920 train_time:66259ms step_avg:50.31ms
step:1318/1920 train_time:66347ms step_avg:50.34ms
step:1319/1920 train_time:66436ms step_avg:50.37ms
step:1320/1920 train_time:66524ms step_avg:50.40ms
step:1321/1920 train_time:66613ms step_avg:50.43ms
step:1322/1920 train_time:66701ms step_avg:50.45ms
step:1323/1920 train_time:66790ms step_avg:50.48ms
step:1324/1920 train_time:66878ms step_avg:50.51ms
step:1325/1920 train_time:66967ms step_avg:50.54ms
step:1326/1920 train_time:67055ms step_avg:50.57ms
step:1327/1920 train_time:67143ms step_avg:50.60ms
step:1328/1920 train_time:67231ms step_avg:50.63ms
step:1329/1920 train_time:67320ms step_avg:50.65ms
step:1330/1920 train_time:67409ms step_avg:50.68ms
step:1331/1920 train_time:67498ms step_avg:50.71ms
step:1332/1920 train_time:67588ms step_avg:50.74ms
step:1333/1920 train_time:67678ms step_avg:50.77ms
step:1334/1920 train_time:67767ms step_avg:50.80ms
step:1335/1920 train_time:67858ms step_avg:50.83ms
step:1336/1920 train_time:67945ms step_avg:50.86ms
step:1337/1920 train_time:68035ms step_avg:50.89ms
step:1338/1920 train_time:68122ms step_avg:50.91ms
step:1339/1920 train_time:68211ms step_avg:50.94ms
step:1340/1920 train_time:68299ms step_avg:50.97ms
step:1341/1920 train_time:68388ms step_avg:51.00ms
step:1342/1920 train_time:68476ms step_avg:51.03ms
step:1343/1920 train_time:68565ms step_avg:51.05ms
step:1344/1920 train_time:68654ms step_avg:51.08ms
step:1345/1920 train_time:68742ms step_avg:51.11ms
step:1346/1920 train_time:68831ms step_avg:51.14ms
step:1347/1920 train_time:68920ms step_avg:51.17ms
step:1348/1920 train_time:69008ms step_avg:51.19ms
step:1349/1920 train_time:69098ms step_avg:51.22ms
step:1350/1920 train_time:69186ms step_avg:51.25ms
step:1351/1920 train_time:69274ms step_avg:51.28ms
step:1352/1920 train_time:69362ms step_avg:51.30ms
step:1353/1920 train_time:69451ms step_avg:51.33ms
step:1354/1920 train_time:69538ms step_avg:51.36ms
step:1355/1920 train_time:69627ms step_avg:51.39ms
step:1356/1920 train_time:69716ms step_avg:51.41ms
step:1357/1920 train_time:69804ms step_avg:51.44ms
step:1358/1920 train_time:69894ms step_avg:51.47ms
step:1359/1920 train_time:69982ms step_avg:51.50ms
step:1360/1920 train_time:70070ms step_avg:51.52ms
step:1361/1920 train_time:70159ms step_avg:51.55ms
step:1362/1920 train_time:70247ms step_avg:51.58ms
step:1363/1920 train_time:70336ms step_avg:51.60ms
step:1364/1920 train_time:70425ms step_avg:51.63ms
step:1365/1920 train_time:70514ms step_avg:51.66ms
step:1366/1920 train_time:70601ms step_avg:51.68ms
step:1367/1920 train_time:70691ms step_avg:51.71ms
step:1368/1920 train_time:70779ms step_avg:51.74ms
step:1369/1920 train_time:70867ms step_avg:51.77ms
step:1370/1920 train_time:70956ms step_avg:51.79ms
step:1371/1920 train_time:71045ms step_avg:51.82ms
step:1372/1920 train_time:71133ms step_avg:51.85ms
step:1373/1920 train_time:71222ms step_avg:51.87ms
step:1374/1920 train_time:71310ms step_avg:51.90ms
step:1375/1920 train_time:71400ms step_avg:51.93ms
step:1376/1920 train_time:71489ms step_avg:51.95ms
step:1377/1920 train_time:71578ms step_avg:51.98ms
step:1378/1920 train_time:71667ms step_avg:52.01ms
step:1379/1920 train_time:71758ms step_avg:52.04ms
step:1380/1920 train_time:71846ms step_avg:52.06ms
step:1381/1920 train_time:71935ms step_avg:52.09ms
step:1382/1920 train_time:72022ms step_avg:52.11ms
step:1383/1920 train_time:72111ms step_avg:52.14ms
step:1384/1920 train_time:72198ms step_avg:52.17ms
step:1385/1920 train_time:72287ms step_avg:52.19ms
step:1386/1920 train_time:72375ms step_avg:52.22ms
step:1387/1920 train_time:72463ms step_avg:52.24ms
step:1388/1920 train_time:72552ms step_avg:52.27ms
step:1389/1920 train_time:72640ms step_avg:52.30ms
step:1390/1920 train_time:72729ms step_avg:52.32ms
step:1391/1920 train_time:72818ms step_avg:52.35ms
step:1392/1920 train_time:72906ms step_avg:52.37ms
step:1393/1920 train_time:72995ms step_avg:52.40ms
step:1394/1920 train_time:73082ms step_avg:52.43ms
step:1395/1920 train_time:73171ms step_avg:52.45ms
step:1396/1920 train_time:73258ms step_avg:52.48ms
step:1397/1920 train_time:73347ms step_avg:52.50ms
step:1398/1920 train_time:73434ms step_avg:52.53ms
step:1399/1920 train_time:73523ms step_avg:52.55ms
step:1400/1920 train_time:73611ms step_avg:52.58ms
step:1401/1920 train_time:73701ms step_avg:52.61ms
step:1402/1920 train_time:73789ms step_avg:52.63ms
step:1403/1920 train_time:73879ms step_avg:52.66ms
step:1404/1920 train_time:73967ms step_avg:52.68ms
step:1405/1920 train_time:74056ms step_avg:52.71ms
step:1406/1920 train_time:74144ms step_avg:52.73ms
step:1407/1920 train_time:74233ms step_avg:52.76ms
step:1408/1920 train_time:74321ms step_avg:52.78ms
step:1409/1920 train_time:74410ms step_avg:52.81ms
step:1410/1920 train_time:74499ms step_avg:52.84ms
step:1411/1920 train_time:74587ms step_avg:52.86ms
step:1412/1920 train_time:74675ms step_avg:52.89ms
step:1413/1920 train_time:74763ms step_avg:52.91ms
step:1414/1920 train_time:74851ms step_avg:52.94ms
step:1415/1920 train_time:74940ms step_avg:52.96ms
step:1416/1920 train_time:75029ms step_avg:52.99ms
step:1417/1920 train_time:75117ms step_avg:53.01ms
step:1418/1920 train_time:75206ms step_avg:53.04ms
step:1419/1920 train_time:75295ms step_avg:53.06ms
step:1420/1920 train_time:75382ms step_avg:53.09ms
step:1421/1920 train_time:75472ms step_avg:53.11ms
step:1422/1920 train_time:75560ms step_avg:53.14ms
step:1423/1920 train_time:75648ms step_avg:53.16ms
step:1424/1920 train_time:75736ms step_avg:53.19ms
step:1425/1920 train_time:75825ms step_avg:53.21ms
step:1426/1920 train_time:75913ms step_avg:53.23ms
step:1427/1920 train_time:76001ms step_avg:53.26ms
step:1428/1920 train_time:76089ms step_avg:53.28ms
step:1429/1920 train_time:76178ms step_avg:53.31ms
step:1430/1920 train_time:76266ms step_avg:53.33ms
step:1431/1920 train_time:76355ms step_avg:53.36ms
step:1432/1920 train_time:76443ms step_avg:53.38ms
step:1433/1920 train_time:76531ms step_avg:53.41ms
step:1434/1920 train_time:76619ms step_avg:53.43ms
step:1435/1920 train_time:76708ms step_avg:53.46ms
step:1436/1920 train_time:76796ms step_avg:53.48ms
step:1437/1920 train_time:76885ms step_avg:53.50ms
step:1438/1920 train_time:76973ms step_avg:53.53ms
step:1439/1920 train_time:77061ms step_avg:53.55ms
step:1440/1920 train_time:77150ms step_avg:53.58ms
step:1441/1920 train_time:77239ms step_avg:53.60ms
step:1442/1920 train_time:77327ms step_avg:53.63ms
step:1443/1920 train_time:77417ms step_avg:53.65ms
step:1444/1920 train_time:77505ms step_avg:53.67ms
step:1445/1920 train_time:77594ms step_avg:53.70ms
step:1446/1920 train_time:77682ms step_avg:53.72ms
step:1447/1920 train_time:77772ms step_avg:53.75ms
step:1448/1920 train_time:77859ms step_avg:53.77ms
step:1449/1920 train_time:77947ms step_avg:53.79ms
step:1450/1920 train_time:78035ms step_avg:53.82ms
step:1451/1920 train_time:78123ms step_avg:53.84ms
step:1452/1920 train_time:78211ms step_avg:53.86ms
step:1453/1920 train_time:78300ms step_avg:53.89ms
step:1454/1920 train_time:78389ms step_avg:53.91ms
step:1455/1920 train_time:78478ms step_avg:53.94ms
step:1456/1920 train_time:78567ms step_avg:53.96ms
step:1457/1920 train_time:78656ms step_avg:53.99ms
step:1458/1920 train_time:78744ms step_avg:54.01ms
step:1459/1920 train_time:78833ms step_avg:54.03ms
step:1460/1920 train_time:78921ms step_avg:54.06ms
step:1461/1920 train_time:79010ms step_avg:54.08ms
step:1462/1920 train_time:79099ms step_avg:54.10ms
step:1463/1920 train_time:79187ms step_avg:54.13ms
step:1464/1920 train_time:79275ms step_avg:54.15ms
step:1465/1920 train_time:79364ms step_avg:54.17ms
step:1466/1920 train_time:79452ms step_avg:54.20ms
step:1467/1920 train_time:79541ms step_avg:54.22ms
step:1468/1920 train_time:79628ms step_avg:54.24ms
step:1469/1920 train_time:79718ms step_avg:54.27ms
step:1470/1920 train_time:79806ms step_avg:54.29ms
step:1471/1920 train_time:79896ms step_avg:54.31ms
step:1472/1920 train_time:79983ms step_avg:54.34ms
step:1473/1920 train_time:80072ms step_avg:54.36ms
step:1474/1920 train_time:80159ms step_avg:54.38ms
step:1475/1920 train_time:80248ms step_avg:54.41ms
step:1476/1920 train_time:80336ms step_avg:54.43ms
step:1477/1920 train_time:80425ms step_avg:54.45ms
step:1478/1920 train_time:80514ms step_avg:54.48ms
step:1479/1920 train_time:80603ms step_avg:54.50ms
step:1480/1920 train_time:80692ms step_avg:54.52ms
step:1481/1920 train_time:80781ms step_avg:54.54ms
step:1482/1920 train_time:80869ms step_avg:54.57ms
step:1483/1920 train_time:80958ms step_avg:54.59ms
step:1484/1920 train_time:81046ms step_avg:54.61ms
step:1485/1920 train_time:81135ms step_avg:54.64ms
step:1486/1920 train_time:81223ms step_avg:54.66ms
step:1487/1920 train_time:81313ms step_avg:54.68ms
step:1488/1920 train_time:81400ms step_avg:54.70ms
step:1489/1920 train_time:81490ms step_avg:54.73ms
step:1490/1920 train_time:81577ms step_avg:54.75ms
step:1491/1920 train_time:81665ms step_avg:54.77ms
step:1492/1920 train_time:81755ms step_avg:54.80ms
step:1493/1920 train_time:81843ms step_avg:54.82ms
step:1494/1920 train_time:81932ms step_avg:54.84ms
step:1495/1920 train_time:82020ms step_avg:54.86ms
step:1496/1920 train_time:82109ms step_avg:54.89ms
step:1497/1920 train_time:82198ms step_avg:54.91ms
step:1498/1920 train_time:82287ms step_avg:54.93ms
step:1499/1920 train_time:82377ms step_avg:54.95ms
step:1500/1920 train_time:82466ms step_avg:54.98ms
step:1500/1920 val_loss:3.4153 train_time:82557ms step_avg:55.04ms
step:1501/1920 train_time:82577ms step_avg:55.01ms
step:1502/1920 train_time:82647ms step_avg:55.02ms
step:1503/1920 train_time:82739ms step_avg:55.05ms
step:1504/1920 train_time:82828ms step_avg:55.07ms
step:1505/1920 train_time:82915ms step_avg:55.09ms
step:1506/1920 train_time:83002ms step_avg:55.11ms
step:1507/1920 train_time:83090ms step_avg:55.14ms
step:1508/1920 train_time:83177ms step_avg:55.16ms
step:1509/1920 train_time:83264ms step_avg:55.18ms
step:1510/1920 train_time:83352ms step_avg:55.20ms
step:1511/1920 train_time:83440ms step_avg:55.22ms
step:1512/1920 train_time:83531ms step_avg:55.25ms
step:1513/1920 train_time:83622ms step_avg:55.27ms
step:1514/1920 train_time:83712ms step_avg:55.29ms
step:1515/1920 train_time:83801ms step_avg:55.31ms
step:1516/1920 train_time:83889ms step_avg:55.34ms
step:1517/1920 train_time:83976ms step_avg:55.36ms
step:1518/1920 train_time:84063ms step_avg:55.38ms
step:1519/1920 train_time:84152ms step_avg:55.40ms
step:1520/1920 train_time:84239ms step_avg:55.42ms
step:1521/1920 train_time:84328ms step_avg:55.44ms
step:1522/1920 train_time:84415ms step_avg:55.46ms
step:1523/1920 train_time:84505ms step_avg:55.49ms
step:1524/1920 train_time:84595ms step_avg:55.51ms
step:1525/1920 train_time:84685ms step_avg:55.53ms
step:1526/1920 train_time:84774ms step_avg:55.55ms
step:1527/1920 train_time:84862ms step_avg:55.57ms
step:1528/1920 train_time:84950ms step_avg:55.60ms
step:1529/1920 train_time:85037ms step_avg:55.62ms
step:1530/1920 train_time:85125ms step_avg:55.64ms
step:1531/1920 train_time:85214ms step_avg:55.66ms
step:1532/1920 train_time:85301ms step_avg:55.68ms
step:1533/1920 train_time:85390ms step_avg:55.70ms
step:1534/1920 train_time:85478ms step_avg:55.72ms
step:1535/1920 train_time:85567ms step_avg:55.74ms
step:1536/1920 train_time:85655ms step_avg:55.77ms
step:1537/1920 train_time:85745ms step_avg:55.79ms
step:1538/1920 train_time:85835ms step_avg:55.81ms
step:1539/1920 train_time:85923ms step_avg:55.83ms
step:1540/1920 train_time:86010ms step_avg:55.85ms
step:1541/1920 train_time:86099ms step_avg:55.87ms
step:1542/1920 train_time:86186ms step_avg:55.89ms
step:1543/1920 train_time:86275ms step_avg:55.91ms
step:1544/1920 train_time:86363ms step_avg:55.93ms
step:1545/1920 train_time:86453ms step_avg:55.96ms
step:1546/1920 train_time:86541ms step_avg:55.98ms
step:1547/1920 train_time:86630ms step_avg:56.00ms
step:1548/1920 train_time:86718ms step_avg:56.02ms
step:1549/1920 train_time:86807ms step_avg:56.04ms
step:1550/1920 train_time:86895ms step_avg:56.06ms
step:1551/1920 train_time:86984ms step_avg:56.08ms
step:1552/1920 train_time:87071ms step_avg:56.10ms
step:1553/1920 train_time:87160ms step_avg:56.12ms
step:1554/1920 train_time:87249ms step_avg:56.14ms
step:1555/1920 train_time:87338ms step_avg:56.17ms
step:1556/1920 train_time:87427ms step_avg:56.19ms
step:1557/1920 train_time:87517ms step_avg:56.21ms
step:1558/1920 train_time:87606ms step_avg:56.23ms
step:1559/1920 train_time:87696ms step_avg:56.25ms
step:1560/1920 train_time:87785ms step_avg:56.27ms
step:1561/1920 train_time:87874ms step_avg:56.29ms
step:1562/1920 train_time:87962ms step_avg:56.31ms
step:1563/1920 train_time:88050ms step_avg:56.33ms
step:1564/1920 train_time:88138ms step_avg:56.35ms
step:1565/1920 train_time:88227ms step_avg:56.37ms
step:1566/1920 train_time:88315ms step_avg:56.40ms
step:1567/1920 train_time:88404ms step_avg:56.42ms
step:1568/1920 train_time:88492ms step_avg:56.44ms
step:1569/1920 train_time:88581ms step_avg:56.46ms
step:1570/1920 train_time:88671ms step_avg:56.48ms
step:1571/1920 train_time:88760ms step_avg:56.50ms
step:1572/1920 train_time:88849ms step_avg:56.52ms
step:1573/1920 train_time:88938ms step_avg:56.54ms
step:1574/1920 train_time:89026ms step_avg:56.56ms
step:1575/1920 train_time:89115ms step_avg:56.58ms
step:1576/1920 train_time:89202ms step_avg:56.60ms
step:1577/1920 train_time:89291ms step_avg:56.62ms
step:1578/1920 train_time:89379ms step_avg:56.64ms
step:1579/1920 train_time:89468ms step_avg:56.66ms
step:1580/1920 train_time:89556ms step_avg:56.68ms
step:1581/1920 train_time:89645ms step_avg:56.70ms
step:1582/1920 train_time:89734ms step_avg:56.72ms
step:1583/1920 train_time:89823ms step_avg:56.74ms
step:1584/1920 train_time:89911ms step_avg:56.76ms
step:1585/1920 train_time:90000ms step_avg:56.78ms
step:1586/1920 train_time:90088ms step_avg:56.80ms
step:1587/1920 train_time:90177ms step_avg:56.82ms
step:1588/1920 train_time:90265ms step_avg:56.84ms
step:1589/1920 train_time:90355ms step_avg:56.86ms
step:1590/1920 train_time:90443ms step_avg:56.88ms
step:1591/1920 train_time:90533ms step_avg:56.90ms
step:1592/1920 train_time:90621ms step_avg:56.92ms
step:1593/1920 train_time:90710ms step_avg:56.94ms
step:1594/1920 train_time:90798ms step_avg:56.96ms
step:1595/1920 train_time:90887ms step_avg:56.98ms
step:1596/1920 train_time:90976ms step_avg:57.00ms
step:1597/1920 train_time:91065ms step_avg:57.02ms
step:1598/1920 train_time:91155ms step_avg:57.04ms
step:1599/1920 train_time:91244ms step_avg:57.06ms
step:1600/1920 train_time:91332ms step_avg:57.08ms
step:1601/1920 train_time:91420ms step_avg:57.10ms
step:1602/1920 train_time:91509ms step_avg:57.12ms
step:1603/1920 train_time:91598ms step_avg:57.14ms
step:1604/1920 train_time:91686ms step_avg:57.16ms
step:1605/1920 train_time:91775ms step_avg:57.18ms
step:1606/1920 train_time:91864ms step_avg:57.20ms
step:1607/1920 train_time:91954ms step_avg:57.22ms
step:1608/1920 train_time:92042ms step_avg:57.24ms
step:1609/1920 train_time:92132ms step_avg:57.26ms
step:1610/1920 train_time:92219ms step_avg:57.28ms
step:1611/1920 train_time:92308ms step_avg:57.30ms
step:1612/1920 train_time:92396ms step_avg:57.32ms
step:1613/1920 train_time:92484ms step_avg:57.34ms
step:1614/1920 train_time:92572ms step_avg:57.36ms
step:1615/1920 train_time:92660ms step_avg:57.37ms
step:1616/1920 train_time:92749ms step_avg:57.39ms
step:1617/1920 train_time:92838ms step_avg:57.41ms
step:1618/1920 train_time:92927ms step_avg:57.43ms
step:1619/1920 train_time:93017ms step_avg:57.45ms
step:1620/1920 train_time:93105ms step_avg:57.47ms
step:1621/1920 train_time:93196ms step_avg:57.49ms
step:1622/1920 train_time:93284ms step_avg:57.51ms
step:1623/1920 train_time:93373ms step_avg:57.53ms
step:1624/1920 train_time:93461ms step_avg:57.55ms
step:1625/1920 train_time:93550ms step_avg:57.57ms
step:1626/1920 train_time:93638ms step_avg:57.59ms
step:1627/1920 train_time:93727ms step_avg:57.61ms
step:1628/1920 train_time:93815ms step_avg:57.63ms
step:1629/1920 train_time:93903ms step_avg:57.64ms
step:1630/1920 train_time:93993ms step_avg:57.66ms
step:1631/1920 train_time:94081ms step_avg:57.68ms
step:1632/1920 train_time:94170ms step_avg:57.70ms
step:1633/1920 train_time:94258ms step_avg:57.72ms
step:1634/1920 train_time:94346ms step_avg:57.74ms
step:1635/1920 train_time:94434ms step_avg:57.76ms
step:1636/1920 train_time:94523ms step_avg:57.78ms
step:1637/1920 train_time:94612ms step_avg:57.80ms
step:1638/1920 train_time:94699ms step_avg:57.81ms
step:1639/1920 train_time:94789ms step_avg:57.83ms
step:1640/1920 train_time:94877ms step_avg:57.85ms
step:1641/1920 train_time:94966ms step_avg:57.87ms
step:1642/1920 train_time:95055ms step_avg:57.89ms
step:1643/1920 train_time:95144ms step_avg:57.91ms
step:1644/1920 train_time:95232ms step_avg:57.93ms
step:1645/1920 train_time:95321ms step_avg:57.95ms
step:1646/1920 train_time:95409ms step_avg:57.96ms
step:1647/1920 train_time:95497ms step_avg:57.98ms
step:1648/1920 train_time:95585ms step_avg:58.00ms
step:1649/1920 train_time:95675ms step_avg:58.02ms
step:1650/1920 train_time:95764ms step_avg:58.04ms
step:1651/1920 train_time:95853ms step_avg:58.06ms
step:1652/1920 train_time:95941ms step_avg:58.08ms
step:1653/1920 train_time:96030ms step_avg:58.09ms
step:1654/1920 train_time:96118ms step_avg:58.11ms
step:1655/1920 train_time:96208ms step_avg:58.13ms
step:1656/1920 train_time:96297ms step_avg:58.15ms
step:1657/1920 train_time:96385ms step_avg:58.17ms
step:1658/1920 train_time:96473ms step_avg:58.19ms
step:1659/1920 train_time:96562ms step_avg:58.20ms
step:1660/1920 train_time:96650ms step_avg:58.22ms
step:1661/1920 train_time:96740ms step_avg:58.24ms
step:1662/1920 train_time:96827ms step_avg:58.26ms
step:1663/1920 train_time:96917ms step_avg:58.28ms
step:1664/1920 train_time:97006ms step_avg:58.30ms
step:1665/1920 train_time:97095ms step_avg:58.32ms
step:1666/1920 train_time:97183ms step_avg:58.33ms
step:1667/1920 train_time:97272ms step_avg:58.35ms
step:1668/1920 train_time:97360ms step_avg:58.37ms
step:1669/1920 train_time:97449ms step_avg:58.39ms
step:1670/1920 train_time:97537ms step_avg:58.41ms
step:1671/1920 train_time:97625ms step_avg:58.42ms
step:1672/1920 train_time:97713ms step_avg:58.44ms
step:1673/1920 train_time:97801ms step_avg:58.46ms
step:1674/1920 train_time:97891ms step_avg:58.48ms
step:1675/1920 train_time:97980ms step_avg:58.50ms
step:1676/1920 train_time:98069ms step_avg:58.51ms
step:1677/1920 train_time:98159ms step_avg:58.53ms
step:1678/1920 train_time:98250ms step_avg:58.55ms
step:1679/1920 train_time:98339ms step_avg:58.57ms
step:1680/1920 train_time:98428ms step_avg:58.59ms
step:1681/1920 train_time:98518ms step_avg:58.61ms
step:1682/1920 train_time:98606ms step_avg:58.62ms
step:1683/1920 train_time:98696ms step_avg:58.64ms
step:1684/1920 train_time:98783ms step_avg:58.66ms
step:1685/1920 train_time:98873ms step_avg:58.68ms
step:1686/1920 train_time:98962ms step_avg:58.70ms
step:1687/1920 train_time:99051ms step_avg:58.71ms
step:1688/1920 train_time:99139ms step_avg:58.73ms
step:1689/1920 train_time:99228ms step_avg:58.75ms
step:1690/1920 train_time:99317ms step_avg:58.77ms
step:1691/1920 train_time:99405ms step_avg:58.78ms
step:1692/1920 train_time:99493ms step_avg:58.80ms
step:1693/1920 train_time:99582ms step_avg:58.82ms
step:1694/1920 train_time:99670ms step_avg:58.84ms
step:1695/1920 train_time:99758ms step_avg:58.85ms
step:1696/1920 train_time:99847ms step_avg:58.87ms
step:1697/1920 train_time:99937ms step_avg:58.89ms
step:1698/1920 train_time:100025ms step_avg:58.91ms
step:1699/1920 train_time:100114ms step_avg:58.93ms
step:1700/1920 train_time:100202ms step_avg:58.94ms
step:1701/1920 train_time:100291ms step_avg:58.96ms
step:1702/1920 train_time:100379ms step_avg:58.98ms
step:1703/1920 train_time:100468ms step_avg:58.99ms
step:1704/1920 train_time:100555ms step_avg:59.01ms
step:1705/1920 train_time:100644ms step_avg:59.03ms
step:1706/1920 train_time:100732ms step_avg:59.05ms
step:1707/1920 train_time:100820ms step_avg:59.06ms
step:1708/1920 train_time:100908ms step_avg:59.08ms
step:1709/1920 train_time:100998ms step_avg:59.10ms
step:1710/1920 train_time:101086ms step_avg:59.11ms
step:1711/1920 train_time:101176ms step_avg:59.13ms
step:1712/1920 train_time:101265ms step_avg:59.15ms
step:1713/1920 train_time:101356ms step_avg:59.17ms
step:1714/1920 train_time:101445ms step_avg:59.19ms
step:1715/1920 train_time:101534ms step_avg:59.20ms
step:1716/1920 train_time:101621ms step_avg:59.22ms
step:1717/1920 train_time:101710ms step_avg:59.24ms
step:1718/1920 train_time:101798ms step_avg:59.25ms
step:1719/1920 train_time:101887ms step_avg:59.27ms
step:1720/1920 train_time:101976ms step_avg:59.29ms
step:1721/1920 train_time:102065ms step_avg:59.31ms
step:1722/1920 train_time:102153ms step_avg:59.32ms
step:1723/1920 train_time:102242ms step_avg:59.34ms
step:1724/1920 train_time:102330ms step_avg:59.36ms
step:1725/1920 train_time:102419ms step_avg:59.37ms
step:1726/1920 train_time:102507ms step_avg:59.39ms
step:1727/1920 train_time:102596ms step_avg:59.41ms
step:1728/1920 train_time:102683ms step_avg:59.42ms
step:1729/1920 train_time:102772ms step_avg:59.44ms
step:1730/1920 train_time:102859ms step_avg:59.46ms
step:1731/1920 train_time:102950ms step_avg:59.47ms
step:1732/1920 train_time:103038ms step_avg:59.49ms
step:1733/1920 train_time:103127ms step_avg:59.51ms
step:1734/1920 train_time:103215ms step_avg:59.52ms
step:1735/1920 train_time:103303ms step_avg:59.54ms
step:1736/1920 train_time:103392ms step_avg:59.56ms
step:1737/1920 train_time:103481ms step_avg:59.57ms
step:1738/1920 train_time:103569ms step_avg:59.59ms
step:1739/1920 train_time:103658ms step_avg:59.61ms
step:1740/1920 train_time:103747ms step_avg:59.62ms
step:1741/1920 train_time:103837ms step_avg:59.64ms
step:1742/1920 train_time:103925ms step_avg:59.66ms
step:1743/1920 train_time:104015ms step_avg:59.68ms
step:1744/1920 train_time:104103ms step_avg:59.69ms
step:1745/1920 train_time:104192ms step_avg:59.71ms
step:1746/1920 train_time:104279ms step_avg:59.72ms
step:1747/1920 train_time:104368ms step_avg:59.74ms
step:1748/1920 train_time:104456ms step_avg:59.76ms
step:1749/1920 train_time:104544ms step_avg:59.77ms
step:1750/1920 train_time:104633ms step_avg:59.79ms
step:1750/1920 val_loss:3.3235 train_time:104724ms step_avg:59.84ms
step:1751/1920 train_time:104742ms step_avg:59.82ms
step:1752/1920 train_time:104814ms step_avg:59.83ms
step:1753/1920 train_time:104906ms step_avg:59.84ms
step:1754/1920 train_time:104994ms step_avg:59.86ms
step:1755/1920 train_time:105082ms step_avg:59.88ms
step:1756/1920 train_time:105170ms step_avg:59.89ms
step:1757/1920 train_time:105257ms step_avg:59.91ms
step:1758/1920 train_time:105345ms step_avg:59.92ms
step:1759/1920 train_time:105433ms step_avg:59.94ms
step:1760/1920 train_time:105521ms step_avg:59.95ms
step:1761/1920 train_time:105608ms step_avg:59.97ms
step:1762/1920 train_time:105697ms step_avg:59.99ms
step:1763/1920 train_time:105788ms step_avg:60.00ms
step:1764/1920 train_time:105877ms step_avg:60.02ms
step:1765/1920 train_time:105968ms step_avg:60.04ms
step:1766/1920 train_time:106055ms step_avg:60.05ms
step:1767/1920 train_time:106144ms step_avg:60.07ms
step:1768/1920 train_time:106231ms step_avg:60.09ms
step:1769/1920 train_time:106320ms step_avg:60.10ms
step:1770/1920 train_time:106407ms step_avg:60.12ms
step:1771/1920 train_time:106496ms step_avg:60.13ms
step:1772/1920 train_time:106584ms step_avg:60.15ms
step:1773/1920 train_time:106674ms step_avg:60.17ms
step:1774/1920 train_time:106764ms step_avg:60.18ms
step:1775/1920 train_time:106854ms step_avg:60.20ms
step:1776/1920 train_time:106942ms step_avg:60.22ms
step:1777/1920 train_time:107032ms step_avg:60.23ms
step:1778/1920 train_time:107119ms step_avg:60.25ms
step:1779/1920 train_time:107207ms step_avg:60.26ms
step:1780/1920 train_time:107295ms step_avg:60.28ms
step:1781/1920 train_time:107383ms step_avg:60.29ms
step:1782/1920 train_time:107471ms step_avg:60.31ms
step:1783/1920 train_time:107560ms step_avg:60.33ms
step:1784/1920 train_time:107648ms step_avg:60.34ms
step:1785/1920 train_time:107737ms step_avg:60.36ms
step:1786/1920 train_time:107826ms step_avg:60.37ms
step:1787/1920 train_time:107915ms step_avg:60.39ms
step:1788/1920 train_time:108004ms step_avg:60.41ms
step:1789/1920 train_time:108094ms step_avg:60.42ms
step:1790/1920 train_time:108182ms step_avg:60.44ms
step:1791/1920 train_time:108271ms step_avg:60.45ms
step:1792/1920 train_time:108357ms step_avg:60.47ms
step:1793/1920 train_time:108445ms step_avg:60.48ms
step:1794/1920 train_time:108533ms step_avg:60.50ms
step:1795/1920 train_time:108621ms step_avg:60.51ms
step:1796/1920 train_time:108710ms step_avg:60.53ms
step:1797/1920 train_time:108799ms step_avg:60.54ms
step:1798/1920 train_time:108887ms step_avg:60.56ms
step:1799/1920 train_time:108977ms step_avg:60.58ms
step:1800/1920 train_time:109065ms step_avg:60.59ms
step:1801/1920 train_time:109154ms step_avg:60.61ms
step:1802/1920 train_time:109242ms step_avg:60.62ms
step:1803/1920 train_time:109331ms step_avg:60.64ms
step:1804/1920 train_time:109418ms step_avg:60.65ms
step:1805/1920 train_time:109506ms step_avg:60.67ms
step:1806/1920 train_time:109594ms step_avg:60.68ms
step:1807/1920 train_time:109683ms step_avg:60.70ms
step:1808/1920 train_time:109772ms step_avg:60.71ms
step:1809/1920 train_time:109860ms step_avg:60.73ms
step:1810/1920 train_time:109949ms step_avg:60.75ms
step:1811/1920 train_time:110039ms step_avg:60.76ms
step:1812/1920 train_time:110127ms step_avg:60.78ms
step:1813/1920 train_time:110217ms step_avg:60.79ms
step:1814/1920 train_time:110305ms step_avg:60.81ms
step:1815/1920 train_time:110394ms step_avg:60.82ms
step:1816/1920 train_time:110483ms step_avg:60.84ms
step:1817/1920 train_time:110571ms step_avg:60.85ms
step:1818/1920 train_time:110659ms step_avg:60.87ms
step:1819/1920 train_time:110749ms step_avg:60.88ms
step:1820/1920 train_time:110837ms step_avg:60.90ms
step:1821/1920 train_time:110925ms step_avg:60.91ms
step:1822/1920 train_time:111014ms step_avg:60.93ms
step:1823/1920 train_time:111103ms step_avg:60.94ms
step:1824/1920 train_time:111191ms step_avg:60.96ms
step:1825/1920 train_time:111279ms step_avg:60.97ms
step:1826/1920 train_time:111367ms step_avg:60.99ms
step:1827/1920 train_time:111456ms step_avg:61.00ms
step:1828/1920 train_time:111544ms step_avg:61.02ms
step:1829/1920 train_time:111633ms step_avg:61.03ms
step:1830/1920 train_time:111721ms step_avg:61.05ms
step:1831/1920 train_time:111810ms step_avg:61.07ms
step:1832/1920 train_time:111898ms step_avg:61.08ms
step:1833/1920 train_time:111987ms step_avg:61.09ms
step:1834/1920 train_time:112075ms step_avg:61.11ms
step:1835/1920 train_time:112163ms step_avg:61.12ms
step:1836/1920 train_time:112251ms step_avg:61.14ms
step:1837/1920 train_time:112339ms step_avg:61.15ms
step:1838/1920 train_time:112428ms step_avg:61.17ms
step:1839/1920 train_time:112517ms step_avg:61.18ms
step:1840/1920 train_time:112605ms step_avg:61.20ms
step:1841/1920 train_time:112694ms step_avg:61.21ms
step:1842/1920 train_time:112783ms step_avg:61.23ms
step:1843/1920 train_time:112873ms step_avg:61.24ms
step:1844/1920 train_time:112961ms step_avg:61.26ms
step:1845/1920 train_time:113050ms step_avg:61.27ms
step:1846/1920 train_time:113137ms step_avg:61.29ms
step:1847/1920 train_time:113226ms step_avg:61.30ms
step:1848/1920 train_time:113313ms step_avg:61.32ms
step:1849/1920 train_time:113402ms step_avg:61.33ms
step:1850/1920 train_time:113491ms step_avg:61.35ms
step:1851/1920 train_time:113580ms step_avg:61.36ms
step:1852/1920 train_time:113669ms step_avg:61.38ms
step:1853/1920 train_time:113758ms step_avg:61.39ms
step:1854/1920 train_time:113846ms step_avg:61.41ms
step:1855/1920 train_time:113937ms step_avg:61.42ms
step:1856/1920 train_time:114025ms step_avg:61.44ms
step:1857/1920 train_time:114114ms step_avg:61.45ms
step:1858/1920 train_time:114202ms step_avg:61.46ms
step:1859/1920 train_time:114290ms step_avg:61.48ms
step:1860/1920 train_time:114378ms step_avg:61.49ms
step:1861/1920 train_time:114468ms step_avg:61.51ms
step:1862/1920 train_time:114556ms step_avg:61.52ms
step:1863/1920 train_time:114644ms step_avg:61.54ms
step:1864/1920 train_time:114732ms step_avg:61.55ms
step:1865/1920 train_time:114821ms step_avg:61.57ms
step:1866/1920 train_time:114909ms step_avg:61.58ms
step:1867/1920 train_time:114998ms step_avg:61.59ms
step:1868/1920 train_time:115086ms step_avg:61.61ms
step:1869/1920 train_time:115175ms step_avg:61.62ms
step:1870/1920 train_time:115263ms step_avg:61.64ms
step:1871/1920 train_time:115352ms step_avg:61.65ms
step:1872/1920 train_time:115440ms step_avg:61.67ms
step:1873/1920 train_time:115529ms step_avg:61.68ms
step:1874/1920 train_time:115617ms step_avg:61.70ms
step:1875/1920 train_time:115706ms step_avg:61.71ms
step:1876/1920 train_time:115794ms step_avg:61.72ms
step:1877/1920 train_time:115882ms step_avg:61.74ms
step:1878/1920 train_time:115972ms step_avg:61.75ms
step:1879/1920 train_time:116060ms step_avg:61.77ms
step:1880/1920 train_time:116149ms step_avg:61.78ms
step:1881/1920 train_time:116238ms step_avg:61.80ms
step:1882/1920 train_time:116327ms step_avg:61.81ms
step:1883/1920 train_time:116415ms step_avg:61.82ms
step:1884/1920 train_time:116504ms step_avg:61.84ms
step:1885/1920 train_time:116594ms step_avg:61.85ms
step:1886/1920 train_time:116682ms step_avg:61.87ms
step:1887/1920 train_time:116773ms step_avg:61.88ms
step:1888/1920 train_time:116862ms step_avg:61.90ms
step:1889/1920 train_time:116952ms step_avg:61.91ms
step:1890/1920 train_time:117041ms step_avg:61.93ms
step:1891/1920 train_time:117130ms step_avg:61.94ms
step:1892/1920 train_time:117217ms step_avg:61.95ms
step:1893/1920 train_time:117307ms step_avg:61.97ms
step:1894/1920 train_time:117394ms step_avg:61.98ms
step:1895/1920 train_time:117485ms step_avg:62.00ms
step:1896/1920 train_time:117573ms step_avg:62.01ms
step:1897/1920 train_time:117662ms step_avg:62.03ms
step:1898/1920 train_time:117750ms step_avg:62.04ms
step:1899/1920 train_time:117840ms step_avg:62.05ms
step:1900/1920 train_time:117928ms step_avg:62.07ms
step:1901/1920 train_time:118018ms step_avg:62.08ms
step:1902/1920 train_time:118107ms step_avg:62.10ms
step:1903/1920 train_time:118196ms step_avg:62.11ms
step:1904/1920 train_time:118285ms step_avg:62.12ms
step:1905/1920 train_time:118375ms step_avg:62.14ms
step:1906/1920 train_time:118464ms step_avg:62.15ms
step:1907/1920 train_time:118553ms step_avg:62.17ms
step:1908/1920 train_time:118641ms step_avg:62.18ms
step:1909/1920 train_time:118730ms step_avg:62.20ms
step:1910/1920 train_time:118818ms step_avg:62.21ms
step:1911/1920 train_time:118907ms step_avg:62.22ms
step:1912/1920 train_time:118996ms step_avg:62.24ms
step:1913/1920 train_time:119086ms step_avg:62.25ms
step:1914/1920 train_time:119176ms step_avg:62.27ms
step:1915/1920 train_time:119266ms step_avg:62.28ms
step:1916/1920 train_time:119355ms step_avg:62.29ms
step:1917/1920 train_time:119445ms step_avg:62.31ms
step:1918/1920 train_time:119533ms step_avg:62.32ms
step:1919/1920 train_time:119621ms step_avg:62.34ms
step:1920/1920 train_time:119709ms step_avg:62.35ms
step:1920/1920 val_loss:3.2781 train_time:119801ms step_avg:62.40ms
peak memory allocated: 29817 MiB reserved: 44958 MiB

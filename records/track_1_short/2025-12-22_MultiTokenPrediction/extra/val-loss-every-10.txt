import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 10  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 13:31:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    270407      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    270408      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    270409      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    270410      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    270411      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    270412      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    270413      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    270414      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    270408      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    270409      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    270410      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    270411      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    270412      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    270413      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    270414      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8361 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:69ms step_avg:68.54ms
step:2/1920 train_time:93ms step_avg:46.29ms
step:3/1920 train_time:127ms step_avg:42.34ms
step:4/1920 train_time:161ms step_avg:40.22ms
step:5/1920 train_time:195ms step_avg:39.03ms
step:6/1920 train_time:263ms step_avg:43.79ms
step:7/1920 train_time:293ms step_avg:41.83ms
step:8/1920 train_time:327ms step_avg:40.87ms
step:9/1920 train_time:361ms step_avg:40.13ms
step:10/1920 train_time:395ms step_avg:39.51ms
step:10/1920 val_loss:7.2161 train_time:433ms step_avg:43.25ms
step:11/1920 train_time:450ms step_avg:40.90ms
step:12/1920 train_time:467ms step_avg:38.90ms
step:13/1920 train_time:502ms step_avg:38.59ms
step:14/1920 train_time:536ms step_avg:38.29ms
step:15/1920 train_time:572ms step_avg:38.12ms
step:16/1920 train_time:607ms step_avg:37.93ms
step:17/1920 train_time:642ms step_avg:37.78ms
step:18/1920 train_time:676ms step_avg:37.58ms
step:19/1920 train_time:712ms step_avg:37.46ms
step:20/1920 train_time:746ms step_avg:37.29ms
step:20/1920 val_loss:6.5623 train_time:784ms step_avg:39.19ms
step:21/1920 train_time:800ms step_avg:38.11ms
step:22/1920 train_time:818ms step_avg:37.19ms
step:23/1920 train_time:854ms step_avg:37.11ms
step:24/1920 train_time:888ms step_avg:37.01ms
step:25/1920 train_time:924ms step_avg:36.95ms
step:26/1920 train_time:959ms step_avg:36.87ms
step:27/1920 train_time:994ms step_avg:36.82ms
step:28/1920 train_time:1029ms step_avg:36.74ms
step:29/1920 train_time:1064ms step_avg:36.68ms
step:30/1920 train_time:1098ms step_avg:36.59ms
step:30/1920 val_loss:6.2093 train_time:1136ms step_avg:37.85ms
step:31/1920 train_time:1153ms step_avg:37.18ms
step:32/1920 train_time:1170ms step_avg:36.58ms
step:33/1920 train_time:1205ms step_avg:36.53ms
step:34/1920 train_time:1240ms step_avg:36.47ms
step:35/1920 train_time:1275ms step_avg:36.44ms
step:36/1920 train_time:1310ms step_avg:36.39ms
step:37/1920 train_time:1345ms step_avg:36.34ms
step:38/1920 train_time:1379ms step_avg:36.28ms
step:39/1920 train_time:1414ms step_avg:36.24ms
step:40/1920 train_time:1448ms step_avg:36.19ms
step:40/1920 val_loss:5.9463 train_time:1485ms step_avg:37.13ms
step:41/1920 train_time:1502ms step_avg:36.63ms
step:42/1920 train_time:1519ms step_avg:36.17ms
step:43/1920 train_time:1554ms step_avg:36.13ms
step:44/1920 train_time:1589ms step_avg:36.11ms
step:45/1920 train_time:1625ms step_avg:36.11ms
step:46/1920 train_time:1660ms step_avg:36.08ms
step:47/1920 train_time:1695ms step_avg:36.06ms
step:48/1920 train_time:1729ms step_avg:36.02ms
step:49/1920 train_time:1764ms step_avg:36.00ms
step:50/1920 train_time:1798ms step_avg:35.97ms
step:50/1920 val_loss:5.6711 train_time:1836ms step_avg:36.72ms
step:51/1920 train_time:1853ms step_avg:36.34ms
step:52/1920 train_time:1870ms step_avg:35.96ms
step:53/1920 train_time:1905ms step_avg:35.95ms
step:54/1920 train_time:1940ms step_avg:35.93ms
step:55/1920 train_time:1976ms step_avg:35.93ms
step:56/1920 train_time:2011ms step_avg:35.91ms
step:57/1920 train_time:2046ms step_avg:35.90ms
step:58/1920 train_time:2081ms step_avg:35.88ms
step:59/1920 train_time:2116ms step_avg:35.86ms
step:60/1920 train_time:2150ms step_avg:35.83ms
step:60/1920 val_loss:5.5088 train_time:2187ms step_avg:36.46ms
step:61/1920 train_time:2204ms step_avg:36.13ms
step:62/1920 train_time:2222ms step_avg:35.83ms
step:63/1920 train_time:2257ms step_avg:35.82ms
step:64/1920 train_time:2291ms step_avg:35.80ms
step:65/1920 train_time:2328ms step_avg:35.82ms
step:66/1920 train_time:2364ms step_avg:35.81ms
step:67/1920 train_time:2399ms step_avg:35.81ms
step:68/1920 train_time:2433ms step_avg:35.78ms
step:69/1920 train_time:2468ms step_avg:35.77ms
step:70/1920 train_time:2502ms step_avg:35.75ms
step:70/1920 val_loss:5.3658 train_time:2540ms step_avg:36.29ms
step:71/1920 train_time:2558ms step_avg:36.02ms
step:72/1920 train_time:2576ms step_avg:35.77ms
step:73/1920 train_time:2610ms step_avg:35.76ms
step:74/1920 train_time:2645ms step_avg:35.74ms
step:75/1920 train_time:2682ms step_avg:35.75ms
step:76/1920 train_time:2716ms step_avg:35.74ms
step:77/1920 train_time:2751ms step_avg:35.73ms
step:78/1920 train_time:2786ms step_avg:35.71ms
step:79/1920 train_time:2821ms step_avg:35.70ms
step:80/1920 train_time:2855ms step_avg:35.69ms
step:80/1920 val_loss:5.2902 train_time:2893ms step_avg:36.16ms
step:81/1920 train_time:2909ms step_avg:35.92ms
step:82/1920 train_time:2927ms step_avg:35.69ms
step:83/1920 train_time:2962ms step_avg:35.68ms
step:84/1920 train_time:2996ms step_avg:35.67ms
step:85/1920 train_time:3032ms step_avg:35.67ms
step:86/1920 train_time:3068ms step_avg:35.67ms
step:87/1920 train_time:3103ms step_avg:35.67ms
step:88/1920 train_time:3137ms step_avg:35.65ms
step:89/1920 train_time:3172ms step_avg:35.64ms
step:90/1920 train_time:3207ms step_avg:35.63ms
step:90/1920 val_loss:5.1821 train_time:3244ms step_avg:36.05ms
step:91/1920 train_time:3261ms step_avg:35.83ms
step:92/1920 train_time:3281ms step_avg:35.67ms
step:93/1920 train_time:3316ms step_avg:35.66ms
step:94/1920 train_time:3351ms step_avg:35.65ms
step:95/1920 train_time:3386ms step_avg:35.65ms
step:96/1920 train_time:3421ms step_avg:35.63ms
step:97/1920 train_time:3456ms step_avg:35.62ms
step:98/1920 train_time:3490ms step_avg:35.61ms
step:99/1920 train_time:3524ms step_avg:35.60ms
step:100/1920 train_time:3558ms step_avg:35.58ms
step:100/1920 val_loss:5.1240 train_time:3596ms step_avg:35.96ms
step:101/1920 train_time:3613ms step_avg:35.77ms
step:102/1920 train_time:3631ms step_avg:35.60ms
step:103/1920 train_time:3666ms step_avg:35.59ms
step:104/1920 train_time:3700ms step_avg:35.58ms
step:105/1920 train_time:3736ms step_avg:35.58ms
step:106/1920 train_time:3772ms step_avg:35.58ms
step:107/1920 train_time:3807ms step_avg:35.58ms
step:108/1920 train_time:3841ms step_avg:35.57ms
step:109/1920 train_time:3876ms step_avg:35.56ms
step:110/1920 train_time:3911ms step_avg:35.55ms
step:110/1920 val_loss:5.0496 train_time:3948ms step_avg:35.89ms
step:111/1920 train_time:3965ms step_avg:35.72ms
step:112/1920 train_time:3985ms step_avg:35.58ms
step:113/1920 train_time:4021ms step_avg:35.58ms
step:114/1920 train_time:4056ms step_avg:35.58ms
step:115/1920 train_time:4091ms step_avg:35.57ms
step:116/1920 train_time:4126ms step_avg:35.57ms
step:117/1920 train_time:4161ms step_avg:35.56ms
step:118/1920 train_time:4195ms step_avg:35.55ms
step:119/1920 train_time:4229ms step_avg:35.54ms
step:120/1920 train_time:4263ms step_avg:35.53ms
step:120/1920 val_loss:5.0070 train_time:4301ms step_avg:35.84ms
step:121/1920 train_time:4317ms step_avg:35.68ms
step:122/1920 train_time:4337ms step_avg:35.55ms
step:123/1920 train_time:4372ms step_avg:35.54ms
step:124/1920 train_time:4407ms step_avg:35.54ms
step:125/1920 train_time:4443ms step_avg:35.54ms
step:126/1920 train_time:4478ms step_avg:35.54ms
step:127/1920 train_time:4513ms step_avg:35.54ms
step:128/1920 train_time:4547ms step_avg:35.53ms
step:129/1920 train_time:4582ms step_avg:35.52ms
step:130/1920 train_time:4616ms step_avg:35.51ms
step:130/1920 val_loss:4.9325 train_time:4654ms step_avg:35.80ms
step:131/1920 train_time:4670ms step_avg:35.65ms
step:132/1920 train_time:4691ms step_avg:35.54ms
step:133/1920 train_time:4726ms step_avg:35.53ms
step:134/1920 train_time:4761ms step_avg:35.53ms
step:135/1920 train_time:4797ms step_avg:35.53ms
step:136/1920 train_time:4831ms step_avg:35.53ms
step:137/1920 train_time:4866ms step_avg:35.52ms
step:138/1920 train_time:4900ms step_avg:35.51ms
step:139/1920 train_time:4935ms step_avg:35.50ms
step:140/1920 train_time:4969ms step_avg:35.49ms
step:140/1920 val_loss:4.9207 train_time:5007ms step_avg:35.76ms
step:141/1920 train_time:5023ms step_avg:35.63ms
step:142/1920 train_time:5041ms step_avg:35.50ms
step:143/1920 train_time:5075ms step_avg:35.49ms
step:144/1920 train_time:5110ms step_avg:35.49ms
step:145/1920 train_time:5146ms step_avg:35.49ms
step:146/1920 train_time:5181ms step_avg:35.48ms
step:147/1920 train_time:5216ms step_avg:35.48ms
step:148/1920 train_time:5250ms step_avg:35.47ms
step:149/1920 train_time:5284ms step_avg:35.47ms
step:150/1920 train_time:5319ms step_avg:35.46ms
step:150/1920 val_loss:4.8774 train_time:5356ms step_avg:35.71ms
step:151/1920 train_time:5373ms step_avg:35.58ms
step:152/1920 train_time:5391ms step_avg:35.47ms
step:153/1920 train_time:5426ms step_avg:35.46ms
step:154/1920 train_time:5461ms step_avg:35.46ms
step:155/1920 train_time:5498ms step_avg:35.47ms
step:156/1920 train_time:5533ms step_avg:35.47ms
step:157/1920 train_time:5568ms step_avg:35.47ms
step:158/1920 train_time:5602ms step_avg:35.46ms
step:159/1920 train_time:5637ms step_avg:35.45ms
step:160/1920 train_time:5671ms step_avg:35.45ms
step:160/1920 val_loss:4.8400 train_time:5709ms step_avg:35.68ms
step:161/1920 train_time:5727ms step_avg:35.57ms
step:162/1920 train_time:5745ms step_avg:35.46ms
step:163/1920 train_time:5780ms step_avg:35.46ms
step:164/1920 train_time:5815ms step_avg:35.46ms
step:165/1920 train_time:5851ms step_avg:35.46ms
step:166/1920 train_time:5885ms step_avg:35.45ms
step:167/1920 train_time:5920ms step_avg:35.45ms
step:168/1920 train_time:5955ms step_avg:35.44ms
step:169/1920 train_time:5989ms step_avg:35.44ms
step:170/1920 train_time:6023ms step_avg:35.43ms
step:170/1920 val_loss:4.8092 train_time:6061ms step_avg:35.65ms
step:171/1920 train_time:6078ms step_avg:35.54ms
step:172/1920 train_time:6095ms step_avg:35.44ms
step:173/1920 train_time:6129ms step_avg:35.43ms
step:174/1920 train_time:6165ms step_avg:35.43ms
step:175/1920 train_time:6201ms step_avg:35.43ms
step:176/1920 train_time:6236ms step_avg:35.43ms
step:177/1920 train_time:6270ms step_avg:35.43ms
step:178/1920 train_time:6304ms step_avg:35.42ms
step:179/1920 train_time:6339ms step_avg:35.41ms
step:180/1920 train_time:6373ms step_avg:35.41ms
step:180/1920 val_loss:4.7650 train_time:6411ms step_avg:35.62ms
step:181/1920 train_time:6427ms step_avg:35.51ms
step:182/1920 train_time:6446ms step_avg:35.42ms
step:183/1920 train_time:6481ms step_avg:35.41ms
step:184/1920 train_time:6516ms step_avg:35.41ms
step:185/1920 train_time:6552ms step_avg:35.41ms
step:186/1920 train_time:6587ms step_avg:35.41ms
step:187/1920 train_time:6622ms step_avg:35.41ms
step:188/1920 train_time:6656ms step_avg:35.40ms
step:189/1920 train_time:6691ms step_avg:35.40ms
step:190/1920 train_time:6725ms step_avg:35.39ms
step:190/1920 val_loss:4.7549 train_time:6762ms step_avg:35.59ms
step:191/1920 train_time:6779ms step_avg:35.49ms
step:192/1920 train_time:6796ms step_avg:35.39ms
step:193/1920 train_time:6830ms step_avg:35.39ms
step:194/1920 train_time:6865ms step_avg:35.39ms
step:195/1920 train_time:6901ms step_avg:35.39ms
step:196/1920 train_time:6936ms step_avg:35.39ms
step:197/1920 train_time:6970ms step_avg:35.38ms
step:198/1920 train_time:7005ms step_avg:35.38ms
step:199/1920 train_time:7039ms step_avg:35.37ms
step:200/1920 train_time:7074ms step_avg:35.37ms
step:200/1920 val_loss:4.7098 train_time:7111ms step_avg:35.56ms
step:201/1920 train_time:7128ms step_avg:35.46ms
step:202/1920 train_time:7148ms step_avg:35.38ms
step:203/1920 train_time:7182ms step_avg:35.38ms
step:204/1920 train_time:7217ms step_avg:35.38ms
step:205/1920 train_time:7253ms step_avg:35.38ms
step:206/1920 train_time:7287ms step_avg:35.38ms
step:207/1920 train_time:7322ms step_avg:35.37ms
step:208/1920 train_time:7356ms step_avg:35.37ms
step:209/1920 train_time:7391ms step_avg:35.36ms
step:210/1920 train_time:7425ms step_avg:35.36ms
step:210/1920 val_loss:4.6764 train_time:7463ms step_avg:35.54ms
step:211/1920 train_time:7481ms step_avg:35.45ms
step:212/1920 train_time:7498ms step_avg:35.37ms
step:213/1920 train_time:7533ms step_avg:35.37ms
step:214/1920 train_time:7568ms step_avg:35.36ms
step:215/1920 train_time:7604ms step_avg:35.37ms
step:216/1920 train_time:7638ms step_avg:35.36ms
step:217/1920 train_time:7673ms step_avg:35.36ms
step:218/1920 train_time:7707ms step_avg:35.36ms
step:219/1920 train_time:7742ms step_avg:35.35ms
step:220/1920 train_time:7776ms step_avg:35.34ms
step:220/1920 val_loss:4.6897 train_time:7813ms step_avg:35.52ms
step:221/1920 train_time:7830ms step_avg:35.43ms
step:222/1920 train_time:7848ms step_avg:35.35ms
step:223/1920 train_time:7883ms step_avg:35.35ms
step:224/1920 train_time:7918ms step_avg:35.35ms
step:225/1920 train_time:7954ms step_avg:35.35ms
step:226/1920 train_time:7989ms step_avg:35.35ms
step:227/1920 train_time:8024ms step_avg:35.35ms
step:228/1920 train_time:8058ms step_avg:35.34ms
step:229/1920 train_time:8093ms step_avg:35.34ms
step:230/1920 train_time:8127ms step_avg:35.34ms
step:230/1920 val_loss:4.6625 train_time:8165ms step_avg:35.50ms
step:231/1920 train_time:8182ms step_avg:35.42ms
step:232/1920 train_time:8200ms step_avg:35.35ms
step:233/1920 train_time:8236ms step_avg:35.35ms
step:234/1920 train_time:8271ms step_avg:35.34ms
step:235/1920 train_time:8307ms step_avg:35.35ms
step:236/1920 train_time:8342ms step_avg:35.35ms
step:237/1920 train_time:8377ms step_avg:35.35ms
step:238/1920 train_time:8411ms step_avg:35.34ms
step:239/1920 train_time:8446ms step_avg:35.34ms
step:240/1920 train_time:8480ms step_avg:35.33ms
step:240/1920 val_loss:4.6432 train_time:8518ms step_avg:35.49ms
step:241/1920 train_time:8534ms step_avg:35.41ms
step:242/1920 train_time:8552ms step_avg:35.34ms
step:243/1920 train_time:8588ms step_avg:35.34ms
step:244/1920 train_time:8623ms step_avg:35.34ms
step:245/1920 train_time:8659ms step_avg:35.34ms
step:246/1920 train_time:8694ms step_avg:35.34ms
step:247/1920 train_time:8728ms step_avg:35.34ms
step:248/1920 train_time:8763ms step_avg:35.33ms
step:249/1920 train_time:8797ms step_avg:35.33ms
step:250/1920 train_time:8831ms step_avg:35.32ms
step:250/1920 val_loss:4.6135 train_time:8869ms step_avg:35.47ms
step:251/1920 train_time:8885ms step_avg:35.40ms
step:252/1920 train_time:8904ms step_avg:35.34ms
step:253/1920 train_time:8939ms step_avg:35.33ms
step:254/1920 train_time:8974ms step_avg:35.33ms
step:255/1920 train_time:9009ms step_avg:35.33ms
step:256/1920 train_time:9044ms step_avg:35.33ms
step:257/1920 train_time:9079ms step_avg:35.33ms
step:258/1920 train_time:9113ms step_avg:35.32ms
step:259/1920 train_time:9148ms step_avg:35.32ms
step:260/1920 train_time:9182ms step_avg:35.32ms
step:260/1920 val_loss:4.6095 train_time:9220ms step_avg:35.46ms
step:261/1920 train_time:9237ms step_avg:35.39ms
step:262/1920 train_time:9256ms step_avg:35.33ms
step:263/1920 train_time:9291ms step_avg:35.33ms
step:264/1920 train_time:9326ms step_avg:35.32ms
step:265/1920 train_time:9361ms step_avg:35.33ms
step:266/1920 train_time:9396ms step_avg:35.32ms
step:267/1920 train_time:9430ms step_avg:35.32ms
step:268/1920 train_time:9464ms step_avg:35.31ms
step:269/1920 train_time:9499ms step_avg:35.31ms
step:270/1920 train_time:9533ms step_avg:35.31ms
step:270/1920 val_loss:4.5661 train_time:9570ms step_avg:35.45ms
step:271/1920 train_time:9587ms step_avg:35.38ms
step:272/1920 train_time:9605ms step_avg:35.31ms
step:273/1920 train_time:9640ms step_avg:35.31ms
step:274/1920 train_time:9675ms step_avg:35.31ms
step:275/1920 train_time:9711ms step_avg:35.31ms
step:276/1920 train_time:9746ms step_avg:35.31ms
step:277/1920 train_time:9781ms step_avg:35.31ms
step:278/1920 train_time:9815ms step_avg:35.31ms
step:279/1920 train_time:9850ms step_avg:35.30ms
step:280/1920 train_time:9884ms step_avg:35.30ms
step:280/1920 val_loss:4.5663 train_time:9922ms step_avg:35.44ms
step:281/1920 train_time:9940ms step_avg:35.37ms
step:282/1920 train_time:9957ms step_avg:35.31ms
step:283/1920 train_time:9991ms step_avg:35.30ms
step:284/1920 train_time:10027ms step_avg:35.30ms
step:285/1920 train_time:10062ms step_avg:35.31ms
step:286/1920 train_time:10097ms step_avg:35.30ms
step:287/1920 train_time:10132ms step_avg:35.30ms
step:288/1920 train_time:10166ms step_avg:35.30ms
step:289/1920 train_time:10201ms step_avg:35.30ms
step:290/1920 train_time:10235ms step_avg:35.29ms
step:290/1920 val_loss:4.5530 train_time:10272ms step_avg:35.42ms
step:291/1920 train_time:10289ms step_avg:35.36ms
step:292/1920 train_time:10306ms step_avg:35.29ms
step:293/1920 train_time:10341ms step_avg:35.29ms
step:294/1920 train_time:10376ms step_avg:35.29ms
step:295/1920 train_time:10411ms step_avg:35.29ms
step:296/1920 train_time:10446ms step_avg:35.29ms
step:297/1920 train_time:10480ms step_avg:35.29ms
step:298/1920 train_time:10515ms step_avg:35.28ms
step:299/1920 train_time:10549ms step_avg:35.28ms
step:300/1920 train_time:10583ms step_avg:35.28ms
step:300/1920 val_loss:4.5378 train_time:10620ms step_avg:35.40ms
step:301/1920 train_time:10637ms step_avg:35.34ms
step:302/1920 train_time:10657ms step_avg:35.29ms
step:303/1920 train_time:10692ms step_avg:35.29ms
step:304/1920 train_time:10726ms step_avg:35.28ms
step:305/1920 train_time:10762ms step_avg:35.28ms
step:306/1920 train_time:10796ms step_avg:35.28ms
step:307/1920 train_time:10831ms step_avg:35.28ms
step:308/1920 train_time:10865ms step_avg:35.28ms
step:309/1920 train_time:10900ms step_avg:35.28ms
step:310/1920 train_time:10934ms step_avg:35.27ms
step:310/1920 val_loss:4.5321 train_time:10972ms step_avg:35.39ms
step:311/1920 train_time:10989ms step_avg:35.33ms
step:312/1920 train_time:11006ms step_avg:35.28ms
step:313/1920 train_time:11042ms step_avg:35.28ms
step:314/1920 train_time:11077ms step_avg:35.28ms
step:315/1920 train_time:11114ms step_avg:35.28ms
step:316/1920 train_time:11148ms step_avg:35.28ms
step:317/1920 train_time:11183ms step_avg:35.28ms
step:318/1920 train_time:11217ms step_avg:35.27ms
step:319/1920 train_time:11252ms step_avg:35.27ms
step:320/1920 train_time:11286ms step_avg:35.27ms
step:320/1920 val_loss:4.5057 train_time:11323ms step_avg:35.39ms
step:321/1920 train_time:11340ms step_avg:35.33ms
step:322/1920 train_time:11357ms step_avg:35.27ms
step:323/1920 train_time:11392ms step_avg:35.27ms
step:324/1920 train_time:11427ms step_avg:35.27ms
step:325/1920 train_time:11463ms step_avg:35.27ms
step:326/1920 train_time:11498ms step_avg:35.27ms
step:327/1920 train_time:11533ms step_avg:35.27ms
step:328/1920 train_time:11567ms step_avg:35.27ms
step:329/1920 train_time:11602ms step_avg:35.26ms
step:330/1920 train_time:11636ms step_avg:35.26ms
step:330/1920 val_loss:4.4885 train_time:11673ms step_avg:35.37ms
step:331/1920 train_time:11690ms step_avg:35.32ms
step:332/1920 train_time:11707ms step_avg:35.26ms
step:333/1920 train_time:11742ms step_avg:35.26ms
step:334/1920 train_time:11777ms step_avg:35.26ms
step:335/1920 train_time:11812ms step_avg:35.26ms
step:336/1920 train_time:11847ms step_avg:35.26ms
step:337/1920 train_time:11882ms step_avg:35.26ms
step:338/1920 train_time:11916ms step_avg:35.25ms
step:339/1920 train_time:11951ms step_avg:35.25ms
step:340/1920 train_time:11985ms step_avg:35.25ms
step:340/1920 val_loss:4.4890 train_time:12022ms step_avg:35.36ms
step:341/1920 train_time:12039ms step_avg:35.31ms
step:342/1920 train_time:12057ms step_avg:35.26ms
step:343/1920 train_time:12093ms step_avg:35.26ms
step:344/1920 train_time:12128ms step_avg:35.26ms
step:345/1920 train_time:12164ms step_avg:35.26ms
step:346/1920 train_time:12198ms step_avg:35.26ms
step:347/1920 train_time:12233ms step_avg:35.25ms
step:348/1920 train_time:12267ms step_avg:35.25ms
step:349/1920 train_time:12302ms step_avg:35.25ms
step:350/1920 train_time:12336ms step_avg:35.25ms
step:350/1920 val_loss:4.4619 train_time:12374ms step_avg:35.35ms
step:351/1920 train_time:12391ms step_avg:35.30ms
step:352/1920 train_time:12408ms step_avg:35.25ms
step:353/1920 train_time:12442ms step_avg:35.25ms
step:354/1920 train_time:12477ms step_avg:35.25ms
step:355/1920 train_time:12513ms step_avg:35.25ms
step:356/1920 train_time:12548ms step_avg:35.25ms
step:357/1920 train_time:12584ms step_avg:35.25ms
step:358/1920 train_time:12618ms step_avg:35.25ms
step:359/1920 train_time:12653ms step_avg:35.24ms
step:360/1920 train_time:12687ms step_avg:35.24ms
step:360/1920 val_loss:4.4484 train_time:12724ms step_avg:35.34ms
step:361/1920 train_time:12742ms step_avg:35.30ms
step:362/1920 train_time:12759ms step_avg:35.25ms
step:363/1920 train_time:12794ms step_avg:35.24ms
step:364/1920 train_time:12829ms step_avg:35.24ms
step:365/1920 train_time:12864ms step_avg:35.24ms
step:366/1920 train_time:12899ms step_avg:35.24ms
step:367/1920 train_time:12933ms step_avg:35.24ms
step:368/1920 train_time:12967ms step_avg:35.24ms
step:369/1920 train_time:13001ms step_avg:35.23ms
step:370/1920 train_time:13036ms step_avg:35.23ms
step:370/1920 val_loss:4.4282 train_time:13073ms step_avg:35.33ms
step:371/1920 train_time:13089ms step_avg:35.28ms
step:372/1920 train_time:13106ms step_avg:35.23ms
step:373/1920 train_time:13141ms step_avg:35.23ms
step:374/1920 train_time:13176ms step_avg:35.23ms
step:375/1920 train_time:13212ms step_avg:35.23ms
step:376/1920 train_time:13246ms step_avg:35.23ms
step:377/1920 train_time:13280ms step_avg:35.23ms
step:378/1920 train_time:13314ms step_avg:35.22ms
step:379/1920 train_time:13349ms step_avg:35.22ms
step:380/1920 train_time:13383ms step_avg:35.22ms
step:380/1920 val_loss:4.4138 train_time:13420ms step_avg:35.32ms
step:381/1920 train_time:13437ms step_avg:35.27ms
step:382/1920 train_time:13454ms step_avg:35.22ms
step:383/1920 train_time:13489ms step_avg:35.22ms
step:384/1920 train_time:13524ms step_avg:35.22ms
step:385/1920 train_time:13560ms step_avg:35.22ms
step:386/1920 train_time:13595ms step_avg:35.22ms
step:387/1920 train_time:13630ms step_avg:35.22ms
step:388/1920 train_time:13665ms step_avg:35.22ms
step:389/1920 train_time:13699ms step_avg:35.22ms
step:390/1920 train_time:13733ms step_avg:35.21ms
step:390/1920 val_loss:4.4091 train_time:13771ms step_avg:35.31ms
step:391/1920 train_time:13788ms step_avg:35.26ms
step:392/1920 train_time:13806ms step_avg:35.22ms
step:393/1920 train_time:13841ms step_avg:35.22ms
step:394/1920 train_time:13876ms step_avg:35.22ms
step:395/1920 train_time:13912ms step_avg:35.22ms
step:396/1920 train_time:13947ms step_avg:35.22ms
step:397/1920 train_time:13981ms step_avg:35.22ms
step:398/1920 train_time:14015ms step_avg:35.21ms
step:399/1920 train_time:14050ms step_avg:35.21ms
step:400/1920 train_time:14084ms step_avg:35.21ms
step:400/1920 val_loss:4.4390 train_time:14121ms step_avg:35.30ms
step:401/1920 train_time:14138ms step_avg:35.26ms
step:402/1920 train_time:14157ms step_avg:35.22ms
step:403/1920 train_time:14192ms step_avg:35.22ms
step:404/1920 train_time:14227ms step_avg:35.21ms
step:405/1920 train_time:14262ms step_avg:35.21ms
step:406/1920 train_time:14296ms step_avg:35.21ms
step:407/1920 train_time:14331ms step_avg:35.21ms
step:408/1920 train_time:14366ms step_avg:35.21ms
step:409/1920 train_time:14400ms step_avg:35.21ms
step:410/1920 train_time:14435ms step_avg:35.21ms
step:410/1920 val_loss:4.3760 train_time:14472ms step_avg:35.30ms
step:411/1920 train_time:14489ms step_avg:35.25ms
step:412/1920 train_time:14506ms step_avg:35.21ms
step:413/1920 train_time:14542ms step_avg:35.21ms
step:414/1920 train_time:14577ms step_avg:35.21ms
step:415/1920 train_time:14612ms step_avg:35.21ms
step:416/1920 train_time:14647ms step_avg:35.21ms
step:417/1920 train_time:14681ms step_avg:35.21ms
step:418/1920 train_time:14716ms step_avg:35.20ms
step:419/1920 train_time:14750ms step_avg:35.20ms
step:420/1920 train_time:14784ms step_avg:35.20ms
step:420/1920 val_loss:4.3750 train_time:14822ms step_avg:35.29ms
step:421/1920 train_time:14840ms step_avg:35.25ms
step:422/1920 train_time:14857ms step_avg:35.21ms
step:423/1920 train_time:14890ms step_avg:35.20ms
step:424/1920 train_time:14925ms step_avg:35.20ms
step:425/1920 train_time:14961ms step_avg:35.20ms
step:426/1920 train_time:14996ms step_avg:35.20ms
step:427/1920 train_time:15031ms step_avg:35.20ms
step:428/1920 train_time:15065ms step_avg:35.20ms
step:429/1920 train_time:15099ms step_avg:35.20ms
step:430/1920 train_time:15133ms step_avg:35.19ms
step:430/1920 val_loss:4.3638 train_time:15171ms step_avg:35.28ms
step:431/1920 train_time:15187ms step_avg:35.24ms
step:432/1920 train_time:15204ms step_avg:35.20ms
step:433/1920 train_time:15239ms step_avg:35.19ms
step:434/1920 train_time:15275ms step_avg:35.20ms
step:435/1920 train_time:15311ms step_avg:35.20ms
step:436/1920 train_time:15345ms step_avg:35.20ms
step:437/1920 train_time:15380ms step_avg:35.20ms
step:438/1920 train_time:15414ms step_avg:35.19ms
step:439/1920 train_time:15449ms step_avg:35.19ms
step:440/1920 train_time:15483ms step_avg:35.19ms
step:440/1920 val_loss:4.3587 train_time:15521ms step_avg:35.27ms
step:441/1920 train_time:15538ms step_avg:35.23ms
step:442/1920 train_time:15555ms step_avg:35.19ms
step:443/1920 train_time:15589ms step_avg:35.19ms
step:444/1920 train_time:15624ms step_avg:35.19ms
step:445/1920 train_time:15661ms step_avg:35.19ms
step:446/1920 train_time:15695ms step_avg:35.19ms
step:447/1920 train_time:15730ms step_avg:35.19ms
step:448/1920 train_time:15764ms step_avg:35.19ms
step:449/1920 train_time:15799ms step_avg:35.19ms
step:450/1920 train_time:15833ms step_avg:35.18ms
step:450/1920 val_loss:4.3387 train_time:15871ms step_avg:35.27ms
step:451/1920 train_time:15888ms step_avg:35.23ms
step:452/1920 train_time:15905ms step_avg:35.19ms
step:453/1920 train_time:15940ms step_avg:35.19ms
step:454/1920 train_time:15975ms step_avg:35.19ms
step:455/1920 train_time:16010ms step_avg:35.19ms
step:456/1920 train_time:16045ms step_avg:35.19ms
step:457/1920 train_time:16081ms step_avg:35.19ms
step:458/1920 train_time:16115ms step_avg:35.19ms
step:459/1920 train_time:16150ms step_avg:35.19ms
step:460/1920 train_time:16185ms step_avg:35.18ms
step:460/1920 val_loss:4.3279 train_time:16222ms step_avg:35.27ms
step:461/1920 train_time:16239ms step_avg:35.23ms
step:462/1920 train_time:16258ms step_avg:35.19ms
step:463/1920 train_time:16293ms step_avg:35.19ms
step:464/1920 train_time:16328ms step_avg:35.19ms
step:465/1920 train_time:16364ms step_avg:35.19ms
step:466/1920 train_time:16399ms step_avg:35.19ms
step:467/1920 train_time:16434ms step_avg:35.19ms
step:468/1920 train_time:16468ms step_avg:35.19ms
step:469/1920 train_time:16502ms step_avg:35.19ms
step:470/1920 train_time:16536ms step_avg:35.18ms
step:470/1920 val_loss:4.3157 train_time:16574ms step_avg:35.26ms
step:471/1920 train_time:16591ms step_avg:35.23ms
step:472/1920 train_time:16609ms step_avg:35.19ms
step:473/1920 train_time:16645ms step_avg:35.19ms
step:474/1920 train_time:16679ms step_avg:35.19ms
step:475/1920 train_time:16714ms step_avg:35.19ms
step:476/1920 train_time:16749ms step_avg:35.19ms
step:477/1920 train_time:16785ms step_avg:35.19ms
step:478/1920 train_time:16819ms step_avg:35.19ms
step:479/1920 train_time:16853ms step_avg:35.18ms
step:480/1920 train_time:16888ms step_avg:35.18ms
step:480/1920 val_loss:4.3084 train_time:16925ms step_avg:35.26ms
step:481/1920 train_time:16942ms step_avg:35.22ms
step:482/1920 train_time:16961ms step_avg:35.19ms
step:483/1920 train_time:16996ms step_avg:35.19ms
step:484/1920 train_time:17031ms step_avg:35.19ms
step:485/1920 train_time:17066ms step_avg:35.19ms
step:486/1920 train_time:17101ms step_avg:35.19ms
step:487/1920 train_time:17136ms step_avg:35.19ms
step:488/1920 train_time:17170ms step_avg:35.18ms
step:489/1920 train_time:17204ms step_avg:35.18ms
step:490/1920 train_time:17238ms step_avg:35.18ms
step:490/1920 val_loss:4.3014 train_time:17275ms step_avg:35.26ms
step:491/1920 train_time:17292ms step_avg:35.22ms
step:492/1920 train_time:17310ms step_avg:35.18ms
step:493/1920 train_time:17345ms step_avg:35.18ms
step:494/1920 train_time:17379ms step_avg:35.18ms
step:495/1920 train_time:17415ms step_avg:35.18ms
step:496/1920 train_time:17450ms step_avg:35.18ms
step:497/1920 train_time:17484ms step_avg:35.18ms
step:498/1920 train_time:17519ms step_avg:35.18ms
step:499/1920 train_time:17553ms step_avg:35.18ms
step:500/1920 train_time:17587ms step_avg:35.17ms
step:500/1920 val_loss:4.2882 train_time:17625ms step_avg:35.25ms
step:501/1920 train_time:17642ms step_avg:35.21ms
step:502/1920 train_time:17659ms step_avg:35.18ms
step:503/1920 train_time:17693ms step_avg:35.18ms
step:504/1920 train_time:17728ms step_avg:35.18ms
step:505/1920 train_time:17764ms step_avg:35.18ms
step:506/1920 train_time:17799ms step_avg:35.18ms
step:507/1920 train_time:17833ms step_avg:35.17ms
step:508/1920 train_time:17867ms step_avg:35.17ms
step:509/1920 train_time:17902ms step_avg:35.17ms
step:510/1920 train_time:17936ms step_avg:35.17ms
step:510/1920 val_loss:4.2828 train_time:17974ms step_avg:35.24ms
step:511/1920 train_time:17990ms step_avg:35.21ms
step:512/1920 train_time:18008ms step_avg:35.17ms
step:513/1920 train_time:18043ms step_avg:35.17ms
step:514/1920 train_time:18078ms step_avg:35.17ms
step:515/1920 train_time:18113ms step_avg:35.17ms
step:516/1920 train_time:18148ms step_avg:35.17ms
step:517/1920 train_time:18183ms step_avg:35.17ms
step:518/1920 train_time:18217ms step_avg:35.17ms
step:519/1920 train_time:18251ms step_avg:35.17ms
step:520/1920 train_time:18285ms step_avg:35.16ms
step:520/1920 val_loss:4.2705 train_time:18323ms step_avg:35.24ms
step:521/1920 train_time:18339ms step_avg:35.20ms
step:522/1920 train_time:18356ms step_avg:35.17ms
step:523/1920 train_time:18391ms step_avg:35.16ms
step:524/1920 train_time:18425ms step_avg:35.16ms
step:525/1920 train_time:18461ms step_avg:35.16ms
step:526/1920 train_time:18495ms step_avg:35.16ms
step:527/1920 train_time:18530ms step_avg:35.16ms
step:528/1920 train_time:18564ms step_avg:35.16ms
step:529/1920 train_time:18598ms step_avg:35.16ms
step:530/1920 train_time:18633ms step_avg:35.16ms
step:530/1920 val_loss:4.2733 train_time:18670ms step_avg:35.23ms
step:531/1920 train_time:18687ms step_avg:35.19ms
step:532/1920 train_time:18704ms step_avg:35.16ms
step:533/1920 train_time:18738ms step_avg:35.16ms
step:534/1920 train_time:18773ms step_avg:35.16ms
step:535/1920 train_time:18809ms step_avg:35.16ms
step:536/1920 train_time:18844ms step_avg:35.16ms
step:537/1920 train_time:18879ms step_avg:35.16ms
step:538/1920 train_time:18913ms step_avg:35.15ms
step:539/1920 train_time:18948ms step_avg:35.15ms
step:540/1920 train_time:18982ms step_avg:35.15ms
step:540/1920 val_loss:4.2554 train_time:19019ms step_avg:35.22ms
step:541/1920 train_time:19036ms step_avg:35.19ms
step:542/1920 train_time:19053ms step_avg:35.15ms
step:543/1920 train_time:19088ms step_avg:35.15ms
step:544/1920 train_time:19123ms step_avg:35.15ms
step:545/1920 train_time:19158ms step_avg:35.15ms
step:546/1920 train_time:19193ms step_avg:35.15ms
step:547/1920 train_time:19228ms step_avg:35.15ms
step:548/1920 train_time:19262ms step_avg:35.15ms
step:549/1920 train_time:19297ms step_avg:35.15ms
step:550/1920 train_time:19331ms step_avg:35.15ms
step:550/1920 val_loss:4.2455 train_time:19369ms step_avg:35.22ms
step:551/1920 train_time:19386ms step_avg:35.18ms
step:552/1920 train_time:19403ms step_avg:35.15ms
step:553/1920 train_time:19438ms step_avg:35.15ms
step:554/1920 train_time:19473ms step_avg:35.15ms
step:555/1920 train_time:19509ms step_avg:35.15ms
step:556/1920 train_time:19543ms step_avg:35.15ms
step:557/1920 train_time:19578ms step_avg:35.15ms
step:558/1920 train_time:19612ms step_avg:35.15ms
step:559/1920 train_time:19647ms step_avg:35.15ms
step:560/1920 train_time:19681ms step_avg:35.15ms
step:560/1920 val_loss:4.2392 train_time:19719ms step_avg:35.21ms
step:561/1920 train_time:19736ms step_avg:35.18ms
step:562/1920 train_time:19753ms step_avg:35.15ms
step:563/1920 train_time:19789ms step_avg:35.15ms
step:564/1920 train_time:19824ms step_avg:35.15ms
step:565/1920 train_time:19860ms step_avg:35.15ms
step:566/1920 train_time:19895ms step_avg:35.15ms
step:567/1920 train_time:19930ms step_avg:35.15ms
step:568/1920 train_time:19964ms step_avg:35.15ms
step:569/1920 train_time:19999ms step_avg:35.15ms
step:570/1920 train_time:20033ms step_avg:35.15ms
step:570/1920 val_loss:4.2374 train_time:20071ms step_avg:35.21ms
step:571/1920 train_time:20088ms step_avg:35.18ms
step:572/1920 train_time:20105ms step_avg:35.15ms
step:573/1920 train_time:20139ms step_avg:35.15ms
step:574/1920 train_time:20174ms step_avg:35.15ms
step:575/1920 train_time:20210ms step_avg:35.15ms
step:576/1920 train_time:20245ms step_avg:35.15ms
step:577/1920 train_time:20280ms step_avg:35.15ms
step:578/1920 train_time:20314ms step_avg:35.15ms
step:579/1920 train_time:20350ms step_avg:35.15ms
step:580/1920 train_time:20384ms step_avg:35.14ms
step:580/1920 val_loss:4.2176 train_time:20421ms step_avg:35.21ms
step:581/1920 train_time:20438ms step_avg:35.18ms
step:582/1920 train_time:20455ms step_avg:35.15ms
step:583/1920 train_time:20489ms step_avg:35.14ms
step:584/1920 train_time:20524ms step_avg:35.14ms
step:585/1920 train_time:20561ms step_avg:35.15ms
step:586/1920 train_time:20595ms step_avg:35.15ms
step:587/1920 train_time:20630ms step_avg:35.15ms
step:588/1920 train_time:20664ms step_avg:35.14ms
step:589/1920 train_time:20699ms step_avg:35.14ms
step:590/1920 train_time:20733ms step_avg:35.14ms
step:590/1920 val_loss:4.2213 train_time:20771ms step_avg:35.20ms
step:591/1920 train_time:20788ms step_avg:35.17ms
step:592/1920 train_time:20807ms step_avg:35.15ms
step:593/1920 train_time:20841ms step_avg:35.15ms
step:594/1920 train_time:20876ms step_avg:35.14ms
step:595/1920 train_time:20911ms step_avg:35.15ms
step:596/1920 train_time:20946ms step_avg:35.14ms
step:597/1920 train_time:20981ms step_avg:35.14ms
step:598/1920 train_time:21015ms step_avg:35.14ms
step:599/1920 train_time:21049ms step_avg:35.14ms
step:600/1920 train_time:21083ms step_avg:35.14ms
step:600/1920 val_loss:4.2178 train_time:21121ms step_avg:35.20ms
step:601/1920 train_time:21138ms step_avg:35.17ms
step:602/1920 train_time:21155ms step_avg:35.14ms
step:603/1920 train_time:21189ms step_avg:35.14ms
step:604/1920 train_time:21224ms step_avg:35.14ms
step:605/1920 train_time:21260ms step_avg:35.14ms
step:606/1920 train_time:21294ms step_avg:35.14ms
step:607/1920 train_time:21329ms step_avg:35.14ms
step:608/1920 train_time:21363ms step_avg:35.14ms
step:609/1920 train_time:21398ms step_avg:35.14ms
step:610/1920 train_time:21432ms step_avg:35.13ms
step:610/1920 val_loss:4.2026 train_time:21469ms step_avg:35.20ms
step:611/1920 train_time:21486ms step_avg:35.17ms
step:612/1920 train_time:21503ms step_avg:35.14ms
step:613/1920 train_time:21537ms step_avg:35.13ms
step:614/1920 train_time:21572ms step_avg:35.13ms
step:615/1920 train_time:21607ms step_avg:35.13ms
step:616/1920 train_time:21642ms step_avg:35.13ms
step:617/1920 train_time:21677ms step_avg:35.13ms
step:618/1920 train_time:21711ms step_avg:35.13ms
step:619/1920 train_time:21745ms step_avg:35.13ms
step:620/1920 train_time:21779ms step_avg:35.13ms
step:620/1920 val_loss:4.2003 train_time:21817ms step_avg:35.19ms
step:621/1920 train_time:21834ms step_avg:35.16ms
step:622/1920 train_time:21851ms step_avg:35.13ms
step:623/1920 train_time:21886ms step_avg:35.13ms
step:624/1920 train_time:21921ms step_avg:35.13ms
step:625/1920 train_time:21955ms step_avg:35.13ms
step:626/1920 train_time:21990ms step_avg:35.13ms
step:627/1920 train_time:22025ms step_avg:35.13ms
step:628/1920 train_time:22060ms step_avg:35.13ms
step:629/1920 train_time:22123ms step_avg:35.17ms
step:630/1920 train_time:22184ms step_avg:35.21ms
step:630/1920 val_loss:4.2141 train_time:22249ms step_avg:35.32ms
step:631/1920 train_time:22267ms step_avg:35.29ms
step:632/1920 train_time:22309ms step_avg:35.30ms
step:633/1920 train_time:22378ms step_avg:35.35ms
step:634/1920 train_time:22443ms step_avg:35.40ms
step:635/1920 train_time:22508ms step_avg:35.45ms
step:636/1920 train_time:22569ms step_avg:35.49ms
step:637/1920 train_time:22631ms step_avg:35.53ms
step:638/1920 train_time:22692ms step_avg:35.57ms
step:639/1920 train_time:22754ms step_avg:35.61ms
step:640/1920 train_time:22816ms step_avg:35.65ms
step:640/1920 val_loss:4.1740 train_time:22880ms step_avg:35.75ms
step:641/1920 train_time:22898ms step_avg:35.72ms
step:642/1920 train_time:22941ms step_avg:35.73ms
step:643/1920 train_time:23007ms step_avg:35.78ms
step:644/1920 train_time:23070ms step_avg:35.82ms
step:645/1920 train_time:23132ms step_avg:35.86ms
step:646/1920 train_time:23194ms step_avg:35.90ms
step:647/1920 train_time:23255ms step_avg:35.94ms
step:648/1920 train_time:23317ms step_avg:35.98ms
step:649/1920 train_time:23379ms step_avg:36.02ms
step:650/1920 train_time:23440ms step_avg:36.06ms
step:650/1920 val_loss:4.1851 train_time:23505ms step_avg:36.16ms
step:651/1920 train_time:23523ms step_avg:36.13ms
step:652/1920 train_time:23565ms step_avg:36.14ms
step:653/1920 train_time:23630ms step_avg:36.19ms
step:654/1920 train_time:23693ms step_avg:36.23ms
step:655/1920 train_time:23756ms step_avg:36.27ms
step:656/1920 train_time:23817ms step_avg:36.31ms
step:657/1920 train_time:23880ms step_avg:36.35ms
step:658/1920 train_time:23941ms step_avg:36.38ms
step:659/1920 train_time:24004ms step_avg:36.42ms
step:660/1920 train_time:24065ms step_avg:36.46ms
step:660/1920 val_loss:4.1537 train_time:24131ms step_avg:36.56ms
step:661/1920 train_time:24149ms step_avg:36.53ms
step:662/1920 train_time:24193ms step_avg:36.55ms
step:663/1920 train_time:24259ms step_avg:36.59ms
step:664/1920 train_time:24322ms step_avg:36.63ms
step:665/1920 train_time:24385ms step_avg:36.67ms
step:666/1920 train_time:24447ms step_avg:36.71ms
step:667/1920 train_time:24509ms step_avg:36.75ms
step:668/1920 train_time:24571ms step_avg:36.78ms
step:669/1920 train_time:24633ms step_avg:36.82ms
step:670/1920 train_time:24694ms step_avg:36.86ms
step:670/1920 val_loss:4.3423 train_time:24759ms step_avg:36.95ms
step:671/1920 train_time:24776ms step_avg:36.92ms
step:672/1920 train_time:24819ms step_avg:36.93ms
step:673/1920 train_time:24884ms step_avg:36.97ms
step:674/1920 train_time:24947ms step_avg:37.01ms
step:675/1920 train_time:25010ms step_avg:37.05ms
step:676/1920 train_time:25072ms step_avg:37.09ms
step:677/1920 train_time:25134ms step_avg:37.13ms
step:678/1920 train_time:25196ms step_avg:37.16ms
step:679/1920 train_time:25258ms step_avg:37.20ms
step:680/1920 train_time:25319ms step_avg:37.23ms
step:680/1920 val_loss:4.1604 train_time:25384ms step_avg:37.33ms
step:681/1920 train_time:25401ms step_avg:37.30ms
step:682/1920 train_time:25446ms step_avg:37.31ms
step:683/1920 train_time:25510ms step_avg:37.35ms
step:684/1920 train_time:25572ms step_avg:37.39ms
step:685/1920 train_time:25635ms step_avg:37.42ms
step:686/1920 train_time:25697ms step_avg:37.46ms
step:687/1920 train_time:25759ms step_avg:37.49ms
step:688/1920 train_time:25820ms step_avg:37.53ms
step:689/1920 train_time:25883ms step_avg:37.57ms
step:690/1920 train_time:25944ms step_avg:37.60ms
step:690/1920 val_loss:4.1137 train_time:26009ms step_avg:37.69ms
step:691/1920 train_time:26028ms step_avg:37.67ms
step:692/1920 train_time:26070ms step_avg:37.67ms
step:693/1920 train_time:26134ms step_avg:37.71ms
step:694/1920 train_time:26198ms step_avg:37.75ms
step:695/1920 train_time:26261ms step_avg:37.78ms
step:696/1920 train_time:26322ms step_avg:37.82ms
step:697/1920 train_time:26385ms step_avg:37.85ms
step:698/1920 train_time:26447ms step_avg:37.89ms
step:699/1920 train_time:26510ms step_avg:37.93ms
step:700/1920 train_time:26572ms step_avg:37.96ms
step:700/1920 val_loss:4.0995 train_time:26637ms step_avg:38.05ms
step:701/1920 train_time:26654ms step_avg:38.02ms
step:702/1920 train_time:26698ms step_avg:38.03ms
step:703/1920 train_time:26762ms step_avg:38.07ms
step:704/1920 train_time:26825ms step_avg:38.10ms
step:705/1920 train_time:26888ms step_avg:38.14ms
step:706/1920 train_time:26950ms step_avg:38.17ms
step:707/1920 train_time:27012ms step_avg:38.21ms
step:708/1920 train_time:27073ms step_avg:38.24ms
step:709/1920 train_time:27136ms step_avg:38.27ms
step:710/1920 train_time:27197ms step_avg:38.31ms
step:710/1920 val_loss:4.1051 train_time:27262ms step_avg:38.40ms
step:711/1920 train_time:27280ms step_avg:38.37ms
step:712/1920 train_time:27323ms step_avg:38.38ms
step:713/1920 train_time:27389ms step_avg:38.41ms
step:714/1920 train_time:27451ms step_avg:38.45ms
step:715/1920 train_time:27514ms step_avg:38.48ms
step:716/1920 train_time:27575ms step_avg:38.51ms
step:717/1920 train_time:27638ms step_avg:38.55ms
step:718/1920 train_time:27699ms step_avg:38.58ms
step:719/1920 train_time:27761ms step_avg:38.61ms
step:720/1920 train_time:27824ms step_avg:38.64ms
step:720/1920 val_loss:4.0691 train_time:27889ms step_avg:38.74ms
step:721/1920 train_time:27907ms step_avg:38.71ms
step:722/1920 train_time:27951ms step_avg:38.71ms
step:723/1920 train_time:28017ms step_avg:38.75ms
step:724/1920 train_time:28079ms step_avg:38.78ms
step:725/1920 train_time:28142ms step_avg:38.82ms
step:726/1920 train_time:28204ms step_avg:38.85ms
step:727/1920 train_time:28267ms step_avg:38.88ms
step:728/1920 train_time:28328ms step_avg:38.91ms
step:729/1920 train_time:28391ms step_avg:38.94ms
step:730/1920 train_time:28452ms step_avg:38.98ms
step:730/1920 val_loss:4.0528 train_time:28518ms step_avg:39.07ms
step:731/1920 train_time:28535ms step_avg:39.04ms
step:732/1920 train_time:28579ms step_avg:39.04ms
step:733/1920 train_time:28644ms step_avg:39.08ms
step:734/1920 train_time:28708ms step_avg:39.11ms
step:735/1920 train_time:28771ms step_avg:39.14ms
step:736/1920 train_time:28833ms step_avg:39.18ms
step:737/1920 train_time:28896ms step_avg:39.21ms
step:738/1920 train_time:28958ms step_avg:39.24ms
step:739/1920 train_time:29020ms step_avg:39.27ms
step:740/1920 train_time:29081ms step_avg:39.30ms
step:740/1920 val_loss:4.0416 train_time:29146ms step_avg:39.39ms
step:741/1920 train_time:29163ms step_avg:39.36ms
step:742/1920 train_time:29208ms step_avg:39.36ms
step:743/1920 train_time:29273ms step_avg:39.40ms
step:744/1920 train_time:29338ms step_avg:39.43ms
step:745/1920 train_time:29400ms step_avg:39.46ms
step:746/1920 train_time:29461ms step_avg:39.49ms
step:747/1920 train_time:29524ms step_avg:39.52ms
step:748/1920 train_time:29585ms step_avg:39.55ms
step:749/1920 train_time:29647ms step_avg:39.58ms
step:750/1920 train_time:29709ms step_avg:39.61ms
step:750/1920 val_loss:4.0299 train_time:29775ms step_avg:39.70ms
step:751/1920 train_time:29793ms step_avg:39.67ms
step:752/1920 train_time:29836ms step_avg:39.68ms
step:753/1920 train_time:29901ms step_avg:39.71ms
step:754/1920 train_time:29964ms step_avg:39.74ms
step:755/1920 train_time:30027ms step_avg:39.77ms
step:756/1920 train_time:30090ms step_avg:39.80ms
step:757/1920 train_time:30152ms step_avg:39.83ms
step:758/1920 train_time:30213ms step_avg:39.86ms
step:759/1920 train_time:30275ms step_avg:39.89ms
step:760/1920 train_time:30337ms step_avg:39.92ms
step:760/1920 val_loss:4.0105 train_time:30403ms step_avg:40.00ms
step:761/1920 train_time:30420ms step_avg:39.97ms
step:762/1920 train_time:30463ms step_avg:39.98ms
step:763/1920 train_time:30527ms step_avg:40.01ms
step:764/1920 train_time:30590ms step_avg:40.04ms
step:765/1920 train_time:30653ms step_avg:40.07ms
step:766/1920 train_time:30715ms step_avg:40.10ms
step:767/1920 train_time:30777ms step_avg:40.13ms
step:768/1920 train_time:30838ms step_avg:40.15ms
step:769/1920 train_time:30901ms step_avg:40.18ms
step:770/1920 train_time:30962ms step_avg:40.21ms
step:770/1920 val_loss:4.0262 train_time:31028ms step_avg:40.30ms
step:771/1920 train_time:31046ms step_avg:40.27ms
step:772/1920 train_time:31089ms step_avg:40.27ms
step:773/1920 train_time:31155ms step_avg:40.30ms
step:774/1920 train_time:31217ms step_avg:40.33ms
step:775/1920 train_time:31280ms step_avg:40.36ms
step:776/1920 train_time:31342ms step_avg:40.39ms
step:777/1920 train_time:31405ms step_avg:40.42ms
step:778/1920 train_time:31467ms step_avg:40.45ms
step:779/1920 train_time:31530ms step_avg:40.47ms
step:780/1920 train_time:31591ms step_avg:40.50ms
step:780/1920 val_loss:4.0123 train_time:31657ms step_avg:40.59ms
step:781/1920 train_time:31674ms step_avg:40.56ms
step:782/1920 train_time:31718ms step_avg:40.56ms
step:783/1920 train_time:31783ms step_avg:40.59ms
step:784/1920 train_time:31845ms step_avg:40.62ms
step:785/1920 train_time:31908ms step_avg:40.65ms
step:786/1920 train_time:31970ms step_avg:40.67ms
step:787/1920 train_time:32032ms step_avg:40.70ms
step:788/1920 train_time:32094ms step_avg:40.73ms
step:789/1920 train_time:32157ms step_avg:40.76ms
step:790/1920 train_time:32218ms step_avg:40.78ms
step:790/1920 val_loss:3.9910 train_time:32283ms step_avg:40.86ms
step:791/1920 train_time:32300ms step_avg:40.83ms
step:792/1920 train_time:32344ms step_avg:40.84ms
step:793/1920 train_time:32409ms step_avg:40.87ms
step:794/1920 train_time:32471ms step_avg:40.90ms
step:795/1920 train_time:32533ms step_avg:40.92ms
step:796/1920 train_time:32595ms step_avg:40.95ms
step:797/1920 train_time:32658ms step_avg:40.98ms
step:798/1920 train_time:32718ms step_avg:41.00ms
step:799/1920 train_time:32781ms step_avg:41.03ms
step:800/1920 train_time:32842ms step_avg:41.05ms
step:800/1920 val_loss:3.9882 train_time:32907ms step_avg:41.13ms
step:801/1920 train_time:32924ms step_avg:41.10ms
step:802/1920 train_time:32968ms step_avg:41.11ms
step:803/1920 train_time:33033ms step_avg:41.14ms
step:804/1920 train_time:33095ms step_avg:41.16ms
step:805/1920 train_time:33158ms step_avg:41.19ms
step:806/1920 train_time:33220ms step_avg:41.22ms
step:807/1920 train_time:33282ms step_avg:41.24ms
step:808/1920 train_time:33344ms step_avg:41.27ms
step:809/1920 train_time:33407ms step_avg:41.29ms
step:810/1920 train_time:33468ms step_avg:41.32ms
step:810/1920 val_loss:3.9790 train_time:33533ms step_avg:41.40ms
step:811/1920 train_time:33551ms step_avg:41.37ms
step:812/1920 train_time:33595ms step_avg:41.37ms
step:813/1920 train_time:33660ms step_avg:41.40ms
step:814/1920 train_time:33723ms step_avg:41.43ms
step:815/1920 train_time:33785ms step_avg:41.45ms
step:816/1920 train_time:33847ms step_avg:41.48ms
step:817/1920 train_time:33909ms step_avg:41.50ms
step:818/1920 train_time:33970ms step_avg:41.53ms
step:819/1920 train_time:34033ms step_avg:41.55ms
step:820/1920 train_time:34095ms step_avg:41.58ms
step:820/1920 val_loss:3.9600 train_time:34160ms step_avg:41.66ms
step:821/1920 train_time:34178ms step_avg:41.63ms
step:822/1920 train_time:34222ms step_avg:41.63ms
step:823/1920 train_time:34287ms step_avg:41.66ms
step:824/1920 train_time:34350ms step_avg:41.69ms
step:825/1920 train_time:34414ms step_avg:41.71ms
step:826/1920 train_time:34475ms step_avg:41.74ms
step:827/1920 train_time:34537ms step_avg:41.76ms
step:828/1920 train_time:34599ms step_avg:41.79ms
step:829/1920 train_time:34661ms step_avg:41.81ms
step:830/1920 train_time:34722ms step_avg:41.83ms
step:830/1920 val_loss:3.9546 train_time:34788ms step_avg:41.91ms
step:831/1920 train_time:34805ms step_avg:41.88ms
step:832/1920 train_time:34848ms step_avg:41.88ms
step:833/1920 train_time:34914ms step_avg:41.91ms
step:834/1920 train_time:34976ms step_avg:41.94ms
step:835/1920 train_time:35039ms step_avg:41.96ms
step:836/1920 train_time:35102ms step_avg:41.99ms
step:837/1920 train_time:35165ms step_avg:42.01ms
step:838/1920 train_time:35226ms step_avg:42.04ms
step:839/1920 train_time:35288ms step_avg:42.06ms
step:840/1920 train_time:35349ms step_avg:42.08ms
step:840/1920 val_loss:3.9567 train_time:35414ms step_avg:42.16ms
step:841/1920 train_time:35432ms step_avg:42.13ms
step:842/1920 train_time:35478ms step_avg:42.14ms
step:843/1920 train_time:35541ms step_avg:42.16ms
step:844/1920 train_time:35604ms step_avg:42.18ms
step:845/1920 train_time:35667ms step_avg:42.21ms
step:846/1920 train_time:35729ms step_avg:42.23ms
step:847/1920 train_time:35791ms step_avg:42.26ms
step:848/1920 train_time:35853ms step_avg:42.28ms
step:849/1920 train_time:35916ms step_avg:42.30ms
step:850/1920 train_time:35977ms step_avg:42.33ms
step:850/1920 val_loss:3.9300 train_time:36043ms step_avg:42.40ms
step:851/1920 train_time:36060ms step_avg:42.37ms
step:852/1920 train_time:36103ms step_avg:42.37ms
step:853/1920 train_time:36168ms step_avg:42.40ms
step:854/1920 train_time:36230ms step_avg:42.42ms
step:855/1920 train_time:36293ms step_avg:42.45ms
step:856/1920 train_time:36355ms step_avg:42.47ms
step:857/1920 train_time:36417ms step_avg:42.49ms
step:858/1920 train_time:36480ms step_avg:42.52ms
step:859/1920 train_time:36542ms step_avg:42.54ms
step:860/1920 train_time:36604ms step_avg:42.56ms
step:860/1920 val_loss:3.9104 train_time:36669ms step_avg:42.64ms
step:861/1920 train_time:36686ms step_avg:42.61ms
step:862/1920 train_time:36730ms step_avg:42.61ms
step:863/1920 train_time:36795ms step_avg:42.64ms
step:864/1920 train_time:36859ms step_avg:42.66ms
step:865/1920 train_time:36922ms step_avg:42.68ms
step:866/1920 train_time:36983ms step_avg:42.71ms
step:867/1920 train_time:37045ms step_avg:42.73ms
step:868/1920 train_time:37107ms step_avg:42.75ms
step:869/1920 train_time:37170ms step_avg:42.77ms
step:870/1920 train_time:37231ms step_avg:42.79ms
step:870/1920 val_loss:3.9022 train_time:37296ms step_avg:42.87ms
step:871/1920 train_time:37314ms step_avg:42.84ms
step:872/1920 train_time:37357ms step_avg:42.84ms
step:873/1920 train_time:37423ms step_avg:42.87ms
step:874/1920 train_time:37486ms step_avg:42.89ms
step:875/1920 train_time:37549ms step_avg:42.91ms
step:876/1920 train_time:37612ms step_avg:42.94ms
step:877/1920 train_time:37674ms step_avg:42.96ms
step:878/1920 train_time:37735ms step_avg:42.98ms
step:879/1920 train_time:37797ms step_avg:43.00ms
step:880/1920 train_time:37858ms step_avg:43.02ms
step:880/1920 val_loss:3.8938 train_time:37924ms step_avg:43.10ms
step:881/1920 train_time:37941ms step_avg:43.07ms
step:882/1920 train_time:37984ms step_avg:43.07ms
step:883/1920 train_time:38048ms step_avg:43.09ms
step:884/1920 train_time:38110ms step_avg:43.11ms
step:885/1920 train_time:38173ms step_avg:43.13ms
step:886/1920 train_time:38235ms step_avg:43.15ms
step:887/1920 train_time:38298ms step_avg:43.18ms
step:888/1920 train_time:38359ms step_avg:43.20ms
step:889/1920 train_time:38422ms step_avg:43.22ms
step:890/1920 train_time:38483ms step_avg:43.24ms
step:890/1920 val_loss:3.8863 train_time:38548ms step_avg:43.31ms
step:891/1920 train_time:38565ms step_avg:43.28ms
step:892/1920 train_time:38609ms step_avg:43.28ms
step:893/1920 train_time:38674ms step_avg:43.31ms
step:894/1920 train_time:38737ms step_avg:43.33ms
step:895/1920 train_time:38799ms step_avg:43.35ms
step:896/1920 train_time:38861ms step_avg:43.37ms
step:897/1920 train_time:38923ms step_avg:43.39ms
step:898/1920 train_time:38986ms step_avg:43.41ms
step:899/1920 train_time:39048ms step_avg:43.44ms
step:900/1920 train_time:39109ms step_avg:43.45ms
step:900/1920 val_loss:3.8849 train_time:39175ms step_avg:43.53ms
step:901/1920 train_time:39192ms step_avg:43.50ms
step:902/1920 train_time:39238ms step_avg:43.50ms
step:903/1920 train_time:39303ms step_avg:43.52ms
step:904/1920 train_time:39365ms step_avg:43.55ms
step:905/1920 train_time:39429ms step_avg:43.57ms
step:906/1920 train_time:39492ms step_avg:43.59ms
step:907/1920 train_time:39554ms step_avg:43.61ms
step:908/1920 train_time:39616ms step_avg:43.63ms
step:909/1920 train_time:39678ms step_avg:43.65ms
step:910/1920 train_time:39739ms step_avg:43.67ms
step:910/1920 val_loss:3.8701 train_time:39805ms step_avg:43.74ms
step:911/1920 train_time:39822ms step_avg:43.71ms
step:912/1920 train_time:39865ms step_avg:43.71ms
step:913/1920 train_time:39930ms step_avg:43.73ms
step:914/1920 train_time:39992ms step_avg:43.75ms
step:915/1920 train_time:40055ms step_avg:43.78ms
step:916/1920 train_time:40116ms step_avg:43.79ms
step:917/1920 train_time:40178ms step_avg:43.81ms
step:918/1920 train_time:40240ms step_avg:43.83ms
step:919/1920 train_time:40302ms step_avg:43.85ms
step:920/1920 train_time:40364ms step_avg:43.87ms
step:920/1920 val_loss:3.8577 train_time:40429ms step_avg:43.94ms
step:921/1920 train_time:40446ms step_avg:43.92ms
step:922/1920 train_time:40491ms step_avg:43.92ms
step:923/1920 train_time:40556ms step_avg:43.94ms
step:924/1920 train_time:40618ms step_avg:43.96ms
step:925/1920 train_time:40681ms step_avg:43.98ms
step:926/1920 train_time:40744ms step_avg:44.00ms
step:927/1920 train_time:40806ms step_avg:44.02ms
step:928/1920 train_time:40868ms step_avg:44.04ms
step:929/1920 train_time:40931ms step_avg:44.06ms
step:930/1920 train_time:40992ms step_avg:44.08ms
step:930/1920 val_loss:3.8485 train_time:41058ms step_avg:44.15ms
step:931/1920 train_time:41076ms step_avg:44.12ms
step:932/1920 train_time:41118ms step_avg:44.12ms
step:933/1920 train_time:41183ms step_avg:44.14ms
step:934/1920 train_time:41245ms step_avg:44.16ms
step:935/1920 train_time:41308ms step_avg:44.18ms
step:936/1920 train_time:41370ms step_avg:44.20ms
step:937/1920 train_time:41432ms step_avg:44.22ms
step:938/1920 train_time:41493ms step_avg:44.24ms
step:939/1920 train_time:41555ms step_avg:44.25ms
step:940/1920 train_time:41617ms step_avg:44.27ms
step:940/1920 val_loss:3.8290 train_time:41682ms step_avg:44.34ms
step:941/1920 train_time:41699ms step_avg:44.31ms
step:942/1920 train_time:41743ms step_avg:44.31ms
step:943/1920 train_time:41808ms step_avg:44.34ms
step:944/1920 train_time:41870ms step_avg:44.35ms
step:945/1920 train_time:41933ms step_avg:44.37ms
step:946/1920 train_time:41996ms step_avg:44.39ms
step:947/1920 train_time:42058ms step_avg:44.41ms
step:948/1920 train_time:42120ms step_avg:44.43ms
step:949/1920 train_time:42182ms step_avg:44.45ms
step:950/1920 train_time:42244ms step_avg:44.47ms
step:950/1920 val_loss:3.8295 train_time:42309ms step_avg:44.54ms
step:951/1920 train_time:42327ms step_avg:44.51ms
step:952/1920 train_time:42373ms step_avg:44.51ms
step:953/1920 train_time:42437ms step_avg:44.53ms
step:954/1920 train_time:42499ms step_avg:44.55ms
step:955/1920 train_time:42561ms step_avg:44.57ms
step:956/1920 train_time:42623ms step_avg:44.58ms
step:957/1920 train_time:42685ms step_avg:44.60ms
step:958/1920 train_time:42747ms step_avg:44.62ms
step:959/1920 train_time:42809ms step_avg:44.64ms
step:960/1920 train_time:42872ms step_avg:44.66ms
step:960/1920 val_loss:3.8174 train_time:42937ms step_avg:44.73ms
step:961/1920 train_time:42955ms step_avg:44.70ms
step:962/1920 train_time:42999ms step_avg:44.70ms
step:963/1920 train_time:43065ms step_avg:44.72ms
step:964/1920 train_time:43127ms step_avg:44.74ms
step:965/1920 train_time:43190ms step_avg:44.76ms
step:966/1920 train_time:43251ms step_avg:44.77ms
step:967/1920 train_time:43314ms step_avg:44.79ms
step:968/1920 train_time:43375ms step_avg:44.81ms
step:969/1920 train_time:43437ms step_avg:44.83ms
step:970/1920 train_time:43499ms step_avg:44.84ms
step:970/1920 val_loss:3.8094 train_time:43566ms step_avg:44.91ms
step:971/1920 train_time:43583ms step_avg:44.88ms
step:972/1920 train_time:43629ms step_avg:44.89ms
step:973/1920 train_time:43695ms step_avg:44.91ms
step:974/1920 train_time:43757ms step_avg:44.93ms
step:975/1920 train_time:43821ms step_avg:44.94ms
step:976/1920 train_time:43884ms step_avg:44.96ms
step:977/1920 train_time:43946ms step_avg:44.98ms
step:978/1920 train_time:44007ms step_avg:45.00ms
step:979/1920 train_time:44070ms step_avg:45.01ms
step:980/1920 train_time:44131ms step_avg:45.03ms
step:980/1920 val_loss:3.7879 train_time:44196ms step_avg:45.10ms
step:981/1920 train_time:44213ms step_avg:45.07ms
step:982/1920 train_time:44257ms step_avg:45.07ms
step:983/1920 train_time:44322ms step_avg:45.09ms
step:984/1920 train_time:44385ms step_avg:45.11ms
step:985/1920 train_time:44448ms step_avg:45.12ms
step:986/1920 train_time:44509ms step_avg:45.14ms
step:987/1920 train_time:44572ms step_avg:45.16ms
step:988/1920 train_time:44633ms step_avg:45.18ms
step:989/1920 train_time:44696ms step_avg:45.19ms
step:990/1920 train_time:44757ms step_avg:45.21ms
step:990/1920 val_loss:3.7843 train_time:44822ms step_avg:45.27ms
step:991/1920 train_time:44839ms step_avg:45.25ms
step:992/1920 train_time:44883ms step_avg:45.25ms
step:993/1920 train_time:44948ms step_avg:45.26ms
step:994/1920 train_time:45011ms step_avg:45.28ms
step:995/1920 train_time:45074ms step_avg:45.30ms
step:996/1920 train_time:45136ms step_avg:45.32ms
step:997/1920 train_time:45198ms step_avg:45.33ms
step:998/1920 train_time:45259ms step_avg:45.35ms
step:999/1920 train_time:45322ms step_avg:45.37ms
step:1000/1920 train_time:45383ms step_avg:45.38ms
step:1000/1920 val_loss:3.7796 train_time:45448ms step_avg:45.45ms
step:1001/1920 train_time:45468ms step_avg:45.42ms
step:1002/1920 train_time:45510ms step_avg:45.42ms
step:1003/1920 train_time:45575ms step_avg:45.44ms
step:1004/1920 train_time:45638ms step_avg:45.46ms
step:1005/1920 train_time:45701ms step_avg:45.47ms
step:1006/1920 train_time:45762ms step_avg:45.49ms
step:1007/1920 train_time:45825ms step_avg:45.51ms
step:1008/1920 train_time:45886ms step_avg:45.52ms
step:1009/1920 train_time:45949ms step_avg:45.54ms
step:1010/1920 train_time:46010ms step_avg:45.55ms
step:1010/1920 val_loss:3.7707 train_time:46075ms step_avg:45.62ms
step:1011/1920 train_time:46092ms step_avg:45.59ms
step:1012/1920 train_time:46137ms step_avg:45.59ms
step:1013/1920 train_time:46202ms step_avg:45.61ms
step:1014/1920 train_time:46266ms step_avg:45.63ms
step:1015/1920 train_time:46329ms step_avg:45.64ms
step:1016/1920 train_time:46391ms step_avg:45.66ms
step:1017/1920 train_time:46453ms step_avg:45.68ms
step:1018/1920 train_time:46514ms step_avg:45.69ms
step:1019/1920 train_time:46577ms step_avg:45.71ms
step:1020/1920 train_time:46638ms step_avg:45.72ms
step:1020/1920 val_loss:3.7639 train_time:46704ms step_avg:45.79ms
step:1021/1920 train_time:46721ms step_avg:45.76ms
step:1022/1920 train_time:46765ms step_avg:45.76ms
step:1023/1920 train_time:46830ms step_avg:45.78ms
step:1024/1920 train_time:46893ms step_avg:45.79ms
step:1025/1920 train_time:46956ms step_avg:45.81ms
step:1026/1920 train_time:47018ms step_avg:45.83ms
step:1027/1920 train_time:47081ms step_avg:45.84ms
step:1028/1920 train_time:47142ms step_avg:45.86ms
step:1029/1920 train_time:47205ms step_avg:45.87ms
step:1030/1920 train_time:47266ms step_avg:45.89ms
step:1030/1920 val_loss:3.7485 train_time:47333ms step_avg:45.95ms
step:1031/1920 train_time:47350ms step_avg:45.93ms
step:1032/1920 train_time:47394ms step_avg:45.92ms
step:1033/1920 train_time:47460ms step_avg:45.94ms
step:1034/1920 train_time:47522ms step_avg:45.96ms
step:1035/1920 train_time:47585ms step_avg:45.98ms
step:1036/1920 train_time:47647ms step_avg:45.99ms
step:1037/1920 train_time:47709ms step_avg:46.01ms
step:1038/1920 train_time:47770ms step_avg:46.02ms
step:1039/1920 train_time:47833ms step_avg:46.04ms
step:1040/1920 train_time:47894ms step_avg:46.05ms
step:1040/1920 val_loss:3.7318 train_time:47960ms step_avg:46.12ms
step:1041/1920 train_time:47977ms step_avg:46.09ms
step:1042/1920 train_time:48021ms step_avg:46.09ms
step:1043/1920 train_time:48086ms step_avg:46.10ms
step:1044/1920 train_time:48148ms step_avg:46.12ms
step:1045/1920 train_time:48211ms step_avg:46.14ms
step:1046/1920 train_time:48273ms step_avg:46.15ms
step:1047/1920 train_time:48336ms step_avg:46.17ms
step:1048/1920 train_time:48398ms step_avg:46.18ms
step:1049/1920 train_time:48460ms step_avg:46.20ms
step:1050/1920 train_time:48521ms step_avg:46.21ms
step:1050/1920 val_loss:3.7362 train_time:48586ms step_avg:46.27ms
step:1051/1920 train_time:48603ms step_avg:46.24ms
step:1052/1920 train_time:48647ms step_avg:46.24ms
step:1053/1920 train_time:48712ms step_avg:46.26ms
step:1054/1920 train_time:48774ms step_avg:46.28ms
step:1055/1920 train_time:48837ms step_avg:46.29ms
step:1056/1920 train_time:48899ms step_avg:46.31ms
step:1057/1920 train_time:48961ms step_avg:46.32ms
step:1058/1920 train_time:49023ms step_avg:46.34ms
step:1059/1920 train_time:49086ms step_avg:46.35ms
step:1060/1920 train_time:49148ms step_avg:46.37ms
step:1060/1920 val_loss:3.7202 train_time:49213ms step_avg:46.43ms
step:1061/1920 train_time:49231ms step_avg:46.40ms
step:1062/1920 train_time:49274ms step_avg:46.40ms
step:1063/1920 train_time:49339ms step_avg:46.41ms
step:1064/1920 train_time:49401ms step_avg:46.43ms
step:1065/1920 train_time:49464ms step_avg:46.45ms
step:1066/1920 train_time:49526ms step_avg:46.46ms
step:1067/1920 train_time:49588ms step_avg:46.47ms
step:1068/1920 train_time:49650ms step_avg:46.49ms
step:1069/1920 train_time:49712ms step_avg:46.50ms
step:1070/1920 train_time:49774ms step_avg:46.52ms
step:1070/1920 val_loss:3.7106 train_time:49840ms step_avg:46.58ms
step:1071/1920 train_time:49857ms step_avg:46.55ms
step:1072/1920 train_time:49901ms step_avg:46.55ms
step:1073/1920 train_time:49965ms step_avg:46.57ms
step:1074/1920 train_time:50029ms step_avg:46.58ms
step:1075/1920 train_time:50091ms step_avg:46.60ms
step:1076/1920 train_time:50153ms step_avg:46.61ms
step:1077/1920 train_time:50215ms step_avg:46.63ms
step:1078/1920 train_time:50277ms step_avg:46.64ms
step:1079/1920 train_time:50340ms step_avg:46.65ms
step:1080/1920 train_time:50401ms step_avg:46.67ms
step:1080/1920 val_loss:3.6998 train_time:50467ms step_avg:46.73ms
step:1081/1920 train_time:50484ms step_avg:46.70ms
step:1082/1920 train_time:50529ms step_avg:46.70ms
step:1083/1920 train_time:50593ms step_avg:46.72ms
step:1084/1920 train_time:50656ms step_avg:46.73ms
step:1085/1920 train_time:50719ms step_avg:46.75ms
step:1086/1920 train_time:50780ms step_avg:46.76ms
step:1087/1920 train_time:50843ms step_avg:46.77ms
step:1088/1920 train_time:50904ms step_avg:46.79ms
step:1089/1920 train_time:50967ms step_avg:46.80ms
step:1090/1920 train_time:51028ms step_avg:46.81ms
step:1090/1920 val_loss:3.6846 train_time:51094ms step_avg:46.88ms
step:1091/1920 train_time:51111ms step_avg:46.85ms
step:1092/1920 train_time:51155ms step_avg:46.84ms
step:1093/1920 train_time:51221ms step_avg:46.86ms
step:1094/1920 train_time:51283ms step_avg:46.88ms
step:1095/1920 train_time:51346ms step_avg:46.89ms
step:1096/1920 train_time:51407ms step_avg:46.90ms
step:1097/1920 train_time:51470ms step_avg:46.92ms
step:1098/1920 train_time:51531ms step_avg:46.93ms
step:1099/1920 train_time:51594ms step_avg:46.95ms
step:1100/1920 train_time:51655ms step_avg:46.96ms
step:1100/1920 val_loss:3.6865 train_time:51720ms step_avg:47.02ms
step:1101/1920 train_time:51737ms step_avg:46.99ms
step:1102/1920 train_time:51781ms step_avg:46.99ms
step:1103/1920 train_time:51847ms step_avg:47.01ms
step:1104/1920 train_time:51911ms step_avg:47.02ms
step:1105/1920 train_time:51974ms step_avg:47.04ms
step:1106/1920 train_time:52035ms step_avg:47.05ms
step:1107/1920 train_time:52098ms step_avg:47.06ms
step:1108/1920 train_time:52159ms step_avg:47.07ms
step:1109/1920 train_time:52221ms step_avg:47.09ms
step:1110/1920 train_time:52283ms step_avg:47.10ms
step:1110/1920 val_loss:3.6738 train_time:52348ms step_avg:47.16ms
step:1111/1920 train_time:52365ms step_avg:47.13ms
step:1112/1920 train_time:52409ms step_avg:47.13ms
step:1113/1920 train_time:52473ms step_avg:47.15ms
step:1114/1920 train_time:52535ms step_avg:47.16ms
step:1115/1920 train_time:52599ms step_avg:47.17ms
step:1116/1920 train_time:52661ms step_avg:47.19ms
step:1117/1920 train_time:52723ms step_avg:47.20ms
step:1118/1920 train_time:52785ms step_avg:47.21ms
step:1119/1920 train_time:52847ms step_avg:47.23ms
step:1120/1920 train_time:52909ms step_avg:47.24ms
step:1120/1920 val_loss:3.6668 train_time:52975ms step_avg:47.30ms
step:1121/1920 train_time:52993ms step_avg:47.27ms
step:1122/1920 train_time:53036ms step_avg:47.27ms
step:1123/1920 train_time:53102ms step_avg:47.29ms
step:1124/1920 train_time:53166ms step_avg:47.30ms
step:1125/1920 train_time:53229ms step_avg:47.31ms
step:1126/1920 train_time:53291ms step_avg:47.33ms
step:1127/1920 train_time:53354ms step_avg:47.34ms
step:1128/1920 train_time:53415ms step_avg:47.35ms
step:1129/1920 train_time:53478ms step_avg:47.37ms
step:1130/1920 train_time:53539ms step_avg:47.38ms
step:1130/1920 val_loss:3.6525 train_time:53604ms step_avg:47.44ms
step:1131/1920 train_time:53621ms step_avg:47.41ms
step:1132/1920 train_time:53666ms step_avg:47.41ms
step:1133/1920 train_time:53730ms step_avg:47.42ms
step:1134/1920 train_time:53793ms step_avg:47.44ms
step:1135/1920 train_time:53857ms step_avg:47.45ms
step:1136/1920 train_time:53920ms step_avg:47.46ms
step:1137/1920 train_time:53982ms step_avg:47.48ms
step:1138/1920 train_time:54043ms step_avg:47.49ms
step:1139/1920 train_time:54106ms step_avg:47.50ms
step:1140/1920 train_time:54167ms step_avg:47.52ms
step:1140/1920 val_loss:3.6382 train_time:54233ms step_avg:47.57ms
step:1141/1920 train_time:54250ms step_avg:47.55ms
step:1142/1920 train_time:54293ms step_avg:47.54ms
step:1143/1920 train_time:54359ms step_avg:47.56ms
step:1144/1920 train_time:54423ms step_avg:47.57ms
step:1145/1920 train_time:54485ms step_avg:47.59ms
step:1146/1920 train_time:54547ms step_avg:47.60ms
step:1147/1920 train_time:54609ms step_avg:47.61ms
step:1148/1920 train_time:54671ms step_avg:47.62ms
step:1149/1920 train_time:54733ms step_avg:47.64ms
step:1150/1920 train_time:54795ms step_avg:47.65ms
step:1150/1920 val_loss:3.6338 train_time:54861ms step_avg:47.71ms
step:1151/1920 train_time:54879ms step_avg:47.68ms
step:1152/1920 train_time:54923ms step_avg:47.68ms
step:1153/1920 train_time:54988ms step_avg:47.69ms
step:1154/1920 train_time:55051ms step_avg:47.70ms
step:1155/1920 train_time:55114ms step_avg:47.72ms
step:1156/1920 train_time:55175ms step_avg:47.73ms
step:1157/1920 train_time:55237ms step_avg:47.74ms
step:1158/1920 train_time:55298ms step_avg:47.75ms
step:1159/1920 train_time:55361ms step_avg:47.77ms
step:1160/1920 train_time:55423ms step_avg:47.78ms
step:1160/1920 val_loss:3.6239 train_time:55490ms step_avg:47.84ms
step:1161/1920 train_time:55507ms step_avg:47.81ms
step:1162/1920 train_time:55551ms step_avg:47.81ms
step:1163/1920 train_time:55616ms step_avg:47.82ms
step:1164/1920 train_time:55679ms step_avg:47.83ms
step:1165/1920 train_time:55742ms step_avg:47.85ms
step:1166/1920 train_time:55804ms step_avg:47.86ms
step:1167/1920 train_time:55865ms step_avg:47.87ms
step:1168/1920 train_time:55927ms step_avg:47.88ms
step:1169/1920 train_time:55989ms step_avg:47.89ms
step:1170/1920 train_time:56050ms step_avg:47.91ms
step:1170/1920 val_loss:3.6137 train_time:56115ms step_avg:47.96ms
step:1171/1920 train_time:56132ms step_avg:47.94ms
step:1172/1920 train_time:56176ms step_avg:47.93ms
step:1173/1920 train_time:56239ms step_avg:47.94ms
step:1174/1920 train_time:56303ms step_avg:47.96ms
step:1175/1920 train_time:56366ms step_avg:47.97ms
step:1176/1920 train_time:56429ms step_avg:47.98ms
step:1177/1920 train_time:56491ms step_avg:48.00ms
step:1178/1920 train_time:56553ms step_avg:48.01ms
step:1179/1920 train_time:56616ms step_avg:48.02ms
step:1180/1920 train_time:56677ms step_avg:48.03ms
step:1180/1920 val_loss:3.6077 train_time:56742ms step_avg:48.09ms
step:1181/1920 train_time:56759ms step_avg:48.06ms
step:1182/1920 train_time:56804ms step_avg:48.06ms
step:1183/1920 train_time:56870ms step_avg:48.07ms
step:1184/1920 train_time:56933ms step_avg:48.09ms
step:1185/1920 train_time:56997ms step_avg:48.10ms
step:1186/1920 train_time:57060ms step_avg:48.11ms
step:1187/1920 train_time:57122ms step_avg:48.12ms
step:1188/1920 train_time:57184ms step_avg:48.13ms
step:1189/1920 train_time:57246ms step_avg:48.15ms
step:1190/1920 train_time:57307ms step_avg:48.16ms
step:1190/1920 val_loss:3.5955 train_time:57372ms step_avg:48.21ms
step:1191/1920 train_time:57389ms step_avg:48.19ms
step:1192/1920 train_time:57432ms step_avg:48.18ms
step:1193/1920 train_time:57497ms step_avg:48.20ms
step:1194/1920 train_time:57560ms step_avg:48.21ms
step:1195/1920 train_time:57622ms step_avg:48.22ms
step:1196/1920 train_time:57684ms step_avg:48.23ms
step:1197/1920 train_time:57746ms step_avg:48.24ms
step:1198/1920 train_time:57807ms step_avg:48.25ms
step:1199/1920 train_time:57870ms step_avg:48.27ms
step:1200/1920 train_time:57932ms step_avg:48.28ms
step:1200/1920 val_loss:3.5883 train_time:57997ms step_avg:48.33ms
step:1201/1920 train_time:58014ms step_avg:48.31ms
step:1202/1920 train_time:58060ms step_avg:48.30ms
step:1203/1920 train_time:58123ms step_avg:48.32ms
step:1204/1920 train_time:58185ms step_avg:48.33ms
step:1205/1920 train_time:58248ms step_avg:48.34ms
step:1206/1920 train_time:58310ms step_avg:48.35ms
step:1207/1920 train_time:58372ms step_avg:48.36ms
step:1208/1920 train_time:58434ms step_avg:48.37ms
step:1209/1920 train_time:58496ms step_avg:48.38ms
step:1210/1920 train_time:58558ms step_avg:48.39ms
step:1210/1920 val_loss:3.5824 train_time:58623ms step_avg:48.45ms
step:1211/1920 train_time:58640ms step_avg:48.42ms
step:1212/1920 train_time:58683ms step_avg:48.42ms
step:1213/1920 train_time:58747ms step_avg:48.43ms
step:1214/1920 train_time:58810ms step_avg:48.44ms
step:1215/1920 train_time:58872ms step_avg:48.45ms
step:1216/1920 train_time:58934ms step_avg:48.47ms
step:1217/1920 train_time:58997ms step_avg:48.48ms
step:1218/1920 train_time:59058ms step_avg:48.49ms
step:1219/1920 train_time:59121ms step_avg:48.50ms
step:1220/1920 train_time:59182ms step_avg:48.51ms
step:1220/1920 val_loss:3.5726 train_time:59248ms step_avg:48.56ms
step:1221/1920 train_time:59265ms step_avg:48.54ms
step:1222/1920 train_time:59310ms step_avg:48.53ms
step:1223/1920 train_time:59376ms step_avg:48.55ms
step:1224/1920 train_time:59439ms step_avg:48.56ms
step:1225/1920 train_time:59501ms step_avg:48.57ms
step:1226/1920 train_time:59563ms step_avg:48.58ms
step:1227/1920 train_time:59625ms step_avg:48.59ms
step:1228/1920 train_time:59687ms step_avg:48.61ms
step:1229/1920 train_time:59750ms step_avg:48.62ms
step:1230/1920 train_time:59811ms step_avg:48.63ms
step:1230/1920 val_loss:3.5623 train_time:59877ms step_avg:48.68ms
step:1231/1920 train_time:59894ms step_avg:48.65ms
step:1232/1920 train_time:59938ms step_avg:48.65ms
step:1233/1920 train_time:60003ms step_avg:48.66ms
step:1234/1920 train_time:60065ms step_avg:48.67ms
step:1235/1920 train_time:60127ms step_avg:48.69ms
step:1236/1920 train_time:60189ms step_avg:48.70ms
step:1237/1920 train_time:60251ms step_avg:48.71ms
step:1238/1920 train_time:60313ms step_avg:48.72ms
step:1239/1920 train_time:60375ms step_avg:48.73ms
step:1240/1920 train_time:60437ms step_avg:48.74ms
step:1240/1920 val_loss:3.5563 train_time:60502ms step_avg:48.79ms
step:1241/1920 train_time:60519ms step_avg:48.77ms
step:1242/1920 train_time:60563ms step_avg:48.76ms
step:1243/1920 train_time:60628ms step_avg:48.78ms
step:1244/1920 train_time:60691ms step_avg:48.79ms
step:1245/1920 train_time:60753ms step_avg:48.80ms
step:1246/1920 train_time:60815ms step_avg:48.81ms
step:1247/1920 train_time:60878ms step_avg:48.82ms
step:1248/1920 train_time:60940ms step_avg:48.83ms
step:1249/1920 train_time:61003ms step_avg:48.84ms
step:1250/1920 train_time:61065ms step_avg:48.85ms
step:1250/1920 val_loss:3.5504 train_time:61130ms step_avg:48.90ms
step:1251/1920 train_time:61148ms step_avg:48.88ms
step:1252/1920 train_time:61191ms step_avg:48.87ms
step:1253/1920 train_time:61257ms step_avg:48.89ms
step:1254/1920 train_time:61320ms step_avg:48.90ms
step:1255/1920 train_time:61383ms step_avg:48.91ms
step:1256/1920 train_time:61471ms step_avg:48.94ms
step:1257/1920 train_time:61559ms step_avg:48.97ms
step:1258/1920 train_time:61646ms step_avg:49.00ms
step:1259/1920 train_time:61735ms step_avg:49.03ms
step:1260/1920 train_time:61823ms step_avg:49.07ms
step:1260/1920 val_loss:3.5339 train_time:61913ms step_avg:49.14ms
step:1261/1920 train_time:61931ms step_avg:49.11ms
step:1262/1920 train_time:62003ms step_avg:49.13ms
step:1263/1920 train_time:62093ms step_avg:49.16ms
step:1264/1920 train_time:62181ms step_avg:49.19ms
step:1265/1920 train_time:62270ms step_avg:49.23ms
step:1266/1920 train_time:62358ms step_avg:49.26ms
step:1267/1920 train_time:62447ms step_avg:49.29ms
step:1268/1920 train_time:62534ms step_avg:49.32ms
step:1269/1920 train_time:62622ms step_avg:49.35ms
step:1270/1920 train_time:62710ms step_avg:49.38ms
step:1270/1920 val_loss:3.5270 train_time:62801ms step_avg:49.45ms
step:1271/1920 train_time:62818ms step_avg:49.42ms
step:1272/1920 train_time:62889ms step_avg:49.44ms
step:1273/1920 train_time:62983ms step_avg:49.48ms
step:1274/1920 train_time:63072ms step_avg:49.51ms
step:1275/1920 train_time:63161ms step_avg:49.54ms
step:1276/1920 train_time:63248ms step_avg:49.57ms
step:1277/1920 train_time:63336ms step_avg:49.60ms
step:1278/1920 train_time:63425ms step_avg:49.63ms
step:1279/1920 train_time:63514ms step_avg:49.66ms
step:1280/1920 train_time:63602ms step_avg:49.69ms
step:1280/1920 val_loss:3.5205 train_time:63693ms step_avg:49.76ms
step:1281/1920 train_time:63710ms step_avg:49.73ms
step:1282/1920 train_time:63782ms step_avg:49.75ms
step:1283/1920 train_time:63874ms step_avg:49.79ms
step:1284/1920 train_time:63963ms step_avg:49.82ms
step:1285/1920 train_time:64052ms step_avg:49.85ms
step:1286/1920 train_time:64139ms step_avg:49.88ms
step:1287/1920 train_time:64228ms step_avg:49.90ms
step:1288/1920 train_time:64315ms step_avg:49.93ms
step:1289/1920 train_time:64405ms step_avg:49.96ms
step:1290/1920 train_time:64493ms step_avg:49.99ms
step:1290/1920 val_loss:3.5123 train_time:64584ms step_avg:50.06ms
step:1291/1920 train_time:64601ms step_avg:50.04ms
step:1292/1920 train_time:64673ms step_avg:50.06ms
step:1293/1920 train_time:64766ms step_avg:50.09ms
step:1294/1920 train_time:64854ms step_avg:50.12ms
step:1295/1920 train_time:64945ms step_avg:50.15ms
step:1296/1920 train_time:65033ms step_avg:50.18ms
step:1297/1920 train_time:65121ms step_avg:50.21ms
step:1298/1920 train_time:65209ms step_avg:50.24ms
step:1299/1920 train_time:65296ms step_avg:50.27ms
step:1300/1920 train_time:65384ms step_avg:50.30ms
step:1300/1920 val_loss:3.5070 train_time:65474ms step_avg:50.36ms
step:1301/1920 train_time:65492ms step_avg:50.34ms
step:1302/1920 train_time:65564ms step_avg:50.36ms
step:1303/1920 train_time:65655ms step_avg:50.39ms
step:1304/1920 train_time:65744ms step_avg:50.42ms
step:1305/1920 train_time:65831ms step_avg:50.45ms
step:1306/1920 train_time:65920ms step_avg:50.47ms
step:1307/1920 train_time:66007ms step_avg:50.50ms
step:1308/1920 train_time:66095ms step_avg:50.53ms
step:1309/1920 train_time:66184ms step_avg:50.56ms
step:1310/1920 train_time:66272ms step_avg:50.59ms
step:1310/1920 val_loss:3.5034 train_time:66362ms step_avg:50.66ms
step:1311/1920 train_time:66379ms step_avg:50.63ms
step:1312/1920 train_time:66452ms step_avg:50.65ms
step:1313/1920 train_time:66543ms step_avg:50.68ms
step:1314/1920 train_time:66631ms step_avg:50.71ms
step:1315/1920 train_time:66719ms step_avg:50.74ms
step:1316/1920 train_time:66808ms step_avg:50.77ms
step:1317/1920 train_time:66896ms step_avg:50.79ms
step:1318/1920 train_time:66984ms step_avg:50.82ms
step:1319/1920 train_time:67073ms step_avg:50.85ms
step:1320/1920 train_time:67161ms step_avg:50.88ms
step:1320/1920 val_loss:3.4959 train_time:67251ms step_avg:50.95ms
step:1321/1920 train_time:67268ms step_avg:50.92ms
step:1322/1920 train_time:67340ms step_avg:50.94ms
step:1323/1920 train_time:67431ms step_avg:50.97ms
step:1324/1920 train_time:67519ms step_avg:51.00ms
step:1325/1920 train_time:67608ms step_avg:51.03ms
step:1326/1920 train_time:67697ms step_avg:51.05ms
step:1327/1920 train_time:67785ms step_avg:51.08ms
step:1328/1920 train_time:67873ms step_avg:51.11ms
step:1329/1920 train_time:67961ms step_avg:51.14ms
step:1330/1920 train_time:68048ms step_avg:51.16ms
step:1330/1920 val_loss:3.4904 train_time:68139ms step_avg:51.23ms
step:1331/1920 train_time:68157ms step_avg:51.21ms
step:1332/1920 train_time:68230ms step_avg:51.22ms
step:1333/1920 train_time:68321ms step_avg:51.25ms
step:1334/1920 train_time:68411ms step_avg:51.28ms
step:1335/1920 train_time:68498ms step_avg:51.31ms
step:1336/1920 train_time:68586ms step_avg:51.34ms
step:1337/1920 train_time:68674ms step_avg:51.36ms
step:1338/1920 train_time:68761ms step_avg:51.39ms
step:1339/1920 train_time:68850ms step_avg:51.42ms
step:1340/1920 train_time:68938ms step_avg:51.45ms
step:1340/1920 val_loss:3.4864 train_time:69029ms step_avg:51.51ms
step:1341/1920 train_time:69046ms step_avg:51.49ms
step:1342/1920 train_time:69117ms step_avg:51.50ms
step:1343/1920 train_time:69210ms step_avg:51.53ms
step:1344/1920 train_time:69301ms step_avg:51.56ms
step:1345/1920 train_time:69390ms step_avg:51.59ms
step:1346/1920 train_time:69478ms step_avg:51.62ms
step:1347/1920 train_time:69566ms step_avg:51.65ms
step:1348/1920 train_time:69653ms step_avg:51.67ms
step:1349/1920 train_time:69741ms step_avg:51.70ms
step:1350/1920 train_time:69828ms step_avg:51.72ms
step:1350/1920 val_loss:3.4801 train_time:69918ms step_avg:51.79ms
step:1351/1920 train_time:69935ms step_avg:51.77ms
step:1352/1920 train_time:70009ms step_avg:51.78ms
step:1353/1920 train_time:70101ms step_avg:51.81ms
step:1354/1920 train_time:70189ms step_avg:51.84ms
step:1355/1920 train_time:70278ms step_avg:51.87ms
step:1356/1920 train_time:70366ms step_avg:51.89ms
step:1357/1920 train_time:70454ms step_avg:51.92ms
step:1358/1920 train_time:70541ms step_avg:51.95ms
step:1359/1920 train_time:70629ms step_avg:51.97ms
step:1360/1920 train_time:70717ms step_avg:52.00ms
step:1360/1920 val_loss:3.4765 train_time:70808ms step_avg:52.06ms
step:1361/1920 train_time:70826ms step_avg:52.04ms
step:1362/1920 train_time:70896ms step_avg:52.05ms
step:1363/1920 train_time:70988ms step_avg:52.08ms
step:1364/1920 train_time:71077ms step_avg:52.11ms
step:1365/1920 train_time:71165ms step_avg:52.14ms
step:1366/1920 train_time:71253ms step_avg:52.16ms
step:1367/1920 train_time:71342ms step_avg:52.19ms
step:1368/1920 train_time:71430ms step_avg:52.21ms
step:1369/1920 train_time:71518ms step_avg:52.24ms
step:1370/1920 train_time:71606ms step_avg:52.27ms
step:1370/1920 val_loss:3.4710 train_time:71696ms step_avg:52.33ms
step:1371/1920 train_time:71714ms step_avg:52.31ms
step:1372/1920 train_time:71785ms step_avg:52.32ms
step:1373/1920 train_time:71879ms step_avg:52.35ms
step:1374/1920 train_time:71966ms step_avg:52.38ms
step:1375/1920 train_time:72053ms step_avg:52.40ms
step:1376/1920 train_time:72141ms step_avg:52.43ms
step:1377/1920 train_time:72229ms step_avg:52.45ms
step:1378/1920 train_time:72318ms step_avg:52.48ms
step:1379/1920 train_time:72407ms step_avg:52.51ms
step:1380/1920 train_time:72494ms step_avg:52.53ms
step:1380/1920 val_loss:3.4656 train_time:72584ms step_avg:52.60ms
step:1381/1920 train_time:72601ms step_avg:52.57ms
step:1382/1920 train_time:72673ms step_avg:52.59ms
step:1383/1920 train_time:72764ms step_avg:52.61ms
step:1384/1920 train_time:72853ms step_avg:52.64ms
step:1385/1920 train_time:72942ms step_avg:52.67ms
step:1386/1920 train_time:73030ms step_avg:52.69ms
step:1387/1920 train_time:73118ms step_avg:52.72ms
step:1388/1920 train_time:73206ms step_avg:52.74ms
step:1389/1920 train_time:73294ms step_avg:52.77ms
step:1390/1920 train_time:73382ms step_avg:52.79ms
step:1390/1920 val_loss:3.4609 train_time:73472ms step_avg:52.86ms
step:1391/1920 train_time:73489ms step_avg:52.83ms
step:1392/1920 train_time:73562ms step_avg:52.85ms
step:1393/1920 train_time:73654ms step_avg:52.87ms
step:1394/1920 train_time:73742ms step_avg:52.90ms
step:1395/1920 train_time:73831ms step_avg:52.93ms
step:1396/1920 train_time:73917ms step_avg:52.95ms
step:1397/1920 train_time:74004ms step_avg:52.97ms
step:1398/1920 train_time:74092ms step_avg:53.00ms
step:1399/1920 train_time:74180ms step_avg:53.02ms
step:1400/1920 train_time:74268ms step_avg:53.05ms
step:1400/1920 val_loss:3.4577 train_time:74358ms step_avg:53.11ms
step:1401/1920 train_time:74375ms step_avg:53.09ms
step:1402/1920 train_time:74447ms step_avg:53.10ms
step:1403/1920 train_time:74539ms step_avg:53.13ms
step:1404/1920 train_time:74629ms step_avg:53.15ms
step:1405/1920 train_time:74717ms step_avg:53.18ms
step:1406/1920 train_time:74806ms step_avg:53.20ms
step:1407/1920 train_time:74895ms step_avg:53.23ms
step:1408/1920 train_time:74982ms step_avg:53.25ms
step:1409/1920 train_time:75070ms step_avg:53.28ms
step:1410/1920 train_time:75156ms step_avg:53.30ms
step:1410/1920 val_loss:3.4519 train_time:75247ms step_avg:53.37ms
step:1411/1920 train_time:75264ms step_avg:53.34ms
step:1412/1920 train_time:75338ms step_avg:53.36ms
step:1413/1920 train_time:75429ms step_avg:53.38ms
step:1414/1920 train_time:75518ms step_avg:53.41ms
step:1415/1920 train_time:75608ms step_avg:53.43ms
step:1416/1920 train_time:75695ms step_avg:53.46ms
step:1417/1920 train_time:75784ms step_avg:53.48ms
step:1418/1920 train_time:75871ms step_avg:53.51ms
step:1419/1920 train_time:75959ms step_avg:53.53ms
step:1420/1920 train_time:76046ms step_avg:53.55ms
step:1420/1920 val_loss:3.4469 train_time:76137ms step_avg:53.62ms
step:1421/1920 train_time:76155ms step_avg:53.59ms
step:1422/1920 train_time:76226ms step_avg:53.60ms
step:1423/1920 train_time:76319ms step_avg:53.63ms
step:1424/1920 train_time:76408ms step_avg:53.66ms
step:1425/1920 train_time:76496ms step_avg:53.68ms
step:1426/1920 train_time:76583ms step_avg:53.70ms
step:1427/1920 train_time:76671ms step_avg:53.73ms
step:1428/1920 train_time:76759ms step_avg:53.75ms
step:1429/1920 train_time:76847ms step_avg:53.78ms
step:1430/1920 train_time:76935ms step_avg:53.80ms
step:1430/1920 val_loss:3.4434 train_time:77025ms step_avg:53.86ms
step:1431/1920 train_time:77043ms step_avg:53.84ms
step:1432/1920 train_time:77116ms step_avg:53.85ms
step:1433/1920 train_time:77206ms step_avg:53.88ms
step:1434/1920 train_time:77295ms step_avg:53.90ms
step:1435/1920 train_time:77383ms step_avg:53.93ms
step:1436/1920 train_time:77471ms step_avg:53.95ms
step:1437/1920 train_time:77558ms step_avg:53.97ms
step:1438/1920 train_time:77646ms step_avg:54.00ms
step:1439/1920 train_time:77734ms step_avg:54.02ms
step:1440/1920 train_time:77823ms step_avg:54.04ms
step:1440/1920 val_loss:3.4385 train_time:77914ms step_avg:54.11ms
step:1441/1920 train_time:77931ms step_avg:54.08ms
step:1442/1920 train_time:78003ms step_avg:54.09ms
step:1443/1920 train_time:78097ms step_avg:54.12ms
step:1444/1920 train_time:78185ms step_avg:54.15ms
step:1445/1920 train_time:78275ms step_avg:54.17ms
step:1446/1920 train_time:78363ms step_avg:54.19ms
step:1447/1920 train_time:78451ms step_avg:54.22ms
step:1448/1920 train_time:78539ms step_avg:54.24ms
step:1449/1920 train_time:78628ms step_avg:54.26ms
step:1450/1920 train_time:78715ms step_avg:54.29ms
step:1450/1920 val_loss:3.4341 train_time:78805ms step_avg:54.35ms
step:1451/1920 train_time:78823ms step_avg:54.32ms
step:1452/1920 train_time:78895ms step_avg:54.34ms
step:1453/1920 train_time:78989ms step_avg:54.36ms
step:1454/1920 train_time:79077ms step_avg:54.39ms
step:1455/1920 train_time:79165ms step_avg:54.41ms
step:1456/1920 train_time:79254ms step_avg:54.43ms
step:1457/1920 train_time:79341ms step_avg:54.46ms
step:1458/1920 train_time:79429ms step_avg:54.48ms
step:1459/1920 train_time:79517ms step_avg:54.50ms
step:1460/1920 train_time:79604ms step_avg:54.52ms
step:1460/1920 val_loss:3.4306 train_time:79697ms step_avg:54.59ms
step:1461/1920 train_time:79714ms step_avg:54.56ms
step:1462/1920 train_time:79786ms step_avg:54.57ms
step:1463/1920 train_time:79877ms step_avg:54.60ms
step:1464/1920 train_time:79966ms step_avg:54.62ms
step:1465/1920 train_time:80055ms step_avg:54.65ms
step:1466/1920 train_time:80143ms step_avg:54.67ms
step:1467/1920 train_time:80231ms step_avg:54.69ms
step:1468/1920 train_time:80320ms step_avg:54.71ms
step:1469/1920 train_time:80407ms step_avg:54.74ms
step:1470/1920 train_time:80494ms step_avg:54.76ms
step:1470/1920 val_loss:3.4258 train_time:80585ms step_avg:54.82ms
step:1471/1920 train_time:80605ms step_avg:54.80ms
step:1472/1920 train_time:80673ms step_avg:54.81ms
step:1473/1920 train_time:80764ms step_avg:54.83ms
step:1474/1920 train_time:80852ms step_avg:54.85ms
step:1475/1920 train_time:80941ms step_avg:54.88ms
step:1476/1920 train_time:81029ms step_avg:54.90ms
step:1477/1920 train_time:81118ms step_avg:54.92ms
step:1478/1920 train_time:81205ms step_avg:54.94ms
step:1479/1920 train_time:81294ms step_avg:54.97ms
step:1480/1920 train_time:81381ms step_avg:54.99ms
step:1480/1920 val_loss:3.4212 train_time:81472ms step_avg:55.05ms
step:1481/1920 train_time:81489ms step_avg:55.02ms
step:1482/1920 train_time:81562ms step_avg:55.04ms
step:1483/1920 train_time:81654ms step_avg:55.06ms
step:1484/1920 train_time:81742ms step_avg:55.08ms
step:1485/1920 train_time:81829ms step_avg:55.10ms
step:1486/1920 train_time:81917ms step_avg:55.13ms
step:1487/1920 train_time:82006ms step_avg:55.15ms
step:1488/1920 train_time:82092ms step_avg:55.17ms
step:1489/1920 train_time:82181ms step_avg:55.19ms
step:1490/1920 train_time:82268ms step_avg:55.21ms
step:1490/1920 val_loss:3.4172 train_time:82360ms step_avg:55.27ms
step:1491/1920 train_time:82377ms step_avg:55.25ms
step:1492/1920 train_time:82448ms step_avg:55.26ms
step:1493/1920 train_time:82540ms step_avg:55.28ms
step:1494/1920 train_time:82630ms step_avg:55.31ms
step:1495/1920 train_time:82718ms step_avg:55.33ms
step:1496/1920 train_time:82806ms step_avg:55.35ms
step:1497/1920 train_time:82895ms step_avg:55.37ms
step:1498/1920 train_time:82983ms step_avg:55.40ms
step:1499/1920 train_time:83071ms step_avg:55.42ms
step:1500/1920 train_time:83158ms step_avg:55.44ms
step:1500/1920 val_loss:3.4136 train_time:83249ms step_avg:55.50ms
step:1501/1920 train_time:83266ms step_avg:55.47ms
step:1502/1920 train_time:83338ms step_avg:55.48ms
step:1503/1920 train_time:83429ms step_avg:55.51ms
step:1504/1920 train_time:83518ms step_avg:55.53ms
step:1505/1920 train_time:83606ms step_avg:55.55ms
step:1506/1920 train_time:83693ms step_avg:55.57ms
step:1507/1920 train_time:83781ms step_avg:55.59ms
step:1508/1920 train_time:83869ms step_avg:55.62ms
step:1509/1920 train_time:83958ms step_avg:55.64ms
step:1510/1920 train_time:84046ms step_avg:55.66ms
step:1510/1920 val_loss:3.4094 train_time:84136ms step_avg:55.72ms
step:1511/1920 train_time:84154ms step_avg:55.69ms
step:1512/1920 train_time:84226ms step_avg:55.70ms
step:1513/1920 train_time:84318ms step_avg:55.73ms
step:1514/1920 train_time:84406ms step_avg:55.75ms
step:1515/1920 train_time:84494ms step_avg:55.77ms
step:1516/1920 train_time:84582ms step_avg:55.79ms
step:1517/1920 train_time:84670ms step_avg:55.81ms
step:1518/1920 train_time:84758ms step_avg:55.84ms
step:1519/1920 train_time:84846ms step_avg:55.86ms
step:1520/1920 train_time:84933ms step_avg:55.88ms
step:1520/1920 val_loss:3.4059 train_time:85023ms step_avg:55.94ms
step:1521/1920 train_time:85041ms step_avg:55.91ms
step:1522/1920 train_time:85112ms step_avg:55.92ms
step:1523/1920 train_time:85203ms step_avg:55.94ms
step:1524/1920 train_time:85291ms step_avg:55.97ms
step:1525/1920 train_time:85380ms step_avg:55.99ms
step:1526/1920 train_time:85468ms step_avg:56.01ms
step:1527/1920 train_time:85556ms step_avg:56.03ms
step:1528/1920 train_time:85643ms step_avg:56.05ms
step:1529/1920 train_time:85732ms step_avg:56.07ms
step:1530/1920 train_time:85820ms step_avg:56.09ms
step:1530/1920 val_loss:3.4016 train_time:85910ms step_avg:56.15ms
step:1531/1920 train_time:85927ms step_avg:56.12ms
step:1532/1920 train_time:86000ms step_avg:56.14ms
step:1533/1920 train_time:86092ms step_avg:56.16ms
step:1534/1920 train_time:86180ms step_avg:56.18ms
step:1535/1920 train_time:86267ms step_avg:56.20ms
step:1536/1920 train_time:86355ms step_avg:56.22ms
step:1537/1920 train_time:86443ms step_avg:56.24ms
step:1538/1920 train_time:86531ms step_avg:56.26ms
step:1539/1920 train_time:86618ms step_avg:56.28ms
step:1540/1920 train_time:86706ms step_avg:56.30ms
step:1540/1920 val_loss:3.3976 train_time:86798ms step_avg:56.36ms
step:1541/1920 train_time:86815ms step_avg:56.34ms
step:1542/1920 train_time:86887ms step_avg:56.35ms
step:1543/1920 train_time:86979ms step_avg:56.37ms
step:1544/1920 train_time:87068ms step_avg:56.39ms
step:1545/1920 train_time:87158ms step_avg:56.41ms
step:1546/1920 train_time:87246ms step_avg:56.43ms
step:1547/1920 train_time:87333ms step_avg:56.45ms
step:1548/1920 train_time:87420ms step_avg:56.47ms
step:1549/1920 train_time:87507ms step_avg:56.49ms
step:1550/1920 train_time:87595ms step_avg:56.51ms
step:1550/1920 val_loss:3.3937 train_time:87687ms step_avg:56.57ms
step:1551/1920 train_time:87704ms step_avg:56.55ms
step:1552/1920 train_time:87777ms step_avg:56.56ms
step:1553/1920 train_time:87869ms step_avg:56.58ms
step:1554/1920 train_time:87956ms step_avg:56.60ms
step:1555/1920 train_time:88044ms step_avg:56.62ms
step:1556/1920 train_time:88132ms step_avg:56.64ms
step:1557/1920 train_time:88220ms step_avg:56.66ms
step:1558/1920 train_time:88308ms step_avg:56.68ms
step:1559/1920 train_time:88396ms step_avg:56.70ms
step:1560/1920 train_time:88484ms step_avg:56.72ms
step:1560/1920 val_loss:3.3899 train_time:88574ms step_avg:56.78ms
step:1561/1920 train_time:88592ms step_avg:56.75ms
step:1562/1920 train_time:88664ms step_avg:56.76ms
step:1563/1920 train_time:88754ms step_avg:56.78ms
step:1564/1920 train_time:88843ms step_avg:56.80ms
step:1565/1920 train_time:88931ms step_avg:56.83ms
step:1566/1920 train_time:89019ms step_avg:56.84ms
step:1567/1920 train_time:89107ms step_avg:56.86ms
step:1568/1920 train_time:89194ms step_avg:56.88ms
step:1569/1920 train_time:89283ms step_avg:56.90ms
step:1570/1920 train_time:89371ms step_avg:56.92ms
step:1570/1920 val_loss:3.3857 train_time:89461ms step_avg:56.98ms
step:1571/1920 train_time:89479ms step_avg:56.96ms
step:1572/1920 train_time:89551ms step_avg:56.97ms
step:1573/1920 train_time:89641ms step_avg:56.99ms
step:1574/1920 train_time:89729ms step_avg:57.01ms
step:1575/1920 train_time:89817ms step_avg:57.03ms
step:1576/1920 train_time:89905ms step_avg:57.05ms
step:1577/1920 train_time:89995ms step_avg:57.07ms
step:1578/1920 train_time:90083ms step_avg:57.09ms
step:1579/1920 train_time:90171ms step_avg:57.11ms
step:1580/1920 train_time:90259ms step_avg:57.13ms
step:1580/1920 val_loss:3.3816 train_time:90350ms step_avg:57.18ms
step:1581/1920 train_time:90367ms step_avg:57.16ms
step:1582/1920 train_time:90438ms step_avg:57.17ms
step:1583/1920 train_time:90528ms step_avg:57.19ms
step:1584/1920 train_time:90616ms step_avg:57.21ms
step:1585/1920 train_time:90704ms step_avg:57.23ms
step:1586/1920 train_time:90792ms step_avg:57.25ms
step:1587/1920 train_time:90880ms step_avg:57.27ms
step:1588/1920 train_time:90968ms step_avg:57.28ms
step:1589/1920 train_time:91058ms step_avg:57.31ms
step:1590/1920 train_time:91146ms step_avg:57.32ms
step:1590/1920 val_loss:3.3781 train_time:91237ms step_avg:57.38ms
step:1591/1920 train_time:91254ms step_avg:57.36ms
step:1592/1920 train_time:91326ms step_avg:57.37ms
step:1593/1920 train_time:91419ms step_avg:57.39ms
step:1594/1920 train_time:91508ms step_avg:57.41ms
step:1595/1920 train_time:91596ms step_avg:57.43ms
step:1596/1920 train_time:91684ms step_avg:57.45ms
step:1597/1920 train_time:91772ms step_avg:57.46ms
step:1598/1920 train_time:91858ms step_avg:57.48ms
step:1599/1920 train_time:91947ms step_avg:57.50ms
step:1600/1920 train_time:92035ms step_avg:57.52ms
step:1600/1920 val_loss:3.3743 train_time:92125ms step_avg:57.58ms
step:1601/1920 train_time:92142ms step_avg:57.55ms
step:1602/1920 train_time:92215ms step_avg:57.56ms
step:1603/1920 train_time:92305ms step_avg:57.58ms
step:1604/1920 train_time:92394ms step_avg:57.60ms
step:1605/1920 train_time:92483ms step_avg:57.62ms
step:1606/1920 train_time:92572ms step_avg:57.64ms
step:1607/1920 train_time:92660ms step_avg:57.66ms
step:1608/1920 train_time:92748ms step_avg:57.68ms
step:1609/1920 train_time:92837ms step_avg:57.70ms
step:1610/1920 train_time:92923ms step_avg:57.72ms
step:1610/1920 val_loss:3.3704 train_time:93014ms step_avg:57.77ms
step:1611/1920 train_time:93031ms step_avg:57.75ms
step:1612/1920 train_time:93104ms step_avg:57.76ms
step:1613/1920 train_time:93196ms step_avg:57.78ms
step:1614/1920 train_time:93284ms step_avg:57.80ms
step:1615/1920 train_time:93373ms step_avg:57.82ms
step:1616/1920 train_time:93460ms step_avg:57.83ms
step:1617/1920 train_time:93548ms step_avg:57.85ms
step:1618/1920 train_time:93636ms step_avg:57.87ms
step:1619/1920 train_time:93724ms step_avg:57.89ms
step:1620/1920 train_time:93811ms step_avg:57.91ms
step:1620/1920 val_loss:3.3667 train_time:93902ms step_avg:57.96ms
step:1621/1920 train_time:93920ms step_avg:57.94ms
step:1622/1920 train_time:93990ms step_avg:57.95ms
step:1623/1920 train_time:94083ms step_avg:57.97ms
step:1624/1920 train_time:94173ms step_avg:57.99ms
step:1625/1920 train_time:94261ms step_avg:58.01ms
step:1626/1920 train_time:94352ms step_avg:58.03ms
step:1627/1920 train_time:94440ms step_avg:58.05ms
step:1628/1920 train_time:94527ms step_avg:58.06ms
step:1629/1920 train_time:94614ms step_avg:58.08ms
step:1630/1920 train_time:94703ms step_avg:58.10ms
step:1630/1920 val_loss:3.3634 train_time:94795ms step_avg:58.16ms
step:1631/1920 train_time:94812ms step_avg:58.13ms
step:1632/1920 train_time:94883ms step_avg:58.14ms
step:1633/1920 train_time:94974ms step_avg:58.16ms
step:1634/1920 train_time:95063ms step_avg:58.18ms
step:1635/1920 train_time:95152ms step_avg:58.20ms
step:1636/1920 train_time:95240ms step_avg:58.22ms
step:1637/1920 train_time:95328ms step_avg:58.23ms
step:1638/1920 train_time:95417ms step_avg:58.25ms
step:1639/1920 train_time:95507ms step_avg:58.27ms
step:1640/1920 train_time:95593ms step_avg:58.29ms
step:1640/1920 val_loss:3.3597 train_time:95684ms step_avg:58.34ms
step:1641/1920 train_time:95701ms step_avg:58.32ms
step:1642/1920 train_time:95773ms step_avg:58.33ms
step:1643/1920 train_time:95866ms step_avg:58.35ms
step:1644/1920 train_time:95955ms step_avg:58.37ms
step:1645/1920 train_time:96043ms step_avg:58.38ms
step:1646/1920 train_time:96131ms step_avg:58.40ms
step:1647/1920 train_time:96219ms step_avg:58.42ms
step:1648/1920 train_time:96306ms step_avg:58.44ms
step:1649/1920 train_time:96394ms step_avg:58.46ms
step:1650/1920 train_time:96483ms step_avg:58.47ms
step:1650/1920 val_loss:3.3561 train_time:96574ms step_avg:58.53ms
step:1651/1920 train_time:96592ms step_avg:58.51ms
step:1652/1920 train_time:96663ms step_avg:58.51ms
step:1653/1920 train_time:96755ms step_avg:58.53ms
step:1654/1920 train_time:96844ms step_avg:58.55ms
step:1655/1920 train_time:96932ms step_avg:58.57ms
step:1656/1920 train_time:97020ms step_avg:58.59ms
step:1657/1920 train_time:97107ms step_avg:58.60ms
step:1658/1920 train_time:97195ms step_avg:58.62ms
step:1659/1920 train_time:97283ms step_avg:58.64ms
step:1660/1920 train_time:97371ms step_avg:58.66ms
step:1660/1920 val_loss:3.3521 train_time:97462ms step_avg:58.71ms
step:1661/1920 train_time:97479ms step_avg:58.69ms
step:1662/1920 train_time:97553ms step_avg:58.70ms
step:1663/1920 train_time:97647ms step_avg:58.72ms
step:1664/1920 train_time:97735ms step_avg:58.73ms
step:1665/1920 train_time:97823ms step_avg:58.75ms
step:1666/1920 train_time:97911ms step_avg:58.77ms
step:1667/1920 train_time:97998ms step_avg:58.79ms
step:1668/1920 train_time:98086ms step_avg:58.80ms
step:1669/1920 train_time:98174ms step_avg:58.82ms
step:1670/1920 train_time:98261ms step_avg:58.84ms
step:1670/1920 val_loss:3.3488 train_time:98352ms step_avg:58.89ms
step:1671/1920 train_time:98369ms step_avg:58.87ms
step:1672/1920 train_time:98441ms step_avg:58.88ms
step:1673/1920 train_time:98533ms step_avg:58.90ms
step:1674/1920 train_time:98622ms step_avg:58.91ms
step:1675/1920 train_time:98710ms step_avg:58.93ms
step:1676/1920 train_time:98798ms step_avg:58.95ms
step:1677/1920 train_time:98886ms step_avg:58.97ms
step:1678/1920 train_time:98974ms step_avg:58.98ms
step:1679/1920 train_time:99062ms step_avg:59.00ms
step:1680/1920 train_time:99150ms step_avg:59.02ms
step:1680/1920 val_loss:3.3460 train_time:99241ms step_avg:59.07ms
step:1681/1920 train_time:99259ms step_avg:59.05ms
step:1682/1920 train_time:99330ms step_avg:59.05ms
step:1683/1920 train_time:99421ms step_avg:59.07ms
step:1684/1920 train_time:99509ms step_avg:59.09ms
step:1685/1920 train_time:99598ms step_avg:59.11ms
step:1686/1920 train_time:99687ms step_avg:59.13ms
step:1687/1920 train_time:99774ms step_avg:59.14ms
step:1688/1920 train_time:99862ms step_avg:59.16ms
step:1689/1920 train_time:99951ms step_avg:59.18ms
step:1690/1920 train_time:100038ms step_avg:59.19ms
step:1690/1920 val_loss:3.3420 train_time:100129ms step_avg:59.25ms
step:1691/1920 train_time:100147ms step_avg:59.22ms
step:1692/1920 train_time:100219ms step_avg:59.23ms
step:1693/1920 train_time:100312ms step_avg:59.25ms
step:1694/1920 train_time:100401ms step_avg:59.27ms
step:1695/1920 train_time:100489ms step_avg:59.29ms
step:1696/1920 train_time:100577ms step_avg:59.30ms
step:1697/1920 train_time:100665ms step_avg:59.32ms
step:1698/1920 train_time:100752ms step_avg:59.34ms
step:1699/1920 train_time:100840ms step_avg:59.35ms
step:1700/1920 train_time:100928ms step_avg:59.37ms
step:1700/1920 val_loss:3.3384 train_time:101018ms step_avg:59.42ms
step:1701/1920 train_time:101035ms step_avg:59.40ms
step:1702/1920 train_time:101108ms step_avg:59.41ms
step:1703/1920 train_time:101197ms step_avg:59.42ms
step:1704/1920 train_time:101285ms step_avg:59.44ms
step:1705/1920 train_time:101374ms step_avg:59.46ms
step:1706/1920 train_time:101462ms step_avg:59.47ms
step:1707/1920 train_time:101551ms step_avg:59.49ms
step:1708/1920 train_time:101639ms step_avg:59.51ms
step:1709/1920 train_time:101726ms step_avg:59.52ms
step:1710/1920 train_time:101814ms step_avg:59.54ms
step:1710/1920 val_loss:3.3353 train_time:101906ms step_avg:59.59ms
step:1711/1920 train_time:101924ms step_avg:59.57ms
step:1712/1920 train_time:101994ms step_avg:59.58ms
step:1713/1920 train_time:102089ms step_avg:59.60ms
step:1714/1920 train_time:102177ms step_avg:59.61ms
step:1715/1920 train_time:102265ms step_avg:59.63ms
step:1716/1920 train_time:102351ms step_avg:59.65ms
step:1717/1920 train_time:102439ms step_avg:59.66ms
step:1718/1920 train_time:102526ms step_avg:59.68ms
step:1719/1920 train_time:102614ms step_avg:59.69ms
step:1720/1920 train_time:102702ms step_avg:59.71ms
step:1720/1920 val_loss:3.3317 train_time:102793ms step_avg:59.76ms
step:1721/1920 train_time:102810ms step_avg:59.74ms
step:1722/1920 train_time:102883ms step_avg:59.75ms
step:1723/1920 train_time:102973ms step_avg:59.76ms
step:1724/1920 train_time:103062ms step_avg:59.78ms
step:1725/1920 train_time:103150ms step_avg:59.80ms
step:1726/1920 train_time:103238ms step_avg:59.81ms
step:1727/1920 train_time:103326ms step_avg:59.83ms
step:1728/1920 train_time:103413ms step_avg:59.85ms
step:1729/1920 train_time:103501ms step_avg:59.86ms
step:1730/1920 train_time:103589ms step_avg:59.88ms
step:1730/1920 val_loss:3.3293 train_time:103679ms step_avg:59.93ms
step:1731/1920 train_time:103697ms step_avg:59.91ms
step:1732/1920 train_time:103768ms step_avg:59.91ms
step:1733/1920 train_time:103860ms step_avg:59.93ms
step:1734/1920 train_time:103949ms step_avg:59.95ms
step:1735/1920 train_time:104037ms step_avg:59.96ms
step:1736/1920 train_time:104125ms step_avg:59.98ms
step:1737/1920 train_time:104213ms step_avg:60.00ms
step:1738/1920 train_time:104301ms step_avg:60.01ms
step:1739/1920 train_time:104388ms step_avg:60.03ms
step:1740/1920 train_time:104476ms step_avg:60.04ms
step:1740/1920 val_loss:3.3263 train_time:104567ms step_avg:60.10ms
step:1741/1920 train_time:104584ms step_avg:60.07ms
step:1742/1920 train_time:104654ms step_avg:60.08ms
step:1743/1920 train_time:104745ms step_avg:60.09ms
step:1744/1920 train_time:104834ms step_avg:60.11ms
step:1745/1920 train_time:104922ms step_avg:60.13ms
step:1746/1920 train_time:105010ms step_avg:60.14ms
step:1747/1920 train_time:105098ms step_avg:60.16ms
step:1748/1920 train_time:105186ms step_avg:60.17ms
step:1749/1920 train_time:105274ms step_avg:60.19ms
step:1750/1920 train_time:105361ms step_avg:60.21ms
step:1750/1920 val_loss:3.3231 train_time:105451ms step_avg:60.26ms
step:1751/1920 train_time:105468ms step_avg:60.23ms
step:1752/1920 train_time:105539ms step_avg:60.24ms
step:1753/1920 train_time:105635ms step_avg:60.26ms
step:1754/1920 train_time:105723ms step_avg:60.28ms
step:1755/1920 train_time:105812ms step_avg:60.29ms
step:1756/1920 train_time:105900ms step_avg:60.31ms
step:1757/1920 train_time:105988ms step_avg:60.32ms
step:1758/1920 train_time:106075ms step_avg:60.34ms
step:1759/1920 train_time:106163ms step_avg:60.35ms
step:1760/1920 train_time:106252ms step_avg:60.37ms
step:1760/1920 val_loss:3.3200 train_time:106342ms step_avg:60.42ms
step:1761/1920 train_time:106359ms step_avg:60.40ms
step:1762/1920 train_time:106430ms step_avg:60.40ms
step:1763/1920 train_time:106523ms step_avg:60.42ms
step:1764/1920 train_time:106613ms step_avg:60.44ms
step:1765/1920 train_time:106701ms step_avg:60.45ms
step:1766/1920 train_time:106789ms step_avg:60.47ms
step:1767/1920 train_time:106877ms step_avg:60.49ms
step:1768/1920 train_time:106966ms step_avg:60.50ms
step:1769/1920 train_time:107053ms step_avg:60.52ms
step:1770/1920 train_time:107140ms step_avg:60.53ms
step:1770/1920 val_loss:3.3170 train_time:107231ms step_avg:60.58ms
step:1771/1920 train_time:107249ms step_avg:60.56ms
step:1772/1920 train_time:107321ms step_avg:60.57ms
step:1773/1920 train_time:107415ms step_avg:60.58ms
step:1774/1920 train_time:107503ms step_avg:60.60ms
step:1775/1920 train_time:107591ms step_avg:60.61ms
step:1776/1920 train_time:107679ms step_avg:60.63ms
step:1777/1920 train_time:107766ms step_avg:60.65ms
step:1778/1920 train_time:107854ms step_avg:60.66ms
step:1779/1920 train_time:107941ms step_avg:60.68ms
step:1780/1920 train_time:108029ms step_avg:60.69ms
step:1780/1920 val_loss:3.3141 train_time:108120ms step_avg:60.74ms
step:1781/1920 train_time:108137ms step_avg:60.72ms
step:1782/1920 train_time:108209ms step_avg:60.72ms
step:1783/1920 train_time:108299ms step_avg:60.74ms
step:1784/1920 train_time:108389ms step_avg:60.76ms
step:1785/1920 train_time:108478ms step_avg:60.77ms
step:1786/1920 train_time:108566ms step_avg:60.79ms
step:1787/1920 train_time:108653ms step_avg:60.80ms
step:1788/1920 train_time:108740ms step_avg:60.82ms
step:1789/1920 train_time:108828ms step_avg:60.83ms
step:1790/1920 train_time:108916ms step_avg:60.85ms
step:1790/1920 val_loss:3.3113 train_time:109007ms step_avg:60.90ms
step:1791/1920 train_time:109024ms step_avg:60.87ms
step:1792/1920 train_time:109095ms step_avg:60.88ms
step:1793/1920 train_time:109186ms step_avg:60.90ms
step:1794/1920 train_time:109274ms step_avg:60.91ms
step:1795/1920 train_time:109363ms step_avg:60.93ms
step:1796/1920 train_time:109450ms step_avg:60.94ms
step:1797/1920 train_time:109537ms step_avg:60.96ms
step:1798/1920 train_time:109626ms step_avg:60.97ms
step:1799/1920 train_time:109716ms step_avg:60.99ms
step:1800/1920 train_time:109803ms step_avg:61.00ms
step:1800/1920 val_loss:3.3084 train_time:109894ms step_avg:61.05ms
step:1801/1920 train_time:109912ms step_avg:61.03ms
step:1802/1920 train_time:109984ms step_avg:61.03ms
step:1803/1920 train_time:110074ms step_avg:61.05ms
step:1804/1920 train_time:110163ms step_avg:61.07ms
step:1805/1920 train_time:110251ms step_avg:61.08ms
step:1806/1920 train_time:110339ms step_avg:61.10ms
step:1807/1920 train_time:110427ms step_avg:61.11ms
step:1808/1920 train_time:110513ms step_avg:61.12ms
step:1809/1920 train_time:110602ms step_avg:61.14ms
step:1810/1920 train_time:110690ms step_avg:61.15ms
step:1810/1920 val_loss:3.3056 train_time:110781ms step_avg:61.20ms
step:1811/1920 train_time:110798ms step_avg:61.18ms
step:1812/1920 train_time:110871ms step_avg:61.19ms
step:1813/1920 train_time:110960ms step_avg:61.20ms
step:1814/1920 train_time:111050ms step_avg:61.22ms
step:1815/1920 train_time:111138ms step_avg:61.23ms
step:1816/1920 train_time:111225ms step_avg:61.25ms
step:1817/1920 train_time:111314ms step_avg:61.26ms
step:1818/1920 train_time:111403ms step_avg:61.28ms
step:1819/1920 train_time:111491ms step_avg:61.29ms
step:1820/1920 train_time:111580ms step_avg:61.31ms
step:1820/1920 val_loss:3.3034 train_time:111670ms step_avg:61.36ms
step:1821/1920 train_time:111688ms step_avg:61.33ms
step:1822/1920 train_time:111760ms step_avg:61.34ms
step:1823/1920 train_time:111852ms step_avg:61.36ms
step:1824/1920 train_time:111941ms step_avg:61.37ms
step:1825/1920 train_time:112029ms step_avg:61.39ms
step:1826/1920 train_time:112117ms step_avg:61.40ms
step:1827/1920 train_time:112204ms step_avg:61.41ms
step:1828/1920 train_time:112291ms step_avg:61.43ms
step:1829/1920 train_time:112379ms step_avg:61.44ms
step:1830/1920 train_time:112467ms step_avg:61.46ms
step:1830/1920 val_loss:3.3007 train_time:112557ms step_avg:61.51ms
step:1831/1920 train_time:112576ms step_avg:61.48ms
step:1832/1920 train_time:112645ms step_avg:61.49ms
step:1833/1920 train_time:112735ms step_avg:61.50ms
step:1834/1920 train_time:112824ms step_avg:61.52ms
step:1835/1920 train_time:112912ms step_avg:61.53ms
step:1836/1920 train_time:113000ms step_avg:61.55ms
step:1837/1920 train_time:113088ms step_avg:61.56ms
step:1838/1920 train_time:113176ms step_avg:61.58ms
step:1839/1920 train_time:113264ms step_avg:61.59ms
step:1840/1920 train_time:113351ms step_avg:61.60ms
step:1840/1920 val_loss:3.2981 train_time:113442ms step_avg:61.65ms
step:1841/1920 train_time:113459ms step_avg:61.63ms
step:1842/1920 train_time:113533ms step_avg:61.64ms
step:1843/1920 train_time:113624ms step_avg:61.65ms
step:1844/1920 train_time:113712ms step_avg:61.67ms
step:1845/1920 train_time:113800ms step_avg:61.68ms
step:1846/1920 train_time:113887ms step_avg:61.69ms
step:1847/1920 train_time:113975ms step_avg:61.71ms
step:1848/1920 train_time:114064ms step_avg:61.72ms
step:1849/1920 train_time:114151ms step_avg:61.74ms
step:1850/1920 train_time:114238ms step_avg:61.75ms
step:1850/1920 val_loss:3.2959 train_time:114330ms step_avg:61.80ms
step:1851/1920 train_time:114347ms step_avg:61.78ms
step:1852/1920 train_time:114420ms step_avg:61.78ms
step:1853/1920 train_time:114512ms step_avg:61.80ms
step:1854/1920 train_time:114601ms step_avg:61.81ms
step:1855/1920 train_time:114689ms step_avg:61.83ms
step:1856/1920 train_time:114777ms step_avg:61.84ms
step:1857/1920 train_time:114865ms step_avg:61.86ms
step:1858/1920 train_time:114953ms step_avg:61.87ms
step:1859/1920 train_time:115040ms step_avg:61.88ms
step:1860/1920 train_time:115128ms step_avg:61.90ms
step:1860/1920 val_loss:3.2936 train_time:115219ms step_avg:61.95ms
step:1861/1920 train_time:115237ms step_avg:61.92ms
step:1862/1920 train_time:115308ms step_avg:61.93ms
step:1863/1920 train_time:115399ms step_avg:61.94ms
step:1864/1920 train_time:115488ms step_avg:61.96ms
step:1865/1920 train_time:115576ms step_avg:61.97ms
step:1866/1920 train_time:115665ms step_avg:61.99ms
step:1867/1920 train_time:115754ms step_avg:62.00ms
step:1868/1920 train_time:115842ms step_avg:62.01ms
step:1869/1920 train_time:115931ms step_avg:62.03ms
step:1870/1920 train_time:116019ms step_avg:62.04ms
step:1870/1920 val_loss:3.2916 train_time:116110ms step_avg:62.09ms
step:1871/1920 train_time:116127ms step_avg:62.07ms
step:1872/1920 train_time:116199ms step_avg:62.07ms
step:1873/1920 train_time:116292ms step_avg:62.09ms
step:1874/1920 train_time:116380ms step_avg:62.10ms
step:1875/1920 train_time:116469ms step_avg:62.12ms
step:1876/1920 train_time:116557ms step_avg:62.13ms
step:1877/1920 train_time:116645ms step_avg:62.14ms
step:1878/1920 train_time:116732ms step_avg:62.16ms
step:1879/1920 train_time:116821ms step_avg:62.17ms
step:1880/1920 train_time:116908ms step_avg:62.19ms
step:1880/1920 val_loss:3.2880 train_time:116999ms step_avg:62.23ms
step:1881/1920 train_time:117018ms step_avg:62.21ms
step:1882/1920 train_time:117088ms step_avg:62.21ms
step:1883/1920 train_time:117180ms step_avg:62.23ms
step:1884/1920 train_time:117268ms step_avg:62.24ms
step:1885/1920 train_time:117356ms step_avg:62.26ms
step:1886/1920 train_time:117445ms step_avg:62.27ms
step:1887/1920 train_time:117533ms step_avg:62.29ms
step:1888/1920 train_time:117621ms step_avg:62.30ms
step:1889/1920 train_time:117709ms step_avg:62.31ms
step:1890/1920 train_time:117797ms step_avg:62.33ms
step:1890/1920 val_loss:3.2854 train_time:117888ms step_avg:62.37ms
step:1891/1920 train_time:117906ms step_avg:62.35ms
step:1892/1920 train_time:117980ms step_avg:62.36ms
step:1893/1920 train_time:118072ms step_avg:62.37ms
step:1894/1920 train_time:118160ms step_avg:62.39ms
step:1895/1920 train_time:118251ms step_avg:62.40ms
step:1896/1920 train_time:118339ms step_avg:62.41ms
step:1897/1920 train_time:118426ms step_avg:62.43ms
step:1898/1920 train_time:118515ms step_avg:62.44ms
step:1899/1920 train_time:118602ms step_avg:62.46ms
step:1900/1920 train_time:118689ms step_avg:62.47ms
step:1900/1920 val_loss:3.2838 train_time:118780ms step_avg:62.52ms
step:1901/1920 train_time:118798ms step_avg:62.49ms
step:1902/1920 train_time:118868ms step_avg:62.50ms
step:1903/1920 train_time:118958ms step_avg:62.51ms
step:1904/1920 train_time:119047ms step_avg:62.52ms
step:1905/1920 train_time:119136ms step_avg:62.54ms
step:1906/1920 train_time:119223ms step_avg:62.55ms
step:1907/1920 train_time:119311ms step_avg:62.57ms
step:1908/1920 train_time:119399ms step_avg:62.58ms
step:1909/1920 train_time:119487ms step_avg:62.59ms
step:1910/1920 train_time:119575ms step_avg:62.60ms
step:1910/1920 val_loss:3.2826 train_time:119666ms step_avg:62.65ms
step:1911/1920 train_time:119683ms step_avg:62.63ms
step:1912/1920 train_time:119755ms step_avg:62.63ms
step:1913/1920 train_time:119846ms step_avg:62.65ms
step:1914/1920 train_time:119934ms step_avg:62.66ms
step:1915/1920 train_time:120023ms step_avg:62.68ms
step:1916/1920 train_time:120110ms step_avg:62.69ms
step:1917/1920 train_time:120198ms step_avg:62.70ms
step:1918/1920 train_time:120286ms step_avg:62.71ms
step:1919/1920 train_time:120374ms step_avg:62.73ms
step:1920/1920 train_time:120462ms step_avg:62.74ms
step:1920/1920 val_loss:3.2786 train_time:120553ms step_avg:62.79ms
peak memory allocated: 29863 MiB reserved: 45098 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:05:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   44C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   43C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    227541      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    227542      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    227543      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    227544      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    227545      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    227546      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    227547      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    227548      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    227542      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    227543      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    227544      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    227545      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    227546      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    227547      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    227548      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8352 train_time:0ms step_avg:0.05ms
step:1/1920 train_time:85ms step_avg:85.31ms
step:2/1920 train_time:110ms step_avg:54.90ms
step:3/1920 train_time:129ms step_avg:43.01ms
step:4/1920 train_time:158ms step_avg:39.42ms
step:5/1920 train_time:192ms step_avg:38.34ms
step:6/1920 train_time:279ms step_avg:46.53ms
step:7/1920 train_time:298ms step_avg:42.55ms
step:8/1920 train_time:332ms step_avg:41.51ms
step:9/1920 train_time:366ms step_avg:40.70ms
step:10/1920 train_time:400ms step_avg:40.04ms
step:11/1920 train_time:435ms step_avg:39.52ms
step:12/1920 train_time:469ms step_avg:39.09ms
step:13/1920 train_time:504ms step_avg:38.76ms
step:14/1920 train_time:538ms step_avg:38.43ms
step:15/1920 train_time:573ms step_avg:38.18ms
step:16/1920 train_time:607ms step_avg:37.94ms
step:17/1920 train_time:642ms step_avg:37.74ms
step:18/1920 train_time:676ms step_avg:37.54ms
step:19/1920 train_time:711ms step_avg:37.40ms
step:20/1920 train_time:745ms step_avg:37.24ms
step:21/1920 train_time:779ms step_avg:37.10ms
step:22/1920 train_time:813ms step_avg:36.97ms
step:23/1920 train_time:848ms step_avg:36.87ms
step:24/1920 train_time:882ms step_avg:36.76ms
step:25/1920 train_time:917ms step_avg:36.66ms
step:26/1920 train_time:951ms step_avg:36.57ms
step:27/1920 train_time:985ms step_avg:36.49ms
step:28/1920 train_time:1020ms step_avg:36.42ms
step:29/1920 train_time:1054ms step_avg:36.34ms
step:30/1920 train_time:1088ms step_avg:36.27ms
step:31/1920 train_time:1123ms step_avg:36.21ms
step:32/1920 train_time:1157ms step_avg:36.15ms
step:33/1920 train_time:1192ms step_avg:36.11ms
step:34/1920 train_time:1226ms step_avg:36.07ms
step:35/1920 train_time:1261ms step_avg:36.03ms
step:36/1920 train_time:1295ms step_avg:35.99ms
step:37/1920 train_time:1331ms step_avg:35.97ms
step:38/1920 train_time:1366ms step_avg:35.94ms
step:39/1920 train_time:1401ms step_avg:35.91ms
step:40/1920 train_time:1435ms step_avg:35.87ms
step:41/1920 train_time:1470ms step_avg:35.85ms
step:42/1920 train_time:1504ms step_avg:35.81ms
step:43/1920 train_time:1539ms step_avg:35.78ms
step:44/1920 train_time:1573ms step_avg:35.76ms
step:45/1920 train_time:1608ms step_avg:35.74ms
step:46/1920 train_time:1643ms step_avg:35.71ms
step:47/1920 train_time:1677ms step_avg:35.69ms
step:48/1920 train_time:1712ms step_avg:35.66ms
step:49/1920 train_time:1747ms step_avg:35.65ms
step:50/1920 train_time:1781ms step_avg:35.62ms
step:51/1920 train_time:1816ms step_avg:35.60ms
step:52/1920 train_time:1850ms step_avg:35.57ms
step:53/1920 train_time:1884ms step_avg:35.55ms
step:54/1920 train_time:1919ms step_avg:35.53ms
step:55/1920 train_time:1953ms step_avg:35.51ms
step:56/1920 train_time:1987ms step_avg:35.49ms
step:57/1920 train_time:2022ms step_avg:35.47ms
step:58/1920 train_time:2056ms step_avg:35.45ms
step:59/1920 train_time:2091ms step_avg:35.43ms
step:60/1920 train_time:2125ms step_avg:35.41ms
step:61/1920 train_time:2160ms step_avg:35.40ms
step:62/1920 train_time:2194ms step_avg:35.39ms
step:63/1920 train_time:2229ms step_avg:35.38ms
step:64/1920 train_time:2263ms step_avg:35.36ms
step:65/1920 train_time:2298ms step_avg:35.35ms
step:66/1920 train_time:2333ms step_avg:35.34ms
step:67/1920 train_time:2367ms step_avg:35.33ms
step:68/1920 train_time:2401ms step_avg:35.32ms
step:69/1920 train_time:2436ms step_avg:35.30ms
step:70/1920 train_time:2470ms step_avg:35.29ms
step:71/1920 train_time:2505ms step_avg:35.28ms
step:72/1920 train_time:2540ms step_avg:35.27ms
step:73/1920 train_time:2574ms step_avg:35.26ms
step:74/1920 train_time:2609ms step_avg:35.25ms
step:75/1920 train_time:2643ms step_avg:35.24ms
step:76/1920 train_time:2678ms step_avg:35.23ms
step:77/1920 train_time:2713ms step_avg:35.23ms
step:78/1920 train_time:2747ms step_avg:35.22ms
step:79/1920 train_time:2781ms step_avg:35.21ms
step:80/1920 train_time:2816ms step_avg:35.19ms
step:81/1920 train_time:2850ms step_avg:35.19ms
step:82/1920 train_time:2885ms step_avg:35.18ms
step:83/1920 train_time:2919ms step_avg:35.17ms
step:84/1920 train_time:2953ms step_avg:35.16ms
step:85/1920 train_time:2988ms step_avg:35.15ms
step:86/1920 train_time:3022ms step_avg:35.14ms
step:87/1920 train_time:3057ms step_avg:35.13ms
step:88/1920 train_time:3091ms step_avg:35.12ms
step:89/1920 train_time:3126ms step_avg:35.12ms
step:90/1920 train_time:3160ms step_avg:35.11ms
step:91/1920 train_time:3195ms step_avg:35.11ms
step:92/1920 train_time:3229ms step_avg:35.10ms
step:93/1920 train_time:3263ms step_avg:35.09ms
step:94/1920 train_time:3298ms step_avg:35.08ms
step:95/1920 train_time:3332ms step_avg:35.08ms
step:96/1920 train_time:3367ms step_avg:35.07ms
step:97/1920 train_time:3402ms step_avg:35.07ms
step:98/1920 train_time:3436ms step_avg:35.06ms
step:99/1920 train_time:3471ms step_avg:35.06ms
step:100/1920 train_time:3505ms step_avg:35.05ms
step:101/1920 train_time:3539ms step_avg:35.04ms
step:102/1920 train_time:3573ms step_avg:35.03ms
step:103/1920 train_time:3608ms step_avg:35.03ms
step:104/1920 train_time:3642ms step_avg:35.02ms
step:105/1920 train_time:3677ms step_avg:35.02ms
step:106/1920 train_time:3711ms step_avg:35.01ms
step:107/1920 train_time:3746ms step_avg:35.01ms
step:108/1920 train_time:3780ms step_avg:35.00ms
step:109/1920 train_time:3815ms step_avg:35.00ms
step:110/1920 train_time:3849ms step_avg:34.99ms
step:111/1920 train_time:3883ms step_avg:34.98ms
step:112/1920 train_time:3918ms step_avg:34.98ms
step:113/1920 train_time:3952ms step_avg:34.97ms
step:114/1920 train_time:3986ms step_avg:34.97ms
step:115/1920 train_time:4021ms step_avg:34.96ms
step:116/1920 train_time:4055ms step_avg:34.96ms
step:117/1920 train_time:4089ms step_avg:34.95ms
step:118/1920 train_time:4124ms step_avg:34.95ms
step:119/1920 train_time:4158ms step_avg:34.94ms
step:120/1920 train_time:4192ms step_avg:34.93ms
step:121/1920 train_time:4227ms step_avg:34.93ms
step:122/1920 train_time:4261ms step_avg:34.92ms
step:123/1920 train_time:4295ms step_avg:34.92ms
step:124/1920 train_time:4329ms step_avg:34.91ms
step:125/1920 train_time:4364ms step_avg:34.91ms
step:126/1920 train_time:4398ms step_avg:34.91ms
step:127/1920 train_time:4433ms step_avg:34.91ms
step:128/1920 train_time:4468ms step_avg:34.90ms
step:129/1920 train_time:4502ms step_avg:34.90ms
step:130/1920 train_time:4537ms step_avg:34.90ms
step:131/1920 train_time:4571ms step_avg:34.89ms
step:132/1920 train_time:4605ms step_avg:34.89ms
step:133/1920 train_time:4639ms step_avg:34.88ms
step:134/1920 train_time:4674ms step_avg:34.88ms
step:135/1920 train_time:4709ms step_avg:34.88ms
step:136/1920 train_time:4743ms step_avg:34.88ms
step:137/1920 train_time:4777ms step_avg:34.87ms
step:138/1920 train_time:4812ms step_avg:34.87ms
step:139/1920 train_time:4846ms step_avg:34.87ms
step:140/1920 train_time:4881ms step_avg:34.86ms
step:141/1920 train_time:4915ms step_avg:34.86ms
step:142/1920 train_time:4950ms step_avg:34.86ms
step:143/1920 train_time:4984ms step_avg:34.85ms
step:144/1920 train_time:5018ms step_avg:34.85ms
step:145/1920 train_time:5053ms step_avg:34.85ms
step:146/1920 train_time:5087ms step_avg:34.84ms
step:147/1920 train_time:5121ms step_avg:34.84ms
step:148/1920 train_time:5155ms step_avg:34.83ms
step:149/1920 train_time:5190ms step_avg:34.83ms
step:150/1920 train_time:5224ms step_avg:34.83ms
step:151/1920 train_time:5259ms step_avg:34.83ms
step:152/1920 train_time:5293ms step_avg:34.82ms
step:153/1920 train_time:5328ms step_avg:34.82ms
step:154/1920 train_time:5362ms step_avg:34.82ms
step:155/1920 train_time:5396ms step_avg:34.82ms
step:156/1920 train_time:5431ms step_avg:34.81ms
step:157/1920 train_time:5466ms step_avg:34.82ms
step:158/1920 train_time:5500ms step_avg:34.81ms
step:159/1920 train_time:5535ms step_avg:34.81ms
step:160/1920 train_time:5570ms step_avg:34.81ms
step:161/1920 train_time:5604ms step_avg:34.81ms
step:162/1920 train_time:5638ms step_avg:34.81ms
step:163/1920 train_time:5673ms step_avg:34.80ms
step:164/1920 train_time:5707ms step_avg:34.80ms
step:165/1920 train_time:5742ms step_avg:34.80ms
step:166/1920 train_time:5776ms step_avg:34.80ms
step:167/1920 train_time:5811ms step_avg:34.80ms
step:168/1920 train_time:5845ms step_avg:34.79ms
step:169/1920 train_time:5880ms step_avg:34.79ms
step:170/1920 train_time:5914ms step_avg:34.79ms
step:171/1920 train_time:5949ms step_avg:34.79ms
step:172/1920 train_time:5983ms step_avg:34.78ms
step:173/1920 train_time:6017ms step_avg:34.78ms
step:174/1920 train_time:6052ms step_avg:34.78ms
step:175/1920 train_time:6086ms step_avg:34.78ms
step:176/1920 train_time:6120ms step_avg:34.78ms
step:177/1920 train_time:6155ms step_avg:34.77ms
step:178/1920 train_time:6189ms step_avg:34.77ms
step:179/1920 train_time:6223ms step_avg:34.77ms
step:180/1920 train_time:6258ms step_avg:34.76ms
step:181/1920 train_time:6292ms step_avg:34.76ms
step:182/1920 train_time:6327ms step_avg:34.76ms
step:183/1920 train_time:6361ms step_avg:34.76ms
step:184/1920 train_time:6395ms step_avg:34.76ms
step:185/1920 train_time:6430ms step_avg:34.75ms
step:186/1920 train_time:6464ms step_avg:34.75ms
step:187/1920 train_time:6498ms step_avg:34.75ms
step:188/1920 train_time:6533ms step_avg:34.75ms
step:189/1920 train_time:6567ms step_avg:34.75ms
step:190/1920 train_time:6602ms step_avg:34.74ms
step:191/1920 train_time:6636ms step_avg:34.74ms
step:192/1920 train_time:6670ms step_avg:34.74ms
step:193/1920 train_time:6705ms step_avg:34.74ms
step:194/1920 train_time:6739ms step_avg:34.74ms
step:195/1920 train_time:6773ms step_avg:34.74ms
step:196/1920 train_time:6808ms step_avg:34.73ms
step:197/1920 train_time:6842ms step_avg:34.73ms
step:198/1920 train_time:6876ms step_avg:34.73ms
step:199/1920 train_time:6911ms step_avg:34.73ms
step:200/1920 train_time:6945ms step_avg:34.73ms
step:201/1920 train_time:6979ms step_avg:34.72ms
step:202/1920 train_time:7013ms step_avg:34.72ms
step:203/1920 train_time:7048ms step_avg:34.72ms
step:204/1920 train_time:7082ms step_avg:34.72ms
step:205/1920 train_time:7117ms step_avg:34.72ms
step:206/1920 train_time:7151ms step_avg:34.71ms
step:207/1920 train_time:7186ms step_avg:34.71ms
step:208/1920 train_time:7220ms step_avg:34.71ms
step:209/1920 train_time:7254ms step_avg:34.71ms
step:210/1920 train_time:7289ms step_avg:34.71ms
step:211/1920 train_time:7323ms step_avg:34.70ms
step:212/1920 train_time:7357ms step_avg:34.70ms
step:213/1920 train_time:7391ms step_avg:34.70ms
step:214/1920 train_time:7425ms step_avg:34.70ms
step:215/1920 train_time:7459ms step_avg:34.69ms
step:216/1920 train_time:7494ms step_avg:34.69ms
step:217/1920 train_time:7528ms step_avg:34.69ms
step:218/1920 train_time:7562ms step_avg:34.69ms
step:219/1920 train_time:7597ms step_avg:34.69ms
step:220/1920 train_time:7631ms step_avg:34.69ms
step:221/1920 train_time:7665ms step_avg:34.68ms
step:222/1920 train_time:7699ms step_avg:34.68ms
step:223/1920 train_time:7734ms step_avg:34.68ms
step:224/1920 train_time:7768ms step_avg:34.68ms
step:225/1920 train_time:7802ms step_avg:34.68ms
step:226/1920 train_time:7836ms step_avg:34.67ms
step:227/1920 train_time:7871ms step_avg:34.67ms
step:228/1920 train_time:7905ms step_avg:34.67ms
step:229/1920 train_time:7940ms step_avg:34.67ms
step:230/1920 train_time:7974ms step_avg:34.67ms
step:231/1920 train_time:8009ms step_avg:34.67ms
step:232/1920 train_time:8043ms step_avg:34.67ms
step:233/1920 train_time:8077ms step_avg:34.67ms
step:234/1920 train_time:8112ms step_avg:34.67ms
step:235/1920 train_time:8146ms step_avg:34.66ms
step:236/1920 train_time:8180ms step_avg:34.66ms
step:237/1920 train_time:8214ms step_avg:34.66ms
step:238/1920 train_time:8249ms step_avg:34.66ms
step:239/1920 train_time:8283ms step_avg:34.66ms
step:240/1920 train_time:8317ms step_avg:34.66ms
step:241/1920 train_time:8352ms step_avg:34.66ms
step:242/1920 train_time:8386ms step_avg:34.65ms
step:243/1920 train_time:8420ms step_avg:34.65ms
step:244/1920 train_time:8454ms step_avg:34.65ms
step:245/1920 train_time:8489ms step_avg:34.65ms
step:246/1920 train_time:8523ms step_avg:34.65ms
step:247/1920 train_time:8558ms step_avg:34.65ms
step:248/1920 train_time:8592ms step_avg:34.64ms
step:249/1920 train_time:8627ms step_avg:34.65ms
step:250/1920 train_time:8661ms step_avg:34.64ms
step:250/1920 val_loss:4.6218 train_time:8698ms step_avg:34.79ms
step:251/1920 train_time:8716ms step_avg:34.72ms
step:252/1920 train_time:8733ms step_avg:34.66ms
step:253/1920 train_time:8769ms step_avg:34.66ms
step:254/1920 train_time:8803ms step_avg:34.66ms
step:255/1920 train_time:8840ms step_avg:34.67ms
step:256/1920 train_time:8875ms step_avg:34.67ms
step:257/1920 train_time:8911ms step_avg:34.67ms
step:258/1920 train_time:8945ms step_avg:34.67ms
step:259/1920 train_time:8980ms step_avg:34.67ms
step:260/1920 train_time:9014ms step_avg:34.67ms
step:261/1920 train_time:9049ms step_avg:34.67ms
step:262/1920 train_time:9083ms step_avg:34.67ms
step:263/1920 train_time:9117ms step_avg:34.67ms
step:264/1920 train_time:9151ms step_avg:34.66ms
step:265/1920 train_time:9186ms step_avg:34.66ms
step:266/1920 train_time:9220ms step_avg:34.66ms
step:267/1920 train_time:9254ms step_avg:34.66ms
step:268/1920 train_time:9288ms step_avg:34.66ms
step:269/1920 train_time:9322ms step_avg:34.65ms
step:270/1920 train_time:9356ms step_avg:34.65ms
step:271/1920 train_time:9390ms step_avg:34.65ms
step:272/1920 train_time:9424ms step_avg:34.65ms
step:273/1920 train_time:9458ms step_avg:34.65ms
step:274/1920 train_time:9493ms step_avg:34.64ms
step:275/1920 train_time:9527ms step_avg:34.64ms
step:276/1920 train_time:9561ms step_avg:34.64ms
step:277/1920 train_time:9595ms step_avg:34.64ms
step:278/1920 train_time:9629ms step_avg:34.64ms
step:279/1920 train_time:9664ms step_avg:34.64ms
step:280/1920 train_time:9698ms step_avg:34.64ms
step:281/1920 train_time:9733ms step_avg:34.64ms
step:282/1920 train_time:9767ms step_avg:34.64ms
step:283/1920 train_time:9802ms step_avg:34.64ms
step:284/1920 train_time:9837ms step_avg:34.64ms
step:285/1920 train_time:9871ms step_avg:34.63ms
step:286/1920 train_time:9905ms step_avg:34.63ms
step:287/1920 train_time:9940ms step_avg:34.63ms
step:288/1920 train_time:9975ms step_avg:34.63ms
step:289/1920 train_time:10009ms step_avg:34.63ms
step:290/1920 train_time:10043ms step_avg:34.63ms
step:291/1920 train_time:10078ms step_avg:34.63ms
step:292/1920 train_time:10112ms step_avg:34.63ms
step:293/1920 train_time:10147ms step_avg:34.63ms
step:294/1920 train_time:10181ms step_avg:34.63ms
step:295/1920 train_time:10215ms step_avg:34.63ms
step:296/1920 train_time:10249ms step_avg:34.63ms
step:297/1920 train_time:10284ms step_avg:34.63ms
step:298/1920 train_time:10318ms step_avg:34.62ms
step:299/1920 train_time:10352ms step_avg:34.62ms
step:300/1920 train_time:10386ms step_avg:34.62ms
step:301/1920 train_time:10421ms step_avg:34.62ms
step:302/1920 train_time:10455ms step_avg:34.62ms
step:303/1920 train_time:10489ms step_avg:34.62ms
step:304/1920 train_time:10523ms step_avg:34.62ms
step:305/1920 train_time:10558ms step_avg:34.62ms
step:306/1920 train_time:10592ms step_avg:34.61ms
step:307/1920 train_time:10626ms step_avg:34.61ms
step:308/1920 train_time:10661ms step_avg:34.61ms
step:309/1920 train_time:10695ms step_avg:34.61ms
step:310/1920 train_time:10729ms step_avg:34.61ms
step:311/1920 train_time:10764ms step_avg:34.61ms
step:312/1920 train_time:10798ms step_avg:34.61ms
step:313/1920 train_time:10833ms step_avg:34.61ms
step:314/1920 train_time:10867ms step_avg:34.61ms
step:315/1920 train_time:10902ms step_avg:34.61ms
step:316/1920 train_time:10936ms step_avg:34.61ms
step:317/1920 train_time:10971ms step_avg:34.61ms
step:318/1920 train_time:11005ms step_avg:34.61ms
step:319/1920 train_time:11040ms step_avg:34.61ms
step:320/1920 train_time:11074ms step_avg:34.61ms
step:321/1920 train_time:11108ms step_avg:34.60ms
step:322/1920 train_time:11142ms step_avg:34.60ms
step:323/1920 train_time:11177ms step_avg:34.60ms
step:324/1920 train_time:11211ms step_avg:34.60ms
step:325/1920 train_time:11245ms step_avg:34.60ms
step:326/1920 train_time:11280ms step_avg:34.60ms
step:327/1920 train_time:11314ms step_avg:34.60ms
step:328/1920 train_time:11348ms step_avg:34.60ms
step:329/1920 train_time:11382ms step_avg:34.60ms
step:330/1920 train_time:11416ms step_avg:34.60ms
step:331/1920 train_time:11451ms step_avg:34.59ms
step:332/1920 train_time:11485ms step_avg:34.59ms
step:333/1920 train_time:11519ms step_avg:34.59ms
step:334/1920 train_time:11554ms step_avg:34.59ms
step:335/1920 train_time:11588ms step_avg:34.59ms
step:336/1920 train_time:11622ms step_avg:34.59ms
step:337/1920 train_time:11657ms step_avg:34.59ms
step:338/1920 train_time:11691ms step_avg:34.59ms
step:339/1920 train_time:11726ms step_avg:34.59ms
step:340/1920 train_time:11760ms step_avg:34.59ms
step:341/1920 train_time:11795ms step_avg:34.59ms
step:342/1920 train_time:11829ms step_avg:34.59ms
step:343/1920 train_time:11863ms step_avg:34.59ms
step:344/1920 train_time:11898ms step_avg:34.59ms
step:345/1920 train_time:11932ms step_avg:34.59ms
step:346/1920 train_time:11966ms step_avg:34.58ms
step:347/1920 train_time:12001ms step_avg:34.58ms
step:348/1920 train_time:12035ms step_avg:34.58ms
step:349/1920 train_time:12069ms step_avg:34.58ms
step:350/1920 train_time:12104ms step_avg:34.58ms
step:351/1920 train_time:12138ms step_avg:34.58ms
step:352/1920 train_time:12173ms step_avg:34.58ms
step:353/1920 train_time:12207ms step_avg:34.58ms
step:354/1920 train_time:12241ms step_avg:34.58ms
step:355/1920 train_time:12276ms step_avg:34.58ms
step:356/1920 train_time:12310ms step_avg:34.58ms
step:357/1920 train_time:12345ms step_avg:34.58ms
step:358/1920 train_time:12379ms step_avg:34.58ms
step:359/1920 train_time:12413ms step_avg:34.58ms
step:360/1920 train_time:12447ms step_avg:34.58ms
step:361/1920 train_time:12482ms step_avg:34.58ms
step:362/1920 train_time:12516ms step_avg:34.58ms
step:363/1920 train_time:12551ms step_avg:34.57ms
step:364/1920 train_time:12585ms step_avg:34.57ms
step:365/1920 train_time:12619ms step_avg:34.57ms
step:366/1920 train_time:12653ms step_avg:34.57ms
step:367/1920 train_time:12688ms step_avg:34.57ms
step:368/1920 train_time:12722ms step_avg:34.57ms
step:369/1920 train_time:12757ms step_avg:34.57ms
step:370/1920 train_time:12791ms step_avg:34.57ms
step:371/1920 train_time:12826ms step_avg:34.57ms
step:372/1920 train_time:12860ms step_avg:34.57ms
step:373/1920 train_time:12894ms step_avg:34.57ms
step:374/1920 train_time:12928ms step_avg:34.57ms
step:375/1920 train_time:12962ms step_avg:34.57ms
step:376/1920 train_time:12997ms step_avg:34.57ms
step:377/1920 train_time:13031ms step_avg:34.56ms
step:378/1920 train_time:13065ms step_avg:34.56ms
step:379/1920 train_time:13100ms step_avg:34.56ms
step:380/1920 train_time:13134ms step_avg:34.56ms
step:381/1920 train_time:13168ms step_avg:34.56ms
step:382/1920 train_time:13203ms step_avg:34.56ms
step:383/1920 train_time:13237ms step_avg:34.56ms
step:384/1920 train_time:13271ms step_avg:34.56ms
step:385/1920 train_time:13306ms step_avg:34.56ms
step:386/1920 train_time:13340ms step_avg:34.56ms
step:387/1920 train_time:13374ms step_avg:34.56ms
step:388/1920 train_time:13408ms step_avg:34.56ms
step:389/1920 train_time:13443ms step_avg:34.56ms
step:390/1920 train_time:13477ms step_avg:34.56ms
step:391/1920 train_time:13512ms step_avg:34.56ms
step:392/1920 train_time:13546ms step_avg:34.56ms
step:393/1920 train_time:13581ms step_avg:34.56ms
step:394/1920 train_time:13615ms step_avg:34.56ms
step:395/1920 train_time:13650ms step_avg:34.56ms
step:396/1920 train_time:13684ms step_avg:34.55ms
step:397/1920 train_time:13718ms step_avg:34.55ms
step:398/1920 train_time:13752ms step_avg:34.55ms
step:399/1920 train_time:13787ms step_avg:34.55ms
step:400/1920 train_time:13821ms step_avg:34.55ms
step:401/1920 train_time:13855ms step_avg:34.55ms
step:402/1920 train_time:13890ms step_avg:34.55ms
step:403/1920 train_time:13924ms step_avg:34.55ms
step:404/1920 train_time:13958ms step_avg:34.55ms
step:405/1920 train_time:13993ms step_avg:34.55ms
step:406/1920 train_time:14027ms step_avg:34.55ms
step:407/1920 train_time:14061ms step_avg:34.55ms
step:408/1920 train_time:14095ms step_avg:34.55ms
step:409/1920 train_time:14130ms step_avg:34.55ms
step:410/1920 train_time:14164ms step_avg:34.55ms
step:411/1920 train_time:14199ms step_avg:34.55ms
step:412/1920 train_time:14233ms step_avg:34.55ms
step:413/1920 train_time:14267ms step_avg:34.55ms
step:414/1920 train_time:14302ms step_avg:34.55ms
step:415/1920 train_time:14336ms step_avg:34.54ms
step:416/1920 train_time:14370ms step_avg:34.54ms
step:417/1920 train_time:14405ms step_avg:34.54ms
step:418/1920 train_time:14439ms step_avg:34.54ms
step:419/1920 train_time:14473ms step_avg:34.54ms
step:420/1920 train_time:14507ms step_avg:34.54ms
step:421/1920 train_time:14542ms step_avg:34.54ms
step:422/1920 train_time:14576ms step_avg:34.54ms
step:423/1920 train_time:14610ms step_avg:34.54ms
step:424/1920 train_time:14644ms step_avg:34.54ms
step:425/1920 train_time:14679ms step_avg:34.54ms
step:426/1920 train_time:14713ms step_avg:34.54ms
step:427/1920 train_time:14748ms step_avg:34.54ms
step:428/1920 train_time:14782ms step_avg:34.54ms
step:429/1920 train_time:14816ms step_avg:34.54ms
step:430/1920 train_time:14850ms step_avg:34.54ms
step:431/1920 train_time:14885ms step_avg:34.53ms
step:432/1920 train_time:14919ms step_avg:34.53ms
step:433/1920 train_time:14953ms step_avg:34.53ms
step:434/1920 train_time:14987ms step_avg:34.53ms
step:435/1920 train_time:15022ms step_avg:34.53ms
step:436/1920 train_time:15056ms step_avg:34.53ms
step:437/1920 train_time:15091ms step_avg:34.53ms
step:438/1920 train_time:15125ms step_avg:34.53ms
step:439/1920 train_time:15160ms step_avg:34.53ms
step:440/1920 train_time:15194ms step_avg:34.53ms
step:441/1920 train_time:15229ms step_avg:34.53ms
step:442/1920 train_time:15263ms step_avg:34.53ms
step:443/1920 train_time:15298ms step_avg:34.53ms
step:444/1920 train_time:15332ms step_avg:34.53ms
step:445/1920 train_time:15366ms step_avg:34.53ms
step:446/1920 train_time:15401ms step_avg:34.53ms
step:447/1920 train_time:15435ms step_avg:34.53ms
step:448/1920 train_time:15469ms step_avg:34.53ms
step:449/1920 train_time:15504ms step_avg:34.53ms
step:450/1920 train_time:15538ms step_avg:34.53ms
step:451/1920 train_time:15572ms step_avg:34.53ms
step:452/1920 train_time:15607ms step_avg:34.53ms
step:453/1920 train_time:15641ms step_avg:34.53ms
step:454/1920 train_time:15676ms step_avg:34.53ms
step:455/1920 train_time:15710ms step_avg:34.53ms
step:456/1920 train_time:15744ms step_avg:34.53ms
step:457/1920 train_time:15779ms step_avg:34.53ms
step:458/1920 train_time:15813ms step_avg:34.53ms
step:459/1920 train_time:15848ms step_avg:34.53ms
step:460/1920 train_time:15882ms step_avg:34.53ms
step:461/1920 train_time:15917ms step_avg:34.53ms
step:462/1920 train_time:15951ms step_avg:34.53ms
step:463/1920 train_time:15985ms step_avg:34.53ms
step:464/1920 train_time:16020ms step_avg:34.52ms
step:465/1920 train_time:16054ms step_avg:34.52ms
step:466/1920 train_time:16088ms step_avg:34.52ms
step:467/1920 train_time:16123ms step_avg:34.52ms
step:468/1920 train_time:16157ms step_avg:34.52ms
step:469/1920 train_time:16191ms step_avg:34.52ms
step:470/1920 train_time:16226ms step_avg:34.52ms
step:471/1920 train_time:16261ms step_avg:34.52ms
step:472/1920 train_time:16295ms step_avg:34.52ms
step:473/1920 train_time:16329ms step_avg:34.52ms
step:474/1920 train_time:16364ms step_avg:34.52ms
step:475/1920 train_time:16398ms step_avg:34.52ms
step:476/1920 train_time:16433ms step_avg:34.52ms
step:477/1920 train_time:16467ms step_avg:34.52ms
step:478/1920 train_time:16501ms step_avg:34.52ms
step:479/1920 train_time:16536ms step_avg:34.52ms
step:480/1920 train_time:16570ms step_avg:34.52ms
step:481/1920 train_time:16604ms step_avg:34.52ms
step:482/1920 train_time:16638ms step_avg:34.52ms
step:483/1920 train_time:16673ms step_avg:34.52ms
step:484/1920 train_time:16707ms step_avg:34.52ms
step:485/1920 train_time:16741ms step_avg:34.52ms
step:486/1920 train_time:16776ms step_avg:34.52ms
step:487/1920 train_time:16810ms step_avg:34.52ms
step:488/1920 train_time:16844ms step_avg:34.52ms
step:489/1920 train_time:16879ms step_avg:34.52ms
step:490/1920 train_time:16913ms step_avg:34.52ms
step:491/1920 train_time:16948ms step_avg:34.52ms
step:492/1920 train_time:16982ms step_avg:34.52ms
step:493/1920 train_time:17016ms step_avg:34.52ms
step:494/1920 train_time:17050ms step_avg:34.52ms
step:495/1920 train_time:17085ms step_avg:34.51ms
step:496/1920 train_time:17119ms step_avg:34.51ms
step:497/1920 train_time:17153ms step_avg:34.51ms
step:498/1920 train_time:17188ms step_avg:34.51ms
step:499/1920 train_time:17222ms step_avg:34.51ms
step:500/1920 train_time:17256ms step_avg:34.51ms
step:500/1920 val_loss:4.2806 train_time:17294ms step_avg:34.59ms
step:501/1920 train_time:17312ms step_avg:34.55ms
step:502/1920 train_time:17330ms step_avg:34.52ms
step:503/1920 train_time:17364ms step_avg:34.52ms
step:504/1920 train_time:17400ms step_avg:34.52ms
step:505/1920 train_time:17436ms step_avg:34.53ms
step:506/1920 train_time:17471ms step_avg:34.53ms
step:507/1920 train_time:17506ms step_avg:34.53ms
step:508/1920 train_time:17541ms step_avg:34.53ms
step:509/1920 train_time:17575ms step_avg:34.53ms
step:510/1920 train_time:17610ms step_avg:34.53ms
step:511/1920 train_time:17644ms step_avg:34.53ms
step:512/1920 train_time:17679ms step_avg:34.53ms
step:513/1920 train_time:17713ms step_avg:34.53ms
step:514/1920 train_time:17747ms step_avg:34.53ms
step:515/1920 train_time:17782ms step_avg:34.53ms
step:516/1920 train_time:17816ms step_avg:34.53ms
step:517/1920 train_time:17850ms step_avg:34.53ms
step:518/1920 train_time:17884ms step_avg:34.53ms
step:519/1920 train_time:17918ms step_avg:34.52ms
step:520/1920 train_time:17952ms step_avg:34.52ms
step:521/1920 train_time:17987ms step_avg:34.52ms
step:522/1920 train_time:18021ms step_avg:34.52ms
step:523/1920 train_time:18055ms step_avg:34.52ms
step:524/1920 train_time:18089ms step_avg:34.52ms
step:525/1920 train_time:18123ms step_avg:34.52ms
step:526/1920 train_time:18157ms step_avg:34.52ms
step:527/1920 train_time:18191ms step_avg:34.52ms
step:528/1920 train_time:18226ms step_avg:34.52ms
step:529/1920 train_time:18260ms step_avg:34.52ms
step:530/1920 train_time:18294ms step_avg:34.52ms
step:531/1920 train_time:18329ms step_avg:34.52ms
step:532/1920 train_time:18364ms step_avg:34.52ms
step:533/1920 train_time:18398ms step_avg:34.52ms
step:534/1920 train_time:18433ms step_avg:34.52ms
step:535/1920 train_time:18468ms step_avg:34.52ms
step:536/1920 train_time:18502ms step_avg:34.52ms
step:537/1920 train_time:18537ms step_avg:34.52ms
step:538/1920 train_time:18572ms step_avg:34.52ms
step:539/1920 train_time:18607ms step_avg:34.52ms
step:540/1920 train_time:18641ms step_avg:34.52ms
step:541/1920 train_time:18676ms step_avg:34.52ms
step:542/1920 train_time:18710ms step_avg:34.52ms
step:543/1920 train_time:18744ms step_avg:34.52ms
step:544/1920 train_time:18779ms step_avg:34.52ms
step:545/1920 train_time:18813ms step_avg:34.52ms
step:546/1920 train_time:18847ms step_avg:34.52ms
step:547/1920 train_time:18882ms step_avg:34.52ms
step:548/1920 train_time:18916ms step_avg:34.52ms
step:549/1920 train_time:18950ms step_avg:34.52ms
step:550/1920 train_time:18984ms step_avg:34.52ms
step:551/1920 train_time:19019ms step_avg:34.52ms
step:552/1920 train_time:19053ms step_avg:34.52ms
step:553/1920 train_time:19087ms step_avg:34.52ms
step:554/1920 train_time:19121ms step_avg:34.51ms
step:555/1920 train_time:19155ms step_avg:34.51ms
step:556/1920 train_time:19190ms step_avg:34.51ms
step:557/1920 train_time:19224ms step_avg:34.51ms
step:558/1920 train_time:19258ms step_avg:34.51ms
step:559/1920 train_time:19293ms step_avg:34.51ms
step:560/1920 train_time:19327ms step_avg:34.51ms
step:561/1920 train_time:19361ms step_avg:34.51ms
step:562/1920 train_time:19396ms step_avg:34.51ms
step:563/1920 train_time:19430ms step_avg:34.51ms
step:564/1920 train_time:19464ms step_avg:34.51ms
step:565/1920 train_time:19499ms step_avg:34.51ms
step:566/1920 train_time:19534ms step_avg:34.51ms
step:567/1920 train_time:19568ms step_avg:34.51ms
step:568/1920 train_time:19603ms step_avg:34.51ms
step:569/1920 train_time:19638ms step_avg:34.51ms
step:570/1920 train_time:19672ms step_avg:34.51ms
step:571/1920 train_time:19707ms step_avg:34.51ms
step:572/1920 train_time:19742ms step_avg:34.51ms
step:573/1920 train_time:19776ms step_avg:34.51ms
step:574/1920 train_time:19811ms step_avg:34.51ms
step:575/1920 train_time:19845ms step_avg:34.51ms
step:576/1920 train_time:19879ms step_avg:34.51ms
step:577/1920 train_time:19914ms step_avg:34.51ms
step:578/1920 train_time:19948ms step_avg:34.51ms
step:579/1920 train_time:19983ms step_avg:34.51ms
step:580/1920 train_time:20017ms step_avg:34.51ms
step:581/1920 train_time:20051ms step_avg:34.51ms
step:582/1920 train_time:20085ms step_avg:34.51ms
step:583/1920 train_time:20119ms step_avg:34.51ms
step:584/1920 train_time:20154ms step_avg:34.51ms
step:585/1920 train_time:20188ms step_avg:34.51ms
step:586/1920 train_time:20222ms step_avg:34.51ms
step:587/1920 train_time:20257ms step_avg:34.51ms
step:588/1920 train_time:20291ms step_avg:34.51ms
step:589/1920 train_time:20326ms step_avg:34.51ms
step:590/1920 train_time:20360ms step_avg:34.51ms
step:591/1920 train_time:20394ms step_avg:34.51ms
step:592/1920 train_time:20429ms step_avg:34.51ms
step:593/1920 train_time:20463ms step_avg:34.51ms
step:594/1920 train_time:20498ms step_avg:34.51ms
step:595/1920 train_time:20533ms step_avg:34.51ms
step:596/1920 train_time:20567ms step_avg:34.51ms
step:597/1920 train_time:20602ms step_avg:34.51ms
step:598/1920 train_time:20636ms step_avg:34.51ms
step:599/1920 train_time:20670ms step_avg:34.51ms
step:600/1920 train_time:20705ms step_avg:34.51ms
step:601/1920 train_time:20739ms step_avg:34.51ms
step:602/1920 train_time:20774ms step_avg:34.51ms
step:603/1920 train_time:20808ms step_avg:34.51ms
step:604/1920 train_time:20843ms step_avg:34.51ms
step:605/1920 train_time:20877ms step_avg:34.51ms
step:606/1920 train_time:20911ms step_avg:34.51ms
step:607/1920 train_time:20946ms step_avg:34.51ms
step:608/1920 train_time:20980ms step_avg:34.51ms
step:609/1920 train_time:21014ms step_avg:34.51ms
step:610/1920 train_time:21049ms step_avg:34.51ms
step:611/1920 train_time:21083ms step_avg:34.51ms
step:612/1920 train_time:21117ms step_avg:34.51ms
step:613/1920 train_time:21152ms step_avg:34.51ms
step:614/1920 train_time:21186ms step_avg:34.51ms
step:615/1920 train_time:21220ms step_avg:34.50ms
step:616/1920 train_time:21254ms step_avg:34.50ms
step:617/1920 train_time:21289ms step_avg:34.50ms
step:618/1920 train_time:21323ms step_avg:34.50ms
step:619/1920 train_time:21357ms step_avg:34.50ms
step:620/1920 train_time:21391ms step_avg:34.50ms
step:621/1920 train_time:21426ms step_avg:34.50ms
step:622/1920 train_time:21460ms step_avg:34.50ms
step:623/1920 train_time:21495ms step_avg:34.50ms
step:624/1920 train_time:21529ms step_avg:34.50ms
step:625/1920 train_time:21563ms step_avg:34.50ms
step:626/1920 train_time:21598ms step_avg:34.50ms
step:627/1920 train_time:21632ms step_avg:34.50ms
step:628/1920 train_time:21668ms step_avg:34.50ms
step:629/1920 train_time:21729ms step_avg:34.55ms
step:630/1920 train_time:21791ms step_avg:34.59ms
step:631/1920 train_time:21854ms step_avg:34.63ms
step:632/1920 train_time:21916ms step_avg:34.68ms
step:633/1920 train_time:21978ms step_avg:34.72ms
step:634/1920 train_time:22040ms step_avg:34.76ms
step:635/1920 train_time:22103ms step_avg:34.81ms
step:636/1920 train_time:22165ms step_avg:34.85ms
step:637/1920 train_time:22228ms step_avg:34.89ms
step:638/1920 train_time:22290ms step_avg:34.94ms
step:639/1920 train_time:22352ms step_avg:34.98ms
step:640/1920 train_time:22414ms step_avg:35.02ms
step:641/1920 train_time:22477ms step_avg:35.07ms
step:642/1920 train_time:22539ms step_avg:35.11ms
step:643/1920 train_time:22602ms step_avg:35.15ms
step:644/1920 train_time:22664ms step_avg:35.19ms
step:645/1920 train_time:22726ms step_avg:35.23ms
step:646/1920 train_time:22788ms step_avg:35.28ms
step:647/1920 train_time:22851ms step_avg:35.32ms
step:648/1920 train_time:22913ms step_avg:35.36ms
step:649/1920 train_time:22976ms step_avg:35.40ms
step:650/1920 train_time:23039ms step_avg:35.44ms
step:651/1920 train_time:23102ms step_avg:35.49ms
step:652/1920 train_time:23163ms step_avg:35.53ms
step:653/1920 train_time:23226ms step_avg:35.57ms
step:654/1920 train_time:23288ms step_avg:35.61ms
step:655/1920 train_time:23350ms step_avg:35.65ms
step:656/1920 train_time:23412ms step_avg:35.69ms
step:657/1920 train_time:23474ms step_avg:35.73ms
step:658/1920 train_time:23536ms step_avg:35.77ms
step:659/1920 train_time:23600ms step_avg:35.81ms
step:660/1920 train_time:23662ms step_avg:35.85ms
step:661/1920 train_time:23724ms step_avg:35.89ms
step:662/1920 train_time:23786ms step_avg:35.93ms
step:663/1920 train_time:23848ms step_avg:35.97ms
step:664/1920 train_time:23910ms step_avg:36.01ms
step:665/1920 train_time:23972ms step_avg:36.05ms
step:666/1920 train_time:24034ms step_avg:36.09ms
step:667/1920 train_time:24097ms step_avg:36.13ms
step:668/1920 train_time:24160ms step_avg:36.17ms
step:669/1920 train_time:24223ms step_avg:36.21ms
step:670/1920 train_time:24285ms step_avg:36.25ms
step:671/1920 train_time:24348ms step_avg:36.29ms
step:672/1920 train_time:24409ms step_avg:36.32ms
step:673/1920 train_time:24472ms step_avg:36.36ms
step:674/1920 train_time:24534ms step_avg:36.40ms
step:675/1920 train_time:24597ms step_avg:36.44ms
step:676/1920 train_time:24659ms step_avg:36.48ms
step:677/1920 train_time:24722ms step_avg:36.52ms
step:678/1920 train_time:24784ms step_avg:36.55ms
step:679/1920 train_time:24846ms step_avg:36.59ms
step:680/1920 train_time:24907ms step_avg:36.63ms
step:681/1920 train_time:24969ms step_avg:36.67ms
step:682/1920 train_time:25031ms step_avg:36.70ms
step:683/1920 train_time:25095ms step_avg:36.74ms
step:684/1920 train_time:25157ms step_avg:36.78ms
step:685/1920 train_time:25221ms step_avg:36.82ms
step:686/1920 train_time:25282ms step_avg:36.85ms
step:687/1920 train_time:25345ms step_avg:36.89ms
step:688/1920 train_time:25407ms step_avg:36.93ms
step:689/1920 train_time:25469ms step_avg:36.97ms
step:690/1920 train_time:25531ms step_avg:37.00ms
step:691/1920 train_time:25594ms step_avg:37.04ms
step:692/1920 train_time:25656ms step_avg:37.08ms
step:693/1920 train_time:25720ms step_avg:37.11ms
step:694/1920 train_time:25782ms step_avg:37.15ms
step:695/1920 train_time:25845ms step_avg:37.19ms
step:696/1920 train_time:25906ms step_avg:37.22ms
step:697/1920 train_time:25969ms step_avg:37.26ms
step:698/1920 train_time:26031ms step_avg:37.29ms
step:699/1920 train_time:26093ms step_avg:37.33ms
step:700/1920 train_time:26155ms step_avg:37.36ms
step:701/1920 train_time:26219ms step_avg:37.40ms
step:702/1920 train_time:26281ms step_avg:37.44ms
step:703/1920 train_time:26344ms step_avg:37.47ms
step:704/1920 train_time:26406ms step_avg:37.51ms
step:705/1920 train_time:26468ms step_avg:37.54ms
step:706/1920 train_time:26530ms step_avg:37.58ms
step:707/1920 train_time:26592ms step_avg:37.61ms
step:708/1920 train_time:26654ms step_avg:37.65ms
step:709/1920 train_time:26716ms step_avg:37.68ms
step:710/1920 train_time:26779ms step_avg:37.72ms
step:711/1920 train_time:26842ms step_avg:37.75ms
step:712/1920 train_time:26904ms step_avg:37.79ms
step:713/1920 train_time:26967ms step_avg:37.82ms
step:714/1920 train_time:27029ms step_avg:37.86ms
step:715/1920 train_time:27091ms step_avg:37.89ms
step:716/1920 train_time:27153ms step_avg:37.92ms
step:717/1920 train_time:27216ms step_avg:37.96ms
step:718/1920 train_time:27279ms step_avg:37.99ms
step:719/1920 train_time:27343ms step_avg:38.03ms
step:720/1920 train_time:27405ms step_avg:38.06ms
step:721/1920 train_time:27467ms step_avg:38.10ms
step:722/1920 train_time:27528ms step_avg:38.13ms
step:723/1920 train_time:27591ms step_avg:38.16ms
step:724/1920 train_time:27652ms step_avg:38.19ms
step:725/1920 train_time:27716ms step_avg:38.23ms
step:726/1920 train_time:27778ms step_avg:38.26ms
step:727/1920 train_time:27841ms step_avg:38.30ms
step:728/1920 train_time:27903ms step_avg:38.33ms
step:729/1920 train_time:27965ms step_avg:38.36ms
step:730/1920 train_time:28027ms step_avg:38.39ms
step:731/1920 train_time:28090ms step_avg:38.43ms
step:732/1920 train_time:28151ms step_avg:38.46ms
step:733/1920 train_time:28214ms step_avg:38.49ms
step:734/1920 train_time:28276ms step_avg:38.52ms
step:735/1920 train_time:28340ms step_avg:38.56ms
step:736/1920 train_time:28402ms step_avg:38.59ms
step:737/1920 train_time:28464ms step_avg:38.62ms
step:738/1920 train_time:28526ms step_avg:38.65ms
step:739/1920 train_time:28589ms step_avg:38.69ms
step:740/1920 train_time:28651ms step_avg:38.72ms
step:741/1920 train_time:28713ms step_avg:38.75ms
step:742/1920 train_time:28776ms step_avg:38.78ms
step:743/1920 train_time:28839ms step_avg:38.81ms
step:744/1920 train_time:28901ms step_avg:38.85ms
step:745/1920 train_time:28963ms step_avg:38.88ms
step:746/1920 train_time:29026ms step_avg:38.91ms
step:747/1920 train_time:29088ms step_avg:38.94ms
step:748/1920 train_time:29150ms step_avg:38.97ms
step:749/1920 train_time:29213ms step_avg:39.00ms
step:750/1920 train_time:29275ms step_avg:39.03ms
step:750/1920 val_loss:4.0419 train_time:29341ms step_avg:39.12ms
step:751/1920 train_time:29365ms step_avg:39.10ms
step:752/1920 train_time:29402ms step_avg:39.10ms
step:753/1920 train_time:29466ms step_avg:39.13ms
step:754/1920 train_time:29534ms step_avg:39.17ms
step:755/1920 train_time:29598ms step_avg:39.20ms
step:756/1920 train_time:29660ms step_avg:39.23ms
step:757/1920 train_time:29722ms step_avg:39.26ms
step:758/1920 train_time:29783ms step_avg:39.29ms
step:759/1920 train_time:29845ms step_avg:39.32ms
step:760/1920 train_time:29906ms step_avg:39.35ms
step:761/1920 train_time:29968ms step_avg:39.38ms
step:762/1920 train_time:30028ms step_avg:39.41ms
step:763/1920 train_time:30090ms step_avg:39.44ms
step:764/1920 train_time:30152ms step_avg:39.47ms
step:765/1920 train_time:30214ms step_avg:39.50ms
step:766/1920 train_time:30279ms step_avg:39.53ms
step:767/1920 train_time:30345ms step_avg:39.56ms
step:768/1920 train_time:30408ms step_avg:39.59ms
step:769/1920 train_time:30472ms step_avg:39.63ms
step:770/1920 train_time:30535ms step_avg:39.66ms
step:771/1920 train_time:30598ms step_avg:39.69ms
step:772/1920 train_time:30659ms step_avg:39.71ms
step:773/1920 train_time:30721ms step_avg:39.74ms
step:774/1920 train_time:30783ms step_avg:39.77ms
step:775/1920 train_time:30845ms step_avg:39.80ms
step:776/1920 train_time:30906ms step_avg:39.83ms
step:777/1920 train_time:30969ms step_avg:39.86ms
step:778/1920 train_time:31030ms step_avg:39.88ms
step:779/1920 train_time:31091ms step_avg:39.91ms
step:780/1920 train_time:31152ms step_avg:39.94ms
step:781/1920 train_time:31216ms step_avg:39.97ms
step:782/1920 train_time:31279ms step_avg:40.00ms
step:783/1920 train_time:31343ms step_avg:40.03ms
step:784/1920 train_time:31405ms step_avg:40.06ms
step:785/1920 train_time:31470ms step_avg:40.09ms
step:786/1920 train_time:31533ms step_avg:40.12ms
step:787/1920 train_time:31596ms step_avg:40.15ms
step:788/1920 train_time:31658ms step_avg:40.18ms
step:789/1920 train_time:31720ms step_avg:40.20ms
step:790/1920 train_time:31783ms step_avg:40.23ms
step:791/1920 train_time:31845ms step_avg:40.26ms
step:792/1920 train_time:31906ms step_avg:40.29ms
step:793/1920 train_time:31968ms step_avg:40.31ms
step:794/1920 train_time:32030ms step_avg:40.34ms
step:795/1920 train_time:32092ms step_avg:40.37ms
step:796/1920 train_time:32154ms step_avg:40.39ms
step:797/1920 train_time:32216ms step_avg:40.42ms
step:798/1920 train_time:32277ms step_avg:40.45ms
step:799/1920 train_time:32341ms step_avg:40.48ms
step:800/1920 train_time:32404ms step_avg:40.50ms
step:801/1920 train_time:32468ms step_avg:40.53ms
step:802/1920 train_time:32531ms step_avg:40.56ms
step:803/1920 train_time:32594ms step_avg:40.59ms
step:804/1920 train_time:32656ms step_avg:40.62ms
step:805/1920 train_time:32719ms step_avg:40.64ms
step:806/1920 train_time:32781ms step_avg:40.67ms
step:807/1920 train_time:32843ms step_avg:40.70ms
step:808/1920 train_time:32905ms step_avg:40.72ms
step:809/1920 train_time:32967ms step_avg:40.75ms
step:810/1920 train_time:33029ms step_avg:40.78ms
step:811/1920 train_time:33091ms step_avg:40.80ms
step:812/1920 train_time:33153ms step_avg:40.83ms
step:813/1920 train_time:33216ms step_avg:40.86ms
step:814/1920 train_time:33277ms step_avg:40.88ms
step:815/1920 train_time:33341ms step_avg:40.91ms
step:816/1920 train_time:33403ms step_avg:40.94ms
step:817/1920 train_time:33467ms step_avg:40.96ms
step:818/1920 train_time:33530ms step_avg:40.99ms
step:819/1920 train_time:33593ms step_avg:41.02ms
step:820/1920 train_time:33655ms step_avg:41.04ms
step:821/1920 train_time:33717ms step_avg:41.07ms
step:822/1920 train_time:33779ms step_avg:41.09ms
step:823/1920 train_time:33842ms step_avg:41.12ms
step:824/1920 train_time:33904ms step_avg:41.15ms
step:825/1920 train_time:33966ms step_avg:41.17ms
step:826/1920 train_time:34028ms step_avg:41.20ms
step:827/1920 train_time:34091ms step_avg:41.22ms
step:828/1920 train_time:34152ms step_avg:41.25ms
step:829/1920 train_time:34215ms step_avg:41.27ms
step:830/1920 train_time:34276ms step_avg:41.30ms
step:831/1920 train_time:34339ms step_avg:41.32ms
step:832/1920 train_time:34401ms step_avg:41.35ms
step:833/1920 train_time:34464ms step_avg:41.37ms
step:834/1920 train_time:34527ms step_avg:41.40ms
step:835/1920 train_time:34591ms step_avg:41.43ms
step:836/1920 train_time:34653ms step_avg:41.45ms
step:837/1920 train_time:34716ms step_avg:41.48ms
step:838/1920 train_time:34777ms step_avg:41.50ms
step:839/1920 train_time:34840ms step_avg:41.53ms
step:840/1920 train_time:34902ms step_avg:41.55ms
step:841/1920 train_time:34964ms step_avg:41.57ms
step:842/1920 train_time:35026ms step_avg:41.60ms
step:843/1920 train_time:35089ms step_avg:41.62ms
step:844/1920 train_time:35151ms step_avg:41.65ms
step:845/1920 train_time:35214ms step_avg:41.67ms
step:846/1920 train_time:35276ms step_avg:41.70ms
step:847/1920 train_time:35338ms step_avg:41.72ms
step:848/1920 train_time:35400ms step_avg:41.75ms
step:849/1920 train_time:35463ms step_avg:41.77ms
step:850/1920 train_time:35526ms step_avg:41.79ms
step:851/1920 train_time:35589ms step_avg:41.82ms
step:852/1920 train_time:35651ms step_avg:41.84ms
step:853/1920 train_time:35714ms step_avg:41.87ms
step:854/1920 train_time:35776ms step_avg:41.89ms
step:855/1920 train_time:35838ms step_avg:41.92ms
step:856/1920 train_time:35901ms step_avg:41.94ms
step:857/1920 train_time:35962ms step_avg:41.96ms
step:858/1920 train_time:36024ms step_avg:41.99ms
step:859/1920 train_time:36088ms step_avg:42.01ms
step:860/1920 train_time:36150ms step_avg:42.04ms
step:861/1920 train_time:36213ms step_avg:42.06ms
step:862/1920 train_time:36274ms step_avg:42.08ms
step:863/1920 train_time:36337ms step_avg:42.11ms
step:864/1920 train_time:36399ms step_avg:42.13ms
step:865/1920 train_time:36461ms step_avg:42.15ms
step:866/1920 train_time:36523ms step_avg:42.17ms
step:867/1920 train_time:36586ms step_avg:42.20ms
step:868/1920 train_time:36649ms step_avg:42.22ms
step:869/1920 train_time:36712ms step_avg:42.25ms
step:870/1920 train_time:36774ms step_avg:42.27ms
step:871/1920 train_time:36837ms step_avg:42.29ms
step:872/1920 train_time:36899ms step_avg:42.32ms
step:873/1920 train_time:36961ms step_avg:42.34ms
step:874/1920 train_time:37023ms step_avg:42.36ms
step:875/1920 train_time:37086ms step_avg:42.38ms
step:876/1920 train_time:37148ms step_avg:42.41ms
step:877/1920 train_time:37211ms step_avg:42.43ms
step:878/1920 train_time:37273ms step_avg:42.45ms
step:879/1920 train_time:37336ms step_avg:42.48ms
step:880/1920 train_time:37397ms step_avg:42.50ms
step:881/1920 train_time:37460ms step_avg:42.52ms
step:882/1920 train_time:37522ms step_avg:42.54ms
step:883/1920 train_time:37585ms step_avg:42.57ms
step:884/1920 train_time:37648ms step_avg:42.59ms
step:885/1920 train_time:37711ms step_avg:42.61ms
step:886/1920 train_time:37773ms step_avg:42.63ms
step:887/1920 train_time:37836ms step_avg:42.66ms
step:888/1920 train_time:37898ms step_avg:42.68ms
step:889/1920 train_time:37960ms step_avg:42.70ms
step:890/1920 train_time:38022ms step_avg:42.72ms
step:891/1920 train_time:38085ms step_avg:42.74ms
step:892/1920 train_time:38146ms step_avg:42.76ms
step:893/1920 train_time:38209ms step_avg:42.79ms
step:894/1920 train_time:38271ms step_avg:42.81ms
step:895/1920 train_time:38334ms step_avg:42.83ms
step:896/1920 train_time:38396ms step_avg:42.85ms
step:897/1920 train_time:38458ms step_avg:42.87ms
step:898/1920 train_time:38520ms step_avg:42.89ms
step:899/1920 train_time:38582ms step_avg:42.92ms
step:900/1920 train_time:38644ms step_avg:42.94ms
step:901/1920 train_time:38708ms step_avg:42.96ms
step:902/1920 train_time:38771ms step_avg:42.98ms
step:903/1920 train_time:38834ms step_avg:43.01ms
step:904/1920 train_time:38895ms step_avg:43.03ms
step:905/1920 train_time:38957ms step_avg:43.05ms
step:906/1920 train_time:39019ms step_avg:43.07ms
step:907/1920 train_time:39082ms step_avg:43.09ms
step:908/1920 train_time:39144ms step_avg:43.11ms
step:909/1920 train_time:39207ms step_avg:43.13ms
step:910/1920 train_time:39269ms step_avg:43.15ms
step:911/1920 train_time:39333ms step_avg:43.18ms
step:912/1920 train_time:39394ms step_avg:43.20ms
step:913/1920 train_time:39457ms step_avg:43.22ms
step:914/1920 train_time:39519ms step_avg:43.24ms
step:915/1920 train_time:39581ms step_avg:43.26ms
step:916/1920 train_time:39643ms step_avg:43.28ms
step:917/1920 train_time:39707ms step_avg:43.30ms
step:918/1920 train_time:39769ms step_avg:43.32ms
step:919/1920 train_time:39832ms step_avg:43.34ms
step:920/1920 train_time:39894ms step_avg:43.36ms
step:921/1920 train_time:39957ms step_avg:43.38ms
step:922/1920 train_time:40018ms step_avg:43.40ms
step:923/1920 train_time:40080ms step_avg:43.42ms
step:924/1920 train_time:40142ms step_avg:43.44ms
step:925/1920 train_time:40205ms step_avg:43.46ms
step:926/1920 train_time:40268ms step_avg:43.49ms
step:927/1920 train_time:40331ms step_avg:43.51ms
step:928/1920 train_time:40393ms step_avg:43.53ms
step:929/1920 train_time:40456ms step_avg:43.55ms
step:930/1920 train_time:40518ms step_avg:43.57ms
step:931/1920 train_time:40581ms step_avg:43.59ms
step:932/1920 train_time:40643ms step_avg:43.61ms
step:933/1920 train_time:40706ms step_avg:43.63ms
step:934/1920 train_time:40768ms step_avg:43.65ms
step:935/1920 train_time:40831ms step_avg:43.67ms
step:936/1920 train_time:40893ms step_avg:43.69ms
step:937/1920 train_time:40955ms step_avg:43.71ms
step:938/1920 train_time:41017ms step_avg:43.73ms
step:939/1920 train_time:41080ms step_avg:43.75ms
step:940/1920 train_time:41141ms step_avg:43.77ms
step:941/1920 train_time:41204ms step_avg:43.79ms
step:942/1920 train_time:41266ms step_avg:43.81ms
step:943/1920 train_time:41330ms step_avg:43.83ms
step:944/1920 train_time:41393ms step_avg:43.85ms
step:945/1920 train_time:41455ms step_avg:43.87ms
step:946/1920 train_time:41517ms step_avg:43.89ms
step:947/1920 train_time:41579ms step_avg:43.91ms
step:948/1920 train_time:41641ms step_avg:43.93ms
step:949/1920 train_time:41704ms step_avg:43.95ms
step:950/1920 train_time:41766ms step_avg:43.96ms
step:951/1920 train_time:41830ms step_avg:43.99ms
step:952/1920 train_time:41892ms step_avg:44.00ms
step:953/1920 train_time:41956ms step_avg:44.02ms
step:954/1920 train_time:42017ms step_avg:44.04ms
step:955/1920 train_time:42080ms step_avg:44.06ms
step:956/1920 train_time:42142ms step_avg:44.08ms
step:957/1920 train_time:42204ms step_avg:44.10ms
step:958/1920 train_time:42266ms step_avg:44.12ms
step:959/1920 train_time:42330ms step_avg:44.14ms
step:960/1920 train_time:42392ms step_avg:44.16ms
step:961/1920 train_time:42455ms step_avg:44.18ms
step:962/1920 train_time:42516ms step_avg:44.20ms
step:963/1920 train_time:42578ms step_avg:44.21ms
step:964/1920 train_time:42640ms step_avg:44.23ms
step:965/1920 train_time:42703ms step_avg:44.25ms
step:966/1920 train_time:42765ms step_avg:44.27ms
step:967/1920 train_time:42828ms step_avg:44.29ms
step:968/1920 train_time:42890ms step_avg:44.31ms
step:969/1920 train_time:42953ms step_avg:44.33ms
step:970/1920 train_time:43015ms step_avg:44.35ms
step:971/1920 train_time:43078ms step_avg:44.36ms
step:972/1920 train_time:43140ms step_avg:44.38ms
step:973/1920 train_time:43202ms step_avg:44.40ms
step:974/1920 train_time:43263ms step_avg:44.42ms
step:975/1920 train_time:43327ms step_avg:44.44ms
step:976/1920 train_time:43390ms step_avg:44.46ms
step:977/1920 train_time:43453ms step_avg:44.48ms
step:978/1920 train_time:43515ms step_avg:44.49ms
step:979/1920 train_time:43577ms step_avg:44.51ms
step:980/1920 train_time:43639ms step_avg:44.53ms
step:981/1920 train_time:43702ms step_avg:44.55ms
step:982/1920 train_time:43763ms step_avg:44.57ms
step:983/1920 train_time:43827ms step_avg:44.58ms
step:984/1920 train_time:43890ms step_avg:44.60ms
step:985/1920 train_time:43953ms step_avg:44.62ms
step:986/1920 train_time:44015ms step_avg:44.64ms
step:987/1920 train_time:44077ms step_avg:44.66ms
step:988/1920 train_time:44139ms step_avg:44.68ms
step:989/1920 train_time:44201ms step_avg:44.69ms
step:990/1920 train_time:44263ms step_avg:44.71ms
step:991/1920 train_time:44327ms step_avg:44.73ms
step:992/1920 train_time:44389ms step_avg:44.75ms
step:993/1920 train_time:44452ms step_avg:44.77ms
step:994/1920 train_time:44514ms step_avg:44.78ms
step:995/1920 train_time:44577ms step_avg:44.80ms
step:996/1920 train_time:44639ms step_avg:44.82ms
step:997/1920 train_time:44701ms step_avg:44.84ms
step:998/1920 train_time:44763ms step_avg:44.85ms
step:999/1920 train_time:44826ms step_avg:44.87ms
step:1000/1920 train_time:44889ms step_avg:44.89ms
step:1000/1920 val_loss:3.7733 train_time:44955ms step_avg:44.95ms
step:1001/1920 train_time:44975ms step_avg:44.93ms
step:1002/1920 train_time:45016ms step_avg:44.93ms
step:1003/1920 train_time:45083ms step_avg:44.95ms
step:1004/1920 train_time:45150ms step_avg:44.97ms
step:1005/1920 train_time:45213ms step_avg:44.99ms
step:1006/1920 train_time:45275ms step_avg:45.00ms
step:1007/1920 train_time:45338ms step_avg:45.02ms
step:1008/1920 train_time:45399ms step_avg:45.04ms
step:1009/1920 train_time:45462ms step_avg:45.06ms
step:1010/1920 train_time:45523ms step_avg:45.07ms
step:1011/1920 train_time:45585ms step_avg:45.09ms
step:1012/1920 train_time:45646ms step_avg:45.10ms
step:1013/1920 train_time:45708ms step_avg:45.12ms
step:1014/1920 train_time:45770ms step_avg:45.14ms
step:1015/1920 train_time:45832ms step_avg:45.15ms
step:1016/1920 train_time:45894ms step_avg:45.17ms
step:1017/1920 train_time:45958ms step_avg:45.19ms
step:1018/1920 train_time:46021ms step_avg:45.21ms
step:1019/1920 train_time:46086ms step_avg:45.23ms
step:1020/1920 train_time:46148ms step_avg:45.24ms
step:1021/1920 train_time:46212ms step_avg:45.26ms
step:1022/1920 train_time:46274ms step_avg:45.28ms
step:1023/1920 train_time:46336ms step_avg:45.29ms
step:1024/1920 train_time:46398ms step_avg:45.31ms
step:1025/1920 train_time:46461ms step_avg:45.33ms
step:1026/1920 train_time:46523ms step_avg:45.34ms
step:1027/1920 train_time:46585ms step_avg:45.36ms
step:1028/1920 train_time:46646ms step_avg:45.38ms
step:1029/1920 train_time:46709ms step_avg:45.39ms
step:1030/1920 train_time:46770ms step_avg:45.41ms
step:1031/1920 train_time:46832ms step_avg:45.42ms
step:1032/1920 train_time:46894ms step_avg:45.44ms
step:1033/1920 train_time:46958ms step_avg:45.46ms
step:1034/1920 train_time:47020ms step_avg:45.47ms
step:1035/1920 train_time:47084ms step_avg:45.49ms
step:1036/1920 train_time:47147ms step_avg:45.51ms
step:1037/1920 train_time:47210ms step_avg:45.53ms
step:1038/1920 train_time:47273ms step_avg:45.54ms
step:1039/1920 train_time:47336ms step_avg:45.56ms
step:1040/1920 train_time:47397ms step_avg:45.57ms
step:1041/1920 train_time:47461ms step_avg:45.59ms
step:1042/1920 train_time:47523ms step_avg:45.61ms
step:1043/1920 train_time:47585ms step_avg:45.62ms
step:1044/1920 train_time:47647ms step_avg:45.64ms
step:1045/1920 train_time:47710ms step_avg:45.66ms
step:1046/1920 train_time:47771ms step_avg:45.67ms
step:1047/1920 train_time:47833ms step_avg:45.69ms
step:1048/1920 train_time:47894ms step_avg:45.70ms
step:1049/1920 train_time:47957ms step_avg:45.72ms
step:1050/1920 train_time:48020ms step_avg:45.73ms
step:1051/1920 train_time:48084ms step_avg:45.75ms
step:1052/1920 train_time:48146ms step_avg:45.77ms
step:1053/1920 train_time:48209ms step_avg:45.78ms
step:1054/1920 train_time:48271ms step_avg:45.80ms
step:1055/1920 train_time:48334ms step_avg:45.81ms
step:1056/1920 train_time:48396ms step_avg:45.83ms
step:1057/1920 train_time:48459ms step_avg:45.85ms
step:1058/1920 train_time:48522ms step_avg:45.86ms
step:1059/1920 train_time:48584ms step_avg:45.88ms
step:1060/1920 train_time:48646ms step_avg:45.89ms
step:1061/1920 train_time:48709ms step_avg:45.91ms
step:1062/1920 train_time:48770ms step_avg:45.92ms
step:1063/1920 train_time:48833ms step_avg:45.94ms
step:1064/1920 train_time:48894ms step_avg:45.95ms
step:1065/1920 train_time:48957ms step_avg:45.97ms
step:1066/1920 train_time:49020ms step_avg:45.98ms
step:1067/1920 train_time:49083ms step_avg:46.00ms
step:1068/1920 train_time:49146ms step_avg:46.02ms
step:1069/1920 train_time:49209ms step_avg:46.03ms
step:1070/1920 train_time:49271ms step_avg:46.05ms
step:1071/1920 train_time:49334ms step_avg:46.06ms
step:1072/1920 train_time:49395ms step_avg:46.08ms
step:1073/1920 train_time:49458ms step_avg:46.09ms
step:1074/1920 train_time:49520ms step_avg:46.11ms
step:1075/1920 train_time:49584ms step_avg:46.12ms
step:1076/1920 train_time:49646ms step_avg:46.14ms
step:1077/1920 train_time:49709ms step_avg:46.16ms
step:1078/1920 train_time:49771ms step_avg:46.17ms
step:1079/1920 train_time:49833ms step_avg:46.18ms
step:1080/1920 train_time:49895ms step_avg:46.20ms
step:1081/1920 train_time:49958ms step_avg:46.21ms
step:1082/1920 train_time:50020ms step_avg:46.23ms
step:1083/1920 train_time:50083ms step_avg:46.24ms
step:1084/1920 train_time:50145ms step_avg:46.26ms
step:1085/1920 train_time:50208ms step_avg:46.27ms
step:1086/1920 train_time:50271ms step_avg:46.29ms
step:1087/1920 train_time:50334ms step_avg:46.31ms
step:1088/1920 train_time:50395ms step_avg:46.32ms
step:1089/1920 train_time:50458ms step_avg:46.33ms
step:1090/1920 train_time:50520ms step_avg:46.35ms
step:1091/1920 train_time:50584ms step_avg:46.36ms
step:1092/1920 train_time:50646ms step_avg:46.38ms
step:1093/1920 train_time:50709ms step_avg:46.39ms
step:1094/1920 train_time:50771ms step_avg:46.41ms
step:1095/1920 train_time:50833ms step_avg:46.42ms
step:1096/1920 train_time:50895ms step_avg:46.44ms
step:1097/1920 train_time:50958ms step_avg:46.45ms
step:1098/1920 train_time:51020ms step_avg:46.47ms
step:1099/1920 train_time:51083ms step_avg:46.48ms
step:1100/1920 train_time:51145ms step_avg:46.50ms
step:1101/1920 train_time:51208ms step_avg:46.51ms
step:1102/1920 train_time:51271ms step_avg:46.53ms
step:1103/1920 train_time:51334ms step_avg:46.54ms
step:1104/1920 train_time:51395ms step_avg:46.55ms
step:1105/1920 train_time:51458ms step_avg:46.57ms
step:1106/1920 train_time:51521ms step_avg:46.58ms
step:1107/1920 train_time:51584ms step_avg:46.60ms
step:1108/1920 train_time:51646ms step_avg:46.61ms
step:1109/1920 train_time:51709ms step_avg:46.63ms
step:1110/1920 train_time:51771ms step_avg:46.64ms
step:1111/1920 train_time:51833ms step_avg:46.65ms
step:1112/1920 train_time:51895ms step_avg:46.67ms
step:1113/1920 train_time:51957ms step_avg:46.68ms
step:1114/1920 train_time:52020ms step_avg:46.70ms
step:1115/1920 train_time:52083ms step_avg:46.71ms
step:1116/1920 train_time:52146ms step_avg:46.73ms
step:1117/1920 train_time:52209ms step_avg:46.74ms
step:1118/1920 train_time:52271ms step_avg:46.75ms
step:1119/1920 train_time:52334ms step_avg:46.77ms
step:1120/1920 train_time:52395ms step_avg:46.78ms
step:1121/1920 train_time:52459ms step_avg:46.80ms
step:1122/1920 train_time:52521ms step_avg:46.81ms
step:1123/1920 train_time:52585ms step_avg:46.83ms
step:1124/1920 train_time:52648ms step_avg:46.84ms
step:1125/1920 train_time:52710ms step_avg:46.85ms
step:1126/1920 train_time:52773ms step_avg:46.87ms
step:1127/1920 train_time:52836ms step_avg:46.88ms
step:1128/1920 train_time:52897ms step_avg:46.89ms
step:1129/1920 train_time:52961ms step_avg:46.91ms
step:1130/1920 train_time:53023ms step_avg:46.92ms
step:1131/1920 train_time:53086ms step_avg:46.94ms
step:1132/1920 train_time:53147ms step_avg:46.95ms
step:1133/1920 train_time:53210ms step_avg:46.96ms
step:1134/1920 train_time:53271ms step_avg:46.98ms
step:1135/1920 train_time:53334ms step_avg:46.99ms
step:1136/1920 train_time:53396ms step_avg:47.00ms
step:1137/1920 train_time:53459ms step_avg:47.02ms
step:1138/1920 train_time:53521ms step_avg:47.03ms
step:1139/1920 train_time:53584ms step_avg:47.04ms
step:1140/1920 train_time:53646ms step_avg:47.06ms
step:1141/1920 train_time:53709ms step_avg:47.07ms
step:1142/1920 train_time:53770ms step_avg:47.08ms
step:1143/1920 train_time:53833ms step_avg:47.10ms
step:1144/1920 train_time:53895ms step_avg:47.11ms
step:1145/1920 train_time:53957ms step_avg:47.12ms
step:1146/1920 train_time:54020ms step_avg:47.14ms
step:1147/1920 train_time:54083ms step_avg:47.15ms
step:1148/1920 train_time:54145ms step_avg:47.16ms
step:1149/1920 train_time:54208ms step_avg:47.18ms
step:1150/1920 train_time:54269ms step_avg:47.19ms
step:1151/1920 train_time:54332ms step_avg:47.20ms
step:1152/1920 train_time:54394ms step_avg:47.22ms
step:1153/1920 train_time:54457ms step_avg:47.23ms
step:1154/1920 train_time:54519ms step_avg:47.24ms
step:1155/1920 train_time:54582ms step_avg:47.26ms
step:1156/1920 train_time:54644ms step_avg:47.27ms
step:1157/1920 train_time:54707ms step_avg:47.28ms
step:1158/1920 train_time:54769ms step_avg:47.30ms
step:1159/1920 train_time:54831ms step_avg:47.31ms
step:1160/1920 train_time:54893ms step_avg:47.32ms
step:1161/1920 train_time:54956ms step_avg:47.33ms
step:1162/1920 train_time:55018ms step_avg:47.35ms
step:1163/1920 train_time:55082ms step_avg:47.36ms
step:1164/1920 train_time:55145ms step_avg:47.38ms
step:1165/1920 train_time:55208ms step_avg:47.39ms
step:1166/1920 train_time:55270ms step_avg:47.40ms
step:1167/1920 train_time:55332ms step_avg:47.41ms
step:1168/1920 train_time:55395ms step_avg:47.43ms
step:1169/1920 train_time:55457ms step_avg:47.44ms
step:1170/1920 train_time:55519ms step_avg:47.45ms
step:1171/1920 train_time:55583ms step_avg:47.47ms
step:1172/1920 train_time:55645ms step_avg:47.48ms
step:1173/1920 train_time:55708ms step_avg:47.49ms
step:1174/1920 train_time:55770ms step_avg:47.50ms
step:1175/1920 train_time:55833ms step_avg:47.52ms
step:1176/1920 train_time:55894ms step_avg:47.53ms
step:1177/1920 train_time:55957ms step_avg:47.54ms
step:1178/1920 train_time:56020ms step_avg:47.56ms
step:1179/1920 train_time:56084ms step_avg:47.57ms
step:1180/1920 train_time:56146ms step_avg:47.58ms
step:1181/1920 train_time:56210ms step_avg:47.60ms
step:1182/1920 train_time:56272ms step_avg:47.61ms
step:1183/1920 train_time:56334ms step_avg:47.62ms
step:1184/1920 train_time:56395ms step_avg:47.63ms
step:1185/1920 train_time:56458ms step_avg:47.64ms
step:1186/1920 train_time:56520ms step_avg:47.66ms
step:1187/1920 train_time:56584ms step_avg:47.67ms
step:1188/1920 train_time:56646ms step_avg:47.68ms
step:1189/1920 train_time:56709ms step_avg:47.69ms
step:1190/1920 train_time:56770ms step_avg:47.71ms
step:1191/1920 train_time:56833ms step_avg:47.72ms
step:1192/1920 train_time:56894ms step_avg:47.73ms
step:1193/1920 train_time:56957ms step_avg:47.74ms
step:1194/1920 train_time:57019ms step_avg:47.75ms
step:1195/1920 train_time:57082ms step_avg:47.77ms
step:1196/1920 train_time:57144ms step_avg:47.78ms
step:1197/1920 train_time:57207ms step_avg:47.79ms
step:1198/1920 train_time:57269ms step_avg:47.80ms
step:1199/1920 train_time:57331ms step_avg:47.82ms
step:1200/1920 train_time:57393ms step_avg:47.83ms
step:1201/1920 train_time:57455ms step_avg:47.84ms
step:1202/1920 train_time:57517ms step_avg:47.85ms
step:1203/1920 train_time:57581ms step_avg:47.86ms
step:1204/1920 train_time:57643ms step_avg:47.88ms
step:1205/1920 train_time:57706ms step_avg:47.89ms
step:1206/1920 train_time:57768ms step_avg:47.90ms
step:1207/1920 train_time:57831ms step_avg:47.91ms
step:1208/1920 train_time:57892ms step_avg:47.92ms
step:1209/1920 train_time:57955ms step_avg:47.94ms
step:1210/1920 train_time:58017ms step_avg:47.95ms
step:1211/1920 train_time:58081ms step_avg:47.96ms
step:1212/1920 train_time:58143ms step_avg:47.97ms
step:1213/1920 train_time:58207ms step_avg:47.99ms
step:1214/1920 train_time:58268ms step_avg:48.00ms
step:1215/1920 train_time:58331ms step_avg:48.01ms
step:1216/1920 train_time:58392ms step_avg:48.02ms
step:1217/1920 train_time:58455ms step_avg:48.03ms
step:1218/1920 train_time:58517ms step_avg:48.04ms
step:1219/1920 train_time:58580ms step_avg:48.06ms
step:1220/1920 train_time:58643ms step_avg:48.07ms
step:1221/1920 train_time:58705ms step_avg:48.08ms
step:1222/1920 train_time:58767ms step_avg:48.09ms
step:1223/1920 train_time:58830ms step_avg:48.10ms
step:1224/1920 train_time:58892ms step_avg:48.11ms
step:1225/1920 train_time:58955ms step_avg:48.13ms
step:1226/1920 train_time:59016ms step_avg:48.14ms
step:1227/1920 train_time:59079ms step_avg:48.15ms
step:1228/1920 train_time:59141ms step_avg:48.16ms
step:1229/1920 train_time:59204ms step_avg:48.17ms
step:1230/1920 train_time:59267ms step_avg:48.18ms
step:1231/1920 train_time:59329ms step_avg:48.20ms
step:1232/1920 train_time:59391ms step_avg:48.21ms
step:1233/1920 train_time:59453ms step_avg:48.22ms
step:1234/1920 train_time:59515ms step_avg:48.23ms
step:1235/1920 train_time:59578ms step_avg:48.24ms
step:1236/1920 train_time:59641ms step_avg:48.25ms
step:1237/1920 train_time:59705ms step_avg:48.27ms
step:1238/1920 train_time:59767ms step_avg:48.28ms
step:1239/1920 train_time:59829ms step_avg:48.29ms
step:1240/1920 train_time:59891ms step_avg:48.30ms
step:1241/1920 train_time:59953ms step_avg:48.31ms
step:1242/1920 train_time:60014ms step_avg:48.32ms
step:1243/1920 train_time:60077ms step_avg:48.33ms
step:1244/1920 train_time:60140ms step_avg:48.34ms
step:1245/1920 train_time:60203ms step_avg:48.36ms
step:1246/1920 train_time:60265ms step_avg:48.37ms
step:1247/1920 train_time:60328ms step_avg:48.38ms
step:1248/1920 train_time:60390ms step_avg:48.39ms
step:1249/1920 train_time:60453ms step_avg:48.40ms
step:1250/1920 train_time:60515ms step_avg:48.41ms
step:1250/1920 val_loss:3.5506 train_time:60580ms step_avg:48.46ms
step:1251/1920 train_time:60600ms step_avg:48.44ms
step:1252/1920 train_time:60641ms step_avg:48.44ms
step:1253/1920 train_time:60709ms step_avg:48.45ms
step:1254/1920 train_time:60773ms step_avg:48.46ms
step:1255/1920 train_time:60837ms step_avg:48.48ms
step:1256/1920 train_time:60925ms step_avg:48.51ms
step:1257/1920 train_time:61013ms step_avg:48.54ms
step:1258/1920 train_time:61100ms step_avg:48.57ms
step:1259/1920 train_time:61188ms step_avg:48.60ms
step:1260/1920 train_time:61274ms step_avg:48.63ms
step:1261/1920 train_time:61362ms step_avg:48.66ms
step:1262/1920 train_time:61449ms step_avg:48.69ms
step:1263/1920 train_time:61539ms step_avg:48.72ms
step:1264/1920 train_time:61629ms step_avg:48.76ms
step:1265/1920 train_time:61721ms step_avg:48.79ms
step:1266/1920 train_time:61810ms step_avg:48.82ms
step:1267/1920 train_time:61900ms step_avg:48.86ms
step:1268/1920 train_time:61987ms step_avg:48.89ms
step:1269/1920 train_time:62075ms step_avg:48.92ms
step:1270/1920 train_time:62162ms step_avg:48.95ms
step:1271/1920 train_time:62250ms step_avg:48.98ms
step:1272/1920 train_time:62338ms step_avg:49.01ms
step:1273/1920 train_time:62426ms step_avg:49.04ms
step:1274/1920 train_time:62515ms step_avg:49.07ms
step:1275/1920 train_time:62605ms step_avg:49.10ms
step:1276/1920 train_time:62695ms step_avg:49.13ms
step:1277/1920 train_time:62786ms step_avg:49.17ms
step:1278/1920 train_time:62876ms step_avg:49.20ms
step:1279/1920 train_time:62964ms step_avg:49.23ms
step:1280/1920 train_time:63052ms step_avg:49.26ms
step:1281/1920 train_time:63140ms step_avg:49.29ms
step:1282/1920 train_time:63226ms step_avg:49.32ms
step:1283/1920 train_time:63314ms step_avg:49.35ms
step:1284/1920 train_time:63402ms step_avg:49.38ms
step:1285/1920 train_time:63489ms step_avg:49.41ms
step:1286/1920 train_time:63578ms step_avg:49.44ms
step:1287/1920 train_time:63669ms step_avg:49.47ms
step:1288/1920 train_time:63758ms step_avg:49.50ms
step:1289/1920 train_time:63848ms step_avg:49.53ms
step:1290/1920 train_time:63937ms step_avg:49.56ms
step:1291/1920 train_time:64026ms step_avg:49.59ms
step:1292/1920 train_time:64113ms step_avg:49.62ms
step:1293/1920 train_time:64202ms step_avg:49.65ms
step:1294/1920 train_time:64288ms step_avg:49.68ms
step:1295/1920 train_time:64376ms step_avg:49.71ms
step:1296/1920 train_time:64464ms step_avg:49.74ms
step:1297/1920 train_time:64552ms step_avg:49.77ms
step:1298/1920 train_time:64642ms step_avg:49.80ms
step:1299/1920 train_time:64731ms step_avg:49.83ms
step:1300/1920 train_time:64820ms step_avg:49.86ms
step:1301/1920 train_time:64909ms step_avg:49.89ms
step:1302/1920 train_time:64997ms step_avg:49.92ms
step:1303/1920 train_time:65086ms step_avg:49.95ms
step:1304/1920 train_time:65173ms step_avg:49.98ms
step:1305/1920 train_time:65262ms step_avg:50.01ms
step:1306/1920 train_time:65349ms step_avg:50.04ms
step:1307/1920 train_time:65438ms step_avg:50.07ms
step:1308/1920 train_time:65525ms step_avg:50.10ms
step:1309/1920 train_time:65614ms step_avg:50.13ms
step:1310/1920 train_time:65702ms step_avg:50.15ms
step:1311/1920 train_time:65792ms step_avg:50.18ms
step:1312/1920 train_time:65880ms step_avg:50.21ms
step:1313/1920 train_time:65969ms step_avg:50.24ms
step:1314/1920 train_time:66057ms step_avg:50.27ms
step:1315/1920 train_time:66147ms step_avg:50.30ms
step:1316/1920 train_time:66235ms step_avg:50.33ms
step:1317/1920 train_time:66325ms step_avg:50.36ms
step:1318/1920 train_time:66412ms step_avg:50.39ms
step:1319/1920 train_time:66501ms step_avg:50.42ms
step:1320/1920 train_time:66590ms step_avg:50.45ms
step:1321/1920 train_time:66678ms step_avg:50.48ms
step:1322/1920 train_time:66766ms step_avg:50.50ms
step:1323/1920 train_time:66854ms step_avg:50.53ms
step:1324/1920 train_time:66943ms step_avg:50.56ms
step:1325/1920 train_time:67031ms step_avg:50.59ms
step:1326/1920 train_time:67120ms step_avg:50.62ms
step:1327/1920 train_time:67208ms step_avg:50.65ms
step:1328/1920 train_time:67296ms step_avg:50.67ms
step:1329/1920 train_time:67385ms step_avg:50.70ms
step:1330/1920 train_time:67473ms step_avg:50.73ms
step:1331/1920 train_time:67564ms step_avg:50.76ms
step:1332/1920 train_time:67651ms step_avg:50.79ms
step:1333/1920 train_time:67740ms step_avg:50.82ms
step:1334/1920 train_time:67828ms step_avg:50.85ms
step:1335/1920 train_time:67917ms step_avg:50.87ms
step:1336/1920 train_time:68005ms step_avg:50.90ms
step:1337/1920 train_time:68093ms step_avg:50.93ms
step:1338/1920 train_time:68181ms step_avg:50.96ms
step:1339/1920 train_time:68269ms step_avg:50.99ms
step:1340/1920 train_time:68358ms step_avg:51.01ms
step:1341/1920 train_time:68447ms step_avg:51.04ms
step:1342/1920 train_time:68537ms step_avg:51.07ms
step:1343/1920 train_time:68625ms step_avg:51.10ms
step:1344/1920 train_time:68714ms step_avg:51.13ms
step:1345/1920 train_time:68803ms step_avg:51.15ms
step:1346/1920 train_time:68892ms step_avg:51.18ms
step:1347/1920 train_time:68982ms step_avg:51.21ms
step:1348/1920 train_time:69069ms step_avg:51.24ms
step:1349/1920 train_time:69159ms step_avg:51.27ms
step:1350/1920 train_time:69247ms step_avg:51.29ms
step:1351/1920 train_time:69335ms step_avg:51.32ms
step:1352/1920 train_time:69424ms step_avg:51.35ms
step:1353/1920 train_time:69513ms step_avg:51.38ms
step:1354/1920 train_time:69602ms step_avg:51.40ms
step:1355/1920 train_time:69691ms step_avg:51.43ms
step:1356/1920 train_time:69778ms step_avg:51.46ms
step:1357/1920 train_time:69867ms step_avg:51.49ms
step:1358/1920 train_time:69957ms step_avg:51.51ms
step:1359/1920 train_time:70046ms step_avg:51.54ms
step:1360/1920 train_time:70134ms step_avg:51.57ms
step:1361/1920 train_time:70225ms step_avg:51.60ms
step:1362/1920 train_time:70313ms step_avg:51.62ms
step:1363/1920 train_time:70402ms step_avg:51.65ms
step:1364/1920 train_time:70489ms step_avg:51.68ms
step:1365/1920 train_time:70578ms step_avg:51.71ms
step:1366/1920 train_time:70666ms step_avg:51.73ms
step:1367/1920 train_time:70754ms step_avg:51.76ms
step:1368/1920 train_time:70843ms step_avg:51.79ms
step:1369/1920 train_time:70932ms step_avg:51.81ms
step:1370/1920 train_time:71021ms step_avg:51.84ms
step:1371/1920 train_time:71110ms step_avg:51.87ms
step:1372/1920 train_time:71199ms step_avg:51.89ms
step:1373/1920 train_time:71287ms step_avg:51.92ms
step:1374/1920 train_time:71375ms step_avg:51.95ms
step:1375/1920 train_time:71465ms step_avg:51.97ms
step:1376/1920 train_time:71553ms step_avg:52.00ms
step:1377/1920 train_time:71643ms step_avg:52.03ms
step:1378/1920 train_time:71730ms step_avg:52.05ms
step:1379/1920 train_time:71819ms step_avg:52.08ms
step:1380/1920 train_time:71907ms step_avg:52.11ms
step:1381/1920 train_time:71996ms step_avg:52.13ms
step:1382/1920 train_time:72084ms step_avg:52.16ms
step:1383/1920 train_time:72172ms step_avg:52.19ms
step:1384/1920 train_time:72261ms step_avg:52.21ms
step:1385/1920 train_time:72350ms step_avg:52.24ms
step:1386/1920 train_time:72438ms step_avg:52.26ms
step:1387/1920 train_time:72527ms step_avg:52.29ms
step:1388/1920 train_time:72616ms step_avg:52.32ms
step:1389/1920 train_time:72705ms step_avg:52.34ms
step:1390/1920 train_time:72793ms step_avg:52.37ms
step:1391/1920 train_time:72882ms step_avg:52.40ms
step:1392/1920 train_time:72970ms step_avg:52.42ms
step:1393/1920 train_time:73058ms step_avg:52.45ms
step:1394/1920 train_time:73146ms step_avg:52.47ms
step:1395/1920 train_time:73235ms step_avg:52.50ms
step:1396/1920 train_time:73324ms step_avg:52.52ms
step:1397/1920 train_time:73413ms step_avg:52.55ms
step:1398/1920 train_time:73502ms step_avg:52.58ms
step:1399/1920 train_time:73590ms step_avg:52.60ms
step:1400/1920 train_time:73679ms step_avg:52.63ms
step:1401/1920 train_time:73768ms step_avg:52.65ms
step:1402/1920 train_time:73856ms step_avg:52.68ms
step:1403/1920 train_time:73946ms step_avg:52.71ms
step:1404/1920 train_time:74034ms step_avg:52.73ms
step:1405/1920 train_time:74123ms step_avg:52.76ms
step:1406/1920 train_time:74210ms step_avg:52.78ms
step:1407/1920 train_time:74299ms step_avg:52.81ms
step:1408/1920 train_time:74387ms step_avg:52.83ms
step:1409/1920 train_time:74476ms step_avg:52.86ms
step:1410/1920 train_time:74565ms step_avg:52.88ms
step:1411/1920 train_time:74653ms step_avg:52.91ms
step:1412/1920 train_time:74741ms step_avg:52.93ms
step:1413/1920 train_time:74830ms step_avg:52.96ms
step:1414/1920 train_time:74918ms step_avg:52.98ms
step:1415/1920 train_time:75008ms step_avg:53.01ms
step:1416/1920 train_time:75098ms step_avg:53.04ms
step:1417/1920 train_time:75187ms step_avg:53.06ms
step:1418/1920 train_time:75276ms step_avg:53.09ms
step:1419/1920 train_time:75366ms step_avg:53.11ms
step:1420/1920 train_time:75455ms step_avg:53.14ms
step:1421/1920 train_time:75545ms step_avg:53.16ms
step:1422/1920 train_time:75633ms step_avg:53.19ms
step:1423/1920 train_time:75724ms step_avg:53.21ms
step:1424/1920 train_time:75811ms step_avg:53.24ms
step:1425/1920 train_time:75900ms step_avg:53.26ms
step:1426/1920 train_time:75988ms step_avg:53.29ms
step:1427/1920 train_time:76076ms step_avg:53.31ms
step:1428/1920 train_time:76165ms step_avg:53.34ms
step:1429/1920 train_time:76253ms step_avg:53.36ms
step:1430/1920 train_time:76342ms step_avg:53.39ms
step:1431/1920 train_time:76430ms step_avg:53.41ms
step:1432/1920 train_time:76519ms step_avg:53.43ms
step:1433/1920 train_time:76607ms step_avg:53.46ms
step:1434/1920 train_time:76696ms step_avg:53.48ms
step:1435/1920 train_time:76785ms step_avg:53.51ms
step:1436/1920 train_time:76873ms step_avg:53.53ms
step:1437/1920 train_time:76962ms step_avg:53.56ms
step:1438/1920 train_time:77049ms step_avg:53.58ms
step:1439/1920 train_time:77138ms step_avg:53.61ms
step:1440/1920 train_time:77226ms step_avg:53.63ms
step:1441/1920 train_time:77315ms step_avg:53.65ms
step:1442/1920 train_time:77403ms step_avg:53.68ms
step:1443/1920 train_time:77492ms step_avg:53.70ms
step:1444/1920 train_time:77580ms step_avg:53.73ms
step:1445/1920 train_time:77668ms step_avg:53.75ms
step:1446/1920 train_time:77757ms step_avg:53.77ms
step:1447/1920 train_time:77846ms step_avg:53.80ms
step:1448/1920 train_time:77935ms step_avg:53.82ms
step:1449/1920 train_time:78024ms step_avg:53.85ms
step:1450/1920 train_time:78113ms step_avg:53.87ms
step:1451/1920 train_time:78202ms step_avg:53.90ms
step:1452/1920 train_time:78290ms step_avg:53.92ms
step:1453/1920 train_time:78379ms step_avg:53.94ms
step:1454/1920 train_time:78468ms step_avg:53.97ms
step:1455/1920 train_time:78558ms step_avg:53.99ms
step:1456/1920 train_time:78645ms step_avg:54.01ms
step:1457/1920 train_time:78733ms step_avg:54.04ms
step:1458/1920 train_time:78822ms step_avg:54.06ms
step:1459/1920 train_time:78910ms step_avg:54.08ms
step:1460/1920 train_time:78998ms step_avg:54.11ms
step:1461/1920 train_time:79088ms step_avg:54.13ms
step:1462/1920 train_time:79175ms step_avg:54.16ms
step:1463/1920 train_time:79266ms step_avg:54.18ms
step:1464/1920 train_time:79355ms step_avg:54.20ms
step:1465/1920 train_time:79444ms step_avg:54.23ms
step:1466/1920 train_time:79532ms step_avg:54.25ms
step:1467/1920 train_time:79620ms step_avg:54.27ms
step:1468/1920 train_time:79708ms step_avg:54.30ms
step:1469/1920 train_time:79797ms step_avg:54.32ms
step:1470/1920 train_time:79884ms step_avg:54.34ms
step:1471/1920 train_time:79973ms step_avg:54.37ms
step:1472/1920 train_time:80062ms step_avg:54.39ms
step:1473/1920 train_time:80150ms step_avg:54.41ms
step:1474/1920 train_time:80238ms step_avg:54.44ms
step:1475/1920 train_time:80328ms step_avg:54.46ms
step:1476/1920 train_time:80416ms step_avg:54.48ms
step:1477/1920 train_time:80506ms step_avg:54.51ms
step:1478/1920 train_time:80595ms step_avg:54.53ms
step:1479/1920 train_time:80685ms step_avg:54.55ms
step:1480/1920 train_time:80773ms step_avg:54.58ms
step:1481/1920 train_time:80862ms step_avg:54.60ms
step:1482/1920 train_time:80950ms step_avg:54.62ms
step:1483/1920 train_time:81039ms step_avg:54.65ms
step:1484/1920 train_time:81127ms step_avg:54.67ms
step:1485/1920 train_time:81216ms step_avg:54.69ms
step:1486/1920 train_time:81304ms step_avg:54.71ms
step:1487/1920 train_time:81392ms step_avg:54.74ms
step:1488/1920 train_time:81480ms step_avg:54.76ms
step:1489/1920 train_time:81569ms step_avg:54.78ms
step:1490/1920 train_time:81658ms step_avg:54.80ms
step:1491/1920 train_time:81748ms step_avg:54.83ms
step:1492/1920 train_time:81837ms step_avg:54.85ms
step:1493/1920 train_time:81926ms step_avg:54.87ms
step:1494/1920 train_time:82014ms step_avg:54.90ms
step:1495/1920 train_time:82103ms step_avg:54.92ms
step:1496/1920 train_time:82191ms step_avg:54.94ms
step:1497/1920 train_time:82280ms step_avg:54.96ms
step:1498/1920 train_time:82368ms step_avg:54.99ms
step:1499/1920 train_time:82458ms step_avg:55.01ms
step:1500/1920 train_time:82545ms step_avg:55.03ms
step:1500/1920 val_loss:3.4139 train_time:82637ms step_avg:55.09ms
step:1501/1920 train_time:82656ms step_avg:55.07ms
step:1502/1920 train_time:82727ms step_avg:55.08ms
step:1503/1920 train_time:82824ms step_avg:55.11ms
step:1504/1920 train_time:82913ms step_avg:55.13ms
step:1505/1920 train_time:83001ms step_avg:55.15ms
step:1506/1920 train_time:83087ms step_avg:55.17ms
step:1507/1920 train_time:83175ms step_avg:55.19ms
step:1508/1920 train_time:83262ms step_avg:55.21ms
step:1509/1920 train_time:83350ms step_avg:55.24ms
step:1510/1920 train_time:83437ms step_avg:55.26ms
step:1511/1920 train_time:83524ms step_avg:55.28ms
step:1512/1920 train_time:83614ms step_avg:55.30ms
step:1513/1920 train_time:83706ms step_avg:55.32ms
step:1514/1920 train_time:83799ms step_avg:55.35ms
step:1515/1920 train_time:83889ms step_avg:55.37ms
step:1516/1920 train_time:83977ms step_avg:55.39ms
step:1517/1920 train_time:84065ms step_avg:55.42ms
step:1518/1920 train_time:84153ms step_avg:55.44ms
step:1519/1920 train_time:84241ms step_avg:55.46ms
step:1520/1920 train_time:84328ms step_avg:55.48ms
step:1521/1920 train_time:84415ms step_avg:55.50ms
step:1522/1920 train_time:84502ms step_avg:55.52ms
step:1523/1920 train_time:84590ms step_avg:55.54ms
step:1524/1920 train_time:84679ms step_avg:55.56ms
step:1525/1920 train_time:84770ms step_avg:55.59ms
step:1526/1920 train_time:84859ms step_avg:55.61ms
step:1527/1920 train_time:84949ms step_avg:55.63ms
step:1528/1920 train_time:85037ms step_avg:55.65ms
step:1529/1920 train_time:85125ms step_avg:55.67ms
step:1530/1920 train_time:85212ms step_avg:55.69ms
step:1531/1920 train_time:85300ms step_avg:55.72ms
step:1532/1920 train_time:85388ms step_avg:55.74ms
step:1533/1920 train_time:85476ms step_avg:55.76ms
step:1534/1920 train_time:85563ms step_avg:55.78ms
step:1535/1920 train_time:85654ms step_avg:55.80ms
step:1536/1920 train_time:85744ms step_avg:55.82ms
step:1537/1920 train_time:85834ms step_avg:55.85ms
step:1538/1920 train_time:85925ms step_avg:55.87ms
step:1539/1920 train_time:86013ms step_avg:55.89ms
step:1540/1920 train_time:86103ms step_avg:55.91ms
step:1541/1920 train_time:86192ms step_avg:55.93ms
step:1542/1920 train_time:86279ms step_avg:55.95ms
step:1543/1920 train_time:86367ms step_avg:55.97ms
step:1544/1920 train_time:86455ms step_avg:55.99ms
step:1545/1920 train_time:86544ms step_avg:56.02ms
step:1546/1920 train_time:86632ms step_avg:56.04ms
step:1547/1920 train_time:86722ms step_avg:56.06ms
step:1548/1920 train_time:86812ms step_avg:56.08ms
step:1549/1920 train_time:86900ms step_avg:56.10ms
step:1550/1920 train_time:86989ms step_avg:56.12ms
step:1551/1920 train_time:87077ms step_avg:56.14ms
step:1552/1920 train_time:87165ms step_avg:56.16ms
step:1553/1920 train_time:87253ms step_avg:56.18ms
step:1554/1920 train_time:87342ms step_avg:56.20ms
step:1555/1920 train_time:87430ms step_avg:56.23ms
step:1556/1920 train_time:87517ms step_avg:56.24ms
step:1557/1920 train_time:87606ms step_avg:56.27ms
step:1558/1920 train_time:87694ms step_avg:56.29ms
step:1559/1920 train_time:87783ms step_avg:56.31ms
step:1560/1920 train_time:87872ms step_avg:56.33ms
step:1561/1920 train_time:87961ms step_avg:56.35ms
step:1562/1920 train_time:88049ms step_avg:56.37ms
step:1563/1920 train_time:88138ms step_avg:56.39ms
step:1564/1920 train_time:88225ms step_avg:56.41ms
step:1565/1920 train_time:88314ms step_avg:56.43ms
step:1566/1920 train_time:88402ms step_avg:56.45ms
step:1567/1920 train_time:88491ms step_avg:56.47ms
step:1568/1920 train_time:88579ms step_avg:56.49ms
step:1569/1920 train_time:88668ms step_avg:56.51ms
step:1570/1920 train_time:88757ms step_avg:56.53ms
step:1571/1920 train_time:88846ms step_avg:56.55ms
step:1572/1920 train_time:88934ms step_avg:56.57ms
step:1573/1920 train_time:89023ms step_avg:56.59ms
step:1574/1920 train_time:89112ms step_avg:56.61ms
step:1575/1920 train_time:89200ms step_avg:56.63ms
step:1576/1920 train_time:89288ms step_avg:56.65ms
step:1577/1920 train_time:89376ms step_avg:56.67ms
step:1578/1920 train_time:89464ms step_avg:56.69ms
step:1579/1920 train_time:89554ms step_avg:56.72ms
step:1580/1920 train_time:89643ms step_avg:56.74ms
step:1581/1920 train_time:89732ms step_avg:56.76ms
step:1582/1920 train_time:89821ms step_avg:56.78ms
step:1583/1920 train_time:89910ms step_avg:56.80ms
step:1584/1920 train_time:89999ms step_avg:56.82ms
step:1585/1920 train_time:90089ms step_avg:56.84ms
step:1586/1920 train_time:90176ms step_avg:56.86ms
step:1587/1920 train_time:90264ms step_avg:56.88ms
step:1588/1920 train_time:90352ms step_avg:56.90ms
step:1589/1920 train_time:90442ms step_avg:56.92ms
step:1590/1920 train_time:90531ms step_avg:56.94ms
step:1591/1920 train_time:90620ms step_avg:56.96ms
step:1592/1920 train_time:90710ms step_avg:56.98ms
step:1593/1920 train_time:90799ms step_avg:57.00ms
step:1594/1920 train_time:90888ms step_avg:57.02ms
step:1595/1920 train_time:90977ms step_avg:57.04ms
step:1596/1920 train_time:91065ms step_avg:57.06ms
step:1597/1920 train_time:91155ms step_avg:57.08ms
step:1598/1920 train_time:91242ms step_avg:57.10ms
step:1599/1920 train_time:91332ms step_avg:57.12ms
step:1600/1920 train_time:91421ms step_avg:57.14ms
step:1601/1920 train_time:91511ms step_avg:57.16ms
step:1602/1920 train_time:91599ms step_avg:57.18ms
step:1603/1920 train_time:91688ms step_avg:57.20ms
step:1604/1920 train_time:91776ms step_avg:57.22ms
step:1605/1920 train_time:91864ms step_avg:57.24ms
step:1606/1920 train_time:91953ms step_avg:57.26ms
step:1607/1920 train_time:92042ms step_avg:57.28ms
step:1608/1920 train_time:92130ms step_avg:57.30ms
step:1609/1920 train_time:92219ms step_avg:57.31ms
step:1610/1920 train_time:92307ms step_avg:57.33ms
step:1611/1920 train_time:92396ms step_avg:57.35ms
step:1612/1920 train_time:92483ms step_avg:57.37ms
step:1613/1920 train_time:92573ms step_avg:57.39ms
step:1614/1920 train_time:92662ms step_avg:57.41ms
step:1615/1920 train_time:92752ms step_avg:57.43ms
step:1616/1920 train_time:92840ms step_avg:57.45ms
step:1617/1920 train_time:92929ms step_avg:57.47ms
step:1618/1920 train_time:93017ms step_avg:57.49ms
step:1619/1920 train_time:93106ms step_avg:57.51ms
step:1620/1920 train_time:93195ms step_avg:57.53ms
step:1621/1920 train_time:93284ms step_avg:57.55ms
step:1622/1920 train_time:93372ms step_avg:57.57ms
step:1623/1920 train_time:93461ms step_avg:57.59ms
step:1624/1920 train_time:93549ms step_avg:57.60ms
step:1625/1920 train_time:93638ms step_avg:57.62ms
step:1626/1920 train_time:93728ms step_avg:57.64ms
step:1627/1920 train_time:93817ms step_avg:57.66ms
step:1628/1920 train_time:93906ms step_avg:57.68ms
step:1629/1920 train_time:93994ms step_avg:57.70ms
step:1630/1920 train_time:94084ms step_avg:57.72ms
step:1631/1920 train_time:94173ms step_avg:57.74ms
step:1632/1920 train_time:94261ms step_avg:57.76ms
step:1633/1920 train_time:94351ms step_avg:57.78ms
step:1634/1920 train_time:94439ms step_avg:57.80ms
step:1635/1920 train_time:94528ms step_avg:57.82ms
step:1636/1920 train_time:94616ms step_avg:57.83ms
step:1637/1920 train_time:94706ms step_avg:57.85ms
step:1638/1920 train_time:94793ms step_avg:57.87ms
step:1639/1920 train_time:94882ms step_avg:57.89ms
step:1640/1920 train_time:94970ms step_avg:57.91ms
step:1641/1920 train_time:95059ms step_avg:57.93ms
step:1642/1920 train_time:95148ms step_avg:57.95ms
step:1643/1920 train_time:95236ms step_avg:57.96ms
step:1644/1920 train_time:95325ms step_avg:57.98ms
step:1645/1920 train_time:95413ms step_avg:58.00ms
step:1646/1920 train_time:95502ms step_avg:58.02ms
step:1647/1920 train_time:95591ms step_avg:58.04ms
step:1648/1920 train_time:95679ms step_avg:58.06ms
step:1649/1920 train_time:95769ms step_avg:58.08ms
step:1650/1920 train_time:95857ms step_avg:58.09ms
step:1651/1920 train_time:95946ms step_avg:58.11ms
step:1652/1920 train_time:96033ms step_avg:58.13ms
step:1653/1920 train_time:96121ms step_avg:58.15ms
step:1654/1920 train_time:96210ms step_avg:58.17ms
step:1655/1920 train_time:96299ms step_avg:58.19ms
step:1656/1920 train_time:96387ms step_avg:58.20ms
step:1657/1920 train_time:96476ms step_avg:58.22ms
step:1658/1920 train_time:96564ms step_avg:58.24ms
step:1659/1920 train_time:96653ms step_avg:58.26ms
step:1660/1920 train_time:96743ms step_avg:58.28ms
step:1661/1920 train_time:96832ms step_avg:58.30ms
step:1662/1920 train_time:96920ms step_avg:58.32ms
step:1663/1920 train_time:97008ms step_avg:58.33ms
step:1664/1920 train_time:97096ms step_avg:58.35ms
step:1665/1920 train_time:97185ms step_avg:58.37ms
step:1666/1920 train_time:97273ms step_avg:58.39ms
step:1667/1920 train_time:97362ms step_avg:58.41ms
step:1668/1920 train_time:97451ms step_avg:58.42ms
step:1669/1920 train_time:97539ms step_avg:58.44ms
step:1670/1920 train_time:97627ms step_avg:58.46ms
step:1671/1920 train_time:97715ms step_avg:58.48ms
step:1672/1920 train_time:97804ms step_avg:58.50ms
step:1673/1920 train_time:97895ms step_avg:58.51ms
step:1674/1920 train_time:97983ms step_avg:58.53ms
step:1675/1920 train_time:98072ms step_avg:58.55ms
step:1676/1920 train_time:98161ms step_avg:58.57ms
step:1677/1920 train_time:98251ms step_avg:58.59ms
step:1678/1920 train_time:98340ms step_avg:58.61ms
step:1679/1920 train_time:98429ms step_avg:58.62ms
step:1680/1920 train_time:98517ms step_avg:58.64ms
step:1681/1920 train_time:98606ms step_avg:58.66ms
step:1682/1920 train_time:98694ms step_avg:58.68ms
step:1683/1920 train_time:98783ms step_avg:58.69ms
step:1684/1920 train_time:98872ms step_avg:58.71ms
step:1685/1920 train_time:98961ms step_avg:58.73ms
step:1686/1920 train_time:99050ms step_avg:58.75ms
step:1687/1920 train_time:99137ms step_avg:58.77ms
step:1688/1920 train_time:99226ms step_avg:58.78ms
step:1689/1920 train_time:99315ms step_avg:58.80ms
step:1690/1920 train_time:99404ms step_avg:58.82ms
step:1691/1920 train_time:99493ms step_avg:58.84ms
step:1692/1920 train_time:99581ms step_avg:58.85ms
step:1693/1920 train_time:99672ms step_avg:58.87ms
step:1694/1920 train_time:99760ms step_avg:58.89ms
step:1695/1920 train_time:99849ms step_avg:58.91ms
step:1696/1920 train_time:99937ms step_avg:58.93ms
step:1697/1920 train_time:100026ms step_avg:58.94ms
step:1698/1920 train_time:100114ms step_avg:58.96ms
step:1699/1920 train_time:100202ms step_avg:58.98ms
step:1700/1920 train_time:100291ms step_avg:58.99ms
step:1701/1920 train_time:100379ms step_avg:59.01ms
step:1702/1920 train_time:100468ms step_avg:59.03ms
step:1703/1920 train_time:100557ms step_avg:59.05ms
step:1704/1920 train_time:100645ms step_avg:59.06ms
step:1705/1920 train_time:100733ms step_avg:59.08ms
step:1706/1920 train_time:100821ms step_avg:59.10ms
step:1707/1920 train_time:100911ms step_avg:59.12ms
step:1708/1920 train_time:100998ms step_avg:59.13ms
step:1709/1920 train_time:101088ms step_avg:59.15ms
step:1710/1920 train_time:101176ms step_avg:59.17ms
step:1711/1920 train_time:101266ms step_avg:59.19ms
step:1712/1920 train_time:101354ms step_avg:59.20ms
step:1713/1920 train_time:101445ms step_avg:59.22ms
step:1714/1920 train_time:101532ms step_avg:59.24ms
step:1715/1920 train_time:101621ms step_avg:59.25ms
step:1716/1920 train_time:101710ms step_avg:59.27ms
step:1717/1920 train_time:101799ms step_avg:59.29ms
step:1718/1920 train_time:101887ms step_avg:59.31ms
step:1719/1920 train_time:101975ms step_avg:59.32ms
step:1720/1920 train_time:102063ms step_avg:59.34ms
step:1721/1920 train_time:102153ms step_avg:59.36ms
step:1722/1920 train_time:102242ms step_avg:59.37ms
step:1723/1920 train_time:102332ms step_avg:59.39ms
step:1724/1920 train_time:102420ms step_avg:59.41ms
step:1725/1920 train_time:102510ms step_avg:59.43ms
step:1726/1920 train_time:102598ms step_avg:59.44ms
step:1727/1920 train_time:102686ms step_avg:59.46ms
step:1728/1920 train_time:102774ms step_avg:59.48ms
step:1729/1920 train_time:102864ms step_avg:59.49ms
step:1730/1920 train_time:102951ms step_avg:59.51ms
step:1731/1920 train_time:103040ms step_avg:59.53ms
step:1732/1920 train_time:103128ms step_avg:59.54ms
step:1733/1920 train_time:103217ms step_avg:59.56ms
step:1734/1920 train_time:103306ms step_avg:59.58ms
step:1735/1920 train_time:103395ms step_avg:59.59ms
step:1736/1920 train_time:103483ms step_avg:59.61ms
step:1737/1920 train_time:103572ms step_avg:59.63ms
step:1738/1920 train_time:103662ms step_avg:59.64ms
step:1739/1920 train_time:103751ms step_avg:59.66ms
step:1740/1920 train_time:103839ms step_avg:59.68ms
step:1741/1920 train_time:103929ms step_avg:59.69ms
step:1742/1920 train_time:104016ms step_avg:59.71ms
step:1743/1920 train_time:104105ms step_avg:59.73ms
step:1744/1920 train_time:104193ms step_avg:59.74ms
step:1745/1920 train_time:104282ms step_avg:59.76ms
step:1746/1920 train_time:104371ms step_avg:59.78ms
step:1747/1920 train_time:104460ms step_avg:59.79ms
step:1748/1920 train_time:104548ms step_avg:59.81ms
step:1749/1920 train_time:104638ms step_avg:59.83ms
step:1750/1920 train_time:104727ms step_avg:59.84ms
step:1750/1920 val_loss:3.3227 train_time:104818ms step_avg:59.90ms
step:1751/1920 train_time:104836ms step_avg:59.87ms
step:1752/1920 train_time:104908ms step_avg:59.88ms
step:1753/1920 train_time:105002ms step_avg:59.90ms
step:1754/1920 train_time:105091ms step_avg:59.92ms
step:1755/1920 train_time:105179ms step_avg:59.93ms
step:1756/1920 train_time:105267ms step_avg:59.95ms
step:1757/1920 train_time:105354ms step_avg:59.96ms
step:1758/1920 train_time:105440ms step_avg:59.98ms
step:1759/1920 train_time:105527ms step_avg:59.99ms
step:1760/1920 train_time:105616ms step_avg:60.01ms
step:1761/1920 train_time:105704ms step_avg:60.02ms
step:1762/1920 train_time:105794ms step_avg:60.04ms
step:1763/1920 train_time:105886ms step_avg:60.06ms
step:1764/1920 train_time:105977ms step_avg:60.08ms
step:1765/1920 train_time:106067ms step_avg:60.09ms
step:1766/1920 train_time:106156ms step_avg:60.11ms
step:1767/1920 train_time:106244ms step_avg:60.13ms
step:1768/1920 train_time:106331ms step_avg:60.14ms
step:1769/1920 train_time:106420ms step_avg:60.16ms
step:1770/1920 train_time:106507ms step_avg:60.17ms
step:1771/1920 train_time:106594ms step_avg:60.19ms
step:1772/1920 train_time:106681ms step_avg:60.20ms
step:1773/1920 train_time:106772ms step_avg:60.22ms
step:1774/1920 train_time:106861ms step_avg:60.24ms
step:1775/1920 train_time:106952ms step_avg:60.25ms
step:1776/1920 train_time:107040ms step_avg:60.27ms
step:1777/1920 train_time:107129ms step_avg:60.29ms
step:1778/1920 train_time:107218ms step_avg:60.30ms
step:1779/1920 train_time:107306ms step_avg:60.32ms
step:1780/1920 train_time:107394ms step_avg:60.33ms
step:1781/1920 train_time:107483ms step_avg:60.35ms
step:1782/1920 train_time:107570ms step_avg:60.36ms
step:1783/1920 train_time:107660ms step_avg:60.38ms
step:1784/1920 train_time:107748ms step_avg:60.40ms
step:1785/1920 train_time:107839ms step_avg:60.41ms
step:1786/1920 train_time:107928ms step_avg:60.43ms
step:1787/1920 train_time:108018ms step_avg:60.45ms
step:1788/1920 train_time:108106ms step_avg:60.46ms
step:1789/1920 train_time:108196ms step_avg:60.48ms
step:1790/1920 train_time:108283ms step_avg:60.49ms
step:1791/1920 train_time:108372ms step_avg:60.51ms
step:1792/1920 train_time:108459ms step_avg:60.52ms
step:1793/1920 train_time:108547ms step_avg:60.54ms
step:1794/1920 train_time:108636ms step_avg:60.55ms
step:1795/1920 train_time:108724ms step_avg:60.57ms
step:1796/1920 train_time:108813ms step_avg:60.59ms
step:1797/1920 train_time:108903ms step_avg:60.60ms
step:1798/1920 train_time:108992ms step_avg:60.62ms
step:1799/1920 train_time:109082ms step_avg:60.63ms
step:1800/1920 train_time:109171ms step_avg:60.65ms
step:1801/1920 train_time:109260ms step_avg:60.67ms
step:1802/1920 train_time:109348ms step_avg:60.68ms
step:1803/1920 train_time:109436ms step_avg:60.70ms
step:1804/1920 train_time:109523ms step_avg:60.71ms
step:1805/1920 train_time:109612ms step_avg:60.73ms
step:1806/1920 train_time:109700ms step_avg:60.74ms
step:1807/1920 train_time:109789ms step_avg:60.76ms
step:1808/1920 train_time:109878ms step_avg:60.77ms
step:1809/1920 train_time:109967ms step_avg:60.79ms
step:1810/1920 train_time:110056ms step_avg:60.80ms
step:1811/1920 train_time:110145ms step_avg:60.82ms
step:1812/1920 train_time:110234ms step_avg:60.84ms
step:1813/1920 train_time:110322ms step_avg:60.85ms
step:1814/1920 train_time:110410ms step_avg:60.87ms
step:1815/1920 train_time:110500ms step_avg:60.88ms
step:1816/1920 train_time:110588ms step_avg:60.90ms
step:1817/1920 train_time:110678ms step_avg:60.91ms
step:1818/1920 train_time:110766ms step_avg:60.93ms
step:1819/1920 train_time:110856ms step_avg:60.94ms
step:1820/1920 train_time:110944ms step_avg:60.96ms
step:1821/1920 train_time:111033ms step_avg:60.97ms
step:1822/1920 train_time:111121ms step_avg:60.99ms
step:1823/1920 train_time:111211ms step_avg:61.00ms
step:1824/1920 train_time:111298ms step_avg:61.02ms
step:1825/1920 train_time:111387ms step_avg:61.03ms
step:1826/1920 train_time:111476ms step_avg:61.05ms
step:1827/1920 train_time:111564ms step_avg:61.06ms
step:1828/1920 train_time:111652ms step_avg:61.08ms
step:1829/1920 train_time:111742ms step_avg:61.09ms
step:1830/1920 train_time:111830ms step_avg:61.11ms
step:1831/1920 train_time:111920ms step_avg:61.13ms
step:1832/1920 train_time:112009ms step_avg:61.14ms
step:1833/1920 train_time:112099ms step_avg:61.16ms
step:1834/1920 train_time:112189ms step_avg:61.17ms
step:1835/1920 train_time:112279ms step_avg:61.19ms
step:1836/1920 train_time:112366ms step_avg:61.20ms
step:1837/1920 train_time:112455ms step_avg:61.22ms
step:1838/1920 train_time:112543ms step_avg:61.23ms
step:1839/1920 train_time:112631ms step_avg:61.25ms
step:1840/1920 train_time:112719ms step_avg:61.26ms
step:1841/1920 train_time:112808ms step_avg:61.28ms
step:1842/1920 train_time:112896ms step_avg:61.29ms
step:1843/1920 train_time:112985ms step_avg:61.30ms
step:1844/1920 train_time:113072ms step_avg:61.32ms
step:1845/1920 train_time:113162ms step_avg:61.33ms
step:1846/1920 train_time:113251ms step_avg:61.35ms
step:1847/1920 train_time:113340ms step_avg:61.36ms
step:1848/1920 train_time:113427ms step_avg:61.38ms
step:1849/1920 train_time:113516ms step_avg:61.39ms
step:1850/1920 train_time:113604ms step_avg:61.41ms
step:1851/1920 train_time:113693ms step_avg:61.42ms
step:1852/1920 train_time:113781ms step_avg:61.44ms
step:1853/1920 train_time:113871ms step_avg:61.45ms
step:1854/1920 train_time:113959ms step_avg:61.47ms
step:1855/1920 train_time:114048ms step_avg:61.48ms
step:1856/1920 train_time:114137ms step_avg:61.50ms
step:1857/1920 train_time:114226ms step_avg:61.51ms
step:1858/1920 train_time:114314ms step_avg:61.53ms
step:1859/1920 train_time:114402ms step_avg:61.54ms
step:1860/1920 train_time:114492ms step_avg:61.55ms
step:1861/1920 train_time:114582ms step_avg:61.57ms
step:1862/1920 train_time:114671ms step_avg:61.58ms
step:1863/1920 train_time:114760ms step_avg:61.60ms
step:1864/1920 train_time:114848ms step_avg:61.61ms
step:1865/1920 train_time:114938ms step_avg:61.63ms
step:1866/1920 train_time:115027ms step_avg:61.64ms
step:1867/1920 train_time:115116ms step_avg:61.66ms
step:1868/1920 train_time:115203ms step_avg:61.67ms
step:1869/1920 train_time:115292ms step_avg:61.69ms
step:1870/1920 train_time:115380ms step_avg:61.70ms
step:1871/1920 train_time:115469ms step_avg:61.72ms
step:1872/1920 train_time:115558ms step_avg:61.73ms
step:1873/1920 train_time:115646ms step_avg:61.74ms
step:1874/1920 train_time:115736ms step_avg:61.76ms
step:1875/1920 train_time:115824ms step_avg:61.77ms
step:1876/1920 train_time:115912ms step_avg:61.79ms
step:1877/1920 train_time:116001ms step_avg:61.80ms
step:1878/1920 train_time:116090ms step_avg:61.82ms
step:1879/1920 train_time:116180ms step_avg:61.83ms
step:1880/1920 train_time:116269ms step_avg:61.84ms
step:1881/1920 train_time:116358ms step_avg:61.86ms
step:1882/1920 train_time:116446ms step_avg:61.87ms
step:1883/1920 train_time:116535ms step_avg:61.89ms
step:1884/1920 train_time:116623ms step_avg:61.90ms
step:1885/1920 train_time:116712ms step_avg:61.92ms
step:1886/1920 train_time:116800ms step_avg:61.93ms
step:1887/1920 train_time:116890ms step_avg:61.94ms
step:1888/1920 train_time:116979ms step_avg:61.96ms
step:1889/1920 train_time:117068ms step_avg:61.97ms
step:1890/1920 train_time:117158ms step_avg:61.99ms
step:1891/1920 train_time:117247ms step_avg:62.00ms
step:1892/1920 train_time:117336ms step_avg:62.02ms
step:1893/1920 train_time:117426ms step_avg:62.03ms
step:1894/1920 train_time:117515ms step_avg:62.05ms
step:1895/1920 train_time:117604ms step_avg:62.06ms
step:1896/1920 train_time:117692ms step_avg:62.07ms
step:1897/1920 train_time:117782ms step_avg:62.09ms
step:1898/1920 train_time:117870ms step_avg:62.10ms
step:1899/1920 train_time:117960ms step_avg:62.12ms
step:1900/1920 train_time:118049ms step_avg:62.13ms
step:1901/1920 train_time:118140ms step_avg:62.15ms
step:1902/1920 train_time:118229ms step_avg:62.16ms
step:1903/1920 train_time:118319ms step_avg:62.18ms
step:1904/1920 train_time:118407ms step_avg:62.19ms
step:1905/1920 train_time:118497ms step_avg:62.20ms
step:1906/1920 train_time:118586ms step_avg:62.22ms
step:1907/1920 train_time:118675ms step_avg:62.23ms
step:1908/1920 train_time:118763ms step_avg:62.24ms
step:1909/1920 train_time:118852ms step_avg:62.26ms
step:1910/1920 train_time:118940ms step_avg:62.27ms
step:1911/1920 train_time:119030ms step_avg:62.29ms
step:1912/1920 train_time:119118ms step_avg:62.30ms
step:1913/1920 train_time:119208ms step_avg:62.31ms
step:1914/1920 train_time:119296ms step_avg:62.33ms
step:1915/1920 train_time:119386ms step_avg:62.34ms
step:1916/1920 train_time:119476ms step_avg:62.36ms
step:1917/1920 train_time:119567ms step_avg:62.37ms
step:1918/1920 train_time:119655ms step_avg:62.39ms
step:1919/1920 train_time:119743ms step_avg:62.40ms
step:1920/1920 train_time:119832ms step_avg:62.41ms
step:1920/1920 val_loss:3.2780 train_time:119923ms step_avg:62.46ms
peak memory allocated: 29863 MiB reserved: 44038 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:34:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    248313      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    248314      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    248315      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    248316      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    248317      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    248318      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    248319      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    248320      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    248314      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    248315      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    248316      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    248317      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    248318      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    248319      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    248320      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8367 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:69ms step_avg:69.18ms
step:2/1920 train_time:91ms step_avg:45.33ms
step:3/1920 train_time:116ms step_avg:38.59ms
step:4/1920 train_time:150ms step_avg:37.39ms
step:5/1920 train_time:184ms step_avg:36.74ms
step:6/1920 train_time:263ms step_avg:43.87ms
step:7/1920 train_time:288ms step_avg:41.18ms
step:8/1920 train_time:322ms step_avg:40.28ms
step:9/1920 train_time:356ms step_avg:39.59ms
step:10/1920 train_time:390ms step_avg:39.03ms
step:11/1920 train_time:425ms step_avg:38.60ms
step:12/1920 train_time:459ms step_avg:38.22ms
step:13/1920 train_time:493ms step_avg:37.94ms
step:14/1920 train_time:527ms step_avg:37.68ms
step:15/1920 train_time:562ms step_avg:37.45ms
step:16/1920 train_time:596ms step_avg:37.24ms
step:17/1920 train_time:630ms step_avg:37.08ms
step:18/1920 train_time:665ms step_avg:36.92ms
step:19/1920 train_time:699ms step_avg:36.77ms
step:20/1920 train_time:733ms step_avg:36.64ms
step:21/1920 train_time:767ms step_avg:36.52ms
step:22/1920 train_time:801ms step_avg:36.43ms
step:23/1920 train_time:836ms step_avg:36.33ms
step:24/1920 train_time:870ms step_avg:36.24ms
step:25/1920 train_time:904ms step_avg:36.16ms
step:26/1920 train_time:938ms step_avg:36.09ms
step:27/1920 train_time:973ms step_avg:36.03ms
step:28/1920 train_time:1007ms step_avg:35.96ms
step:29/1920 train_time:1041ms step_avg:35.91ms
step:30/1920 train_time:1075ms step_avg:35.85ms
step:31/1920 train_time:1110ms step_avg:35.80ms
step:32/1920 train_time:1144ms step_avg:35.75ms
step:33/1920 train_time:1179ms step_avg:35.73ms
step:34/1920 train_time:1214ms step_avg:35.71ms
step:35/1920 train_time:1249ms step_avg:35.70ms
step:36/1920 train_time:1284ms step_avg:35.66ms
step:37/1920 train_time:1319ms step_avg:35.64ms
step:38/1920 train_time:1353ms step_avg:35.61ms
step:39/1920 train_time:1388ms step_avg:35.59ms
step:40/1920 train_time:1423ms step_avg:35.57ms
step:41/1920 train_time:1457ms step_avg:35.54ms
step:42/1920 train_time:1492ms step_avg:35.51ms
step:43/1920 train_time:1526ms step_avg:35.48ms
step:44/1920 train_time:1560ms step_avg:35.46ms
step:45/1920 train_time:1594ms step_avg:35.43ms
step:46/1920 train_time:1629ms step_avg:35.41ms
step:47/1920 train_time:1663ms step_avg:35.38ms
step:48/1920 train_time:1697ms step_avg:35.36ms
step:49/1920 train_time:1731ms step_avg:35.33ms
step:50/1920 train_time:1766ms step_avg:35.31ms
step:51/1920 train_time:1800ms step_avg:35.29ms
step:52/1920 train_time:1834ms step_avg:35.27ms
step:53/1920 train_time:1868ms step_avg:35.25ms
step:54/1920 train_time:1902ms step_avg:35.23ms
step:55/1920 train_time:1937ms step_avg:35.21ms
step:56/1920 train_time:1971ms step_avg:35.19ms
step:57/1920 train_time:2005ms step_avg:35.17ms
step:58/1920 train_time:2039ms step_avg:35.16ms
step:59/1920 train_time:2073ms step_avg:35.14ms
step:60/1920 train_time:2108ms step_avg:35.13ms
step:61/1920 train_time:2142ms step_avg:35.11ms
step:62/1920 train_time:2176ms step_avg:35.10ms
step:63/1920 train_time:2211ms step_avg:35.10ms
step:64/1920 train_time:2246ms step_avg:35.09ms
step:65/1920 train_time:2281ms step_avg:35.09ms
step:66/1920 train_time:2315ms step_avg:35.08ms
step:67/1920 train_time:2350ms step_avg:35.08ms
step:68/1920 train_time:2384ms step_avg:35.07ms
step:69/1920 train_time:2419ms step_avg:35.06ms
step:70/1920 train_time:2454ms step_avg:35.05ms
step:71/1920 train_time:2488ms step_avg:35.05ms
step:72/1920 train_time:2523ms step_avg:35.04ms
step:73/1920 train_time:2557ms step_avg:35.03ms
step:74/1920 train_time:2592ms step_avg:35.02ms
step:75/1920 train_time:2627ms step_avg:35.02ms
step:76/1920 train_time:2661ms step_avg:35.01ms
step:77/1920 train_time:2695ms step_avg:35.00ms
step:78/1920 train_time:2729ms step_avg:34.99ms
step:79/1920 train_time:2763ms step_avg:34.98ms
step:80/1920 train_time:2798ms step_avg:34.97ms
step:81/1920 train_time:2832ms step_avg:34.97ms
step:82/1920 train_time:2867ms step_avg:34.96ms
step:83/1920 train_time:2901ms step_avg:34.95ms
step:84/1920 train_time:2935ms step_avg:34.94ms
step:85/1920 train_time:2970ms step_avg:34.94ms
step:86/1920 train_time:3004ms step_avg:34.93ms
step:87/1920 train_time:3039ms step_avg:34.93ms
step:88/1920 train_time:3073ms step_avg:34.92ms
step:89/1920 train_time:3107ms step_avg:34.92ms
step:90/1920 train_time:3142ms step_avg:34.91ms
step:91/1920 train_time:3177ms step_avg:34.91ms
step:92/1920 train_time:3211ms step_avg:34.90ms
step:93/1920 train_time:3245ms step_avg:34.90ms
step:94/1920 train_time:3280ms step_avg:34.89ms
step:95/1920 train_time:3314ms step_avg:34.89ms
step:96/1920 train_time:3349ms step_avg:34.88ms
step:97/1920 train_time:3383ms step_avg:34.88ms
step:98/1920 train_time:3418ms step_avg:34.87ms
step:99/1920 train_time:3452ms step_avg:34.87ms
step:100/1920 train_time:3486ms step_avg:34.86ms
step:101/1920 train_time:3521ms step_avg:34.86ms
step:102/1920 train_time:3555ms step_avg:34.85ms
step:103/1920 train_time:3590ms step_avg:34.86ms
step:104/1920 train_time:3625ms step_avg:34.85ms
step:105/1920 train_time:3660ms step_avg:34.85ms
step:106/1920 train_time:3694ms step_avg:34.85ms
step:107/1920 train_time:3728ms step_avg:34.84ms
step:108/1920 train_time:3763ms step_avg:34.84ms
step:109/1920 train_time:3797ms step_avg:34.83ms
step:110/1920 train_time:3831ms step_avg:34.83ms
step:111/1920 train_time:3866ms step_avg:34.83ms
step:112/1920 train_time:3900ms step_avg:34.82ms
step:113/1920 train_time:3934ms step_avg:34.82ms
step:114/1920 train_time:3968ms step_avg:34.81ms
step:115/1920 train_time:4003ms step_avg:34.80ms
step:116/1920 train_time:4037ms step_avg:34.80ms
step:117/1920 train_time:4071ms step_avg:34.79ms
step:118/1920 train_time:4105ms step_avg:34.79ms
step:119/1920 train_time:4140ms step_avg:34.79ms
step:120/1920 train_time:4174ms step_avg:34.78ms
step:121/1920 train_time:4208ms step_avg:34.78ms
step:122/1920 train_time:4242ms step_avg:34.77ms
step:123/1920 train_time:4277ms step_avg:34.77ms
step:124/1920 train_time:4311ms step_avg:34.77ms
step:125/1920 train_time:4346ms step_avg:34.77ms
step:126/1920 train_time:4380ms step_avg:34.76ms
step:127/1920 train_time:4415ms step_avg:34.76ms
step:128/1920 train_time:4449ms step_avg:34.76ms
step:129/1920 train_time:4484ms step_avg:34.76ms
step:130/1920 train_time:4518ms step_avg:34.75ms
step:131/1920 train_time:4553ms step_avg:34.75ms
step:132/1920 train_time:4587ms step_avg:34.75ms
step:133/1920 train_time:4621ms step_avg:34.75ms
step:134/1920 train_time:4656ms step_avg:34.75ms
step:135/1920 train_time:4691ms step_avg:34.75ms
step:136/1920 train_time:4725ms step_avg:34.74ms
step:137/1920 train_time:4760ms step_avg:34.74ms
step:138/1920 train_time:4794ms step_avg:34.74ms
step:139/1920 train_time:4829ms step_avg:34.74ms
step:140/1920 train_time:4863ms step_avg:34.74ms
step:141/1920 train_time:4898ms step_avg:34.74ms
step:142/1920 train_time:4932ms step_avg:34.73ms
step:143/1920 train_time:4966ms step_avg:34.73ms
step:144/1920 train_time:5001ms step_avg:34.73ms
step:145/1920 train_time:5035ms step_avg:34.72ms
step:146/1920 train_time:5069ms step_avg:34.72ms
step:147/1920 train_time:5103ms step_avg:34.72ms
step:148/1920 train_time:5137ms step_avg:34.71ms
step:149/1920 train_time:5172ms step_avg:34.71ms
step:150/1920 train_time:5206ms step_avg:34.71ms
step:151/1920 train_time:5240ms step_avg:34.70ms
step:152/1920 train_time:5274ms step_avg:34.70ms
step:153/1920 train_time:5309ms step_avg:34.70ms
step:154/1920 train_time:5343ms step_avg:34.69ms
step:155/1920 train_time:5377ms step_avg:34.69ms
step:156/1920 train_time:5411ms step_avg:34.69ms
step:157/1920 train_time:5446ms step_avg:34.69ms
step:158/1920 train_time:5480ms step_avg:34.68ms
step:159/1920 train_time:5514ms step_avg:34.68ms
step:160/1920 train_time:5549ms step_avg:34.68ms
step:161/1920 train_time:5583ms step_avg:34.68ms
step:162/1920 train_time:5617ms step_avg:34.67ms
step:163/1920 train_time:5652ms step_avg:34.67ms
step:164/1920 train_time:5686ms step_avg:34.67ms
step:165/1920 train_time:5720ms step_avg:34.67ms
step:166/1920 train_time:5755ms step_avg:34.67ms
step:167/1920 train_time:5790ms step_avg:34.67ms
step:168/1920 train_time:5824ms step_avg:34.67ms
step:169/1920 train_time:5859ms step_avg:34.67ms
step:170/1920 train_time:5893ms step_avg:34.67ms
step:171/1920 train_time:5928ms step_avg:34.67ms
step:172/1920 train_time:5962ms step_avg:34.66ms
step:173/1920 train_time:5997ms step_avg:34.66ms
step:174/1920 train_time:6031ms step_avg:34.66ms
step:175/1920 train_time:6065ms step_avg:34.66ms
step:176/1920 train_time:6100ms step_avg:34.66ms
step:177/1920 train_time:6134ms step_avg:34.66ms
step:178/1920 train_time:6168ms step_avg:34.65ms
step:179/1920 train_time:6202ms step_avg:34.65ms
step:180/1920 train_time:6236ms step_avg:34.65ms
step:181/1920 train_time:6271ms step_avg:34.65ms
step:182/1920 train_time:6305ms step_avg:34.64ms
step:183/1920 train_time:6340ms step_avg:34.64ms
step:184/1920 train_time:6374ms step_avg:34.64ms
step:185/1920 train_time:6409ms step_avg:34.64ms
step:186/1920 train_time:6443ms step_avg:34.64ms
step:187/1920 train_time:6478ms step_avg:34.64ms
step:188/1920 train_time:6512ms step_avg:34.64ms
step:189/1920 train_time:6547ms step_avg:34.64ms
step:190/1920 train_time:6581ms step_avg:34.64ms
step:191/1920 train_time:6615ms step_avg:34.63ms
step:192/1920 train_time:6649ms step_avg:34.63ms
step:193/1920 train_time:6684ms step_avg:34.63ms
step:194/1920 train_time:6718ms step_avg:34.63ms
step:195/1920 train_time:6752ms step_avg:34.63ms
step:196/1920 train_time:6786ms step_avg:34.62ms
step:197/1920 train_time:6821ms step_avg:34.62ms
step:198/1920 train_time:6855ms step_avg:34.62ms
step:199/1920 train_time:6890ms step_avg:34.62ms
step:200/1920 train_time:6924ms step_avg:34.62ms
step:201/1920 train_time:6958ms step_avg:34.62ms
step:202/1920 train_time:6992ms step_avg:34.62ms
step:203/1920 train_time:7027ms step_avg:34.61ms
step:204/1920 train_time:7061ms step_avg:34.61ms
step:205/1920 train_time:7095ms step_avg:34.61ms
step:206/1920 train_time:7129ms step_avg:34.61ms
step:207/1920 train_time:7163ms step_avg:34.61ms
step:208/1920 train_time:7197ms step_avg:34.60ms
step:209/1920 train_time:7232ms step_avg:34.60ms
step:210/1920 train_time:7266ms step_avg:34.60ms
step:211/1920 train_time:7300ms step_avg:34.60ms
step:212/1920 train_time:7334ms step_avg:34.60ms
step:213/1920 train_time:7369ms step_avg:34.59ms
step:214/1920 train_time:7403ms step_avg:34.59ms
step:215/1920 train_time:7437ms step_avg:34.59ms
step:216/1920 train_time:7472ms step_avg:34.59ms
step:217/1920 train_time:7506ms step_avg:34.59ms
step:218/1920 train_time:7541ms step_avg:34.59ms
step:219/1920 train_time:7575ms step_avg:34.59ms
step:220/1920 train_time:7609ms step_avg:34.59ms
step:221/1920 train_time:7643ms step_avg:34.59ms
step:222/1920 train_time:7678ms step_avg:34.58ms
step:223/1920 train_time:7712ms step_avg:34.58ms
step:224/1920 train_time:7746ms step_avg:34.58ms
step:225/1920 train_time:7781ms step_avg:34.58ms
step:226/1920 train_time:7815ms step_avg:34.58ms
step:227/1920 train_time:7850ms step_avg:34.58ms
step:228/1920 train_time:7884ms step_avg:34.58ms
step:229/1920 train_time:7918ms step_avg:34.58ms
step:230/1920 train_time:7953ms step_avg:34.58ms
step:231/1920 train_time:7987ms step_avg:34.58ms
step:232/1920 train_time:8021ms step_avg:34.58ms
step:233/1920 train_time:8056ms step_avg:34.58ms
step:234/1920 train_time:8091ms step_avg:34.58ms
step:235/1920 train_time:8125ms step_avg:34.58ms
step:236/1920 train_time:8160ms step_avg:34.57ms
step:237/1920 train_time:8194ms step_avg:34.57ms
step:238/1920 train_time:8228ms step_avg:34.57ms
step:239/1920 train_time:8262ms step_avg:34.57ms
step:240/1920 train_time:8296ms step_avg:34.57ms
step:241/1920 train_time:8331ms step_avg:34.57ms
step:242/1920 train_time:8365ms step_avg:34.57ms
step:243/1920 train_time:8399ms step_avg:34.56ms
step:244/1920 train_time:8433ms step_avg:34.56ms
step:245/1920 train_time:8468ms step_avg:34.56ms
step:246/1920 train_time:8502ms step_avg:34.56ms
step:247/1920 train_time:8536ms step_avg:34.56ms
step:248/1920 train_time:8570ms step_avg:34.56ms
step:249/1920 train_time:8605ms step_avg:34.56ms
step:250/1920 train_time:8639ms step_avg:34.56ms
step:250/1920 val_loss:4.6043 train_time:8677ms step_avg:34.71ms
step:251/1920 train_time:8694ms step_avg:34.64ms
step:252/1920 train_time:8712ms step_avg:34.57ms
step:253/1920 train_time:8746ms step_avg:34.57ms
step:254/1920 train_time:8780ms step_avg:34.57ms
step:255/1920 train_time:8816ms step_avg:34.57ms
step:256/1920 train_time:8851ms step_avg:34.57ms
step:257/1920 train_time:8885ms step_avg:34.57ms
step:258/1920 train_time:8919ms step_avg:34.57ms
step:259/1920 train_time:8954ms step_avg:34.57ms
step:260/1920 train_time:8988ms step_avg:34.57ms
step:261/1920 train_time:9023ms step_avg:34.57ms
step:262/1920 train_time:9057ms step_avg:34.57ms
step:263/1920 train_time:9091ms step_avg:34.57ms
step:264/1920 train_time:9125ms step_avg:34.56ms
step:265/1920 train_time:9160ms step_avg:34.56ms
step:266/1920 train_time:9194ms step_avg:34.56ms
step:267/1920 train_time:9228ms step_avg:34.56ms
step:268/1920 train_time:9262ms step_avg:34.56ms
step:269/1920 train_time:9296ms step_avg:34.56ms
step:270/1920 train_time:9330ms step_avg:34.56ms
step:271/1920 train_time:9364ms step_avg:34.55ms
step:272/1920 train_time:9398ms step_avg:34.55ms
step:273/1920 train_time:9432ms step_avg:34.55ms
step:274/1920 train_time:9466ms step_avg:34.55ms
step:275/1920 train_time:9500ms step_avg:34.55ms
step:276/1920 train_time:9535ms step_avg:34.55ms
step:277/1920 train_time:9569ms step_avg:34.54ms
step:278/1920 train_time:9603ms step_avg:34.54ms
step:279/1920 train_time:9637ms step_avg:34.54ms
step:280/1920 train_time:9672ms step_avg:34.54ms
step:281/1920 train_time:9706ms step_avg:34.54ms
step:282/1920 train_time:9741ms step_avg:34.54ms
step:283/1920 train_time:9776ms step_avg:34.54ms
step:284/1920 train_time:9810ms step_avg:34.54ms
step:285/1920 train_time:9845ms step_avg:34.54ms
step:286/1920 train_time:9879ms step_avg:34.54ms
step:287/1920 train_time:9914ms step_avg:34.54ms
step:288/1920 train_time:9948ms step_avg:34.54ms
step:289/1920 train_time:9983ms step_avg:34.54ms
step:290/1920 train_time:10017ms step_avg:34.54ms
step:291/1920 train_time:10051ms step_avg:34.54ms
step:292/1920 train_time:10085ms step_avg:34.54ms
step:293/1920 train_time:10120ms step_avg:34.54ms
step:294/1920 train_time:10154ms step_avg:34.54ms
step:295/1920 train_time:10188ms step_avg:34.54ms
step:296/1920 train_time:10222ms step_avg:34.53ms
step:297/1920 train_time:10256ms step_avg:34.53ms
step:298/1920 train_time:10290ms step_avg:34.53ms
step:299/1920 train_time:10324ms step_avg:34.53ms
step:300/1920 train_time:10359ms step_avg:34.53ms
step:301/1920 train_time:10394ms step_avg:34.53ms
step:302/1920 train_time:10428ms step_avg:34.53ms
step:303/1920 train_time:10462ms step_avg:34.53ms
step:304/1920 train_time:10496ms step_avg:34.53ms
step:305/1920 train_time:10530ms step_avg:34.53ms
step:306/1920 train_time:10564ms step_avg:34.52ms
step:307/1920 train_time:10599ms step_avg:34.52ms
step:308/1920 train_time:10633ms step_avg:34.52ms
step:309/1920 train_time:10667ms step_avg:34.52ms
step:310/1920 train_time:10701ms step_avg:34.52ms
step:311/1920 train_time:10735ms step_avg:34.52ms
step:312/1920 train_time:10770ms step_avg:34.52ms
step:313/1920 train_time:10804ms step_avg:34.52ms
step:314/1920 train_time:10838ms step_avg:34.52ms
step:315/1920 train_time:10873ms step_avg:34.52ms
step:316/1920 train_time:10907ms step_avg:34.52ms
step:317/1920 train_time:10942ms step_avg:34.52ms
step:318/1920 train_time:10977ms step_avg:34.52ms
step:319/1920 train_time:11011ms step_avg:34.52ms
step:320/1920 train_time:11045ms step_avg:34.52ms
step:321/1920 train_time:11079ms step_avg:34.51ms
step:322/1920 train_time:11113ms step_avg:34.51ms
step:323/1920 train_time:11147ms step_avg:34.51ms
step:324/1920 train_time:11181ms step_avg:34.51ms
step:325/1920 train_time:11216ms step_avg:34.51ms
step:326/1920 train_time:11250ms step_avg:34.51ms
step:327/1920 train_time:11284ms step_avg:34.51ms
step:328/1920 train_time:11318ms step_avg:34.51ms
step:329/1920 train_time:11353ms step_avg:34.51ms
step:330/1920 train_time:11387ms step_avg:34.51ms
step:331/1920 train_time:11421ms step_avg:34.50ms
step:332/1920 train_time:11455ms step_avg:34.50ms
step:333/1920 train_time:11490ms step_avg:34.50ms
step:334/1920 train_time:11524ms step_avg:34.50ms
step:335/1920 train_time:11558ms step_avg:34.50ms
step:336/1920 train_time:11592ms step_avg:34.50ms
step:337/1920 train_time:11626ms step_avg:34.50ms
step:338/1920 train_time:11660ms step_avg:34.50ms
step:339/1920 train_time:11695ms step_avg:34.50ms
step:340/1920 train_time:11730ms step_avg:34.50ms
step:341/1920 train_time:11764ms step_avg:34.50ms
step:342/1920 train_time:11798ms step_avg:34.50ms
step:343/1920 train_time:11832ms step_avg:34.50ms
step:344/1920 train_time:11866ms step_avg:34.50ms
step:345/1920 train_time:11901ms step_avg:34.50ms
step:346/1920 train_time:11935ms step_avg:34.49ms
step:347/1920 train_time:11969ms step_avg:34.49ms
step:348/1920 train_time:12004ms step_avg:34.49ms
step:349/1920 train_time:12038ms step_avg:34.49ms
step:350/1920 train_time:12072ms step_avg:34.49ms
step:351/1920 train_time:12106ms step_avg:34.49ms
step:352/1920 train_time:12140ms step_avg:34.49ms
step:353/1920 train_time:12175ms step_avg:34.49ms
step:354/1920 train_time:12209ms step_avg:34.49ms
step:355/1920 train_time:12243ms step_avg:34.49ms
step:356/1920 train_time:12278ms step_avg:34.49ms
step:357/1920 train_time:12312ms step_avg:34.49ms
step:358/1920 train_time:12346ms step_avg:34.49ms
step:359/1920 train_time:12380ms step_avg:34.48ms
step:360/1920 train_time:12414ms step_avg:34.48ms
step:361/1920 train_time:12449ms step_avg:34.48ms
step:362/1920 train_time:12483ms step_avg:34.48ms
step:363/1920 train_time:12517ms step_avg:34.48ms
step:364/1920 train_time:12551ms step_avg:34.48ms
step:365/1920 train_time:12586ms step_avg:34.48ms
step:366/1920 train_time:12620ms step_avg:34.48ms
step:367/1920 train_time:12654ms step_avg:34.48ms
step:368/1920 train_time:12689ms step_avg:34.48ms
step:369/1920 train_time:12723ms step_avg:34.48ms
step:370/1920 train_time:12758ms step_avg:34.48ms
step:371/1920 train_time:12792ms step_avg:34.48ms
step:372/1920 train_time:12826ms step_avg:34.48ms
step:373/1920 train_time:12861ms step_avg:34.48ms
step:374/1920 train_time:12895ms step_avg:34.48ms
step:375/1920 train_time:12929ms step_avg:34.48ms
step:376/1920 train_time:12963ms step_avg:34.48ms
step:377/1920 train_time:12997ms step_avg:34.48ms
step:378/1920 train_time:13032ms step_avg:34.48ms
step:379/1920 train_time:13066ms step_avg:34.47ms
step:380/1920 train_time:13100ms step_avg:34.47ms
step:381/1920 train_time:13135ms step_avg:34.47ms
step:382/1920 train_time:13169ms step_avg:34.47ms
step:383/1920 train_time:13203ms step_avg:34.47ms
step:384/1920 train_time:13237ms step_avg:34.47ms
step:385/1920 train_time:13272ms step_avg:34.47ms
step:386/1920 train_time:13306ms step_avg:34.47ms
step:387/1920 train_time:13341ms step_avg:34.47ms
step:388/1920 train_time:13374ms step_avg:34.47ms
step:389/1920 train_time:13409ms step_avg:34.47ms
step:390/1920 train_time:13443ms step_avg:34.47ms
step:391/1920 train_time:13477ms step_avg:34.47ms
step:392/1920 train_time:13511ms step_avg:34.47ms
step:393/1920 train_time:13545ms step_avg:34.47ms
step:394/1920 train_time:13580ms step_avg:34.47ms
step:395/1920 train_time:13614ms step_avg:34.47ms
step:396/1920 train_time:13649ms step_avg:34.47ms
step:397/1920 train_time:13683ms step_avg:34.47ms
step:398/1920 train_time:13717ms step_avg:34.46ms
step:399/1920 train_time:13751ms step_avg:34.46ms
step:400/1920 train_time:13785ms step_avg:34.46ms
step:401/1920 train_time:13819ms step_avg:34.46ms
step:402/1920 train_time:13854ms step_avg:34.46ms
step:403/1920 train_time:13888ms step_avg:34.46ms
step:404/1920 train_time:13922ms step_avg:34.46ms
step:405/1920 train_time:13957ms step_avg:34.46ms
step:406/1920 train_time:13991ms step_avg:34.46ms
step:407/1920 train_time:14025ms step_avg:34.46ms
step:408/1920 train_time:14059ms step_avg:34.46ms
step:409/1920 train_time:14094ms step_avg:34.46ms
step:410/1920 train_time:14128ms step_avg:34.46ms
step:411/1920 train_time:14163ms step_avg:34.46ms
step:412/1920 train_time:14197ms step_avg:34.46ms
step:413/1920 train_time:14232ms step_avg:34.46ms
step:414/1920 train_time:14266ms step_avg:34.46ms
step:415/1920 train_time:14300ms step_avg:34.46ms
step:416/1920 train_time:14334ms step_avg:34.46ms
step:417/1920 train_time:14369ms step_avg:34.46ms
step:418/1920 train_time:14403ms step_avg:34.46ms
step:419/1920 train_time:14437ms step_avg:34.46ms
step:420/1920 train_time:14471ms step_avg:34.46ms
step:421/1920 train_time:14505ms step_avg:34.45ms
step:422/1920 train_time:14539ms step_avg:34.45ms
step:423/1920 train_time:14574ms step_avg:34.45ms
step:424/1920 train_time:14608ms step_avg:34.45ms
step:425/1920 train_time:14643ms step_avg:34.45ms
step:426/1920 train_time:14677ms step_avg:34.45ms
step:427/1920 train_time:14711ms step_avg:34.45ms
step:428/1920 train_time:14746ms step_avg:34.45ms
step:429/1920 train_time:14780ms step_avg:34.45ms
step:430/1920 train_time:14814ms step_avg:34.45ms
step:431/1920 train_time:14848ms step_avg:34.45ms
step:432/1920 train_time:14882ms step_avg:34.45ms
step:433/1920 train_time:14916ms step_avg:34.45ms
step:434/1920 train_time:14951ms step_avg:34.45ms
step:435/1920 train_time:14985ms step_avg:34.45ms
step:436/1920 train_time:15019ms step_avg:34.45ms
step:437/1920 train_time:15054ms step_avg:34.45ms
step:438/1920 train_time:15088ms step_avg:34.45ms
step:439/1920 train_time:15122ms step_avg:34.45ms
step:440/1920 train_time:15156ms step_avg:34.45ms
step:441/1920 train_time:15191ms step_avg:34.45ms
step:442/1920 train_time:15225ms step_avg:34.45ms
step:443/1920 train_time:15260ms step_avg:34.45ms
step:444/1920 train_time:15294ms step_avg:34.45ms
step:445/1920 train_time:15329ms step_avg:34.45ms
step:446/1920 train_time:15363ms step_avg:34.45ms
step:447/1920 train_time:15397ms step_avg:34.45ms
step:448/1920 train_time:15431ms step_avg:34.45ms
step:449/1920 train_time:15466ms step_avg:34.44ms
step:450/1920 train_time:15500ms step_avg:34.44ms
step:451/1920 train_time:15534ms step_avg:34.44ms
step:452/1920 train_time:15568ms step_avg:34.44ms
step:453/1920 train_time:15602ms step_avg:34.44ms
step:454/1920 train_time:15637ms step_avg:34.44ms
step:455/1920 train_time:15671ms step_avg:34.44ms
step:456/1920 train_time:15705ms step_avg:34.44ms
step:457/1920 train_time:15739ms step_avg:34.44ms
step:458/1920 train_time:15773ms step_avg:34.44ms
step:459/1920 train_time:15808ms step_avg:34.44ms
step:460/1920 train_time:15842ms step_avg:34.44ms
step:461/1920 train_time:15876ms step_avg:34.44ms
step:462/1920 train_time:15911ms step_avg:34.44ms
step:463/1920 train_time:15945ms step_avg:34.44ms
step:464/1920 train_time:15979ms step_avg:34.44ms
step:465/1920 train_time:16013ms step_avg:34.44ms
step:466/1920 train_time:16047ms step_avg:34.44ms
step:467/1920 train_time:16082ms step_avg:34.44ms
step:468/1920 train_time:16116ms step_avg:34.44ms
step:469/1920 train_time:16150ms step_avg:34.44ms
step:470/1920 train_time:16185ms step_avg:34.44ms
step:471/1920 train_time:16219ms step_avg:34.43ms
step:472/1920 train_time:16253ms step_avg:34.43ms
step:473/1920 train_time:16287ms step_avg:34.43ms
step:474/1920 train_time:16321ms step_avg:34.43ms
step:475/1920 train_time:16355ms step_avg:34.43ms
step:476/1920 train_time:16390ms step_avg:34.43ms
step:477/1920 train_time:16424ms step_avg:34.43ms
step:478/1920 train_time:16458ms step_avg:34.43ms
step:479/1920 train_time:16492ms step_avg:34.43ms
step:480/1920 train_time:16527ms step_avg:34.43ms
step:481/1920 train_time:16561ms step_avg:34.43ms
step:482/1920 train_time:16595ms step_avg:34.43ms
step:483/1920 train_time:16629ms step_avg:34.43ms
step:484/1920 train_time:16664ms step_avg:34.43ms
step:485/1920 train_time:16698ms step_avg:34.43ms
step:486/1920 train_time:16732ms step_avg:34.43ms
step:487/1920 train_time:16766ms step_avg:34.43ms
step:488/1920 train_time:16801ms step_avg:34.43ms
step:489/1920 train_time:16835ms step_avg:34.43ms
step:490/1920 train_time:16869ms step_avg:34.43ms
step:491/1920 train_time:16903ms step_avg:34.43ms
step:492/1920 train_time:16938ms step_avg:34.43ms
step:493/1920 train_time:16971ms step_avg:34.42ms
step:494/1920 train_time:17005ms step_avg:34.42ms
step:495/1920 train_time:17040ms step_avg:34.42ms
step:496/1920 train_time:17074ms step_avg:34.42ms
step:497/1920 train_time:17109ms step_avg:34.42ms
step:498/1920 train_time:17143ms step_avg:34.42ms
step:499/1920 train_time:17177ms step_avg:34.42ms
step:500/1920 train_time:17212ms step_avg:34.42ms
step:500/1920 val_loss:4.2912 train_time:17249ms step_avg:34.50ms
step:501/1920 train_time:17267ms step_avg:34.47ms
step:502/1920 train_time:17285ms step_avg:34.43ms
step:503/1920 train_time:17318ms step_avg:34.43ms
step:504/1920 train_time:17353ms step_avg:34.43ms
step:505/1920 train_time:17388ms step_avg:34.43ms
step:506/1920 train_time:17423ms step_avg:34.43ms
step:507/1920 train_time:17458ms step_avg:34.43ms
step:508/1920 train_time:17492ms step_avg:34.43ms
step:509/1920 train_time:17526ms step_avg:34.43ms
step:510/1920 train_time:17560ms step_avg:34.43ms
step:511/1920 train_time:17594ms step_avg:34.43ms
step:512/1920 train_time:17628ms step_avg:34.43ms
step:513/1920 train_time:17662ms step_avg:34.43ms
step:514/1920 train_time:17697ms step_avg:34.43ms
step:515/1920 train_time:17731ms step_avg:34.43ms
step:516/1920 train_time:17765ms step_avg:34.43ms
step:517/1920 train_time:17799ms step_avg:34.43ms
step:518/1920 train_time:17833ms step_avg:34.43ms
step:519/1920 train_time:17867ms step_avg:34.43ms
step:520/1920 train_time:17901ms step_avg:34.42ms
step:521/1920 train_time:17935ms step_avg:34.42ms
step:522/1920 train_time:17969ms step_avg:34.42ms
step:523/1920 train_time:18003ms step_avg:34.42ms
step:524/1920 train_time:18037ms step_avg:34.42ms
step:525/1920 train_time:18071ms step_avg:34.42ms
step:526/1920 train_time:18106ms step_avg:34.42ms
step:527/1920 train_time:18140ms step_avg:34.42ms
step:528/1920 train_time:18174ms step_avg:34.42ms
step:529/1920 train_time:18209ms step_avg:34.42ms
step:530/1920 train_time:18243ms step_avg:34.42ms
step:531/1920 train_time:18278ms step_avg:34.42ms
step:532/1920 train_time:18312ms step_avg:34.42ms
step:533/1920 train_time:18347ms step_avg:34.42ms
step:534/1920 train_time:18381ms step_avg:34.42ms
step:535/1920 train_time:18417ms step_avg:34.42ms
step:536/1920 train_time:18451ms step_avg:34.42ms
step:537/1920 train_time:18486ms step_avg:34.42ms
step:538/1920 train_time:18520ms step_avg:34.42ms
step:539/1920 train_time:18555ms step_avg:34.42ms
step:540/1920 train_time:18589ms step_avg:34.42ms
step:541/1920 train_time:18623ms step_avg:34.42ms
step:542/1920 train_time:18657ms step_avg:34.42ms
step:543/1920 train_time:18692ms step_avg:34.42ms
step:544/1920 train_time:18726ms step_avg:34.42ms
step:545/1920 train_time:18760ms step_avg:34.42ms
step:546/1920 train_time:18794ms step_avg:34.42ms
step:547/1920 train_time:18828ms step_avg:34.42ms
step:548/1920 train_time:18862ms step_avg:34.42ms
step:549/1920 train_time:18897ms step_avg:34.42ms
step:550/1920 train_time:18931ms step_avg:34.42ms
step:551/1920 train_time:18965ms step_avg:34.42ms
step:552/1920 train_time:18999ms step_avg:34.42ms
step:553/1920 train_time:19033ms step_avg:34.42ms
step:554/1920 train_time:19067ms step_avg:34.42ms
step:555/1920 train_time:19102ms step_avg:34.42ms
step:556/1920 train_time:19136ms step_avg:34.42ms
step:557/1920 train_time:19170ms step_avg:34.42ms
step:558/1920 train_time:19204ms step_avg:34.42ms
step:559/1920 train_time:19238ms step_avg:34.42ms
step:560/1920 train_time:19273ms step_avg:34.42ms
step:561/1920 train_time:19307ms step_avg:34.41ms
step:562/1920 train_time:19341ms step_avg:34.41ms
step:563/1920 train_time:19376ms step_avg:34.41ms
step:564/1920 train_time:19410ms step_avg:34.41ms
step:565/1920 train_time:19444ms step_avg:34.41ms
step:566/1920 train_time:19478ms step_avg:34.41ms
step:567/1920 train_time:19513ms step_avg:34.41ms
step:568/1920 train_time:19547ms step_avg:34.41ms
step:569/1920 train_time:19582ms step_avg:34.41ms
step:570/1920 train_time:19616ms step_avg:34.41ms
step:571/1920 train_time:19650ms step_avg:34.41ms
step:572/1920 train_time:19684ms step_avg:34.41ms
step:573/1920 train_time:19719ms step_avg:34.41ms
step:574/1920 train_time:19753ms step_avg:34.41ms
step:575/1920 train_time:19787ms step_avg:34.41ms
step:576/1920 train_time:19821ms step_avg:34.41ms
step:577/1920 train_time:19856ms step_avg:34.41ms
step:578/1920 train_time:19890ms step_avg:34.41ms
step:579/1920 train_time:19924ms step_avg:34.41ms
step:580/1920 train_time:19959ms step_avg:34.41ms
step:581/1920 train_time:19993ms step_avg:34.41ms
step:582/1920 train_time:20027ms step_avg:34.41ms
step:583/1920 train_time:20061ms step_avg:34.41ms
step:584/1920 train_time:20095ms step_avg:34.41ms
step:585/1920 train_time:20129ms step_avg:34.41ms
step:586/1920 train_time:20164ms step_avg:34.41ms
step:587/1920 train_time:20198ms step_avg:34.41ms
step:588/1920 train_time:20232ms step_avg:34.41ms
step:589/1920 train_time:20266ms step_avg:34.41ms
step:590/1920 train_time:20300ms step_avg:34.41ms
step:591/1920 train_time:20335ms step_avg:34.41ms
step:592/1920 train_time:20370ms step_avg:34.41ms
step:593/1920 train_time:20404ms step_avg:34.41ms
step:594/1920 train_time:20438ms step_avg:34.41ms
step:595/1920 train_time:20472ms step_avg:34.41ms
step:596/1920 train_time:20507ms step_avg:34.41ms
step:597/1920 train_time:20541ms step_avg:34.41ms
step:598/1920 train_time:20575ms step_avg:34.41ms
step:599/1920 train_time:20609ms step_avg:34.41ms
step:600/1920 train_time:20644ms step_avg:34.41ms
step:601/1920 train_time:20678ms step_avg:34.41ms
step:602/1920 train_time:20712ms step_avg:34.41ms
step:603/1920 train_time:20746ms step_avg:34.40ms
step:604/1920 train_time:20780ms step_avg:34.40ms
step:605/1920 train_time:20815ms step_avg:34.40ms
step:606/1920 train_time:20849ms step_avg:34.40ms
step:607/1920 train_time:20884ms step_avg:34.40ms
step:608/1920 train_time:20918ms step_avg:34.40ms
step:609/1920 train_time:20953ms step_avg:34.40ms
step:610/1920 train_time:20987ms step_avg:34.40ms
step:611/1920 train_time:21022ms step_avg:34.41ms
step:612/1920 train_time:21056ms step_avg:34.40ms
step:613/1920 train_time:21090ms step_avg:34.40ms
step:614/1920 train_time:21124ms step_avg:34.40ms
step:615/1920 train_time:21159ms step_avg:34.40ms
step:616/1920 train_time:21193ms step_avg:34.40ms
step:617/1920 train_time:21227ms step_avg:34.40ms
step:618/1920 train_time:21261ms step_avg:34.40ms
step:619/1920 train_time:21296ms step_avg:34.40ms
step:620/1920 train_time:21330ms step_avg:34.40ms
step:621/1920 train_time:21364ms step_avg:34.40ms
step:622/1920 train_time:21399ms step_avg:34.40ms
step:623/1920 train_time:21433ms step_avg:34.40ms
step:624/1920 train_time:21467ms step_avg:34.40ms
step:625/1920 train_time:21501ms step_avg:34.40ms
step:626/1920 train_time:21535ms step_avg:34.40ms
step:627/1920 train_time:21570ms step_avg:34.40ms
step:628/1920 train_time:21605ms step_avg:34.40ms
step:629/1920 train_time:21666ms step_avg:34.45ms
step:630/1920 train_time:21728ms step_avg:34.49ms
step:631/1920 train_time:21791ms step_avg:34.53ms
step:632/1920 train_time:21852ms step_avg:34.58ms
step:633/1920 train_time:21915ms step_avg:34.62ms
step:634/1920 train_time:21976ms step_avg:34.66ms
step:635/1920 train_time:22039ms step_avg:34.71ms
step:636/1920 train_time:22101ms step_avg:34.75ms
step:637/1920 train_time:22164ms step_avg:34.79ms
step:638/1920 train_time:22226ms step_avg:34.84ms
step:639/1920 train_time:22289ms step_avg:34.88ms
step:640/1920 train_time:22351ms step_avg:34.92ms
step:641/1920 train_time:22413ms step_avg:34.97ms
step:642/1920 train_time:22475ms step_avg:35.01ms
step:643/1920 train_time:22538ms step_avg:35.05ms
step:644/1920 train_time:22599ms step_avg:35.09ms
step:645/1920 train_time:22661ms step_avg:35.13ms
step:646/1920 train_time:22723ms step_avg:35.18ms
step:647/1920 train_time:22786ms step_avg:35.22ms
step:648/1920 train_time:22849ms step_avg:35.26ms
step:649/1920 train_time:22912ms step_avg:35.30ms
step:650/1920 train_time:22974ms step_avg:35.34ms
step:651/1920 train_time:23036ms step_avg:35.39ms
step:652/1920 train_time:23097ms step_avg:35.43ms
step:653/1920 train_time:23160ms step_avg:35.47ms
step:654/1920 train_time:23221ms step_avg:35.51ms
step:655/1920 train_time:23284ms step_avg:35.55ms
step:656/1920 train_time:23346ms step_avg:35.59ms
step:657/1920 train_time:23409ms step_avg:35.63ms
step:658/1920 train_time:23470ms step_avg:35.67ms
step:659/1920 train_time:23533ms step_avg:35.71ms
step:660/1920 train_time:23594ms step_avg:35.75ms
step:661/1920 train_time:23657ms step_avg:35.79ms
step:662/1920 train_time:23718ms step_avg:35.83ms
step:663/1920 train_time:23781ms step_avg:35.87ms
step:664/1920 train_time:23843ms step_avg:35.91ms
step:665/1920 train_time:23906ms step_avg:35.95ms
step:666/1920 train_time:23968ms step_avg:35.99ms
step:667/1920 train_time:24032ms step_avg:36.03ms
step:668/1920 train_time:24094ms step_avg:36.07ms
step:669/1920 train_time:24156ms step_avg:36.11ms
step:670/1920 train_time:24218ms step_avg:36.15ms
step:671/1920 train_time:24280ms step_avg:36.19ms
step:672/1920 train_time:24342ms step_avg:36.22ms
step:673/1920 train_time:24405ms step_avg:36.26ms
step:674/1920 train_time:24467ms step_avg:36.30ms
step:675/1920 train_time:24530ms step_avg:36.34ms
step:676/1920 train_time:24592ms step_avg:36.38ms
step:677/1920 train_time:24654ms step_avg:36.42ms
step:678/1920 train_time:24716ms step_avg:36.45ms
step:679/1920 train_time:24779ms step_avg:36.49ms
step:680/1920 train_time:24841ms step_avg:36.53ms
step:681/1920 train_time:24903ms step_avg:36.57ms
step:682/1920 train_time:24965ms step_avg:36.61ms
step:683/1920 train_time:25028ms step_avg:36.64ms
step:684/1920 train_time:25090ms step_avg:36.68ms
step:685/1920 train_time:25153ms step_avg:36.72ms
step:686/1920 train_time:25215ms step_avg:36.76ms
step:687/1920 train_time:25277ms step_avg:36.79ms
step:688/1920 train_time:25338ms step_avg:36.83ms
step:689/1920 train_time:25400ms step_avg:36.87ms
step:690/1920 train_time:25462ms step_avg:36.90ms
step:691/1920 train_time:25525ms step_avg:36.94ms
step:692/1920 train_time:25587ms step_avg:36.98ms
step:693/1920 train_time:25650ms step_avg:37.01ms
step:694/1920 train_time:25712ms step_avg:37.05ms
step:695/1920 train_time:25774ms step_avg:37.09ms
step:696/1920 train_time:25836ms step_avg:37.12ms
step:697/1920 train_time:25899ms step_avg:37.16ms
step:698/1920 train_time:25961ms step_avg:37.19ms
step:699/1920 train_time:26024ms step_avg:37.23ms
step:700/1920 train_time:26086ms step_avg:37.27ms
step:701/1920 train_time:26150ms step_avg:37.30ms
step:702/1920 train_time:26212ms step_avg:37.34ms
step:703/1920 train_time:26275ms step_avg:37.37ms
step:704/1920 train_time:26336ms step_avg:37.41ms
step:705/1920 train_time:26398ms step_avg:37.44ms
step:706/1920 train_time:26461ms step_avg:37.48ms
step:707/1920 train_time:26523ms step_avg:37.51ms
step:708/1920 train_time:26585ms step_avg:37.55ms
step:709/1920 train_time:26648ms step_avg:37.59ms
step:710/1920 train_time:26710ms step_avg:37.62ms
step:711/1920 train_time:26773ms step_avg:37.66ms
step:712/1920 train_time:26834ms step_avg:37.69ms
step:713/1920 train_time:26896ms step_avg:37.72ms
step:714/1920 train_time:26958ms step_avg:37.76ms
step:715/1920 train_time:27020ms step_avg:37.79ms
step:716/1920 train_time:27082ms step_avg:37.82ms
step:717/1920 train_time:27145ms step_avg:37.86ms
step:718/1920 train_time:27208ms step_avg:37.89ms
step:719/1920 train_time:27271ms step_avg:37.93ms
step:720/1920 train_time:27333ms step_avg:37.96ms
step:721/1920 train_time:27395ms step_avg:38.00ms
step:722/1920 train_time:27457ms step_avg:38.03ms
step:723/1920 train_time:27519ms step_avg:38.06ms
step:724/1920 train_time:27581ms step_avg:38.10ms
step:725/1920 train_time:27644ms step_avg:38.13ms
step:726/1920 train_time:27707ms step_avg:38.16ms
step:727/1920 train_time:27770ms step_avg:38.20ms
step:728/1920 train_time:27832ms step_avg:38.23ms
step:729/1920 train_time:27894ms step_avg:38.26ms
step:730/1920 train_time:27956ms step_avg:38.30ms
step:731/1920 train_time:28019ms step_avg:38.33ms
step:732/1920 train_time:28080ms step_avg:38.36ms
step:733/1920 train_time:28143ms step_avg:38.39ms
step:734/1920 train_time:28205ms step_avg:38.43ms
step:735/1920 train_time:28268ms step_avg:38.46ms
step:736/1920 train_time:28330ms step_avg:38.49ms
step:737/1920 train_time:28393ms step_avg:38.53ms
step:738/1920 train_time:28455ms step_avg:38.56ms
step:739/1920 train_time:28518ms step_avg:38.59ms
step:740/1920 train_time:28579ms step_avg:38.62ms
step:741/1920 train_time:28642ms step_avg:38.65ms
step:742/1920 train_time:28703ms step_avg:38.68ms
step:743/1920 train_time:28767ms step_avg:38.72ms
step:744/1920 train_time:28829ms step_avg:38.75ms
step:745/1920 train_time:28892ms step_avg:38.78ms
step:746/1920 train_time:28953ms step_avg:38.81ms
step:747/1920 train_time:29016ms step_avg:38.84ms
step:748/1920 train_time:29077ms step_avg:38.87ms
step:749/1920 train_time:29140ms step_avg:38.91ms
step:750/1920 train_time:29202ms step_avg:38.94ms
step:750/1920 val_loss:4.0367 train_time:29267ms step_avg:39.02ms
step:751/1920 train_time:29285ms step_avg:38.99ms
step:752/1920 train_time:29327ms step_avg:39.00ms
step:753/1920 train_time:29392ms step_avg:39.03ms
step:754/1920 train_time:29458ms step_avg:39.07ms
step:755/1920 train_time:29521ms step_avg:39.10ms
step:756/1920 train_time:29583ms step_avg:39.13ms
step:757/1920 train_time:29645ms step_avg:39.16ms
step:758/1920 train_time:29706ms step_avg:39.19ms
step:759/1920 train_time:29768ms step_avg:39.22ms
step:760/1920 train_time:29828ms step_avg:39.25ms
step:761/1920 train_time:29891ms step_avg:39.28ms
step:762/1920 train_time:29951ms step_avg:39.31ms
step:763/1920 train_time:30013ms step_avg:39.34ms
step:764/1920 train_time:30074ms step_avg:39.36ms
step:765/1920 train_time:30137ms step_avg:39.40ms
step:766/1920 train_time:30202ms step_avg:39.43ms
step:767/1920 train_time:30267ms step_avg:39.46ms
step:768/1920 train_time:30330ms step_avg:39.49ms
step:769/1920 train_time:30393ms step_avg:39.52ms
step:770/1920 train_time:30455ms step_avg:39.55ms
step:771/1920 train_time:30519ms step_avg:39.58ms
step:772/1920 train_time:30581ms step_avg:39.61ms
step:773/1920 train_time:30643ms step_avg:39.64ms
step:774/1920 train_time:30704ms step_avg:39.67ms
step:775/1920 train_time:30766ms step_avg:39.70ms
step:776/1920 train_time:30827ms step_avg:39.73ms
step:777/1920 train_time:30890ms step_avg:39.76ms
step:778/1920 train_time:30951ms step_avg:39.78ms
step:779/1920 train_time:31013ms step_avg:39.81ms
step:780/1920 train_time:31075ms step_avg:39.84ms
step:781/1920 train_time:31138ms step_avg:39.87ms
step:782/1920 train_time:31200ms step_avg:39.90ms
step:783/1920 train_time:31264ms step_avg:39.93ms
step:784/1920 train_time:31327ms step_avg:39.96ms
step:785/1920 train_time:31390ms step_avg:39.99ms
step:786/1920 train_time:31452ms step_avg:40.02ms
step:787/1920 train_time:31515ms step_avg:40.04ms
step:788/1920 train_time:31577ms step_avg:40.07ms
step:789/1920 train_time:31640ms step_avg:40.10ms
step:790/1920 train_time:31702ms step_avg:40.13ms
step:791/1920 train_time:31764ms step_avg:40.16ms
step:792/1920 train_time:31825ms step_avg:40.18ms
step:793/1920 train_time:31888ms step_avg:40.21ms
step:794/1920 train_time:31949ms step_avg:40.24ms
step:795/1920 train_time:32012ms step_avg:40.27ms
step:796/1920 train_time:32074ms step_avg:40.29ms
step:797/1920 train_time:32137ms step_avg:40.32ms
step:798/1920 train_time:32199ms step_avg:40.35ms
step:799/1920 train_time:32262ms step_avg:40.38ms
step:800/1920 train_time:32324ms step_avg:40.41ms
step:801/1920 train_time:32388ms step_avg:40.43ms
step:802/1920 train_time:32450ms step_avg:40.46ms
step:803/1920 train_time:32513ms step_avg:40.49ms
step:804/1920 train_time:32575ms step_avg:40.52ms
step:805/1920 train_time:32638ms step_avg:40.54ms
step:806/1920 train_time:32700ms step_avg:40.57ms
step:807/1920 train_time:32762ms step_avg:40.60ms
step:808/1920 train_time:32824ms step_avg:40.62ms
step:809/1920 train_time:32887ms step_avg:40.65ms
step:810/1920 train_time:32948ms step_avg:40.68ms
step:811/1920 train_time:33011ms step_avg:40.70ms
step:812/1920 train_time:33073ms step_avg:40.73ms
step:813/1920 train_time:33135ms step_avg:40.76ms
step:814/1920 train_time:33197ms step_avg:40.78ms
step:815/1920 train_time:33260ms step_avg:40.81ms
step:816/1920 train_time:33322ms step_avg:40.84ms
step:817/1920 train_time:33385ms step_avg:40.86ms
step:818/1920 train_time:33448ms step_avg:40.89ms
step:819/1920 train_time:33511ms step_avg:40.92ms
step:820/1920 train_time:33572ms step_avg:40.94ms
step:821/1920 train_time:33635ms step_avg:40.97ms
step:822/1920 train_time:33697ms step_avg:40.99ms
step:823/1920 train_time:33760ms step_avg:41.02ms
step:824/1920 train_time:33822ms step_avg:41.05ms
step:825/1920 train_time:33884ms step_avg:41.07ms
step:826/1920 train_time:33946ms step_avg:41.10ms
step:827/1920 train_time:34009ms step_avg:41.12ms
step:828/1920 train_time:34071ms step_avg:41.15ms
step:829/1920 train_time:34134ms step_avg:41.18ms
step:830/1920 train_time:34196ms step_avg:41.20ms
step:831/1920 train_time:34258ms step_avg:41.23ms
step:832/1920 train_time:34320ms step_avg:41.25ms
step:833/1920 train_time:34383ms step_avg:41.28ms
step:834/1920 train_time:34445ms step_avg:41.30ms
step:835/1920 train_time:34509ms step_avg:41.33ms
step:836/1920 train_time:34570ms step_avg:41.35ms
step:837/1920 train_time:34634ms step_avg:41.38ms
step:838/1920 train_time:34695ms step_avg:41.40ms
step:839/1920 train_time:34758ms step_avg:41.43ms
step:840/1920 train_time:34820ms step_avg:41.45ms
step:841/1920 train_time:34882ms step_avg:41.48ms
step:842/1920 train_time:34943ms step_avg:41.50ms
step:843/1920 train_time:35006ms step_avg:41.53ms
step:844/1920 train_time:35068ms step_avg:41.55ms
step:845/1920 train_time:35131ms step_avg:41.58ms
step:846/1920 train_time:35193ms step_avg:41.60ms
step:847/1920 train_time:35256ms step_avg:41.62ms
step:848/1920 train_time:35317ms step_avg:41.65ms
step:849/1920 train_time:35380ms step_avg:41.67ms
step:850/1920 train_time:35441ms step_avg:41.70ms
step:851/1920 train_time:35504ms step_avg:41.72ms
step:852/1920 train_time:35567ms step_avg:41.75ms
step:853/1920 train_time:35630ms step_avg:41.77ms
step:854/1920 train_time:35692ms step_avg:41.79ms
step:855/1920 train_time:35755ms step_avg:41.82ms
step:856/1920 train_time:35817ms step_avg:41.84ms
step:857/1920 train_time:35879ms step_avg:41.87ms
step:858/1920 train_time:35941ms step_avg:41.89ms
step:859/1920 train_time:36003ms step_avg:41.91ms
step:860/1920 train_time:36066ms step_avg:41.94ms
step:861/1920 train_time:36129ms step_avg:41.96ms
step:862/1920 train_time:36190ms step_avg:41.98ms
step:863/1920 train_time:36253ms step_avg:42.01ms
step:864/1920 train_time:36315ms step_avg:42.03ms
step:865/1920 train_time:36378ms step_avg:42.06ms
step:866/1920 train_time:36440ms step_avg:42.08ms
step:867/1920 train_time:36503ms step_avg:42.10ms
step:868/1920 train_time:36565ms step_avg:42.13ms
step:869/1920 train_time:36629ms step_avg:42.15ms
step:870/1920 train_time:36692ms step_avg:42.17ms
step:871/1920 train_time:36754ms step_avg:42.20ms
step:872/1920 train_time:36816ms step_avg:42.22ms
step:873/1920 train_time:36878ms step_avg:42.24ms
step:874/1920 train_time:36940ms step_avg:42.27ms
step:875/1920 train_time:37003ms step_avg:42.29ms
step:876/1920 train_time:37064ms step_avg:42.31ms
step:877/1920 train_time:37127ms step_avg:42.33ms
step:878/1920 train_time:37189ms step_avg:42.36ms
step:879/1920 train_time:37252ms step_avg:42.38ms
step:880/1920 train_time:37314ms step_avg:42.40ms
step:881/1920 train_time:37376ms step_avg:42.43ms
step:882/1920 train_time:37438ms step_avg:42.45ms
step:883/1920 train_time:37501ms step_avg:42.47ms
step:884/1920 train_time:37562ms step_avg:42.49ms
step:885/1920 train_time:37627ms step_avg:42.52ms
step:886/1920 train_time:37689ms step_avg:42.54ms
step:887/1920 train_time:37752ms step_avg:42.56ms
step:888/1920 train_time:37814ms step_avg:42.58ms
step:889/1920 train_time:37876ms step_avg:42.61ms
step:890/1920 train_time:37938ms step_avg:42.63ms
step:891/1920 train_time:38001ms step_avg:42.65ms
step:892/1920 train_time:38063ms step_avg:42.67ms
step:893/1920 train_time:38125ms step_avg:42.69ms
step:894/1920 train_time:38187ms step_avg:42.72ms
step:895/1920 train_time:38251ms step_avg:42.74ms
step:896/1920 train_time:38312ms step_avg:42.76ms
step:897/1920 train_time:38375ms step_avg:42.78ms
step:898/1920 train_time:38436ms step_avg:42.80ms
step:899/1920 train_time:38499ms step_avg:42.82ms
step:900/1920 train_time:38561ms step_avg:42.85ms
step:901/1920 train_time:38624ms step_avg:42.87ms
step:902/1920 train_time:38687ms step_avg:42.89ms
step:903/1920 train_time:38750ms step_avg:42.91ms
step:904/1920 train_time:38812ms step_avg:42.93ms
step:905/1920 train_time:38875ms step_avg:42.96ms
step:906/1920 train_time:38937ms step_avg:42.98ms
step:907/1920 train_time:38999ms step_avg:43.00ms
step:908/1920 train_time:39061ms step_avg:43.02ms
step:909/1920 train_time:39124ms step_avg:43.04ms
step:910/1920 train_time:39186ms step_avg:43.06ms
step:911/1920 train_time:39250ms step_avg:43.08ms
step:912/1920 train_time:39311ms step_avg:43.10ms
step:913/1920 train_time:39374ms step_avg:43.13ms
step:914/1920 train_time:39435ms step_avg:43.15ms
step:915/1920 train_time:39498ms step_avg:43.17ms
step:916/1920 train_time:39561ms step_avg:43.19ms
step:917/1920 train_time:39623ms step_avg:43.21ms
step:918/1920 train_time:39685ms step_avg:43.23ms
step:919/1920 train_time:39749ms step_avg:43.25ms
step:920/1920 train_time:39811ms step_avg:43.27ms
step:921/1920 train_time:39874ms step_avg:43.29ms
step:922/1920 train_time:39936ms step_avg:43.31ms
step:923/1920 train_time:39998ms step_avg:43.34ms
step:924/1920 train_time:40061ms step_avg:43.36ms
step:925/1920 train_time:40123ms step_avg:43.38ms
step:926/1920 train_time:40185ms step_avg:43.40ms
step:927/1920 train_time:40248ms step_avg:43.42ms
step:928/1920 train_time:40310ms step_avg:43.44ms
step:929/1920 train_time:40373ms step_avg:43.46ms
step:930/1920 train_time:40435ms step_avg:43.48ms
step:931/1920 train_time:40497ms step_avg:43.50ms
step:932/1920 train_time:40559ms step_avg:43.52ms
step:933/1920 train_time:40621ms step_avg:43.54ms
step:934/1920 train_time:40683ms step_avg:43.56ms
step:935/1920 train_time:40746ms step_avg:43.58ms
step:936/1920 train_time:40808ms step_avg:43.60ms
step:937/1920 train_time:40871ms step_avg:43.62ms
step:938/1920 train_time:40933ms step_avg:43.64ms
step:939/1920 train_time:40996ms step_avg:43.66ms
step:940/1920 train_time:41057ms step_avg:43.68ms
step:941/1920 train_time:41120ms step_avg:43.70ms
step:942/1920 train_time:41182ms step_avg:43.72ms
step:943/1920 train_time:41245ms step_avg:43.74ms
step:944/1920 train_time:41307ms step_avg:43.76ms
step:945/1920 train_time:41371ms step_avg:43.78ms
step:946/1920 train_time:41433ms step_avg:43.80ms
step:947/1920 train_time:41495ms step_avg:43.82ms
step:948/1920 train_time:41557ms step_avg:43.84ms
step:949/1920 train_time:41619ms step_avg:43.86ms
step:950/1920 train_time:41681ms step_avg:43.87ms
step:951/1920 train_time:41745ms step_avg:43.90ms
step:952/1920 train_time:41807ms step_avg:43.92ms
step:953/1920 train_time:41871ms step_avg:43.94ms
step:954/1920 train_time:41933ms step_avg:43.95ms
step:955/1920 train_time:41996ms step_avg:43.97ms
step:956/1920 train_time:42057ms step_avg:43.99ms
step:957/1920 train_time:42120ms step_avg:44.01ms
step:958/1920 train_time:42182ms step_avg:44.03ms
step:959/1920 train_time:42245ms step_avg:44.05ms
step:960/1920 train_time:42307ms step_avg:44.07ms
step:961/1920 train_time:42371ms step_avg:44.09ms
step:962/1920 train_time:42433ms step_avg:44.11ms
step:963/1920 train_time:42495ms step_avg:44.13ms
step:964/1920 train_time:42557ms step_avg:44.15ms
step:965/1920 train_time:42620ms step_avg:44.17ms
step:966/1920 train_time:42682ms step_avg:44.18ms
step:967/1920 train_time:42744ms step_avg:44.20ms
step:968/1920 train_time:42807ms step_avg:44.22ms
step:969/1920 train_time:42870ms step_avg:44.24ms
step:970/1920 train_time:42932ms step_avg:44.26ms
step:971/1920 train_time:42995ms step_avg:44.28ms
step:972/1920 train_time:43056ms step_avg:44.30ms
step:973/1920 train_time:43119ms step_avg:44.32ms
step:974/1920 train_time:43181ms step_avg:44.33ms
step:975/1920 train_time:43244ms step_avg:44.35ms
step:976/1920 train_time:43305ms step_avg:44.37ms
step:977/1920 train_time:43368ms step_avg:44.39ms
step:978/1920 train_time:43430ms step_avg:44.41ms
step:979/1920 train_time:43494ms step_avg:44.43ms
step:980/1920 train_time:43556ms step_avg:44.44ms
step:981/1920 train_time:43618ms step_avg:44.46ms
step:982/1920 train_time:43680ms step_avg:44.48ms
step:983/1920 train_time:43743ms step_avg:44.50ms
step:984/1920 train_time:43805ms step_avg:44.52ms
step:985/1920 train_time:43868ms step_avg:44.54ms
step:986/1920 train_time:43930ms step_avg:44.55ms
step:987/1920 train_time:43993ms step_avg:44.57ms
step:988/1920 train_time:44055ms step_avg:44.59ms
step:989/1920 train_time:44118ms step_avg:44.61ms
step:990/1920 train_time:44179ms step_avg:44.63ms
step:991/1920 train_time:44241ms step_avg:44.64ms
step:992/1920 train_time:44303ms step_avg:44.66ms
step:993/1920 train_time:44366ms step_avg:44.68ms
step:994/1920 train_time:44428ms step_avg:44.70ms
step:995/1920 train_time:44491ms step_avg:44.71ms
step:996/1920 train_time:44553ms step_avg:44.73ms
step:997/1920 train_time:44615ms step_avg:44.75ms
step:998/1920 train_time:44678ms step_avg:44.77ms
step:999/1920 train_time:44740ms step_avg:44.78ms
step:1000/1920 train_time:44802ms step_avg:44.80ms
step:1000/1920 val_loss:3.7817 train_time:44867ms step_avg:44.87ms
step:1001/1920 train_time:44884ms step_avg:44.84ms
step:1002/1920 train_time:44928ms step_avg:44.84ms
step:1003/1920 train_time:44993ms step_avg:44.86ms
step:1004/1920 train_time:45056ms step_avg:44.88ms
step:1005/1920 train_time:45118ms step_avg:44.89ms
step:1006/1920 train_time:45179ms step_avg:44.91ms
step:1007/1920 train_time:45241ms step_avg:44.93ms
step:1008/1920 train_time:45303ms step_avg:44.94ms
step:1009/1920 train_time:45365ms step_avg:44.96ms
step:1010/1920 train_time:45426ms step_avg:44.98ms
step:1011/1920 train_time:45488ms step_avg:44.99ms
step:1012/1920 train_time:45549ms step_avg:45.01ms
step:1013/1920 train_time:45612ms step_avg:45.03ms
step:1014/1920 train_time:45675ms step_avg:45.04ms
step:1015/1920 train_time:45738ms step_avg:45.06ms
step:1016/1920 train_time:45801ms step_avg:45.08ms
step:1017/1920 train_time:45864ms step_avg:45.10ms
step:1018/1920 train_time:45927ms step_avg:45.11ms
step:1019/1920 train_time:45990ms step_avg:45.13ms
step:1020/1920 train_time:46053ms step_avg:45.15ms
step:1021/1920 train_time:46116ms step_avg:45.17ms
step:1022/1920 train_time:46178ms step_avg:45.18ms
step:1023/1920 train_time:46240ms step_avg:45.20ms
step:1024/1920 train_time:46302ms step_avg:45.22ms
step:1025/1920 train_time:46364ms step_avg:45.23ms
step:1026/1920 train_time:46426ms step_avg:45.25ms
step:1027/1920 train_time:46487ms step_avg:45.27ms
step:1028/1920 train_time:46549ms step_avg:45.28ms
step:1029/1920 train_time:46611ms step_avg:45.30ms
step:1030/1920 train_time:46674ms step_avg:45.31ms
step:1031/1920 train_time:46736ms step_avg:45.33ms
step:1032/1920 train_time:46798ms step_avg:45.35ms
step:1033/1920 train_time:46862ms step_avg:45.36ms
step:1034/1920 train_time:46924ms step_avg:45.38ms
step:1035/1920 train_time:46987ms step_avg:45.40ms
step:1036/1920 train_time:47050ms step_avg:45.41ms
step:1037/1920 train_time:47113ms step_avg:45.43ms
step:1038/1920 train_time:47176ms step_avg:45.45ms
step:1039/1920 train_time:47239ms step_avg:45.47ms
step:1040/1920 train_time:47300ms step_avg:45.48ms
step:1041/1920 train_time:47363ms step_avg:45.50ms
step:1042/1920 train_time:47425ms step_avg:45.51ms
step:1043/1920 train_time:47487ms step_avg:45.53ms
step:1044/1920 train_time:47549ms step_avg:45.54ms
step:1045/1920 train_time:47611ms step_avg:45.56ms
step:1046/1920 train_time:47673ms step_avg:45.58ms
step:1047/1920 train_time:47736ms step_avg:45.59ms
step:1048/1920 train_time:47798ms step_avg:45.61ms
step:1049/1920 train_time:47861ms step_avg:45.63ms
step:1050/1920 train_time:47924ms step_avg:45.64ms
step:1051/1920 train_time:47986ms step_avg:45.66ms
step:1052/1920 train_time:48048ms step_avg:45.67ms
step:1053/1920 train_time:48112ms step_avg:45.69ms
step:1054/1920 train_time:48174ms step_avg:45.71ms
step:1055/1920 train_time:48238ms step_avg:45.72ms
step:1056/1920 train_time:48300ms step_avg:45.74ms
step:1057/1920 train_time:48363ms step_avg:45.75ms
step:1058/1920 train_time:48424ms step_avg:45.77ms
step:1059/1920 train_time:48486ms step_avg:45.78ms
step:1060/1920 train_time:48548ms step_avg:45.80ms
step:1061/1920 train_time:48610ms step_avg:45.82ms
step:1062/1920 train_time:48672ms step_avg:45.83ms
step:1063/1920 train_time:48735ms step_avg:45.85ms
step:1064/1920 train_time:48797ms step_avg:45.86ms
step:1065/1920 train_time:48860ms step_avg:45.88ms
step:1066/1920 train_time:48922ms step_avg:45.89ms
step:1067/1920 train_time:48984ms step_avg:45.91ms
step:1068/1920 train_time:49047ms step_avg:45.92ms
step:1069/1920 train_time:49111ms step_avg:45.94ms
step:1070/1920 train_time:49173ms step_avg:45.96ms
step:1071/1920 train_time:49237ms step_avg:45.97ms
step:1072/1920 train_time:49299ms step_avg:45.99ms
step:1073/1920 train_time:49362ms step_avg:46.00ms
step:1074/1920 train_time:49423ms step_avg:46.02ms
step:1075/1920 train_time:49485ms step_avg:46.03ms
step:1076/1920 train_time:49547ms step_avg:46.05ms
step:1077/1920 train_time:49609ms step_avg:46.06ms
step:1078/1920 train_time:49672ms step_avg:46.08ms
step:1079/1920 train_time:49735ms step_avg:46.09ms
step:1080/1920 train_time:49797ms step_avg:46.11ms
step:1081/1920 train_time:49860ms step_avg:46.12ms
step:1082/1920 train_time:49921ms step_avg:46.14ms
step:1083/1920 train_time:49984ms step_avg:46.15ms
step:1084/1920 train_time:50045ms step_avg:46.17ms
step:1085/1920 train_time:50108ms step_avg:46.18ms
step:1086/1920 train_time:50171ms step_avg:46.20ms
step:1087/1920 train_time:50234ms step_avg:46.21ms
step:1088/1920 train_time:50297ms step_avg:46.23ms
step:1089/1920 train_time:50359ms step_avg:46.24ms
step:1090/1920 train_time:50421ms step_avg:46.26ms
step:1091/1920 train_time:50483ms step_avg:46.27ms
step:1092/1920 train_time:50545ms step_avg:46.29ms
step:1093/1920 train_time:50607ms step_avg:46.30ms
step:1094/1920 train_time:50668ms step_avg:46.31ms
step:1095/1920 train_time:50731ms step_avg:46.33ms
step:1096/1920 train_time:50793ms step_avg:46.34ms
step:1097/1920 train_time:50856ms step_avg:46.36ms
step:1098/1920 train_time:50918ms step_avg:46.37ms
step:1099/1920 train_time:50980ms step_avg:46.39ms
step:1100/1920 train_time:51042ms step_avg:46.40ms
step:1101/1920 train_time:51105ms step_avg:46.42ms
step:1102/1920 train_time:51167ms step_avg:46.43ms
step:1103/1920 train_time:51230ms step_avg:46.45ms
step:1104/1920 train_time:51293ms step_avg:46.46ms
step:1105/1920 train_time:51356ms step_avg:46.48ms
step:1106/1920 train_time:51418ms step_avg:46.49ms
step:1107/1920 train_time:51481ms step_avg:46.50ms
step:1108/1920 train_time:51543ms step_avg:46.52ms
step:1109/1920 train_time:51606ms step_avg:46.53ms
step:1110/1920 train_time:51667ms step_avg:46.55ms
step:1111/1920 train_time:51730ms step_avg:46.56ms
step:1112/1920 train_time:51792ms step_avg:46.58ms
step:1113/1920 train_time:51856ms step_avg:46.59ms
step:1114/1920 train_time:51918ms step_avg:46.61ms
step:1115/1920 train_time:51981ms step_avg:46.62ms
step:1116/1920 train_time:52042ms step_avg:46.63ms
step:1117/1920 train_time:52105ms step_avg:46.65ms
step:1118/1920 train_time:52167ms step_avg:46.66ms
step:1119/1920 train_time:52230ms step_avg:46.68ms
step:1120/1920 train_time:52292ms step_avg:46.69ms
step:1121/1920 train_time:52355ms step_avg:46.70ms
step:1122/1920 train_time:52417ms step_avg:46.72ms
step:1123/1920 train_time:52479ms step_avg:46.73ms
step:1124/1920 train_time:52541ms step_avg:46.74ms
step:1125/1920 train_time:52604ms step_avg:46.76ms
step:1126/1920 train_time:52665ms step_avg:46.77ms
step:1127/1920 train_time:52728ms step_avg:46.79ms
step:1128/1920 train_time:52790ms step_avg:46.80ms
step:1129/1920 train_time:52853ms step_avg:46.81ms
step:1130/1920 train_time:52916ms step_avg:46.83ms
step:1131/1920 train_time:52978ms step_avg:46.84ms
step:1132/1920 train_time:53041ms step_avg:46.86ms
step:1133/1920 train_time:53103ms step_avg:46.87ms
step:1134/1920 train_time:53165ms step_avg:46.88ms
step:1135/1920 train_time:53228ms step_avg:46.90ms
step:1136/1920 train_time:53291ms step_avg:46.91ms
step:1137/1920 train_time:53355ms step_avg:46.93ms
step:1138/1920 train_time:53416ms step_avg:46.94ms
step:1139/1920 train_time:53479ms step_avg:46.95ms
step:1140/1920 train_time:53541ms step_avg:46.97ms
step:1141/1920 train_time:53603ms step_avg:46.98ms
step:1142/1920 train_time:53665ms step_avg:46.99ms
step:1143/1920 train_time:53728ms step_avg:47.01ms
step:1144/1920 train_time:53790ms step_avg:47.02ms
step:1145/1920 train_time:53853ms step_avg:47.03ms
step:1146/1920 train_time:53915ms step_avg:47.05ms
step:1147/1920 train_time:53978ms step_avg:47.06ms
step:1148/1920 train_time:54040ms step_avg:47.07ms
step:1149/1920 train_time:54102ms step_avg:47.09ms
step:1150/1920 train_time:54164ms step_avg:47.10ms
step:1151/1920 train_time:54226ms step_avg:47.11ms
step:1152/1920 train_time:54288ms step_avg:47.12ms
step:1153/1920 train_time:54351ms step_avg:47.14ms
step:1154/1920 train_time:54414ms step_avg:47.15ms
step:1155/1920 train_time:54476ms step_avg:47.17ms
step:1156/1920 train_time:54538ms step_avg:47.18ms
step:1157/1920 train_time:54600ms step_avg:47.19ms
step:1158/1920 train_time:54662ms step_avg:47.20ms
step:1159/1920 train_time:54725ms step_avg:47.22ms
step:1160/1920 train_time:54786ms step_avg:47.23ms
step:1161/1920 train_time:54850ms step_avg:47.24ms
step:1162/1920 train_time:54912ms step_avg:47.26ms
step:1163/1920 train_time:54975ms step_avg:47.27ms
step:1164/1920 train_time:55038ms step_avg:47.28ms
step:1165/1920 train_time:55100ms step_avg:47.30ms
step:1166/1920 train_time:55162ms step_avg:47.31ms
step:1167/1920 train_time:55225ms step_avg:47.32ms
step:1168/1920 train_time:55287ms step_avg:47.33ms
step:1169/1920 train_time:55350ms step_avg:47.35ms
step:1170/1920 train_time:55412ms step_avg:47.36ms
step:1171/1920 train_time:55475ms step_avg:47.37ms
step:1172/1920 train_time:55537ms step_avg:47.39ms
step:1173/1920 train_time:55600ms step_avg:47.40ms
step:1174/1920 train_time:55661ms step_avg:47.41ms
step:1175/1920 train_time:55724ms step_avg:47.42ms
step:1176/1920 train_time:55785ms step_avg:47.44ms
step:1177/1920 train_time:55848ms step_avg:47.45ms
step:1178/1920 train_time:55910ms step_avg:47.46ms
step:1179/1920 train_time:55974ms step_avg:47.48ms
step:1180/1920 train_time:56036ms step_avg:47.49ms
step:1181/1920 train_time:56098ms step_avg:47.50ms
step:1182/1920 train_time:56161ms step_avg:47.51ms
step:1183/1920 train_time:56224ms step_avg:47.53ms
step:1184/1920 train_time:56286ms step_avg:47.54ms
step:1185/1920 train_time:56348ms step_avg:47.55ms
step:1186/1920 train_time:56410ms step_avg:47.56ms
step:1187/1920 train_time:56474ms step_avg:47.58ms
step:1188/1920 train_time:56536ms step_avg:47.59ms
step:1189/1920 train_time:56598ms step_avg:47.60ms
step:1190/1920 train_time:56660ms step_avg:47.61ms
step:1191/1920 train_time:56722ms step_avg:47.63ms
step:1192/1920 train_time:56784ms step_avg:47.64ms
step:1193/1920 train_time:56847ms step_avg:47.65ms
step:1194/1920 train_time:56908ms step_avg:47.66ms
step:1195/1920 train_time:56971ms step_avg:47.67ms
step:1196/1920 train_time:57034ms step_avg:47.69ms
step:1197/1920 train_time:57097ms step_avg:47.70ms
step:1198/1920 train_time:57159ms step_avg:47.71ms
step:1199/1920 train_time:57221ms step_avg:47.72ms
step:1200/1920 train_time:57284ms step_avg:47.74ms
step:1201/1920 train_time:57346ms step_avg:47.75ms
step:1202/1920 train_time:57408ms step_avg:47.76ms
step:1203/1920 train_time:57471ms step_avg:47.77ms
step:1204/1920 train_time:57534ms step_avg:47.79ms
step:1205/1920 train_time:57597ms step_avg:47.80ms
step:1206/1920 train_time:57659ms step_avg:47.81ms
step:1207/1920 train_time:57722ms step_avg:47.82ms
step:1208/1920 train_time:57784ms step_avg:47.83ms
step:1209/1920 train_time:57846ms step_avg:47.85ms
step:1210/1920 train_time:57908ms step_avg:47.86ms
step:1211/1920 train_time:57972ms step_avg:47.87ms
step:1212/1920 train_time:58033ms step_avg:47.88ms
step:1213/1920 train_time:58096ms step_avg:47.89ms
step:1214/1920 train_time:58158ms step_avg:47.91ms
step:1215/1920 train_time:58221ms step_avg:47.92ms
step:1216/1920 train_time:58283ms step_avg:47.93ms
step:1217/1920 train_time:58346ms step_avg:47.94ms
step:1218/1920 train_time:58408ms step_avg:47.95ms
step:1219/1920 train_time:58471ms step_avg:47.97ms
step:1220/1920 train_time:58533ms step_avg:47.98ms
step:1221/1920 train_time:58597ms step_avg:47.99ms
step:1222/1920 train_time:58659ms step_avg:48.00ms
step:1223/1920 train_time:58721ms step_avg:48.01ms
step:1224/1920 train_time:58783ms step_avg:48.03ms
step:1225/1920 train_time:58845ms step_avg:48.04ms
step:1226/1920 train_time:58908ms step_avg:48.05ms
step:1227/1920 train_time:58970ms step_avg:48.06ms
step:1228/1920 train_time:59032ms step_avg:48.07ms
step:1229/1920 train_time:59096ms step_avg:48.08ms
step:1230/1920 train_time:59158ms step_avg:48.10ms
step:1231/1920 train_time:59221ms step_avg:48.11ms
step:1232/1920 train_time:59283ms step_avg:48.12ms
step:1233/1920 train_time:59345ms step_avg:48.13ms
step:1234/1920 train_time:59407ms step_avg:48.14ms
step:1235/1920 train_time:59470ms step_avg:48.15ms
step:1236/1920 train_time:59532ms step_avg:48.17ms
step:1237/1920 train_time:59596ms step_avg:48.18ms
step:1238/1920 train_time:59657ms step_avg:48.19ms
step:1239/1920 train_time:59720ms step_avg:48.20ms
step:1240/1920 train_time:59782ms step_avg:48.21ms
step:1241/1920 train_time:59844ms step_avg:48.22ms
step:1242/1920 train_time:59906ms step_avg:48.23ms
step:1243/1920 train_time:59968ms step_avg:48.24ms
step:1244/1920 train_time:60030ms step_avg:48.26ms
step:1245/1920 train_time:60093ms step_avg:48.27ms
step:1246/1920 train_time:60155ms step_avg:48.28ms
step:1247/1920 train_time:60218ms step_avg:48.29ms
step:1248/1920 train_time:60280ms step_avg:48.30ms
step:1249/1920 train_time:60342ms step_avg:48.31ms
step:1250/1920 train_time:60404ms step_avg:48.32ms
step:1250/1920 val_loss:3.5556 train_time:60469ms step_avg:48.38ms
step:1251/1920 train_time:60486ms step_avg:48.35ms
step:1252/1920 train_time:60530ms step_avg:48.35ms
step:1253/1920 train_time:60596ms step_avg:48.36ms
step:1254/1920 train_time:60658ms step_avg:48.37ms
step:1255/1920 train_time:60722ms step_avg:48.38ms
step:1256/1920 train_time:60811ms step_avg:48.42ms
step:1257/1920 train_time:60899ms step_avg:48.45ms
step:1258/1920 train_time:60985ms step_avg:48.48ms
step:1259/1920 train_time:61075ms step_avg:48.51ms
step:1260/1920 train_time:61161ms step_avg:48.54ms
step:1261/1920 train_time:61251ms step_avg:48.57ms
step:1262/1920 train_time:61337ms step_avg:48.60ms
step:1263/1920 train_time:61427ms step_avg:48.64ms
step:1264/1920 train_time:61519ms step_avg:48.67ms
step:1265/1920 train_time:61609ms step_avg:48.70ms
step:1266/1920 train_time:61696ms step_avg:48.73ms
step:1267/1920 train_time:61785ms step_avg:48.77ms
step:1268/1920 train_time:61873ms step_avg:48.80ms
step:1269/1920 train_time:61960ms step_avg:48.83ms
step:1270/1920 train_time:62048ms step_avg:48.86ms
step:1271/1920 train_time:62136ms step_avg:48.89ms
step:1272/1920 train_time:62225ms step_avg:48.92ms
step:1273/1920 train_time:62313ms step_avg:48.95ms
step:1274/1920 train_time:62401ms step_avg:48.98ms
step:1275/1920 train_time:62492ms step_avg:49.01ms
step:1276/1920 train_time:62580ms step_avg:49.04ms
step:1277/1920 train_time:62669ms step_avg:49.08ms
step:1278/1920 train_time:62757ms step_avg:49.11ms
step:1279/1920 train_time:62846ms step_avg:49.14ms
step:1280/1920 train_time:62934ms step_avg:49.17ms
step:1281/1920 train_time:63021ms step_avg:49.20ms
step:1282/1920 train_time:63110ms step_avg:49.23ms
step:1283/1920 train_time:63198ms step_avg:49.26ms
step:1284/1920 train_time:63286ms step_avg:49.29ms
step:1285/1920 train_time:63375ms step_avg:49.32ms
step:1286/1920 train_time:63464ms step_avg:49.35ms
step:1287/1920 train_time:63553ms step_avg:49.38ms
step:1288/1920 train_time:63641ms step_avg:49.41ms
step:1289/1920 train_time:63730ms step_avg:49.44ms
step:1290/1920 train_time:63817ms step_avg:49.47ms
step:1291/1920 train_time:63907ms step_avg:49.50ms
step:1292/1920 train_time:63994ms step_avg:49.53ms
step:1293/1920 train_time:64082ms step_avg:49.56ms
step:1294/1920 train_time:64170ms step_avg:49.59ms
step:1295/1920 train_time:64258ms step_avg:49.62ms
step:1296/1920 train_time:64346ms step_avg:49.65ms
step:1297/1920 train_time:64436ms step_avg:49.68ms
step:1298/1920 train_time:64525ms step_avg:49.71ms
step:1299/1920 train_time:64614ms step_avg:49.74ms
step:1300/1920 train_time:64702ms step_avg:49.77ms
step:1301/1920 train_time:64791ms step_avg:49.80ms
step:1302/1920 train_time:64879ms step_avg:49.83ms
step:1303/1920 train_time:64968ms step_avg:49.86ms
step:1304/1920 train_time:65055ms step_avg:49.89ms
step:1305/1920 train_time:65144ms step_avg:49.92ms
step:1306/1920 train_time:65232ms step_avg:49.95ms
step:1307/1920 train_time:65320ms step_avg:49.98ms
step:1308/1920 train_time:65408ms step_avg:50.01ms
step:1309/1920 train_time:65497ms step_avg:50.04ms
step:1310/1920 train_time:65585ms step_avg:50.07ms
step:1311/1920 train_time:65675ms step_avg:50.10ms
step:1312/1920 train_time:65763ms step_avg:50.12ms
step:1313/1920 train_time:65852ms step_avg:50.15ms
step:1314/1920 train_time:65940ms step_avg:50.18ms
step:1315/1920 train_time:66028ms step_avg:50.21ms
step:1316/1920 train_time:66115ms step_avg:50.24ms
step:1317/1920 train_time:66204ms step_avg:50.27ms
step:1318/1920 train_time:66292ms step_avg:50.30ms
step:1319/1920 train_time:66380ms step_avg:50.33ms
step:1320/1920 train_time:66469ms step_avg:50.35ms
step:1321/1920 train_time:66557ms step_avg:50.38ms
step:1322/1920 train_time:66646ms step_avg:50.41ms
step:1323/1920 train_time:66736ms step_avg:50.44ms
step:1324/1920 train_time:66825ms step_avg:50.47ms
step:1325/1920 train_time:66913ms step_avg:50.50ms
step:1326/1920 train_time:67001ms step_avg:50.53ms
step:1327/1920 train_time:67089ms step_avg:50.56ms
step:1328/1920 train_time:67177ms step_avg:50.59ms
step:1329/1920 train_time:67266ms step_avg:50.61ms
step:1330/1920 train_time:67355ms step_avg:50.64ms
step:1331/1920 train_time:67444ms step_avg:50.67ms
step:1332/1920 train_time:67532ms step_avg:50.70ms
step:1333/1920 train_time:67620ms step_avg:50.73ms
step:1334/1920 train_time:67709ms step_avg:50.76ms
step:1335/1920 train_time:67798ms step_avg:50.78ms
step:1336/1920 train_time:67886ms step_avg:50.81ms
step:1337/1920 train_time:67976ms step_avg:50.84ms
step:1338/1920 train_time:68063ms step_avg:50.87ms
step:1339/1920 train_time:68152ms step_avg:50.90ms
step:1340/1920 train_time:68239ms step_avg:50.92ms
step:1341/1920 train_time:68329ms step_avg:50.95ms
step:1342/1920 train_time:68416ms step_avg:50.98ms
step:1343/1920 train_time:68504ms step_avg:51.01ms
step:1344/1920 train_time:68592ms step_avg:51.04ms
step:1345/1920 train_time:68681ms step_avg:51.06ms
step:1346/1920 train_time:68769ms step_avg:51.09ms
step:1347/1920 train_time:68857ms step_avg:51.12ms
step:1348/1920 train_time:68945ms step_avg:51.15ms
step:1349/1920 train_time:69034ms step_avg:51.17ms
step:1350/1920 train_time:69122ms step_avg:51.20ms
step:1351/1920 train_time:69210ms step_avg:51.23ms
step:1352/1920 train_time:69298ms step_avg:51.26ms
step:1353/1920 train_time:69386ms step_avg:51.28ms
step:1354/1920 train_time:69474ms step_avg:51.31ms
step:1355/1920 train_time:69563ms step_avg:51.34ms
step:1356/1920 train_time:69652ms step_avg:51.37ms
step:1357/1920 train_time:69740ms step_avg:51.39ms
step:1358/1920 train_time:69828ms step_avg:51.42ms
step:1359/1920 train_time:69916ms step_avg:51.45ms
step:1360/1920 train_time:70004ms step_avg:51.47ms
step:1361/1920 train_time:70093ms step_avg:51.50ms
step:1362/1920 train_time:70180ms step_avg:51.53ms
step:1363/1920 train_time:70269ms step_avg:51.55ms
step:1364/1920 train_time:70358ms step_avg:51.58ms
step:1365/1920 train_time:70446ms step_avg:51.61ms
step:1366/1920 train_time:70535ms step_avg:51.64ms
step:1367/1920 train_time:70624ms step_avg:51.66ms
step:1368/1920 train_time:70713ms step_avg:51.69ms
step:1369/1920 train_time:70801ms step_avg:51.72ms
step:1370/1920 train_time:70889ms step_avg:51.74ms
step:1371/1920 train_time:70977ms step_avg:51.77ms
step:1372/1920 train_time:71066ms step_avg:51.80ms
step:1373/1920 train_time:71155ms step_avg:51.82ms
step:1374/1920 train_time:71243ms step_avg:51.85ms
step:1375/1920 train_time:71333ms step_avg:51.88ms
step:1376/1920 train_time:71422ms step_avg:51.91ms
step:1377/1920 train_time:71510ms step_avg:51.93ms
step:1378/1920 train_time:71598ms step_avg:51.96ms
step:1379/1920 train_time:71686ms step_avg:51.98ms
step:1380/1920 train_time:71774ms step_avg:52.01ms
step:1381/1920 train_time:71863ms step_avg:52.04ms
step:1382/1920 train_time:71952ms step_avg:52.06ms
step:1383/1920 train_time:72040ms step_avg:52.09ms
step:1384/1920 train_time:72128ms step_avg:52.12ms
step:1385/1920 train_time:72216ms step_avg:52.14ms
step:1386/1920 train_time:72304ms step_avg:52.17ms
step:1387/1920 train_time:72394ms step_avg:52.19ms
step:1388/1920 train_time:72483ms step_avg:52.22ms
step:1389/1920 train_time:72572ms step_avg:52.25ms
step:1390/1920 train_time:72659ms step_avg:52.27ms
step:1391/1920 train_time:72749ms step_avg:52.30ms
step:1392/1920 train_time:72837ms step_avg:52.33ms
step:1393/1920 train_time:72926ms step_avg:52.35ms
step:1394/1920 train_time:73014ms step_avg:52.38ms
step:1395/1920 train_time:73103ms step_avg:52.40ms
step:1396/1920 train_time:73191ms step_avg:52.43ms
step:1397/1920 train_time:73279ms step_avg:52.45ms
step:1398/1920 train_time:73366ms step_avg:52.48ms
step:1399/1920 train_time:73456ms step_avg:52.51ms
step:1400/1920 train_time:73545ms step_avg:52.53ms
step:1401/1920 train_time:73635ms step_avg:52.56ms
step:1402/1920 train_time:73722ms step_avg:52.58ms
step:1403/1920 train_time:73811ms step_avg:52.61ms
step:1404/1920 train_time:73899ms step_avg:52.63ms
step:1405/1920 train_time:73987ms step_avg:52.66ms
step:1406/1920 train_time:74075ms step_avg:52.68ms
step:1407/1920 train_time:74163ms step_avg:52.71ms
step:1408/1920 train_time:74252ms step_avg:52.74ms
step:1409/1920 train_time:74340ms step_avg:52.76ms
step:1410/1920 train_time:74428ms step_avg:52.79ms
step:1411/1920 train_time:74517ms step_avg:52.81ms
step:1412/1920 train_time:74605ms step_avg:52.84ms
step:1413/1920 train_time:74695ms step_avg:52.86ms
step:1414/1920 train_time:74782ms step_avg:52.89ms
step:1415/1920 train_time:74871ms step_avg:52.91ms
step:1416/1920 train_time:74958ms step_avg:52.94ms
step:1417/1920 train_time:75047ms step_avg:52.96ms
step:1418/1920 train_time:75135ms step_avg:52.99ms
step:1419/1920 train_time:75223ms step_avg:53.01ms
step:1420/1920 train_time:75313ms step_avg:53.04ms
step:1421/1920 train_time:75401ms step_avg:53.06ms
step:1422/1920 train_time:75490ms step_avg:53.09ms
step:1423/1920 train_time:75578ms step_avg:53.11ms
step:1424/1920 train_time:75666ms step_avg:53.14ms
step:1425/1920 train_time:75756ms step_avg:53.16ms
step:1426/1920 train_time:75846ms step_avg:53.19ms
step:1427/1920 train_time:75934ms step_avg:53.21ms
step:1428/1920 train_time:76022ms step_avg:53.24ms
step:1429/1920 train_time:76110ms step_avg:53.26ms
step:1430/1920 train_time:76198ms step_avg:53.29ms
step:1431/1920 train_time:76286ms step_avg:53.31ms
step:1432/1920 train_time:76375ms step_avg:53.33ms
step:1433/1920 train_time:76463ms step_avg:53.36ms
step:1434/1920 train_time:76551ms step_avg:53.38ms
step:1435/1920 train_time:76639ms step_avg:53.41ms
step:1436/1920 train_time:76726ms step_avg:53.43ms
step:1437/1920 train_time:76816ms step_avg:53.46ms
step:1438/1920 train_time:76905ms step_avg:53.48ms
step:1439/1920 train_time:76994ms step_avg:53.50ms
step:1440/1920 train_time:77081ms step_avg:53.53ms
step:1441/1920 train_time:77170ms step_avg:53.55ms
step:1442/1920 train_time:77258ms step_avg:53.58ms
step:1443/1920 train_time:77347ms step_avg:53.60ms
step:1444/1920 train_time:77434ms step_avg:53.62ms
step:1445/1920 train_time:77524ms step_avg:53.65ms
step:1446/1920 train_time:77612ms step_avg:53.67ms
step:1447/1920 train_time:77700ms step_avg:53.70ms
step:1448/1920 train_time:77788ms step_avg:53.72ms
step:1449/1920 train_time:77877ms step_avg:53.75ms
step:1450/1920 train_time:77967ms step_avg:53.77ms
step:1451/1920 train_time:78055ms step_avg:53.79ms
step:1452/1920 train_time:78143ms step_avg:53.82ms
step:1453/1920 train_time:78232ms step_avg:53.84ms
step:1454/1920 train_time:78319ms step_avg:53.86ms
step:1455/1920 train_time:78408ms step_avg:53.89ms
step:1456/1920 train_time:78495ms step_avg:53.91ms
step:1457/1920 train_time:78584ms step_avg:53.94ms
step:1458/1920 train_time:78672ms step_avg:53.96ms
step:1459/1920 train_time:78760ms step_avg:53.98ms
step:1460/1920 train_time:78849ms step_avg:54.01ms
step:1461/1920 train_time:78938ms step_avg:54.03ms
step:1462/1920 train_time:79027ms step_avg:54.05ms
step:1463/1920 train_time:79116ms step_avg:54.08ms
step:1464/1920 train_time:79204ms step_avg:54.10ms
step:1465/1920 train_time:79293ms step_avg:54.13ms
step:1466/1920 train_time:79381ms step_avg:54.15ms
step:1467/1920 train_time:79470ms step_avg:54.17ms
step:1468/1920 train_time:79558ms step_avg:54.19ms
step:1469/1920 train_time:79647ms step_avg:54.22ms
step:1470/1920 train_time:79735ms step_avg:54.24ms
step:1471/1920 train_time:79823ms step_avg:54.26ms
step:1472/1920 train_time:79911ms step_avg:54.29ms
step:1473/1920 train_time:80000ms step_avg:54.31ms
step:1474/1920 train_time:80088ms step_avg:54.33ms
step:1475/1920 train_time:80176ms step_avg:54.36ms
step:1476/1920 train_time:80264ms step_avg:54.38ms
step:1477/1920 train_time:80354ms step_avg:54.40ms
step:1478/1920 train_time:80442ms step_avg:54.43ms
step:1479/1920 train_time:80531ms step_avg:54.45ms
step:1480/1920 train_time:80618ms step_avg:54.47ms
step:1481/1920 train_time:80707ms step_avg:54.49ms
step:1482/1920 train_time:80795ms step_avg:54.52ms
step:1483/1920 train_time:80883ms step_avg:54.54ms
step:1484/1920 train_time:80971ms step_avg:54.56ms
step:1485/1920 train_time:81059ms step_avg:54.59ms
step:1486/1920 train_time:81147ms step_avg:54.61ms
step:1487/1920 train_time:81237ms step_avg:54.63ms
step:1488/1920 train_time:81325ms step_avg:54.65ms
step:1489/1920 train_time:81415ms step_avg:54.68ms
step:1490/1920 train_time:81503ms step_avg:54.70ms
step:1491/1920 train_time:81591ms step_avg:54.72ms
step:1492/1920 train_time:81679ms step_avg:54.74ms
step:1493/1920 train_time:81767ms step_avg:54.77ms
step:1494/1920 train_time:81856ms step_avg:54.79ms
step:1495/1920 train_time:81945ms step_avg:54.81ms
step:1496/1920 train_time:82032ms step_avg:54.83ms
step:1497/1920 train_time:82121ms step_avg:54.86ms
step:1498/1920 train_time:82210ms step_avg:54.88ms
step:1499/1920 train_time:82298ms step_avg:54.90ms
step:1500/1920 train_time:82387ms step_avg:54.92ms
step:1500/1920 val_loss:3.4137 train_time:82477ms step_avg:54.98ms
step:1501/1920 train_time:82496ms step_avg:54.96ms
step:1502/1920 train_time:82567ms step_avg:54.97ms
step:1503/1920 train_time:82659ms step_avg:55.00ms
step:1504/1920 train_time:82747ms step_avg:55.02ms
step:1505/1920 train_time:82834ms step_avg:55.04ms
step:1506/1920 train_time:82922ms step_avg:55.06ms
step:1507/1920 train_time:83009ms step_avg:55.08ms
step:1508/1920 train_time:83096ms step_avg:55.10ms
step:1509/1920 train_time:83184ms step_avg:55.13ms
step:1510/1920 train_time:83271ms step_avg:55.15ms
step:1511/1920 train_time:83360ms step_avg:55.17ms
step:1512/1920 train_time:83450ms step_avg:55.19ms
step:1513/1920 train_time:83541ms step_avg:55.22ms
step:1514/1920 train_time:83630ms step_avg:55.24ms
step:1515/1920 train_time:83719ms step_avg:55.26ms
step:1516/1920 train_time:83807ms step_avg:55.28ms
step:1517/1920 train_time:83895ms step_avg:55.30ms
step:1518/1920 train_time:83982ms step_avg:55.32ms
step:1519/1920 train_time:84071ms step_avg:55.35ms
step:1520/1920 train_time:84158ms step_avg:55.37ms
step:1521/1920 train_time:84247ms step_avg:55.39ms
step:1522/1920 train_time:84334ms step_avg:55.41ms
step:1523/1920 train_time:84424ms step_avg:55.43ms
step:1524/1920 train_time:84512ms step_avg:55.45ms
step:1525/1920 train_time:84602ms step_avg:55.48ms
step:1526/1920 train_time:84692ms step_avg:55.50ms
step:1527/1920 train_time:84780ms step_avg:55.52ms
step:1528/1920 train_time:84868ms step_avg:55.54ms
step:1529/1920 train_time:84956ms step_avg:55.56ms
step:1530/1920 train_time:85044ms step_avg:55.58ms
step:1531/1920 train_time:85132ms step_avg:55.61ms
step:1532/1920 train_time:85220ms step_avg:55.63ms
step:1533/1920 train_time:85310ms step_avg:55.65ms
step:1534/1920 train_time:85398ms step_avg:55.67ms
step:1535/1920 train_time:85488ms step_avg:55.69ms
step:1536/1920 train_time:85576ms step_avg:55.71ms
step:1537/1920 train_time:85666ms step_avg:55.74ms
step:1538/1920 train_time:85754ms step_avg:55.76ms
step:1539/1920 train_time:85843ms step_avg:55.78ms
step:1540/1920 train_time:85931ms step_avg:55.80ms
step:1541/1920 train_time:86019ms step_avg:55.82ms
step:1542/1920 train_time:86107ms step_avg:55.84ms
step:1543/1920 train_time:86194ms step_avg:55.86ms
step:1544/1920 train_time:86282ms step_avg:55.88ms
step:1545/1920 train_time:86371ms step_avg:55.90ms
step:1546/1920 train_time:86459ms step_avg:55.92ms
step:1547/1920 train_time:86550ms step_avg:55.95ms
step:1548/1920 train_time:86639ms step_avg:55.97ms
step:1549/1920 train_time:86729ms step_avg:55.99ms
step:1550/1920 train_time:86817ms step_avg:56.01ms
step:1551/1920 train_time:86906ms step_avg:56.03ms
step:1552/1920 train_time:86994ms step_avg:56.05ms
step:1553/1920 train_time:87083ms step_avg:56.07ms
step:1554/1920 train_time:87170ms step_avg:56.09ms
step:1555/1920 train_time:87258ms step_avg:56.11ms
step:1556/1920 train_time:87346ms step_avg:56.14ms
step:1557/1920 train_time:87435ms step_avg:56.16ms
step:1558/1920 train_time:87524ms step_avg:56.18ms
step:1559/1920 train_time:87614ms step_avg:56.20ms
step:1560/1920 train_time:87702ms step_avg:56.22ms
step:1561/1920 train_time:87792ms step_avg:56.24ms
step:1562/1920 train_time:87881ms step_avg:56.26ms
step:1563/1920 train_time:87970ms step_avg:56.28ms
step:1564/1920 train_time:88058ms step_avg:56.30ms
step:1565/1920 train_time:88146ms step_avg:56.32ms
step:1566/1920 train_time:88234ms step_avg:56.34ms
step:1567/1920 train_time:88322ms step_avg:56.36ms
step:1568/1920 train_time:88409ms step_avg:56.38ms
step:1569/1920 train_time:88498ms step_avg:56.40ms
step:1570/1920 train_time:88587ms step_avg:56.43ms
step:1571/1920 train_time:88677ms step_avg:56.45ms
step:1572/1920 train_time:88766ms step_avg:56.47ms
step:1573/1920 train_time:88854ms step_avg:56.49ms
step:1574/1920 train_time:88943ms step_avg:56.51ms
step:1575/1920 train_time:89031ms step_avg:56.53ms
step:1576/1920 train_time:89119ms step_avg:56.55ms
step:1577/1920 train_time:89209ms step_avg:56.57ms
step:1578/1920 train_time:89296ms step_avg:56.59ms
step:1579/1920 train_time:89385ms step_avg:56.61ms
step:1580/1920 train_time:89473ms step_avg:56.63ms
step:1581/1920 train_time:89563ms step_avg:56.65ms
step:1582/1920 train_time:89651ms step_avg:56.67ms
step:1583/1920 train_time:89740ms step_avg:56.69ms
step:1584/1920 train_time:89829ms step_avg:56.71ms
step:1585/1920 train_time:89917ms step_avg:56.73ms
step:1586/1920 train_time:90006ms step_avg:56.75ms
step:1587/1920 train_time:90095ms step_avg:56.77ms
step:1588/1920 train_time:90185ms step_avg:56.79ms
step:1589/1920 train_time:90273ms step_avg:56.81ms
step:1590/1920 train_time:90361ms step_avg:56.83ms
step:1591/1920 train_time:90450ms step_avg:56.85ms
step:1592/1920 train_time:90539ms step_avg:56.87ms
step:1593/1920 train_time:90630ms step_avg:56.89ms
step:1594/1920 train_time:90718ms step_avg:56.91ms
step:1595/1920 train_time:90809ms step_avg:56.93ms
step:1596/1920 train_time:90896ms step_avg:56.95ms
step:1597/1920 train_time:90985ms step_avg:56.97ms
step:1598/1920 train_time:91072ms step_avg:56.99ms
step:1599/1920 train_time:91161ms step_avg:57.01ms
step:1600/1920 train_time:91249ms step_avg:57.03ms
step:1601/1920 train_time:91338ms step_avg:57.05ms
step:1602/1920 train_time:91426ms step_avg:57.07ms
step:1603/1920 train_time:91514ms step_avg:57.09ms
step:1604/1920 train_time:91603ms step_avg:57.11ms
step:1605/1920 train_time:91692ms step_avg:57.13ms
step:1606/1920 train_time:91782ms step_avg:57.15ms
step:1607/1920 train_time:91871ms step_avg:57.17ms
step:1608/1920 train_time:91959ms step_avg:57.19ms
step:1609/1920 train_time:92049ms step_avg:57.21ms
step:1610/1920 train_time:92137ms step_avg:57.23ms
step:1611/1920 train_time:92226ms step_avg:57.25ms
step:1612/1920 train_time:92313ms step_avg:57.27ms
step:1613/1920 train_time:92401ms step_avg:57.29ms
step:1614/1920 train_time:92489ms step_avg:57.30ms
step:1615/1920 train_time:92577ms step_avg:57.32ms
step:1616/1920 train_time:92666ms step_avg:57.34ms
step:1617/1920 train_time:92755ms step_avg:57.36ms
step:1618/1920 train_time:92843ms step_avg:57.38ms
step:1619/1920 train_time:92932ms step_avg:57.40ms
step:1620/1920 train_time:93020ms step_avg:57.42ms
step:1621/1920 train_time:93109ms step_avg:57.44ms
step:1622/1920 train_time:93197ms step_avg:57.46ms
step:1623/1920 train_time:93286ms step_avg:57.48ms
step:1624/1920 train_time:93374ms step_avg:57.50ms
step:1625/1920 train_time:93463ms step_avg:57.52ms
step:1626/1920 train_time:93551ms step_avg:57.53ms
step:1627/1920 train_time:93639ms step_avg:57.55ms
step:1628/1920 train_time:93727ms step_avg:57.57ms
step:1629/1920 train_time:93816ms step_avg:57.59ms
step:1630/1920 train_time:93905ms step_avg:57.61ms
step:1631/1920 train_time:93994ms step_avg:57.63ms
step:1632/1920 train_time:94082ms step_avg:57.65ms
step:1633/1920 train_time:94172ms step_avg:57.67ms
step:1634/1920 train_time:94260ms step_avg:57.69ms
step:1635/1920 train_time:94349ms step_avg:57.71ms
step:1636/1920 train_time:94437ms step_avg:57.72ms
step:1637/1920 train_time:94526ms step_avg:57.74ms
step:1638/1920 train_time:94614ms step_avg:57.76ms
step:1639/1920 train_time:94703ms step_avg:57.78ms
step:1640/1920 train_time:94791ms step_avg:57.80ms
step:1641/1920 train_time:94880ms step_avg:57.82ms
step:1642/1920 train_time:94968ms step_avg:57.84ms
step:1643/1920 train_time:95057ms step_avg:57.86ms
step:1644/1920 train_time:95146ms step_avg:57.87ms
step:1645/1920 train_time:95233ms step_avg:57.89ms
step:1646/1920 train_time:95322ms step_avg:57.91ms
step:1647/1920 train_time:95411ms step_avg:57.93ms
step:1648/1920 train_time:95500ms step_avg:57.95ms
step:1649/1920 train_time:95589ms step_avg:57.97ms
step:1650/1920 train_time:95677ms step_avg:57.99ms
step:1651/1920 train_time:95766ms step_avg:58.00ms
step:1652/1920 train_time:95853ms step_avg:58.02ms
step:1653/1920 train_time:95942ms step_avg:58.04ms
step:1654/1920 train_time:96030ms step_avg:58.06ms
step:1655/1920 train_time:96120ms step_avg:58.08ms
step:1656/1920 train_time:96208ms step_avg:58.10ms
step:1657/1920 train_time:96296ms step_avg:58.11ms
step:1658/1920 train_time:96385ms step_avg:58.13ms
step:1659/1920 train_time:96473ms step_avg:58.15ms
step:1660/1920 train_time:96562ms step_avg:58.17ms
step:1661/1920 train_time:96650ms step_avg:58.19ms
step:1662/1920 train_time:96738ms step_avg:58.21ms
step:1663/1920 train_time:96828ms step_avg:58.22ms
step:1664/1920 train_time:96915ms step_avg:58.24ms
step:1665/1920 train_time:97004ms step_avg:58.26ms
step:1666/1920 train_time:97091ms step_avg:58.28ms
step:1667/1920 train_time:97180ms step_avg:58.30ms
step:1668/1920 train_time:97268ms step_avg:58.31ms
step:1669/1920 train_time:97357ms step_avg:58.33ms
step:1670/1920 train_time:97445ms step_avg:58.35ms
step:1671/1920 train_time:97533ms step_avg:58.37ms
step:1672/1920 train_time:97622ms step_avg:58.39ms
step:1673/1920 train_time:97711ms step_avg:58.40ms
step:1674/1920 train_time:97799ms step_avg:58.42ms
step:1675/1920 train_time:97889ms step_avg:58.44ms
step:1676/1920 train_time:97977ms step_avg:58.46ms
step:1677/1920 train_time:98066ms step_avg:58.48ms
step:1678/1920 train_time:98153ms step_avg:58.49ms
step:1679/1920 train_time:98242ms step_avg:58.51ms
step:1680/1920 train_time:98329ms step_avg:58.53ms
step:1681/1920 train_time:98418ms step_avg:58.55ms
step:1682/1920 train_time:98506ms step_avg:58.56ms
step:1683/1920 train_time:98594ms step_avg:58.58ms
step:1684/1920 train_time:98682ms step_avg:58.60ms
step:1685/1920 train_time:98771ms step_avg:58.62ms
step:1686/1920 train_time:98861ms step_avg:58.64ms
step:1687/1920 train_time:98950ms step_avg:58.65ms
step:1688/1920 train_time:99038ms step_avg:58.67ms
step:1689/1920 train_time:99126ms step_avg:58.69ms
step:1690/1920 train_time:99214ms step_avg:58.71ms
step:1691/1920 train_time:99303ms step_avg:58.72ms
step:1692/1920 train_time:99391ms step_avg:58.74ms
step:1693/1920 train_time:99480ms step_avg:58.76ms
step:1694/1920 train_time:99568ms step_avg:58.78ms
step:1695/1920 train_time:99657ms step_avg:58.79ms
step:1696/1920 train_time:99745ms step_avg:58.81ms
step:1697/1920 train_time:99833ms step_avg:58.83ms
step:1698/1920 train_time:99921ms step_avg:58.85ms
step:1699/1920 train_time:100011ms step_avg:58.86ms
step:1700/1920 train_time:100098ms step_avg:58.88ms
step:1701/1920 train_time:100188ms step_avg:58.90ms
step:1702/1920 train_time:100276ms step_avg:58.92ms
step:1703/1920 train_time:100364ms step_avg:58.93ms
step:1704/1920 train_time:100452ms step_avg:58.95ms
step:1705/1920 train_time:100541ms step_avg:58.97ms
step:1706/1920 train_time:100629ms step_avg:58.99ms
step:1707/1920 train_time:100717ms step_avg:59.00ms
step:1708/1920 train_time:100805ms step_avg:59.02ms
step:1709/1920 train_time:100893ms step_avg:59.04ms
step:1710/1920 train_time:100982ms step_avg:59.05ms
step:1711/1920 train_time:101072ms step_avg:59.07ms
step:1712/1920 train_time:101161ms step_avg:59.09ms
step:1713/1920 train_time:101250ms step_avg:59.11ms
step:1714/1920 train_time:101338ms step_avg:59.12ms
step:1715/1920 train_time:101427ms step_avg:59.14ms
step:1716/1920 train_time:101516ms step_avg:59.16ms
step:1717/1920 train_time:101604ms step_avg:59.18ms
step:1718/1920 train_time:101692ms step_avg:59.19ms
step:1719/1920 train_time:101781ms step_avg:59.21ms
step:1720/1920 train_time:101869ms step_avg:59.23ms
step:1721/1920 train_time:101958ms step_avg:59.24ms
step:1722/1920 train_time:102047ms step_avg:59.26ms
step:1723/1920 train_time:102134ms step_avg:59.28ms
step:1724/1920 train_time:102223ms step_avg:59.29ms
step:1725/1920 train_time:102311ms step_avg:59.31ms
step:1726/1920 train_time:102399ms step_avg:59.33ms
step:1727/1920 train_time:102488ms step_avg:59.34ms
step:1728/1920 train_time:102576ms step_avg:59.36ms
step:1729/1920 train_time:102665ms step_avg:59.38ms
step:1730/1920 train_time:102753ms step_avg:59.39ms
step:1731/1920 train_time:102843ms step_avg:59.41ms
step:1732/1920 train_time:102931ms step_avg:59.43ms
step:1733/1920 train_time:103019ms step_avg:59.45ms
step:1734/1920 train_time:103107ms step_avg:59.46ms
step:1735/1920 train_time:103195ms step_avg:59.48ms
step:1736/1920 train_time:103283ms step_avg:59.49ms
step:1737/1920 train_time:103371ms step_avg:59.51ms
step:1738/1920 train_time:103460ms step_avg:59.53ms
step:1739/1920 train_time:103550ms step_avg:59.55ms
step:1740/1920 train_time:103638ms step_avg:59.56ms
step:1741/1920 train_time:103729ms step_avg:59.58ms
step:1742/1920 train_time:103817ms step_avg:59.60ms
step:1743/1920 train_time:103907ms step_avg:59.61ms
step:1744/1920 train_time:103994ms step_avg:59.63ms
step:1745/1920 train_time:104083ms step_avg:59.65ms
step:1746/1920 train_time:104170ms step_avg:59.66ms
step:1747/1920 train_time:104259ms step_avg:59.68ms
step:1748/1920 train_time:104347ms step_avg:59.69ms
step:1749/1920 train_time:104435ms step_avg:59.71ms
step:1750/1920 train_time:104523ms step_avg:59.73ms
step:1750/1920 val_loss:3.3232 train_time:104615ms step_avg:59.78ms
step:1751/1920 train_time:104632ms step_avg:59.76ms
step:1752/1920 train_time:104706ms step_avg:59.76ms
step:1753/1920 train_time:104796ms step_avg:59.78ms
step:1754/1920 train_time:104883ms step_avg:59.80ms
step:1755/1920 train_time:104971ms step_avg:59.81ms
step:1756/1920 train_time:105059ms step_avg:59.83ms
step:1757/1920 train_time:105146ms step_avg:59.84ms
step:1758/1920 train_time:105234ms step_avg:59.86ms
step:1759/1920 train_time:105322ms step_avg:59.88ms
step:1760/1920 train_time:105411ms step_avg:59.89ms
step:1761/1920 train_time:105499ms step_avg:59.91ms
step:1762/1920 train_time:105590ms step_avg:59.93ms
step:1763/1920 train_time:105681ms step_avg:59.94ms
step:1764/1920 train_time:105771ms step_avg:59.96ms
step:1765/1920 train_time:105860ms step_avg:59.98ms
step:1766/1920 train_time:105947ms step_avg:59.99ms
step:1767/1920 train_time:106035ms step_avg:60.01ms
step:1768/1920 train_time:106123ms step_avg:60.02ms
step:1769/1920 train_time:106211ms step_avg:60.04ms
step:1770/1920 train_time:106299ms step_avg:60.06ms
step:1771/1920 train_time:106386ms step_avg:60.07ms
step:1772/1920 train_time:106474ms step_avg:60.09ms
step:1773/1920 train_time:106565ms step_avg:60.10ms
step:1774/1920 train_time:106654ms step_avg:60.12ms
step:1775/1920 train_time:106745ms step_avg:60.14ms
step:1776/1920 train_time:106833ms step_avg:60.15ms
step:1777/1920 train_time:106922ms step_avg:60.17ms
step:1778/1920 train_time:107010ms step_avg:60.19ms
step:1779/1920 train_time:107099ms step_avg:60.20ms
step:1780/1920 train_time:107185ms step_avg:60.22ms
step:1781/1920 train_time:107275ms step_avg:60.23ms
step:1782/1920 train_time:107363ms step_avg:60.25ms
step:1783/1920 train_time:107451ms step_avg:60.26ms
step:1784/1920 train_time:107540ms step_avg:60.28ms
step:1785/1920 train_time:107628ms step_avg:60.30ms
step:1786/1920 train_time:107718ms step_avg:60.31ms
step:1787/1920 train_time:107807ms step_avg:60.33ms
step:1788/1920 train_time:107895ms step_avg:60.34ms
step:1789/1920 train_time:107984ms step_avg:60.36ms
step:1790/1920 train_time:108072ms step_avg:60.38ms
step:1791/1920 train_time:108161ms step_avg:60.39ms
step:1792/1920 train_time:108248ms step_avg:60.41ms
step:1793/1920 train_time:108338ms step_avg:60.42ms
step:1794/1920 train_time:108426ms step_avg:60.44ms
step:1795/1920 train_time:108515ms step_avg:60.45ms
step:1796/1920 train_time:108603ms step_avg:60.47ms
step:1797/1920 train_time:108693ms step_avg:60.49ms
step:1798/1920 train_time:108782ms step_avg:60.50ms
step:1799/1920 train_time:108871ms step_avg:60.52ms
step:1800/1920 train_time:108959ms step_avg:60.53ms
step:1801/1920 train_time:109047ms step_avg:60.55ms
step:1802/1920 train_time:109135ms step_avg:60.56ms
step:1803/1920 train_time:109225ms step_avg:60.58ms
step:1804/1920 train_time:109314ms step_avg:60.60ms
step:1805/1920 train_time:109403ms step_avg:60.61ms
step:1806/1920 train_time:109491ms step_avg:60.63ms
step:1807/1920 train_time:109579ms step_avg:60.64ms
step:1808/1920 train_time:109667ms step_avg:60.66ms
step:1809/1920 train_time:109757ms step_avg:60.67ms
step:1810/1920 train_time:109845ms step_avg:60.69ms
step:1811/1920 train_time:109934ms step_avg:60.70ms
step:1812/1920 train_time:110023ms step_avg:60.72ms
step:1813/1920 train_time:110111ms step_avg:60.73ms
step:1814/1920 train_time:110200ms step_avg:60.75ms
step:1815/1920 train_time:110288ms step_avg:60.76ms
step:1816/1920 train_time:110377ms step_avg:60.78ms
step:1817/1920 train_time:110465ms step_avg:60.80ms
step:1818/1920 train_time:110554ms step_avg:60.81ms
step:1819/1920 train_time:110644ms step_avg:60.83ms
step:1820/1920 train_time:110733ms step_avg:60.84ms
step:1821/1920 train_time:110824ms step_avg:60.86ms
step:1822/1920 train_time:110913ms step_avg:60.87ms
step:1823/1920 train_time:111002ms step_avg:60.89ms
step:1824/1920 train_time:111089ms step_avg:60.90ms
step:1825/1920 train_time:111178ms step_avg:60.92ms
step:1826/1920 train_time:111265ms step_avg:60.93ms
step:1827/1920 train_time:111354ms step_avg:60.95ms
step:1828/1920 train_time:111442ms step_avg:60.96ms
step:1829/1920 train_time:111530ms step_avg:60.98ms
step:1830/1920 train_time:111618ms step_avg:60.99ms
step:1831/1920 train_time:111707ms step_avg:61.01ms
step:1832/1920 train_time:111795ms step_avg:61.02ms
step:1833/1920 train_time:111884ms step_avg:61.04ms
step:1834/1920 train_time:111973ms step_avg:61.05ms
step:1835/1920 train_time:112062ms step_avg:61.07ms
step:1836/1920 train_time:112149ms step_avg:61.08ms
step:1837/1920 train_time:112237ms step_avg:61.10ms
step:1838/1920 train_time:112325ms step_avg:61.11ms
step:1839/1920 train_time:112414ms step_avg:61.13ms
step:1840/1920 train_time:112502ms step_avg:61.14ms
step:1841/1920 train_time:112591ms step_avg:61.16ms
step:1842/1920 train_time:112679ms step_avg:61.17ms
step:1843/1920 train_time:112768ms step_avg:61.19ms
step:1844/1920 train_time:112856ms step_avg:61.20ms
step:1845/1920 train_time:112946ms step_avg:61.22ms
step:1846/1920 train_time:113034ms step_avg:61.23ms
step:1847/1920 train_time:113123ms step_avg:61.25ms
step:1848/1920 train_time:113212ms step_avg:61.26ms
step:1849/1920 train_time:113301ms step_avg:61.28ms
step:1850/1920 train_time:113389ms step_avg:61.29ms
step:1851/1920 train_time:113478ms step_avg:61.31ms
step:1852/1920 train_time:113566ms step_avg:61.32ms
step:1853/1920 train_time:113656ms step_avg:61.34ms
step:1854/1920 train_time:113743ms step_avg:61.35ms
step:1855/1920 train_time:113832ms step_avg:61.36ms
step:1856/1920 train_time:113920ms step_avg:61.38ms
step:1857/1920 train_time:114009ms step_avg:61.39ms
step:1858/1920 train_time:114097ms step_avg:61.41ms
step:1859/1920 train_time:114186ms step_avg:61.42ms
step:1860/1920 train_time:114275ms step_avg:61.44ms
step:1861/1920 train_time:114365ms step_avg:61.45ms
step:1862/1920 train_time:114453ms step_avg:61.47ms
step:1863/1920 train_time:114543ms step_avg:61.48ms
step:1864/1920 train_time:114631ms step_avg:61.50ms
step:1865/1920 train_time:114721ms step_avg:61.51ms
step:1866/1920 train_time:114808ms step_avg:61.53ms
step:1867/1920 train_time:114896ms step_avg:61.54ms
step:1868/1920 train_time:114984ms step_avg:61.55ms
step:1869/1920 train_time:115072ms step_avg:61.57ms
step:1870/1920 train_time:115161ms step_avg:61.58ms
step:1871/1920 train_time:115250ms step_avg:61.60ms
step:1872/1920 train_time:115338ms step_avg:61.61ms
step:1873/1920 train_time:115426ms step_avg:61.63ms
step:1874/1920 train_time:115515ms step_avg:61.64ms
step:1875/1920 train_time:115604ms step_avg:61.66ms
step:1876/1920 train_time:115692ms step_avg:61.67ms
step:1877/1920 train_time:115781ms step_avg:61.68ms
step:1878/1920 train_time:115868ms step_avg:61.70ms
step:1879/1920 train_time:115957ms step_avg:61.71ms
step:1880/1920 train_time:116045ms step_avg:61.73ms
step:1881/1920 train_time:116133ms step_avg:61.74ms
step:1882/1920 train_time:116222ms step_avg:61.75ms
step:1883/1920 train_time:116311ms step_avg:61.77ms
step:1884/1920 train_time:116400ms step_avg:61.78ms
step:1885/1920 train_time:116490ms step_avg:61.80ms
step:1886/1920 train_time:116578ms step_avg:61.81ms
step:1887/1920 train_time:116668ms step_avg:61.83ms
step:1888/1920 train_time:116756ms step_avg:61.84ms
step:1889/1920 train_time:116845ms step_avg:61.86ms
step:1890/1920 train_time:116933ms step_avg:61.87ms
step:1891/1920 train_time:117024ms step_avg:61.88ms
step:1892/1920 train_time:117113ms step_avg:61.90ms
step:1893/1920 train_time:117202ms step_avg:61.91ms
step:1894/1920 train_time:117291ms step_avg:61.93ms
step:1895/1920 train_time:117381ms step_avg:61.94ms
step:1896/1920 train_time:117470ms step_avg:61.96ms
step:1897/1920 train_time:117559ms step_avg:61.97ms
step:1898/1920 train_time:117647ms step_avg:61.98ms
step:1899/1920 train_time:117736ms step_avg:62.00ms
step:1900/1920 train_time:117824ms step_avg:62.01ms
step:1901/1920 train_time:117913ms step_avg:62.03ms
step:1902/1920 train_time:118002ms step_avg:62.04ms
step:1903/1920 train_time:118091ms step_avg:62.06ms
step:1904/1920 train_time:118180ms step_avg:62.07ms
step:1905/1920 train_time:118269ms step_avg:62.08ms
step:1906/1920 train_time:118356ms step_avg:62.10ms
step:1907/1920 train_time:118446ms step_avg:62.11ms
step:1908/1920 train_time:118535ms step_avg:62.13ms
step:1909/1920 train_time:118625ms step_avg:62.14ms
step:1910/1920 train_time:118713ms step_avg:62.15ms
step:1911/1920 train_time:118803ms step_avg:62.17ms
step:1912/1920 train_time:118891ms step_avg:62.18ms
step:1913/1920 train_time:118980ms step_avg:62.20ms
step:1914/1920 train_time:119068ms step_avg:62.21ms
step:1915/1920 train_time:119158ms step_avg:62.22ms
step:1916/1920 train_time:119246ms step_avg:62.24ms
step:1917/1920 train_time:119336ms step_avg:62.25ms
step:1918/1920 train_time:119424ms step_avg:62.26ms
step:1919/1920 train_time:119514ms step_avg:62.28ms
step:1920/1920 train_time:119602ms step_avg:62.29ms
step:1920/1920 val_loss:3.2783 train_time:119694ms step_avg:62.34ms
peak memory allocated: 29863 MiB reserved: 44438 MiB

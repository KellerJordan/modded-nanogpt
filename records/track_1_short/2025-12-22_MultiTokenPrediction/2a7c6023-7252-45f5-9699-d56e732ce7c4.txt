import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:27:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    243037      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    243038      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    243039      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    243040      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    243041      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    243042      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    243043      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    243044      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    243038      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    243039      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    243040      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    243041      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    243042      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    243043      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    243044      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8339 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:78ms step_avg:78.12ms
step:2/1920 train_time:99ms step_avg:49.49ms
step:3/1920 train_time:123ms step_avg:41.11ms
step:4/1920 train_time:157ms step_avg:39.36ms
step:5/1920 train_time:192ms step_avg:38.30ms
step:6/1920 train_time:267ms step_avg:44.56ms
step:7/1920 train_time:301ms step_avg:42.99ms
step:8/1920 train_time:335ms step_avg:41.89ms
step:9/1920 train_time:369ms step_avg:41.02ms
step:10/1920 train_time:403ms step_avg:40.33ms
step:11/1920 train_time:438ms step_avg:39.79ms
step:12/1920 train_time:472ms step_avg:39.35ms
step:13/1920 train_time:506ms step_avg:38.94ms
step:14/1920 train_time:541ms step_avg:38.61ms
step:15/1920 train_time:575ms step_avg:38.32ms
step:16/1920 train_time:609ms step_avg:38.06ms
step:17/1920 train_time:643ms step_avg:37.85ms
step:18/1920 train_time:678ms step_avg:37.66ms
step:19/1920 train_time:712ms step_avg:37.48ms
step:20/1920 train_time:746ms step_avg:37.31ms
step:21/1920 train_time:781ms step_avg:37.17ms
step:22/1920 train_time:815ms step_avg:37.03ms
step:23/1920 train_time:849ms step_avg:36.92ms
step:24/1920 train_time:883ms step_avg:36.81ms
step:25/1920 train_time:918ms step_avg:36.72ms
step:26/1920 train_time:952ms step_avg:36.62ms
step:27/1920 train_time:987ms step_avg:36.55ms
step:28/1920 train_time:1021ms step_avg:36.47ms
step:29/1920 train_time:1055ms step_avg:36.39ms
step:30/1920 train_time:1090ms step_avg:36.32ms
step:31/1920 train_time:1124ms step_avg:36.26ms
step:32/1920 train_time:1158ms step_avg:36.19ms
step:33/1920 train_time:1193ms step_avg:36.15ms
step:34/1920 train_time:1228ms step_avg:36.11ms
step:35/1920 train_time:1263ms step_avg:36.09ms
step:36/1920 train_time:1297ms step_avg:36.04ms
step:37/1920 train_time:1333ms step_avg:36.02ms
step:38/1920 train_time:1367ms step_avg:35.98ms
step:39/1920 train_time:1402ms step_avg:35.95ms
step:40/1920 train_time:1436ms step_avg:35.91ms
step:41/1920 train_time:1471ms step_avg:35.88ms
step:42/1920 train_time:1506ms step_avg:35.85ms
step:43/1920 train_time:1540ms step_avg:35.82ms
step:44/1920 train_time:1574ms step_avg:35.78ms
step:45/1920 train_time:1609ms step_avg:35.75ms
step:46/1920 train_time:1644ms step_avg:35.73ms
step:47/1920 train_time:1678ms step_avg:35.70ms
step:48/1920 train_time:1712ms step_avg:35.67ms
step:49/1920 train_time:1747ms step_avg:35.64ms
step:50/1920 train_time:1781ms step_avg:35.62ms
step:51/1920 train_time:1815ms step_avg:35.59ms
step:52/1920 train_time:1849ms step_avg:35.57ms
step:53/1920 train_time:1884ms step_avg:35.54ms
step:54/1920 train_time:1918ms step_avg:35.52ms
step:55/1920 train_time:1952ms step_avg:35.50ms
step:56/1920 train_time:1987ms step_avg:35.48ms
step:57/1920 train_time:2021ms step_avg:35.45ms
step:58/1920 train_time:2055ms step_avg:35.43ms
step:59/1920 train_time:2090ms step_avg:35.42ms
step:60/1920 train_time:2124ms step_avg:35.40ms
step:61/1920 train_time:2159ms step_avg:35.39ms
step:62/1920 train_time:2193ms step_avg:35.37ms
step:63/1920 train_time:2228ms step_avg:35.37ms
step:64/1920 train_time:2263ms step_avg:35.35ms
step:65/1920 train_time:2298ms step_avg:35.35ms
step:66/1920 train_time:2332ms step_avg:35.34ms
step:67/1920 train_time:2368ms step_avg:35.34ms
step:68/1920 train_time:2402ms step_avg:35.32ms
step:69/1920 train_time:2437ms step_avg:35.32ms
step:70/1920 train_time:2471ms step_avg:35.30ms
step:71/1920 train_time:2506ms step_avg:35.30ms
step:72/1920 train_time:2541ms step_avg:35.29ms
step:73/1920 train_time:2575ms step_avg:35.27ms
step:74/1920 train_time:2609ms step_avg:35.26ms
step:75/1920 train_time:2644ms step_avg:35.25ms
step:76/1920 train_time:2678ms step_avg:35.24ms
step:77/1920 train_time:2712ms step_avg:35.23ms
step:78/1920 train_time:2747ms step_avg:35.21ms
step:79/1920 train_time:2781ms step_avg:35.21ms
step:80/1920 train_time:2815ms step_avg:35.19ms
step:81/1920 train_time:2850ms step_avg:35.19ms
step:82/1920 train_time:2884ms step_avg:35.18ms
step:83/1920 train_time:2919ms step_avg:35.17ms
step:84/1920 train_time:2954ms step_avg:35.16ms
step:85/1920 train_time:2988ms step_avg:35.15ms
step:86/1920 train_time:3022ms step_avg:35.14ms
step:87/1920 train_time:3057ms step_avg:35.14ms
step:88/1920 train_time:3091ms step_avg:35.13ms
step:89/1920 train_time:3125ms step_avg:35.12ms
step:90/1920 train_time:3160ms step_avg:35.11ms
step:91/1920 train_time:3194ms step_avg:35.10ms
step:92/1920 train_time:3228ms step_avg:35.09ms
step:93/1920 train_time:3263ms step_avg:35.08ms
step:94/1920 train_time:3297ms step_avg:35.07ms
step:95/1920 train_time:3332ms step_avg:35.07ms
step:96/1920 train_time:3366ms step_avg:35.06ms
step:97/1920 train_time:3401ms step_avg:35.06ms
step:98/1920 train_time:3435ms step_avg:35.05ms
step:99/1920 train_time:3470ms step_avg:35.05ms
step:100/1920 train_time:3505ms step_avg:35.05ms
step:101/1920 train_time:3539ms step_avg:35.04ms
step:102/1920 train_time:3573ms step_avg:35.03ms
step:103/1920 train_time:3608ms step_avg:35.03ms
step:104/1920 train_time:3643ms step_avg:35.03ms
step:105/1920 train_time:3677ms step_avg:35.02ms
step:106/1920 train_time:3711ms step_avg:35.01ms
step:107/1920 train_time:3746ms step_avg:35.01ms
step:108/1920 train_time:3780ms step_avg:35.00ms
step:109/1920 train_time:3815ms step_avg:35.00ms
step:110/1920 train_time:3849ms step_avg:34.99ms
step:111/1920 train_time:3884ms step_avg:34.99ms
step:112/1920 train_time:3918ms step_avg:34.99ms
step:113/1920 train_time:3953ms step_avg:34.98ms
step:114/1920 train_time:3987ms step_avg:34.97ms
step:115/1920 train_time:4021ms step_avg:34.97ms
step:116/1920 train_time:4056ms step_avg:34.96ms
step:117/1920 train_time:4090ms step_avg:34.96ms
step:118/1920 train_time:4125ms step_avg:34.95ms
step:119/1920 train_time:4159ms step_avg:34.95ms
step:120/1920 train_time:4193ms step_avg:34.94ms
step:121/1920 train_time:4228ms step_avg:34.94ms
step:122/1920 train_time:4262ms step_avg:34.93ms
step:123/1920 train_time:4297ms step_avg:34.93ms
step:124/1920 train_time:4331ms step_avg:34.93ms
step:125/1920 train_time:4366ms step_avg:34.93ms
step:126/1920 train_time:4400ms step_avg:34.92ms
step:127/1920 train_time:4435ms step_avg:34.92ms
step:128/1920 train_time:4469ms step_avg:34.92ms
step:129/1920 train_time:4504ms step_avg:34.91ms
step:130/1920 train_time:4538ms step_avg:34.91ms
step:131/1920 train_time:4573ms step_avg:34.91ms
step:132/1920 train_time:4607ms step_avg:34.90ms
step:133/1920 train_time:4642ms step_avg:34.90ms
step:134/1920 train_time:4676ms step_avg:34.89ms
step:135/1920 train_time:4711ms step_avg:34.89ms
step:136/1920 train_time:4745ms step_avg:34.89ms
step:137/1920 train_time:4779ms step_avg:34.89ms
step:138/1920 train_time:4814ms step_avg:34.88ms
step:139/1920 train_time:4848ms step_avg:34.88ms
step:140/1920 train_time:4882ms step_avg:34.87ms
step:141/1920 train_time:4917ms step_avg:34.87ms
step:142/1920 train_time:4951ms step_avg:34.87ms
step:143/1920 train_time:4986ms step_avg:34.87ms
step:144/1920 train_time:5020ms step_avg:34.86ms
step:145/1920 train_time:5055ms step_avg:34.86ms
step:146/1920 train_time:5089ms step_avg:34.86ms
step:147/1920 train_time:5123ms step_avg:34.85ms
step:148/1920 train_time:5158ms step_avg:34.85ms
step:149/1920 train_time:5192ms step_avg:34.85ms
step:150/1920 train_time:5226ms step_avg:34.84ms
step:151/1920 train_time:5261ms step_avg:34.84ms
step:152/1920 train_time:5296ms step_avg:34.84ms
step:153/1920 train_time:5330ms step_avg:34.84ms
step:154/1920 train_time:5365ms step_avg:34.84ms
step:155/1920 train_time:5399ms step_avg:34.83ms
step:156/1920 train_time:5433ms step_avg:34.83ms
step:157/1920 train_time:5468ms step_avg:34.83ms
step:158/1920 train_time:5502ms step_avg:34.82ms
step:159/1920 train_time:5537ms step_avg:34.82ms
step:160/1920 train_time:5571ms step_avg:34.82ms
step:161/1920 train_time:5606ms step_avg:34.82ms
step:162/1920 train_time:5640ms step_avg:34.81ms
step:163/1920 train_time:5675ms step_avg:34.82ms
step:164/1920 train_time:5709ms step_avg:34.81ms
step:165/1920 train_time:5744ms step_avg:34.81ms
step:166/1920 train_time:5778ms step_avg:34.81ms
step:167/1920 train_time:5812ms step_avg:34.81ms
step:168/1920 train_time:5847ms step_avg:34.80ms
step:169/1920 train_time:5881ms step_avg:34.80ms
step:170/1920 train_time:5916ms step_avg:34.80ms
step:171/1920 train_time:5950ms step_avg:34.80ms
step:172/1920 train_time:5984ms step_avg:34.79ms
step:173/1920 train_time:6019ms step_avg:34.79ms
step:174/1920 train_time:6053ms step_avg:34.79ms
step:175/1920 train_time:6088ms step_avg:34.79ms
step:176/1920 train_time:6123ms step_avg:34.79ms
step:177/1920 train_time:6157ms step_avg:34.79ms
step:178/1920 train_time:6191ms step_avg:34.78ms
step:179/1920 train_time:6226ms step_avg:34.78ms
step:180/1920 train_time:6260ms step_avg:34.78ms
step:181/1920 train_time:6295ms step_avg:34.78ms
step:182/1920 train_time:6329ms step_avg:34.78ms
step:183/1920 train_time:6363ms step_avg:34.77ms
step:184/1920 train_time:6398ms step_avg:34.77ms
step:185/1920 train_time:6433ms step_avg:34.77ms
step:186/1920 train_time:6467ms step_avg:34.77ms
step:187/1920 train_time:6502ms step_avg:34.77ms
step:188/1920 train_time:6536ms step_avg:34.77ms
step:189/1920 train_time:6570ms step_avg:34.76ms
step:190/1920 train_time:6605ms step_avg:34.76ms
step:191/1920 train_time:6639ms step_avg:34.76ms
step:192/1920 train_time:6674ms step_avg:34.76ms
step:193/1920 train_time:6709ms step_avg:34.76ms
step:194/1920 train_time:6743ms step_avg:34.76ms
step:195/1920 train_time:6778ms step_avg:34.76ms
step:196/1920 train_time:6812ms step_avg:34.75ms
step:197/1920 train_time:6847ms step_avg:34.76ms
step:198/1920 train_time:6881ms step_avg:34.75ms
step:199/1920 train_time:6916ms step_avg:34.75ms
step:200/1920 train_time:6950ms step_avg:34.75ms
step:201/1920 train_time:6985ms step_avg:34.75ms
step:202/1920 train_time:7019ms step_avg:34.75ms
step:203/1920 train_time:7053ms step_avg:34.75ms
step:204/1920 train_time:7087ms step_avg:34.74ms
step:205/1920 train_time:7122ms step_avg:34.74ms
step:206/1920 train_time:7156ms step_avg:34.74ms
step:207/1920 train_time:7190ms step_avg:34.74ms
step:208/1920 train_time:7225ms step_avg:34.73ms
step:209/1920 train_time:7259ms step_avg:34.73ms
step:210/1920 train_time:7293ms step_avg:34.73ms
step:211/1920 train_time:7328ms step_avg:34.73ms
step:212/1920 train_time:7362ms step_avg:34.73ms
step:213/1920 train_time:7397ms step_avg:34.73ms
step:214/1920 train_time:7431ms step_avg:34.73ms
step:215/1920 train_time:7466ms step_avg:34.73ms
step:216/1920 train_time:7500ms step_avg:34.72ms
step:217/1920 train_time:7535ms step_avg:34.72ms
step:218/1920 train_time:7569ms step_avg:34.72ms
step:219/1920 train_time:7603ms step_avg:34.72ms
step:220/1920 train_time:7638ms step_avg:34.72ms
step:221/1920 train_time:7673ms step_avg:34.72ms
step:222/1920 train_time:7707ms step_avg:34.72ms
step:223/1920 train_time:7742ms step_avg:34.72ms
step:224/1920 train_time:7776ms step_avg:34.71ms
step:225/1920 train_time:7810ms step_avg:34.71ms
step:226/1920 train_time:7845ms step_avg:34.71ms
step:227/1920 train_time:7879ms step_avg:34.71ms
step:228/1920 train_time:7913ms step_avg:34.71ms
step:229/1920 train_time:7948ms step_avg:34.71ms
step:230/1920 train_time:7982ms step_avg:34.70ms
step:231/1920 train_time:8016ms step_avg:34.70ms
step:232/1920 train_time:8051ms step_avg:34.70ms
step:233/1920 train_time:8085ms step_avg:34.70ms
step:234/1920 train_time:8119ms step_avg:34.70ms
step:235/1920 train_time:8154ms step_avg:34.70ms
step:236/1920 train_time:8188ms step_avg:34.70ms
step:237/1920 train_time:8223ms step_avg:34.70ms
step:238/1920 train_time:8257ms step_avg:34.69ms
step:239/1920 train_time:8292ms step_avg:34.69ms
step:240/1920 train_time:8326ms step_avg:34.69ms
step:241/1920 train_time:8360ms step_avg:34.69ms
step:242/1920 train_time:8394ms step_avg:34.69ms
step:243/1920 train_time:8429ms step_avg:34.69ms
step:244/1920 train_time:8464ms step_avg:34.69ms
step:245/1920 train_time:8498ms step_avg:34.69ms
step:246/1920 train_time:8533ms step_avg:34.69ms
step:247/1920 train_time:8568ms step_avg:34.69ms
step:248/1920 train_time:8602ms step_avg:34.69ms
step:249/1920 train_time:8637ms step_avg:34.69ms
step:250/1920 train_time:8671ms step_avg:34.69ms
step:250/1920 val_loss:4.6300 train_time:8709ms step_avg:34.84ms
step:251/1920 train_time:8727ms step_avg:34.77ms
step:252/1920 train_time:8744ms step_avg:34.70ms
step:253/1920 train_time:8779ms step_avg:34.70ms
step:254/1920 train_time:8813ms step_avg:34.70ms
step:255/1920 train_time:8850ms step_avg:34.70ms
step:256/1920 train_time:8885ms step_avg:34.71ms
step:257/1920 train_time:8921ms step_avg:34.71ms
step:258/1920 train_time:8956ms step_avg:34.71ms
step:259/1920 train_time:8991ms step_avg:34.71ms
step:260/1920 train_time:9025ms step_avg:34.71ms
step:261/1920 train_time:9059ms step_avg:34.71ms
step:262/1920 train_time:9094ms step_avg:34.71ms
step:263/1920 train_time:9128ms step_avg:34.71ms
step:264/1920 train_time:9162ms step_avg:34.71ms
step:265/1920 train_time:9196ms step_avg:34.70ms
step:266/1920 train_time:9231ms step_avg:34.70ms
step:267/1920 train_time:9265ms step_avg:34.70ms
step:268/1920 train_time:9299ms step_avg:34.70ms
step:269/1920 train_time:9333ms step_avg:34.69ms
step:270/1920 train_time:9367ms step_avg:34.69ms
step:271/1920 train_time:9401ms step_avg:34.69ms
step:272/1920 train_time:9436ms step_avg:34.69ms
step:273/1920 train_time:9470ms step_avg:34.69ms
step:274/1920 train_time:9504ms step_avg:34.69ms
step:275/1920 train_time:9538ms step_avg:34.68ms
step:276/1920 train_time:9572ms step_avg:34.68ms
step:277/1920 train_time:9606ms step_avg:34.68ms
step:278/1920 train_time:9640ms step_avg:34.68ms
step:279/1920 train_time:9674ms step_avg:34.67ms
step:280/1920 train_time:9708ms step_avg:34.67ms
step:281/1920 train_time:9743ms step_avg:34.67ms
step:282/1920 train_time:9777ms step_avg:34.67ms
step:283/1920 train_time:9812ms step_avg:34.67ms
step:284/1920 train_time:9846ms step_avg:34.67ms
step:285/1920 train_time:9881ms step_avg:34.67ms
step:286/1920 train_time:9915ms step_avg:34.67ms
step:287/1920 train_time:9950ms step_avg:34.67ms
step:288/1920 train_time:9985ms step_avg:34.67ms
step:289/1920 train_time:10020ms step_avg:34.67ms
step:290/1920 train_time:10054ms step_avg:34.67ms
step:291/1920 train_time:10088ms step_avg:34.67ms
step:292/1920 train_time:10122ms step_avg:34.67ms
step:293/1920 train_time:10157ms step_avg:34.67ms
step:294/1920 train_time:10192ms step_avg:34.67ms
step:295/1920 train_time:10226ms step_avg:34.66ms
step:296/1920 train_time:10260ms step_avg:34.66ms
step:297/1920 train_time:10294ms step_avg:34.66ms
step:298/1920 train_time:10328ms step_avg:34.66ms
step:299/1920 train_time:10362ms step_avg:34.66ms
step:300/1920 train_time:10397ms step_avg:34.66ms
step:301/1920 train_time:10431ms step_avg:34.65ms
step:302/1920 train_time:10465ms step_avg:34.65ms
step:303/1920 train_time:10499ms step_avg:34.65ms
step:304/1920 train_time:10534ms step_avg:34.65ms
step:305/1920 train_time:10568ms step_avg:34.65ms
step:306/1920 train_time:10602ms step_avg:34.65ms
step:307/1920 train_time:10635ms step_avg:34.64ms
step:308/1920 train_time:10670ms step_avg:34.64ms
step:309/1920 train_time:10704ms step_avg:34.64ms
step:310/1920 train_time:10738ms step_avg:34.64ms
step:311/1920 train_time:10772ms step_avg:34.64ms
step:312/1920 train_time:10806ms step_avg:34.64ms
step:313/1920 train_time:10841ms step_avg:34.63ms
step:314/1920 train_time:10875ms step_avg:34.63ms
step:315/1920 train_time:10909ms step_avg:34.63ms
step:316/1920 train_time:10944ms step_avg:34.63ms
step:317/1920 train_time:10978ms step_avg:34.63ms
step:318/1920 train_time:11013ms step_avg:34.63ms
step:319/1920 train_time:11047ms step_avg:34.63ms
step:320/1920 train_time:11082ms step_avg:34.63ms
step:321/1920 train_time:11116ms step_avg:34.63ms
step:322/1920 train_time:11150ms step_avg:34.63ms
step:323/1920 train_time:11184ms step_avg:34.63ms
step:324/1920 train_time:11218ms step_avg:34.62ms
step:325/1920 train_time:11253ms step_avg:34.62ms
step:326/1920 train_time:11287ms step_avg:34.62ms
step:327/1920 train_time:11321ms step_avg:34.62ms
step:328/1920 train_time:11356ms step_avg:34.62ms
step:329/1920 train_time:11390ms step_avg:34.62ms
step:330/1920 train_time:11425ms step_avg:34.62ms
step:331/1920 train_time:11459ms step_avg:34.62ms
step:332/1920 train_time:11493ms step_avg:34.62ms
step:333/1920 train_time:11528ms step_avg:34.62ms
step:334/1920 train_time:11562ms step_avg:34.62ms
step:335/1920 train_time:11596ms step_avg:34.62ms
step:336/1920 train_time:11630ms step_avg:34.61ms
step:337/1920 train_time:11665ms step_avg:34.61ms
step:338/1920 train_time:11699ms step_avg:34.61ms
step:339/1920 train_time:11733ms step_avg:34.61ms
step:340/1920 train_time:11767ms step_avg:34.61ms
step:341/1920 train_time:11802ms step_avg:34.61ms
step:342/1920 train_time:11836ms step_avg:34.61ms
step:343/1920 train_time:11870ms step_avg:34.61ms
step:344/1920 train_time:11905ms step_avg:34.61ms
step:345/1920 train_time:11939ms step_avg:34.60ms
step:346/1920 train_time:11973ms step_avg:34.60ms
step:347/1920 train_time:12007ms step_avg:34.60ms
step:348/1920 train_time:12042ms step_avg:34.60ms
step:349/1920 train_time:12076ms step_avg:34.60ms
step:350/1920 train_time:12110ms step_avg:34.60ms
step:351/1920 train_time:12144ms step_avg:34.60ms
step:352/1920 train_time:12179ms step_avg:34.60ms
step:353/1920 train_time:12213ms step_avg:34.60ms
step:354/1920 train_time:12247ms step_avg:34.60ms
step:355/1920 train_time:12282ms step_avg:34.60ms
step:356/1920 train_time:12316ms step_avg:34.60ms
step:357/1920 train_time:12350ms step_avg:34.60ms
step:358/1920 train_time:12385ms step_avg:34.59ms
step:359/1920 train_time:12419ms step_avg:34.59ms
step:360/1920 train_time:12453ms step_avg:34.59ms
step:361/1920 train_time:12488ms step_avg:34.59ms
step:362/1920 train_time:12522ms step_avg:34.59ms
step:363/1920 train_time:12556ms step_avg:34.59ms
step:364/1920 train_time:12591ms step_avg:34.59ms
step:365/1920 train_time:12625ms step_avg:34.59ms
step:366/1920 train_time:12659ms step_avg:34.59ms
step:367/1920 train_time:12693ms step_avg:34.59ms
step:368/1920 train_time:12727ms step_avg:34.58ms
step:369/1920 train_time:12761ms step_avg:34.58ms
step:370/1920 train_time:12796ms step_avg:34.58ms
step:371/1920 train_time:12830ms step_avg:34.58ms
step:372/1920 train_time:12864ms step_avg:34.58ms
step:373/1920 train_time:12899ms step_avg:34.58ms
step:374/1920 train_time:12933ms step_avg:34.58ms
step:375/1920 train_time:12967ms step_avg:34.58ms
step:376/1920 train_time:13001ms step_avg:34.58ms
step:377/1920 train_time:13036ms step_avg:34.58ms
step:378/1920 train_time:13070ms step_avg:34.58ms
step:379/1920 train_time:13104ms step_avg:34.58ms
step:380/1920 train_time:13138ms step_avg:34.57ms
step:381/1920 train_time:13173ms step_avg:34.57ms
step:382/1920 train_time:13207ms step_avg:34.57ms
step:383/1920 train_time:13241ms step_avg:34.57ms
step:384/1920 train_time:13275ms step_avg:34.57ms
step:385/1920 train_time:13310ms step_avg:34.57ms
step:386/1920 train_time:13344ms step_avg:34.57ms
step:387/1920 train_time:13378ms step_avg:34.57ms
step:388/1920 train_time:13412ms step_avg:34.57ms
step:389/1920 train_time:13447ms step_avg:34.57ms
step:390/1920 train_time:13481ms step_avg:34.57ms
step:391/1920 train_time:13516ms step_avg:34.57ms
step:392/1920 train_time:13550ms step_avg:34.57ms
step:393/1920 train_time:13584ms step_avg:34.56ms
step:394/1920 train_time:13618ms step_avg:34.56ms
step:395/1920 train_time:13652ms step_avg:34.56ms
step:396/1920 train_time:13686ms step_avg:34.56ms
step:397/1920 train_time:13721ms step_avg:34.56ms
step:398/1920 train_time:13755ms step_avg:34.56ms
step:399/1920 train_time:13790ms step_avg:34.56ms
step:400/1920 train_time:13824ms step_avg:34.56ms
step:401/1920 train_time:13858ms step_avg:34.56ms
step:402/1920 train_time:13893ms step_avg:34.56ms
step:403/1920 train_time:13927ms step_avg:34.56ms
step:404/1920 train_time:13961ms step_avg:34.56ms
step:405/1920 train_time:13996ms step_avg:34.56ms
step:406/1920 train_time:14030ms step_avg:34.56ms
step:407/1920 train_time:14064ms step_avg:34.56ms
step:408/1920 train_time:14099ms step_avg:34.56ms
step:409/1920 train_time:14133ms step_avg:34.55ms
step:410/1920 train_time:14167ms step_avg:34.55ms
step:411/1920 train_time:14202ms step_avg:34.55ms
step:412/1920 train_time:14236ms step_avg:34.55ms
step:413/1920 train_time:14270ms step_avg:34.55ms
step:414/1920 train_time:14305ms step_avg:34.55ms
step:415/1920 train_time:14339ms step_avg:34.55ms
step:416/1920 train_time:14374ms step_avg:34.55ms
step:417/1920 train_time:14408ms step_avg:34.55ms
step:418/1920 train_time:14442ms step_avg:34.55ms
step:419/1920 train_time:14476ms step_avg:34.55ms
step:420/1920 train_time:14510ms step_avg:34.55ms
step:421/1920 train_time:14544ms step_avg:34.55ms
step:422/1920 train_time:14578ms step_avg:34.55ms
step:423/1920 train_time:14613ms step_avg:34.55ms
step:424/1920 train_time:14647ms step_avg:34.54ms
step:425/1920 train_time:14681ms step_avg:34.54ms
step:426/1920 train_time:14716ms step_avg:34.54ms
step:427/1920 train_time:14750ms step_avg:34.54ms
step:428/1920 train_time:14784ms step_avg:34.54ms
step:429/1920 train_time:14818ms step_avg:34.54ms
step:430/1920 train_time:14853ms step_avg:34.54ms
step:431/1920 train_time:14887ms step_avg:34.54ms
step:432/1920 train_time:14921ms step_avg:34.54ms
step:433/1920 train_time:14956ms step_avg:34.54ms
step:434/1920 train_time:14990ms step_avg:34.54ms
step:435/1920 train_time:15024ms step_avg:34.54ms
step:436/1920 train_time:15059ms step_avg:34.54ms
step:437/1920 train_time:15093ms step_avg:34.54ms
step:438/1920 train_time:15127ms step_avg:34.54ms
step:439/1920 train_time:15162ms step_avg:34.54ms
step:440/1920 train_time:15196ms step_avg:34.54ms
step:441/1920 train_time:15231ms step_avg:34.54ms
step:442/1920 train_time:15265ms step_avg:34.54ms
step:443/1920 train_time:15299ms step_avg:34.54ms
step:444/1920 train_time:15333ms step_avg:34.53ms
step:445/1920 train_time:15368ms step_avg:34.53ms
step:446/1920 train_time:15402ms step_avg:34.53ms
step:447/1920 train_time:15437ms step_avg:34.53ms
step:448/1920 train_time:15471ms step_avg:34.53ms
step:449/1920 train_time:15505ms step_avg:34.53ms
step:450/1920 train_time:15539ms step_avg:34.53ms
step:451/1920 train_time:15573ms step_avg:34.53ms
step:452/1920 train_time:15607ms step_avg:34.53ms
step:453/1920 train_time:15642ms step_avg:34.53ms
step:454/1920 train_time:15676ms step_avg:34.53ms
step:455/1920 train_time:15710ms step_avg:34.53ms
step:456/1920 train_time:15744ms step_avg:34.53ms
step:457/1920 train_time:15779ms step_avg:34.53ms
step:458/1920 train_time:15813ms step_avg:34.53ms
step:459/1920 train_time:15848ms step_avg:34.53ms
step:460/1920 train_time:15882ms step_avg:34.53ms
step:461/1920 train_time:15917ms step_avg:34.53ms
step:462/1920 train_time:15951ms step_avg:34.53ms
step:463/1920 train_time:15986ms step_avg:34.53ms
step:464/1920 train_time:16020ms step_avg:34.53ms
step:465/1920 train_time:16055ms step_avg:34.53ms
step:466/1920 train_time:16089ms step_avg:34.52ms
step:467/1920 train_time:16124ms step_avg:34.53ms
step:468/1920 train_time:16158ms step_avg:34.52ms
step:469/1920 train_time:16192ms step_avg:34.52ms
step:470/1920 train_time:16226ms step_avg:34.52ms
step:471/1920 train_time:16261ms step_avg:34.52ms
step:472/1920 train_time:16295ms step_avg:34.52ms
step:473/1920 train_time:16329ms step_avg:34.52ms
step:474/1920 train_time:16364ms step_avg:34.52ms
step:475/1920 train_time:16398ms step_avg:34.52ms
step:476/1920 train_time:16432ms step_avg:34.52ms
step:477/1920 train_time:16467ms step_avg:34.52ms
step:478/1920 train_time:16501ms step_avg:34.52ms
step:479/1920 train_time:16535ms step_avg:34.52ms
step:480/1920 train_time:16569ms step_avg:34.52ms
step:481/1920 train_time:16603ms step_avg:34.52ms
step:482/1920 train_time:16637ms step_avg:34.52ms
step:483/1920 train_time:16671ms step_avg:34.52ms
step:484/1920 train_time:16705ms step_avg:34.52ms
step:485/1920 train_time:16740ms step_avg:34.52ms
step:486/1920 train_time:16774ms step_avg:34.51ms
step:487/1920 train_time:16809ms step_avg:34.52ms
step:488/1920 train_time:16843ms step_avg:34.52ms
step:489/1920 train_time:16878ms step_avg:34.51ms
step:490/1920 train_time:16912ms step_avg:34.51ms
step:491/1920 train_time:16946ms step_avg:34.51ms
step:492/1920 train_time:16981ms step_avg:34.51ms
step:493/1920 train_time:17015ms step_avg:34.51ms
step:494/1920 train_time:17049ms step_avg:34.51ms
step:495/1920 train_time:17084ms step_avg:34.51ms
step:496/1920 train_time:17118ms step_avg:34.51ms
step:497/1920 train_time:17152ms step_avg:34.51ms
step:498/1920 train_time:17187ms step_avg:34.51ms
step:499/1920 train_time:17221ms step_avg:34.51ms
step:500/1920 train_time:17255ms step_avg:34.51ms
step:500/1920 val_loss:4.2912 train_time:17293ms step_avg:34.59ms
step:501/1920 train_time:17310ms step_avg:34.55ms
step:502/1920 train_time:17328ms step_avg:34.52ms
step:503/1920 train_time:17361ms step_avg:34.51ms
step:504/1920 train_time:17396ms step_avg:34.52ms
step:505/1920 train_time:17432ms step_avg:34.52ms
step:506/1920 train_time:17467ms step_avg:34.52ms
step:507/1920 train_time:17502ms step_avg:34.52ms
step:508/1920 train_time:17537ms step_avg:34.52ms
step:509/1920 train_time:17571ms step_avg:34.52ms
step:510/1920 train_time:17606ms step_avg:34.52ms
step:511/1920 train_time:17640ms step_avg:34.52ms
step:512/1920 train_time:17674ms step_avg:34.52ms
step:513/1920 train_time:17708ms step_avg:34.52ms
step:514/1920 train_time:17742ms step_avg:34.52ms
step:515/1920 train_time:17777ms step_avg:34.52ms
step:516/1920 train_time:17811ms step_avg:34.52ms
step:517/1920 train_time:17845ms step_avg:34.52ms
step:518/1920 train_time:17879ms step_avg:34.52ms
step:519/1920 train_time:17914ms step_avg:34.52ms
step:520/1920 train_time:17948ms step_avg:34.51ms
step:521/1920 train_time:17982ms step_avg:34.51ms
step:522/1920 train_time:18016ms step_avg:34.51ms
step:523/1920 train_time:18050ms step_avg:34.51ms
step:524/1920 train_time:18084ms step_avg:34.51ms
step:525/1920 train_time:18118ms step_avg:34.51ms
step:526/1920 train_time:18153ms step_avg:34.51ms
step:527/1920 train_time:18187ms step_avg:34.51ms
step:528/1920 train_time:18221ms step_avg:34.51ms
step:529/1920 train_time:18255ms step_avg:34.51ms
step:530/1920 train_time:18289ms step_avg:34.51ms
step:531/1920 train_time:18324ms step_avg:34.51ms
step:532/1920 train_time:18358ms step_avg:34.51ms
step:533/1920 train_time:18393ms step_avg:34.51ms
step:534/1920 train_time:18427ms step_avg:34.51ms
step:535/1920 train_time:18462ms step_avg:34.51ms
step:536/1920 train_time:18496ms step_avg:34.51ms
step:537/1920 train_time:18531ms step_avg:34.51ms
step:538/1920 train_time:18565ms step_avg:34.51ms
step:539/1920 train_time:18600ms step_avg:34.51ms
step:540/1920 train_time:18634ms step_avg:34.51ms
step:541/1920 train_time:18668ms step_avg:34.51ms
step:542/1920 train_time:18703ms step_avg:34.51ms
step:543/1920 train_time:18737ms step_avg:34.51ms
step:544/1920 train_time:18771ms step_avg:34.51ms
step:545/1920 train_time:18805ms step_avg:34.51ms
step:546/1920 train_time:18840ms step_avg:34.50ms
step:547/1920 train_time:18874ms step_avg:34.50ms
step:548/1920 train_time:18909ms step_avg:34.50ms
step:549/1920 train_time:18943ms step_avg:34.50ms
step:550/1920 train_time:18977ms step_avg:34.50ms
step:551/1920 train_time:19011ms step_avg:34.50ms
step:552/1920 train_time:19045ms step_avg:34.50ms
step:553/1920 train_time:19080ms step_avg:34.50ms
step:554/1920 train_time:19114ms step_avg:34.50ms
step:555/1920 train_time:19148ms step_avg:34.50ms
step:556/1920 train_time:19182ms step_avg:34.50ms
step:557/1920 train_time:19217ms step_avg:34.50ms
step:558/1920 train_time:19251ms step_avg:34.50ms
step:559/1920 train_time:19285ms step_avg:34.50ms
step:560/1920 train_time:19320ms step_avg:34.50ms
step:561/1920 train_time:19355ms step_avg:34.50ms
step:562/1920 train_time:19389ms step_avg:34.50ms
step:563/1920 train_time:19424ms step_avg:34.50ms
step:564/1920 train_time:19458ms step_avg:34.50ms
step:565/1920 train_time:19492ms step_avg:34.50ms
step:566/1920 train_time:19526ms step_avg:34.50ms
step:567/1920 train_time:19561ms step_avg:34.50ms
step:568/1920 train_time:19595ms step_avg:34.50ms
step:569/1920 train_time:19630ms step_avg:34.50ms
step:570/1920 train_time:19664ms step_avg:34.50ms
step:571/1920 train_time:19699ms step_avg:34.50ms
step:572/1920 train_time:19733ms step_avg:34.50ms
step:573/1920 train_time:19767ms step_avg:34.50ms
step:574/1920 train_time:19802ms step_avg:34.50ms
step:575/1920 train_time:19836ms step_avg:34.50ms
step:576/1920 train_time:19870ms step_avg:34.50ms
step:577/1920 train_time:19904ms step_avg:34.50ms
step:578/1920 train_time:19938ms step_avg:34.50ms
step:579/1920 train_time:19973ms step_avg:34.50ms
step:580/1920 train_time:20007ms step_avg:34.50ms
step:581/1920 train_time:20041ms step_avg:34.49ms
step:582/1920 train_time:20075ms step_avg:34.49ms
step:583/1920 train_time:20110ms step_avg:34.49ms
step:584/1920 train_time:20144ms step_avg:34.49ms
step:585/1920 train_time:20178ms step_avg:34.49ms
step:586/1920 train_time:20213ms step_avg:34.49ms
step:587/1920 train_time:20247ms step_avg:34.49ms
step:588/1920 train_time:20281ms step_avg:34.49ms
step:589/1920 train_time:20316ms step_avg:34.49ms
step:590/1920 train_time:20350ms step_avg:34.49ms
step:591/1920 train_time:20385ms step_avg:34.49ms
step:592/1920 train_time:20419ms step_avg:34.49ms
step:593/1920 train_time:20454ms step_avg:34.49ms
step:594/1920 train_time:20488ms step_avg:34.49ms
step:595/1920 train_time:20522ms step_avg:34.49ms
step:596/1920 train_time:20557ms step_avg:34.49ms
step:597/1920 train_time:20591ms step_avg:34.49ms
step:598/1920 train_time:20626ms step_avg:34.49ms
step:599/1920 train_time:20660ms step_avg:34.49ms
step:600/1920 train_time:20694ms step_avg:34.49ms
step:601/1920 train_time:20728ms step_avg:34.49ms
step:602/1920 train_time:20762ms step_avg:34.49ms
step:603/1920 train_time:20797ms step_avg:34.49ms
step:604/1920 train_time:20831ms step_avg:34.49ms
step:605/1920 train_time:20866ms step_avg:34.49ms
step:606/1920 train_time:20900ms step_avg:34.49ms
step:607/1920 train_time:20935ms step_avg:34.49ms
step:608/1920 train_time:20969ms step_avg:34.49ms
step:609/1920 train_time:21003ms step_avg:34.49ms
step:610/1920 train_time:21038ms step_avg:34.49ms
step:611/1920 train_time:21072ms step_avg:34.49ms
step:612/1920 train_time:21106ms step_avg:34.49ms
step:613/1920 train_time:21141ms step_avg:34.49ms
step:614/1920 train_time:21175ms step_avg:34.49ms
step:615/1920 train_time:21209ms step_avg:34.49ms
step:616/1920 train_time:21243ms step_avg:34.49ms
step:617/1920 train_time:21278ms step_avg:34.49ms
step:618/1920 train_time:21312ms step_avg:34.49ms
step:619/1920 train_time:21346ms step_avg:34.49ms
step:620/1920 train_time:21380ms step_avg:34.48ms
step:621/1920 train_time:21415ms step_avg:34.49ms
step:622/1920 train_time:21450ms step_avg:34.48ms
step:623/1920 train_time:21484ms step_avg:34.49ms
step:624/1920 train_time:21519ms step_avg:34.48ms
step:625/1920 train_time:21553ms step_avg:34.49ms
step:626/1920 train_time:21588ms step_avg:34.49ms
step:627/1920 train_time:21622ms step_avg:34.49ms
step:628/1920 train_time:21657ms step_avg:34.49ms
step:629/1920 train_time:21719ms step_avg:34.53ms
step:630/1920 train_time:21780ms step_avg:34.57ms
step:631/1920 train_time:21843ms step_avg:34.62ms
step:632/1920 train_time:21905ms step_avg:34.66ms
step:633/1920 train_time:21968ms step_avg:34.71ms
step:634/1920 train_time:22030ms step_avg:34.75ms
step:635/1920 train_time:22093ms step_avg:34.79ms
step:636/1920 train_time:22155ms step_avg:34.83ms
step:637/1920 train_time:22218ms step_avg:34.88ms
step:638/1920 train_time:22280ms step_avg:34.92ms
step:639/1920 train_time:22342ms step_avg:34.96ms
step:640/1920 train_time:22404ms step_avg:35.01ms
step:641/1920 train_time:22468ms step_avg:35.05ms
step:642/1920 train_time:22530ms step_avg:35.09ms
step:643/1920 train_time:22593ms step_avg:35.14ms
step:644/1920 train_time:22654ms step_avg:35.18ms
step:645/1920 train_time:22717ms step_avg:35.22ms
step:646/1920 train_time:22778ms step_avg:35.26ms
step:647/1920 train_time:22841ms step_avg:35.30ms
step:648/1920 train_time:22903ms step_avg:35.34ms
step:649/1920 train_time:22966ms step_avg:35.39ms
step:650/1920 train_time:23028ms step_avg:35.43ms
step:651/1920 train_time:23091ms step_avg:35.47ms
step:652/1920 train_time:23152ms step_avg:35.51ms
step:653/1920 train_time:23215ms step_avg:35.55ms
step:654/1920 train_time:23277ms step_avg:35.59ms
step:655/1920 train_time:23339ms step_avg:35.63ms
step:656/1920 train_time:23401ms step_avg:35.67ms
step:657/1920 train_time:23464ms step_avg:35.71ms
step:658/1920 train_time:23526ms step_avg:35.75ms
step:659/1920 train_time:23589ms step_avg:35.80ms
step:660/1920 train_time:23651ms step_avg:35.83ms
step:661/1920 train_time:23713ms step_avg:35.88ms
step:662/1920 train_time:23776ms step_avg:35.91ms
step:663/1920 train_time:23838ms step_avg:35.96ms
step:664/1920 train_time:23900ms step_avg:35.99ms
step:665/1920 train_time:23963ms step_avg:36.04ms
step:666/1920 train_time:24026ms step_avg:36.07ms
step:667/1920 train_time:24089ms step_avg:36.12ms
step:668/1920 train_time:24151ms step_avg:36.15ms
step:669/1920 train_time:24214ms step_avg:36.19ms
step:670/1920 train_time:24276ms step_avg:36.23ms
step:671/1920 train_time:24339ms step_avg:36.27ms
step:672/1920 train_time:24401ms step_avg:36.31ms
step:673/1920 train_time:24463ms step_avg:36.35ms
step:674/1920 train_time:24525ms step_avg:36.39ms
step:675/1920 train_time:24588ms step_avg:36.43ms
step:676/1920 train_time:24650ms step_avg:36.46ms
step:677/1920 train_time:24712ms step_avg:36.50ms
step:678/1920 train_time:24774ms step_avg:36.54ms
step:679/1920 train_time:24837ms step_avg:36.58ms
step:680/1920 train_time:24898ms step_avg:36.62ms
step:681/1920 train_time:24961ms step_avg:36.65ms
step:682/1920 train_time:25022ms step_avg:36.69ms
step:683/1920 train_time:25085ms step_avg:36.73ms
step:684/1920 train_time:25148ms step_avg:36.77ms
step:685/1920 train_time:25211ms step_avg:36.80ms
step:686/1920 train_time:25272ms step_avg:36.84ms
step:687/1920 train_time:25335ms step_avg:36.88ms
step:688/1920 train_time:25397ms step_avg:36.91ms
step:689/1920 train_time:25459ms step_avg:36.95ms
step:690/1920 train_time:25521ms step_avg:36.99ms
step:691/1920 train_time:25584ms step_avg:37.02ms
step:692/1920 train_time:25645ms step_avg:37.06ms
step:693/1920 train_time:25709ms step_avg:37.10ms
step:694/1920 train_time:25771ms step_avg:37.13ms
step:695/1920 train_time:25834ms step_avg:37.17ms
step:696/1920 train_time:25895ms step_avg:37.21ms
step:697/1920 train_time:25958ms step_avg:37.24ms
step:698/1920 train_time:26020ms step_avg:37.28ms
step:699/1920 train_time:26083ms step_avg:37.31ms
step:700/1920 train_time:26144ms step_avg:37.35ms
step:701/1920 train_time:26208ms step_avg:37.39ms
step:702/1920 train_time:26270ms step_avg:37.42ms
step:703/1920 train_time:26333ms step_avg:37.46ms
step:704/1920 train_time:26394ms step_avg:37.49ms
step:705/1920 train_time:26458ms step_avg:37.53ms
step:706/1920 train_time:26520ms step_avg:37.56ms
step:707/1920 train_time:26582ms step_avg:37.60ms
step:708/1920 train_time:26644ms step_avg:37.63ms
step:709/1920 train_time:26707ms step_avg:37.67ms
step:710/1920 train_time:26769ms step_avg:37.70ms
step:711/1920 train_time:26831ms step_avg:37.74ms
step:712/1920 train_time:26893ms step_avg:37.77ms
step:713/1920 train_time:26956ms step_avg:37.81ms
step:714/1920 train_time:27018ms step_avg:37.84ms
step:715/1920 train_time:27080ms step_avg:37.87ms
step:716/1920 train_time:27142ms step_avg:37.91ms
step:717/1920 train_time:27204ms step_avg:37.94ms
step:718/1920 train_time:27267ms step_avg:37.98ms
step:719/1920 train_time:27329ms step_avg:38.01ms
step:720/1920 train_time:27391ms step_avg:38.04ms
step:721/1920 train_time:27453ms step_avg:38.08ms
step:722/1920 train_time:27515ms step_avg:38.11ms
step:723/1920 train_time:27577ms step_avg:38.14ms
step:724/1920 train_time:27640ms step_avg:38.18ms
step:725/1920 train_time:27702ms step_avg:38.21ms
step:726/1920 train_time:27764ms step_avg:38.24ms
step:727/1920 train_time:27827ms step_avg:38.28ms
step:728/1920 train_time:27890ms step_avg:38.31ms
step:729/1920 train_time:27952ms step_avg:38.34ms
step:730/1920 train_time:28014ms step_avg:38.38ms
step:731/1920 train_time:28076ms step_avg:38.41ms
step:732/1920 train_time:28138ms step_avg:38.44ms
step:733/1920 train_time:28201ms step_avg:38.47ms
step:734/1920 train_time:28263ms step_avg:38.51ms
step:735/1920 train_time:28326ms step_avg:38.54ms
step:736/1920 train_time:28389ms step_avg:38.57ms
step:737/1920 train_time:28452ms step_avg:38.60ms
step:738/1920 train_time:28513ms step_avg:38.64ms
step:739/1920 train_time:28576ms step_avg:38.67ms
step:740/1920 train_time:28638ms step_avg:38.70ms
step:741/1920 train_time:28700ms step_avg:38.73ms
step:742/1920 train_time:28762ms step_avg:38.76ms
step:743/1920 train_time:28825ms step_avg:38.80ms
step:744/1920 train_time:28887ms step_avg:38.83ms
step:745/1920 train_time:28951ms step_avg:38.86ms
step:746/1920 train_time:29013ms step_avg:38.89ms
step:747/1920 train_time:29075ms step_avg:38.92ms
step:748/1920 train_time:29138ms step_avg:38.95ms
step:749/1920 train_time:29201ms step_avg:38.99ms
step:750/1920 train_time:29262ms step_avg:39.02ms
step:750/1920 val_loss:4.0487 train_time:29328ms step_avg:39.10ms
step:751/1920 train_time:29345ms step_avg:39.07ms
step:752/1920 train_time:29389ms step_avg:39.08ms
step:753/1920 train_time:29455ms step_avg:39.12ms
step:754/1920 train_time:29521ms step_avg:39.15ms
step:755/1920 train_time:29583ms step_avg:39.18ms
step:756/1920 train_time:29644ms step_avg:39.21ms
step:757/1920 train_time:29706ms step_avg:39.24ms
step:758/1920 train_time:29767ms step_avg:39.27ms
step:759/1920 train_time:29828ms step_avg:39.30ms
step:760/1920 train_time:29889ms step_avg:39.33ms
step:761/1920 train_time:29951ms step_avg:39.36ms
step:762/1920 train_time:30012ms step_avg:39.39ms
step:763/1920 train_time:30074ms step_avg:39.42ms
step:764/1920 train_time:30135ms step_avg:39.44ms
step:765/1920 train_time:30197ms step_avg:39.47ms
step:766/1920 train_time:30260ms step_avg:39.50ms
step:767/1920 train_time:30325ms step_avg:39.54ms
step:768/1920 train_time:30388ms step_avg:39.57ms
step:769/1920 train_time:30452ms step_avg:39.60ms
step:770/1920 train_time:30515ms step_avg:39.63ms
step:771/1920 train_time:30579ms step_avg:39.66ms
step:772/1920 train_time:30642ms step_avg:39.69ms
step:773/1920 train_time:30705ms step_avg:39.72ms
step:774/1920 train_time:30766ms step_avg:39.75ms
step:775/1920 train_time:30828ms step_avg:39.78ms
step:776/1920 train_time:30889ms step_avg:39.81ms
step:777/1920 train_time:30951ms step_avg:39.83ms
step:778/1920 train_time:31013ms step_avg:39.86ms
step:779/1920 train_time:31075ms step_avg:39.89ms
step:780/1920 train_time:31136ms step_avg:39.92ms
step:781/1920 train_time:31199ms step_avg:39.95ms
step:782/1920 train_time:31261ms step_avg:39.98ms
step:783/1920 train_time:31325ms step_avg:40.01ms
step:784/1920 train_time:31388ms step_avg:40.04ms
step:785/1920 train_time:31450ms step_avg:40.06ms
step:786/1920 train_time:31513ms step_avg:40.09ms
step:787/1920 train_time:31576ms step_avg:40.12ms
step:788/1920 train_time:31639ms step_avg:40.15ms
step:789/1920 train_time:31702ms step_avg:40.18ms
step:790/1920 train_time:31764ms step_avg:40.21ms
step:791/1920 train_time:31826ms step_avg:40.24ms
step:792/1920 train_time:31888ms step_avg:40.26ms
step:793/1920 train_time:31950ms step_avg:40.29ms
step:794/1920 train_time:32011ms step_avg:40.32ms
step:795/1920 train_time:32073ms step_avg:40.34ms
step:796/1920 train_time:32134ms step_avg:40.37ms
step:797/1920 train_time:32197ms step_avg:40.40ms
step:798/1920 train_time:32259ms step_avg:40.42ms
step:799/1920 train_time:32322ms step_avg:40.45ms
step:800/1920 train_time:32384ms step_avg:40.48ms
step:801/1920 train_time:32448ms step_avg:40.51ms
step:802/1920 train_time:32510ms step_avg:40.54ms
step:803/1920 train_time:32573ms step_avg:40.56ms
step:804/1920 train_time:32636ms step_avg:40.59ms
step:805/1920 train_time:32700ms step_avg:40.62ms
step:806/1920 train_time:32762ms step_avg:40.65ms
step:807/1920 train_time:32824ms step_avg:40.67ms
step:808/1920 train_time:32885ms step_avg:40.70ms
step:809/1920 train_time:32947ms step_avg:40.73ms
step:810/1920 train_time:33008ms step_avg:40.75ms
step:811/1920 train_time:33070ms step_avg:40.78ms
step:812/1920 train_time:33132ms step_avg:40.80ms
step:813/1920 train_time:33195ms step_avg:40.83ms
step:814/1920 train_time:33256ms step_avg:40.86ms
step:815/1920 train_time:33319ms step_avg:40.88ms
step:816/1920 train_time:33381ms step_avg:40.91ms
step:817/1920 train_time:33444ms step_avg:40.94ms
step:818/1920 train_time:33506ms step_avg:40.96ms
step:819/1920 train_time:33570ms step_avg:40.99ms
step:820/1920 train_time:33631ms step_avg:41.01ms
step:821/1920 train_time:33695ms step_avg:41.04ms
step:822/1920 train_time:33757ms step_avg:41.07ms
step:823/1920 train_time:33820ms step_avg:41.09ms
step:824/1920 train_time:33882ms step_avg:41.12ms
step:825/1920 train_time:33945ms step_avg:41.15ms
step:826/1920 train_time:34006ms step_avg:41.17ms
step:827/1920 train_time:34068ms step_avg:41.20ms
step:828/1920 train_time:34130ms step_avg:41.22ms
step:829/1920 train_time:34192ms step_avg:41.24ms
step:830/1920 train_time:34253ms step_avg:41.27ms
step:831/1920 train_time:34316ms step_avg:41.30ms
step:832/1920 train_time:34379ms step_avg:41.32ms
step:833/1920 train_time:34442ms step_avg:41.35ms
step:834/1920 train_time:34504ms step_avg:41.37ms
step:835/1920 train_time:34566ms step_avg:41.40ms
step:836/1920 train_time:34628ms step_avg:41.42ms
step:837/1920 train_time:34691ms step_avg:41.45ms
step:838/1920 train_time:34753ms step_avg:41.47ms
step:839/1920 train_time:34817ms step_avg:41.50ms
step:840/1920 train_time:34880ms step_avg:41.52ms
step:841/1920 train_time:34943ms step_avg:41.55ms
step:842/1920 train_time:35004ms step_avg:41.57ms
step:843/1920 train_time:35066ms step_avg:41.60ms
step:844/1920 train_time:35128ms step_avg:41.62ms
step:845/1920 train_time:35190ms step_avg:41.65ms
step:846/1920 train_time:35252ms step_avg:41.67ms
step:847/1920 train_time:35315ms step_avg:41.69ms
step:848/1920 train_time:35377ms step_avg:41.72ms
step:849/1920 train_time:35440ms step_avg:41.74ms
step:850/1920 train_time:35502ms step_avg:41.77ms
step:851/1920 train_time:35565ms step_avg:41.79ms
step:852/1920 train_time:35628ms step_avg:41.82ms
step:853/1920 train_time:35690ms step_avg:41.84ms
step:854/1920 train_time:35752ms step_avg:41.86ms
step:855/1920 train_time:35816ms step_avg:41.89ms
step:856/1920 train_time:35878ms step_avg:41.91ms
step:857/1920 train_time:35942ms step_avg:41.94ms
step:858/1920 train_time:36003ms step_avg:41.96ms
step:859/1920 train_time:36066ms step_avg:41.99ms
step:860/1920 train_time:36127ms step_avg:42.01ms
step:861/1920 train_time:36190ms step_avg:42.03ms
step:862/1920 train_time:36251ms step_avg:42.05ms
step:863/1920 train_time:36314ms step_avg:42.08ms
step:864/1920 train_time:36376ms step_avg:42.10ms
step:865/1920 train_time:36439ms step_avg:42.13ms
step:866/1920 train_time:36502ms step_avg:42.15ms
step:867/1920 train_time:36564ms step_avg:42.17ms
step:868/1920 train_time:36626ms step_avg:42.20ms
step:869/1920 train_time:36689ms step_avg:42.22ms
step:870/1920 train_time:36751ms step_avg:42.24ms
step:871/1920 train_time:36814ms step_avg:42.27ms
step:872/1920 train_time:36876ms step_avg:42.29ms
step:873/1920 train_time:36939ms step_avg:42.31ms
step:874/1920 train_time:37001ms step_avg:42.34ms
step:875/1920 train_time:37064ms step_avg:42.36ms
step:876/1920 train_time:37125ms step_avg:42.38ms
step:877/1920 train_time:37188ms step_avg:42.40ms
step:878/1920 train_time:37249ms step_avg:42.43ms
step:879/1920 train_time:37312ms step_avg:42.45ms
step:880/1920 train_time:37374ms step_avg:42.47ms
step:881/1920 train_time:37437ms step_avg:42.49ms
step:882/1920 train_time:37499ms step_avg:42.52ms
step:883/1920 train_time:37562ms step_avg:42.54ms
step:884/1920 train_time:37624ms step_avg:42.56ms
step:885/1920 train_time:37687ms step_avg:42.58ms
step:886/1920 train_time:37749ms step_avg:42.61ms
step:887/1920 train_time:37812ms step_avg:42.63ms
step:888/1920 train_time:37874ms step_avg:42.65ms
step:889/1920 train_time:37936ms step_avg:42.67ms
step:890/1920 train_time:37999ms step_avg:42.70ms
step:891/1920 train_time:38062ms step_avg:42.72ms
step:892/1920 train_time:38124ms step_avg:42.74ms
step:893/1920 train_time:38186ms step_avg:42.76ms
step:894/1920 train_time:38247ms step_avg:42.78ms
step:895/1920 train_time:38310ms step_avg:42.80ms
step:896/1920 train_time:38372ms step_avg:42.83ms
step:897/1920 train_time:38435ms step_avg:42.85ms
step:898/1920 train_time:38497ms step_avg:42.87ms
step:899/1920 train_time:38561ms step_avg:42.89ms
step:900/1920 train_time:38622ms step_avg:42.91ms
step:901/1920 train_time:38685ms step_avg:42.94ms
step:902/1920 train_time:38747ms step_avg:42.96ms
step:903/1920 train_time:38810ms step_avg:42.98ms
step:904/1920 train_time:38872ms step_avg:43.00ms
step:905/1920 train_time:38935ms step_avg:43.02ms
step:906/1920 train_time:38997ms step_avg:43.04ms
step:907/1920 train_time:39060ms step_avg:43.07ms
step:908/1920 train_time:39122ms step_avg:43.09ms
step:909/1920 train_time:39185ms step_avg:43.11ms
step:910/1920 train_time:39247ms step_avg:43.13ms
step:911/1920 train_time:39309ms step_avg:43.15ms
step:912/1920 train_time:39371ms step_avg:43.17ms
step:913/1920 train_time:39434ms step_avg:43.19ms
step:914/1920 train_time:39495ms step_avg:43.21ms
step:915/1920 train_time:39559ms step_avg:43.23ms
step:916/1920 train_time:39621ms step_avg:43.25ms
step:917/1920 train_time:39683ms step_avg:43.28ms
step:918/1920 train_time:39745ms step_avg:43.30ms
step:919/1920 train_time:39809ms step_avg:43.32ms
step:920/1920 train_time:39870ms step_avg:43.34ms
step:921/1920 train_time:39933ms step_avg:43.36ms
step:922/1920 train_time:39995ms step_avg:43.38ms
step:923/1920 train_time:40058ms step_avg:43.40ms
step:924/1920 train_time:40120ms step_avg:43.42ms
step:925/1920 train_time:40183ms step_avg:43.44ms
step:926/1920 train_time:40244ms step_avg:43.46ms
step:927/1920 train_time:40307ms step_avg:43.48ms
step:928/1920 train_time:40369ms step_avg:43.50ms
step:929/1920 train_time:40432ms step_avg:43.52ms
step:930/1920 train_time:40493ms step_avg:43.54ms
step:931/1920 train_time:40556ms step_avg:43.56ms
step:932/1920 train_time:40618ms step_avg:43.58ms
step:933/1920 train_time:40681ms step_avg:43.60ms
step:934/1920 train_time:40743ms step_avg:43.62ms
step:935/1920 train_time:40806ms step_avg:43.64ms
step:936/1920 train_time:40868ms step_avg:43.66ms
step:937/1920 train_time:40930ms step_avg:43.68ms
step:938/1920 train_time:40991ms step_avg:43.70ms
step:939/1920 train_time:41055ms step_avg:43.72ms
step:940/1920 train_time:41118ms step_avg:43.74ms
step:941/1920 train_time:41181ms step_avg:43.76ms
step:942/1920 train_time:41242ms step_avg:43.78ms
step:943/1920 train_time:41305ms step_avg:43.80ms
step:944/1920 train_time:41367ms step_avg:43.82ms
step:945/1920 train_time:41429ms step_avg:43.84ms
step:946/1920 train_time:41491ms step_avg:43.86ms
step:947/1920 train_time:41554ms step_avg:43.88ms
step:948/1920 train_time:41616ms step_avg:43.90ms
step:949/1920 train_time:41680ms step_avg:43.92ms
step:950/1920 train_time:41742ms step_avg:43.94ms
step:951/1920 train_time:41804ms step_avg:43.96ms
step:952/1920 train_time:41866ms step_avg:43.98ms
step:953/1920 train_time:41929ms step_avg:44.00ms
step:954/1920 train_time:41991ms step_avg:44.02ms
step:955/1920 train_time:42054ms step_avg:44.04ms
step:956/1920 train_time:42116ms step_avg:44.05ms
step:957/1920 train_time:42179ms step_avg:44.07ms
step:958/1920 train_time:42241ms step_avg:44.09ms
step:959/1920 train_time:42304ms step_avg:44.11ms
step:960/1920 train_time:42367ms step_avg:44.13ms
step:961/1920 train_time:42429ms step_avg:44.15ms
step:962/1920 train_time:42491ms step_avg:44.17ms
step:963/1920 train_time:42553ms step_avg:44.19ms
step:964/1920 train_time:42616ms step_avg:44.21ms
step:965/1920 train_time:42680ms step_avg:44.23ms
step:966/1920 train_time:42741ms step_avg:44.25ms
step:967/1920 train_time:42804ms step_avg:44.26ms
step:968/1920 train_time:42866ms step_avg:44.28ms
step:969/1920 train_time:42929ms step_avg:44.30ms
step:970/1920 train_time:42991ms step_avg:44.32ms
step:971/1920 train_time:43054ms step_avg:44.34ms
step:972/1920 train_time:43116ms step_avg:44.36ms
step:973/1920 train_time:43179ms step_avg:44.38ms
step:974/1920 train_time:43241ms step_avg:44.40ms
step:975/1920 train_time:43304ms step_avg:44.41ms
step:976/1920 train_time:43366ms step_avg:44.43ms
step:977/1920 train_time:43429ms step_avg:44.45ms
step:978/1920 train_time:43490ms step_avg:44.47ms
step:979/1920 train_time:43553ms step_avg:44.49ms
step:980/1920 train_time:43615ms step_avg:44.51ms
step:981/1920 train_time:43679ms step_avg:44.52ms
step:982/1920 train_time:43740ms step_avg:44.54ms
step:983/1920 train_time:43803ms step_avg:44.56ms
step:984/1920 train_time:43865ms step_avg:44.58ms
step:985/1920 train_time:43928ms step_avg:44.60ms
step:986/1920 train_time:43989ms step_avg:44.61ms
step:987/1920 train_time:44052ms step_avg:44.63ms
step:988/1920 train_time:44114ms step_avg:44.65ms
step:989/1920 train_time:44178ms step_avg:44.67ms
step:990/1920 train_time:44240ms step_avg:44.69ms
step:991/1920 train_time:44303ms step_avg:44.71ms
step:992/1920 train_time:44365ms step_avg:44.72ms
step:993/1920 train_time:44428ms step_avg:44.74ms
step:994/1920 train_time:44489ms step_avg:44.76ms
step:995/1920 train_time:44552ms step_avg:44.78ms
step:996/1920 train_time:44613ms step_avg:44.79ms
step:997/1920 train_time:44677ms step_avg:44.81ms
step:998/1920 train_time:44739ms step_avg:44.83ms
step:999/1920 train_time:44802ms step_avg:44.85ms
step:1000/1920 train_time:44864ms step_avg:44.86ms
step:1000/1920 val_loss:3.7779 train_time:44929ms step_avg:44.93ms
step:1001/1920 train_time:44946ms step_avg:44.90ms
step:1002/1920 train_time:44993ms step_avg:44.90ms
step:1003/1920 train_time:45059ms step_avg:44.92ms
step:1004/1920 train_time:45122ms step_avg:44.94ms
step:1005/1920 train_time:45184ms step_avg:44.96ms
step:1006/1920 train_time:45245ms step_avg:44.98ms
step:1007/1920 train_time:45308ms step_avg:44.99ms
step:1008/1920 train_time:45369ms step_avg:45.01ms
step:1009/1920 train_time:45431ms step_avg:45.03ms
step:1010/1920 train_time:45492ms step_avg:45.04ms
step:1011/1920 train_time:45554ms step_avg:45.06ms
step:1012/1920 train_time:45616ms step_avg:45.07ms
step:1013/1920 train_time:45678ms step_avg:45.09ms
step:1014/1920 train_time:45739ms step_avg:45.11ms
step:1015/1920 train_time:45801ms step_avg:45.12ms
step:1016/1920 train_time:45864ms step_avg:45.14ms
step:1017/1920 train_time:45928ms step_avg:45.16ms
step:1018/1920 train_time:45992ms step_avg:45.18ms
step:1019/1920 train_time:46057ms step_avg:45.20ms
step:1020/1920 train_time:46119ms step_avg:45.21ms
step:1021/1920 train_time:46182ms step_avg:45.23ms
step:1022/1920 train_time:46244ms step_avg:45.25ms
step:1023/1920 train_time:46306ms step_avg:45.26ms
step:1024/1920 train_time:46367ms step_avg:45.28ms
step:1025/1920 train_time:46429ms step_avg:45.30ms
step:1026/1920 train_time:46491ms step_avg:45.31ms
step:1027/1920 train_time:46553ms step_avg:45.33ms
step:1028/1920 train_time:46614ms step_avg:45.34ms
step:1029/1920 train_time:46676ms step_avg:45.36ms
step:1030/1920 train_time:46738ms step_avg:45.38ms
step:1031/1920 train_time:46801ms step_avg:45.39ms
step:1032/1920 train_time:46863ms step_avg:45.41ms
step:1033/1920 train_time:46927ms step_avg:45.43ms
step:1034/1920 train_time:46989ms step_avg:45.44ms
step:1035/1920 train_time:47053ms step_avg:45.46ms
step:1036/1920 train_time:47116ms step_avg:45.48ms
step:1037/1920 train_time:47180ms step_avg:45.50ms
step:1038/1920 train_time:47242ms step_avg:45.51ms
step:1039/1920 train_time:47304ms step_avg:45.53ms
step:1040/1920 train_time:47366ms step_avg:45.54ms
step:1041/1920 train_time:47429ms step_avg:45.56ms
step:1042/1920 train_time:47490ms step_avg:45.58ms
step:1043/1920 train_time:47553ms step_avg:45.59ms
step:1044/1920 train_time:47614ms step_avg:45.61ms
step:1045/1920 train_time:47677ms step_avg:45.62ms
step:1046/1920 train_time:47738ms step_avg:45.64ms
step:1047/1920 train_time:47801ms step_avg:45.66ms
step:1048/1920 train_time:47863ms step_avg:45.67ms
step:1049/1920 train_time:47926ms step_avg:45.69ms
step:1050/1920 train_time:47988ms step_avg:45.70ms
step:1051/1920 train_time:48052ms step_avg:45.72ms
step:1052/1920 train_time:48115ms step_avg:45.74ms
step:1053/1920 train_time:48178ms step_avg:45.75ms
step:1054/1920 train_time:48240ms step_avg:45.77ms
step:1055/1920 train_time:48303ms step_avg:45.78ms
step:1056/1920 train_time:48365ms step_avg:45.80ms
step:1057/1920 train_time:48428ms step_avg:45.82ms
step:1058/1920 train_time:48489ms step_avg:45.83ms
step:1059/1920 train_time:48552ms step_avg:45.85ms
step:1060/1920 train_time:48613ms step_avg:45.86ms
step:1061/1920 train_time:48676ms step_avg:45.88ms
step:1062/1920 train_time:48738ms step_avg:45.89ms
step:1063/1920 train_time:48800ms step_avg:45.91ms
step:1064/1920 train_time:48863ms step_avg:45.92ms
step:1065/1920 train_time:48925ms step_avg:45.94ms
step:1066/1920 train_time:48987ms step_avg:45.95ms
step:1067/1920 train_time:49051ms step_avg:45.97ms
step:1068/1920 train_time:49114ms step_avg:45.99ms
step:1069/1920 train_time:49178ms step_avg:46.00ms
step:1070/1920 train_time:49240ms step_avg:46.02ms
step:1071/1920 train_time:49303ms step_avg:46.03ms
step:1072/1920 train_time:49365ms step_avg:46.05ms
step:1073/1920 train_time:49426ms step_avg:46.06ms
step:1074/1920 train_time:49488ms step_avg:46.08ms
step:1075/1920 train_time:49551ms step_avg:46.09ms
step:1076/1920 train_time:49612ms step_avg:46.11ms
step:1077/1920 train_time:49676ms step_avg:46.12ms
step:1078/1920 train_time:49737ms step_avg:46.14ms
step:1079/1920 train_time:49801ms step_avg:46.15ms
step:1080/1920 train_time:49863ms step_avg:46.17ms
step:1081/1920 train_time:49926ms step_avg:46.18ms
step:1082/1920 train_time:49987ms step_avg:46.20ms
step:1083/1920 train_time:50050ms step_avg:46.21ms
step:1084/1920 train_time:50113ms step_avg:46.23ms
step:1085/1920 train_time:50178ms step_avg:46.25ms
step:1086/1920 train_time:50240ms step_avg:46.26ms
step:1087/1920 train_time:50303ms step_avg:46.28ms
step:1088/1920 train_time:50365ms step_avg:46.29ms
step:1089/1920 train_time:50427ms step_avg:46.31ms
step:1090/1920 train_time:50488ms step_avg:46.32ms
step:1091/1920 train_time:50551ms step_avg:46.33ms
step:1092/1920 train_time:50613ms step_avg:46.35ms
step:1093/1920 train_time:50676ms step_avg:46.36ms
step:1094/1920 train_time:50738ms step_avg:46.38ms
step:1095/1920 train_time:50801ms step_avg:46.39ms
step:1096/1920 train_time:50863ms step_avg:46.41ms
step:1097/1920 train_time:50926ms step_avg:46.42ms
step:1098/1920 train_time:50988ms step_avg:46.44ms
step:1099/1920 train_time:51051ms step_avg:46.45ms
step:1100/1920 train_time:51113ms step_avg:46.47ms
step:1101/1920 train_time:51177ms step_avg:46.48ms
step:1102/1920 train_time:51239ms step_avg:46.50ms
step:1103/1920 train_time:51303ms step_avg:46.51ms
step:1104/1920 train_time:51364ms step_avg:46.53ms
step:1105/1920 train_time:51426ms step_avg:46.54ms
step:1106/1920 train_time:51488ms step_avg:46.55ms
step:1107/1920 train_time:51551ms step_avg:46.57ms
step:1108/1920 train_time:51613ms step_avg:46.58ms
step:1109/1920 train_time:51676ms step_avg:46.60ms
step:1110/1920 train_time:51738ms step_avg:46.61ms
step:1111/1920 train_time:51801ms step_avg:46.63ms
step:1112/1920 train_time:51862ms step_avg:46.64ms
step:1113/1920 train_time:51925ms step_avg:46.65ms
step:1114/1920 train_time:51987ms step_avg:46.67ms
step:1115/1920 train_time:52050ms step_avg:46.68ms
step:1116/1920 train_time:52112ms step_avg:46.70ms
step:1117/1920 train_time:52176ms step_avg:46.71ms
step:1118/1920 train_time:52238ms step_avg:46.72ms
step:1119/1920 train_time:52302ms step_avg:46.74ms
step:1120/1920 train_time:52363ms step_avg:46.75ms
step:1121/1920 train_time:52426ms step_avg:46.77ms
step:1122/1920 train_time:52488ms step_avg:46.78ms
step:1123/1920 train_time:52550ms step_avg:46.79ms
step:1124/1920 train_time:52612ms step_avg:46.81ms
step:1125/1920 train_time:52675ms step_avg:46.82ms
step:1126/1920 train_time:52737ms step_avg:46.84ms
step:1127/1920 train_time:52800ms step_avg:46.85ms
step:1128/1920 train_time:52862ms step_avg:46.86ms
step:1129/1920 train_time:52925ms step_avg:46.88ms
step:1130/1920 train_time:52986ms step_avg:46.89ms
step:1131/1920 train_time:53049ms step_avg:46.90ms
step:1132/1920 train_time:53111ms step_avg:46.92ms
step:1133/1920 train_time:53174ms step_avg:46.93ms
step:1134/1920 train_time:53236ms step_avg:46.95ms
step:1135/1920 train_time:53300ms step_avg:46.96ms
step:1136/1920 train_time:53362ms step_avg:46.97ms
step:1137/1920 train_time:53424ms step_avg:46.99ms
step:1138/1920 train_time:53486ms step_avg:47.00ms
step:1139/1920 train_time:53549ms step_avg:47.01ms
step:1140/1920 train_time:53611ms step_avg:47.03ms
step:1141/1920 train_time:53674ms step_avg:47.04ms
step:1142/1920 train_time:53736ms step_avg:47.05ms
step:1143/1920 train_time:53799ms step_avg:47.07ms
step:1144/1920 train_time:53861ms step_avg:47.08ms
step:1145/1920 train_time:53924ms step_avg:47.10ms
step:1146/1920 train_time:53986ms step_avg:47.11ms
step:1147/1920 train_time:54048ms step_avg:47.12ms
step:1148/1920 train_time:54110ms step_avg:47.13ms
step:1149/1920 train_time:54173ms step_avg:47.15ms
step:1150/1920 train_time:54236ms step_avg:47.16ms
step:1151/1920 train_time:54299ms step_avg:47.18ms
step:1152/1920 train_time:54360ms step_avg:47.19ms
step:1153/1920 train_time:54423ms step_avg:47.20ms
step:1154/1920 train_time:54485ms step_avg:47.21ms
step:1155/1920 train_time:54547ms step_avg:47.23ms
step:1156/1920 train_time:54609ms step_avg:47.24ms
step:1157/1920 train_time:54673ms step_avg:47.25ms
step:1158/1920 train_time:54734ms step_avg:47.27ms
step:1159/1920 train_time:54798ms step_avg:47.28ms
step:1160/1920 train_time:54860ms step_avg:47.29ms
step:1161/1920 train_time:54923ms step_avg:47.31ms
step:1162/1920 train_time:54984ms step_avg:47.32ms
step:1163/1920 train_time:55047ms step_avg:47.33ms
step:1164/1920 train_time:55109ms step_avg:47.34ms
step:1165/1920 train_time:55172ms step_avg:47.36ms
step:1166/1920 train_time:55234ms step_avg:47.37ms
step:1167/1920 train_time:55297ms step_avg:47.38ms
step:1168/1920 train_time:55359ms step_avg:47.40ms
step:1169/1920 train_time:55423ms step_avg:47.41ms
step:1170/1920 train_time:55484ms step_avg:47.42ms
step:1171/1920 train_time:55547ms step_avg:47.44ms
step:1172/1920 train_time:55608ms step_avg:47.45ms
step:1173/1920 train_time:55671ms step_avg:47.46ms
step:1174/1920 train_time:55734ms step_avg:47.47ms
step:1175/1920 train_time:55797ms step_avg:47.49ms
step:1176/1920 train_time:55858ms step_avg:47.50ms
step:1177/1920 train_time:55921ms step_avg:47.51ms
step:1178/1920 train_time:55983ms step_avg:47.52ms
step:1179/1920 train_time:56045ms step_avg:47.54ms
step:1180/1920 train_time:56107ms step_avg:47.55ms
step:1181/1920 train_time:56171ms step_avg:47.56ms
step:1182/1920 train_time:56232ms step_avg:47.57ms
step:1183/1920 train_time:56296ms step_avg:47.59ms
step:1184/1920 train_time:56358ms step_avg:47.60ms
step:1185/1920 train_time:56421ms step_avg:47.61ms
step:1186/1920 train_time:56483ms step_avg:47.62ms
step:1187/1920 train_time:56545ms step_avg:47.64ms
step:1188/1920 train_time:56606ms step_avg:47.65ms
step:1189/1920 train_time:56669ms step_avg:47.66ms
step:1190/1920 train_time:56730ms step_avg:47.67ms
step:1191/1920 train_time:56794ms step_avg:47.69ms
step:1192/1920 train_time:56856ms step_avg:47.70ms
step:1193/1920 train_time:56919ms step_avg:47.71ms
step:1194/1920 train_time:56981ms step_avg:47.72ms
step:1195/1920 train_time:57044ms step_avg:47.74ms
step:1196/1920 train_time:57106ms step_avg:47.75ms
step:1197/1920 train_time:57168ms step_avg:47.76ms
step:1198/1920 train_time:57230ms step_avg:47.77ms
step:1199/1920 train_time:57293ms step_avg:47.78ms
step:1200/1920 train_time:57356ms step_avg:47.80ms
step:1201/1920 train_time:57419ms step_avg:47.81ms
step:1202/1920 train_time:57480ms step_avg:47.82ms
step:1203/1920 train_time:57543ms step_avg:47.83ms
step:1204/1920 train_time:57605ms step_avg:47.84ms
step:1205/1920 train_time:57667ms step_avg:47.86ms
step:1206/1920 train_time:57729ms step_avg:47.87ms
step:1207/1920 train_time:57792ms step_avg:47.88ms
step:1208/1920 train_time:57854ms step_avg:47.89ms
step:1209/1920 train_time:57918ms step_avg:47.91ms
step:1210/1920 train_time:57980ms step_avg:47.92ms
step:1211/1920 train_time:58043ms step_avg:47.93ms
step:1212/1920 train_time:58105ms step_avg:47.94ms
step:1213/1920 train_time:58167ms step_avg:47.95ms
step:1214/1920 train_time:58229ms step_avg:47.96ms
step:1215/1920 train_time:58292ms step_avg:47.98ms
step:1216/1920 train_time:58354ms step_avg:47.99ms
step:1217/1920 train_time:58417ms step_avg:48.00ms
step:1218/1920 train_time:58480ms step_avg:48.01ms
step:1219/1920 train_time:58542ms step_avg:48.02ms
step:1220/1920 train_time:58604ms step_avg:48.04ms
step:1221/1920 train_time:58667ms step_avg:48.05ms
step:1222/1920 train_time:58729ms step_avg:48.06ms
step:1223/1920 train_time:58792ms step_avg:48.07ms
step:1224/1920 train_time:58854ms step_avg:48.08ms
step:1225/1920 train_time:58917ms step_avg:48.10ms
step:1226/1920 train_time:58979ms step_avg:48.11ms
step:1227/1920 train_time:59042ms step_avg:48.12ms
step:1228/1920 train_time:59104ms step_avg:48.13ms
step:1229/1920 train_time:59167ms step_avg:48.14ms
step:1230/1920 train_time:59228ms step_avg:48.15ms
step:1231/1920 train_time:59291ms step_avg:48.16ms
step:1232/1920 train_time:59353ms step_avg:48.18ms
step:1233/1920 train_time:59417ms step_avg:48.19ms
step:1234/1920 train_time:59479ms step_avg:48.20ms
step:1235/1920 train_time:59541ms step_avg:48.21ms
step:1236/1920 train_time:59603ms step_avg:48.22ms
step:1237/1920 train_time:59666ms step_avg:48.23ms
step:1238/1920 train_time:59728ms step_avg:48.25ms
step:1239/1920 train_time:59790ms step_avg:48.26ms
step:1240/1920 train_time:59852ms step_avg:48.27ms
step:1241/1920 train_time:59915ms step_avg:48.28ms
step:1242/1920 train_time:59977ms step_avg:48.29ms
step:1243/1920 train_time:60040ms step_avg:48.30ms
step:1244/1920 train_time:60102ms step_avg:48.31ms
step:1245/1920 train_time:60165ms step_avg:48.32ms
step:1246/1920 train_time:60226ms step_avg:48.34ms
step:1247/1920 train_time:60289ms step_avg:48.35ms
step:1248/1920 train_time:60351ms step_avg:48.36ms
step:1249/1920 train_time:60414ms step_avg:48.37ms
step:1250/1920 train_time:60476ms step_avg:48.38ms
step:1250/1920 val_loss:3.5552 train_time:60542ms step_avg:48.43ms
step:1251/1920 train_time:60559ms step_avg:48.41ms
step:1252/1920 train_time:60603ms step_avg:48.40ms
step:1253/1920 train_time:60669ms step_avg:48.42ms
step:1254/1920 train_time:60733ms step_avg:48.43ms
step:1255/1920 train_time:60797ms step_avg:48.44ms
step:1256/1920 train_time:60885ms step_avg:48.48ms
step:1257/1920 train_time:60973ms step_avg:48.51ms
step:1258/1920 train_time:61060ms step_avg:48.54ms
step:1259/1920 train_time:61148ms step_avg:48.57ms
step:1260/1920 train_time:61236ms step_avg:48.60ms
step:1261/1920 train_time:61324ms step_avg:48.63ms
step:1262/1920 train_time:61411ms step_avg:48.66ms
step:1263/1920 train_time:61501ms step_avg:48.69ms
step:1264/1920 train_time:61591ms step_avg:48.73ms
step:1265/1920 train_time:61682ms step_avg:48.76ms
step:1266/1920 train_time:61771ms step_avg:48.79ms
step:1267/1920 train_time:61861ms step_avg:48.82ms
step:1268/1920 train_time:61948ms step_avg:48.85ms
step:1269/1920 train_time:62036ms step_avg:48.89ms
step:1270/1920 train_time:62123ms step_avg:48.92ms
step:1271/1920 train_time:62211ms step_avg:48.95ms
step:1272/1920 train_time:62299ms step_avg:48.98ms
step:1273/1920 train_time:62387ms step_avg:49.01ms
step:1274/1920 train_time:62476ms step_avg:49.04ms
step:1275/1920 train_time:62567ms step_avg:49.07ms
step:1276/1920 train_time:62656ms step_avg:49.10ms
step:1277/1920 train_time:62746ms step_avg:49.14ms
step:1278/1920 train_time:62835ms step_avg:49.17ms
step:1279/1920 train_time:62924ms step_avg:49.20ms
step:1280/1920 train_time:63011ms step_avg:49.23ms
step:1281/1920 train_time:63099ms step_avg:49.26ms
step:1282/1920 train_time:63186ms step_avg:49.29ms
step:1283/1920 train_time:63274ms step_avg:49.32ms
step:1284/1920 train_time:63362ms step_avg:49.35ms
step:1285/1920 train_time:63451ms step_avg:49.38ms
step:1286/1920 train_time:63539ms step_avg:49.41ms
step:1287/1920 train_time:63629ms step_avg:49.44ms
step:1288/1920 train_time:63718ms step_avg:49.47ms
step:1289/1920 train_time:63807ms step_avg:49.50ms
step:1290/1920 train_time:63896ms step_avg:49.53ms
step:1291/1920 train_time:63985ms step_avg:49.56ms
step:1292/1920 train_time:64072ms step_avg:49.59ms
step:1293/1920 train_time:64160ms step_avg:49.62ms
step:1294/1920 train_time:64247ms step_avg:49.65ms
step:1295/1920 train_time:64335ms step_avg:49.68ms
step:1296/1920 train_time:64423ms step_avg:49.71ms
step:1297/1920 train_time:64512ms step_avg:49.74ms
step:1298/1920 train_time:64601ms step_avg:49.77ms
step:1299/1920 train_time:64690ms step_avg:49.80ms
step:1300/1920 train_time:64779ms step_avg:49.83ms
step:1301/1920 train_time:64868ms step_avg:49.86ms
step:1302/1920 train_time:64956ms step_avg:49.89ms
step:1303/1920 train_time:65045ms step_avg:49.92ms
step:1304/1920 train_time:65132ms step_avg:49.95ms
step:1305/1920 train_time:65221ms step_avg:49.98ms
step:1306/1920 train_time:65308ms step_avg:50.01ms
step:1307/1920 train_time:65396ms step_avg:50.04ms
step:1308/1920 train_time:65484ms step_avg:50.06ms
step:1309/1920 train_time:65573ms step_avg:50.09ms
step:1310/1920 train_time:65662ms step_avg:50.12ms
step:1311/1920 train_time:65751ms step_avg:50.15ms
step:1312/1920 train_time:65839ms step_avg:50.18ms
step:1313/1920 train_time:65928ms step_avg:50.21ms
step:1314/1920 train_time:66017ms step_avg:50.24ms
step:1315/1920 train_time:66106ms step_avg:50.27ms
step:1316/1920 train_time:66193ms step_avg:50.30ms
step:1317/1920 train_time:66282ms step_avg:50.33ms
step:1318/1920 train_time:66369ms step_avg:50.36ms
step:1319/1920 train_time:66457ms step_avg:50.38ms
step:1320/1920 train_time:66546ms step_avg:50.41ms
step:1321/1920 train_time:66636ms step_avg:50.44ms
step:1322/1920 train_time:66725ms step_avg:50.47ms
step:1323/1920 train_time:66813ms step_avg:50.50ms
step:1324/1920 train_time:66901ms step_avg:50.53ms
step:1325/1920 train_time:66990ms step_avg:50.56ms
step:1326/1920 train_time:67078ms step_avg:50.59ms
step:1327/1920 train_time:67167ms step_avg:50.62ms
step:1328/1920 train_time:67255ms step_avg:50.64ms
step:1329/1920 train_time:67344ms step_avg:50.67ms
step:1330/1920 train_time:67432ms step_avg:50.70ms
step:1331/1920 train_time:67520ms step_avg:50.73ms
step:1332/1920 train_time:67608ms step_avg:50.76ms
step:1333/1920 train_time:67697ms step_avg:50.79ms
step:1334/1920 train_time:67786ms step_avg:50.81ms
step:1335/1920 train_time:67875ms step_avg:50.84ms
step:1336/1920 train_time:67964ms step_avg:50.87ms
step:1337/1920 train_time:68052ms step_avg:50.90ms
step:1338/1920 train_time:68139ms step_avg:50.93ms
step:1339/1920 train_time:68229ms step_avg:50.95ms
step:1340/1920 train_time:68317ms step_avg:50.98ms
step:1341/1920 train_time:68407ms step_avg:51.01ms
step:1342/1920 train_time:68495ms step_avg:51.04ms
step:1343/1920 train_time:68583ms step_avg:51.07ms
step:1344/1920 train_time:68671ms step_avg:51.09ms
step:1345/1920 train_time:68760ms step_avg:51.12ms
step:1346/1920 train_time:68848ms step_avg:51.15ms
step:1347/1920 train_time:68937ms step_avg:51.18ms
step:1348/1920 train_time:69025ms step_avg:51.21ms
step:1349/1920 train_time:69113ms step_avg:51.23ms
step:1350/1920 train_time:69202ms step_avg:51.26ms
step:1351/1920 train_time:69290ms step_avg:51.29ms
step:1352/1920 train_time:69378ms step_avg:51.32ms
step:1353/1920 train_time:69467ms step_avg:51.34ms
step:1354/1920 train_time:69556ms step_avg:51.37ms
step:1355/1920 train_time:69644ms step_avg:51.40ms
step:1356/1920 train_time:69732ms step_avg:51.42ms
step:1357/1920 train_time:69821ms step_avg:51.45ms
step:1358/1920 train_time:69908ms step_avg:51.48ms
step:1359/1920 train_time:69997ms step_avg:51.51ms
step:1360/1920 train_time:70086ms step_avg:51.53ms
step:1361/1920 train_time:70175ms step_avg:51.56ms
step:1362/1920 train_time:70263ms step_avg:51.59ms
step:1363/1920 train_time:70351ms step_avg:51.61ms
step:1364/1920 train_time:70440ms step_avg:51.64ms
step:1365/1920 train_time:70529ms step_avg:51.67ms
step:1366/1920 train_time:70617ms step_avg:51.70ms
step:1367/1920 train_time:70707ms step_avg:51.72ms
step:1368/1920 train_time:70795ms step_avg:51.75ms
step:1369/1920 train_time:70884ms step_avg:51.78ms
step:1370/1920 train_time:70972ms step_avg:51.80ms
step:1371/1920 train_time:71062ms step_avg:51.83ms
step:1372/1920 train_time:71149ms step_avg:51.86ms
step:1373/1920 train_time:71238ms step_avg:51.89ms
step:1374/1920 train_time:71327ms step_avg:51.91ms
step:1375/1920 train_time:71415ms step_avg:51.94ms
step:1376/1920 train_time:71505ms step_avg:51.97ms
step:1377/1920 train_time:71593ms step_avg:51.99ms
step:1378/1920 train_time:71682ms step_avg:52.02ms
step:1379/1920 train_time:71771ms step_avg:52.05ms
step:1380/1920 train_time:71860ms step_avg:52.07ms
step:1381/1920 train_time:71948ms step_avg:52.10ms
step:1382/1920 train_time:72036ms step_avg:52.12ms
step:1383/1920 train_time:72126ms step_avg:52.15ms
step:1384/1920 train_time:72213ms step_avg:52.18ms
step:1385/1920 train_time:72303ms step_avg:52.20ms
step:1386/1920 train_time:72390ms step_avg:52.23ms
step:1387/1920 train_time:72480ms step_avg:52.26ms
step:1388/1920 train_time:72568ms step_avg:52.28ms
step:1389/1920 train_time:72658ms step_avg:52.31ms
step:1390/1920 train_time:72745ms step_avg:52.33ms
step:1391/1920 train_time:72834ms step_avg:52.36ms
step:1392/1920 train_time:72922ms step_avg:52.39ms
step:1393/1920 train_time:73011ms step_avg:52.41ms
step:1394/1920 train_time:73099ms step_avg:52.44ms
step:1395/1920 train_time:73187ms step_avg:52.46ms
step:1396/1920 train_time:73275ms step_avg:52.49ms
step:1397/1920 train_time:73364ms step_avg:52.52ms
step:1398/1920 train_time:73452ms step_avg:52.54ms
step:1399/1920 train_time:73541ms step_avg:52.57ms
step:1400/1920 train_time:73628ms step_avg:52.59ms
step:1401/1920 train_time:73718ms step_avg:52.62ms
step:1402/1920 train_time:73806ms step_avg:52.64ms
step:1403/1920 train_time:73894ms step_avg:52.67ms
step:1404/1920 train_time:73982ms step_avg:52.69ms
step:1405/1920 train_time:74070ms step_avg:52.72ms
step:1406/1920 train_time:74158ms step_avg:52.74ms
step:1407/1920 train_time:74247ms step_avg:52.77ms
step:1408/1920 train_time:74335ms step_avg:52.79ms
step:1409/1920 train_time:74424ms step_avg:52.82ms
step:1410/1920 train_time:74512ms step_avg:52.85ms
step:1411/1920 train_time:74601ms step_avg:52.87ms
step:1412/1920 train_time:74688ms step_avg:52.90ms
step:1413/1920 train_time:74777ms step_avg:52.92ms
step:1414/1920 train_time:74866ms step_avg:52.95ms
step:1415/1920 train_time:74955ms step_avg:52.97ms
step:1416/1920 train_time:75043ms step_avg:53.00ms
step:1417/1920 train_time:75131ms step_avg:53.02ms
step:1418/1920 train_time:75219ms step_avg:53.05ms
step:1419/1920 train_time:75307ms step_avg:53.07ms
step:1420/1920 train_time:75396ms step_avg:53.10ms
step:1421/1920 train_time:75485ms step_avg:53.12ms
step:1422/1920 train_time:75573ms step_avg:53.15ms
step:1423/1920 train_time:75663ms step_avg:53.17ms
step:1424/1920 train_time:75751ms step_avg:53.20ms
step:1425/1920 train_time:75839ms step_avg:53.22ms
step:1426/1920 train_time:75927ms step_avg:53.24ms
step:1427/1920 train_time:76016ms step_avg:53.27ms
step:1428/1920 train_time:76105ms step_avg:53.29ms
step:1429/1920 train_time:76193ms step_avg:53.32ms
step:1430/1920 train_time:76281ms step_avg:53.34ms
step:1431/1920 train_time:76369ms step_avg:53.37ms
step:1432/1920 train_time:76458ms step_avg:53.39ms
step:1433/1920 train_time:76547ms step_avg:53.42ms
step:1434/1920 train_time:76635ms step_avg:53.44ms
step:1435/1920 train_time:76725ms step_avg:53.47ms
step:1436/1920 train_time:76813ms step_avg:53.49ms
step:1437/1920 train_time:76902ms step_avg:53.52ms
step:1438/1920 train_time:76990ms step_avg:53.54ms
step:1439/1920 train_time:77078ms step_avg:53.56ms
step:1440/1920 train_time:77166ms step_avg:53.59ms
step:1441/1920 train_time:77255ms step_avg:53.61ms
step:1442/1920 train_time:77342ms step_avg:53.64ms
step:1443/1920 train_time:77431ms step_avg:53.66ms
step:1444/1920 train_time:77519ms step_avg:53.68ms
step:1445/1920 train_time:77608ms step_avg:53.71ms
step:1446/1920 train_time:77696ms step_avg:53.73ms
step:1447/1920 train_time:77788ms step_avg:53.76ms
step:1448/1920 train_time:77877ms step_avg:53.78ms
step:1449/1920 train_time:77966ms step_avg:53.81ms
step:1450/1920 train_time:78054ms step_avg:53.83ms
step:1451/1920 train_time:78143ms step_avg:53.85ms
step:1452/1920 train_time:78230ms step_avg:53.88ms
step:1453/1920 train_time:78318ms step_avg:53.90ms
step:1454/1920 train_time:78406ms step_avg:53.92ms
step:1455/1920 train_time:78495ms step_avg:53.95ms
step:1456/1920 train_time:78584ms step_avg:53.97ms
step:1457/1920 train_time:78673ms step_avg:54.00ms
step:1458/1920 train_time:78761ms step_avg:54.02ms
step:1459/1920 train_time:78850ms step_avg:54.04ms
step:1460/1920 train_time:78939ms step_avg:54.07ms
step:1461/1920 train_time:79028ms step_avg:54.09ms
step:1462/1920 train_time:79116ms step_avg:54.12ms
step:1463/1920 train_time:79206ms step_avg:54.14ms
step:1464/1920 train_time:79294ms step_avg:54.16ms
step:1465/1920 train_time:79383ms step_avg:54.19ms
step:1466/1920 train_time:79471ms step_avg:54.21ms
step:1467/1920 train_time:79560ms step_avg:54.23ms
step:1468/1920 train_time:79648ms step_avg:54.26ms
step:1469/1920 train_time:79737ms step_avg:54.28ms
step:1470/1920 train_time:79826ms step_avg:54.30ms
step:1471/1920 train_time:79914ms step_avg:54.33ms
step:1472/1920 train_time:80003ms step_avg:54.35ms
step:1473/1920 train_time:80090ms step_avg:54.37ms
step:1474/1920 train_time:80179ms step_avg:54.40ms
step:1475/1920 train_time:80269ms step_avg:54.42ms
step:1476/1920 train_time:80357ms step_avg:54.44ms
step:1477/1920 train_time:80447ms step_avg:54.47ms
step:1478/1920 train_time:80536ms step_avg:54.49ms
step:1479/1920 train_time:80626ms step_avg:54.51ms
step:1480/1920 train_time:80714ms step_avg:54.54ms
step:1481/1920 train_time:80803ms step_avg:54.56ms
step:1482/1920 train_time:80891ms step_avg:54.58ms
step:1483/1920 train_time:80979ms step_avg:54.61ms
step:1484/1920 train_time:81067ms step_avg:54.63ms
step:1485/1920 train_time:81156ms step_avg:54.65ms
step:1486/1920 train_time:81244ms step_avg:54.67ms
step:1487/1920 train_time:81332ms step_avg:54.70ms
step:1488/1920 train_time:81421ms step_avg:54.72ms
step:1489/1920 train_time:81509ms step_avg:54.74ms
step:1490/1920 train_time:81599ms step_avg:54.76ms
step:1491/1920 train_time:81689ms step_avg:54.79ms
step:1492/1920 train_time:81778ms step_avg:54.81ms
step:1493/1920 train_time:81867ms step_avg:54.83ms
step:1494/1920 train_time:81957ms step_avg:54.86ms
step:1495/1920 train_time:82045ms step_avg:54.88ms
step:1496/1920 train_time:82133ms step_avg:54.90ms
step:1497/1920 train_time:82221ms step_avg:54.92ms
step:1498/1920 train_time:82309ms step_avg:54.95ms
step:1499/1920 train_time:82398ms step_avg:54.97ms
step:1500/1920 train_time:82486ms step_avg:54.99ms
step:1500/1920 val_loss:3.4166 train_time:82578ms step_avg:55.05ms
step:1501/1920 train_time:82597ms step_avg:55.03ms
step:1502/1920 train_time:82669ms step_avg:55.04ms
step:1503/1920 train_time:82764ms step_avg:55.07ms
step:1504/1920 train_time:82853ms step_avg:55.09ms
step:1505/1920 train_time:82941ms step_avg:55.11ms
step:1506/1920 train_time:83028ms step_avg:55.13ms
step:1507/1920 train_time:83116ms step_avg:55.15ms
step:1508/1920 train_time:83202ms step_avg:55.17ms
step:1509/1920 train_time:83290ms step_avg:55.20ms
step:1510/1920 train_time:83376ms step_avg:55.22ms
step:1511/1920 train_time:83464ms step_avg:55.24ms
step:1512/1920 train_time:83552ms step_avg:55.26ms
step:1513/1920 train_time:83644ms step_avg:55.28ms
step:1514/1920 train_time:83735ms step_avg:55.31ms
step:1515/1920 train_time:83824ms step_avg:55.33ms
step:1516/1920 train_time:83912ms step_avg:55.35ms
step:1517/1920 train_time:84001ms step_avg:55.37ms
step:1518/1920 train_time:84089ms step_avg:55.39ms
step:1519/1920 train_time:84177ms step_avg:55.42ms
step:1520/1920 train_time:84263ms step_avg:55.44ms
step:1521/1920 train_time:84351ms step_avg:55.46ms
step:1522/1920 train_time:84439ms step_avg:55.48ms
step:1523/1920 train_time:84528ms step_avg:55.50ms
step:1524/1920 train_time:84618ms step_avg:55.52ms
step:1525/1920 train_time:84709ms step_avg:55.55ms
step:1526/1920 train_time:84798ms step_avg:55.57ms
step:1527/1920 train_time:84888ms step_avg:55.59ms
step:1528/1920 train_time:84975ms step_avg:55.61ms
step:1529/1920 train_time:85063ms step_avg:55.63ms
step:1530/1920 train_time:85150ms step_avg:55.65ms
step:1531/1920 train_time:85238ms step_avg:55.67ms
step:1532/1920 train_time:85325ms step_avg:55.70ms
step:1533/1920 train_time:85414ms step_avg:55.72ms
step:1534/1920 train_time:85502ms step_avg:55.74ms
step:1535/1920 train_time:85592ms step_avg:55.76ms
step:1536/1920 train_time:85680ms step_avg:55.78ms
step:1537/1920 train_time:85770ms step_avg:55.80ms
step:1538/1920 train_time:85859ms step_avg:55.83ms
step:1539/1920 train_time:85948ms step_avg:55.85ms
step:1540/1920 train_time:86035ms step_avg:55.87ms
step:1541/1920 train_time:86124ms step_avg:55.89ms
step:1542/1920 train_time:86211ms step_avg:55.91ms
step:1543/1920 train_time:86299ms step_avg:55.93ms
step:1544/1920 train_time:86387ms step_avg:55.95ms
step:1545/1920 train_time:86476ms step_avg:55.97ms
step:1546/1920 train_time:86564ms step_avg:55.99ms
step:1547/1920 train_time:86653ms step_avg:56.01ms
step:1548/1920 train_time:86742ms step_avg:56.03ms
step:1549/1920 train_time:86831ms step_avg:56.06ms
step:1550/1920 train_time:86920ms step_avg:56.08ms
step:1551/1920 train_time:87008ms step_avg:56.10ms
step:1552/1920 train_time:87097ms step_avg:56.12ms
step:1553/1920 train_time:87184ms step_avg:56.14ms
step:1554/1920 train_time:87272ms step_avg:56.16ms
step:1555/1920 train_time:87360ms step_avg:56.18ms
step:1556/1920 train_time:87448ms step_avg:56.20ms
step:1557/1920 train_time:87537ms step_avg:56.22ms
step:1558/1920 train_time:87626ms step_avg:56.24ms
step:1559/1920 train_time:87716ms step_avg:56.26ms
step:1560/1920 train_time:87804ms step_avg:56.28ms
step:1561/1920 train_time:87893ms step_avg:56.31ms
step:1562/1920 train_time:87982ms step_avg:56.33ms
step:1563/1920 train_time:88070ms step_avg:56.35ms
step:1564/1920 train_time:88158ms step_avg:56.37ms
step:1565/1920 train_time:88246ms step_avg:56.39ms
step:1566/1920 train_time:88335ms step_avg:56.41ms
step:1567/1920 train_time:88423ms step_avg:56.43ms
step:1568/1920 train_time:88511ms step_avg:56.45ms
step:1569/1920 train_time:88601ms step_avg:56.47ms
step:1570/1920 train_time:88689ms step_avg:56.49ms
step:1571/1920 train_time:88780ms step_avg:56.51ms
step:1572/1920 train_time:88869ms step_avg:56.53ms
step:1573/1920 train_time:88958ms step_avg:56.55ms
step:1574/1920 train_time:89045ms step_avg:56.57ms
step:1575/1920 train_time:89135ms step_avg:56.59ms
step:1576/1920 train_time:89222ms step_avg:56.61ms
step:1577/1920 train_time:89311ms step_avg:56.63ms
step:1578/1920 train_time:89399ms step_avg:56.65ms
step:1579/1920 train_time:89488ms step_avg:56.67ms
step:1580/1920 train_time:89576ms step_avg:56.69ms
step:1581/1920 train_time:89665ms step_avg:56.71ms
step:1582/1920 train_time:89755ms step_avg:56.74ms
step:1583/1920 train_time:89844ms step_avg:56.76ms
step:1584/1920 train_time:89932ms step_avg:56.78ms
step:1585/1920 train_time:90021ms step_avg:56.80ms
step:1586/1920 train_time:90109ms step_avg:56.82ms
step:1587/1920 train_time:90199ms step_avg:56.84ms
step:1588/1920 train_time:90287ms step_avg:56.86ms
step:1589/1920 train_time:90376ms step_avg:56.88ms
step:1590/1920 train_time:90464ms step_avg:56.90ms
step:1591/1920 train_time:90553ms step_avg:56.92ms
step:1592/1920 train_time:90641ms step_avg:56.94ms
step:1593/1920 train_time:90730ms step_avg:56.96ms
step:1594/1920 train_time:90818ms step_avg:56.97ms
step:1595/1920 train_time:90906ms step_avg:56.99ms
step:1596/1920 train_time:90995ms step_avg:57.01ms
step:1597/1920 train_time:91083ms step_avg:57.03ms
step:1598/1920 train_time:91171ms step_avg:57.05ms
step:1599/1920 train_time:91260ms step_avg:57.07ms
step:1600/1920 train_time:91348ms step_avg:57.09ms
step:1601/1920 train_time:91438ms step_avg:57.11ms
step:1602/1920 train_time:91526ms step_avg:57.13ms
step:1603/1920 train_time:91615ms step_avg:57.15ms
step:1604/1920 train_time:91703ms step_avg:57.17ms
step:1605/1920 train_time:91793ms step_avg:57.19ms
step:1606/1920 train_time:91881ms step_avg:57.21ms
step:1607/1920 train_time:91969ms step_avg:57.23ms
step:1608/1920 train_time:92057ms step_avg:57.25ms
step:1609/1920 train_time:92146ms step_avg:57.27ms
step:1610/1920 train_time:92234ms step_avg:57.29ms
step:1611/1920 train_time:92322ms step_avg:57.31ms
step:1612/1920 train_time:92411ms step_avg:57.33ms
step:1613/1920 train_time:92500ms step_avg:57.35ms
step:1614/1920 train_time:92589ms step_avg:57.37ms
step:1615/1920 train_time:92678ms step_avg:57.39ms
step:1616/1920 train_time:92766ms step_avg:57.40ms
step:1617/1920 train_time:92856ms step_avg:57.42ms
step:1618/1920 train_time:92943ms step_avg:57.44ms
step:1619/1920 train_time:93032ms step_avg:57.46ms
step:1620/1920 train_time:93121ms step_avg:57.48ms
step:1621/1920 train_time:93209ms step_avg:57.50ms
step:1622/1920 train_time:93297ms step_avg:57.52ms
step:1623/1920 train_time:93386ms step_avg:57.54ms
step:1624/1920 train_time:93475ms step_avg:57.56ms
step:1625/1920 train_time:93564ms step_avg:57.58ms
step:1626/1920 train_time:93653ms step_avg:57.60ms
step:1627/1920 train_time:93741ms step_avg:57.62ms
step:1628/1920 train_time:93831ms step_avg:57.64ms
step:1629/1920 train_time:93920ms step_avg:57.66ms
step:1630/1920 train_time:94008ms step_avg:57.67ms
step:1631/1920 train_time:94099ms step_avg:57.69ms
step:1632/1920 train_time:94186ms step_avg:57.71ms
step:1633/1920 train_time:94274ms step_avg:57.73ms
step:1634/1920 train_time:94362ms step_avg:57.75ms
step:1635/1920 train_time:94451ms step_avg:57.77ms
step:1636/1920 train_time:94539ms step_avg:57.79ms
step:1637/1920 train_time:94627ms step_avg:57.81ms
step:1638/1920 train_time:94716ms step_avg:57.82ms
step:1639/1920 train_time:94804ms step_avg:57.84ms
step:1640/1920 train_time:94892ms step_avg:57.86ms
step:1641/1920 train_time:94981ms step_avg:57.88ms
step:1642/1920 train_time:95069ms step_avg:57.90ms
step:1643/1920 train_time:95158ms step_avg:57.92ms
step:1644/1920 train_time:95245ms step_avg:57.94ms
step:1645/1920 train_time:95334ms step_avg:57.95ms
step:1646/1920 train_time:95422ms step_avg:57.97ms
step:1647/1920 train_time:95510ms step_avg:57.99ms
step:1648/1920 train_time:95599ms step_avg:58.01ms
step:1649/1920 train_time:95688ms step_avg:58.03ms
step:1650/1920 train_time:95776ms step_avg:58.05ms
step:1651/1920 train_time:95864ms step_avg:58.06ms
step:1652/1920 train_time:95951ms step_avg:58.08ms
step:1653/1920 train_time:96040ms step_avg:58.10ms
step:1654/1920 train_time:96130ms step_avg:58.12ms
step:1655/1920 train_time:96219ms step_avg:58.14ms
step:1656/1920 train_time:96307ms step_avg:58.16ms
step:1657/1920 train_time:96397ms step_avg:58.18ms
step:1658/1920 train_time:96485ms step_avg:58.19ms
step:1659/1920 train_time:96574ms step_avg:58.21ms
step:1660/1920 train_time:96662ms step_avg:58.23ms
step:1661/1920 train_time:96751ms step_avg:58.25ms
step:1662/1920 train_time:96839ms step_avg:58.27ms
step:1663/1920 train_time:96928ms step_avg:58.28ms
step:1664/1920 train_time:97016ms step_avg:58.30ms
step:1665/1920 train_time:97104ms step_avg:58.32ms
step:1666/1920 train_time:97192ms step_avg:58.34ms
step:1667/1920 train_time:97280ms step_avg:58.36ms
step:1668/1920 train_time:97369ms step_avg:58.37ms
step:1669/1920 train_time:97458ms step_avg:58.39ms
step:1670/1920 train_time:97545ms step_avg:58.41ms
step:1671/1920 train_time:97634ms step_avg:58.43ms
step:1672/1920 train_time:97722ms step_avg:58.45ms
step:1673/1920 train_time:97811ms step_avg:58.46ms
step:1674/1920 train_time:97899ms step_avg:58.48ms
step:1675/1920 train_time:97989ms step_avg:58.50ms
step:1676/1920 train_time:98077ms step_avg:58.52ms
step:1677/1920 train_time:98166ms step_avg:58.54ms
step:1678/1920 train_time:98256ms step_avg:58.56ms
step:1679/1920 train_time:98345ms step_avg:58.57ms
step:1680/1920 train_time:98434ms step_avg:58.59ms
step:1681/1920 train_time:98522ms step_avg:58.61ms
step:1682/1920 train_time:98610ms step_avg:58.63ms
step:1683/1920 train_time:98699ms step_avg:58.64ms
step:1684/1920 train_time:98788ms step_avg:58.66ms
step:1685/1920 train_time:98877ms step_avg:58.68ms
step:1686/1920 train_time:98965ms step_avg:58.70ms
step:1687/1920 train_time:99054ms step_avg:58.72ms
step:1688/1920 train_time:99141ms step_avg:58.73ms
step:1689/1920 train_time:99230ms step_avg:58.75ms
step:1690/1920 train_time:99319ms step_avg:58.77ms
step:1691/1920 train_time:99408ms step_avg:58.79ms
step:1692/1920 train_time:99496ms step_avg:58.80ms
step:1693/1920 train_time:99585ms step_avg:58.82ms
step:1694/1920 train_time:99673ms step_avg:58.84ms
step:1695/1920 train_time:99762ms step_avg:58.86ms
step:1696/1920 train_time:99851ms step_avg:58.87ms
step:1697/1920 train_time:99940ms step_avg:58.89ms
step:1698/1920 train_time:100028ms step_avg:58.91ms
step:1699/1920 train_time:100118ms step_avg:58.93ms
step:1700/1920 train_time:100206ms step_avg:58.94ms
step:1701/1920 train_time:100296ms step_avg:58.96ms
step:1702/1920 train_time:100384ms step_avg:58.98ms
step:1703/1920 train_time:100474ms step_avg:59.00ms
step:1704/1920 train_time:100561ms step_avg:59.01ms
step:1705/1920 train_time:100650ms step_avg:59.03ms
step:1706/1920 train_time:100738ms step_avg:59.05ms
step:1707/1920 train_time:100826ms step_avg:59.07ms
step:1708/1920 train_time:100914ms step_avg:59.08ms
step:1709/1920 train_time:101003ms step_avg:59.10ms
step:1710/1920 train_time:101091ms step_avg:59.12ms
step:1711/1920 train_time:101181ms step_avg:59.14ms
step:1712/1920 train_time:101269ms step_avg:59.15ms
step:1713/1920 train_time:101360ms step_avg:59.17ms
step:1714/1920 train_time:101449ms step_avg:59.19ms
step:1715/1920 train_time:101538ms step_avg:59.21ms
step:1716/1920 train_time:101626ms step_avg:59.22ms
step:1717/1920 train_time:101715ms step_avg:59.24ms
step:1718/1920 train_time:101803ms step_avg:59.26ms
step:1719/1920 train_time:101892ms step_avg:59.27ms
step:1720/1920 train_time:101980ms step_avg:59.29ms
step:1721/1920 train_time:102069ms step_avg:59.31ms
step:1722/1920 train_time:102157ms step_avg:59.32ms
step:1723/1920 train_time:102246ms step_avg:59.34ms
step:1724/1920 train_time:102334ms step_avg:59.36ms
step:1725/1920 train_time:102423ms step_avg:59.38ms
step:1726/1920 train_time:102511ms step_avg:59.39ms
step:1727/1920 train_time:102600ms step_avg:59.41ms
step:1728/1920 train_time:102688ms step_avg:59.43ms
step:1729/1920 train_time:102777ms step_avg:59.44ms
step:1730/1920 train_time:102864ms step_avg:59.46ms
step:1731/1920 train_time:102953ms step_avg:59.48ms
step:1732/1920 train_time:103041ms step_avg:59.49ms
step:1733/1920 train_time:103130ms step_avg:59.51ms
step:1734/1920 train_time:103219ms step_avg:59.53ms
step:1735/1920 train_time:103308ms step_avg:59.54ms
step:1736/1920 train_time:103397ms step_avg:59.56ms
step:1737/1920 train_time:103485ms step_avg:59.58ms
step:1738/1920 train_time:103573ms step_avg:59.59ms
step:1739/1920 train_time:103662ms step_avg:59.61ms
step:1740/1920 train_time:103750ms step_avg:59.63ms
step:1741/1920 train_time:103841ms step_avg:59.64ms
step:1742/1920 train_time:103929ms step_avg:59.66ms
step:1743/1920 train_time:104018ms step_avg:59.68ms
step:1744/1920 train_time:104106ms step_avg:59.69ms
step:1745/1920 train_time:104195ms step_avg:59.71ms
step:1746/1920 train_time:104282ms step_avg:59.73ms
step:1747/1920 train_time:104371ms step_avg:59.74ms
step:1748/1920 train_time:104460ms step_avg:59.76ms
step:1749/1920 train_time:104549ms step_avg:59.78ms
step:1750/1920 train_time:104637ms step_avg:59.79ms
step:1750/1920 val_loss:3.3257 train_time:104728ms step_avg:59.84ms
step:1751/1920 train_time:104746ms step_avg:59.82ms
step:1752/1920 train_time:104818ms step_avg:59.83ms
step:1753/1920 train_time:104912ms step_avg:59.85ms
step:1754/1920 train_time:105000ms step_avg:59.86ms
step:1755/1920 train_time:105091ms step_avg:59.88ms
step:1756/1920 train_time:105180ms step_avg:59.90ms
step:1757/1920 train_time:105266ms step_avg:59.91ms
step:1758/1920 train_time:105353ms step_avg:59.93ms
step:1759/1920 train_time:105441ms step_avg:59.94ms
step:1760/1920 train_time:105529ms step_avg:59.96ms
step:1761/1920 train_time:105617ms step_avg:59.98ms
step:1762/1920 train_time:105707ms step_avg:59.99ms
step:1763/1920 train_time:105799ms step_avg:60.01ms
step:1764/1920 train_time:105890ms step_avg:60.03ms
step:1765/1920 train_time:105980ms step_avg:60.05ms
step:1766/1920 train_time:106068ms step_avg:60.06ms
step:1767/1920 train_time:106158ms step_avg:60.08ms
step:1768/1920 train_time:106247ms step_avg:60.09ms
step:1769/1920 train_time:106334ms step_avg:60.11ms
step:1770/1920 train_time:106422ms step_avg:60.13ms
step:1771/1920 train_time:106509ms step_avg:60.14ms
step:1772/1920 train_time:106596ms step_avg:60.16ms
step:1773/1920 train_time:106686ms step_avg:60.17ms
step:1774/1920 train_time:106776ms step_avg:60.19ms
step:1775/1920 train_time:106866ms step_avg:60.21ms
step:1776/1920 train_time:106955ms step_avg:60.22ms
step:1777/1920 train_time:107044ms step_avg:60.24ms
step:1778/1920 train_time:107133ms step_avg:60.25ms
step:1779/1920 train_time:107221ms step_avg:60.27ms
step:1780/1920 train_time:107309ms step_avg:60.29ms
step:1781/1920 train_time:107396ms step_avg:60.30ms
step:1782/1920 train_time:107484ms step_avg:60.32ms
step:1783/1920 train_time:107572ms step_avg:60.33ms
step:1784/1920 train_time:107660ms step_avg:60.35ms
step:1785/1920 train_time:107750ms step_avg:60.36ms
step:1786/1920 train_time:107839ms step_avg:60.38ms
step:1787/1920 train_time:107929ms step_avg:60.40ms
step:1788/1920 train_time:108019ms step_avg:60.41ms
step:1789/1920 train_time:108108ms step_avg:60.43ms
step:1790/1920 train_time:108196ms step_avg:60.44ms
step:1791/1920 train_time:108285ms step_avg:60.46ms
step:1792/1920 train_time:108372ms step_avg:60.48ms
step:1793/1920 train_time:108460ms step_avg:60.49ms
step:1794/1920 train_time:108548ms step_avg:60.51ms
step:1795/1920 train_time:108637ms step_avg:60.52ms
step:1796/1920 train_time:108725ms step_avg:60.54ms
step:1797/1920 train_time:108814ms step_avg:60.55ms
step:1798/1920 train_time:108902ms step_avg:60.57ms
step:1799/1920 train_time:108992ms step_avg:60.58ms
step:1800/1920 train_time:109081ms step_avg:60.60ms
step:1801/1920 train_time:109170ms step_avg:60.62ms
step:1802/1920 train_time:109258ms step_avg:60.63ms
step:1803/1920 train_time:109347ms step_avg:60.65ms
step:1804/1920 train_time:109435ms step_avg:60.66ms
step:1805/1920 train_time:109523ms step_avg:60.68ms
step:1806/1920 train_time:109611ms step_avg:60.69ms
step:1807/1920 train_time:109700ms step_avg:60.71ms
step:1808/1920 train_time:109788ms step_avg:60.72ms
step:1809/1920 train_time:109877ms step_avg:60.74ms
step:1810/1920 train_time:109966ms step_avg:60.75ms
step:1811/1920 train_time:110056ms step_avg:60.77ms
step:1812/1920 train_time:110145ms step_avg:60.79ms
step:1813/1920 train_time:110233ms step_avg:60.80ms
step:1814/1920 train_time:110322ms step_avg:60.82ms
step:1815/1920 train_time:110411ms step_avg:60.83ms
step:1816/1920 train_time:110499ms step_avg:60.85ms
step:1817/1920 train_time:110588ms step_avg:60.86ms
step:1818/1920 train_time:110676ms step_avg:60.88ms
step:1819/1920 train_time:110765ms step_avg:60.89ms
step:1820/1920 train_time:110853ms step_avg:60.91ms
step:1821/1920 train_time:110942ms step_avg:60.92ms
step:1822/1920 train_time:111031ms step_avg:60.94ms
step:1823/1920 train_time:111121ms step_avg:60.95ms
step:1824/1920 train_time:111209ms step_avg:60.97ms
step:1825/1920 train_time:111298ms step_avg:60.99ms
step:1826/1920 train_time:111387ms step_avg:61.00ms
step:1827/1920 train_time:111475ms step_avg:61.02ms
step:1828/1920 train_time:111564ms step_avg:61.03ms
step:1829/1920 train_time:111652ms step_avg:61.05ms
step:1830/1920 train_time:111740ms step_avg:61.06ms
step:1831/1920 train_time:111829ms step_avg:61.08ms
step:1832/1920 train_time:111918ms step_avg:61.09ms
step:1833/1920 train_time:112008ms step_avg:61.11ms
step:1834/1920 train_time:112096ms step_avg:61.12ms
step:1835/1920 train_time:112185ms step_avg:61.14ms
step:1836/1920 train_time:112273ms step_avg:61.15ms
step:1837/1920 train_time:112361ms step_avg:61.17ms
step:1838/1920 train_time:112449ms step_avg:61.18ms
step:1839/1920 train_time:112538ms step_avg:61.20ms
step:1840/1920 train_time:112626ms step_avg:61.21ms
step:1841/1920 train_time:112714ms step_avg:61.22ms
step:1842/1920 train_time:112802ms step_avg:61.24ms
step:1843/1920 train_time:112891ms step_avg:61.25ms
step:1844/1920 train_time:112979ms step_avg:61.27ms
step:1845/1920 train_time:113069ms step_avg:61.28ms
step:1846/1920 train_time:113159ms step_avg:61.30ms
step:1847/1920 train_time:113249ms step_avg:61.32ms
step:1848/1920 train_time:113338ms step_avg:61.33ms
step:1849/1920 train_time:113427ms step_avg:61.35ms
step:1850/1920 train_time:113515ms step_avg:61.36ms
step:1851/1920 train_time:113603ms step_avg:61.37ms
step:1852/1920 train_time:113690ms step_avg:61.39ms
step:1853/1920 train_time:113779ms step_avg:61.40ms
step:1854/1920 train_time:113867ms step_avg:61.42ms
step:1855/1920 train_time:113956ms step_avg:61.43ms
step:1856/1920 train_time:114045ms step_avg:61.45ms
step:1857/1920 train_time:114133ms step_avg:61.46ms
step:1858/1920 train_time:114222ms step_avg:61.48ms
step:1859/1920 train_time:114311ms step_avg:61.49ms
step:1860/1920 train_time:114400ms step_avg:61.51ms
step:1861/1920 train_time:114490ms step_avg:61.52ms
step:1862/1920 train_time:114578ms step_avg:61.53ms
step:1863/1920 train_time:114668ms step_avg:61.55ms
step:1864/1920 train_time:114755ms step_avg:61.56ms
step:1865/1920 train_time:114844ms step_avg:61.58ms
step:1866/1920 train_time:114932ms step_avg:61.59ms
step:1867/1920 train_time:115021ms step_avg:61.61ms
step:1868/1920 train_time:115111ms step_avg:61.62ms
step:1869/1920 train_time:115199ms step_avg:61.64ms
step:1870/1920 train_time:115287ms step_avg:61.65ms
step:1871/1920 train_time:115376ms step_avg:61.67ms
step:1872/1920 train_time:115465ms step_avg:61.68ms
step:1873/1920 train_time:115553ms step_avg:61.69ms
step:1874/1920 train_time:115642ms step_avg:61.71ms
step:1875/1920 train_time:115731ms step_avg:61.72ms
step:1876/1920 train_time:115819ms step_avg:61.74ms
step:1877/1920 train_time:115908ms step_avg:61.75ms
step:1878/1920 train_time:115996ms step_avg:61.77ms
step:1879/1920 train_time:116085ms step_avg:61.78ms
step:1880/1920 train_time:116173ms step_avg:61.79ms
step:1881/1920 train_time:116264ms step_avg:61.81ms
step:1882/1920 train_time:116351ms step_avg:61.82ms
step:1883/1920 train_time:116441ms step_avg:61.84ms
step:1884/1920 train_time:116529ms step_avg:61.85ms
step:1885/1920 train_time:116617ms step_avg:61.87ms
step:1886/1920 train_time:116706ms step_avg:61.88ms
step:1887/1920 train_time:116795ms step_avg:61.89ms
step:1888/1920 train_time:116884ms step_avg:61.91ms
step:1889/1920 train_time:116972ms step_avg:61.92ms
step:1890/1920 train_time:117060ms step_avg:61.94ms
step:1891/1920 train_time:117150ms step_avg:61.95ms
step:1892/1920 train_time:117238ms step_avg:61.97ms
step:1893/1920 train_time:117329ms step_avg:61.98ms
step:1894/1920 train_time:117417ms step_avg:61.99ms
step:1895/1920 train_time:117507ms step_avg:62.01ms
step:1896/1920 train_time:117594ms step_avg:62.02ms
step:1897/1920 train_time:117683ms step_avg:62.04ms
step:1898/1920 train_time:117771ms step_avg:62.05ms
step:1899/1920 train_time:117861ms step_avg:62.06ms
step:1900/1920 train_time:117950ms step_avg:62.08ms
step:1901/1920 train_time:118039ms step_avg:62.09ms
step:1902/1920 train_time:118128ms step_avg:62.11ms
step:1903/1920 train_time:118218ms step_avg:62.12ms
step:1904/1920 train_time:118305ms step_avg:62.14ms
step:1905/1920 train_time:118395ms step_avg:62.15ms
step:1906/1920 train_time:118482ms step_avg:62.16ms
step:1907/1920 train_time:118571ms step_avg:62.18ms
step:1908/1920 train_time:118661ms step_avg:62.19ms
step:1909/1920 train_time:118750ms step_avg:62.21ms
step:1910/1920 train_time:118838ms step_avg:62.22ms
step:1911/1920 train_time:118929ms step_avg:62.23ms
step:1912/1920 train_time:119017ms step_avg:62.25ms
step:1913/1920 train_time:119107ms step_avg:62.26ms
step:1914/1920 train_time:119194ms step_avg:62.28ms
step:1915/1920 train_time:119284ms step_avg:62.29ms
step:1916/1920 train_time:119372ms step_avg:62.30ms
step:1917/1920 train_time:119462ms step_avg:62.32ms
step:1918/1920 train_time:119550ms step_avg:62.33ms
step:1919/1920 train_time:119640ms step_avg:62.34ms
step:1920/1920 train_time:119728ms step_avg:62.36ms
step:1920/1920 val_loss:3.2809 train_time:119820ms step_avg:62.41ms
peak memory allocated: 29863 MiB reserved: 44698 MiB

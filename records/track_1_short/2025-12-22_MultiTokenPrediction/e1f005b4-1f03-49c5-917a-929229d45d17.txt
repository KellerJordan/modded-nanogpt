import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:30:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    245663      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    245664      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    245665      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    245666      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    245667      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    245668      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    245669      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    245670      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    245664      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    245665      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    245666      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    245667      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    245668      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    245669      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    245670      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8345 train_time:0ms step_avg:0.03ms
step:1/1920 train_time:66ms step_avg:66.31ms
step:2/1920 train_time:88ms step_avg:44.11ms
step:3/1920 train_time:122ms step_avg:40.73ms
step:4/1920 train_time:156ms step_avg:39.03ms
step:5/1920 train_time:190ms step_avg:38.10ms
step:6/1920 train_time:257ms step_avg:42.87ms
step:7/1920 train_time:287ms step_avg:40.96ms
step:8/1920 train_time:321ms step_avg:40.10ms
step:9/1920 train_time:355ms step_avg:39.45ms
step:10/1920 train_time:389ms step_avg:38.91ms
step:11/1920 train_time:424ms step_avg:38.50ms
step:12/1920 train_time:458ms step_avg:38.13ms
step:13/1920 train_time:492ms step_avg:37.86ms
step:14/1920 train_time:526ms step_avg:37.59ms
step:15/1920 train_time:561ms step_avg:37.40ms
step:16/1920 train_time:595ms step_avg:37.19ms
step:17/1920 train_time:630ms step_avg:37.04ms
step:18/1920 train_time:664ms step_avg:36.87ms
step:19/1920 train_time:698ms step_avg:36.76ms
step:20/1920 train_time:732ms step_avg:36.62ms
step:21/1920 train_time:767ms step_avg:36.52ms
step:22/1920 train_time:801ms step_avg:36.41ms
step:23/1920 train_time:836ms step_avg:36.33ms
step:24/1920 train_time:870ms step_avg:36.24ms
step:25/1920 train_time:904ms step_avg:36.16ms
step:26/1920 train_time:938ms step_avg:36.09ms
step:27/1920 train_time:973ms step_avg:36.02ms
step:28/1920 train_time:1007ms step_avg:35.96ms
step:29/1920 train_time:1041ms step_avg:35.91ms
step:30/1920 train_time:1075ms step_avg:35.85ms
step:31/1920 train_time:1110ms step_avg:35.81ms
step:32/1920 train_time:1144ms step_avg:35.76ms
step:33/1920 train_time:1179ms step_avg:35.73ms
step:34/1920 train_time:1214ms step_avg:35.69ms
step:35/1920 train_time:1249ms step_avg:35.68ms
step:36/1920 train_time:1283ms step_avg:35.64ms
step:37/1920 train_time:1318ms step_avg:35.62ms
step:38/1920 train_time:1352ms step_avg:35.58ms
step:39/1920 train_time:1387ms step_avg:35.56ms
step:40/1920 train_time:1421ms step_avg:35.53ms
step:41/1920 train_time:1456ms step_avg:35.51ms
step:42/1920 train_time:1490ms step_avg:35.48ms
step:43/1920 train_time:1525ms step_avg:35.47ms
step:44/1920 train_time:1559ms step_avg:35.44ms
step:45/1920 train_time:1594ms step_avg:35.42ms
step:46/1920 train_time:1628ms step_avg:35.40ms
step:47/1920 train_time:1662ms step_avg:35.37ms
step:48/1920 train_time:1697ms step_avg:35.35ms
step:49/1920 train_time:1731ms step_avg:35.33ms
step:50/1920 train_time:1765ms step_avg:35.31ms
step:51/1920 train_time:1800ms step_avg:35.30ms
step:52/1920 train_time:1834ms step_avg:35.27ms
step:53/1920 train_time:1869ms step_avg:35.26ms
step:54/1920 train_time:1903ms step_avg:35.24ms
step:55/1920 train_time:1937ms step_avg:35.22ms
step:56/1920 train_time:1971ms step_avg:35.20ms
step:57/1920 train_time:2006ms step_avg:35.19ms
step:58/1920 train_time:2040ms step_avg:35.18ms
step:59/1920 train_time:2075ms step_avg:35.16ms
step:60/1920 train_time:2109ms step_avg:35.14ms
step:61/1920 train_time:2143ms step_avg:35.14ms
step:62/1920 train_time:2177ms step_avg:35.12ms
step:63/1920 train_time:2212ms step_avg:35.12ms
step:64/1920 train_time:2247ms step_avg:35.10ms
step:65/1920 train_time:2281ms step_avg:35.10ms
step:66/1920 train_time:2316ms step_avg:35.09ms
step:67/1920 train_time:2350ms step_avg:35.08ms
step:68/1920 train_time:2384ms step_avg:35.06ms
step:69/1920 train_time:2419ms step_avg:35.06ms
step:70/1920 train_time:2453ms step_avg:35.04ms
step:71/1920 train_time:2488ms step_avg:35.04ms
step:72/1920 train_time:2522ms step_avg:35.03ms
step:73/1920 train_time:2557ms step_avg:35.02ms
step:74/1920 train_time:2591ms step_avg:35.02ms
step:75/1920 train_time:2626ms step_avg:35.02ms
step:76/1920 train_time:2660ms step_avg:35.01ms
step:77/1920 train_time:2695ms step_avg:35.00ms
step:78/1920 train_time:2729ms step_avg:34.99ms
step:79/1920 train_time:2764ms step_avg:34.99ms
step:80/1920 train_time:2798ms step_avg:34.98ms
step:81/1920 train_time:2833ms step_avg:34.97ms
step:82/1920 train_time:2867ms step_avg:34.96ms
step:83/1920 train_time:2902ms step_avg:34.96ms
step:84/1920 train_time:2936ms step_avg:34.95ms
step:85/1920 train_time:2971ms step_avg:34.95ms
step:86/1920 train_time:3005ms step_avg:34.94ms
step:87/1920 train_time:3040ms step_avg:34.94ms
step:88/1920 train_time:3074ms step_avg:34.93ms
step:89/1920 train_time:3108ms step_avg:34.92ms
step:90/1920 train_time:3142ms step_avg:34.91ms
step:91/1920 train_time:3177ms step_avg:34.91ms
step:92/1920 train_time:3211ms step_avg:34.90ms
step:93/1920 train_time:3246ms step_avg:34.90ms
step:94/1920 train_time:3280ms step_avg:34.89ms
step:95/1920 train_time:3314ms step_avg:34.89ms
step:96/1920 train_time:3349ms step_avg:34.88ms
step:97/1920 train_time:3383ms step_avg:34.88ms
step:98/1920 train_time:3418ms step_avg:34.87ms
step:99/1920 train_time:3452ms step_avg:34.87ms
step:100/1920 train_time:3486ms step_avg:34.86ms
step:101/1920 train_time:3521ms step_avg:34.86ms
step:102/1920 train_time:3555ms step_avg:34.85ms
step:103/1920 train_time:3590ms step_avg:34.85ms
step:104/1920 train_time:3624ms step_avg:34.85ms
step:105/1920 train_time:3659ms step_avg:34.85ms
step:106/1920 train_time:3693ms step_avg:34.84ms
step:107/1920 train_time:3728ms step_avg:34.84ms
step:108/1920 train_time:3762ms step_avg:34.84ms
step:109/1920 train_time:3797ms step_avg:34.83ms
step:110/1920 train_time:3831ms step_avg:34.83ms
step:111/1920 train_time:3866ms step_avg:34.83ms
step:112/1920 train_time:3900ms step_avg:34.82ms
step:113/1920 train_time:3935ms step_avg:34.82ms
step:114/1920 train_time:3969ms step_avg:34.81ms
step:115/1920 train_time:4003ms step_avg:34.81ms
step:116/1920 train_time:4037ms step_avg:34.81ms
step:117/1920 train_time:4072ms step_avg:34.80ms
step:118/1920 train_time:4106ms step_avg:34.80ms
step:119/1920 train_time:4141ms step_avg:34.80ms
step:120/1920 train_time:4175ms step_avg:34.79ms
step:121/1920 train_time:4209ms step_avg:34.79ms
step:122/1920 train_time:4244ms step_avg:34.78ms
step:123/1920 train_time:4278ms step_avg:34.78ms
step:124/1920 train_time:4312ms step_avg:34.77ms
step:125/1920 train_time:4347ms step_avg:34.77ms
step:126/1920 train_time:4381ms step_avg:34.77ms
step:127/1920 train_time:4416ms step_avg:34.77ms
step:128/1920 train_time:4450ms step_avg:34.77ms
step:129/1920 train_time:4485ms step_avg:34.77ms
step:130/1920 train_time:4519ms step_avg:34.76ms
step:131/1920 train_time:4554ms step_avg:34.76ms
step:132/1920 train_time:4588ms step_avg:34.76ms
step:133/1920 train_time:4622ms step_avg:34.75ms
step:134/1920 train_time:4656ms step_avg:34.75ms
step:135/1920 train_time:4691ms step_avg:34.75ms
step:136/1920 train_time:4725ms step_avg:34.74ms
step:137/1920 train_time:4759ms step_avg:34.74ms
step:138/1920 train_time:4793ms step_avg:34.73ms
step:139/1920 train_time:4828ms step_avg:34.73ms
step:140/1920 train_time:4863ms step_avg:34.73ms
step:141/1920 train_time:4897ms step_avg:34.73ms
step:142/1920 train_time:4931ms step_avg:34.72ms
step:143/1920 train_time:4966ms step_avg:34.73ms
step:144/1920 train_time:5000ms step_avg:34.72ms
step:145/1920 train_time:5034ms step_avg:34.72ms
step:146/1920 train_time:5068ms step_avg:34.72ms
step:147/1920 train_time:5103ms step_avg:34.71ms
step:148/1920 train_time:5137ms step_avg:34.71ms
step:149/1920 train_time:5171ms step_avg:34.71ms
step:150/1920 train_time:5206ms step_avg:34.71ms
step:151/1920 train_time:5240ms step_avg:34.70ms
step:152/1920 train_time:5274ms step_avg:34.70ms
step:153/1920 train_time:5309ms step_avg:34.70ms
step:154/1920 train_time:5343ms step_avg:34.70ms
step:155/1920 train_time:5377ms step_avg:34.69ms
step:156/1920 train_time:5411ms step_avg:34.69ms
step:157/1920 train_time:5446ms step_avg:34.69ms
step:158/1920 train_time:5480ms step_avg:34.68ms
step:159/1920 train_time:5515ms step_avg:34.68ms
step:160/1920 train_time:5549ms step_avg:34.68ms
step:161/1920 train_time:5584ms step_avg:34.68ms
step:162/1920 train_time:5618ms step_avg:34.68ms
step:163/1920 train_time:5653ms step_avg:34.68ms
step:164/1920 train_time:5687ms step_avg:34.68ms
step:165/1920 train_time:5721ms step_avg:34.68ms
step:166/1920 train_time:5756ms step_avg:34.67ms
step:167/1920 train_time:5791ms step_avg:34.67ms
step:168/1920 train_time:5825ms step_avg:34.67ms
step:169/1920 train_time:5859ms step_avg:34.67ms
step:170/1920 train_time:5893ms step_avg:34.67ms
step:171/1920 train_time:5928ms step_avg:34.67ms
step:172/1920 train_time:5962ms step_avg:34.67ms
step:173/1920 train_time:5997ms step_avg:34.66ms
step:174/1920 train_time:6031ms step_avg:34.66ms
step:175/1920 train_time:6066ms step_avg:34.66ms
step:176/1920 train_time:6100ms step_avg:34.66ms
step:177/1920 train_time:6134ms step_avg:34.66ms
step:178/1920 train_time:6168ms step_avg:34.65ms
step:179/1920 train_time:6203ms step_avg:34.65ms
step:180/1920 train_time:6237ms step_avg:34.65ms
step:181/1920 train_time:6272ms step_avg:34.65ms
step:182/1920 train_time:6306ms step_avg:34.65ms
step:183/1920 train_time:6341ms step_avg:34.65ms
step:184/1920 train_time:6375ms step_avg:34.65ms
step:185/1920 train_time:6409ms step_avg:34.65ms
step:186/1920 train_time:6444ms step_avg:34.64ms
step:187/1920 train_time:6478ms step_avg:34.64ms
step:188/1920 train_time:6512ms step_avg:34.64ms
step:189/1920 train_time:6548ms step_avg:34.64ms
step:190/1920 train_time:6582ms step_avg:34.64ms
step:191/1920 train_time:6616ms step_avg:34.64ms
step:192/1920 train_time:6651ms step_avg:34.64ms
step:193/1920 train_time:6685ms step_avg:34.64ms
step:194/1920 train_time:6719ms step_avg:34.63ms
step:195/1920 train_time:6754ms step_avg:34.64ms
step:196/1920 train_time:6788ms step_avg:34.63ms
step:197/1920 train_time:6823ms step_avg:34.63ms
step:198/1920 train_time:6857ms step_avg:34.63ms
step:199/1920 train_time:6892ms step_avg:34.63ms
step:200/1920 train_time:6926ms step_avg:34.63ms
step:201/1920 train_time:6961ms step_avg:34.63ms
step:202/1920 train_time:6995ms step_avg:34.63ms
step:203/1920 train_time:7030ms step_avg:34.63ms
step:204/1920 train_time:7064ms step_avg:34.63ms
step:205/1920 train_time:7098ms step_avg:34.62ms
step:206/1920 train_time:7132ms step_avg:34.62ms
step:207/1920 train_time:7167ms step_avg:34.62ms
step:208/1920 train_time:7201ms step_avg:34.62ms
step:209/1920 train_time:7235ms step_avg:34.62ms
step:210/1920 train_time:7269ms step_avg:34.62ms
step:211/1920 train_time:7304ms step_avg:34.62ms
step:212/1920 train_time:7338ms step_avg:34.61ms
step:213/1920 train_time:7373ms step_avg:34.61ms
step:214/1920 train_time:7407ms step_avg:34.61ms
step:215/1920 train_time:7441ms step_avg:34.61ms
step:216/1920 train_time:7476ms step_avg:34.61ms
step:217/1920 train_time:7510ms step_avg:34.61ms
step:218/1920 train_time:7544ms step_avg:34.61ms
step:219/1920 train_time:7579ms step_avg:34.61ms
step:220/1920 train_time:7613ms step_avg:34.60ms
step:221/1920 train_time:7648ms step_avg:34.61ms
step:222/1920 train_time:7682ms step_avg:34.61ms
step:223/1920 train_time:7717ms step_avg:34.60ms
step:224/1920 train_time:7751ms step_avg:34.60ms
step:225/1920 train_time:7786ms step_avg:34.60ms
step:226/1920 train_time:7820ms step_avg:34.60ms
step:227/1920 train_time:7855ms step_avg:34.60ms
step:228/1920 train_time:7889ms step_avg:34.60ms
step:229/1920 train_time:7924ms step_avg:34.60ms
step:230/1920 train_time:7958ms step_avg:34.60ms
step:231/1920 train_time:7993ms step_avg:34.60ms
step:232/1920 train_time:8027ms step_avg:34.60ms
step:233/1920 train_time:8061ms step_avg:34.60ms
step:234/1920 train_time:8095ms step_avg:34.60ms
step:235/1920 train_time:8130ms step_avg:34.60ms
step:236/1920 train_time:8164ms step_avg:34.59ms
step:237/1920 train_time:8198ms step_avg:34.59ms
step:238/1920 train_time:8232ms step_avg:34.59ms
step:239/1920 train_time:8267ms step_avg:34.59ms
step:240/1920 train_time:8301ms step_avg:34.59ms
step:241/1920 train_time:8336ms step_avg:34.59ms
step:242/1920 train_time:8370ms step_avg:34.59ms
step:243/1920 train_time:8404ms step_avg:34.58ms
step:244/1920 train_time:8438ms step_avg:34.58ms
step:245/1920 train_time:8473ms step_avg:34.58ms
step:246/1920 train_time:8507ms step_avg:34.58ms
step:247/1920 train_time:8542ms step_avg:34.58ms
step:248/1920 train_time:8576ms step_avg:34.58ms
step:249/1920 train_time:8611ms step_avg:34.58ms
step:250/1920 train_time:8645ms step_avg:34.58ms
step:250/1920 val_loss:4.6125 train_time:8682ms step_avg:34.73ms
step:251/1920 train_time:8700ms step_avg:34.66ms
step:252/1920 train_time:8718ms step_avg:34.59ms
step:253/1920 train_time:8753ms step_avg:34.59ms
step:254/1920 train_time:8787ms step_avg:34.60ms
step:255/1920 train_time:8824ms step_avg:34.60ms
step:256/1920 train_time:8859ms step_avg:34.60ms
step:257/1920 train_time:8894ms step_avg:34.61ms
step:258/1920 train_time:8928ms step_avg:34.60ms
step:259/1920 train_time:8962ms step_avg:34.60ms
step:260/1920 train_time:8996ms step_avg:34.60ms
step:261/1920 train_time:9031ms step_avg:34.60ms
step:262/1920 train_time:9065ms step_avg:34.60ms
step:263/1920 train_time:9099ms step_avg:34.60ms
step:264/1920 train_time:9133ms step_avg:34.60ms
step:265/1920 train_time:9168ms step_avg:34.60ms
step:266/1920 train_time:9202ms step_avg:34.59ms
step:267/1920 train_time:9236ms step_avg:34.59ms
step:268/1920 train_time:9270ms step_avg:34.59ms
step:269/1920 train_time:9305ms step_avg:34.59ms
step:270/1920 train_time:9339ms step_avg:34.59ms
step:271/1920 train_time:9373ms step_avg:34.59ms
step:272/1920 train_time:9407ms step_avg:34.58ms
step:273/1920 train_time:9441ms step_avg:34.58ms
step:274/1920 train_time:9475ms step_avg:34.58ms
step:275/1920 train_time:9510ms step_avg:34.58ms
step:276/1920 train_time:9544ms step_avg:34.58ms
step:277/1920 train_time:9578ms step_avg:34.58ms
step:278/1920 train_time:9612ms step_avg:34.58ms
step:279/1920 train_time:9647ms step_avg:34.58ms
step:280/1920 train_time:9681ms step_avg:34.57ms
step:281/1920 train_time:9716ms step_avg:34.58ms
step:282/1920 train_time:9750ms step_avg:34.58ms
step:283/1920 train_time:9785ms step_avg:34.58ms
step:284/1920 train_time:9820ms step_avg:34.58ms
step:285/1920 train_time:9854ms step_avg:34.58ms
step:286/1920 train_time:9889ms step_avg:34.58ms
step:287/1920 train_time:9923ms step_avg:34.58ms
step:288/1920 train_time:9958ms step_avg:34.58ms
step:289/1920 train_time:9992ms step_avg:34.57ms
step:290/1920 train_time:10026ms step_avg:34.57ms
step:291/1920 train_time:10060ms step_avg:34.57ms
step:292/1920 train_time:10094ms step_avg:34.57ms
step:293/1920 train_time:10129ms step_avg:34.57ms
step:294/1920 train_time:10163ms step_avg:34.57ms
step:295/1920 train_time:10197ms step_avg:34.57ms
step:296/1920 train_time:10231ms step_avg:34.56ms
step:297/1920 train_time:10265ms step_avg:34.56ms
step:298/1920 train_time:10300ms step_avg:34.56ms
step:299/1920 train_time:10334ms step_avg:34.56ms
step:300/1920 train_time:10368ms step_avg:34.56ms
step:301/1920 train_time:10403ms step_avg:34.56ms
step:302/1920 train_time:10437ms step_avg:34.56ms
step:303/1920 train_time:10471ms step_avg:34.56ms
step:304/1920 train_time:10505ms step_avg:34.56ms
step:305/1920 train_time:10539ms step_avg:34.56ms
step:306/1920 train_time:10573ms step_avg:34.55ms
step:307/1920 train_time:10608ms step_avg:34.55ms
step:308/1920 train_time:10642ms step_avg:34.55ms
step:309/1920 train_time:10677ms step_avg:34.55ms
step:310/1920 train_time:10711ms step_avg:34.55ms
step:311/1920 train_time:10746ms step_avg:34.55ms
step:312/1920 train_time:10780ms step_avg:34.55ms
step:313/1920 train_time:10815ms step_avg:34.55ms
step:314/1920 train_time:10849ms step_avg:34.55ms
step:315/1920 train_time:10884ms step_avg:34.55ms
step:316/1920 train_time:10918ms step_avg:34.55ms
step:317/1920 train_time:10952ms step_avg:34.55ms
step:318/1920 train_time:10986ms step_avg:34.55ms
step:319/1920 train_time:11021ms step_avg:34.55ms
step:320/1920 train_time:11055ms step_avg:34.55ms
step:321/1920 train_time:11090ms step_avg:34.55ms
step:322/1920 train_time:11124ms step_avg:34.55ms
step:323/1920 train_time:11158ms step_avg:34.54ms
step:324/1920 train_time:11192ms step_avg:34.54ms
step:325/1920 train_time:11227ms step_avg:34.54ms
step:326/1920 train_time:11261ms step_avg:34.54ms
step:327/1920 train_time:11295ms step_avg:34.54ms
step:328/1920 train_time:11329ms step_avg:34.54ms
step:329/1920 train_time:11364ms step_avg:34.54ms
step:330/1920 train_time:11398ms step_avg:34.54ms
step:331/1920 train_time:11432ms step_avg:34.54ms
step:332/1920 train_time:11466ms step_avg:34.54ms
step:333/1920 train_time:11500ms step_avg:34.54ms
step:334/1920 train_time:11534ms step_avg:34.53ms
step:335/1920 train_time:11569ms step_avg:34.53ms
step:336/1920 train_time:11603ms step_avg:34.53ms
step:337/1920 train_time:11637ms step_avg:34.53ms
step:338/1920 train_time:11672ms step_avg:34.53ms
step:339/1920 train_time:11706ms step_avg:34.53ms
step:340/1920 train_time:11740ms step_avg:34.53ms
step:341/1920 train_time:11775ms step_avg:34.53ms
step:342/1920 train_time:11809ms step_avg:34.53ms
step:343/1920 train_time:11844ms step_avg:34.53ms
step:344/1920 train_time:11878ms step_avg:34.53ms
step:345/1920 train_time:11913ms step_avg:34.53ms
step:346/1920 train_time:11947ms step_avg:34.53ms
step:347/1920 train_time:11982ms step_avg:34.53ms
step:348/1920 train_time:12016ms step_avg:34.53ms
step:349/1920 train_time:12051ms step_avg:34.53ms
step:350/1920 train_time:12085ms step_avg:34.53ms
step:351/1920 train_time:12119ms step_avg:34.53ms
step:352/1920 train_time:12153ms step_avg:34.53ms
step:353/1920 train_time:12188ms step_avg:34.53ms
step:354/1920 train_time:12223ms step_avg:34.53ms
step:355/1920 train_time:12257ms step_avg:34.53ms
step:356/1920 train_time:12291ms step_avg:34.53ms
step:357/1920 train_time:12326ms step_avg:34.53ms
step:358/1920 train_time:12360ms step_avg:34.53ms
step:359/1920 train_time:12395ms step_avg:34.53ms
step:360/1920 train_time:12429ms step_avg:34.52ms
step:361/1920 train_time:12463ms step_avg:34.52ms
step:362/1920 train_time:12497ms step_avg:34.52ms
step:363/1920 train_time:12532ms step_avg:34.52ms
step:364/1920 train_time:12566ms step_avg:34.52ms
step:365/1920 train_time:12601ms step_avg:34.52ms
step:366/1920 train_time:12635ms step_avg:34.52ms
step:367/1920 train_time:12669ms step_avg:34.52ms
step:368/1920 train_time:12703ms step_avg:34.52ms
step:369/1920 train_time:12738ms step_avg:34.52ms
step:370/1920 train_time:12772ms step_avg:34.52ms
step:371/1920 train_time:12807ms step_avg:34.52ms
step:372/1920 train_time:12841ms step_avg:34.52ms
step:373/1920 train_time:12875ms step_avg:34.52ms
step:374/1920 train_time:12910ms step_avg:34.52ms
step:375/1920 train_time:12944ms step_avg:34.52ms
step:376/1920 train_time:12978ms step_avg:34.52ms
step:377/1920 train_time:13013ms step_avg:34.52ms
step:378/1920 train_time:13047ms step_avg:34.52ms
step:379/1920 train_time:13081ms step_avg:34.52ms
step:380/1920 train_time:13116ms step_avg:34.51ms
step:381/1920 train_time:13150ms step_avg:34.52ms
step:382/1920 train_time:13185ms step_avg:34.51ms
step:383/1920 train_time:13219ms step_avg:34.52ms
step:384/1920 train_time:13254ms step_avg:34.51ms
step:385/1920 train_time:13289ms step_avg:34.52ms
step:386/1920 train_time:13323ms step_avg:34.52ms
step:387/1920 train_time:13357ms step_avg:34.51ms
step:388/1920 train_time:13391ms step_avg:34.51ms
step:389/1920 train_time:13426ms step_avg:34.51ms
step:390/1920 train_time:13460ms step_avg:34.51ms
step:391/1920 train_time:13494ms step_avg:34.51ms
step:392/1920 train_time:13528ms step_avg:34.51ms
step:393/1920 train_time:13562ms step_avg:34.51ms
step:394/1920 train_time:13596ms step_avg:34.51ms
step:395/1920 train_time:13631ms step_avg:34.51ms
step:396/1920 train_time:13665ms step_avg:34.51ms
step:397/1920 train_time:13699ms step_avg:34.51ms
step:398/1920 train_time:13734ms step_avg:34.51ms
step:399/1920 train_time:13768ms step_avg:34.51ms
step:400/1920 train_time:13802ms step_avg:34.50ms
step:401/1920 train_time:13837ms step_avg:34.51ms
step:402/1920 train_time:13871ms step_avg:34.50ms
step:403/1920 train_time:13905ms step_avg:34.50ms
step:404/1920 train_time:13939ms step_avg:34.50ms
step:405/1920 train_time:13974ms step_avg:34.50ms
step:406/1920 train_time:14008ms step_avg:34.50ms
step:407/1920 train_time:14043ms step_avg:34.50ms
step:408/1920 train_time:14077ms step_avg:34.50ms
step:409/1920 train_time:14112ms step_avg:34.50ms
step:410/1920 train_time:14146ms step_avg:34.50ms
step:411/1920 train_time:14180ms step_avg:34.50ms
step:412/1920 train_time:14214ms step_avg:34.50ms
step:413/1920 train_time:14249ms step_avg:34.50ms
step:414/1920 train_time:14283ms step_avg:34.50ms
step:415/1920 train_time:14318ms step_avg:34.50ms
step:416/1920 train_time:14352ms step_avg:34.50ms
step:417/1920 train_time:14386ms step_avg:34.50ms
step:418/1920 train_time:14421ms step_avg:34.50ms
step:419/1920 train_time:14455ms step_avg:34.50ms
step:420/1920 train_time:14489ms step_avg:34.50ms
step:421/1920 train_time:14523ms step_avg:34.50ms
step:422/1920 train_time:14558ms step_avg:34.50ms
step:423/1920 train_time:14592ms step_avg:34.50ms
step:424/1920 train_time:14626ms step_avg:34.50ms
step:425/1920 train_time:14661ms step_avg:34.50ms
step:426/1920 train_time:14695ms step_avg:34.49ms
step:427/1920 train_time:14729ms step_avg:34.49ms
step:428/1920 train_time:14763ms step_avg:34.49ms
step:429/1920 train_time:14798ms step_avg:34.49ms
step:430/1920 train_time:14832ms step_avg:34.49ms
step:431/1920 train_time:14866ms step_avg:34.49ms
step:432/1920 train_time:14900ms step_avg:34.49ms
step:433/1920 train_time:14934ms step_avg:34.49ms
step:434/1920 train_time:14968ms step_avg:34.49ms
step:435/1920 train_time:15003ms step_avg:34.49ms
step:436/1920 train_time:15037ms step_avg:34.49ms
step:437/1920 train_time:15072ms step_avg:34.49ms
step:438/1920 train_time:15106ms step_avg:34.49ms
step:439/1920 train_time:15140ms step_avg:34.49ms
step:440/1920 train_time:15174ms step_avg:34.49ms
step:441/1920 train_time:15209ms step_avg:34.49ms
step:442/1920 train_time:15243ms step_avg:34.49ms
step:443/1920 train_time:15278ms step_avg:34.49ms
step:444/1920 train_time:15312ms step_avg:34.49ms
step:445/1920 train_time:15347ms step_avg:34.49ms
step:446/1920 train_time:15381ms step_avg:34.49ms
step:447/1920 train_time:15416ms step_avg:34.49ms
step:448/1920 train_time:15450ms step_avg:34.49ms
step:449/1920 train_time:15485ms step_avg:34.49ms
step:450/1920 train_time:15519ms step_avg:34.49ms
step:451/1920 train_time:15553ms step_avg:34.49ms
step:452/1920 train_time:15587ms step_avg:34.49ms
step:453/1920 train_time:15622ms step_avg:34.48ms
step:454/1920 train_time:15656ms step_avg:34.48ms
step:455/1920 train_time:15690ms step_avg:34.48ms
step:456/1920 train_time:15725ms step_avg:34.48ms
step:457/1920 train_time:15759ms step_avg:34.48ms
step:458/1920 train_time:15793ms step_avg:34.48ms
step:459/1920 train_time:15828ms step_avg:34.48ms
step:460/1920 train_time:15862ms step_avg:34.48ms
step:461/1920 train_time:15896ms step_avg:34.48ms
step:462/1920 train_time:15930ms step_avg:34.48ms
step:463/1920 train_time:15965ms step_avg:34.48ms
step:464/1920 train_time:15999ms step_avg:34.48ms
step:465/1920 train_time:16033ms step_avg:34.48ms
step:466/1920 train_time:16067ms step_avg:34.48ms
step:467/1920 train_time:16101ms step_avg:34.48ms
step:468/1920 train_time:16135ms step_avg:34.48ms
step:469/1920 train_time:16170ms step_avg:34.48ms
step:470/1920 train_time:16204ms step_avg:34.48ms
step:471/1920 train_time:16239ms step_avg:34.48ms
step:472/1920 train_time:16273ms step_avg:34.48ms
step:473/1920 train_time:16308ms step_avg:34.48ms
step:474/1920 train_time:16343ms step_avg:34.48ms
step:475/1920 train_time:16377ms step_avg:34.48ms
step:476/1920 train_time:16411ms step_avg:34.48ms
step:477/1920 train_time:16446ms step_avg:34.48ms
step:478/1920 train_time:16480ms step_avg:34.48ms
step:479/1920 train_time:16515ms step_avg:34.48ms
step:480/1920 train_time:16549ms step_avg:34.48ms
step:481/1920 train_time:16584ms step_avg:34.48ms
step:482/1920 train_time:16618ms step_avg:34.48ms
step:483/1920 train_time:16653ms step_avg:34.48ms
step:484/1920 train_time:16687ms step_avg:34.48ms
step:485/1920 train_time:16721ms step_avg:34.48ms
step:486/1920 train_time:16755ms step_avg:34.48ms
step:487/1920 train_time:16790ms step_avg:34.48ms
step:488/1920 train_time:16824ms step_avg:34.48ms
step:489/1920 train_time:16858ms step_avg:34.48ms
step:490/1920 train_time:16893ms step_avg:34.47ms
step:491/1920 train_time:16927ms step_avg:34.47ms
step:492/1920 train_time:16961ms step_avg:34.47ms
step:493/1920 train_time:16995ms step_avg:34.47ms
step:494/1920 train_time:17029ms step_avg:34.47ms
step:495/1920 train_time:17064ms step_avg:34.47ms
step:496/1920 train_time:17098ms step_avg:34.47ms
step:497/1920 train_time:17132ms step_avg:34.47ms
step:498/1920 train_time:17167ms step_avg:34.47ms
step:499/1920 train_time:17201ms step_avg:34.47ms
step:500/1920 train_time:17235ms step_avg:34.47ms
step:500/1920 val_loss:4.2971 train_time:17272ms step_avg:34.54ms
step:501/1920 train_time:17290ms step_avg:34.51ms
step:502/1920 train_time:17308ms step_avg:34.48ms
step:503/1920 train_time:17343ms step_avg:34.48ms
step:504/1920 train_time:17377ms step_avg:34.48ms
step:505/1920 train_time:17412ms step_avg:34.48ms
step:506/1920 train_time:17447ms step_avg:34.48ms
step:507/1920 train_time:17481ms step_avg:34.48ms
step:508/1920 train_time:17515ms step_avg:34.48ms
step:509/1920 train_time:17550ms step_avg:34.48ms
step:510/1920 train_time:17584ms step_avg:34.48ms
step:511/1920 train_time:17618ms step_avg:34.48ms
step:512/1920 train_time:17652ms step_avg:34.48ms
step:513/1920 train_time:17686ms step_avg:34.48ms
step:514/1920 train_time:17720ms step_avg:34.47ms
step:515/1920 train_time:17755ms step_avg:34.47ms
step:516/1920 train_time:17789ms step_avg:34.47ms
step:517/1920 train_time:17823ms step_avg:34.47ms
step:518/1920 train_time:17857ms step_avg:34.47ms
step:519/1920 train_time:17891ms step_avg:34.47ms
step:520/1920 train_time:17925ms step_avg:34.47ms
step:521/1920 train_time:17959ms step_avg:34.47ms
step:522/1920 train_time:17993ms step_avg:34.47ms
step:523/1920 train_time:18028ms step_avg:34.47ms
step:524/1920 train_time:18062ms step_avg:34.47ms
step:525/1920 train_time:18096ms step_avg:34.47ms
step:526/1920 train_time:18130ms step_avg:34.47ms
step:527/1920 train_time:18165ms step_avg:34.47ms
step:528/1920 train_time:18199ms step_avg:34.47ms
step:529/1920 train_time:18234ms step_avg:34.47ms
step:530/1920 train_time:18269ms step_avg:34.47ms
step:531/1920 train_time:18303ms step_avg:34.47ms
step:532/1920 train_time:18337ms step_avg:34.47ms
step:533/1920 train_time:18372ms step_avg:34.47ms
step:534/1920 train_time:18407ms step_avg:34.47ms
step:535/1920 train_time:18441ms step_avg:34.47ms
step:536/1920 train_time:18476ms step_avg:34.47ms
step:537/1920 train_time:18510ms step_avg:34.47ms
step:538/1920 train_time:18545ms step_avg:34.47ms
step:539/1920 train_time:18579ms step_avg:34.47ms
step:540/1920 train_time:18613ms step_avg:34.47ms
step:541/1920 train_time:18647ms step_avg:34.47ms
step:542/1920 train_time:18682ms step_avg:34.47ms
step:543/1920 train_time:18716ms step_avg:34.47ms
step:544/1920 train_time:18750ms step_avg:34.47ms
step:545/1920 train_time:18784ms step_avg:34.47ms
step:546/1920 train_time:18818ms step_avg:34.47ms
step:547/1920 train_time:18853ms step_avg:34.47ms
step:548/1920 train_time:18887ms step_avg:34.47ms
step:549/1920 train_time:18921ms step_avg:34.47ms
step:550/1920 train_time:18955ms step_avg:34.46ms
step:551/1920 train_time:18990ms step_avg:34.46ms
step:552/1920 train_time:19024ms step_avg:34.46ms
step:553/1920 train_time:19058ms step_avg:34.46ms
step:554/1920 train_time:19092ms step_avg:34.46ms
step:555/1920 train_time:19127ms step_avg:34.46ms
step:556/1920 train_time:19161ms step_avg:34.46ms
step:557/1920 train_time:19195ms step_avg:34.46ms
step:558/1920 train_time:19230ms step_avg:34.46ms
step:559/1920 train_time:19264ms step_avg:34.46ms
step:560/1920 train_time:19298ms step_avg:34.46ms
step:561/1920 train_time:19332ms step_avg:34.46ms
step:562/1920 train_time:19367ms step_avg:34.46ms
step:563/1920 train_time:19401ms step_avg:34.46ms
step:564/1920 train_time:19435ms step_avg:34.46ms
step:565/1920 train_time:19470ms step_avg:34.46ms
step:566/1920 train_time:19504ms step_avg:34.46ms
step:567/1920 train_time:19538ms step_avg:34.46ms
step:568/1920 train_time:19572ms step_avg:34.46ms
step:569/1920 train_time:19607ms step_avg:34.46ms
step:570/1920 train_time:19641ms step_avg:34.46ms
step:571/1920 train_time:19675ms step_avg:34.46ms
step:572/1920 train_time:19709ms step_avg:34.46ms
step:573/1920 train_time:19744ms step_avg:34.46ms
step:574/1920 train_time:19778ms step_avg:34.46ms
step:575/1920 train_time:19812ms step_avg:34.46ms
step:576/1920 train_time:19846ms step_avg:34.46ms
step:577/1920 train_time:19881ms step_avg:34.46ms
step:578/1920 train_time:19915ms step_avg:34.46ms
step:579/1920 train_time:19949ms step_avg:34.46ms
step:580/1920 train_time:19984ms step_avg:34.45ms
step:581/1920 train_time:20018ms step_avg:34.45ms
step:582/1920 train_time:20052ms step_avg:34.45ms
step:583/1920 train_time:20087ms step_avg:34.45ms
step:584/1920 train_time:20120ms step_avg:34.45ms
step:585/1920 train_time:20155ms step_avg:34.45ms
step:586/1920 train_time:20189ms step_avg:34.45ms
step:587/1920 train_time:20223ms step_avg:34.45ms
step:588/1920 train_time:20258ms step_avg:34.45ms
step:589/1920 train_time:20292ms step_avg:34.45ms
step:590/1920 train_time:20326ms step_avg:34.45ms
step:591/1920 train_time:20361ms step_avg:34.45ms
step:592/1920 train_time:20395ms step_avg:34.45ms
step:593/1920 train_time:20430ms step_avg:34.45ms
step:594/1920 train_time:20464ms step_avg:34.45ms
step:595/1920 train_time:20498ms step_avg:34.45ms
step:596/1920 train_time:20533ms step_avg:34.45ms
step:597/1920 train_time:20567ms step_avg:34.45ms
step:598/1920 train_time:20601ms step_avg:34.45ms
step:599/1920 train_time:20636ms step_avg:34.45ms
step:600/1920 train_time:20670ms step_avg:34.45ms
step:601/1920 train_time:20704ms step_avg:34.45ms
step:602/1920 train_time:20738ms step_avg:34.45ms
step:603/1920 train_time:20773ms step_avg:34.45ms
step:604/1920 train_time:20807ms step_avg:34.45ms
step:605/1920 train_time:20842ms step_avg:34.45ms
step:606/1920 train_time:20876ms step_avg:34.45ms
step:607/1920 train_time:20910ms step_avg:34.45ms
step:608/1920 train_time:20944ms step_avg:34.45ms
step:609/1920 train_time:20979ms step_avg:34.45ms
step:610/1920 train_time:21013ms step_avg:34.45ms
step:611/1920 train_time:21047ms step_avg:34.45ms
step:612/1920 train_time:21081ms step_avg:34.45ms
step:613/1920 train_time:21115ms step_avg:34.45ms
step:614/1920 train_time:21150ms step_avg:34.45ms
step:615/1920 train_time:21184ms step_avg:34.45ms
step:616/1920 train_time:21219ms step_avg:34.45ms
step:617/1920 train_time:21253ms step_avg:34.45ms
step:618/1920 train_time:21288ms step_avg:34.45ms
step:619/1920 train_time:21322ms step_avg:34.45ms
step:620/1920 train_time:21356ms step_avg:34.45ms
step:621/1920 train_time:21391ms step_avg:34.45ms
step:622/1920 train_time:21425ms step_avg:34.45ms
step:623/1920 train_time:21460ms step_avg:34.45ms
step:624/1920 train_time:21494ms step_avg:34.45ms
step:625/1920 train_time:21528ms step_avg:34.45ms
step:626/1920 train_time:21563ms step_avg:34.44ms
step:627/1920 train_time:21597ms step_avg:34.44ms
step:628/1920 train_time:21632ms step_avg:34.45ms
step:629/1920 train_time:21693ms step_avg:34.49ms
step:630/1920 train_time:21755ms step_avg:34.53ms
step:631/1920 train_time:21817ms step_avg:34.58ms
step:632/1920 train_time:21879ms step_avg:34.62ms
step:633/1920 train_time:21941ms step_avg:34.66ms
step:634/1920 train_time:22003ms step_avg:34.71ms
step:635/1920 train_time:22066ms step_avg:34.75ms
step:636/1920 train_time:22127ms step_avg:34.79ms
step:637/1920 train_time:22190ms step_avg:34.83ms
step:638/1920 train_time:22251ms step_avg:34.88ms
step:639/1920 train_time:22314ms step_avg:34.92ms
step:640/1920 train_time:22375ms step_avg:34.96ms
step:641/1920 train_time:22438ms step_avg:35.00ms
step:642/1920 train_time:22500ms step_avg:35.05ms
step:643/1920 train_time:22563ms step_avg:35.09ms
step:644/1920 train_time:22625ms step_avg:35.13ms
step:645/1920 train_time:22688ms step_avg:35.18ms
step:646/1920 train_time:22750ms step_avg:35.22ms
step:647/1920 train_time:22812ms step_avg:35.26ms
step:648/1920 train_time:22873ms step_avg:35.30ms
step:649/1920 train_time:22936ms step_avg:35.34ms
step:650/1920 train_time:22998ms step_avg:35.38ms
step:651/1920 train_time:23061ms step_avg:35.42ms
step:652/1920 train_time:23123ms step_avg:35.46ms
step:653/1920 train_time:23185ms step_avg:35.51ms
step:654/1920 train_time:23247ms step_avg:35.55ms
step:655/1920 train_time:23310ms step_avg:35.59ms
step:656/1920 train_time:23371ms step_avg:35.63ms
step:657/1920 train_time:23434ms step_avg:35.67ms
step:658/1920 train_time:23496ms step_avg:35.71ms
step:659/1920 train_time:23558ms step_avg:35.75ms
step:660/1920 train_time:23620ms step_avg:35.79ms
step:661/1920 train_time:23683ms step_avg:35.83ms
step:662/1920 train_time:23746ms step_avg:35.87ms
step:663/1920 train_time:23808ms step_avg:35.91ms
step:664/1920 train_time:23870ms step_avg:35.95ms
step:665/1920 train_time:23932ms step_avg:35.99ms
step:666/1920 train_time:23994ms step_avg:36.03ms
step:667/1920 train_time:24056ms step_avg:36.07ms
step:668/1920 train_time:24118ms step_avg:36.10ms
step:669/1920 train_time:24181ms step_avg:36.15ms
step:670/1920 train_time:24244ms step_avg:36.19ms
step:671/1920 train_time:24307ms step_avg:36.22ms
step:672/1920 train_time:24369ms step_avg:36.26ms
step:673/1920 train_time:24432ms step_avg:36.30ms
step:674/1920 train_time:24494ms step_avg:36.34ms
step:675/1920 train_time:24556ms step_avg:36.38ms
step:676/1920 train_time:24618ms step_avg:36.42ms
step:677/1920 train_time:24680ms step_avg:36.46ms
step:678/1920 train_time:24742ms step_avg:36.49ms
step:679/1920 train_time:24805ms step_avg:36.53ms
step:680/1920 train_time:24867ms step_avg:36.57ms
step:681/1920 train_time:24930ms step_avg:36.61ms
step:682/1920 train_time:24991ms step_avg:36.64ms
step:683/1920 train_time:25054ms step_avg:36.68ms
step:684/1920 train_time:25116ms step_avg:36.72ms
step:685/1920 train_time:25179ms step_avg:36.76ms
step:686/1920 train_time:25240ms step_avg:36.79ms
step:687/1920 train_time:25304ms step_avg:36.83ms
step:688/1920 train_time:25366ms step_avg:36.87ms
step:689/1920 train_time:25429ms step_avg:36.91ms
step:690/1920 train_time:25491ms step_avg:36.94ms
step:691/1920 train_time:25553ms step_avg:36.98ms
step:692/1920 train_time:25616ms step_avg:37.02ms
step:693/1920 train_time:25678ms step_avg:37.05ms
step:694/1920 train_time:25740ms step_avg:37.09ms
step:695/1920 train_time:25803ms step_avg:37.13ms
step:696/1920 train_time:25865ms step_avg:37.16ms
step:697/1920 train_time:25928ms step_avg:37.20ms
step:698/1920 train_time:25990ms step_avg:37.23ms
step:699/1920 train_time:26052ms step_avg:37.27ms
step:700/1920 train_time:26114ms step_avg:37.31ms
step:701/1920 train_time:26177ms step_avg:37.34ms
step:702/1920 train_time:26238ms step_avg:37.38ms
step:703/1920 train_time:26301ms step_avg:37.41ms
step:704/1920 train_time:26364ms step_avg:37.45ms
step:705/1920 train_time:26427ms step_avg:37.49ms
step:706/1920 train_time:26489ms step_avg:37.52ms
step:707/1920 train_time:26552ms step_avg:37.56ms
step:708/1920 train_time:26613ms step_avg:37.59ms
step:709/1920 train_time:26676ms step_avg:37.62ms
step:710/1920 train_time:26738ms step_avg:37.66ms
step:711/1920 train_time:26801ms step_avg:37.69ms
step:712/1920 train_time:26863ms step_avg:37.73ms
step:713/1920 train_time:26926ms step_avg:37.76ms
step:714/1920 train_time:26988ms step_avg:37.80ms
step:715/1920 train_time:27051ms step_avg:37.83ms
step:716/1920 train_time:27113ms step_avg:37.87ms
step:717/1920 train_time:27175ms step_avg:37.90ms
step:718/1920 train_time:27237ms step_avg:37.93ms
step:719/1920 train_time:27299ms step_avg:37.97ms
step:720/1920 train_time:27362ms step_avg:38.00ms
step:721/1920 train_time:27425ms step_avg:38.04ms
step:722/1920 train_time:27487ms step_avg:38.07ms
step:723/1920 train_time:27550ms step_avg:38.11ms
step:724/1920 train_time:27612ms step_avg:38.14ms
step:725/1920 train_time:27674ms step_avg:38.17ms
step:726/1920 train_time:27736ms step_avg:38.20ms
step:727/1920 train_time:27799ms step_avg:38.24ms
step:728/1920 train_time:27861ms step_avg:38.27ms
step:729/1920 train_time:27924ms step_avg:38.30ms
step:730/1920 train_time:27986ms step_avg:38.34ms
step:731/1920 train_time:28049ms step_avg:38.37ms
step:732/1920 train_time:28111ms step_avg:38.40ms
step:733/1920 train_time:28173ms step_avg:38.44ms
step:734/1920 train_time:28235ms step_avg:38.47ms
step:735/1920 train_time:28298ms step_avg:38.50ms
step:736/1920 train_time:28359ms step_avg:38.53ms
step:737/1920 train_time:28423ms step_avg:38.57ms
step:738/1920 train_time:28485ms step_avg:38.60ms
step:739/1920 train_time:28548ms step_avg:38.63ms
step:740/1920 train_time:28610ms step_avg:38.66ms
step:741/1920 train_time:28672ms step_avg:38.69ms
step:742/1920 train_time:28735ms step_avg:38.73ms
step:743/1920 train_time:28797ms step_avg:38.76ms
step:744/1920 train_time:28858ms step_avg:38.79ms
step:745/1920 train_time:28921ms step_avg:38.82ms
step:746/1920 train_time:28983ms step_avg:38.85ms
step:747/1920 train_time:29046ms step_avg:38.88ms
step:748/1920 train_time:29108ms step_avg:38.91ms
step:749/1920 train_time:29170ms step_avg:38.95ms
step:750/1920 train_time:29232ms step_avg:38.98ms
step:750/1920 val_loss:4.0432 train_time:29297ms step_avg:39.06ms
step:751/1920 train_time:29315ms step_avg:39.03ms
step:752/1920 train_time:29357ms step_avg:39.04ms
step:753/1920 train_time:29424ms step_avg:39.08ms
step:754/1920 train_time:29491ms step_avg:39.11ms
step:755/1920 train_time:29554ms step_avg:39.14ms
step:756/1920 train_time:29616ms step_avg:39.17ms
step:757/1920 train_time:29678ms step_avg:39.21ms
step:758/1920 train_time:29739ms step_avg:39.23ms
step:759/1920 train_time:29801ms step_avg:39.26ms
step:760/1920 train_time:29862ms step_avg:39.29ms
step:761/1920 train_time:29924ms step_avg:39.32ms
step:762/1920 train_time:29985ms step_avg:39.35ms
step:763/1920 train_time:30047ms step_avg:39.38ms
step:764/1920 train_time:30108ms step_avg:39.41ms
step:765/1920 train_time:30171ms step_avg:39.44ms
step:766/1920 train_time:30235ms step_avg:39.47ms
step:767/1920 train_time:30299ms step_avg:39.50ms
step:768/1920 train_time:30362ms step_avg:39.53ms
step:769/1920 train_time:30425ms step_avg:39.56ms
step:770/1920 train_time:30488ms step_avg:39.59ms
step:771/1920 train_time:30550ms step_avg:39.62ms
step:772/1920 train_time:30612ms step_avg:39.65ms
step:773/1920 train_time:30675ms step_avg:39.68ms
step:774/1920 train_time:30736ms step_avg:39.71ms
step:775/1920 train_time:30798ms step_avg:39.74ms
step:776/1920 train_time:30859ms step_avg:39.77ms
step:777/1920 train_time:30921ms step_avg:39.80ms
step:778/1920 train_time:30983ms step_avg:39.82ms
step:779/1920 train_time:31045ms step_avg:39.85ms
step:780/1920 train_time:31106ms step_avg:39.88ms
step:781/1920 train_time:31168ms step_avg:39.91ms
step:782/1920 train_time:31232ms step_avg:39.94ms
step:783/1920 train_time:31295ms step_avg:39.97ms
step:784/1920 train_time:31358ms step_avg:40.00ms
step:785/1920 train_time:31421ms step_avg:40.03ms
step:786/1920 train_time:31483ms step_avg:40.06ms
step:787/1920 train_time:31546ms step_avg:40.08ms
step:788/1920 train_time:31608ms step_avg:40.11ms
step:789/1920 train_time:31671ms step_avg:40.14ms
step:790/1920 train_time:31732ms step_avg:40.17ms
step:791/1920 train_time:31795ms step_avg:40.20ms
step:792/1920 train_time:31857ms step_avg:40.22ms
step:793/1920 train_time:31919ms step_avg:40.25ms
step:794/1920 train_time:31980ms step_avg:40.28ms
step:795/1920 train_time:32043ms step_avg:40.31ms
step:796/1920 train_time:32104ms step_avg:40.33ms
step:797/1920 train_time:32167ms step_avg:40.36ms
step:798/1920 train_time:32229ms step_avg:40.39ms
step:799/1920 train_time:32293ms step_avg:40.42ms
step:800/1920 train_time:32355ms step_avg:40.44ms
step:801/1920 train_time:32418ms step_avg:40.47ms
step:802/1920 train_time:32479ms step_avg:40.50ms
step:803/1920 train_time:32542ms step_avg:40.53ms
step:804/1920 train_time:32604ms step_avg:40.55ms
step:805/1920 train_time:32667ms step_avg:40.58ms
step:806/1920 train_time:32729ms step_avg:40.61ms
step:807/1920 train_time:32792ms step_avg:40.63ms
step:808/1920 train_time:32854ms step_avg:40.66ms
step:809/1920 train_time:32917ms step_avg:40.69ms
step:810/1920 train_time:32978ms step_avg:40.71ms
step:811/1920 train_time:33041ms step_avg:40.74ms
step:812/1920 train_time:33102ms step_avg:40.77ms
step:813/1920 train_time:33165ms step_avg:40.79ms
step:814/1920 train_time:33226ms step_avg:40.82ms
step:815/1920 train_time:33289ms step_avg:40.85ms
step:816/1920 train_time:33352ms step_avg:40.87ms
step:817/1920 train_time:33415ms step_avg:40.90ms
step:818/1920 train_time:33477ms step_avg:40.93ms
step:819/1920 train_time:33540ms step_avg:40.95ms
step:820/1920 train_time:33602ms step_avg:40.98ms
step:821/1920 train_time:33665ms step_avg:41.00ms
step:822/1920 train_time:33726ms step_avg:41.03ms
step:823/1920 train_time:33789ms step_avg:41.06ms
step:824/1920 train_time:33852ms step_avg:41.08ms
step:825/1920 train_time:33915ms step_avg:41.11ms
step:826/1920 train_time:33976ms step_avg:41.13ms
step:827/1920 train_time:34038ms step_avg:41.16ms
step:828/1920 train_time:34100ms step_avg:41.18ms
step:829/1920 train_time:34162ms step_avg:41.21ms
step:830/1920 train_time:34224ms step_avg:41.23ms
step:831/1920 train_time:34287ms step_avg:41.26ms
step:832/1920 train_time:34348ms step_avg:41.28ms
step:833/1920 train_time:34411ms step_avg:41.31ms
step:834/1920 train_time:34473ms step_avg:41.34ms
step:835/1920 train_time:34536ms step_avg:41.36ms
step:836/1920 train_time:34598ms step_avg:41.39ms
step:837/1920 train_time:34660ms step_avg:41.41ms
step:838/1920 train_time:34722ms step_avg:41.43ms
step:839/1920 train_time:34785ms step_avg:41.46ms
step:840/1920 train_time:34847ms step_avg:41.48ms
step:841/1920 train_time:34910ms step_avg:41.51ms
step:842/1920 train_time:34972ms step_avg:41.53ms
step:843/1920 train_time:35035ms step_avg:41.56ms
step:844/1920 train_time:35096ms step_avg:41.58ms
step:845/1920 train_time:35158ms step_avg:41.61ms
step:846/1920 train_time:35220ms step_avg:41.63ms
step:847/1920 train_time:35282ms step_avg:41.66ms
step:848/1920 train_time:35345ms step_avg:41.68ms
step:849/1920 train_time:35408ms step_avg:41.71ms
step:850/1920 train_time:35471ms step_avg:41.73ms
step:851/1920 train_time:35534ms step_avg:41.76ms
step:852/1920 train_time:35596ms step_avg:41.78ms
step:853/1920 train_time:35659ms step_avg:41.80ms
step:854/1920 train_time:35721ms step_avg:41.83ms
step:855/1920 train_time:35784ms step_avg:41.85ms
step:856/1920 train_time:35845ms step_avg:41.88ms
step:857/1920 train_time:35908ms step_avg:41.90ms
step:858/1920 train_time:35970ms step_avg:41.92ms
step:859/1920 train_time:36033ms step_avg:41.95ms
step:860/1920 train_time:36096ms step_avg:41.97ms
step:861/1920 train_time:36158ms step_avg:42.00ms
step:862/1920 train_time:36219ms step_avg:42.02ms
step:863/1920 train_time:36282ms step_avg:42.04ms
step:864/1920 train_time:36344ms step_avg:42.06ms
step:865/1920 train_time:36407ms step_avg:42.09ms
step:866/1920 train_time:36469ms step_avg:42.11ms
step:867/1920 train_time:36532ms step_avg:42.14ms
step:868/1920 train_time:36594ms step_avg:42.16ms
step:869/1920 train_time:36657ms step_avg:42.18ms
step:870/1920 train_time:36719ms step_avg:42.21ms
step:871/1920 train_time:36782ms step_avg:42.23ms
step:872/1920 train_time:36844ms step_avg:42.25ms
step:873/1920 train_time:36906ms step_avg:42.28ms
step:874/1920 train_time:36969ms step_avg:42.30ms
step:875/1920 train_time:37033ms step_avg:42.32ms
step:876/1920 train_time:37094ms step_avg:42.35ms
step:877/1920 train_time:37157ms step_avg:42.37ms
step:878/1920 train_time:37218ms step_avg:42.39ms
step:879/1920 train_time:37281ms step_avg:42.41ms
step:880/1920 train_time:37343ms step_avg:42.44ms
step:881/1920 train_time:37406ms step_avg:42.46ms
step:882/1920 train_time:37467ms step_avg:42.48ms
step:883/1920 train_time:37530ms step_avg:42.50ms
step:884/1920 train_time:37593ms step_avg:42.53ms
step:885/1920 train_time:37656ms step_avg:42.55ms
step:886/1920 train_time:37717ms step_avg:42.57ms
step:887/1920 train_time:37780ms step_avg:42.59ms
step:888/1920 train_time:37842ms step_avg:42.61ms
step:889/1920 train_time:37904ms step_avg:42.64ms
step:890/1920 train_time:37966ms step_avg:42.66ms
step:891/1920 train_time:38029ms step_avg:42.68ms
step:892/1920 train_time:38091ms step_avg:42.70ms
step:893/1920 train_time:38154ms step_avg:42.73ms
step:894/1920 train_time:38216ms step_avg:42.75ms
step:895/1920 train_time:38278ms step_avg:42.77ms
step:896/1920 train_time:38340ms step_avg:42.79ms
step:897/1920 train_time:38403ms step_avg:42.81ms
step:898/1920 train_time:38464ms step_avg:42.83ms
step:899/1920 train_time:38527ms step_avg:42.86ms
step:900/1920 train_time:38590ms step_avg:42.88ms
step:901/1920 train_time:38653ms step_avg:42.90ms
step:902/1920 train_time:38715ms step_avg:42.92ms
step:903/1920 train_time:38778ms step_avg:42.94ms
step:904/1920 train_time:38840ms step_avg:42.96ms
step:905/1920 train_time:38902ms step_avg:42.99ms
step:906/1920 train_time:38964ms step_avg:43.01ms
step:907/1920 train_time:39027ms step_avg:43.03ms
step:908/1920 train_time:39090ms step_avg:43.05ms
step:909/1920 train_time:39153ms step_avg:43.07ms
step:910/1920 train_time:39215ms step_avg:43.09ms
step:911/1920 train_time:39278ms step_avg:43.11ms
step:912/1920 train_time:39339ms step_avg:43.14ms
step:913/1920 train_time:39402ms step_avg:43.16ms
step:914/1920 train_time:39463ms step_avg:43.18ms
step:915/1920 train_time:39526ms step_avg:43.20ms
step:916/1920 train_time:39588ms step_avg:43.22ms
step:917/1920 train_time:39651ms step_avg:43.24ms
step:918/1920 train_time:39713ms step_avg:43.26ms
step:919/1920 train_time:39776ms step_avg:43.28ms
step:920/1920 train_time:39838ms step_avg:43.30ms
step:921/1920 train_time:39900ms step_avg:43.32ms
step:922/1920 train_time:39962ms step_avg:43.34ms
step:923/1920 train_time:40024ms step_avg:43.36ms
step:924/1920 train_time:40086ms step_avg:43.38ms
step:925/1920 train_time:40149ms step_avg:43.40ms
step:926/1920 train_time:40212ms step_avg:43.42ms
step:927/1920 train_time:40275ms step_avg:43.45ms
step:928/1920 train_time:40337ms step_avg:43.47ms
step:929/1920 train_time:40399ms step_avg:43.49ms
step:930/1920 train_time:40461ms step_avg:43.51ms
step:931/1920 train_time:40523ms step_avg:43.53ms
step:932/1920 train_time:40585ms step_avg:43.55ms
step:933/1920 train_time:40649ms step_avg:43.57ms
step:934/1920 train_time:40711ms step_avg:43.59ms
step:935/1920 train_time:40774ms step_avg:43.61ms
step:936/1920 train_time:40836ms step_avg:43.63ms
step:937/1920 train_time:40899ms step_avg:43.65ms
step:938/1920 train_time:40960ms step_avg:43.67ms
step:939/1920 train_time:41023ms step_avg:43.69ms
step:940/1920 train_time:41085ms step_avg:43.71ms
step:941/1920 train_time:41148ms step_avg:43.73ms
step:942/1920 train_time:41210ms step_avg:43.75ms
step:943/1920 train_time:41274ms step_avg:43.77ms
step:944/1920 train_time:41336ms step_avg:43.79ms
step:945/1920 train_time:41399ms step_avg:43.81ms
step:946/1920 train_time:41461ms step_avg:43.83ms
step:947/1920 train_time:41524ms step_avg:43.85ms
step:948/1920 train_time:41586ms step_avg:43.87ms
step:949/1920 train_time:41648ms step_avg:43.89ms
step:950/1920 train_time:41710ms step_avg:43.91ms
step:951/1920 train_time:41773ms step_avg:43.93ms
step:952/1920 train_time:41836ms step_avg:43.95ms
step:953/1920 train_time:41899ms step_avg:43.97ms
step:954/1920 train_time:41961ms step_avg:43.98ms
step:955/1920 train_time:42023ms step_avg:44.00ms
step:956/1920 train_time:42085ms step_avg:44.02ms
step:957/1920 train_time:42148ms step_avg:44.04ms
step:958/1920 train_time:42210ms step_avg:44.06ms
step:959/1920 train_time:42274ms step_avg:44.08ms
step:960/1920 train_time:42336ms step_avg:44.10ms
step:961/1920 train_time:42398ms step_avg:44.12ms
step:962/1920 train_time:42460ms step_avg:44.14ms
step:963/1920 train_time:42523ms step_avg:44.16ms
step:964/1920 train_time:42585ms step_avg:44.18ms
step:965/1920 train_time:42648ms step_avg:44.19ms
step:966/1920 train_time:42709ms step_avg:44.21ms
step:967/1920 train_time:42772ms step_avg:44.23ms
step:968/1920 train_time:42834ms step_avg:44.25ms
step:969/1920 train_time:42897ms step_avg:44.27ms
step:970/1920 train_time:42958ms step_avg:44.29ms
step:971/1920 train_time:43021ms step_avg:44.31ms
step:972/1920 train_time:43082ms step_avg:44.32ms
step:973/1920 train_time:43145ms step_avg:44.34ms
step:974/1920 train_time:43207ms step_avg:44.36ms
step:975/1920 train_time:43270ms step_avg:44.38ms
step:976/1920 train_time:43333ms step_avg:44.40ms
step:977/1920 train_time:43395ms step_avg:44.42ms
step:978/1920 train_time:43457ms step_avg:44.43ms
step:979/1920 train_time:43519ms step_avg:44.45ms
step:980/1920 train_time:43582ms step_avg:44.47ms
step:981/1920 train_time:43644ms step_avg:44.49ms
step:982/1920 train_time:43705ms step_avg:44.51ms
step:983/1920 train_time:43769ms step_avg:44.53ms
step:984/1920 train_time:43831ms step_avg:44.54ms
step:985/1920 train_time:43893ms step_avg:44.56ms
step:986/1920 train_time:43955ms step_avg:44.58ms
step:987/1920 train_time:44018ms step_avg:44.60ms
step:988/1920 train_time:44079ms step_avg:44.61ms
step:989/1920 train_time:44142ms step_avg:44.63ms
step:990/1920 train_time:44203ms step_avg:44.65ms
step:991/1920 train_time:44266ms step_avg:44.67ms
step:992/1920 train_time:44328ms step_avg:44.69ms
step:993/1920 train_time:44391ms step_avg:44.70ms
step:994/1920 train_time:44453ms step_avg:44.72ms
step:995/1920 train_time:44516ms step_avg:44.74ms
step:996/1920 train_time:44578ms step_avg:44.76ms
step:997/1920 train_time:44640ms step_avg:44.77ms
step:998/1920 train_time:44702ms step_avg:44.79ms
step:999/1920 train_time:44765ms step_avg:44.81ms
step:1000/1920 train_time:44827ms step_avg:44.83ms
step:1000/1920 val_loss:3.7807 train_time:44892ms step_avg:44.89ms
step:1001/1920 train_time:44910ms step_avg:44.87ms
step:1002/1920 train_time:44952ms step_avg:44.86ms
step:1003/1920 train_time:45017ms step_avg:44.88ms
step:1004/1920 train_time:45081ms step_avg:44.90ms
step:1005/1920 train_time:45143ms step_avg:44.92ms
step:1006/1920 train_time:45205ms step_avg:44.94ms
step:1007/1920 train_time:45267ms step_avg:44.95ms
step:1008/1920 train_time:45328ms step_avg:44.97ms
step:1009/1920 train_time:45390ms step_avg:44.99ms
step:1010/1920 train_time:45451ms step_avg:45.00ms
step:1011/1920 train_time:45514ms step_avg:45.02ms
step:1012/1920 train_time:45575ms step_avg:45.03ms
step:1013/1920 train_time:45639ms step_avg:45.05ms
step:1014/1920 train_time:45701ms step_avg:45.07ms
step:1015/1920 train_time:45764ms step_avg:45.09ms
step:1016/1920 train_time:45827ms step_avg:45.11ms
step:1017/1920 train_time:45891ms step_avg:45.12ms
step:1018/1920 train_time:45954ms step_avg:45.14ms
step:1019/1920 train_time:46017ms step_avg:45.16ms
step:1020/1920 train_time:46079ms step_avg:45.18ms
step:1021/1920 train_time:46143ms step_avg:45.19ms
step:1022/1920 train_time:46204ms step_avg:45.21ms
step:1023/1920 train_time:46267ms step_avg:45.23ms
step:1024/1920 train_time:46328ms step_avg:45.24ms
step:1025/1920 train_time:46390ms step_avg:45.26ms
step:1026/1920 train_time:46452ms step_avg:45.27ms
step:1027/1920 train_time:46514ms step_avg:45.29ms
step:1028/1920 train_time:46576ms step_avg:45.31ms
step:1029/1920 train_time:46639ms step_avg:45.32ms
step:1030/1920 train_time:46701ms step_avg:45.34ms
step:1031/1920 train_time:46764ms step_avg:45.36ms
step:1032/1920 train_time:46826ms step_avg:45.37ms
step:1033/1920 train_time:46889ms step_avg:45.39ms
step:1034/1920 train_time:46951ms step_avg:45.41ms
step:1035/1920 train_time:47014ms step_avg:45.42ms
step:1036/1920 train_time:47077ms step_avg:45.44ms
step:1037/1920 train_time:47140ms step_avg:45.46ms
step:1038/1920 train_time:47202ms step_avg:45.47ms
step:1039/1920 train_time:47265ms step_avg:45.49ms
step:1040/1920 train_time:47326ms step_avg:45.51ms
step:1041/1920 train_time:47389ms step_avg:45.52ms
step:1042/1920 train_time:47451ms step_avg:45.54ms
step:1043/1920 train_time:47513ms step_avg:45.55ms
step:1044/1920 train_time:47575ms step_avg:45.57ms
step:1045/1920 train_time:47638ms step_avg:45.59ms
step:1046/1920 train_time:47700ms step_avg:45.60ms
step:1047/1920 train_time:47763ms step_avg:45.62ms
step:1048/1920 train_time:47826ms step_avg:45.64ms
step:1049/1920 train_time:47888ms step_avg:45.65ms
step:1050/1920 train_time:47950ms step_avg:45.67ms
step:1051/1920 train_time:48013ms step_avg:45.68ms
step:1052/1920 train_time:48076ms step_avg:45.70ms
step:1053/1920 train_time:48139ms step_avg:45.72ms
step:1054/1920 train_time:48202ms step_avg:45.73ms
step:1055/1920 train_time:48264ms step_avg:45.75ms
step:1056/1920 train_time:48326ms step_avg:45.76ms
step:1057/1920 train_time:48389ms step_avg:45.78ms
step:1058/1920 train_time:48451ms step_avg:45.80ms
step:1059/1920 train_time:48513ms step_avg:45.81ms
step:1060/1920 train_time:48575ms step_avg:45.83ms
step:1061/1920 train_time:48638ms step_avg:45.84ms
step:1062/1920 train_time:48701ms step_avg:45.86ms
step:1063/1920 train_time:48764ms step_avg:45.87ms
step:1064/1920 train_time:48826ms step_avg:45.89ms
step:1065/1920 train_time:48888ms step_avg:45.90ms
step:1066/1920 train_time:48950ms step_avg:45.92ms
step:1067/1920 train_time:49012ms step_avg:45.93ms
step:1068/1920 train_time:49075ms step_avg:45.95ms
step:1069/1920 train_time:49138ms step_avg:45.97ms
step:1070/1920 train_time:49201ms step_avg:45.98ms
step:1071/1920 train_time:49264ms step_avg:46.00ms
step:1072/1920 train_time:49325ms step_avg:46.01ms
step:1073/1920 train_time:49388ms step_avg:46.03ms
step:1074/1920 train_time:49450ms step_avg:46.04ms
step:1075/1920 train_time:49513ms step_avg:46.06ms
step:1076/1920 train_time:49574ms step_avg:46.07ms
step:1077/1920 train_time:49638ms step_avg:46.09ms
step:1078/1920 train_time:49700ms step_avg:46.10ms
step:1079/1920 train_time:49763ms step_avg:46.12ms
step:1080/1920 train_time:49825ms step_avg:46.13ms
step:1081/1920 train_time:49888ms step_avg:46.15ms
step:1082/1920 train_time:49950ms step_avg:46.16ms
step:1083/1920 train_time:50012ms step_avg:46.18ms
step:1084/1920 train_time:50075ms step_avg:46.19ms
step:1085/1920 train_time:50138ms step_avg:46.21ms
step:1086/1920 train_time:50200ms step_avg:46.22ms
step:1087/1920 train_time:50262ms step_avg:46.24ms
step:1088/1920 train_time:50324ms step_avg:46.25ms
step:1089/1920 train_time:50387ms step_avg:46.27ms
step:1090/1920 train_time:50448ms step_avg:46.28ms
step:1091/1920 train_time:50511ms step_avg:46.30ms
step:1092/1920 train_time:50573ms step_avg:46.31ms
step:1093/1920 train_time:50636ms step_avg:46.33ms
step:1094/1920 train_time:50699ms step_avg:46.34ms
step:1095/1920 train_time:50762ms step_avg:46.36ms
step:1096/1920 train_time:50824ms step_avg:46.37ms
step:1097/1920 train_time:50886ms step_avg:46.39ms
step:1098/1920 train_time:50948ms step_avg:46.40ms
step:1099/1920 train_time:51011ms step_avg:46.42ms
step:1100/1920 train_time:51073ms step_avg:46.43ms
step:1101/1920 train_time:51136ms step_avg:46.44ms
step:1102/1920 train_time:51198ms step_avg:46.46ms
step:1103/1920 train_time:51261ms step_avg:46.47ms
step:1104/1920 train_time:51323ms step_avg:46.49ms
step:1105/1920 train_time:51386ms step_avg:46.50ms
step:1106/1920 train_time:51448ms step_avg:46.52ms
step:1107/1920 train_time:51510ms step_avg:46.53ms
step:1108/1920 train_time:51572ms step_avg:46.55ms
step:1109/1920 train_time:51636ms step_avg:46.56ms
step:1110/1920 train_time:51698ms step_avg:46.57ms
step:1111/1920 train_time:51761ms step_avg:46.59ms
step:1112/1920 train_time:51823ms step_avg:46.60ms
step:1113/1920 train_time:51886ms step_avg:46.62ms
step:1114/1920 train_time:51948ms step_avg:46.63ms
step:1115/1920 train_time:52010ms step_avg:46.65ms
step:1116/1920 train_time:52072ms step_avg:46.66ms
step:1117/1920 train_time:52135ms step_avg:46.67ms
step:1118/1920 train_time:52198ms step_avg:46.69ms
step:1119/1920 train_time:52261ms step_avg:46.70ms
step:1120/1920 train_time:52323ms step_avg:46.72ms
step:1121/1920 train_time:52385ms step_avg:46.73ms
step:1122/1920 train_time:52447ms step_avg:46.74ms
step:1123/1920 train_time:52509ms step_avg:46.76ms
step:1124/1920 train_time:52572ms step_avg:46.77ms
step:1125/1920 train_time:52634ms step_avg:46.79ms
step:1126/1920 train_time:52697ms step_avg:46.80ms
step:1127/1920 train_time:52760ms step_avg:46.81ms
step:1128/1920 train_time:52822ms step_avg:46.83ms
step:1129/1920 train_time:52885ms step_avg:46.84ms
step:1130/1920 train_time:52946ms step_avg:46.86ms
step:1131/1920 train_time:53008ms step_avg:46.87ms
step:1132/1920 train_time:53070ms step_avg:46.88ms
step:1133/1920 train_time:53133ms step_avg:46.90ms
step:1134/1920 train_time:53195ms step_avg:46.91ms
step:1135/1920 train_time:53258ms step_avg:46.92ms
step:1136/1920 train_time:53320ms step_avg:46.94ms
step:1137/1920 train_time:53383ms step_avg:46.95ms
step:1138/1920 train_time:53445ms step_avg:46.96ms
step:1139/1920 train_time:53508ms step_avg:46.98ms
step:1140/1920 train_time:53570ms step_avg:46.99ms
step:1141/1920 train_time:53633ms step_avg:47.01ms
step:1142/1920 train_time:53694ms step_avg:47.02ms
step:1143/1920 train_time:53757ms step_avg:47.03ms
step:1144/1920 train_time:53820ms step_avg:47.05ms
step:1145/1920 train_time:53883ms step_avg:47.06ms
step:1146/1920 train_time:53945ms step_avg:47.07ms
step:1147/1920 train_time:54007ms step_avg:47.09ms
step:1148/1920 train_time:54068ms step_avg:47.10ms
step:1149/1920 train_time:54131ms step_avg:47.11ms
step:1150/1920 train_time:54192ms step_avg:47.12ms
step:1151/1920 train_time:54255ms step_avg:47.14ms
step:1152/1920 train_time:54318ms step_avg:47.15ms
step:1153/1920 train_time:54381ms step_avg:47.16ms
step:1154/1920 train_time:54443ms step_avg:47.18ms
step:1155/1920 train_time:54505ms step_avg:47.19ms
step:1156/1920 train_time:54567ms step_avg:47.20ms
step:1157/1920 train_time:54629ms step_avg:47.22ms
step:1158/1920 train_time:54691ms step_avg:47.23ms
step:1159/1920 train_time:54755ms step_avg:47.24ms
step:1160/1920 train_time:54817ms step_avg:47.26ms
step:1161/1920 train_time:54880ms step_avg:47.27ms
step:1162/1920 train_time:54942ms step_avg:47.28ms
step:1163/1920 train_time:55005ms step_avg:47.30ms
step:1164/1920 train_time:55066ms step_avg:47.31ms
step:1165/1920 train_time:55129ms step_avg:47.32ms
step:1166/1920 train_time:55191ms step_avg:47.33ms
step:1167/1920 train_time:55253ms step_avg:47.35ms
step:1168/1920 train_time:55316ms step_avg:47.36ms
step:1169/1920 train_time:55379ms step_avg:47.37ms
step:1170/1920 train_time:55441ms step_avg:47.39ms
step:1171/1920 train_time:55504ms step_avg:47.40ms
step:1172/1920 train_time:55565ms step_avg:47.41ms
step:1173/1920 train_time:55628ms step_avg:47.42ms
step:1174/1920 train_time:55690ms step_avg:47.44ms
step:1175/1920 train_time:55753ms step_avg:47.45ms
step:1176/1920 train_time:55815ms step_avg:47.46ms
step:1177/1920 train_time:55878ms step_avg:47.48ms
step:1178/1920 train_time:55940ms step_avg:47.49ms
step:1179/1920 train_time:56003ms step_avg:47.50ms
step:1180/1920 train_time:56064ms step_avg:47.51ms
step:1181/1920 train_time:56127ms step_avg:47.52ms
step:1182/1920 train_time:56189ms step_avg:47.54ms
step:1183/1920 train_time:56251ms step_avg:47.55ms
step:1184/1920 train_time:56313ms step_avg:47.56ms
step:1185/1920 train_time:56378ms step_avg:47.58ms
step:1186/1920 train_time:56440ms step_avg:47.59ms
step:1187/1920 train_time:56502ms step_avg:47.60ms
step:1188/1920 train_time:56564ms step_avg:47.61ms
step:1189/1920 train_time:56627ms step_avg:47.63ms
step:1190/1920 train_time:56688ms step_avg:47.64ms
step:1191/1920 train_time:56751ms step_avg:47.65ms
step:1192/1920 train_time:56813ms step_avg:47.66ms
step:1193/1920 train_time:56876ms step_avg:47.68ms
step:1194/1920 train_time:56938ms step_avg:47.69ms
step:1195/1920 train_time:57001ms step_avg:47.70ms
step:1196/1920 train_time:57063ms step_avg:47.71ms
step:1197/1920 train_time:57125ms step_avg:47.72ms
step:1198/1920 train_time:57187ms step_avg:47.74ms
step:1199/1920 train_time:57250ms step_avg:47.75ms
step:1200/1920 train_time:57312ms step_avg:47.76ms
step:1201/1920 train_time:57375ms step_avg:47.77ms
step:1202/1920 train_time:57437ms step_avg:47.78ms
step:1203/1920 train_time:57501ms step_avg:47.80ms
step:1204/1920 train_time:57562ms step_avg:47.81ms
step:1205/1920 train_time:57625ms step_avg:47.82ms
step:1206/1920 train_time:57687ms step_avg:47.83ms
step:1207/1920 train_time:57750ms step_avg:47.85ms
step:1208/1920 train_time:57811ms step_avg:47.86ms
step:1209/1920 train_time:57874ms step_avg:47.87ms
step:1210/1920 train_time:57936ms step_avg:47.88ms
step:1211/1920 train_time:57999ms step_avg:47.89ms
step:1212/1920 train_time:58062ms step_avg:47.91ms
step:1213/1920 train_time:58124ms step_avg:47.92ms
step:1214/1920 train_time:58186ms step_avg:47.93ms
step:1215/1920 train_time:58249ms step_avg:47.94ms
step:1216/1920 train_time:58310ms step_avg:47.95ms
step:1217/1920 train_time:58373ms step_avg:47.96ms
step:1218/1920 train_time:58436ms step_avg:47.98ms
step:1219/1920 train_time:58499ms step_avg:47.99ms
step:1220/1920 train_time:58560ms step_avg:48.00ms
step:1221/1920 train_time:58623ms step_avg:48.01ms
step:1222/1920 train_time:58685ms step_avg:48.02ms
step:1223/1920 train_time:58748ms step_avg:48.04ms
step:1224/1920 train_time:58809ms step_avg:48.05ms
step:1225/1920 train_time:58872ms step_avg:48.06ms
step:1226/1920 train_time:58934ms step_avg:48.07ms
step:1227/1920 train_time:58998ms step_avg:48.08ms
step:1228/1920 train_time:59060ms step_avg:48.09ms
step:1229/1920 train_time:59122ms step_avg:48.11ms
step:1230/1920 train_time:59184ms step_avg:48.12ms
step:1231/1920 train_time:59247ms step_avg:48.13ms
step:1232/1920 train_time:59309ms step_avg:48.14ms
step:1233/1920 train_time:59371ms step_avg:48.15ms
step:1234/1920 train_time:59433ms step_avg:48.16ms
step:1235/1920 train_time:59496ms step_avg:48.18ms
step:1236/1920 train_time:59558ms step_avg:48.19ms
step:1237/1920 train_time:59621ms step_avg:48.20ms
step:1238/1920 train_time:59682ms step_avg:48.21ms
step:1239/1920 train_time:59745ms step_avg:48.22ms
step:1240/1920 train_time:59807ms step_avg:48.23ms
step:1241/1920 train_time:59869ms step_avg:48.24ms
step:1242/1920 train_time:59931ms step_avg:48.25ms
step:1243/1920 train_time:59994ms step_avg:48.27ms
step:1244/1920 train_time:60056ms step_avg:48.28ms
step:1245/1920 train_time:60119ms step_avg:48.29ms
step:1246/1920 train_time:60181ms step_avg:48.30ms
step:1247/1920 train_time:60244ms step_avg:48.31ms
step:1248/1920 train_time:60305ms step_avg:48.32ms
step:1249/1920 train_time:60368ms step_avg:48.33ms
step:1250/1920 train_time:60430ms step_avg:48.34ms
step:1250/1920 val_loss:3.5527 train_time:60495ms step_avg:48.40ms
step:1251/1920 train_time:60513ms step_avg:48.37ms
step:1252/1920 train_time:60556ms step_avg:48.37ms
step:1253/1920 train_time:60621ms step_avg:48.38ms
step:1254/1920 train_time:60685ms step_avg:48.39ms
step:1255/1920 train_time:60747ms step_avg:48.40ms
step:1256/1920 train_time:60834ms step_avg:48.43ms
step:1257/1920 train_time:60922ms step_avg:48.47ms
step:1258/1920 train_time:61009ms step_avg:48.50ms
step:1259/1920 train_time:61100ms step_avg:48.53ms
step:1260/1920 train_time:61187ms step_avg:48.56ms
step:1261/1920 train_time:61275ms step_avg:48.59ms
step:1262/1920 train_time:61362ms step_avg:48.62ms
step:1263/1920 train_time:61455ms step_avg:48.66ms
step:1264/1920 train_time:61544ms step_avg:48.69ms
step:1265/1920 train_time:61635ms step_avg:48.72ms
step:1266/1920 train_time:61723ms step_avg:48.75ms
step:1267/1920 train_time:61812ms step_avg:48.79ms
step:1268/1920 train_time:61899ms step_avg:48.82ms
step:1269/1920 train_time:61986ms step_avg:48.85ms
step:1270/1920 train_time:62073ms step_avg:48.88ms
step:1271/1920 train_time:62161ms step_avg:48.91ms
step:1272/1920 train_time:62249ms step_avg:48.94ms
step:1273/1920 train_time:62338ms step_avg:48.97ms
step:1274/1920 train_time:62426ms step_avg:49.00ms
step:1275/1920 train_time:62516ms step_avg:49.03ms
step:1276/1920 train_time:62604ms step_avg:49.06ms
step:1277/1920 train_time:62695ms step_avg:49.10ms
step:1278/1920 train_time:62783ms step_avg:49.13ms
step:1279/1920 train_time:62872ms step_avg:49.16ms
step:1280/1920 train_time:62960ms step_avg:49.19ms
step:1281/1920 train_time:63047ms step_avg:49.22ms
step:1282/1920 train_time:63134ms step_avg:49.25ms
step:1283/1920 train_time:63222ms step_avg:49.28ms
step:1284/1920 train_time:63310ms step_avg:49.31ms
step:1285/1920 train_time:63400ms step_avg:49.34ms
step:1286/1920 train_time:63488ms step_avg:49.37ms
step:1287/1920 train_time:63579ms step_avg:49.40ms
step:1288/1920 train_time:63668ms step_avg:49.43ms
step:1289/1920 train_time:63758ms step_avg:49.46ms
step:1290/1920 train_time:63845ms step_avg:49.49ms
step:1291/1920 train_time:63934ms step_avg:49.52ms
step:1292/1920 train_time:64021ms step_avg:49.55ms
step:1293/1920 train_time:64109ms step_avg:49.58ms
step:1294/1920 train_time:64196ms step_avg:49.61ms
step:1295/1920 train_time:64284ms step_avg:49.64ms
step:1296/1920 train_time:64372ms step_avg:49.67ms
step:1297/1920 train_time:64461ms step_avg:49.70ms
step:1298/1920 train_time:64549ms step_avg:49.73ms
step:1299/1920 train_time:64639ms step_avg:49.76ms
step:1300/1920 train_time:64727ms step_avg:49.79ms
step:1301/1920 train_time:64816ms step_avg:49.82ms
step:1302/1920 train_time:64904ms step_avg:49.85ms
step:1303/1920 train_time:64993ms step_avg:49.88ms
step:1304/1920 train_time:65080ms step_avg:49.91ms
step:1305/1920 train_time:65168ms step_avg:49.94ms
step:1306/1920 train_time:65255ms step_avg:49.97ms
step:1307/1920 train_time:65343ms step_avg:49.99ms
step:1308/1920 train_time:65431ms step_avg:50.02ms
step:1309/1920 train_time:65520ms step_avg:50.05ms
step:1310/1920 train_time:65610ms step_avg:50.08ms
step:1311/1920 train_time:65699ms step_avg:50.11ms
step:1312/1920 train_time:65787ms step_avg:50.14ms
step:1313/1920 train_time:65877ms step_avg:50.17ms
step:1314/1920 train_time:65964ms step_avg:50.20ms
step:1315/1920 train_time:66052ms step_avg:50.23ms
step:1316/1920 train_time:66140ms step_avg:50.26ms
step:1317/1920 train_time:66227ms step_avg:50.29ms
step:1318/1920 train_time:66315ms step_avg:50.31ms
step:1319/1920 train_time:66403ms step_avg:50.34ms
step:1320/1920 train_time:66493ms step_avg:50.37ms
step:1321/1920 train_time:66581ms step_avg:50.40ms
step:1322/1920 train_time:66671ms step_avg:50.43ms
step:1323/1920 train_time:66760ms step_avg:50.46ms
step:1324/1920 train_time:66848ms step_avg:50.49ms
step:1325/1920 train_time:66938ms step_avg:50.52ms
step:1326/1920 train_time:67025ms step_avg:50.55ms
step:1327/1920 train_time:67113ms step_avg:50.57ms
step:1328/1920 train_time:67200ms step_avg:50.60ms
step:1329/1920 train_time:67289ms step_avg:50.63ms
step:1330/1920 train_time:67376ms step_avg:50.66ms
step:1331/1920 train_time:67465ms step_avg:50.69ms
step:1332/1920 train_time:67554ms step_avg:50.72ms
step:1333/1920 train_time:67642ms step_avg:50.74ms
step:1334/1920 train_time:67731ms step_avg:50.77ms
step:1335/1920 train_time:67820ms step_avg:50.80ms
step:1336/1920 train_time:67909ms step_avg:50.83ms
step:1337/1920 train_time:67999ms step_avg:50.86ms
step:1338/1920 train_time:68086ms step_avg:50.89ms
step:1339/1920 train_time:68175ms step_avg:50.91ms
step:1340/1920 train_time:68262ms step_avg:50.94ms
step:1341/1920 train_time:68351ms step_avg:50.97ms
step:1342/1920 train_time:68439ms step_avg:51.00ms
step:1343/1920 train_time:68527ms step_avg:51.03ms
step:1344/1920 train_time:68615ms step_avg:51.05ms
step:1345/1920 train_time:68703ms step_avg:51.08ms
step:1346/1920 train_time:68792ms step_avg:51.11ms
step:1347/1920 train_time:68881ms step_avg:51.14ms
step:1348/1920 train_time:68969ms step_avg:51.16ms
step:1349/1920 train_time:69059ms step_avg:51.19ms
step:1350/1920 train_time:69147ms step_avg:51.22ms
step:1351/1920 train_time:69237ms step_avg:51.25ms
step:1352/1920 train_time:69324ms step_avg:51.28ms
step:1353/1920 train_time:69412ms step_avg:51.30ms
step:1354/1920 train_time:69501ms step_avg:51.33ms
step:1355/1920 train_time:69591ms step_avg:51.36ms
step:1356/1920 train_time:69679ms step_avg:51.39ms
step:1357/1920 train_time:69768ms step_avg:51.41ms
step:1358/1920 train_time:69857ms step_avg:51.44ms
step:1359/1920 train_time:69946ms step_avg:51.47ms
step:1360/1920 train_time:70034ms step_avg:51.50ms
step:1361/1920 train_time:70122ms step_avg:51.52ms
step:1362/1920 train_time:70210ms step_avg:51.55ms
step:1363/1920 train_time:70300ms step_avg:51.58ms
step:1364/1920 train_time:70387ms step_avg:51.60ms
step:1365/1920 train_time:70476ms step_avg:51.63ms
step:1366/1920 train_time:70563ms step_avg:51.66ms
step:1367/1920 train_time:70652ms step_avg:51.68ms
step:1368/1920 train_time:70740ms step_avg:51.71ms
step:1369/1920 train_time:70829ms step_avg:51.74ms
step:1370/1920 train_time:70917ms step_avg:51.76ms
step:1371/1920 train_time:71006ms step_avg:51.79ms
step:1372/1920 train_time:71094ms step_avg:51.82ms
step:1373/1920 train_time:71182ms step_avg:51.84ms
step:1374/1920 train_time:71271ms step_avg:51.87ms
step:1375/1920 train_time:71360ms step_avg:51.90ms
step:1376/1920 train_time:71449ms step_avg:51.92ms
step:1377/1920 train_time:71538ms step_avg:51.95ms
step:1378/1920 train_time:71626ms step_avg:51.98ms
step:1379/1920 train_time:71714ms step_avg:52.00ms
step:1380/1920 train_time:71802ms step_avg:52.03ms
step:1381/1920 train_time:71892ms step_avg:52.06ms
step:1382/1920 train_time:71980ms step_avg:52.08ms
step:1383/1920 train_time:72068ms step_avg:52.11ms
step:1384/1920 train_time:72156ms step_avg:52.14ms
step:1385/1920 train_time:72244ms step_avg:52.16ms
step:1386/1920 train_time:72332ms step_avg:52.19ms
step:1387/1920 train_time:72421ms step_avg:52.21ms
step:1388/1920 train_time:72509ms step_avg:52.24ms
step:1389/1920 train_time:72599ms step_avg:52.27ms
step:1390/1920 train_time:72686ms step_avg:52.29ms
step:1391/1920 train_time:72776ms step_avg:52.32ms
step:1392/1920 train_time:72865ms step_avg:52.35ms
step:1393/1920 train_time:72953ms step_avg:52.37ms
step:1394/1920 train_time:73041ms step_avg:52.40ms
step:1395/1920 train_time:73130ms step_avg:52.42ms
step:1396/1920 train_time:73217ms step_avg:52.45ms
step:1397/1920 train_time:73306ms step_avg:52.47ms
step:1398/1920 train_time:73394ms step_avg:52.50ms
step:1399/1920 train_time:73483ms step_avg:52.53ms
step:1400/1920 train_time:73571ms step_avg:52.55ms
step:1401/1920 train_time:73661ms step_avg:52.58ms
step:1402/1920 train_time:73749ms step_avg:52.60ms
step:1403/1920 train_time:73838ms step_avg:52.63ms
step:1404/1920 train_time:73926ms step_avg:52.65ms
step:1405/1920 train_time:74014ms step_avg:52.68ms
step:1406/1920 train_time:74102ms step_avg:52.70ms
step:1407/1920 train_time:74192ms step_avg:52.73ms
step:1408/1920 train_time:74280ms step_avg:52.76ms
step:1409/1920 train_time:74368ms step_avg:52.78ms
step:1410/1920 train_time:74456ms step_avg:52.81ms
step:1411/1920 train_time:74546ms step_avg:52.83ms
step:1412/1920 train_time:74634ms step_avg:52.86ms
step:1413/1920 train_time:74723ms step_avg:52.88ms
step:1414/1920 train_time:74812ms step_avg:52.91ms
step:1415/1920 train_time:74902ms step_avg:52.93ms
step:1416/1920 train_time:74990ms step_avg:52.96ms
step:1417/1920 train_time:75080ms step_avg:52.99ms
step:1418/1920 train_time:75169ms step_avg:53.01ms
step:1419/1920 train_time:75257ms step_avg:53.04ms
step:1420/1920 train_time:75345ms step_avg:53.06ms
step:1421/1920 train_time:75433ms step_avg:53.08ms
step:1422/1920 train_time:75522ms step_avg:53.11ms
step:1423/1920 train_time:75611ms step_avg:53.13ms
step:1424/1920 train_time:75699ms step_avg:53.16ms
step:1425/1920 train_time:75789ms step_avg:53.19ms
step:1426/1920 train_time:75877ms step_avg:53.21ms
step:1427/1920 train_time:75965ms step_avg:53.23ms
step:1428/1920 train_time:76053ms step_avg:53.26ms
step:1429/1920 train_time:76141ms step_avg:53.28ms
step:1430/1920 train_time:76228ms step_avg:53.31ms
step:1431/1920 train_time:76317ms step_avg:53.33ms
step:1432/1920 train_time:76404ms step_avg:53.35ms
step:1433/1920 train_time:76493ms step_avg:53.38ms
step:1434/1920 train_time:76581ms step_avg:53.40ms
step:1435/1920 train_time:76670ms step_avg:53.43ms
step:1436/1920 train_time:76758ms step_avg:53.45ms
step:1437/1920 train_time:76847ms step_avg:53.48ms
step:1438/1920 train_time:76935ms step_avg:53.50ms
step:1439/1920 train_time:77024ms step_avg:53.53ms
step:1440/1920 train_time:77112ms step_avg:53.55ms
step:1441/1920 train_time:77201ms step_avg:53.57ms
step:1442/1920 train_time:77289ms step_avg:53.60ms
step:1443/1920 train_time:77378ms step_avg:53.62ms
step:1444/1920 train_time:77466ms step_avg:53.65ms
step:1445/1920 train_time:77554ms step_avg:53.67ms
step:1446/1920 train_time:77642ms step_avg:53.69ms
step:1447/1920 train_time:77731ms step_avg:53.72ms
step:1448/1920 train_time:77819ms step_avg:53.74ms
step:1449/1920 train_time:77907ms step_avg:53.77ms
step:1450/1920 train_time:77995ms step_avg:53.79ms
step:1451/1920 train_time:78083ms step_avg:53.81ms
step:1452/1920 train_time:78171ms step_avg:53.84ms
step:1453/1920 train_time:78260ms step_avg:53.86ms
step:1454/1920 train_time:78348ms step_avg:53.88ms
step:1455/1920 train_time:78438ms step_avg:53.91ms
step:1456/1920 train_time:78526ms step_avg:53.93ms
step:1457/1920 train_time:78616ms step_avg:53.96ms
step:1458/1920 train_time:78703ms step_avg:53.98ms
step:1459/1920 train_time:78793ms step_avg:54.00ms
step:1460/1920 train_time:78880ms step_avg:54.03ms
step:1461/1920 train_time:78970ms step_avg:54.05ms
step:1462/1920 train_time:79057ms step_avg:54.07ms
step:1463/1920 train_time:79146ms step_avg:54.10ms
step:1464/1920 train_time:79234ms step_avg:54.12ms
step:1465/1920 train_time:79322ms step_avg:54.14ms
step:1466/1920 train_time:79410ms step_avg:54.17ms
step:1467/1920 train_time:79499ms step_avg:54.19ms
step:1468/1920 train_time:79587ms step_avg:54.21ms
step:1469/1920 train_time:79676ms step_avg:54.24ms
step:1470/1920 train_time:79763ms step_avg:54.26ms
step:1471/1920 train_time:79854ms step_avg:54.29ms
step:1472/1920 train_time:79942ms step_avg:54.31ms
step:1473/1920 train_time:80031ms step_avg:54.33ms
step:1474/1920 train_time:80119ms step_avg:54.36ms
step:1475/1920 train_time:80207ms step_avg:54.38ms
step:1476/1920 train_time:80296ms step_avg:54.40ms
step:1477/1920 train_time:80384ms step_avg:54.42ms
step:1478/1920 train_time:80473ms step_avg:54.45ms
step:1479/1920 train_time:80562ms step_avg:54.47ms
step:1480/1920 train_time:80650ms step_avg:54.49ms
step:1481/1920 train_time:80739ms step_avg:54.52ms
step:1482/1920 train_time:80828ms step_avg:54.54ms
step:1483/1920 train_time:80916ms step_avg:54.56ms
step:1484/1920 train_time:81003ms step_avg:54.58ms
step:1485/1920 train_time:81092ms step_avg:54.61ms
step:1486/1920 train_time:81180ms step_avg:54.63ms
step:1487/1920 train_time:81268ms step_avg:54.65ms
step:1488/1920 train_time:81356ms step_avg:54.67ms
step:1489/1920 train_time:81444ms step_avg:54.70ms
step:1490/1920 train_time:81533ms step_avg:54.72ms
step:1491/1920 train_time:81622ms step_avg:54.74ms
step:1492/1920 train_time:81710ms step_avg:54.77ms
step:1493/1920 train_time:81799ms step_avg:54.79ms
step:1494/1920 train_time:81888ms step_avg:54.81ms
step:1495/1920 train_time:81978ms step_avg:54.83ms
step:1496/1920 train_time:82066ms step_avg:54.86ms
step:1497/1920 train_time:82155ms step_avg:54.88ms
step:1498/1920 train_time:82242ms step_avg:54.90ms
step:1499/1920 train_time:82332ms step_avg:54.92ms
step:1500/1920 train_time:82420ms step_avg:54.95ms
step:1500/1920 val_loss:3.4158 train_time:82511ms step_avg:55.01ms
step:1501/1920 train_time:82529ms step_avg:54.98ms
step:1502/1920 train_time:82603ms step_avg:55.00ms
step:1503/1920 train_time:82693ms step_avg:55.02ms
step:1504/1920 train_time:82780ms step_avg:55.04ms
step:1505/1920 train_time:82869ms step_avg:55.06ms
step:1506/1920 train_time:82956ms step_avg:55.08ms
step:1507/1920 train_time:83043ms step_avg:55.11ms
step:1508/1920 train_time:83131ms step_avg:55.13ms
step:1509/1920 train_time:83219ms step_avg:55.15ms
step:1510/1920 train_time:83307ms step_avg:55.17ms
step:1511/1920 train_time:83395ms step_avg:55.19ms
step:1512/1920 train_time:83484ms step_avg:55.21ms
step:1513/1920 train_time:83576ms step_avg:55.24ms
step:1514/1920 train_time:83664ms step_avg:55.26ms
step:1515/1920 train_time:83753ms step_avg:55.28ms
step:1516/1920 train_time:83841ms step_avg:55.30ms
step:1517/1920 train_time:83930ms step_avg:55.33ms
step:1518/1920 train_time:84017ms step_avg:55.35ms
step:1519/1920 train_time:84104ms step_avg:55.37ms
step:1520/1920 train_time:84192ms step_avg:55.39ms
step:1521/1920 train_time:84280ms step_avg:55.41ms
step:1522/1920 train_time:84368ms step_avg:55.43ms
step:1523/1920 train_time:84458ms step_avg:55.45ms
step:1524/1920 train_time:84546ms step_avg:55.48ms
step:1525/1920 train_time:84636ms step_avg:55.50ms
step:1526/1920 train_time:84724ms step_avg:55.52ms
step:1527/1920 train_time:84813ms step_avg:55.54ms
step:1528/1920 train_time:84901ms step_avg:55.56ms
step:1529/1920 train_time:84989ms step_avg:55.58ms
step:1530/1920 train_time:85077ms step_avg:55.61ms
step:1531/1920 train_time:85164ms step_avg:55.63ms
step:1532/1920 train_time:85252ms step_avg:55.65ms
step:1533/1920 train_time:85341ms step_avg:55.67ms
step:1534/1920 train_time:85431ms step_avg:55.69ms
step:1535/1920 train_time:85521ms step_avg:55.71ms
step:1536/1920 train_time:85609ms step_avg:55.74ms
step:1537/1920 train_time:85699ms step_avg:55.76ms
step:1538/1920 train_time:85787ms step_avg:55.78ms
step:1539/1920 train_time:85876ms step_avg:55.80ms
step:1540/1920 train_time:85963ms step_avg:55.82ms
step:1541/1920 train_time:86052ms step_avg:55.84ms
step:1542/1920 train_time:86139ms step_avg:55.86ms
step:1543/1920 train_time:86229ms step_avg:55.88ms
step:1544/1920 train_time:86317ms step_avg:55.90ms
step:1545/1920 train_time:86406ms step_avg:55.93ms
step:1546/1920 train_time:86494ms step_avg:55.95ms
step:1547/1920 train_time:86583ms step_avg:55.97ms
step:1548/1920 train_time:86671ms step_avg:55.99ms
step:1549/1920 train_time:86760ms step_avg:56.01ms
step:1550/1920 train_time:86849ms step_avg:56.03ms
step:1551/1920 train_time:86938ms step_avg:56.05ms
step:1552/1920 train_time:87025ms step_avg:56.07ms
step:1553/1920 train_time:87113ms step_avg:56.09ms
step:1554/1920 train_time:87201ms step_avg:56.11ms
step:1555/1920 train_time:87289ms step_avg:56.13ms
step:1556/1920 train_time:87377ms step_avg:56.16ms
step:1557/1920 train_time:87466ms step_avg:56.18ms
step:1558/1920 train_time:87554ms step_avg:56.20ms
step:1559/1920 train_time:87643ms step_avg:56.22ms
step:1560/1920 train_time:87732ms step_avg:56.24ms
step:1561/1920 train_time:87822ms step_avg:56.26ms
step:1562/1920 train_time:87910ms step_avg:56.28ms
step:1563/1920 train_time:88000ms step_avg:56.30ms
step:1564/1920 train_time:88089ms step_avg:56.32ms
step:1565/1920 train_time:88178ms step_avg:56.34ms
step:1566/1920 train_time:88265ms step_avg:56.36ms
step:1567/1920 train_time:88354ms step_avg:56.38ms
step:1568/1920 train_time:88442ms step_avg:56.40ms
step:1569/1920 train_time:88531ms step_avg:56.43ms
step:1570/1920 train_time:88619ms step_avg:56.45ms
step:1571/1920 train_time:88709ms step_avg:56.47ms
step:1572/1920 train_time:88797ms step_avg:56.49ms
step:1573/1920 train_time:88885ms step_avg:56.51ms
step:1574/1920 train_time:88974ms step_avg:56.53ms
step:1575/1920 train_time:89062ms step_avg:56.55ms
step:1576/1920 train_time:89151ms step_avg:56.57ms
step:1577/1920 train_time:89241ms step_avg:56.59ms
step:1578/1920 train_time:89329ms step_avg:56.61ms
step:1579/1920 train_time:89419ms step_avg:56.63ms
step:1580/1920 train_time:89508ms step_avg:56.65ms
step:1581/1920 train_time:89597ms step_avg:56.67ms
step:1582/1920 train_time:89685ms step_avg:56.69ms
step:1583/1920 train_time:89774ms step_avg:56.71ms
step:1584/1920 train_time:89861ms step_avg:56.73ms
step:1585/1920 train_time:89950ms step_avg:56.75ms
step:1586/1920 train_time:90039ms step_avg:56.77ms
step:1587/1920 train_time:90127ms step_avg:56.79ms
step:1588/1920 train_time:90216ms step_avg:56.81ms
step:1589/1920 train_time:90305ms step_avg:56.83ms
step:1590/1920 train_time:90393ms step_avg:56.85ms
step:1591/1920 train_time:90482ms step_avg:56.87ms
step:1592/1920 train_time:90570ms step_avg:56.89ms
step:1593/1920 train_time:90660ms step_avg:56.91ms
step:1594/1920 train_time:90748ms step_avg:56.93ms
step:1595/1920 train_time:90838ms step_avg:56.95ms
step:1596/1920 train_time:90926ms step_avg:56.97ms
step:1597/1920 train_time:91014ms step_avg:56.99ms
step:1598/1920 train_time:91102ms step_avg:57.01ms
step:1599/1920 train_time:91191ms step_avg:57.03ms
step:1600/1920 train_time:91280ms step_avg:57.05ms
step:1601/1920 train_time:91369ms step_avg:57.07ms
step:1602/1920 train_time:91458ms step_avg:57.09ms
step:1603/1920 train_time:91546ms step_avg:57.11ms
step:1604/1920 train_time:91633ms step_avg:57.13ms
step:1605/1920 train_time:91722ms step_avg:57.15ms
step:1606/1920 train_time:91811ms step_avg:57.17ms
step:1607/1920 train_time:91900ms step_avg:57.19ms
step:1608/1920 train_time:91988ms step_avg:57.21ms
step:1609/1920 train_time:92078ms step_avg:57.23ms
step:1610/1920 train_time:92166ms step_avg:57.25ms
step:1611/1920 train_time:92254ms step_avg:57.27ms
step:1612/1920 train_time:92342ms step_avg:57.28ms
step:1613/1920 train_time:92431ms step_avg:57.30ms
step:1614/1920 train_time:92519ms step_avg:57.32ms
step:1615/1920 train_time:92608ms step_avg:57.34ms
step:1616/1920 train_time:92697ms step_avg:57.36ms
step:1617/1920 train_time:92785ms step_avg:57.38ms
step:1618/1920 train_time:92873ms step_avg:57.40ms
step:1619/1920 train_time:92962ms step_avg:57.42ms
step:1620/1920 train_time:93049ms step_avg:57.44ms
step:1621/1920 train_time:93140ms step_avg:57.46ms
step:1622/1920 train_time:93230ms step_avg:57.48ms
step:1623/1920 train_time:93320ms step_avg:57.50ms
step:1624/1920 train_time:93408ms step_avg:57.52ms
step:1625/1920 train_time:93498ms step_avg:57.54ms
step:1626/1920 train_time:93586ms step_avg:57.56ms
step:1627/1920 train_time:93674ms step_avg:57.57ms
step:1628/1920 train_time:93762ms step_avg:57.59ms
step:1629/1920 train_time:93851ms step_avg:57.61ms
step:1630/1920 train_time:93940ms step_avg:57.63ms
step:1631/1920 train_time:94029ms step_avg:57.65ms
step:1632/1920 train_time:94117ms step_avg:57.67ms
step:1633/1920 train_time:94205ms step_avg:57.69ms
step:1634/1920 train_time:94294ms step_avg:57.71ms
step:1635/1920 train_time:94382ms step_avg:57.73ms
step:1636/1920 train_time:94471ms step_avg:57.74ms
step:1637/1920 train_time:94559ms step_avg:57.76ms
step:1638/1920 train_time:94647ms step_avg:57.78ms
step:1639/1920 train_time:94736ms step_avg:57.80ms
step:1640/1920 train_time:94824ms step_avg:57.82ms
step:1641/1920 train_time:94913ms step_avg:57.84ms
step:1642/1920 train_time:95001ms step_avg:57.86ms
step:1643/1920 train_time:95090ms step_avg:57.88ms
step:1644/1920 train_time:95179ms step_avg:57.89ms
step:1645/1920 train_time:95267ms step_avg:57.91ms
step:1646/1920 train_time:95356ms step_avg:57.93ms
step:1647/1920 train_time:95443ms step_avg:57.95ms
step:1648/1920 train_time:95532ms step_avg:57.97ms
step:1649/1920 train_time:95621ms step_avg:57.99ms
step:1650/1920 train_time:95710ms step_avg:58.01ms
step:1651/1920 train_time:95799ms step_avg:58.02ms
step:1652/1920 train_time:95887ms step_avg:58.04ms
step:1653/1920 train_time:95977ms step_avg:58.06ms
step:1654/1920 train_time:96065ms step_avg:58.08ms
step:1655/1920 train_time:96154ms step_avg:58.10ms
step:1656/1920 train_time:96242ms step_avg:58.12ms
step:1657/1920 train_time:96330ms step_avg:58.14ms
step:1658/1920 train_time:96418ms step_avg:58.15ms
step:1659/1920 train_time:96507ms step_avg:58.17ms
step:1660/1920 train_time:96595ms step_avg:58.19ms
step:1661/1920 train_time:96684ms step_avg:58.21ms
step:1662/1920 train_time:96772ms step_avg:58.23ms
step:1663/1920 train_time:96861ms step_avg:58.24ms
step:1664/1920 train_time:96949ms step_avg:58.26ms
step:1665/1920 train_time:97040ms step_avg:58.28ms
step:1666/1920 train_time:97129ms step_avg:58.30ms
step:1667/1920 train_time:97218ms step_avg:58.32ms
step:1668/1920 train_time:97305ms step_avg:58.34ms
step:1669/1920 train_time:97395ms step_avg:58.36ms
step:1670/1920 train_time:97482ms step_avg:58.37ms
step:1671/1920 train_time:97571ms step_avg:58.39ms
step:1672/1920 train_time:97659ms step_avg:58.41ms
step:1673/1920 train_time:97748ms step_avg:58.43ms
step:1674/1920 train_time:97837ms step_avg:58.45ms
step:1675/1920 train_time:97927ms step_avg:58.46ms
step:1676/1920 train_time:98016ms step_avg:58.48ms
step:1677/1920 train_time:98104ms step_avg:58.50ms
step:1678/1920 train_time:98192ms step_avg:58.52ms
step:1679/1920 train_time:98281ms step_avg:58.54ms
step:1680/1920 train_time:98370ms step_avg:58.55ms
step:1681/1920 train_time:98460ms step_avg:58.57ms
step:1682/1920 train_time:98547ms step_avg:58.59ms
step:1683/1920 train_time:98636ms step_avg:58.61ms
step:1684/1920 train_time:98723ms step_avg:58.62ms
step:1685/1920 train_time:98812ms step_avg:58.64ms
step:1686/1920 train_time:98900ms step_avg:58.66ms
step:1687/1920 train_time:98989ms step_avg:58.68ms
step:1688/1920 train_time:99077ms step_avg:58.69ms
step:1689/1920 train_time:99166ms step_avg:58.71ms
step:1690/1920 train_time:99255ms step_avg:58.73ms
step:1691/1920 train_time:99343ms step_avg:58.75ms
step:1692/1920 train_time:99431ms step_avg:58.77ms
step:1693/1920 train_time:99520ms step_avg:58.78ms
step:1694/1920 train_time:99608ms step_avg:58.80ms
step:1695/1920 train_time:99697ms step_avg:58.82ms
step:1696/1920 train_time:99785ms step_avg:58.84ms
step:1697/1920 train_time:99873ms step_avg:58.85ms
step:1698/1920 train_time:99961ms step_avg:58.87ms
step:1699/1920 train_time:100050ms step_avg:58.89ms
step:1700/1920 train_time:100138ms step_avg:58.90ms
step:1701/1920 train_time:100226ms step_avg:58.92ms
step:1702/1920 train_time:100314ms step_avg:58.94ms
step:1703/1920 train_time:100403ms step_avg:58.96ms
step:1704/1920 train_time:100491ms step_avg:58.97ms
step:1705/1920 train_time:100580ms step_avg:58.99ms
step:1706/1920 train_time:100668ms step_avg:59.01ms
step:1707/1920 train_time:100757ms step_avg:59.03ms
step:1708/1920 train_time:100844ms step_avg:59.04ms
step:1709/1920 train_time:100933ms step_avg:59.06ms
step:1710/1920 train_time:101021ms step_avg:59.08ms
step:1711/1920 train_time:101110ms step_avg:59.09ms
step:1712/1920 train_time:101199ms step_avg:59.11ms
step:1713/1920 train_time:101287ms step_avg:59.13ms
step:1714/1920 train_time:101375ms step_avg:59.15ms
step:1715/1920 train_time:101463ms step_avg:59.16ms
step:1716/1920 train_time:101551ms step_avg:59.18ms
step:1717/1920 train_time:101640ms step_avg:59.20ms
step:1718/1920 train_time:101727ms step_avg:59.21ms
step:1719/1920 train_time:101818ms step_avg:59.23ms
step:1720/1920 train_time:101905ms step_avg:59.25ms
step:1721/1920 train_time:101995ms step_avg:59.26ms
step:1722/1920 train_time:102083ms step_avg:59.28ms
step:1723/1920 train_time:102172ms step_avg:59.30ms
step:1724/1920 train_time:102260ms step_avg:59.32ms
step:1725/1920 train_time:102349ms step_avg:59.33ms
step:1726/1920 train_time:102438ms step_avg:59.35ms
step:1727/1920 train_time:102527ms step_avg:59.37ms
step:1728/1920 train_time:102614ms step_avg:59.38ms
step:1729/1920 train_time:102703ms step_avg:59.40ms
step:1730/1920 train_time:102791ms step_avg:59.42ms
step:1731/1920 train_time:102881ms step_avg:59.43ms
step:1732/1920 train_time:102970ms step_avg:59.45ms
step:1733/1920 train_time:103059ms step_avg:59.47ms
step:1734/1920 train_time:103147ms step_avg:59.49ms
step:1735/1920 train_time:103236ms step_avg:59.50ms
step:1736/1920 train_time:103324ms step_avg:59.52ms
step:1737/1920 train_time:103412ms step_avg:59.54ms
step:1738/1920 train_time:103501ms step_avg:59.55ms
step:1739/1920 train_time:103589ms step_avg:59.57ms
step:1740/1920 train_time:103678ms step_avg:59.58ms
step:1741/1920 train_time:103766ms step_avg:59.60ms
step:1742/1920 train_time:103854ms step_avg:59.62ms
step:1743/1920 train_time:103943ms step_avg:59.63ms
step:1744/1920 train_time:104031ms step_avg:59.65ms
step:1745/1920 train_time:104120ms step_avg:59.67ms
step:1746/1920 train_time:104209ms step_avg:59.68ms
step:1747/1920 train_time:104298ms step_avg:59.70ms
step:1748/1920 train_time:104385ms step_avg:59.72ms
step:1749/1920 train_time:104474ms step_avg:59.73ms
step:1750/1920 train_time:104561ms step_avg:59.75ms
step:1750/1920 val_loss:3.3240 train_time:104653ms step_avg:59.80ms
step:1751/1920 train_time:104672ms step_avg:59.78ms
step:1752/1920 train_time:104742ms step_avg:59.78ms
step:1753/1920 train_time:104834ms step_avg:59.80ms
step:1754/1920 train_time:104923ms step_avg:59.82ms
step:1755/1920 train_time:105011ms step_avg:59.84ms
step:1756/1920 train_time:105099ms step_avg:59.85ms
step:1757/1920 train_time:105186ms step_avg:59.87ms
step:1758/1920 train_time:105275ms step_avg:59.88ms
step:1759/1920 train_time:105362ms step_avg:59.90ms
step:1760/1920 train_time:105450ms step_avg:59.91ms
step:1761/1920 train_time:105540ms step_avg:59.93ms
step:1762/1920 train_time:105630ms step_avg:59.95ms
step:1763/1920 train_time:105721ms step_avg:59.97ms
step:1764/1920 train_time:105810ms step_avg:59.98ms
step:1765/1920 train_time:105900ms step_avg:60.00ms
step:1766/1920 train_time:105988ms step_avg:60.02ms
step:1767/1920 train_time:106077ms step_avg:60.03ms
step:1768/1920 train_time:106164ms step_avg:60.05ms
step:1769/1920 train_time:106253ms step_avg:60.06ms
step:1770/1920 train_time:106340ms step_avg:60.08ms
step:1771/1920 train_time:106428ms step_avg:60.09ms
step:1772/1920 train_time:106516ms step_avg:60.11ms
step:1773/1920 train_time:106605ms step_avg:60.13ms
step:1774/1920 train_time:106694ms step_avg:60.14ms
step:1775/1920 train_time:106783ms step_avg:60.16ms
step:1776/1920 train_time:106872ms step_avg:60.18ms
step:1777/1920 train_time:106962ms step_avg:60.19ms
step:1778/1920 train_time:107050ms step_avg:60.21ms
step:1779/1920 train_time:107139ms step_avg:60.22ms
step:1780/1920 train_time:107227ms step_avg:60.24ms
step:1781/1920 train_time:107316ms step_avg:60.26ms
step:1782/1920 train_time:107403ms step_avg:60.27ms
step:1783/1920 train_time:107492ms step_avg:60.29ms
step:1784/1920 train_time:107580ms step_avg:60.30ms
step:1785/1920 train_time:107671ms step_avg:60.32ms
step:1786/1920 train_time:107759ms step_avg:60.34ms
step:1787/1920 train_time:107849ms step_avg:60.35ms
step:1788/1920 train_time:107937ms step_avg:60.37ms
step:1789/1920 train_time:108025ms step_avg:60.38ms
step:1790/1920 train_time:108114ms step_avg:60.40ms
step:1791/1920 train_time:108202ms step_avg:60.41ms
step:1792/1920 train_time:108290ms step_avg:60.43ms
step:1793/1920 train_time:108379ms step_avg:60.45ms
step:1794/1920 train_time:108467ms step_avg:60.46ms
step:1795/1920 train_time:108556ms step_avg:60.48ms
step:1796/1920 train_time:108643ms step_avg:60.49ms
step:1797/1920 train_time:108733ms step_avg:60.51ms
step:1798/1920 train_time:108821ms step_avg:60.52ms
step:1799/1920 train_time:108910ms step_avg:60.54ms
step:1800/1920 train_time:108998ms step_avg:60.55ms
step:1801/1920 train_time:109087ms step_avg:60.57ms
step:1802/1920 train_time:109175ms step_avg:60.59ms
step:1803/1920 train_time:109263ms step_avg:60.60ms
step:1804/1920 train_time:109351ms step_avg:60.62ms
step:1805/1920 train_time:109441ms step_avg:60.63ms
step:1806/1920 train_time:109530ms step_avg:60.65ms
step:1807/1920 train_time:109619ms step_avg:60.66ms
step:1808/1920 train_time:109706ms step_avg:60.68ms
step:1809/1920 train_time:109795ms step_avg:60.69ms
step:1810/1920 train_time:109885ms step_avg:60.71ms
step:1811/1920 train_time:109973ms step_avg:60.73ms
step:1812/1920 train_time:110062ms step_avg:60.74ms
step:1813/1920 train_time:110151ms step_avg:60.76ms
step:1814/1920 train_time:110239ms step_avg:60.77ms
step:1815/1920 train_time:110327ms step_avg:60.79ms
step:1816/1920 train_time:110416ms step_avg:60.80ms
step:1817/1920 train_time:110504ms step_avg:60.82ms
step:1818/1920 train_time:110592ms step_avg:60.83ms
step:1819/1920 train_time:110682ms step_avg:60.85ms
step:1820/1920 train_time:110770ms step_avg:60.86ms
step:1821/1920 train_time:110859ms step_avg:60.88ms
step:1822/1920 train_time:110948ms step_avg:60.89ms
step:1823/1920 train_time:111037ms step_avg:60.91ms
step:1824/1920 train_time:111125ms step_avg:60.92ms
step:1825/1920 train_time:111212ms step_avg:60.94ms
step:1826/1920 train_time:111300ms step_avg:60.95ms
step:1827/1920 train_time:111390ms step_avg:60.97ms
step:1828/1920 train_time:111479ms step_avg:60.98ms
step:1829/1920 train_time:111567ms step_avg:61.00ms
step:1830/1920 train_time:111656ms step_avg:61.01ms
step:1831/1920 train_time:111744ms step_avg:61.03ms
step:1832/1920 train_time:111832ms step_avg:61.04ms
step:1833/1920 train_time:111921ms step_avg:61.06ms
step:1834/1920 train_time:112010ms step_avg:61.07ms
step:1835/1920 train_time:112100ms step_avg:61.09ms
step:1836/1920 train_time:112187ms step_avg:61.10ms
step:1837/1920 train_time:112276ms step_avg:61.12ms
step:1838/1920 train_time:112364ms step_avg:61.13ms
step:1839/1920 train_time:112453ms step_avg:61.15ms
step:1840/1920 train_time:112541ms step_avg:61.16ms
step:1841/1920 train_time:112629ms step_avg:61.18ms
step:1842/1920 train_time:112717ms step_avg:61.19ms
step:1843/1920 train_time:112806ms step_avg:61.21ms
step:1844/1920 train_time:112894ms step_avg:61.22ms
step:1845/1920 train_time:112982ms step_avg:61.24ms
step:1846/1920 train_time:113071ms step_avg:61.25ms
step:1847/1920 train_time:113159ms step_avg:61.27ms
step:1848/1920 train_time:113247ms step_avg:61.28ms
step:1849/1920 train_time:113335ms step_avg:61.30ms
step:1850/1920 train_time:113423ms step_avg:61.31ms
step:1851/1920 train_time:113512ms step_avg:61.32ms
step:1852/1920 train_time:113599ms step_avg:61.34ms
step:1853/1920 train_time:113688ms step_avg:61.35ms
step:1854/1920 train_time:113777ms step_avg:61.37ms
step:1855/1920 train_time:113866ms step_avg:61.38ms
step:1856/1920 train_time:113954ms step_avg:61.40ms
step:1857/1920 train_time:114043ms step_avg:61.41ms
step:1858/1920 train_time:114131ms step_avg:61.43ms
step:1859/1920 train_time:114220ms step_avg:61.44ms
step:1860/1920 train_time:114309ms step_avg:61.46ms
step:1861/1920 train_time:114398ms step_avg:61.47ms
step:1862/1920 train_time:114487ms step_avg:61.49ms
step:1863/1920 train_time:114576ms step_avg:61.50ms
step:1864/1920 train_time:114663ms step_avg:61.51ms
step:1865/1920 train_time:114752ms step_avg:61.53ms
step:1866/1920 train_time:114840ms step_avg:61.54ms
step:1867/1920 train_time:114929ms step_avg:61.56ms
step:1868/1920 train_time:115017ms step_avg:61.57ms
step:1869/1920 train_time:115106ms step_avg:61.59ms
step:1870/1920 train_time:115193ms step_avg:61.60ms
step:1871/1920 train_time:115282ms step_avg:61.62ms
step:1872/1920 train_time:115371ms step_avg:61.63ms
step:1873/1920 train_time:115461ms step_avg:61.64ms
step:1874/1920 train_time:115550ms step_avg:61.66ms
step:1875/1920 train_time:115640ms step_avg:61.67ms
step:1876/1920 train_time:115728ms step_avg:61.69ms
step:1877/1920 train_time:115818ms step_avg:61.70ms
step:1878/1920 train_time:115906ms step_avg:61.72ms
step:1879/1920 train_time:115994ms step_avg:61.73ms
step:1880/1920 train_time:116082ms step_avg:61.75ms
step:1881/1920 train_time:116172ms step_avg:61.76ms
step:1882/1920 train_time:116260ms step_avg:61.77ms
step:1883/1920 train_time:116349ms step_avg:61.79ms
step:1884/1920 train_time:116437ms step_avg:61.80ms
step:1885/1920 train_time:116526ms step_avg:61.82ms
step:1886/1920 train_time:116614ms step_avg:61.83ms
step:1887/1920 train_time:116704ms step_avg:61.85ms
step:1888/1920 train_time:116793ms step_avg:61.86ms
step:1889/1920 train_time:116881ms step_avg:61.87ms
step:1890/1920 train_time:116970ms step_avg:61.89ms
step:1891/1920 train_time:117060ms step_avg:61.90ms
step:1892/1920 train_time:117148ms step_avg:61.92ms
step:1893/1920 train_time:117238ms step_avg:61.93ms
step:1894/1920 train_time:117326ms step_avg:61.95ms
step:1895/1920 train_time:117414ms step_avg:61.96ms
step:1896/1920 train_time:117502ms step_avg:61.97ms
step:1897/1920 train_time:117592ms step_avg:61.99ms
step:1898/1920 train_time:117679ms step_avg:62.00ms
step:1899/1920 train_time:117769ms step_avg:62.02ms
step:1900/1920 train_time:117858ms step_avg:62.03ms
step:1901/1920 train_time:117948ms step_avg:62.05ms
step:1902/1920 train_time:118036ms step_avg:62.06ms
step:1903/1920 train_time:118124ms step_avg:62.07ms
step:1904/1920 train_time:118213ms step_avg:62.09ms
step:1905/1920 train_time:118302ms step_avg:62.10ms
step:1906/1920 train_time:118390ms step_avg:62.11ms
step:1907/1920 train_time:118480ms step_avg:62.13ms
step:1908/1920 train_time:118568ms step_avg:62.14ms
step:1909/1920 train_time:118658ms step_avg:62.16ms
step:1910/1920 train_time:118746ms step_avg:62.17ms
step:1911/1920 train_time:118836ms step_avg:62.19ms
step:1912/1920 train_time:118925ms step_avg:62.20ms
step:1913/1920 train_time:119014ms step_avg:62.21ms
step:1914/1920 train_time:119101ms step_avg:62.23ms
step:1915/1920 train_time:119191ms step_avg:62.24ms
step:1916/1920 train_time:119280ms step_avg:62.25ms
step:1917/1920 train_time:119369ms step_avg:62.27ms
step:1918/1920 train_time:119457ms step_avg:62.28ms
step:1919/1920 train_time:119546ms step_avg:62.30ms
step:1920/1920 train_time:119635ms step_avg:62.31ms
step:1920/1920 val_loss:3.2789 train_time:119726ms step_avg:62.36ms
peak memory allocated: 29863 MiB reserved: 45538 MiB

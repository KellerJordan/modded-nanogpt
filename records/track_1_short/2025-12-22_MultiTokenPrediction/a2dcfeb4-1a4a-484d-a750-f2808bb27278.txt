import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:38:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    250860      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    250861      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    250862      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    250863      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    250864      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    250865      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    250866      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    250867      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    250861      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    250862      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    250863      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    250864      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    250865      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    250866      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    250867      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8332 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:63ms step_avg:62.76ms
step:2/1920 train_time:85ms step_avg:42.63ms
step:3/1920 train_time:119ms step_avg:39.61ms
step:4/1920 train_time:153ms step_avg:38.17ms
step:5/1920 train_time:187ms step_avg:37.38ms
step:6/1920 train_time:252ms step_avg:42.05ms
step:7/1920 train_time:283ms step_avg:40.50ms
step:8/1920 train_time:318ms step_avg:39.70ms
step:9/1920 train_time:352ms step_avg:39.09ms
step:10/1920 train_time:386ms step_avg:38.59ms
step:11/1920 train_time:420ms step_avg:38.21ms
step:12/1920 train_time:454ms step_avg:37.86ms
step:13/1920 train_time:489ms step_avg:37.62ms
step:14/1920 train_time:523ms step_avg:37.37ms
step:15/1920 train_time:557ms step_avg:37.16ms
step:16/1920 train_time:592ms step_avg:36.97ms
step:17/1920 train_time:626ms step_avg:36.81ms
step:18/1920 train_time:660ms step_avg:36.67ms
step:19/1920 train_time:694ms step_avg:36.54ms
step:20/1920 train_time:728ms step_avg:36.42ms
step:21/1920 train_time:763ms step_avg:36.32ms
step:22/1920 train_time:797ms step_avg:36.22ms
step:23/1920 train_time:831ms step_avg:36.14ms
step:24/1920 train_time:865ms step_avg:36.06ms
step:25/1920 train_time:900ms step_avg:35.99ms
step:26/1920 train_time:934ms step_avg:35.92ms
step:27/1920 train_time:968ms step_avg:35.86ms
step:28/1920 train_time:1002ms step_avg:35.80ms
step:29/1920 train_time:1037ms step_avg:35.76ms
step:30/1920 train_time:1071ms step_avg:35.71ms
step:31/1920 train_time:1105ms step_avg:35.66ms
step:32/1920 train_time:1140ms step_avg:35.63ms
step:33/1920 train_time:1176ms step_avg:35.63ms
step:34/1920 train_time:1210ms step_avg:35.60ms
step:35/1920 train_time:1245ms step_avg:35.58ms
step:36/1920 train_time:1280ms step_avg:35.56ms
step:37/1920 train_time:1315ms step_avg:35.55ms
step:38/1920 train_time:1350ms step_avg:35.52ms
step:39/1920 train_time:1384ms step_avg:35.50ms
step:40/1920 train_time:1418ms step_avg:35.46ms
step:41/1920 train_time:1453ms step_avg:35.44ms
step:42/1920 train_time:1487ms step_avg:35.41ms
step:43/1920 train_time:1522ms step_avg:35.40ms
step:44/1920 train_time:1556ms step_avg:35.37ms
step:45/1920 train_time:1591ms step_avg:35.35ms
step:46/1920 train_time:1625ms step_avg:35.33ms
step:47/1920 train_time:1660ms step_avg:35.31ms
step:48/1920 train_time:1694ms step_avg:35.29ms
step:49/1920 train_time:1729ms step_avg:35.28ms
step:50/1920 train_time:1763ms step_avg:35.25ms
step:51/1920 train_time:1798ms step_avg:35.25ms
step:52/1920 train_time:1832ms step_avg:35.22ms
step:53/1920 train_time:1866ms step_avg:35.21ms
step:54/1920 train_time:1900ms step_avg:35.19ms
step:55/1920 train_time:1935ms step_avg:35.18ms
step:56/1920 train_time:1969ms step_avg:35.16ms
step:57/1920 train_time:2003ms step_avg:35.15ms
step:58/1920 train_time:2038ms step_avg:35.13ms
step:59/1920 train_time:2072ms step_avg:35.12ms
step:60/1920 train_time:2106ms step_avg:35.10ms
step:61/1920 train_time:2141ms step_avg:35.11ms
step:62/1920 train_time:2176ms step_avg:35.09ms
step:63/1920 train_time:2211ms step_avg:35.09ms
step:64/1920 train_time:2245ms step_avg:35.08ms
step:65/1920 train_time:2280ms step_avg:35.07ms
step:66/1920 train_time:2314ms step_avg:35.06ms
step:67/1920 train_time:2349ms step_avg:35.06ms
step:68/1920 train_time:2383ms step_avg:35.05ms
step:69/1920 train_time:2417ms step_avg:35.04ms
step:70/1920 train_time:2452ms step_avg:35.03ms
step:71/1920 train_time:2487ms step_avg:35.02ms
step:72/1920 train_time:2521ms step_avg:35.01ms
step:73/1920 train_time:2555ms step_avg:35.01ms
step:74/1920 train_time:2590ms step_avg:35.00ms
step:75/1920 train_time:2624ms step_avg:34.99ms
step:76/1920 train_time:2659ms step_avg:34.98ms
step:77/1920 train_time:2693ms step_avg:34.98ms
step:78/1920 train_time:2727ms step_avg:34.96ms
step:79/1920 train_time:2762ms step_avg:34.96ms
step:80/1920 train_time:2796ms step_avg:34.95ms
step:81/1920 train_time:2830ms step_avg:34.94ms
step:82/1920 train_time:2864ms step_avg:34.93ms
step:83/1920 train_time:2899ms step_avg:34.93ms
step:84/1920 train_time:2933ms step_avg:34.92ms
step:85/1920 train_time:2967ms step_avg:34.91ms
step:86/1920 train_time:3002ms step_avg:34.90ms
step:87/1920 train_time:3036ms step_avg:34.90ms
step:88/1920 train_time:3070ms step_avg:34.89ms
step:89/1920 train_time:3105ms step_avg:34.88ms
step:90/1920 train_time:3139ms step_avg:34.88ms
step:91/1920 train_time:3174ms step_avg:34.88ms
step:92/1920 train_time:3209ms step_avg:34.88ms
step:93/1920 train_time:3243ms step_avg:34.87ms
step:94/1920 train_time:3277ms step_avg:34.86ms
step:95/1920 train_time:3311ms step_avg:34.86ms
step:96/1920 train_time:3346ms step_avg:34.85ms
step:97/1920 train_time:3381ms step_avg:34.86ms
step:98/1920 train_time:3415ms step_avg:34.85ms
step:99/1920 train_time:3450ms step_avg:34.85ms
step:100/1920 train_time:3484ms step_avg:34.84ms
step:101/1920 train_time:3519ms step_avg:34.84ms
step:102/1920 train_time:3553ms step_avg:34.84ms
step:103/1920 train_time:3588ms step_avg:34.84ms
step:104/1920 train_time:3622ms step_avg:34.83ms
step:105/1920 train_time:3657ms step_avg:34.82ms
step:106/1920 train_time:3691ms step_avg:34.82ms
step:107/1920 train_time:3725ms step_avg:34.81ms
step:108/1920 train_time:3759ms step_avg:34.80ms
step:109/1920 train_time:3794ms step_avg:34.80ms
step:110/1920 train_time:3828ms step_avg:34.80ms
step:111/1920 train_time:3863ms step_avg:34.80ms
step:112/1920 train_time:3897ms step_avg:34.79ms
step:113/1920 train_time:3931ms step_avg:34.79ms
step:114/1920 train_time:3966ms step_avg:34.79ms
step:115/1920 train_time:4000ms step_avg:34.78ms
step:116/1920 train_time:4034ms step_avg:34.78ms
step:117/1920 train_time:4068ms step_avg:34.77ms
step:118/1920 train_time:4102ms step_avg:34.77ms
step:119/1920 train_time:4138ms step_avg:34.77ms
step:120/1920 train_time:4172ms step_avg:34.76ms
step:121/1920 train_time:4206ms step_avg:34.76ms
step:122/1920 train_time:4240ms step_avg:34.76ms
step:123/1920 train_time:4275ms step_avg:34.76ms
step:124/1920 train_time:4309ms step_avg:34.75ms
step:125/1920 train_time:4344ms step_avg:34.75ms
step:126/1920 train_time:4378ms step_avg:34.75ms
step:127/1920 train_time:4413ms step_avg:34.75ms
step:128/1920 train_time:4447ms step_avg:34.74ms
step:129/1920 train_time:4482ms step_avg:34.75ms
step:130/1920 train_time:4517ms step_avg:34.74ms
step:131/1920 train_time:4552ms step_avg:34.75ms
step:132/1920 train_time:4586ms step_avg:34.74ms
step:133/1920 train_time:4620ms step_avg:34.74ms
step:134/1920 train_time:4654ms step_avg:34.73ms
step:135/1920 train_time:4689ms step_avg:34.73ms
step:136/1920 train_time:4723ms step_avg:34.73ms
step:137/1920 train_time:4757ms step_avg:34.72ms
step:138/1920 train_time:4791ms step_avg:34.72ms
step:139/1920 train_time:4826ms step_avg:34.72ms
step:140/1920 train_time:4860ms step_avg:34.71ms
step:141/1920 train_time:4894ms step_avg:34.71ms
step:142/1920 train_time:4929ms step_avg:34.71ms
step:143/1920 train_time:4963ms step_avg:34.71ms
step:144/1920 train_time:4997ms step_avg:34.70ms
step:145/1920 train_time:5032ms step_avg:34.70ms
step:146/1920 train_time:5066ms step_avg:34.70ms
step:147/1920 train_time:5101ms step_avg:34.70ms
step:148/1920 train_time:5136ms step_avg:34.70ms
step:149/1920 train_time:5170ms step_avg:34.70ms
step:150/1920 train_time:5204ms step_avg:34.70ms
step:151/1920 train_time:5239ms step_avg:34.69ms
step:152/1920 train_time:5273ms step_avg:34.69ms
step:153/1920 train_time:5308ms step_avg:34.69ms
step:154/1920 train_time:5342ms step_avg:34.69ms
step:155/1920 train_time:5376ms step_avg:34.69ms
step:156/1920 train_time:5411ms step_avg:34.68ms
step:157/1920 train_time:5445ms step_avg:34.68ms
step:158/1920 train_time:5479ms step_avg:34.68ms
step:159/1920 train_time:5514ms step_avg:34.68ms
step:160/1920 train_time:5548ms step_avg:34.67ms
step:161/1920 train_time:5582ms step_avg:34.67ms
step:162/1920 train_time:5617ms step_avg:34.67ms
step:163/1920 train_time:5651ms step_avg:34.67ms
step:164/1920 train_time:5685ms step_avg:34.67ms
step:165/1920 train_time:5720ms step_avg:34.66ms
step:166/1920 train_time:5754ms step_avg:34.66ms
step:167/1920 train_time:5788ms step_avg:34.66ms
step:168/1920 train_time:5822ms step_avg:34.66ms
step:169/1920 train_time:5857ms step_avg:34.66ms
step:170/1920 train_time:5891ms step_avg:34.65ms
step:171/1920 train_time:5926ms step_avg:34.65ms
step:172/1920 train_time:5960ms step_avg:34.65ms
step:173/1920 train_time:5994ms step_avg:34.65ms
step:174/1920 train_time:6028ms step_avg:34.65ms
step:175/1920 train_time:6063ms step_avg:34.65ms
step:176/1920 train_time:6097ms step_avg:34.64ms
step:177/1920 train_time:6132ms step_avg:34.64ms
step:178/1920 train_time:6166ms step_avg:34.64ms
step:179/1920 train_time:6200ms step_avg:34.64ms
step:180/1920 train_time:6235ms step_avg:34.64ms
step:181/1920 train_time:6269ms step_avg:34.64ms
step:182/1920 train_time:6303ms step_avg:34.63ms
step:183/1920 train_time:6338ms step_avg:34.63ms
step:184/1920 train_time:6372ms step_avg:34.63ms
step:185/1920 train_time:6406ms step_avg:34.63ms
step:186/1920 train_time:6441ms step_avg:34.63ms
step:187/1920 train_time:6476ms step_avg:34.63ms
step:188/1920 train_time:6510ms step_avg:34.63ms
step:189/1920 train_time:6544ms step_avg:34.63ms
step:190/1920 train_time:6578ms step_avg:34.62ms
step:191/1920 train_time:6613ms step_avg:34.63ms
step:192/1920 train_time:6648ms step_avg:34.62ms
step:193/1920 train_time:6682ms step_avg:34.62ms
step:194/1920 train_time:6716ms step_avg:34.62ms
step:195/1920 train_time:6750ms step_avg:34.62ms
step:196/1920 train_time:6784ms step_avg:34.61ms
step:197/1920 train_time:6819ms step_avg:34.61ms
step:198/1920 train_time:6853ms step_avg:34.61ms
step:199/1920 train_time:6887ms step_avg:34.61ms
step:200/1920 train_time:6921ms step_avg:34.61ms
step:201/1920 train_time:6956ms step_avg:34.61ms
step:202/1920 train_time:6990ms step_avg:34.61ms
step:203/1920 train_time:7024ms step_avg:34.60ms
step:204/1920 train_time:7058ms step_avg:34.60ms
step:205/1920 train_time:7093ms step_avg:34.60ms
step:206/1920 train_time:7127ms step_avg:34.60ms
step:207/1920 train_time:7162ms step_avg:34.60ms
step:208/1920 train_time:7196ms step_avg:34.60ms
step:209/1920 train_time:7231ms step_avg:34.60ms
step:210/1920 train_time:7265ms step_avg:34.59ms
step:211/1920 train_time:7299ms step_avg:34.59ms
step:212/1920 train_time:7333ms step_avg:34.59ms
step:213/1920 train_time:7368ms step_avg:34.59ms
step:214/1920 train_time:7402ms step_avg:34.59ms
step:215/1920 train_time:7437ms step_avg:34.59ms
step:216/1920 train_time:7471ms step_avg:34.59ms
step:217/1920 train_time:7505ms step_avg:34.59ms
step:218/1920 train_time:7540ms step_avg:34.58ms
step:219/1920 train_time:7575ms step_avg:34.59ms
step:220/1920 train_time:7609ms step_avg:34.59ms
step:221/1920 train_time:7644ms step_avg:34.59ms
step:222/1920 train_time:7678ms step_avg:34.58ms
step:223/1920 train_time:7712ms step_avg:34.58ms
step:224/1920 train_time:7746ms step_avg:34.58ms
step:225/1920 train_time:7781ms step_avg:34.58ms
step:226/1920 train_time:7815ms step_avg:34.58ms
step:227/1920 train_time:7849ms step_avg:34.58ms
step:228/1920 train_time:7883ms step_avg:34.58ms
step:229/1920 train_time:7918ms step_avg:34.58ms
step:230/1920 train_time:7952ms step_avg:34.58ms
step:231/1920 train_time:7987ms step_avg:34.58ms
step:232/1920 train_time:8021ms step_avg:34.57ms
step:233/1920 train_time:8056ms step_avg:34.57ms
step:234/1920 train_time:8090ms step_avg:34.57ms
step:235/1920 train_time:8124ms step_avg:34.57ms
step:236/1920 train_time:8158ms step_avg:34.57ms
step:237/1920 train_time:8193ms step_avg:34.57ms
step:238/1920 train_time:8227ms step_avg:34.57ms
step:239/1920 train_time:8262ms step_avg:34.57ms
step:240/1920 train_time:8296ms step_avg:34.57ms
step:241/1920 train_time:8330ms step_avg:34.57ms
step:242/1920 train_time:8365ms step_avg:34.57ms
step:243/1920 train_time:8399ms step_avg:34.56ms
step:244/1920 train_time:8433ms step_avg:34.56ms
step:245/1920 train_time:8467ms step_avg:34.56ms
step:246/1920 train_time:8501ms step_avg:34.56ms
step:247/1920 train_time:8536ms step_avg:34.56ms
step:248/1920 train_time:8570ms step_avg:34.56ms
step:249/1920 train_time:8605ms step_avg:34.56ms
step:250/1920 train_time:8639ms step_avg:34.56ms
step:250/1920 val_loss:4.6141 train_time:8677ms step_avg:34.71ms
step:251/1920 train_time:8695ms step_avg:34.64ms
step:252/1920 train_time:8713ms step_avg:34.58ms
step:253/1920 train_time:8747ms step_avg:34.57ms
step:254/1920 train_time:8782ms step_avg:34.57ms
step:255/1920 train_time:8817ms step_avg:34.58ms
step:256/1920 train_time:8852ms step_avg:34.58ms
step:257/1920 train_time:8886ms step_avg:34.58ms
step:258/1920 train_time:8920ms step_avg:34.57ms
step:259/1920 train_time:8954ms step_avg:34.57ms
step:260/1920 train_time:8989ms step_avg:34.57ms
step:261/1920 train_time:9023ms step_avg:34.57ms
step:262/1920 train_time:9057ms step_avg:34.57ms
step:263/1920 train_time:9091ms step_avg:34.57ms
step:264/1920 train_time:9125ms step_avg:34.56ms
step:265/1920 train_time:9159ms step_avg:34.56ms
step:266/1920 train_time:9193ms step_avg:34.56ms
step:267/1920 train_time:9227ms step_avg:34.56ms
step:268/1920 train_time:9261ms step_avg:34.56ms
step:269/1920 train_time:9295ms step_avg:34.56ms
step:270/1920 train_time:9329ms step_avg:34.55ms
step:271/1920 train_time:9364ms step_avg:34.55ms
step:272/1920 train_time:9398ms step_avg:34.55ms
step:273/1920 train_time:9432ms step_avg:34.55ms
step:274/1920 train_time:9466ms step_avg:34.55ms
step:275/1920 train_time:9500ms step_avg:34.55ms
step:276/1920 train_time:9534ms step_avg:34.54ms
step:277/1920 train_time:9568ms step_avg:34.54ms
step:278/1920 train_time:9602ms step_avg:34.54ms
step:279/1920 train_time:9638ms step_avg:34.54ms
step:280/1920 train_time:9673ms step_avg:34.55ms
step:281/1920 train_time:9707ms step_avg:34.54ms
step:282/1920 train_time:9741ms step_avg:34.54ms
step:283/1920 train_time:9776ms step_avg:34.55ms
step:284/1920 train_time:9811ms step_avg:34.54ms
step:285/1920 train_time:9845ms step_avg:34.54ms
step:286/1920 train_time:9879ms step_avg:34.54ms
step:287/1920 train_time:9914ms step_avg:34.54ms
step:288/1920 train_time:9948ms step_avg:34.54ms
step:289/1920 train_time:9982ms step_avg:34.54ms
step:290/1920 train_time:10016ms step_avg:34.54ms
step:291/1920 train_time:10051ms step_avg:34.54ms
step:292/1920 train_time:10085ms step_avg:34.54ms
step:293/1920 train_time:10119ms step_avg:34.54ms
step:294/1920 train_time:10153ms step_avg:34.54ms
step:295/1920 train_time:10187ms step_avg:34.53ms
step:296/1920 train_time:10221ms step_avg:34.53ms
step:297/1920 train_time:10256ms step_avg:34.53ms
step:298/1920 train_time:10290ms step_avg:34.53ms
step:299/1920 train_time:10324ms step_avg:34.53ms
step:300/1920 train_time:10358ms step_avg:34.53ms
step:301/1920 train_time:10393ms step_avg:34.53ms
step:302/1920 train_time:10427ms step_avg:34.53ms
step:303/1920 train_time:10461ms step_avg:34.52ms
step:304/1920 train_time:10495ms step_avg:34.52ms
step:305/1920 train_time:10529ms step_avg:34.52ms
step:306/1920 train_time:10563ms step_avg:34.52ms
step:307/1920 train_time:10597ms step_avg:34.52ms
step:308/1920 train_time:10632ms step_avg:34.52ms
step:309/1920 train_time:10666ms step_avg:34.52ms
step:310/1920 train_time:10700ms step_avg:34.52ms
step:311/1920 train_time:10735ms step_avg:34.52ms
step:312/1920 train_time:10770ms step_avg:34.52ms
step:313/1920 train_time:10804ms step_avg:34.52ms
step:314/1920 train_time:10838ms step_avg:34.52ms
step:315/1920 train_time:10872ms step_avg:34.52ms
step:316/1920 train_time:10907ms step_avg:34.51ms
step:317/1920 train_time:10941ms step_avg:34.51ms
step:318/1920 train_time:10976ms step_avg:34.51ms
step:319/1920 train_time:11010ms step_avg:34.51ms
step:320/1920 train_time:11044ms step_avg:34.51ms
step:321/1920 train_time:11078ms step_avg:34.51ms
step:322/1920 train_time:11113ms step_avg:34.51ms
step:323/1920 train_time:11147ms step_avg:34.51ms
step:324/1920 train_time:11182ms step_avg:34.51ms
step:325/1920 train_time:11216ms step_avg:34.51ms
step:326/1920 train_time:11250ms step_avg:34.51ms
step:327/1920 train_time:11284ms step_avg:34.51ms
step:328/1920 train_time:11318ms step_avg:34.51ms
step:329/1920 train_time:11352ms step_avg:34.51ms
step:330/1920 train_time:11386ms step_avg:34.50ms
step:331/1920 train_time:11421ms step_avg:34.50ms
step:332/1920 train_time:11455ms step_avg:34.50ms
step:333/1920 train_time:11489ms step_avg:34.50ms
step:334/1920 train_time:11523ms step_avg:34.50ms
step:335/1920 train_time:11558ms step_avg:34.50ms
step:336/1920 train_time:11592ms step_avg:34.50ms
step:337/1920 train_time:11626ms step_avg:34.50ms
step:338/1920 train_time:11660ms step_avg:34.50ms
step:339/1920 train_time:11695ms step_avg:34.50ms
step:340/1920 train_time:11729ms step_avg:34.50ms
step:341/1920 train_time:11763ms step_avg:34.50ms
step:342/1920 train_time:11797ms step_avg:34.50ms
step:343/1920 train_time:11832ms step_avg:34.49ms
step:344/1920 train_time:11866ms step_avg:34.49ms
step:345/1920 train_time:11901ms step_avg:34.50ms
step:346/1920 train_time:11935ms step_avg:34.50ms
step:347/1920 train_time:11970ms step_avg:34.49ms
step:348/1920 train_time:12004ms step_avg:34.50ms
step:349/1920 train_time:12039ms step_avg:34.50ms
step:350/1920 train_time:12074ms step_avg:34.50ms
step:351/1920 train_time:12108ms step_avg:34.49ms
step:352/1920 train_time:12142ms step_avg:34.49ms
step:353/1920 train_time:12176ms step_avg:34.49ms
step:354/1920 train_time:12210ms step_avg:34.49ms
step:355/1920 train_time:12244ms step_avg:34.49ms
step:356/1920 train_time:12278ms step_avg:34.49ms
step:357/1920 train_time:12313ms step_avg:34.49ms
step:358/1920 train_time:12347ms step_avg:34.49ms
step:359/1920 train_time:12381ms step_avg:34.49ms
step:360/1920 train_time:12415ms step_avg:34.49ms
step:361/1920 train_time:12449ms step_avg:34.49ms
step:362/1920 train_time:12483ms step_avg:34.48ms
step:363/1920 train_time:12517ms step_avg:34.48ms
step:364/1920 train_time:12551ms step_avg:34.48ms
step:365/1920 train_time:12586ms step_avg:34.48ms
step:366/1920 train_time:12620ms step_avg:34.48ms
step:367/1920 train_time:12654ms step_avg:34.48ms
step:368/1920 train_time:12688ms step_avg:34.48ms
step:369/1920 train_time:12723ms step_avg:34.48ms
step:370/1920 train_time:12757ms step_avg:34.48ms
step:371/1920 train_time:12792ms step_avg:34.48ms
step:372/1920 train_time:12826ms step_avg:34.48ms
step:373/1920 train_time:12861ms step_avg:34.48ms
step:374/1920 train_time:12895ms step_avg:34.48ms
step:375/1920 train_time:12929ms step_avg:34.48ms
step:376/1920 train_time:12964ms step_avg:34.48ms
step:377/1920 train_time:12998ms step_avg:34.48ms
step:378/1920 train_time:13033ms step_avg:34.48ms
step:379/1920 train_time:13067ms step_avg:34.48ms
step:380/1920 train_time:13102ms step_avg:34.48ms
step:381/1920 train_time:13136ms step_avg:34.48ms
step:382/1920 train_time:13170ms step_avg:34.48ms
step:383/1920 train_time:13204ms step_avg:34.48ms
step:384/1920 train_time:13238ms step_avg:34.47ms
step:385/1920 train_time:13273ms step_avg:34.47ms
step:386/1920 train_time:13307ms step_avg:34.47ms
step:387/1920 train_time:13341ms step_avg:34.47ms
step:388/1920 train_time:13376ms step_avg:34.47ms
step:389/1920 train_time:13410ms step_avg:34.47ms
step:390/1920 train_time:13444ms step_avg:34.47ms
step:391/1920 train_time:13478ms step_avg:34.47ms
step:392/1920 train_time:13512ms step_avg:34.47ms
step:393/1920 train_time:13547ms step_avg:34.47ms
step:394/1920 train_time:13581ms step_avg:34.47ms
step:395/1920 train_time:13615ms step_avg:34.47ms
step:396/1920 train_time:13650ms step_avg:34.47ms
step:397/1920 train_time:13684ms step_avg:34.47ms
step:398/1920 train_time:13718ms step_avg:34.47ms
step:399/1920 train_time:13752ms step_avg:34.47ms
step:400/1920 train_time:13786ms step_avg:34.47ms
step:401/1920 train_time:13820ms step_avg:34.46ms
step:402/1920 train_time:13855ms step_avg:34.46ms
step:403/1920 train_time:13889ms step_avg:34.46ms
step:404/1920 train_time:13923ms step_avg:34.46ms
step:405/1920 train_time:13958ms step_avg:34.46ms
step:406/1920 train_time:13992ms step_avg:34.46ms
step:407/1920 train_time:14027ms step_avg:34.46ms
step:408/1920 train_time:14061ms step_avg:34.46ms
step:409/1920 train_time:14095ms step_avg:34.46ms
step:410/1920 train_time:14129ms step_avg:34.46ms
step:411/1920 train_time:14164ms step_avg:34.46ms
step:412/1920 train_time:14198ms step_avg:34.46ms
step:413/1920 train_time:14233ms step_avg:34.46ms
step:414/1920 train_time:14267ms step_avg:34.46ms
step:415/1920 train_time:14302ms step_avg:34.46ms
step:416/1920 train_time:14336ms step_avg:34.46ms
step:417/1920 train_time:14370ms step_avg:34.46ms
step:418/1920 train_time:14404ms step_avg:34.46ms
step:419/1920 train_time:14439ms step_avg:34.46ms
step:420/1920 train_time:14473ms step_avg:34.46ms
step:421/1920 train_time:14507ms step_avg:34.46ms
step:422/1920 train_time:14542ms step_avg:34.46ms
step:423/1920 train_time:14576ms step_avg:34.46ms
step:424/1920 train_time:14610ms step_avg:34.46ms
step:425/1920 train_time:14645ms step_avg:34.46ms
step:426/1920 train_time:14679ms step_avg:34.46ms
step:427/1920 train_time:14714ms step_avg:34.46ms
step:428/1920 train_time:14748ms step_avg:34.46ms
step:429/1920 train_time:14782ms step_avg:34.46ms
step:430/1920 train_time:14816ms step_avg:34.46ms
step:431/1920 train_time:14850ms step_avg:34.46ms
step:432/1920 train_time:14884ms step_avg:34.45ms
step:433/1920 train_time:14919ms step_avg:34.46ms
step:434/1920 train_time:14953ms step_avg:34.45ms
step:435/1920 train_time:14988ms step_avg:34.45ms
step:436/1920 train_time:15022ms step_avg:34.45ms
step:437/1920 train_time:15056ms step_avg:34.45ms
step:438/1920 train_time:15090ms step_avg:34.45ms
step:439/1920 train_time:15125ms step_avg:34.45ms
step:440/1920 train_time:15159ms step_avg:34.45ms
step:441/1920 train_time:15194ms step_avg:34.45ms
step:442/1920 train_time:15228ms step_avg:34.45ms
step:443/1920 train_time:15262ms step_avg:34.45ms
step:444/1920 train_time:15296ms step_avg:34.45ms
step:445/1920 train_time:15330ms step_avg:34.45ms
step:446/1920 train_time:15364ms step_avg:34.45ms
step:447/1920 train_time:15399ms step_avg:34.45ms
step:448/1920 train_time:15433ms step_avg:34.45ms
step:449/1920 train_time:15468ms step_avg:34.45ms
step:450/1920 train_time:15502ms step_avg:34.45ms
step:451/1920 train_time:15536ms step_avg:34.45ms
step:452/1920 train_time:15571ms step_avg:34.45ms
step:453/1920 train_time:15605ms step_avg:34.45ms
step:454/1920 train_time:15639ms step_avg:34.45ms
step:455/1920 train_time:15674ms step_avg:34.45ms
step:456/1920 train_time:15708ms step_avg:34.45ms
step:457/1920 train_time:15743ms step_avg:34.45ms
step:458/1920 train_time:15777ms step_avg:34.45ms
step:459/1920 train_time:15811ms step_avg:34.45ms
step:460/1920 train_time:15846ms step_avg:34.45ms
step:461/1920 train_time:15881ms step_avg:34.45ms
step:462/1920 train_time:15915ms step_avg:34.45ms
step:463/1920 train_time:15949ms step_avg:34.45ms
step:464/1920 train_time:15983ms step_avg:34.45ms
step:465/1920 train_time:16018ms step_avg:34.45ms
step:466/1920 train_time:16052ms step_avg:34.45ms
step:467/1920 train_time:16086ms step_avg:34.45ms
step:468/1920 train_time:16120ms step_avg:34.45ms
step:469/1920 train_time:16155ms step_avg:34.45ms
step:470/1920 train_time:16189ms step_avg:34.45ms
step:471/1920 train_time:16224ms step_avg:34.45ms
step:472/1920 train_time:16258ms step_avg:34.45ms
step:473/1920 train_time:16293ms step_avg:34.45ms
step:474/1920 train_time:16327ms step_avg:34.44ms
step:475/1920 train_time:16361ms step_avg:34.44ms
step:476/1920 train_time:16395ms step_avg:34.44ms
step:477/1920 train_time:16429ms step_avg:34.44ms
step:478/1920 train_time:16463ms step_avg:34.44ms
step:479/1920 train_time:16498ms step_avg:34.44ms
step:480/1920 train_time:16532ms step_avg:34.44ms
step:481/1920 train_time:16566ms step_avg:34.44ms
step:482/1920 train_time:16601ms step_avg:34.44ms
step:483/1920 train_time:16635ms step_avg:34.44ms
step:484/1920 train_time:16669ms step_avg:34.44ms
step:485/1920 train_time:16703ms step_avg:34.44ms
step:486/1920 train_time:16737ms step_avg:34.44ms
step:487/1920 train_time:16772ms step_avg:34.44ms
step:488/1920 train_time:16806ms step_avg:34.44ms
step:489/1920 train_time:16840ms step_avg:34.44ms
step:490/1920 train_time:16875ms step_avg:34.44ms
step:491/1920 train_time:16908ms step_avg:34.44ms
step:492/1920 train_time:16943ms step_avg:34.44ms
step:493/1920 train_time:16977ms step_avg:34.44ms
step:494/1920 train_time:17011ms step_avg:34.44ms
step:495/1920 train_time:17046ms step_avg:34.44ms
step:496/1920 train_time:17080ms step_avg:34.44ms
step:497/1920 train_time:17114ms step_avg:34.44ms
step:498/1920 train_time:17149ms step_avg:34.43ms
step:499/1920 train_time:17183ms step_avg:34.43ms
step:500/1920 train_time:17217ms step_avg:34.43ms
step:500/1920 val_loss:4.2871 train_time:17254ms step_avg:34.51ms
step:501/1920 train_time:17272ms step_avg:34.47ms
step:502/1920 train_time:17289ms step_avg:34.44ms
step:503/1920 train_time:17324ms step_avg:34.44ms
step:504/1920 train_time:17359ms step_avg:34.44ms
step:505/1920 train_time:17395ms step_avg:34.44ms
step:506/1920 train_time:17429ms step_avg:34.45ms
step:507/1920 train_time:17464ms step_avg:34.45ms
step:508/1920 train_time:17498ms step_avg:34.44ms
step:509/1920 train_time:17532ms step_avg:34.44ms
step:510/1920 train_time:17566ms step_avg:34.44ms
step:511/1920 train_time:17601ms step_avg:34.44ms
step:512/1920 train_time:17635ms step_avg:34.44ms
step:513/1920 train_time:17669ms step_avg:34.44ms
step:514/1920 train_time:17703ms step_avg:34.44ms
step:515/1920 train_time:17737ms step_avg:34.44ms
step:516/1920 train_time:17771ms step_avg:34.44ms
step:517/1920 train_time:17805ms step_avg:34.44ms
step:518/1920 train_time:17839ms step_avg:34.44ms
step:519/1920 train_time:17873ms step_avg:34.44ms
step:520/1920 train_time:17907ms step_avg:34.44ms
step:521/1920 train_time:17942ms step_avg:34.44ms
step:522/1920 train_time:17976ms step_avg:34.44ms
step:523/1920 train_time:18010ms step_avg:34.44ms
step:524/1920 train_time:18044ms step_avg:34.43ms
step:525/1920 train_time:18078ms step_avg:34.43ms
step:526/1920 train_time:18112ms step_avg:34.43ms
step:527/1920 train_time:18146ms step_avg:34.43ms
step:528/1920 train_time:18180ms step_avg:34.43ms
step:529/1920 train_time:18215ms step_avg:34.43ms
step:530/1920 train_time:18249ms step_avg:34.43ms
step:531/1920 train_time:18283ms step_avg:34.43ms
step:532/1920 train_time:18318ms step_avg:34.43ms
step:533/1920 train_time:18352ms step_avg:34.43ms
step:534/1920 train_time:18386ms step_avg:34.43ms
step:535/1920 train_time:18421ms step_avg:34.43ms
step:536/1920 train_time:18456ms step_avg:34.43ms
step:537/1920 train_time:18491ms step_avg:34.43ms
step:538/1920 train_time:18525ms step_avg:34.43ms
step:539/1920 train_time:18560ms step_avg:34.43ms
step:540/1920 train_time:18594ms step_avg:34.43ms
step:541/1920 train_time:18628ms step_avg:34.43ms
step:542/1920 train_time:18662ms step_avg:34.43ms
step:543/1920 train_time:18696ms step_avg:34.43ms
step:544/1920 train_time:18731ms step_avg:34.43ms
step:545/1920 train_time:18765ms step_avg:34.43ms
step:546/1920 train_time:18799ms step_avg:34.43ms
step:547/1920 train_time:18834ms step_avg:34.43ms
step:548/1920 train_time:18868ms step_avg:34.43ms
step:549/1920 train_time:18902ms step_avg:34.43ms
step:550/1920 train_time:18937ms step_avg:34.43ms
step:551/1920 train_time:18971ms step_avg:34.43ms
step:552/1920 train_time:19005ms step_avg:34.43ms
step:553/1920 train_time:19039ms step_avg:34.43ms
step:554/1920 train_time:19073ms step_avg:34.43ms
step:555/1920 train_time:19107ms step_avg:34.43ms
step:556/1920 train_time:19141ms step_avg:34.43ms
step:557/1920 train_time:19175ms step_avg:34.43ms
step:558/1920 train_time:19209ms step_avg:34.43ms
step:559/1920 train_time:19244ms step_avg:34.42ms
step:560/1920 train_time:19278ms step_avg:34.42ms
step:561/1920 train_time:19312ms step_avg:34.42ms
step:562/1920 train_time:19346ms step_avg:34.42ms
step:563/1920 train_time:19381ms step_avg:34.43ms
step:564/1920 train_time:19416ms step_avg:34.42ms
step:565/1920 train_time:19450ms step_avg:34.43ms
step:566/1920 train_time:19484ms step_avg:34.42ms
step:567/1920 train_time:19520ms step_avg:34.43ms
step:568/1920 train_time:19554ms step_avg:34.43ms
step:569/1920 train_time:19588ms step_avg:34.43ms
step:570/1920 train_time:19622ms step_avg:34.43ms
step:571/1920 train_time:19657ms step_avg:34.43ms
step:572/1920 train_time:19691ms step_avg:34.42ms
step:573/1920 train_time:19725ms step_avg:34.42ms
step:574/1920 train_time:19759ms step_avg:34.42ms
step:575/1920 train_time:19793ms step_avg:34.42ms
step:576/1920 train_time:19827ms step_avg:34.42ms
step:577/1920 train_time:19862ms step_avg:34.42ms
step:578/1920 train_time:19896ms step_avg:34.42ms
step:579/1920 train_time:19930ms step_avg:34.42ms
step:580/1920 train_time:19965ms step_avg:34.42ms
step:581/1920 train_time:19999ms step_avg:34.42ms
step:582/1920 train_time:20033ms step_avg:34.42ms
step:583/1920 train_time:20067ms step_avg:34.42ms
step:584/1920 train_time:20101ms step_avg:34.42ms
step:585/1920 train_time:20136ms step_avg:34.42ms
step:586/1920 train_time:20170ms step_avg:34.42ms
step:587/1920 train_time:20204ms step_avg:34.42ms
step:588/1920 train_time:20238ms step_avg:34.42ms
step:589/1920 train_time:20272ms step_avg:34.42ms
step:590/1920 train_time:20306ms step_avg:34.42ms
step:591/1920 train_time:20341ms step_avg:34.42ms
step:592/1920 train_time:20375ms step_avg:34.42ms
step:593/1920 train_time:20410ms step_avg:34.42ms
step:594/1920 train_time:20444ms step_avg:34.42ms
step:595/1920 train_time:20479ms step_avg:34.42ms
step:596/1920 train_time:20513ms step_avg:34.42ms
step:597/1920 train_time:20548ms step_avg:34.42ms
step:598/1920 train_time:20582ms step_avg:34.42ms
step:599/1920 train_time:20617ms step_avg:34.42ms
step:600/1920 train_time:20651ms step_avg:34.42ms
step:601/1920 train_time:20686ms step_avg:34.42ms
step:602/1920 train_time:20720ms step_avg:34.42ms
step:603/1920 train_time:20754ms step_avg:34.42ms
step:604/1920 train_time:20788ms step_avg:34.42ms
step:605/1920 train_time:20823ms step_avg:34.42ms
step:606/1920 train_time:20857ms step_avg:34.42ms
step:607/1920 train_time:20891ms step_avg:34.42ms
step:608/1920 train_time:20925ms step_avg:34.42ms
step:609/1920 train_time:20960ms step_avg:34.42ms
step:610/1920 train_time:20994ms step_avg:34.42ms
step:611/1920 train_time:21029ms step_avg:34.42ms
step:612/1920 train_time:21063ms step_avg:34.42ms
step:613/1920 train_time:21098ms step_avg:34.42ms
step:614/1920 train_time:21132ms step_avg:34.42ms
step:615/1920 train_time:21166ms step_avg:34.42ms
step:616/1920 train_time:21200ms step_avg:34.42ms
step:617/1920 train_time:21234ms step_avg:34.42ms
step:618/1920 train_time:21269ms step_avg:34.42ms
step:619/1920 train_time:21303ms step_avg:34.42ms
step:620/1920 train_time:21337ms step_avg:34.41ms
step:621/1920 train_time:21371ms step_avg:34.41ms
step:622/1920 train_time:21405ms step_avg:34.41ms
step:623/1920 train_time:21440ms step_avg:34.41ms
step:624/1920 train_time:21474ms step_avg:34.41ms
step:625/1920 train_time:21509ms step_avg:34.41ms
step:626/1920 train_time:21543ms step_avg:34.41ms
step:627/1920 train_time:21578ms step_avg:34.41ms
step:628/1920 train_time:21613ms step_avg:34.42ms
step:629/1920 train_time:21674ms step_avg:34.46ms
step:630/1920 train_time:21736ms step_avg:34.50ms
step:631/1920 train_time:21798ms step_avg:34.55ms
step:632/1920 train_time:21860ms step_avg:34.59ms
step:633/1920 train_time:21922ms step_avg:34.63ms
step:634/1920 train_time:21984ms step_avg:34.67ms
step:635/1920 train_time:22047ms step_avg:34.72ms
step:636/1920 train_time:22109ms step_avg:34.76ms
step:637/1920 train_time:22172ms step_avg:34.81ms
step:638/1920 train_time:22233ms step_avg:34.85ms
step:639/1920 train_time:22296ms step_avg:34.89ms
step:640/1920 train_time:22358ms step_avg:34.93ms
step:641/1920 train_time:22420ms step_avg:34.98ms
step:642/1920 train_time:22482ms step_avg:35.02ms
step:643/1920 train_time:22545ms step_avg:35.06ms
step:644/1920 train_time:22606ms step_avg:35.10ms
step:645/1920 train_time:22669ms step_avg:35.15ms
step:646/1920 train_time:22731ms step_avg:35.19ms
step:647/1920 train_time:22795ms step_avg:35.23ms
step:648/1920 train_time:22856ms step_avg:35.27ms
step:649/1920 train_time:22918ms step_avg:35.31ms
step:650/1920 train_time:22980ms step_avg:35.35ms
step:651/1920 train_time:23043ms step_avg:35.40ms
step:652/1920 train_time:23104ms step_avg:35.44ms
step:653/1920 train_time:23166ms step_avg:35.48ms
step:654/1920 train_time:23227ms step_avg:35.52ms
step:655/1920 train_time:23290ms step_avg:35.56ms
step:656/1920 train_time:23352ms step_avg:35.60ms
step:657/1920 train_time:23415ms step_avg:35.64ms
step:658/1920 train_time:23477ms step_avg:35.68ms
step:659/1920 train_time:23539ms step_avg:35.72ms
step:660/1920 train_time:23601ms step_avg:35.76ms
step:661/1920 train_time:23663ms step_avg:35.80ms
step:662/1920 train_time:23725ms step_avg:35.84ms
step:663/1920 train_time:23788ms step_avg:35.88ms
step:664/1920 train_time:23850ms step_avg:35.92ms
step:665/1920 train_time:23913ms step_avg:35.96ms
step:666/1920 train_time:23975ms step_avg:36.00ms
step:667/1920 train_time:24037ms step_avg:36.04ms
step:668/1920 train_time:24099ms step_avg:36.08ms
step:669/1920 train_time:24162ms step_avg:36.12ms
step:670/1920 train_time:24224ms step_avg:36.15ms
step:671/1920 train_time:24286ms step_avg:36.19ms
step:672/1920 train_time:24348ms step_avg:36.23ms
step:673/1920 train_time:24411ms step_avg:36.27ms
step:674/1920 train_time:24473ms step_avg:36.31ms
step:675/1920 train_time:24537ms step_avg:36.35ms
step:676/1920 train_time:24599ms step_avg:36.39ms
step:677/1920 train_time:24662ms step_avg:36.43ms
step:678/1920 train_time:24723ms step_avg:36.47ms
step:679/1920 train_time:24786ms step_avg:36.50ms
step:680/1920 train_time:24847ms step_avg:36.54ms
step:681/1920 train_time:24910ms step_avg:36.58ms
step:682/1920 train_time:24972ms step_avg:36.62ms
step:683/1920 train_time:25035ms step_avg:36.65ms
step:684/1920 train_time:25097ms step_avg:36.69ms
step:685/1920 train_time:25159ms step_avg:36.73ms
step:686/1920 train_time:25221ms step_avg:36.77ms
step:687/1920 train_time:25283ms step_avg:36.80ms
step:688/1920 train_time:25345ms step_avg:36.84ms
step:689/1920 train_time:25408ms step_avg:36.88ms
step:690/1920 train_time:25470ms step_avg:36.91ms
step:691/1920 train_time:25533ms step_avg:36.95ms
step:692/1920 train_time:25595ms step_avg:36.99ms
step:693/1920 train_time:25658ms step_avg:37.02ms
step:694/1920 train_time:25720ms step_avg:37.06ms
step:695/1920 train_time:25782ms step_avg:37.10ms
step:696/1920 train_time:25844ms step_avg:37.13ms
step:697/1920 train_time:25907ms step_avg:37.17ms
step:698/1920 train_time:25969ms step_avg:37.20ms
step:699/1920 train_time:26032ms step_avg:37.24ms
step:700/1920 train_time:26095ms step_avg:37.28ms
step:701/1920 train_time:26158ms step_avg:37.32ms
step:702/1920 train_time:26220ms step_avg:37.35ms
step:703/1920 train_time:26282ms step_avg:37.39ms
step:704/1920 train_time:26344ms step_avg:37.42ms
step:705/1920 train_time:26406ms step_avg:37.46ms
step:706/1920 train_time:26468ms step_avg:37.49ms
step:707/1920 train_time:26530ms step_avg:37.53ms
step:708/1920 train_time:26593ms step_avg:37.56ms
step:709/1920 train_time:26656ms step_avg:37.60ms
step:710/1920 train_time:26718ms step_avg:37.63ms
step:711/1920 train_time:26780ms step_avg:37.67ms
step:712/1920 train_time:26842ms step_avg:37.70ms
step:713/1920 train_time:26904ms step_avg:37.73ms
step:714/1920 train_time:26965ms step_avg:37.77ms
step:715/1920 train_time:27028ms step_avg:37.80ms
step:716/1920 train_time:27090ms step_avg:37.84ms
step:717/1920 train_time:27153ms step_avg:37.87ms
step:718/1920 train_time:27215ms step_avg:37.90ms
step:719/1920 train_time:27278ms step_avg:37.94ms
step:720/1920 train_time:27339ms step_avg:37.97ms
step:721/1920 train_time:27402ms step_avg:38.01ms
step:722/1920 train_time:27465ms step_avg:38.04ms
step:723/1920 train_time:27528ms step_avg:38.07ms
step:724/1920 train_time:27589ms step_avg:38.11ms
step:725/1920 train_time:27652ms step_avg:38.14ms
step:726/1920 train_time:27714ms step_avg:38.17ms
step:727/1920 train_time:27777ms step_avg:38.21ms
step:728/1920 train_time:27838ms step_avg:38.24ms
step:729/1920 train_time:27900ms step_avg:38.27ms
step:730/1920 train_time:27962ms step_avg:38.30ms
step:731/1920 train_time:28025ms step_avg:38.34ms
step:732/1920 train_time:28087ms step_avg:38.37ms
step:733/1920 train_time:28150ms step_avg:38.40ms
step:734/1920 train_time:28212ms step_avg:38.44ms
step:735/1920 train_time:28275ms step_avg:38.47ms
step:736/1920 train_time:28336ms step_avg:38.50ms
step:737/1920 train_time:28399ms step_avg:38.53ms
step:738/1920 train_time:28461ms step_avg:38.57ms
step:739/1920 train_time:28524ms step_avg:38.60ms
step:740/1920 train_time:28585ms step_avg:38.63ms
step:741/1920 train_time:28648ms step_avg:38.66ms
step:742/1920 train_time:28710ms step_avg:38.69ms
step:743/1920 train_time:28773ms step_avg:38.73ms
step:744/1920 train_time:28835ms step_avg:38.76ms
step:745/1920 train_time:28897ms step_avg:38.79ms
step:746/1920 train_time:28959ms step_avg:38.82ms
step:747/1920 train_time:29022ms step_avg:38.85ms
step:748/1920 train_time:29084ms step_avg:38.88ms
step:749/1920 train_time:29146ms step_avg:38.91ms
step:750/1920 train_time:29208ms step_avg:38.94ms
step:750/1920 val_loss:4.0467 train_time:29274ms step_avg:39.03ms
step:751/1920 train_time:29292ms step_avg:39.00ms
step:752/1920 train_time:29334ms step_avg:39.01ms
step:753/1920 train_time:29401ms step_avg:39.05ms
step:754/1920 train_time:29469ms step_avg:39.08ms
step:755/1920 train_time:29533ms step_avg:39.12ms
step:756/1920 train_time:29595ms step_avg:39.15ms
step:757/1920 train_time:29657ms step_avg:39.18ms
step:758/1920 train_time:29718ms step_avg:39.21ms
step:759/1920 train_time:29779ms step_avg:39.24ms
step:760/1920 train_time:29840ms step_avg:39.26ms
step:761/1920 train_time:29902ms step_avg:39.29ms
step:762/1920 train_time:29963ms step_avg:39.32ms
step:763/1920 train_time:30025ms step_avg:39.35ms
step:764/1920 train_time:30086ms step_avg:39.38ms
step:765/1920 train_time:30148ms step_avg:39.41ms
step:766/1920 train_time:30212ms step_avg:39.44ms
step:767/1920 train_time:30276ms step_avg:39.47ms
step:768/1920 train_time:30339ms step_avg:39.50ms
step:769/1920 train_time:30403ms step_avg:39.54ms
step:770/1920 train_time:30466ms step_avg:39.57ms
step:771/1920 train_time:30529ms step_avg:39.60ms
step:772/1920 train_time:30592ms step_avg:39.63ms
step:773/1920 train_time:30655ms step_avg:39.66ms
step:774/1920 train_time:30716ms step_avg:39.68ms
step:775/1920 train_time:30778ms step_avg:39.71ms
step:776/1920 train_time:30840ms step_avg:39.74ms
step:777/1920 train_time:30901ms step_avg:39.77ms
step:778/1920 train_time:30963ms step_avg:39.80ms
step:779/1920 train_time:31025ms step_avg:39.83ms
step:780/1920 train_time:31086ms step_avg:39.85ms
step:781/1920 train_time:31148ms step_avg:39.88ms
step:782/1920 train_time:31211ms step_avg:39.91ms
step:783/1920 train_time:31273ms step_avg:39.94ms
step:784/1920 train_time:31335ms step_avg:39.97ms
step:785/1920 train_time:31399ms step_avg:40.00ms
step:786/1920 train_time:31461ms step_avg:40.03ms
step:787/1920 train_time:31524ms step_avg:40.06ms
step:788/1920 train_time:31587ms step_avg:40.08ms
step:789/1920 train_time:31650ms step_avg:40.11ms
step:790/1920 train_time:31712ms step_avg:40.14ms
step:791/1920 train_time:31774ms step_avg:40.17ms
step:792/1920 train_time:31836ms step_avg:40.20ms
step:793/1920 train_time:31898ms step_avg:40.22ms
step:794/1920 train_time:31960ms step_avg:40.25ms
step:795/1920 train_time:32022ms step_avg:40.28ms
step:796/1920 train_time:32083ms step_avg:40.31ms
step:797/1920 train_time:32146ms step_avg:40.33ms
step:798/1920 train_time:32207ms step_avg:40.36ms
step:799/1920 train_time:32270ms step_avg:40.39ms
step:800/1920 train_time:32332ms step_avg:40.42ms
step:801/1920 train_time:32395ms step_avg:40.44ms
step:802/1920 train_time:32457ms step_avg:40.47ms
step:803/1920 train_time:32520ms step_avg:40.50ms
step:804/1920 train_time:32581ms step_avg:40.52ms
step:805/1920 train_time:32644ms step_avg:40.55ms
step:806/1920 train_time:32707ms step_avg:40.58ms
step:807/1920 train_time:32770ms step_avg:40.61ms
step:808/1920 train_time:32831ms step_avg:40.63ms
step:809/1920 train_time:32894ms step_avg:40.66ms
step:810/1920 train_time:32955ms step_avg:40.69ms
step:811/1920 train_time:33017ms step_avg:40.71ms
step:812/1920 train_time:33079ms step_avg:40.74ms
step:813/1920 train_time:33142ms step_avg:40.76ms
step:814/1920 train_time:33203ms step_avg:40.79ms
step:815/1920 train_time:33267ms step_avg:40.82ms
step:816/1920 train_time:33329ms step_avg:40.84ms
step:817/1920 train_time:33391ms step_avg:40.87ms
step:818/1920 train_time:33454ms step_avg:40.90ms
step:819/1920 train_time:33516ms step_avg:40.92ms
step:820/1920 train_time:33578ms step_avg:40.95ms
step:821/1920 train_time:33641ms step_avg:40.98ms
step:822/1920 train_time:33703ms step_avg:41.00ms
step:823/1920 train_time:33766ms step_avg:41.03ms
step:824/1920 train_time:33828ms step_avg:41.05ms
step:825/1920 train_time:33891ms step_avg:41.08ms
step:826/1920 train_time:33953ms step_avg:41.11ms
step:827/1920 train_time:34016ms step_avg:41.13ms
step:828/1920 train_time:34077ms step_avg:41.16ms
step:829/1920 train_time:34140ms step_avg:41.18ms
step:830/1920 train_time:34201ms step_avg:41.21ms
step:831/1920 train_time:34264ms step_avg:41.23ms
step:832/1920 train_time:34326ms step_avg:41.26ms
step:833/1920 train_time:34389ms step_avg:41.28ms
step:834/1920 train_time:34451ms step_avg:41.31ms
step:835/1920 train_time:34514ms step_avg:41.33ms
step:836/1920 train_time:34576ms step_avg:41.36ms
step:837/1920 train_time:34639ms step_avg:41.38ms
step:838/1920 train_time:34701ms step_avg:41.41ms
step:839/1920 train_time:34763ms step_avg:41.43ms
step:840/1920 train_time:34826ms step_avg:41.46ms
step:841/1920 train_time:34889ms step_avg:41.48ms
step:842/1920 train_time:34952ms step_avg:41.51ms
step:843/1920 train_time:35014ms step_avg:41.53ms
step:844/1920 train_time:35075ms step_avg:41.56ms
step:845/1920 train_time:35138ms step_avg:41.58ms
step:846/1920 train_time:35200ms step_avg:41.61ms
step:847/1920 train_time:35262ms step_avg:41.63ms
step:848/1920 train_time:35324ms step_avg:41.66ms
step:849/1920 train_time:35387ms step_avg:41.68ms
step:850/1920 train_time:35449ms step_avg:41.71ms
step:851/1920 train_time:35512ms step_avg:41.73ms
step:852/1920 train_time:35573ms step_avg:41.75ms
step:853/1920 train_time:35636ms step_avg:41.78ms
step:854/1920 train_time:35698ms step_avg:41.80ms
step:855/1920 train_time:35760ms step_avg:41.82ms
step:856/1920 train_time:35822ms step_avg:41.85ms
step:857/1920 train_time:35886ms step_avg:41.87ms
step:858/1920 train_time:35948ms step_avg:41.90ms
step:859/1920 train_time:36011ms step_avg:41.92ms
step:860/1920 train_time:36073ms step_avg:41.95ms
step:861/1920 train_time:36135ms step_avg:41.97ms
step:862/1920 train_time:36197ms step_avg:41.99ms
step:863/1920 train_time:36259ms step_avg:42.02ms
step:864/1920 train_time:36321ms step_avg:42.04ms
step:865/1920 train_time:36384ms step_avg:42.06ms
step:866/1920 train_time:36446ms step_avg:42.09ms
step:867/1920 train_time:36510ms step_avg:42.11ms
step:868/1920 train_time:36572ms step_avg:42.13ms
step:869/1920 train_time:36634ms step_avg:42.16ms
step:870/1920 train_time:36696ms step_avg:42.18ms
step:871/1920 train_time:36759ms step_avg:42.20ms
step:872/1920 train_time:36821ms step_avg:42.23ms
step:873/1920 train_time:36883ms step_avg:42.25ms
step:874/1920 train_time:36946ms step_avg:42.27ms
step:875/1920 train_time:37009ms step_avg:42.30ms
step:876/1920 train_time:37071ms step_avg:42.32ms
step:877/1920 train_time:37133ms step_avg:42.34ms
step:878/1920 train_time:37195ms step_avg:42.36ms
step:879/1920 train_time:37257ms step_avg:42.39ms
step:880/1920 train_time:37319ms step_avg:42.41ms
step:881/1920 train_time:37381ms step_avg:42.43ms
step:882/1920 train_time:37443ms step_avg:42.45ms
step:883/1920 train_time:37505ms step_avg:42.47ms
step:884/1920 train_time:37568ms step_avg:42.50ms
step:885/1920 train_time:37631ms step_avg:42.52ms
step:886/1920 train_time:37693ms step_avg:42.54ms
step:887/1920 train_time:37755ms step_avg:42.56ms
step:888/1920 train_time:37817ms step_avg:42.59ms
step:889/1920 train_time:37879ms step_avg:42.61ms
step:890/1920 train_time:37941ms step_avg:42.63ms
step:891/1920 train_time:38004ms step_avg:42.65ms
step:892/1920 train_time:38066ms step_avg:42.68ms
step:893/1920 train_time:38129ms step_avg:42.70ms
step:894/1920 train_time:38190ms step_avg:42.72ms
step:895/1920 train_time:38253ms step_avg:42.74ms
step:896/1920 train_time:38315ms step_avg:42.76ms
step:897/1920 train_time:38377ms step_avg:42.78ms
step:898/1920 train_time:38439ms step_avg:42.81ms
step:899/1920 train_time:38502ms step_avg:42.83ms
step:900/1920 train_time:38564ms step_avg:42.85ms
step:901/1920 train_time:38627ms step_avg:42.87ms
step:902/1920 train_time:38689ms step_avg:42.89ms
step:903/1920 train_time:38752ms step_avg:42.91ms
step:904/1920 train_time:38814ms step_avg:42.94ms
step:905/1920 train_time:38876ms step_avg:42.96ms
step:906/1920 train_time:38938ms step_avg:42.98ms
step:907/1920 train_time:39001ms step_avg:43.00ms
step:908/1920 train_time:39063ms step_avg:43.02ms
step:909/1920 train_time:39125ms step_avg:43.04ms
step:910/1920 train_time:39188ms step_avg:43.06ms
step:911/1920 train_time:39251ms step_avg:43.09ms
step:912/1920 train_time:39313ms step_avg:43.11ms
step:913/1920 train_time:39375ms step_avg:43.13ms
step:914/1920 train_time:39438ms step_avg:43.15ms
step:915/1920 train_time:39500ms step_avg:43.17ms
step:916/1920 train_time:39562ms step_avg:43.19ms
step:917/1920 train_time:39625ms step_avg:43.21ms
step:918/1920 train_time:39687ms step_avg:43.23ms
step:919/1920 train_time:39750ms step_avg:43.25ms
step:920/1920 train_time:39811ms step_avg:43.27ms
step:921/1920 train_time:39874ms step_avg:43.29ms
step:922/1920 train_time:39935ms step_avg:43.31ms
step:923/1920 train_time:39998ms step_avg:43.33ms
step:924/1920 train_time:40059ms step_avg:43.35ms
step:925/1920 train_time:40122ms step_avg:43.38ms
step:926/1920 train_time:40184ms step_avg:43.40ms
step:927/1920 train_time:40248ms step_avg:43.42ms
step:928/1920 train_time:40310ms step_avg:43.44ms
step:929/1920 train_time:40373ms step_avg:43.46ms
step:930/1920 train_time:40434ms step_avg:43.48ms
step:931/1920 train_time:40497ms step_avg:43.50ms
step:932/1920 train_time:40559ms step_avg:43.52ms
step:933/1920 train_time:40621ms step_avg:43.54ms
step:934/1920 train_time:40683ms step_avg:43.56ms
step:935/1920 train_time:40746ms step_avg:43.58ms
step:936/1920 train_time:40808ms step_avg:43.60ms
step:937/1920 train_time:40871ms step_avg:43.62ms
step:938/1920 train_time:40933ms step_avg:43.64ms
step:939/1920 train_time:40995ms step_avg:43.66ms
step:940/1920 train_time:41057ms step_avg:43.68ms
step:941/1920 train_time:41119ms step_avg:43.70ms
step:942/1920 train_time:41181ms step_avg:43.72ms
step:943/1920 train_time:41245ms step_avg:43.74ms
step:944/1920 train_time:41307ms step_avg:43.76ms
step:945/1920 train_time:41370ms step_avg:43.78ms
step:946/1920 train_time:41432ms step_avg:43.80ms
step:947/1920 train_time:41495ms step_avg:43.82ms
step:948/1920 train_time:41556ms step_avg:43.84ms
step:949/1920 train_time:41619ms step_avg:43.86ms
step:950/1920 train_time:41680ms step_avg:43.87ms
step:951/1920 train_time:41743ms step_avg:43.89ms
step:952/1920 train_time:41806ms step_avg:43.91ms
step:953/1920 train_time:41870ms step_avg:43.93ms
step:954/1920 train_time:41932ms step_avg:43.95ms
step:955/1920 train_time:41994ms step_avg:43.97ms
step:956/1920 train_time:42055ms step_avg:43.99ms
step:957/1920 train_time:42118ms step_avg:44.01ms
step:958/1920 train_time:42180ms step_avg:44.03ms
step:959/1920 train_time:42243ms step_avg:44.05ms
step:960/1920 train_time:42305ms step_avg:44.07ms
step:961/1920 train_time:42369ms step_avg:44.09ms
step:962/1920 train_time:42431ms step_avg:44.11ms
step:963/1920 train_time:42494ms step_avg:44.13ms
step:964/1920 train_time:42555ms step_avg:44.14ms
step:965/1920 train_time:42617ms step_avg:44.16ms
step:966/1920 train_time:42678ms step_avg:44.18ms
step:967/1920 train_time:42741ms step_avg:44.20ms
step:968/1920 train_time:42803ms step_avg:44.22ms
step:969/1920 train_time:42866ms step_avg:44.24ms
step:970/1920 train_time:42928ms step_avg:44.26ms
step:971/1920 train_time:42991ms step_avg:44.28ms
step:972/1920 train_time:43053ms step_avg:44.29ms
step:973/1920 train_time:43115ms step_avg:44.31ms
step:974/1920 train_time:43177ms step_avg:44.33ms
step:975/1920 train_time:43240ms step_avg:44.35ms
step:976/1920 train_time:43302ms step_avg:44.37ms
step:977/1920 train_time:43366ms step_avg:44.39ms
step:978/1920 train_time:43429ms step_avg:44.41ms
step:979/1920 train_time:43492ms step_avg:44.43ms
step:980/1920 train_time:43554ms step_avg:44.44ms
step:981/1920 train_time:43616ms step_avg:44.46ms
step:982/1920 train_time:43678ms step_avg:44.48ms
step:983/1920 train_time:43740ms step_avg:44.50ms
step:984/1920 train_time:43802ms step_avg:44.51ms
step:985/1920 train_time:43866ms step_avg:44.53ms
step:986/1920 train_time:43929ms step_avg:44.55ms
step:987/1920 train_time:43992ms step_avg:44.57ms
step:988/1920 train_time:44053ms step_avg:44.59ms
step:989/1920 train_time:44116ms step_avg:44.61ms
step:990/1920 train_time:44177ms step_avg:44.62ms
step:991/1920 train_time:44240ms step_avg:44.64ms
step:992/1920 train_time:44302ms step_avg:44.66ms
step:993/1920 train_time:44365ms step_avg:44.68ms
step:994/1920 train_time:44427ms step_avg:44.70ms
step:995/1920 train_time:44490ms step_avg:44.71ms
step:996/1920 train_time:44552ms step_avg:44.73ms
step:997/1920 train_time:44615ms step_avg:44.75ms
step:998/1920 train_time:44676ms step_avg:44.77ms
step:999/1920 train_time:44739ms step_avg:44.78ms
step:1000/1920 train_time:44801ms step_avg:44.80ms
step:1000/1920 val_loss:3.7784 train_time:44866ms step_avg:44.87ms
step:1001/1920 train_time:44884ms step_avg:44.84ms
step:1002/1920 train_time:44928ms step_avg:44.84ms
step:1003/1920 train_time:44993ms step_avg:44.86ms
step:1004/1920 train_time:45056ms step_avg:44.88ms
step:1005/1920 train_time:45118ms step_avg:44.89ms
step:1006/1920 train_time:45180ms step_avg:44.91ms
step:1007/1920 train_time:45242ms step_avg:44.93ms
step:1008/1920 train_time:45303ms step_avg:44.94ms
step:1009/1920 train_time:45365ms step_avg:44.96ms
step:1010/1920 train_time:45426ms step_avg:44.98ms
step:1011/1920 train_time:45490ms step_avg:45.00ms
step:1012/1920 train_time:45551ms step_avg:45.01ms
step:1013/1920 train_time:45614ms step_avg:45.03ms
step:1014/1920 train_time:45675ms step_avg:45.04ms
step:1015/1920 train_time:45738ms step_avg:45.06ms
step:1016/1920 train_time:45800ms step_avg:45.08ms
step:1017/1920 train_time:45863ms step_avg:45.10ms
step:1018/1920 train_time:45926ms step_avg:45.11ms
step:1019/1920 train_time:45990ms step_avg:45.13ms
step:1020/1920 train_time:46053ms step_avg:45.15ms
step:1021/1920 train_time:46116ms step_avg:45.17ms
step:1022/1920 train_time:46178ms step_avg:45.18ms
step:1023/1920 train_time:46240ms step_avg:45.20ms
step:1024/1920 train_time:46302ms step_avg:45.22ms
step:1025/1920 train_time:46364ms step_avg:45.23ms
step:1026/1920 train_time:46425ms step_avg:45.25ms
step:1027/1920 train_time:46487ms step_avg:45.27ms
step:1028/1920 train_time:46550ms step_avg:45.28ms
step:1029/1920 train_time:46613ms step_avg:45.30ms
step:1030/1920 train_time:46674ms step_avg:45.31ms
step:1031/1920 train_time:46737ms step_avg:45.33ms
step:1032/1920 train_time:46799ms step_avg:45.35ms
step:1033/1920 train_time:46861ms step_avg:45.36ms
step:1034/1920 train_time:46924ms step_avg:45.38ms
step:1035/1920 train_time:46987ms step_avg:45.40ms
step:1036/1920 train_time:47050ms step_avg:45.42ms
step:1037/1920 train_time:47114ms step_avg:45.43ms
step:1038/1920 train_time:47176ms step_avg:45.45ms
step:1039/1920 train_time:47238ms step_avg:45.47ms
step:1040/1920 train_time:47300ms step_avg:45.48ms
step:1041/1920 train_time:47363ms step_avg:45.50ms
step:1042/1920 train_time:47424ms step_avg:45.51ms
step:1043/1920 train_time:47486ms step_avg:45.53ms
step:1044/1920 train_time:47548ms step_avg:45.54ms
step:1045/1920 train_time:47611ms step_avg:45.56ms
step:1046/1920 train_time:47673ms step_avg:45.58ms
step:1047/1920 train_time:47736ms step_avg:45.59ms
step:1048/1920 train_time:47798ms step_avg:45.61ms
step:1049/1920 train_time:47861ms step_avg:45.63ms
step:1050/1920 train_time:47923ms step_avg:45.64ms
step:1051/1920 train_time:47986ms step_avg:45.66ms
step:1052/1920 train_time:48049ms step_avg:45.67ms
step:1053/1920 train_time:48112ms step_avg:45.69ms
step:1054/1920 train_time:48174ms step_avg:45.71ms
step:1055/1920 train_time:48237ms step_avg:45.72ms
step:1056/1920 train_time:48299ms step_avg:45.74ms
step:1057/1920 train_time:48361ms step_avg:45.75ms
step:1058/1920 train_time:48423ms step_avg:45.77ms
step:1059/1920 train_time:48485ms step_avg:45.78ms
step:1060/1920 train_time:48547ms step_avg:45.80ms
step:1061/1920 train_time:48610ms step_avg:45.81ms
step:1062/1920 train_time:48672ms step_avg:45.83ms
step:1063/1920 train_time:48735ms step_avg:45.85ms
step:1064/1920 train_time:48798ms step_avg:45.86ms
step:1065/1920 train_time:48861ms step_avg:45.88ms
step:1066/1920 train_time:48923ms step_avg:45.89ms
step:1067/1920 train_time:48986ms step_avg:45.91ms
step:1068/1920 train_time:49048ms step_avg:45.92ms
step:1069/1920 train_time:49111ms step_avg:45.94ms
step:1070/1920 train_time:49174ms step_avg:45.96ms
step:1071/1920 train_time:49237ms step_avg:45.97ms
step:1072/1920 train_time:49298ms step_avg:45.99ms
step:1073/1920 train_time:49361ms step_avg:46.00ms
step:1074/1920 train_time:49422ms step_avg:46.02ms
step:1075/1920 train_time:49485ms step_avg:46.03ms
step:1076/1920 train_time:49546ms step_avg:46.05ms
step:1077/1920 train_time:49609ms step_avg:46.06ms
step:1078/1920 train_time:49671ms step_avg:46.08ms
step:1079/1920 train_time:49734ms step_avg:46.09ms
step:1080/1920 train_time:49797ms step_avg:46.11ms
step:1081/1920 train_time:49859ms step_avg:46.12ms
step:1082/1920 train_time:49921ms step_avg:46.14ms
step:1083/1920 train_time:49984ms step_avg:46.15ms
step:1084/1920 train_time:50046ms step_avg:46.17ms
step:1085/1920 train_time:50109ms step_avg:46.18ms
step:1086/1920 train_time:50171ms step_avg:46.20ms
step:1087/1920 train_time:50235ms step_avg:46.21ms
step:1088/1920 train_time:50297ms step_avg:46.23ms
step:1089/1920 train_time:50360ms step_avg:46.24ms
step:1090/1920 train_time:50422ms step_avg:46.26ms
step:1091/1920 train_time:50484ms step_avg:46.27ms
step:1092/1920 train_time:50545ms step_avg:46.29ms
step:1093/1920 train_time:50608ms step_avg:46.30ms
step:1094/1920 train_time:50671ms step_avg:46.32ms
step:1095/1920 train_time:50733ms step_avg:46.33ms
step:1096/1920 train_time:50796ms step_avg:46.35ms
step:1097/1920 train_time:50858ms step_avg:46.36ms
step:1098/1920 train_time:50920ms step_avg:46.38ms
step:1099/1920 train_time:50983ms step_avg:46.39ms
step:1100/1920 train_time:51045ms step_avg:46.40ms
step:1101/1920 train_time:51108ms step_avg:46.42ms
step:1102/1920 train_time:51170ms step_avg:46.43ms
step:1103/1920 train_time:51233ms step_avg:46.45ms
step:1104/1920 train_time:51295ms step_avg:46.46ms
step:1105/1920 train_time:51358ms step_avg:46.48ms
step:1106/1920 train_time:51420ms step_avg:46.49ms
step:1107/1920 train_time:51483ms step_avg:46.51ms
step:1108/1920 train_time:51545ms step_avg:46.52ms
step:1109/1920 train_time:51607ms step_avg:46.54ms
step:1110/1920 train_time:51670ms step_avg:46.55ms
step:1111/1920 train_time:51734ms step_avg:46.56ms
step:1112/1920 train_time:51796ms step_avg:46.58ms
step:1113/1920 train_time:51859ms step_avg:46.59ms
step:1114/1920 train_time:51920ms step_avg:46.61ms
step:1115/1920 train_time:51983ms step_avg:46.62ms
step:1116/1920 train_time:52045ms step_avg:46.63ms
step:1117/1920 train_time:52108ms step_avg:46.65ms
step:1118/1920 train_time:52170ms step_avg:46.66ms
step:1119/1920 train_time:52234ms step_avg:46.68ms
step:1120/1920 train_time:52296ms step_avg:46.69ms
step:1121/1920 train_time:52359ms step_avg:46.71ms
step:1122/1920 train_time:52421ms step_avg:46.72ms
step:1123/1920 train_time:52483ms step_avg:46.73ms
step:1124/1920 train_time:52545ms step_avg:46.75ms
step:1125/1920 train_time:52608ms step_avg:46.76ms
step:1126/1920 train_time:52669ms step_avg:46.78ms
step:1127/1920 train_time:52733ms step_avg:46.79ms
step:1128/1920 train_time:52795ms step_avg:46.80ms
step:1129/1920 train_time:52858ms step_avg:46.82ms
step:1130/1920 train_time:52919ms step_avg:46.83ms
step:1131/1920 train_time:52982ms step_avg:46.85ms
step:1132/1920 train_time:53044ms step_avg:46.86ms
step:1133/1920 train_time:53107ms step_avg:46.87ms
step:1134/1920 train_time:53169ms step_avg:46.89ms
step:1135/1920 train_time:53232ms step_avg:46.90ms
step:1136/1920 train_time:53295ms step_avg:46.91ms
step:1137/1920 train_time:53358ms step_avg:46.93ms
step:1138/1920 train_time:53420ms step_avg:46.94ms
step:1139/1920 train_time:53482ms step_avg:46.96ms
step:1140/1920 train_time:53544ms step_avg:46.97ms
step:1141/1920 train_time:53607ms step_avg:46.98ms
step:1142/1920 train_time:53669ms step_avg:47.00ms
step:1143/1920 train_time:53733ms step_avg:47.01ms
step:1144/1920 train_time:53795ms step_avg:47.02ms
step:1145/1920 train_time:53857ms step_avg:47.04ms
step:1146/1920 train_time:53919ms step_avg:47.05ms
step:1147/1920 train_time:53982ms step_avg:47.06ms
step:1148/1920 train_time:54043ms step_avg:47.08ms
step:1149/1920 train_time:54106ms step_avg:47.09ms
step:1150/1920 train_time:54168ms step_avg:47.10ms
step:1151/1920 train_time:54232ms step_avg:47.12ms
step:1152/1920 train_time:54294ms step_avg:47.13ms
step:1153/1920 train_time:54357ms step_avg:47.14ms
step:1154/1920 train_time:54419ms step_avg:47.16ms
step:1155/1920 train_time:54482ms step_avg:47.17ms
step:1156/1920 train_time:54544ms step_avg:47.18ms
step:1157/1920 train_time:54606ms step_avg:47.20ms
step:1158/1920 train_time:54668ms step_avg:47.21ms
step:1159/1920 train_time:54731ms step_avg:47.22ms
step:1160/1920 train_time:54793ms step_avg:47.24ms
step:1161/1920 train_time:54856ms step_avg:47.25ms
step:1162/1920 train_time:54918ms step_avg:47.26ms
step:1163/1920 train_time:54981ms step_avg:47.27ms
step:1164/1920 train_time:55042ms step_avg:47.29ms
step:1165/1920 train_time:55105ms step_avg:47.30ms
step:1166/1920 train_time:55166ms step_avg:47.31ms
step:1167/1920 train_time:55230ms step_avg:47.33ms
step:1168/1920 train_time:55293ms step_avg:47.34ms
step:1169/1920 train_time:55356ms step_avg:47.35ms
step:1170/1920 train_time:55418ms step_avg:47.37ms
step:1171/1920 train_time:55480ms step_avg:47.38ms
step:1172/1920 train_time:55542ms step_avg:47.39ms
step:1173/1920 train_time:55604ms step_avg:47.40ms
step:1174/1920 train_time:55666ms step_avg:47.42ms
step:1175/1920 train_time:55730ms step_avg:47.43ms
step:1176/1920 train_time:55792ms step_avg:47.44ms
step:1177/1920 train_time:55855ms step_avg:47.46ms
step:1178/1920 train_time:55917ms step_avg:47.47ms
step:1179/1920 train_time:55979ms step_avg:47.48ms
step:1180/1920 train_time:56041ms step_avg:47.49ms
step:1181/1920 train_time:56103ms step_avg:47.50ms
step:1182/1920 train_time:56165ms step_avg:47.52ms
step:1183/1920 train_time:56228ms step_avg:47.53ms
step:1184/1920 train_time:56291ms step_avg:47.54ms
step:1185/1920 train_time:56354ms step_avg:47.56ms
step:1186/1920 train_time:56416ms step_avg:47.57ms
step:1187/1920 train_time:56479ms step_avg:47.58ms
step:1188/1920 train_time:56540ms step_avg:47.59ms
step:1189/1920 train_time:56603ms step_avg:47.61ms
step:1190/1920 train_time:56665ms step_avg:47.62ms
step:1191/1920 train_time:56728ms step_avg:47.63ms
step:1192/1920 train_time:56791ms step_avg:47.64ms
step:1193/1920 train_time:56853ms step_avg:47.66ms
step:1194/1920 train_time:56916ms step_avg:47.67ms
step:1195/1920 train_time:56979ms step_avg:47.68ms
step:1196/1920 train_time:57040ms step_avg:47.69ms
step:1197/1920 train_time:57102ms step_avg:47.70ms
step:1198/1920 train_time:57164ms step_avg:47.72ms
step:1199/1920 train_time:57227ms step_avg:47.73ms
step:1200/1920 train_time:57289ms step_avg:47.74ms
step:1201/1920 train_time:57352ms step_avg:47.75ms
step:1202/1920 train_time:57414ms step_avg:47.77ms
step:1203/1920 train_time:57477ms step_avg:47.78ms
step:1204/1920 train_time:57538ms step_avg:47.79ms
step:1205/1920 train_time:57601ms step_avg:47.80ms
step:1206/1920 train_time:57663ms step_avg:47.81ms
step:1207/1920 train_time:57726ms step_avg:47.83ms
step:1208/1920 train_time:57787ms step_avg:47.84ms
step:1209/1920 train_time:57850ms step_avg:47.85ms
step:1210/1920 train_time:57913ms step_avg:47.86ms
step:1211/1920 train_time:57976ms step_avg:47.87ms
step:1212/1920 train_time:58038ms step_avg:47.89ms
step:1213/1920 train_time:58100ms step_avg:47.90ms
step:1214/1920 train_time:58162ms step_avg:47.91ms
step:1215/1920 train_time:58224ms step_avg:47.92ms
step:1216/1920 train_time:58286ms step_avg:47.93ms
step:1217/1920 train_time:58350ms step_avg:47.95ms
step:1218/1920 train_time:58412ms step_avg:47.96ms
step:1219/1920 train_time:58475ms step_avg:47.97ms
step:1220/1920 train_time:58537ms step_avg:47.98ms
step:1221/1920 train_time:58600ms step_avg:47.99ms
step:1222/1920 train_time:58662ms step_avg:48.00ms
step:1223/1920 train_time:58723ms step_avg:48.02ms
step:1224/1920 train_time:58785ms step_avg:48.03ms
step:1225/1920 train_time:58848ms step_avg:48.04ms
step:1226/1920 train_time:58911ms step_avg:48.05ms
step:1227/1920 train_time:58975ms step_avg:48.06ms
step:1228/1920 train_time:59036ms step_avg:48.08ms
step:1229/1920 train_time:59099ms step_avg:48.09ms
step:1230/1920 train_time:59161ms step_avg:48.10ms
step:1231/1920 train_time:59224ms step_avg:48.11ms
step:1232/1920 train_time:59286ms step_avg:48.12ms
step:1233/1920 train_time:59349ms step_avg:48.13ms
step:1234/1920 train_time:59410ms step_avg:48.14ms
step:1235/1920 train_time:59474ms step_avg:48.16ms
step:1236/1920 train_time:59535ms step_avg:48.17ms
step:1237/1920 train_time:59598ms step_avg:48.18ms
step:1238/1920 train_time:59660ms step_avg:48.19ms
step:1239/1920 train_time:59723ms step_avg:48.20ms
step:1240/1920 train_time:59784ms step_avg:48.21ms
step:1241/1920 train_time:59847ms step_avg:48.22ms
step:1242/1920 train_time:59909ms step_avg:48.24ms
step:1243/1920 train_time:59973ms step_avg:48.25ms
step:1244/1920 train_time:60035ms step_avg:48.26ms
step:1245/1920 train_time:60098ms step_avg:48.27ms
step:1246/1920 train_time:60160ms step_avg:48.28ms
step:1247/1920 train_time:60222ms step_avg:48.29ms
step:1248/1920 train_time:60284ms step_avg:48.30ms
step:1249/1920 train_time:60347ms step_avg:48.32ms
step:1250/1920 train_time:60410ms step_avg:48.33ms
step:1250/1920 val_loss:3.5528 train_time:60475ms step_avg:48.38ms
step:1251/1920 train_time:60493ms step_avg:48.36ms
step:1252/1920 train_time:60536ms step_avg:48.35ms
step:1253/1920 train_time:60602ms step_avg:48.37ms
step:1254/1920 train_time:60665ms step_avg:48.38ms
step:1255/1920 train_time:60728ms step_avg:48.39ms
step:1256/1920 train_time:60815ms step_avg:48.42ms
step:1257/1920 train_time:60903ms step_avg:48.45ms
step:1258/1920 train_time:60991ms step_avg:48.48ms
step:1259/1920 train_time:61079ms step_avg:48.51ms
step:1260/1920 train_time:61166ms step_avg:48.54ms
step:1261/1920 train_time:61256ms step_avg:48.58ms
step:1262/1920 train_time:61344ms step_avg:48.61ms
step:1263/1920 train_time:61434ms step_avg:48.64ms
step:1264/1920 train_time:61525ms step_avg:48.67ms
step:1265/1920 train_time:61617ms step_avg:48.71ms
step:1266/1920 train_time:61705ms step_avg:48.74ms
step:1267/1920 train_time:61795ms step_avg:48.77ms
step:1268/1920 train_time:61882ms step_avg:48.80ms
step:1269/1920 train_time:61971ms step_avg:48.83ms
step:1270/1920 train_time:62057ms step_avg:48.86ms
step:1271/1920 train_time:62146ms step_avg:48.90ms
step:1272/1920 train_time:62233ms step_avg:48.93ms
step:1273/1920 train_time:62322ms step_avg:48.96ms
step:1274/1920 train_time:62412ms step_avg:48.99ms
step:1275/1920 train_time:62502ms step_avg:49.02ms
step:1276/1920 train_time:62591ms step_avg:49.05ms
step:1277/1920 train_time:62680ms step_avg:49.08ms
step:1278/1920 train_time:62769ms step_avg:49.12ms
step:1279/1920 train_time:62857ms step_avg:49.15ms
step:1280/1920 train_time:62946ms step_avg:49.18ms
step:1281/1920 train_time:63035ms step_avg:49.21ms
step:1282/1920 train_time:63123ms step_avg:49.24ms
step:1283/1920 train_time:63211ms step_avg:49.27ms
step:1284/1920 train_time:63298ms step_avg:49.30ms
step:1285/1920 train_time:63388ms step_avg:49.33ms
step:1286/1920 train_time:63476ms step_avg:49.36ms
step:1287/1920 train_time:63566ms step_avg:49.39ms
step:1288/1920 train_time:63654ms step_avg:49.42ms
step:1289/1920 train_time:63743ms step_avg:49.45ms
step:1290/1920 train_time:63830ms step_avg:49.48ms
step:1291/1920 train_time:63918ms step_avg:49.51ms
step:1292/1920 train_time:64006ms step_avg:49.54ms
step:1293/1920 train_time:64095ms step_avg:49.57ms
step:1294/1920 train_time:64183ms step_avg:49.60ms
step:1295/1920 train_time:64272ms step_avg:49.63ms
step:1296/1920 train_time:64360ms step_avg:49.66ms
step:1297/1920 train_time:64448ms step_avg:49.69ms
step:1298/1920 train_time:64536ms step_avg:49.72ms
step:1299/1920 train_time:64626ms step_avg:49.75ms
step:1300/1920 train_time:64715ms step_avg:49.78ms
step:1301/1920 train_time:64803ms step_avg:49.81ms
step:1302/1920 train_time:64891ms step_avg:49.84ms
step:1303/1920 train_time:64980ms step_avg:49.87ms
step:1304/1920 train_time:65068ms step_avg:49.90ms
step:1305/1920 train_time:65156ms step_avg:49.93ms
step:1306/1920 train_time:65244ms step_avg:49.96ms
step:1307/1920 train_time:65332ms step_avg:49.99ms
step:1308/1920 train_time:65420ms step_avg:50.02ms
step:1309/1920 train_time:65509ms step_avg:50.05ms
step:1310/1920 train_time:65596ms step_avg:50.07ms
step:1311/1920 train_time:65685ms step_avg:50.10ms
step:1312/1920 train_time:65773ms step_avg:50.13ms
step:1313/1920 train_time:65862ms step_avg:50.16ms
step:1314/1920 train_time:65951ms step_avg:50.19ms
step:1315/1920 train_time:66039ms step_avg:50.22ms
step:1316/1920 train_time:66127ms step_avg:50.25ms
step:1317/1920 train_time:66215ms step_avg:50.28ms
step:1318/1920 train_time:66304ms step_avg:50.31ms
step:1319/1920 train_time:66393ms step_avg:50.34ms
step:1320/1920 train_time:66483ms step_avg:50.37ms
step:1321/1920 train_time:66571ms step_avg:50.39ms
step:1322/1920 train_time:66659ms step_avg:50.42ms
step:1323/1920 train_time:66748ms step_avg:50.45ms
step:1324/1920 train_time:66836ms step_avg:50.48ms
step:1325/1920 train_time:66924ms step_avg:50.51ms
step:1326/1920 train_time:67013ms step_avg:50.54ms
step:1327/1920 train_time:67102ms step_avg:50.57ms
step:1328/1920 train_time:67190ms step_avg:50.60ms
step:1329/1920 train_time:67279ms step_avg:50.62ms
step:1330/1920 train_time:67367ms step_avg:50.65ms
step:1331/1920 train_time:67456ms step_avg:50.68ms
step:1332/1920 train_time:67545ms step_avg:50.71ms
step:1333/1920 train_time:67634ms step_avg:50.74ms
step:1334/1920 train_time:67723ms step_avg:50.77ms
step:1335/1920 train_time:67813ms step_avg:50.80ms
step:1336/1920 train_time:67900ms step_avg:50.82ms
step:1337/1920 train_time:67990ms step_avg:50.85ms
step:1338/1920 train_time:68078ms step_avg:50.88ms
step:1339/1920 train_time:68166ms step_avg:50.91ms
step:1340/1920 train_time:68254ms step_avg:50.94ms
step:1341/1920 train_time:68343ms step_avg:50.96ms
step:1342/1920 train_time:68431ms step_avg:50.99ms
step:1343/1920 train_time:68520ms step_avg:51.02ms
step:1344/1920 train_time:68609ms step_avg:51.05ms
step:1345/1920 train_time:68697ms step_avg:51.08ms
step:1346/1920 train_time:68786ms step_avg:51.10ms
step:1347/1920 train_time:68876ms step_avg:51.13ms
step:1348/1920 train_time:68964ms step_avg:51.16ms
step:1349/1920 train_time:69054ms step_avg:51.19ms
step:1350/1920 train_time:69142ms step_avg:51.22ms
step:1351/1920 train_time:69231ms step_avg:51.24ms
step:1352/1920 train_time:69319ms step_avg:51.27ms
step:1353/1920 train_time:69408ms step_avg:51.30ms
step:1354/1920 train_time:69495ms step_avg:51.33ms
step:1355/1920 train_time:69585ms step_avg:51.35ms
step:1356/1920 train_time:69673ms step_avg:51.38ms
step:1357/1920 train_time:69762ms step_avg:51.41ms
step:1358/1920 train_time:69851ms step_avg:51.44ms
step:1359/1920 train_time:69940ms step_avg:51.46ms
step:1360/1920 train_time:70030ms step_avg:51.49ms
step:1361/1920 train_time:70119ms step_avg:51.52ms
step:1362/1920 train_time:70207ms step_avg:51.55ms
step:1363/1920 train_time:70296ms step_avg:51.57ms
step:1364/1920 train_time:70384ms step_avg:51.60ms
step:1365/1920 train_time:70473ms step_avg:51.63ms
step:1366/1920 train_time:70562ms step_avg:51.66ms
step:1367/1920 train_time:70650ms step_avg:51.68ms
step:1368/1920 train_time:70738ms step_avg:51.71ms
step:1369/1920 train_time:70827ms step_avg:51.74ms
step:1370/1920 train_time:70915ms step_avg:51.76ms
step:1371/1920 train_time:71004ms step_avg:51.79ms
step:1372/1920 train_time:71093ms step_avg:51.82ms
step:1373/1920 train_time:71181ms step_avg:51.84ms
step:1374/1920 train_time:71270ms step_avg:51.87ms
step:1375/1920 train_time:71358ms step_avg:51.90ms
step:1376/1920 train_time:71446ms step_avg:51.92ms
step:1377/1920 train_time:71536ms step_avg:51.95ms
step:1378/1920 train_time:71623ms step_avg:51.98ms
step:1379/1920 train_time:71712ms step_avg:52.00ms
step:1380/1920 train_time:71800ms step_avg:52.03ms
step:1381/1920 train_time:71890ms step_avg:52.06ms
step:1382/1920 train_time:71978ms step_avg:52.08ms
step:1383/1920 train_time:72066ms step_avg:52.11ms
step:1384/1920 train_time:72154ms step_avg:52.13ms
step:1385/1920 train_time:72243ms step_avg:52.16ms
step:1386/1920 train_time:72331ms step_avg:52.19ms
step:1387/1920 train_time:72419ms step_avg:52.21ms
step:1388/1920 train_time:72508ms step_avg:52.24ms
step:1389/1920 train_time:72598ms step_avg:52.27ms
step:1390/1920 train_time:72685ms step_avg:52.29ms
step:1391/1920 train_time:72775ms step_avg:52.32ms
step:1392/1920 train_time:72864ms step_avg:52.34ms
step:1393/1920 train_time:72953ms step_avg:52.37ms
step:1394/1920 train_time:73041ms step_avg:52.40ms
step:1395/1920 train_time:73130ms step_avg:52.42ms
step:1396/1920 train_time:73218ms step_avg:52.45ms
step:1397/1920 train_time:73306ms step_avg:52.47ms
step:1398/1920 train_time:73394ms step_avg:52.50ms
step:1399/1920 train_time:73483ms step_avg:52.53ms
step:1400/1920 train_time:73571ms step_avg:52.55ms
step:1401/1920 train_time:73660ms step_avg:52.58ms
step:1402/1920 train_time:73748ms step_avg:52.60ms
step:1403/1920 train_time:73837ms step_avg:52.63ms
step:1404/1920 train_time:73925ms step_avg:52.65ms
step:1405/1920 train_time:74014ms step_avg:52.68ms
step:1406/1920 train_time:74102ms step_avg:52.70ms
step:1407/1920 train_time:74193ms step_avg:52.73ms
step:1408/1920 train_time:74281ms step_avg:52.76ms
step:1409/1920 train_time:74371ms step_avg:52.78ms
step:1410/1920 train_time:74458ms step_avg:52.81ms
step:1411/1920 train_time:74547ms step_avg:52.83ms
step:1412/1920 train_time:74635ms step_avg:52.86ms
step:1413/1920 train_time:74724ms step_avg:52.88ms
step:1414/1920 train_time:74812ms step_avg:52.91ms
step:1415/1920 train_time:74901ms step_avg:52.93ms
step:1416/1920 train_time:74989ms step_avg:52.96ms
step:1417/1920 train_time:75078ms step_avg:52.98ms
step:1418/1920 train_time:75167ms step_avg:53.01ms
step:1419/1920 train_time:75255ms step_avg:53.03ms
step:1420/1920 train_time:75344ms step_avg:53.06ms
step:1421/1920 train_time:75434ms step_avg:53.08ms
step:1422/1920 train_time:75522ms step_avg:53.11ms
step:1423/1920 train_time:75610ms step_avg:53.13ms
step:1424/1920 train_time:75698ms step_avg:53.16ms
step:1425/1920 train_time:75787ms step_avg:53.18ms
step:1426/1920 train_time:75874ms step_avg:53.21ms
step:1427/1920 train_time:75963ms step_avg:53.23ms
step:1428/1920 train_time:76051ms step_avg:53.26ms
step:1429/1920 train_time:76139ms step_avg:53.28ms
step:1430/1920 train_time:76227ms step_avg:53.31ms
step:1431/1920 train_time:76316ms step_avg:53.33ms
step:1432/1920 train_time:76405ms step_avg:53.36ms
step:1433/1920 train_time:76494ms step_avg:53.38ms
step:1434/1920 train_time:76582ms step_avg:53.40ms
step:1435/1920 train_time:76672ms step_avg:53.43ms
step:1436/1920 train_time:76760ms step_avg:53.45ms
step:1437/1920 train_time:76848ms step_avg:53.48ms
step:1438/1920 train_time:76935ms step_avg:53.50ms
step:1439/1920 train_time:77024ms step_avg:53.53ms
step:1440/1920 train_time:77112ms step_avg:53.55ms
step:1441/1920 train_time:77201ms step_avg:53.57ms
step:1442/1920 train_time:77290ms step_avg:53.60ms
step:1443/1920 train_time:77378ms step_avg:53.62ms
step:1444/1920 train_time:77466ms step_avg:53.65ms
step:1445/1920 train_time:77555ms step_avg:53.67ms
step:1446/1920 train_time:77643ms step_avg:53.70ms
step:1447/1920 train_time:77732ms step_avg:53.72ms
step:1448/1920 train_time:77820ms step_avg:53.74ms
step:1449/1920 train_time:77909ms step_avg:53.77ms
step:1450/1920 train_time:77996ms step_avg:53.79ms
step:1451/1920 train_time:78085ms step_avg:53.81ms
step:1452/1920 train_time:78173ms step_avg:53.84ms
step:1453/1920 train_time:78261ms step_avg:53.86ms
step:1454/1920 train_time:78350ms step_avg:53.89ms
step:1455/1920 train_time:78439ms step_avg:53.91ms
step:1456/1920 train_time:78527ms step_avg:53.93ms
step:1457/1920 train_time:78616ms step_avg:53.96ms
step:1458/1920 train_time:78705ms step_avg:53.98ms
step:1459/1920 train_time:78794ms step_avg:54.01ms
step:1460/1920 train_time:78882ms step_avg:54.03ms
step:1461/1920 train_time:78971ms step_avg:54.05ms
step:1462/1920 train_time:79059ms step_avg:54.08ms
step:1463/1920 train_time:79148ms step_avg:54.10ms
step:1464/1920 train_time:79236ms step_avg:54.12ms
step:1465/1920 train_time:79324ms step_avg:54.15ms
step:1466/1920 train_time:79413ms step_avg:54.17ms
step:1467/1920 train_time:79502ms step_avg:54.19ms
step:1468/1920 train_time:79590ms step_avg:54.22ms
step:1469/1920 train_time:79679ms step_avg:54.24ms
step:1470/1920 train_time:79768ms step_avg:54.26ms
step:1471/1920 train_time:79856ms step_avg:54.29ms
step:1472/1920 train_time:79944ms step_avg:54.31ms
step:1473/1920 train_time:80034ms step_avg:54.33ms
step:1474/1920 train_time:80121ms step_avg:54.36ms
step:1475/1920 train_time:80210ms step_avg:54.38ms
step:1476/1920 train_time:80298ms step_avg:54.40ms
step:1477/1920 train_time:80387ms step_avg:54.43ms
step:1478/1920 train_time:80475ms step_avg:54.45ms
step:1479/1920 train_time:80565ms step_avg:54.47ms
step:1480/1920 train_time:80653ms step_avg:54.49ms
step:1481/1920 train_time:80741ms step_avg:54.52ms
step:1482/1920 train_time:80830ms step_avg:54.54ms
step:1483/1920 train_time:80919ms step_avg:54.56ms
step:1484/1920 train_time:81007ms step_avg:54.59ms
step:1485/1920 train_time:81096ms step_avg:54.61ms
step:1486/1920 train_time:81184ms step_avg:54.63ms
step:1487/1920 train_time:81274ms step_avg:54.66ms
step:1488/1920 train_time:81362ms step_avg:54.68ms
step:1489/1920 train_time:81452ms step_avg:54.70ms
step:1490/1920 train_time:81540ms step_avg:54.72ms
step:1491/1920 train_time:81629ms step_avg:54.75ms
step:1492/1920 train_time:81716ms step_avg:54.77ms
step:1493/1920 train_time:81805ms step_avg:54.79ms
step:1494/1920 train_time:81893ms step_avg:54.81ms
step:1495/1920 train_time:81982ms step_avg:54.84ms
step:1496/1920 train_time:82070ms step_avg:54.86ms
step:1497/1920 train_time:82159ms step_avg:54.88ms
step:1498/1920 train_time:82248ms step_avg:54.91ms
step:1499/1920 train_time:82336ms step_avg:54.93ms
step:1500/1920 train_time:82424ms step_avg:54.95ms
step:1500/1920 val_loss:3.4139 train_time:82517ms step_avg:55.01ms
step:1501/1920 train_time:82535ms step_avg:54.99ms
step:1502/1920 train_time:82606ms step_avg:55.00ms
step:1503/1920 train_time:82698ms step_avg:55.02ms
step:1504/1920 train_time:82787ms step_avg:55.04ms
step:1505/1920 train_time:82875ms step_avg:55.07ms
step:1506/1920 train_time:82963ms step_avg:55.09ms
step:1507/1920 train_time:83050ms step_avg:55.11ms
step:1508/1920 train_time:83138ms step_avg:55.13ms
step:1509/1920 train_time:83226ms step_avg:55.15ms
step:1510/1920 train_time:83313ms step_avg:55.17ms
step:1511/1920 train_time:83402ms step_avg:55.20ms
step:1512/1920 train_time:83491ms step_avg:55.22ms
step:1513/1920 train_time:83582ms step_avg:55.24ms
step:1514/1920 train_time:83672ms step_avg:55.27ms
step:1515/1920 train_time:83762ms step_avg:55.29ms
step:1516/1920 train_time:83849ms step_avg:55.31ms
step:1517/1920 train_time:83938ms step_avg:55.33ms
step:1518/1920 train_time:84026ms step_avg:55.35ms
step:1519/1920 train_time:84114ms step_avg:55.37ms
step:1520/1920 train_time:84202ms step_avg:55.40ms
step:1521/1920 train_time:84290ms step_avg:55.42ms
step:1522/1920 train_time:84378ms step_avg:55.44ms
step:1523/1920 train_time:84467ms step_avg:55.46ms
step:1524/1920 train_time:84556ms step_avg:55.48ms
step:1525/1920 train_time:84646ms step_avg:55.51ms
step:1526/1920 train_time:84734ms step_avg:55.53ms
step:1527/1920 train_time:84823ms step_avg:55.55ms
step:1528/1920 train_time:84910ms step_avg:55.57ms
step:1529/1920 train_time:84998ms step_avg:55.59ms
step:1530/1920 train_time:85085ms step_avg:55.61ms
step:1531/1920 train_time:85173ms step_avg:55.63ms
step:1532/1920 train_time:85261ms step_avg:55.65ms
step:1533/1920 train_time:85350ms step_avg:55.68ms
step:1534/1920 train_time:85439ms step_avg:55.70ms
step:1535/1920 train_time:85528ms step_avg:55.72ms
step:1536/1920 train_time:85617ms step_avg:55.74ms
step:1537/1920 train_time:85707ms step_avg:55.76ms
step:1538/1920 train_time:85796ms step_avg:55.78ms
step:1539/1920 train_time:85884ms step_avg:55.81ms
step:1540/1920 train_time:85972ms step_avg:55.83ms
step:1541/1920 train_time:86060ms step_avg:55.85ms
step:1542/1920 train_time:86147ms step_avg:55.87ms
step:1543/1920 train_time:86236ms step_avg:55.89ms
step:1544/1920 train_time:86323ms step_avg:55.91ms
step:1545/1920 train_time:86412ms step_avg:55.93ms
step:1546/1920 train_time:86500ms step_avg:55.95ms
step:1547/1920 train_time:86590ms step_avg:55.97ms
step:1548/1920 train_time:86679ms step_avg:55.99ms
step:1549/1920 train_time:86769ms step_avg:56.02ms
step:1550/1920 train_time:86857ms step_avg:56.04ms
step:1551/1920 train_time:86946ms step_avg:56.06ms
step:1552/1920 train_time:87033ms step_avg:56.08ms
step:1553/1920 train_time:87122ms step_avg:56.10ms
step:1554/1920 train_time:87210ms step_avg:56.12ms
step:1555/1920 train_time:87298ms step_avg:56.14ms
step:1556/1920 train_time:87386ms step_avg:56.16ms
step:1557/1920 train_time:87474ms step_avg:56.18ms
step:1558/1920 train_time:87565ms step_avg:56.20ms
step:1559/1920 train_time:87653ms step_avg:56.22ms
step:1560/1920 train_time:87741ms step_avg:56.24ms
step:1561/1920 train_time:87831ms step_avg:56.27ms
step:1562/1920 train_time:87920ms step_avg:56.29ms
step:1563/1920 train_time:88009ms step_avg:56.31ms
step:1564/1920 train_time:88097ms step_avg:56.33ms
step:1565/1920 train_time:88186ms step_avg:56.35ms
step:1566/1920 train_time:88274ms step_avg:56.37ms
step:1567/1920 train_time:88363ms step_avg:56.39ms
step:1568/1920 train_time:88450ms step_avg:56.41ms
step:1569/1920 train_time:88539ms step_avg:56.43ms
step:1570/1920 train_time:88627ms step_avg:56.45ms
step:1571/1920 train_time:88717ms step_avg:56.47ms
step:1572/1920 train_time:88805ms step_avg:56.49ms
step:1573/1920 train_time:88894ms step_avg:56.51ms
step:1574/1920 train_time:88983ms step_avg:56.53ms
step:1575/1920 train_time:89070ms step_avg:56.55ms
step:1576/1920 train_time:89158ms step_avg:56.57ms
step:1577/1920 train_time:89247ms step_avg:56.59ms
step:1578/1920 train_time:89335ms step_avg:56.61ms
step:1579/1920 train_time:89424ms step_avg:56.63ms
step:1580/1920 train_time:89511ms step_avg:56.65ms
step:1581/1920 train_time:89601ms step_avg:56.67ms
step:1582/1920 train_time:89689ms step_avg:56.69ms
step:1583/1920 train_time:89779ms step_avg:56.71ms
step:1584/1920 train_time:89867ms step_avg:56.73ms
step:1585/1920 train_time:89956ms step_avg:56.75ms
step:1586/1920 train_time:90045ms step_avg:56.78ms
step:1587/1920 train_time:90133ms step_avg:56.79ms
step:1588/1920 train_time:90222ms step_avg:56.81ms
step:1589/1920 train_time:90311ms step_avg:56.83ms
step:1590/1920 train_time:90399ms step_avg:56.85ms
step:1591/1920 train_time:90488ms step_avg:56.87ms
step:1592/1920 train_time:90576ms step_avg:56.89ms
step:1593/1920 train_time:90666ms step_avg:56.92ms
step:1594/1920 train_time:90753ms step_avg:56.93ms
step:1595/1920 train_time:90843ms step_avg:56.96ms
step:1596/1920 train_time:90931ms step_avg:56.97ms
step:1597/1920 train_time:91020ms step_avg:56.99ms
step:1598/1920 train_time:91108ms step_avg:57.01ms
step:1599/1920 train_time:91197ms step_avg:57.03ms
step:1600/1920 train_time:91286ms step_avg:57.05ms
step:1601/1920 train_time:91374ms step_avg:57.07ms
step:1602/1920 train_time:91463ms step_avg:57.09ms
step:1603/1920 train_time:91551ms step_avg:57.11ms
step:1604/1920 train_time:91639ms step_avg:57.13ms
step:1605/1920 train_time:91728ms step_avg:57.15ms
step:1606/1920 train_time:91817ms step_avg:57.17ms
step:1607/1920 train_time:91907ms step_avg:57.19ms
step:1608/1920 train_time:91995ms step_avg:57.21ms
step:1609/1920 train_time:92086ms step_avg:57.23ms
step:1610/1920 train_time:92173ms step_avg:57.25ms
step:1611/1920 train_time:92262ms step_avg:57.27ms
step:1612/1920 train_time:92350ms step_avg:57.29ms
step:1613/1920 train_time:92440ms step_avg:57.31ms
step:1614/1920 train_time:92528ms step_avg:57.33ms
step:1615/1920 train_time:92616ms step_avg:57.35ms
step:1616/1920 train_time:92705ms step_avg:57.37ms
step:1617/1920 train_time:92794ms step_avg:57.39ms
step:1618/1920 train_time:92884ms step_avg:57.41ms
step:1619/1920 train_time:92973ms step_avg:57.43ms
step:1620/1920 train_time:93061ms step_avg:57.45ms
step:1621/1920 train_time:93150ms step_avg:57.46ms
step:1622/1920 train_time:93239ms step_avg:57.48ms
step:1623/1920 train_time:93330ms step_avg:57.50ms
step:1624/1920 train_time:93418ms step_avg:57.52ms
step:1625/1920 train_time:93509ms step_avg:57.54ms
step:1626/1920 train_time:93597ms step_avg:57.56ms
step:1627/1920 train_time:93686ms step_avg:57.58ms
step:1628/1920 train_time:93773ms step_avg:57.60ms
step:1629/1920 train_time:93862ms step_avg:57.62ms
step:1630/1920 train_time:93950ms step_avg:57.64ms
step:1631/1920 train_time:94040ms step_avg:57.66ms
step:1632/1920 train_time:94127ms step_avg:57.68ms
step:1633/1920 train_time:94216ms step_avg:57.70ms
step:1634/1920 train_time:94305ms step_avg:57.71ms
step:1635/1920 train_time:94393ms step_avg:57.73ms
step:1636/1920 train_time:94482ms step_avg:57.75ms
step:1637/1920 train_time:94571ms step_avg:57.77ms
step:1638/1920 train_time:94660ms step_avg:57.79ms
step:1639/1920 train_time:94749ms step_avg:57.81ms
step:1640/1920 train_time:94836ms step_avg:57.83ms
step:1641/1920 train_time:94925ms step_avg:57.85ms
step:1642/1920 train_time:95013ms step_avg:57.86ms
step:1643/1920 train_time:95101ms step_avg:57.88ms
step:1644/1920 train_time:95190ms step_avg:57.90ms
step:1645/1920 train_time:95279ms step_avg:57.92ms
step:1646/1920 train_time:95368ms step_avg:57.94ms
step:1647/1920 train_time:95456ms step_avg:57.96ms
step:1648/1920 train_time:95544ms step_avg:57.98ms
step:1649/1920 train_time:95632ms step_avg:57.99ms
step:1650/1920 train_time:95721ms step_avg:58.01ms
step:1651/1920 train_time:95811ms step_avg:58.03ms
step:1652/1920 train_time:95899ms step_avg:58.05ms
step:1653/1920 train_time:95989ms step_avg:58.07ms
step:1654/1920 train_time:96077ms step_avg:58.09ms
step:1655/1920 train_time:96166ms step_avg:58.11ms
step:1656/1920 train_time:96254ms step_avg:58.12ms
step:1657/1920 train_time:96344ms step_avg:58.14ms
step:1658/1920 train_time:96432ms step_avg:58.16ms
step:1659/1920 train_time:96521ms step_avg:58.18ms
step:1660/1920 train_time:96609ms step_avg:58.20ms
step:1661/1920 train_time:96697ms step_avg:58.22ms
step:1662/1920 train_time:96785ms step_avg:58.23ms
step:1663/1920 train_time:96874ms step_avg:58.25ms
step:1664/1920 train_time:96962ms step_avg:58.27ms
step:1665/1920 train_time:97051ms step_avg:58.29ms
step:1666/1920 train_time:97140ms step_avg:58.31ms
step:1667/1920 train_time:97228ms step_avg:58.33ms
step:1668/1920 train_time:97317ms step_avg:58.34ms
step:1669/1920 train_time:97407ms step_avg:58.36ms
step:1670/1920 train_time:97494ms step_avg:58.38ms
step:1671/1920 train_time:97584ms step_avg:58.40ms
step:1672/1920 train_time:97672ms step_avg:58.42ms
step:1673/1920 train_time:97761ms step_avg:58.43ms
step:1674/1920 train_time:97849ms step_avg:58.45ms
step:1675/1920 train_time:97938ms step_avg:58.47ms
step:1676/1920 train_time:98028ms step_avg:58.49ms
step:1677/1920 train_time:98116ms step_avg:58.51ms
step:1678/1920 train_time:98206ms step_avg:58.53ms
step:1679/1920 train_time:98294ms step_avg:58.54ms
step:1680/1920 train_time:98382ms step_avg:58.56ms
step:1681/1920 train_time:98471ms step_avg:58.58ms
step:1682/1920 train_time:98560ms step_avg:58.60ms
step:1683/1920 train_time:98650ms step_avg:58.62ms
step:1684/1920 train_time:98739ms step_avg:58.63ms
step:1685/1920 train_time:98828ms step_avg:58.65ms
step:1686/1920 train_time:98916ms step_avg:58.67ms
step:1687/1920 train_time:99005ms step_avg:58.69ms
step:1688/1920 train_time:99093ms step_avg:58.70ms
step:1689/1920 train_time:99182ms step_avg:58.72ms
step:1690/1920 train_time:99271ms step_avg:58.74ms
step:1691/1920 train_time:99359ms step_avg:58.76ms
step:1692/1920 train_time:99448ms step_avg:58.78ms
step:1693/1920 train_time:99536ms step_avg:58.79ms
step:1694/1920 train_time:99625ms step_avg:58.81ms
step:1695/1920 train_time:99713ms step_avg:58.83ms
step:1696/1920 train_time:99801ms step_avg:58.85ms
step:1697/1920 train_time:99891ms step_avg:58.86ms
step:1698/1920 train_time:99979ms step_avg:58.88ms
step:1699/1920 train_time:100070ms step_avg:58.90ms
step:1700/1920 train_time:100157ms step_avg:58.92ms
step:1701/1920 train_time:100247ms step_avg:58.93ms
step:1702/1920 train_time:100335ms step_avg:58.95ms
step:1703/1920 train_time:100425ms step_avg:58.97ms
step:1704/1920 train_time:100513ms step_avg:58.99ms
step:1705/1920 train_time:100602ms step_avg:59.00ms
step:1706/1920 train_time:100690ms step_avg:59.02ms
step:1707/1920 train_time:100778ms step_avg:59.04ms
step:1708/1920 train_time:100866ms step_avg:59.06ms
step:1709/1920 train_time:100955ms step_avg:59.07ms
step:1710/1920 train_time:101044ms step_avg:59.09ms
step:1711/1920 train_time:101133ms step_avg:59.11ms
step:1712/1920 train_time:101223ms step_avg:59.13ms
step:1713/1920 train_time:101311ms step_avg:59.14ms
step:1714/1920 train_time:101400ms step_avg:59.16ms
step:1715/1920 train_time:101489ms step_avg:59.18ms
step:1716/1920 train_time:101577ms step_avg:59.19ms
step:1717/1920 train_time:101666ms step_avg:59.21ms
step:1718/1920 train_time:101754ms step_avg:59.23ms
step:1719/1920 train_time:101843ms step_avg:59.25ms
step:1720/1920 train_time:101931ms step_avg:59.26ms
step:1721/1920 train_time:102020ms step_avg:59.28ms
step:1722/1920 train_time:102108ms step_avg:59.30ms
step:1723/1920 train_time:102197ms step_avg:59.31ms
step:1724/1920 train_time:102285ms step_avg:59.33ms
step:1725/1920 train_time:102373ms step_avg:59.35ms
step:1726/1920 train_time:102461ms step_avg:59.36ms
step:1727/1920 train_time:102550ms step_avg:59.38ms
step:1728/1920 train_time:102639ms step_avg:59.40ms
step:1729/1920 train_time:102729ms step_avg:59.42ms
step:1730/1920 train_time:102816ms step_avg:59.43ms
step:1731/1920 train_time:102905ms step_avg:59.45ms
step:1732/1920 train_time:102992ms step_avg:59.46ms
step:1733/1920 train_time:103081ms step_avg:59.48ms
step:1734/1920 train_time:103169ms step_avg:59.50ms
step:1735/1920 train_time:103258ms step_avg:59.51ms
step:1736/1920 train_time:103346ms step_avg:59.53ms
step:1737/1920 train_time:103435ms step_avg:59.55ms
step:1738/1920 train_time:103523ms step_avg:59.56ms
step:1739/1920 train_time:103611ms step_avg:59.58ms
step:1740/1920 train_time:103700ms step_avg:59.60ms
step:1741/1920 train_time:103789ms step_avg:59.61ms
step:1742/1920 train_time:103878ms step_avg:59.63ms
step:1743/1920 train_time:103966ms step_avg:59.65ms
step:1744/1920 train_time:104054ms step_avg:59.66ms
step:1745/1920 train_time:104142ms step_avg:59.68ms
step:1746/1920 train_time:104230ms step_avg:59.70ms
step:1747/1920 train_time:104320ms step_avg:59.71ms
step:1748/1920 train_time:104408ms step_avg:59.73ms
step:1749/1920 train_time:104497ms step_avg:59.75ms
step:1750/1920 train_time:104585ms step_avg:59.76ms
step:1750/1920 val_loss:3.3222 train_time:104676ms step_avg:59.81ms
step:1751/1920 train_time:104694ms step_avg:59.79ms
step:1752/1920 train_time:104767ms step_avg:59.80ms
step:1753/1920 train_time:104859ms step_avg:59.82ms
step:1754/1920 train_time:104946ms step_avg:59.83ms
step:1755/1920 train_time:105034ms step_avg:59.85ms
step:1756/1920 train_time:105122ms step_avg:59.86ms
step:1757/1920 train_time:105208ms step_avg:59.88ms
step:1758/1920 train_time:105295ms step_avg:59.89ms
step:1759/1920 train_time:105383ms step_avg:59.91ms
step:1760/1920 train_time:105470ms step_avg:59.93ms
step:1761/1920 train_time:105559ms step_avg:59.94ms
step:1762/1920 train_time:105648ms step_avg:59.96ms
step:1763/1920 train_time:105741ms step_avg:59.98ms
step:1764/1920 train_time:105831ms step_avg:60.00ms
step:1765/1920 train_time:105921ms step_avg:60.01ms
step:1766/1920 train_time:106009ms step_avg:60.03ms
step:1767/1920 train_time:106098ms step_avg:60.04ms
step:1768/1920 train_time:106185ms step_avg:60.06ms
step:1769/1920 train_time:106272ms step_avg:60.07ms
step:1770/1920 train_time:106359ms step_avg:60.09ms
step:1771/1920 train_time:106447ms step_avg:60.11ms
step:1772/1920 train_time:106534ms step_avg:60.12ms
step:1773/1920 train_time:106625ms step_avg:60.14ms
step:1774/1920 train_time:106716ms step_avg:60.16ms
step:1775/1920 train_time:106807ms step_avg:60.17ms
step:1776/1920 train_time:106896ms step_avg:60.19ms
step:1777/1920 train_time:106986ms step_avg:60.21ms
step:1778/1920 train_time:107074ms step_avg:60.22ms
step:1779/1920 train_time:107163ms step_avg:60.24ms
step:1780/1920 train_time:107250ms step_avg:60.25ms
step:1781/1920 train_time:107338ms step_avg:60.27ms
step:1782/1920 train_time:107425ms step_avg:60.28ms
step:1783/1920 train_time:107512ms step_avg:60.30ms
step:1784/1920 train_time:107601ms step_avg:60.31ms
step:1785/1920 train_time:107690ms step_avg:60.33ms
step:1786/1920 train_time:107780ms step_avg:60.35ms
step:1787/1920 train_time:107870ms step_avg:60.36ms
step:1788/1920 train_time:107959ms step_avg:60.38ms
step:1789/1920 train_time:108048ms step_avg:60.40ms
step:1790/1920 train_time:108136ms step_avg:60.41ms
step:1791/1920 train_time:108224ms step_avg:60.43ms
step:1792/1920 train_time:108311ms step_avg:60.44ms
step:1793/1920 train_time:108399ms step_avg:60.46ms
step:1794/1920 train_time:108486ms step_avg:60.47ms
step:1795/1920 train_time:108574ms step_avg:60.49ms
step:1796/1920 train_time:108663ms step_avg:60.50ms
step:1797/1920 train_time:108752ms step_avg:60.52ms
step:1798/1920 train_time:108843ms step_avg:60.54ms
step:1799/1920 train_time:108932ms step_avg:60.55ms
step:1800/1920 train_time:109021ms step_avg:60.57ms
step:1801/1920 train_time:109109ms step_avg:60.58ms
step:1802/1920 train_time:109197ms step_avg:60.60ms
step:1803/1920 train_time:109286ms step_avg:60.61ms
step:1804/1920 train_time:109374ms step_avg:60.63ms
step:1805/1920 train_time:109464ms step_avg:60.64ms
step:1806/1920 train_time:109552ms step_avg:60.66ms
step:1807/1920 train_time:109641ms step_avg:60.68ms
step:1808/1920 train_time:109730ms step_avg:60.69ms
step:1809/1920 train_time:109820ms step_avg:60.71ms
step:1810/1920 train_time:109909ms step_avg:60.72ms
step:1811/1920 train_time:109998ms step_avg:60.74ms
step:1812/1920 train_time:110086ms step_avg:60.75ms
step:1813/1920 train_time:110174ms step_avg:60.77ms
step:1814/1920 train_time:110262ms step_avg:60.78ms
step:1815/1920 train_time:110350ms step_avg:60.80ms
step:1816/1920 train_time:110438ms step_avg:60.81ms
step:1817/1920 train_time:110526ms step_avg:60.83ms
step:1818/1920 train_time:110615ms step_avg:60.84ms
step:1819/1920 train_time:110705ms step_avg:60.86ms
step:1820/1920 train_time:110793ms step_avg:60.88ms
step:1821/1920 train_time:110884ms step_avg:60.89ms
step:1822/1920 train_time:110973ms step_avg:60.91ms
step:1823/1920 train_time:111062ms step_avg:60.92ms
step:1824/1920 train_time:111149ms step_avg:60.94ms
step:1825/1920 train_time:111238ms step_avg:60.95ms
step:1826/1920 train_time:111325ms step_avg:60.97ms
step:1827/1920 train_time:111414ms step_avg:60.98ms
step:1828/1920 train_time:111502ms step_avg:61.00ms
step:1829/1920 train_time:111591ms step_avg:61.01ms
step:1830/1920 train_time:111680ms step_avg:61.03ms
step:1831/1920 train_time:111769ms step_avg:61.04ms
step:1832/1920 train_time:111857ms step_avg:61.06ms
step:1833/1920 train_time:111947ms step_avg:61.07ms
step:1834/1920 train_time:112036ms step_avg:61.09ms
step:1835/1920 train_time:112125ms step_avg:61.10ms
step:1836/1920 train_time:112214ms step_avg:61.12ms
step:1837/1920 train_time:112302ms step_avg:61.13ms
step:1838/1920 train_time:112390ms step_avg:61.15ms
step:1839/1920 train_time:112479ms step_avg:61.16ms
step:1840/1920 train_time:112567ms step_avg:61.18ms
step:1841/1920 train_time:112656ms step_avg:61.19ms
step:1842/1920 train_time:112743ms step_avg:61.21ms
step:1843/1920 train_time:112831ms step_avg:61.22ms
step:1844/1920 train_time:112920ms step_avg:61.24ms
step:1845/1920 train_time:113009ms step_avg:61.25ms
step:1846/1920 train_time:113098ms step_avg:61.27ms
step:1847/1920 train_time:113187ms step_avg:61.28ms
step:1848/1920 train_time:113275ms step_avg:61.30ms
step:1849/1920 train_time:113364ms step_avg:61.31ms
step:1850/1920 train_time:113452ms step_avg:61.33ms
step:1851/1920 train_time:113542ms step_avg:61.34ms
step:1852/1920 train_time:113631ms step_avg:61.36ms
step:1853/1920 train_time:113720ms step_avg:61.37ms
step:1854/1920 train_time:113808ms step_avg:61.39ms
step:1855/1920 train_time:113896ms step_avg:61.40ms
step:1856/1920 train_time:113985ms step_avg:61.41ms
step:1857/1920 train_time:114074ms step_avg:61.43ms
step:1858/1920 train_time:114163ms step_avg:61.44ms
step:1859/1920 train_time:114252ms step_avg:61.46ms
step:1860/1920 train_time:114340ms step_avg:61.47ms
step:1861/1920 train_time:114428ms step_avg:61.49ms
step:1862/1920 train_time:114517ms step_avg:61.50ms
step:1863/1920 train_time:114607ms step_avg:61.52ms
step:1864/1920 train_time:114695ms step_avg:61.53ms
step:1865/1920 train_time:114784ms step_avg:61.55ms
step:1866/1920 train_time:114873ms step_avg:61.56ms
step:1867/1920 train_time:114963ms step_avg:61.58ms
step:1868/1920 train_time:115051ms step_avg:61.59ms
step:1869/1920 train_time:115139ms step_avg:61.60ms
step:1870/1920 train_time:115228ms step_avg:61.62ms
step:1871/1920 train_time:115316ms step_avg:61.63ms
step:1872/1920 train_time:115404ms step_avg:61.65ms
step:1873/1920 train_time:115492ms step_avg:61.66ms
step:1874/1920 train_time:115581ms step_avg:61.68ms
step:1875/1920 train_time:115669ms step_avg:61.69ms
step:1876/1920 train_time:115757ms step_avg:61.70ms
step:1877/1920 train_time:115846ms step_avg:61.72ms
step:1878/1920 train_time:115934ms step_avg:61.73ms
step:1879/1920 train_time:116023ms step_avg:61.75ms
step:1880/1920 train_time:116113ms step_avg:61.76ms
step:1881/1920 train_time:116202ms step_avg:61.78ms
step:1882/1920 train_time:116290ms step_avg:61.79ms
step:1883/1920 train_time:116379ms step_avg:61.81ms
step:1884/1920 train_time:116467ms step_avg:61.82ms
step:1885/1920 train_time:116557ms step_avg:61.83ms
step:1886/1920 train_time:116644ms step_avg:61.85ms
step:1887/1920 train_time:116733ms step_avg:61.86ms
step:1888/1920 train_time:116822ms step_avg:61.88ms
step:1889/1920 train_time:116910ms step_avg:61.89ms
step:1890/1920 train_time:116999ms step_avg:61.90ms
step:1891/1920 train_time:117089ms step_avg:61.92ms
step:1892/1920 train_time:117177ms step_avg:61.93ms
step:1893/1920 train_time:117267ms step_avg:61.95ms
step:1894/1920 train_time:117355ms step_avg:61.96ms
step:1895/1920 train_time:117445ms step_avg:61.98ms
step:1896/1920 train_time:117533ms step_avg:61.99ms
step:1897/1920 train_time:117623ms step_avg:62.00ms
step:1898/1920 train_time:117712ms step_avg:62.02ms
step:1899/1920 train_time:117801ms step_avg:62.03ms
step:1900/1920 train_time:117889ms step_avg:62.05ms
step:1901/1920 train_time:117978ms step_avg:62.06ms
step:1902/1920 train_time:118067ms step_avg:62.07ms
step:1903/1920 train_time:118156ms step_avg:62.09ms
step:1904/1920 train_time:118245ms step_avg:62.10ms
step:1905/1920 train_time:118335ms step_avg:62.12ms
step:1906/1920 train_time:118423ms step_avg:62.13ms
step:1907/1920 train_time:118512ms step_avg:62.15ms
step:1908/1920 train_time:118600ms step_avg:62.16ms
step:1909/1920 train_time:118689ms step_avg:62.17ms
step:1910/1920 train_time:118777ms step_avg:62.19ms
step:1911/1920 train_time:118866ms step_avg:62.20ms
step:1912/1920 train_time:118955ms step_avg:62.22ms
step:1913/1920 train_time:119045ms step_avg:62.23ms
step:1914/1920 train_time:119135ms step_avg:62.24ms
step:1915/1920 train_time:119225ms step_avg:62.26ms
step:1916/1920 train_time:119314ms step_avg:62.27ms
step:1917/1920 train_time:119404ms step_avg:62.29ms
step:1918/1920 train_time:119492ms step_avg:62.30ms
step:1919/1920 train_time:119582ms step_avg:62.31ms
step:1920/1920 train_time:119669ms step_avg:62.33ms
step:1920/1920 val_loss:3.2776 train_time:119760ms step_avg:62.38ms
peak memory allocated: 29863 MiB reserved: 43978 MiB

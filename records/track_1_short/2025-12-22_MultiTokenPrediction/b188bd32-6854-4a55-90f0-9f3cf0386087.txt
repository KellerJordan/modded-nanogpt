import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:16:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    235296      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    235297      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    235298      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    235299      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    235300      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    235301      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    235302      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    235303      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    235297      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    235298      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    235299      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    235300      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    235301      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    235302      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    235303      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8353 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:72ms step_avg:71.84ms
step:2/1920 train_time:95ms step_avg:47.39ms
step:3/1920 train_time:119ms step_avg:39.50ms
step:4/1920 train_time:152ms step_avg:38.10ms
step:5/1920 train_time:187ms step_avg:37.32ms
step:6/1920 train_time:262ms step_avg:43.74ms
step:7/1920 train_time:285ms step_avg:40.78ms
step:8/1920 train_time:320ms step_avg:39.96ms
step:9/1920 train_time:354ms step_avg:39.29ms
step:10/1920 train_time:388ms step_avg:38.77ms
step:11/1920 train_time:422ms step_avg:38.35ms
step:12/1920 train_time:456ms step_avg:38.01ms
step:13/1920 train_time:490ms step_avg:37.72ms
step:14/1920 train_time:525ms step_avg:37.47ms
step:15/1920 train_time:559ms step_avg:37.25ms
step:16/1920 train_time:593ms step_avg:37.06ms
step:17/1920 train_time:627ms step_avg:36.91ms
step:18/1920 train_time:662ms step_avg:36.76ms
step:19/1920 train_time:696ms step_avg:36.62ms
step:20/1920 train_time:730ms step_avg:36.50ms
step:21/1920 train_time:765ms step_avg:36.41ms
step:22/1920 train_time:799ms step_avg:36.31ms
step:23/1920 train_time:833ms step_avg:36.22ms
step:24/1920 train_time:867ms step_avg:36.14ms
step:25/1920 train_time:902ms step_avg:36.07ms
step:26/1920 train_time:936ms step_avg:36.00ms
step:27/1920 train_time:970ms step_avg:35.93ms
step:28/1920 train_time:1004ms step_avg:35.87ms
step:29/1920 train_time:1039ms step_avg:35.81ms
step:30/1920 train_time:1073ms step_avg:35.76ms
step:31/1920 train_time:1107ms step_avg:35.71ms
step:32/1920 train_time:1142ms step_avg:35.67ms
step:33/1920 train_time:1177ms step_avg:35.66ms
step:34/1920 train_time:1212ms step_avg:35.64ms
step:35/1920 train_time:1246ms step_avg:35.61ms
step:36/1920 train_time:1281ms step_avg:35.60ms
step:37/1920 train_time:1317ms step_avg:35.59ms
step:38/1920 train_time:1351ms step_avg:35.57ms
step:39/1920 train_time:1386ms step_avg:35.53ms
step:40/1920 train_time:1420ms step_avg:35.50ms
step:41/1920 train_time:1454ms step_avg:35.47ms
step:42/1920 train_time:1489ms step_avg:35.45ms
step:43/1920 train_time:1523ms step_avg:35.42ms
step:44/1920 train_time:1558ms step_avg:35.40ms
step:45/1920 train_time:1592ms step_avg:35.37ms
step:46/1920 train_time:1626ms step_avg:35.34ms
step:47/1920 train_time:1660ms step_avg:35.32ms
step:48/1920 train_time:1695ms step_avg:35.30ms
step:49/1920 train_time:1729ms step_avg:35.29ms
step:50/1920 train_time:1763ms step_avg:35.26ms
step:51/1920 train_time:1798ms step_avg:35.25ms
step:52/1920 train_time:1832ms step_avg:35.23ms
step:53/1920 train_time:1866ms step_avg:35.21ms
step:54/1920 train_time:1901ms step_avg:35.20ms
step:55/1920 train_time:1935ms step_avg:35.18ms
step:56/1920 train_time:1969ms step_avg:35.16ms
step:57/1920 train_time:2003ms step_avg:35.14ms
step:58/1920 train_time:2037ms step_avg:35.13ms
step:59/1920 train_time:2072ms step_avg:35.11ms
step:60/1920 train_time:2106ms step_avg:35.09ms
step:61/1920 train_time:2140ms step_avg:35.09ms
step:62/1920 train_time:2175ms step_avg:35.07ms
step:63/1920 train_time:2209ms step_avg:35.06ms
step:64/1920 train_time:2243ms step_avg:35.05ms
step:65/1920 train_time:2279ms step_avg:35.06ms
step:66/1920 train_time:2313ms step_avg:35.05ms
step:67/1920 train_time:2347ms step_avg:35.04ms
step:68/1920 train_time:2382ms step_avg:35.03ms
step:69/1920 train_time:2417ms step_avg:35.03ms
step:70/1920 train_time:2452ms step_avg:35.02ms
step:71/1920 train_time:2486ms step_avg:35.01ms
step:72/1920 train_time:2520ms step_avg:35.00ms
step:73/1920 train_time:2554ms step_avg:34.99ms
step:74/1920 train_time:2589ms step_avg:34.99ms
step:75/1920 train_time:2623ms step_avg:34.98ms
step:76/1920 train_time:2658ms step_avg:34.97ms
step:77/1920 train_time:2692ms step_avg:34.96ms
step:78/1920 train_time:2726ms step_avg:34.95ms
step:79/1920 train_time:2761ms step_avg:34.95ms
step:80/1920 train_time:2795ms step_avg:34.94ms
step:81/1920 train_time:2830ms step_avg:34.94ms
step:82/1920 train_time:2864ms step_avg:34.93ms
step:83/1920 train_time:2899ms step_avg:34.92ms
step:84/1920 train_time:2933ms step_avg:34.91ms
step:85/1920 train_time:2967ms step_avg:34.91ms
step:86/1920 train_time:3001ms step_avg:34.90ms
step:87/1920 train_time:3035ms step_avg:34.89ms
step:88/1920 train_time:3070ms step_avg:34.88ms
step:89/1920 train_time:3104ms step_avg:34.88ms
step:90/1920 train_time:3138ms step_avg:34.87ms
step:91/1920 train_time:3173ms step_avg:34.87ms
step:92/1920 train_time:3207ms step_avg:34.86ms
step:93/1920 train_time:3242ms step_avg:34.86ms
step:94/1920 train_time:3276ms step_avg:34.85ms
step:95/1920 train_time:3311ms step_avg:34.85ms
step:96/1920 train_time:3345ms step_avg:34.84ms
step:97/1920 train_time:3380ms step_avg:34.85ms
step:98/1920 train_time:3415ms step_avg:34.84ms
step:99/1920 train_time:3449ms step_avg:34.84ms
step:100/1920 train_time:3484ms step_avg:34.84ms
step:101/1920 train_time:3519ms step_avg:34.84ms
step:102/1920 train_time:3553ms step_avg:34.83ms
step:103/1920 train_time:3588ms step_avg:34.83ms
step:104/1920 train_time:3622ms step_avg:34.83ms
step:105/1920 train_time:3657ms step_avg:34.83ms
step:106/1920 train_time:3691ms step_avg:34.82ms
step:107/1920 train_time:3726ms step_avg:34.82ms
step:108/1920 train_time:3760ms step_avg:34.82ms
step:109/1920 train_time:3794ms step_avg:34.81ms
step:110/1920 train_time:3829ms step_avg:34.81ms
step:111/1920 train_time:3863ms step_avg:34.80ms
step:112/1920 train_time:3898ms step_avg:34.80ms
step:113/1920 train_time:3932ms step_avg:34.80ms
step:114/1920 train_time:3966ms step_avg:34.79ms
step:115/1920 train_time:4001ms step_avg:34.79ms
step:116/1920 train_time:4035ms step_avg:34.78ms
step:117/1920 train_time:4070ms step_avg:34.78ms
step:118/1920 train_time:4104ms step_avg:34.78ms
step:119/1920 train_time:4138ms step_avg:34.77ms
step:120/1920 train_time:4172ms step_avg:34.77ms
step:121/1920 train_time:4207ms step_avg:34.77ms
step:122/1920 train_time:4241ms step_avg:34.76ms
step:123/1920 train_time:4275ms step_avg:34.76ms
step:124/1920 train_time:4309ms step_avg:34.75ms
step:125/1920 train_time:4344ms step_avg:34.75ms
step:126/1920 train_time:4378ms step_avg:34.75ms
step:127/1920 train_time:4413ms step_avg:34.75ms
step:128/1920 train_time:4447ms step_avg:34.74ms
step:129/1920 train_time:4482ms step_avg:34.74ms
step:130/1920 train_time:4516ms step_avg:34.74ms
step:131/1920 train_time:4551ms step_avg:34.74ms
step:132/1920 train_time:4585ms step_avg:34.73ms
step:133/1920 train_time:4619ms step_avg:34.73ms
step:134/1920 train_time:4654ms step_avg:34.73ms
step:135/1920 train_time:4688ms step_avg:34.73ms
step:136/1920 train_time:4723ms step_avg:34.73ms
step:137/1920 train_time:4758ms step_avg:34.73ms
step:138/1920 train_time:4792ms step_avg:34.72ms
step:139/1920 train_time:4826ms step_avg:34.72ms
step:140/1920 train_time:4861ms step_avg:34.72ms
step:141/1920 train_time:4895ms step_avg:34.71ms
step:142/1920 train_time:4929ms step_avg:34.71ms
step:143/1920 train_time:4963ms step_avg:34.71ms
step:144/1920 train_time:4997ms step_avg:34.70ms
step:145/1920 train_time:5032ms step_avg:34.70ms
step:146/1920 train_time:5066ms step_avg:34.70ms
step:147/1920 train_time:5100ms step_avg:34.69ms
step:148/1920 train_time:5134ms step_avg:34.69ms
step:149/1920 train_time:5169ms step_avg:34.69ms
step:150/1920 train_time:5203ms step_avg:34.69ms
step:151/1920 train_time:5237ms step_avg:34.68ms
step:152/1920 train_time:5271ms step_avg:34.68ms
step:153/1920 train_time:5306ms step_avg:34.68ms
step:154/1920 train_time:5340ms step_avg:34.68ms
step:155/1920 train_time:5375ms step_avg:34.68ms
step:156/1920 train_time:5409ms step_avg:34.67ms
step:157/1920 train_time:5444ms step_avg:34.67ms
step:158/1920 train_time:5478ms step_avg:34.67ms
step:159/1920 train_time:5513ms step_avg:34.67ms
step:160/1920 train_time:5547ms step_avg:34.67ms
step:161/1920 train_time:5581ms step_avg:34.67ms
step:162/1920 train_time:5616ms step_avg:34.66ms
step:163/1920 train_time:5650ms step_avg:34.66ms
step:164/1920 train_time:5684ms step_avg:34.66ms
step:165/1920 train_time:5719ms step_avg:34.66ms
step:166/1920 train_time:5753ms step_avg:34.66ms
step:167/1920 train_time:5787ms step_avg:34.66ms
step:168/1920 train_time:5822ms step_avg:34.65ms
step:169/1920 train_time:5856ms step_avg:34.65ms
step:170/1920 train_time:5891ms step_avg:34.65ms
step:171/1920 train_time:5925ms step_avg:34.65ms
step:172/1920 train_time:5959ms step_avg:34.65ms
step:173/1920 train_time:5994ms step_avg:34.65ms
step:174/1920 train_time:6028ms step_avg:34.65ms
step:175/1920 train_time:6063ms step_avg:34.65ms
step:176/1920 train_time:6097ms step_avg:34.64ms
step:177/1920 train_time:6132ms step_avg:34.64ms
step:178/1920 train_time:6166ms step_avg:34.64ms
step:179/1920 train_time:6200ms step_avg:34.64ms
step:180/1920 train_time:6235ms step_avg:34.64ms
step:181/1920 train_time:6269ms step_avg:34.64ms
step:182/1920 train_time:6303ms step_avg:34.63ms
step:183/1920 train_time:6338ms step_avg:34.63ms
step:184/1920 train_time:6372ms step_avg:34.63ms
step:185/1920 train_time:6406ms step_avg:34.63ms
step:186/1920 train_time:6440ms step_avg:34.63ms
step:187/1920 train_time:6475ms step_avg:34.63ms
step:188/1920 train_time:6509ms step_avg:34.62ms
step:189/1920 train_time:6543ms step_avg:34.62ms
step:190/1920 train_time:6578ms step_avg:34.62ms
step:191/1920 train_time:6612ms step_avg:34.62ms
step:192/1920 train_time:6646ms step_avg:34.62ms
step:193/1920 train_time:6681ms step_avg:34.62ms
step:194/1920 train_time:6716ms step_avg:34.62ms
step:195/1920 train_time:6750ms step_avg:34.62ms
step:196/1920 train_time:6784ms step_avg:34.61ms
step:197/1920 train_time:6819ms step_avg:34.62ms
step:198/1920 train_time:6854ms step_avg:34.61ms
step:199/1920 train_time:6888ms step_avg:34.61ms
step:200/1920 train_time:6922ms step_avg:34.61ms
step:201/1920 train_time:6957ms step_avg:34.61ms
step:202/1920 train_time:6992ms step_avg:34.61ms
step:203/1920 train_time:7026ms step_avg:34.61ms
step:204/1920 train_time:7060ms step_avg:34.61ms
step:205/1920 train_time:7094ms step_avg:34.61ms
step:206/1920 train_time:7129ms step_avg:34.61ms
step:207/1920 train_time:7163ms step_avg:34.60ms
step:208/1920 train_time:7197ms step_avg:34.60ms
step:209/1920 train_time:7231ms step_avg:34.60ms
step:210/1920 train_time:7265ms step_avg:34.59ms
step:211/1920 train_time:7299ms step_avg:34.59ms
step:212/1920 train_time:7333ms step_avg:34.59ms
step:213/1920 train_time:7368ms step_avg:34.59ms
step:214/1920 train_time:7402ms step_avg:34.59ms
step:215/1920 train_time:7437ms step_avg:34.59ms
step:216/1920 train_time:7471ms step_avg:34.59ms
step:217/1920 train_time:7505ms step_avg:34.59ms
step:218/1920 train_time:7540ms step_avg:34.59ms
step:219/1920 train_time:7574ms step_avg:34.58ms
step:220/1920 train_time:7608ms step_avg:34.58ms
step:221/1920 train_time:7642ms step_avg:34.58ms
step:222/1920 train_time:7677ms step_avg:34.58ms
step:223/1920 train_time:7711ms step_avg:34.58ms
step:224/1920 train_time:7745ms step_avg:34.58ms
step:225/1920 train_time:7780ms step_avg:34.58ms
step:226/1920 train_time:7814ms step_avg:34.58ms
step:227/1920 train_time:7848ms step_avg:34.57ms
step:228/1920 train_time:7883ms step_avg:34.57ms
step:229/1920 train_time:7917ms step_avg:34.57ms
step:230/1920 train_time:7952ms step_avg:34.57ms
step:231/1920 train_time:7986ms step_avg:34.57ms
step:232/1920 train_time:8020ms step_avg:34.57ms
step:233/1920 train_time:8055ms step_avg:34.57ms
step:234/1920 train_time:8089ms step_avg:34.57ms
step:235/1920 train_time:8123ms step_avg:34.57ms
step:236/1920 train_time:8157ms step_avg:34.57ms
step:237/1920 train_time:8192ms step_avg:34.56ms
step:238/1920 train_time:8226ms step_avg:34.56ms
step:239/1920 train_time:8260ms step_avg:34.56ms
step:240/1920 train_time:8294ms step_avg:34.56ms
step:241/1920 train_time:8328ms step_avg:34.56ms
step:242/1920 train_time:8363ms step_avg:34.56ms
step:243/1920 train_time:8397ms step_avg:34.56ms
step:244/1920 train_time:8431ms step_avg:34.55ms
step:245/1920 train_time:8466ms step_avg:34.55ms
step:246/1920 train_time:8500ms step_avg:34.55ms
step:247/1920 train_time:8535ms step_avg:34.55ms
step:248/1920 train_time:8569ms step_avg:34.55ms
step:249/1920 train_time:8603ms step_avg:34.55ms
step:250/1920 train_time:8637ms step_avg:34.55ms
step:250/1920 val_loss:4.6026 train_time:8675ms step_avg:34.70ms
step:251/1920 train_time:8694ms step_avg:34.64ms
step:252/1920 train_time:8712ms step_avg:34.57ms
step:253/1920 train_time:8743ms step_avg:34.56ms
step:254/1920 train_time:8777ms step_avg:34.56ms
step:255/1920 train_time:8813ms step_avg:34.56ms
step:256/1920 train_time:8847ms step_avg:34.56ms
step:257/1920 train_time:8882ms step_avg:34.56ms
step:258/1920 train_time:8916ms step_avg:34.56ms
step:259/1920 train_time:8950ms step_avg:34.56ms
step:260/1920 train_time:8984ms step_avg:34.55ms
step:261/1920 train_time:9019ms step_avg:34.55ms
step:262/1920 train_time:9053ms step_avg:34.55ms
step:263/1920 train_time:9087ms step_avg:34.55ms
step:264/1920 train_time:9121ms step_avg:34.55ms
step:265/1920 train_time:9155ms step_avg:34.55ms
step:266/1920 train_time:9189ms step_avg:34.55ms
step:267/1920 train_time:9223ms step_avg:34.54ms
step:268/1920 train_time:9258ms step_avg:34.54ms
step:269/1920 train_time:9292ms step_avg:34.54ms
step:270/1920 train_time:9326ms step_avg:34.54ms
step:271/1920 train_time:9360ms step_avg:34.54ms
step:272/1920 train_time:9394ms step_avg:34.54ms
step:273/1920 train_time:9428ms step_avg:34.53ms
step:274/1920 train_time:9462ms step_avg:34.53ms
step:275/1920 train_time:9496ms step_avg:34.53ms
step:276/1920 train_time:9530ms step_avg:34.53ms
step:277/1920 train_time:9564ms step_avg:34.53ms
step:278/1920 train_time:9599ms step_avg:34.53ms
step:279/1920 train_time:9634ms step_avg:34.53ms
step:280/1920 train_time:9669ms step_avg:34.53ms
step:281/1920 train_time:9704ms step_avg:34.53ms
step:282/1920 train_time:9738ms step_avg:34.53ms
step:283/1920 train_time:9773ms step_avg:34.53ms
step:284/1920 train_time:9808ms step_avg:34.53ms
step:285/1920 train_time:9843ms step_avg:34.54ms
step:286/1920 train_time:9877ms step_avg:34.53ms
step:287/1920 train_time:9911ms step_avg:34.53ms
step:288/1920 train_time:9946ms step_avg:34.53ms
step:289/1920 train_time:9980ms step_avg:34.53ms
step:290/1920 train_time:10014ms step_avg:34.53ms
step:291/1920 train_time:10049ms step_avg:34.53ms
step:292/1920 train_time:10083ms step_avg:34.53ms
step:293/1920 train_time:10118ms step_avg:34.53ms
step:294/1920 train_time:10152ms step_avg:34.53ms
step:295/1920 train_time:10187ms step_avg:34.53ms
step:296/1920 train_time:10221ms step_avg:34.53ms
step:297/1920 train_time:10255ms step_avg:34.53ms
step:298/1920 train_time:10289ms step_avg:34.53ms
step:299/1920 train_time:10324ms step_avg:34.53ms
step:300/1920 train_time:10358ms step_avg:34.53ms
step:301/1920 train_time:10392ms step_avg:34.52ms
step:302/1920 train_time:10426ms step_avg:34.52ms
step:303/1920 train_time:10460ms step_avg:34.52ms
step:304/1920 train_time:10495ms step_avg:34.52ms
step:305/1920 train_time:10529ms step_avg:34.52ms
step:306/1920 train_time:10563ms step_avg:34.52ms
step:307/1920 train_time:10597ms step_avg:34.52ms
step:308/1920 train_time:10632ms step_avg:34.52ms
step:309/1920 train_time:10666ms step_avg:34.52ms
step:310/1920 train_time:10701ms step_avg:34.52ms
step:311/1920 train_time:10736ms step_avg:34.52ms
step:312/1920 train_time:10770ms step_avg:34.52ms
step:313/1920 train_time:10805ms step_avg:34.52ms
step:314/1920 train_time:10839ms step_avg:34.52ms
step:315/1920 train_time:10874ms step_avg:34.52ms
step:316/1920 train_time:10908ms step_avg:34.52ms
step:317/1920 train_time:10943ms step_avg:34.52ms
step:318/1920 train_time:10977ms step_avg:34.52ms
step:319/1920 train_time:11011ms step_avg:34.52ms
step:320/1920 train_time:11045ms step_avg:34.52ms
step:321/1920 train_time:11080ms step_avg:34.52ms
step:322/1920 train_time:11114ms step_avg:34.51ms
step:323/1920 train_time:11148ms step_avg:34.51ms
step:324/1920 train_time:11182ms step_avg:34.51ms
step:325/1920 train_time:11217ms step_avg:34.51ms
step:326/1920 train_time:11251ms step_avg:34.51ms
step:327/1920 train_time:11285ms step_avg:34.51ms
step:328/1920 train_time:11319ms step_avg:34.51ms
step:329/1920 train_time:11354ms step_avg:34.51ms
step:330/1920 train_time:11388ms step_avg:34.51ms
step:331/1920 train_time:11423ms step_avg:34.51ms
step:332/1920 train_time:11457ms step_avg:34.51ms
step:333/1920 train_time:11491ms step_avg:34.51ms
step:334/1920 train_time:11525ms step_avg:34.51ms
step:335/1920 train_time:11560ms step_avg:34.51ms
step:336/1920 train_time:11594ms step_avg:34.51ms
step:337/1920 train_time:11628ms step_avg:34.51ms
step:338/1920 train_time:11662ms step_avg:34.50ms
step:339/1920 train_time:11697ms step_avg:34.50ms
step:340/1920 train_time:11731ms step_avg:34.50ms
step:341/1920 train_time:11765ms step_avg:34.50ms
step:342/1920 train_time:11800ms step_avg:34.50ms
step:343/1920 train_time:11834ms step_avg:34.50ms
step:344/1920 train_time:11868ms step_avg:34.50ms
step:345/1920 train_time:11903ms step_avg:34.50ms
step:346/1920 train_time:11937ms step_avg:34.50ms
step:347/1920 train_time:11972ms step_avg:34.50ms
step:348/1920 train_time:12006ms step_avg:34.50ms
step:349/1920 train_time:12041ms step_avg:34.50ms
step:350/1920 train_time:12075ms step_avg:34.50ms
step:351/1920 train_time:12109ms step_avg:34.50ms
step:352/1920 train_time:12143ms step_avg:34.50ms
step:353/1920 train_time:12178ms step_avg:34.50ms
step:354/1920 train_time:12213ms step_avg:34.50ms
step:355/1920 train_time:12246ms step_avg:34.50ms
step:356/1920 train_time:12281ms step_avg:34.50ms
step:357/1920 train_time:12315ms step_avg:34.50ms
step:358/1920 train_time:12349ms step_avg:34.49ms
step:359/1920 train_time:12383ms step_avg:34.49ms
step:360/1920 train_time:12418ms step_avg:34.49ms
step:361/1920 train_time:12452ms step_avg:34.49ms
step:362/1920 train_time:12486ms step_avg:34.49ms
step:363/1920 train_time:12520ms step_avg:34.49ms
step:364/1920 train_time:12554ms step_avg:34.49ms
step:365/1920 train_time:12588ms step_avg:34.49ms
step:366/1920 train_time:12622ms step_avg:34.49ms
step:367/1920 train_time:12657ms step_avg:34.49ms
step:368/1920 train_time:12691ms step_avg:34.49ms
step:369/1920 train_time:12726ms step_avg:34.49ms
step:370/1920 train_time:12760ms step_avg:34.49ms
step:371/1920 train_time:12795ms step_avg:34.49ms
step:372/1920 train_time:12829ms step_avg:34.49ms
step:373/1920 train_time:12863ms step_avg:34.49ms
step:374/1920 train_time:12897ms step_avg:34.48ms
step:375/1920 train_time:12932ms step_avg:34.48ms
step:376/1920 train_time:12966ms step_avg:34.48ms
step:377/1920 train_time:13000ms step_avg:34.48ms
step:378/1920 train_time:13034ms step_avg:34.48ms
step:379/1920 train_time:13069ms step_avg:34.48ms
step:380/1920 train_time:13103ms step_avg:34.48ms
step:381/1920 train_time:13138ms step_avg:34.48ms
step:382/1920 train_time:13172ms step_avg:34.48ms
step:383/1920 train_time:13206ms step_avg:34.48ms
step:384/1920 train_time:13240ms step_avg:34.48ms
step:385/1920 train_time:13275ms step_avg:34.48ms
step:386/1920 train_time:13309ms step_avg:34.48ms
step:387/1920 train_time:13344ms step_avg:34.48ms
step:388/1920 train_time:13378ms step_avg:34.48ms
step:389/1920 train_time:13412ms step_avg:34.48ms
step:390/1920 train_time:13447ms step_avg:34.48ms
step:391/1920 train_time:13481ms step_avg:34.48ms
step:392/1920 train_time:13515ms step_avg:34.48ms
step:393/1920 train_time:13549ms step_avg:34.48ms
step:394/1920 train_time:13583ms step_avg:34.48ms
step:395/1920 train_time:13618ms step_avg:34.48ms
step:396/1920 train_time:13652ms step_avg:34.47ms
step:397/1920 train_time:13686ms step_avg:34.47ms
step:398/1920 train_time:13720ms step_avg:34.47ms
step:399/1920 train_time:13755ms step_avg:34.47ms
step:400/1920 train_time:13789ms step_avg:34.47ms
step:401/1920 train_time:13823ms step_avg:34.47ms
step:402/1920 train_time:13858ms step_avg:34.47ms
step:403/1920 train_time:13892ms step_avg:34.47ms
step:404/1920 train_time:13926ms step_avg:34.47ms
step:405/1920 train_time:13961ms step_avg:34.47ms
step:406/1920 train_time:13995ms step_avg:34.47ms
step:407/1920 train_time:14029ms step_avg:34.47ms
step:408/1920 train_time:14063ms step_avg:34.47ms
step:409/1920 train_time:14097ms step_avg:34.47ms
step:410/1920 train_time:14132ms step_avg:34.47ms
step:411/1920 train_time:14166ms step_avg:34.47ms
step:412/1920 train_time:14200ms step_avg:34.47ms
step:413/1920 train_time:14234ms step_avg:34.47ms
step:414/1920 train_time:14269ms step_avg:34.47ms
step:415/1920 train_time:14303ms step_avg:34.47ms
step:416/1920 train_time:14338ms step_avg:34.47ms
step:417/1920 train_time:14372ms step_avg:34.47ms
step:418/1920 train_time:14407ms step_avg:34.47ms
step:419/1920 train_time:14441ms step_avg:34.47ms
step:420/1920 train_time:14476ms step_avg:34.47ms
step:421/1920 train_time:14510ms step_avg:34.47ms
step:422/1920 train_time:14544ms step_avg:34.46ms
step:423/1920 train_time:14579ms step_avg:34.47ms
step:424/1920 train_time:14613ms step_avg:34.47ms
step:425/1920 train_time:14647ms step_avg:34.46ms
step:426/1920 train_time:14681ms step_avg:34.46ms
step:427/1920 train_time:14716ms step_avg:34.46ms
step:428/1920 train_time:14750ms step_avg:34.46ms
step:429/1920 train_time:14784ms step_avg:34.46ms
step:430/1920 train_time:14818ms step_avg:34.46ms
step:431/1920 train_time:14853ms step_avg:34.46ms
step:432/1920 train_time:14887ms step_avg:34.46ms
step:433/1920 train_time:14921ms step_avg:34.46ms
step:434/1920 train_time:14955ms step_avg:34.46ms
step:435/1920 train_time:14990ms step_avg:34.46ms
step:436/1920 train_time:15024ms step_avg:34.46ms
step:437/1920 train_time:15059ms step_avg:34.46ms
step:438/1920 train_time:15093ms step_avg:34.46ms
step:439/1920 train_time:15128ms step_avg:34.46ms
step:440/1920 train_time:15162ms step_avg:34.46ms
step:441/1920 train_time:15197ms step_avg:34.46ms
step:442/1920 train_time:15231ms step_avg:34.46ms
step:443/1920 train_time:15265ms step_avg:34.46ms
step:444/1920 train_time:15299ms step_avg:34.46ms
step:445/1920 train_time:15334ms step_avg:34.46ms
step:446/1920 train_time:15368ms step_avg:34.46ms
step:447/1920 train_time:15403ms step_avg:34.46ms
step:448/1920 train_time:15437ms step_avg:34.46ms
step:449/1920 train_time:15471ms step_avg:34.46ms
step:450/1920 train_time:15506ms step_avg:34.46ms
step:451/1920 train_time:15540ms step_avg:34.46ms
step:452/1920 train_time:15575ms step_avg:34.46ms
step:453/1920 train_time:15609ms step_avg:34.46ms
step:454/1920 train_time:15643ms step_avg:34.46ms
step:455/1920 train_time:15678ms step_avg:34.46ms
step:456/1920 train_time:15712ms step_avg:34.46ms
step:457/1920 train_time:15747ms step_avg:34.46ms
step:458/1920 train_time:15781ms step_avg:34.46ms
step:459/1920 train_time:15815ms step_avg:34.46ms
step:460/1920 train_time:15849ms step_avg:34.46ms
step:461/1920 train_time:15884ms step_avg:34.45ms
step:462/1920 train_time:15918ms step_avg:34.45ms
step:463/1920 train_time:15952ms step_avg:34.45ms
step:464/1920 train_time:15986ms step_avg:34.45ms
step:465/1920 train_time:16020ms step_avg:34.45ms
step:466/1920 train_time:16055ms step_avg:34.45ms
step:467/1920 train_time:16089ms step_avg:34.45ms
step:468/1920 train_time:16123ms step_avg:34.45ms
step:469/1920 train_time:16157ms step_avg:34.45ms
step:470/1920 train_time:16192ms step_avg:34.45ms
step:471/1920 train_time:16226ms step_avg:34.45ms
step:472/1920 train_time:16260ms step_avg:34.45ms
step:473/1920 train_time:16295ms step_avg:34.45ms
step:474/1920 train_time:16329ms step_avg:34.45ms
step:475/1920 train_time:16364ms step_avg:34.45ms
step:476/1920 train_time:16398ms step_avg:34.45ms
step:477/1920 train_time:16432ms step_avg:34.45ms
step:478/1920 train_time:16467ms step_avg:34.45ms
step:479/1920 train_time:16502ms step_avg:34.45ms
step:480/1920 train_time:16536ms step_avg:34.45ms
step:481/1920 train_time:16571ms step_avg:34.45ms
step:482/1920 train_time:16605ms step_avg:34.45ms
step:483/1920 train_time:16639ms step_avg:34.45ms
step:484/1920 train_time:16673ms step_avg:34.45ms
step:485/1920 train_time:16708ms step_avg:34.45ms
step:486/1920 train_time:16743ms step_avg:34.45ms
step:487/1920 train_time:16778ms step_avg:34.45ms
step:488/1920 train_time:16812ms step_avg:34.45ms
step:489/1920 train_time:16847ms step_avg:34.45ms
step:490/1920 train_time:16881ms step_avg:34.45ms
step:491/1920 train_time:16916ms step_avg:34.45ms
step:492/1920 train_time:16950ms step_avg:34.45ms
step:493/1920 train_time:16984ms step_avg:34.45ms
step:494/1920 train_time:17018ms step_avg:34.45ms
step:495/1920 train_time:17053ms step_avg:34.45ms
step:496/1920 train_time:17087ms step_avg:34.45ms
step:497/1920 train_time:17122ms step_avg:34.45ms
step:498/1920 train_time:17156ms step_avg:34.45ms
step:499/1920 train_time:17190ms step_avg:34.45ms
step:500/1920 train_time:17224ms step_avg:34.45ms
step:500/1920 val_loss:4.2915 train_time:17262ms step_avg:34.52ms
step:501/1920 train_time:17280ms step_avg:34.49ms
step:502/1920 train_time:17298ms step_avg:34.46ms
step:503/1920 train_time:17333ms step_avg:34.46ms
step:504/1920 train_time:17367ms step_avg:34.46ms
step:505/1920 train_time:17402ms step_avg:34.46ms
step:506/1920 train_time:17436ms step_avg:34.46ms
step:507/1920 train_time:17471ms step_avg:34.46ms
step:508/1920 train_time:17505ms step_avg:34.46ms
step:509/1920 train_time:17539ms step_avg:34.46ms
step:510/1920 train_time:17573ms step_avg:34.46ms
step:511/1920 train_time:17607ms step_avg:34.46ms
step:512/1920 train_time:17641ms step_avg:34.46ms
step:513/1920 train_time:17675ms step_avg:34.45ms
step:514/1920 train_time:17709ms step_avg:34.45ms
step:515/1920 train_time:17743ms step_avg:34.45ms
step:516/1920 train_time:17778ms step_avg:34.45ms
step:517/1920 train_time:17812ms step_avg:34.45ms
step:518/1920 train_time:17846ms step_avg:34.45ms
step:519/1920 train_time:17879ms step_avg:34.45ms
step:520/1920 train_time:17913ms step_avg:34.45ms
step:521/1920 train_time:17948ms step_avg:34.45ms
step:522/1920 train_time:17982ms step_avg:34.45ms
step:523/1920 train_time:18016ms step_avg:34.45ms
step:524/1920 train_time:18050ms step_avg:34.45ms
step:525/1920 train_time:18084ms step_avg:34.45ms
step:526/1920 train_time:18118ms step_avg:34.45ms
step:527/1920 train_time:18152ms step_avg:34.44ms
step:528/1920 train_time:18186ms step_avg:34.44ms
step:529/1920 train_time:18221ms step_avg:34.44ms
step:530/1920 train_time:18255ms step_avg:34.44ms
step:531/1920 train_time:18290ms step_avg:34.45ms
step:532/1920 train_time:18325ms step_avg:34.45ms
step:533/1920 train_time:18359ms step_avg:34.45ms
step:534/1920 train_time:18393ms step_avg:34.44ms
step:535/1920 train_time:18428ms step_avg:34.44ms
step:536/1920 train_time:18462ms step_avg:34.44ms
step:537/1920 train_time:18497ms step_avg:34.44ms
step:538/1920 train_time:18531ms step_avg:34.44ms
step:539/1920 train_time:18565ms step_avg:34.44ms
step:540/1920 train_time:18599ms step_avg:34.44ms
step:541/1920 train_time:18634ms step_avg:34.44ms
step:542/1920 train_time:18668ms step_avg:34.44ms
step:543/1920 train_time:18702ms step_avg:34.44ms
step:544/1920 train_time:18737ms step_avg:34.44ms
step:545/1920 train_time:18771ms step_avg:34.44ms
step:546/1920 train_time:18805ms step_avg:34.44ms
step:547/1920 train_time:18839ms step_avg:34.44ms
step:548/1920 train_time:18874ms step_avg:34.44ms
step:549/1920 train_time:18908ms step_avg:34.44ms
step:550/1920 train_time:18942ms step_avg:34.44ms
step:551/1920 train_time:18976ms step_avg:34.44ms
step:552/1920 train_time:19010ms step_avg:34.44ms
step:553/1920 train_time:19044ms step_avg:34.44ms
step:554/1920 train_time:19079ms step_avg:34.44ms
step:555/1920 train_time:19112ms step_avg:34.44ms
step:556/1920 train_time:19147ms step_avg:34.44ms
step:557/1920 train_time:19181ms step_avg:34.44ms
step:558/1920 train_time:19215ms step_avg:34.44ms
step:559/1920 train_time:19250ms step_avg:34.44ms
step:560/1920 train_time:19284ms step_avg:34.44ms
step:561/1920 train_time:19318ms step_avg:34.44ms
step:562/1920 train_time:19353ms step_avg:34.44ms
step:563/1920 train_time:19387ms step_avg:34.43ms
step:564/1920 train_time:19421ms step_avg:34.43ms
step:565/1920 train_time:19455ms step_avg:34.43ms
step:566/1920 train_time:19490ms step_avg:34.43ms
step:567/1920 train_time:19524ms step_avg:34.43ms
step:568/1920 train_time:19559ms step_avg:34.43ms
step:569/1920 train_time:19593ms step_avg:34.43ms
step:570/1920 train_time:19627ms step_avg:34.43ms
step:571/1920 train_time:19662ms step_avg:34.43ms
step:572/1920 train_time:19696ms step_avg:34.43ms
step:573/1920 train_time:19730ms step_avg:34.43ms
step:574/1920 train_time:19764ms step_avg:34.43ms
step:575/1920 train_time:19799ms step_avg:34.43ms
step:576/1920 train_time:19833ms step_avg:34.43ms
step:577/1920 train_time:19867ms step_avg:34.43ms
step:578/1920 train_time:19901ms step_avg:34.43ms
step:579/1920 train_time:19935ms step_avg:34.43ms
step:580/1920 train_time:19969ms step_avg:34.43ms
step:581/1920 train_time:20003ms step_avg:34.43ms
step:582/1920 train_time:20038ms step_avg:34.43ms
step:583/1920 train_time:20072ms step_avg:34.43ms
step:584/1920 train_time:20106ms step_avg:34.43ms
step:585/1920 train_time:20140ms step_avg:34.43ms
step:586/1920 train_time:20174ms step_avg:34.43ms
step:587/1920 train_time:20209ms step_avg:34.43ms
step:588/1920 train_time:20243ms step_avg:34.43ms
step:589/1920 train_time:20278ms step_avg:34.43ms
step:590/1920 train_time:20312ms step_avg:34.43ms
step:591/1920 train_time:20346ms step_avg:34.43ms
step:592/1920 train_time:20380ms step_avg:34.43ms
step:593/1920 train_time:20415ms step_avg:34.43ms
step:594/1920 train_time:20449ms step_avg:34.43ms
step:595/1920 train_time:20484ms step_avg:34.43ms
step:596/1920 train_time:20518ms step_avg:34.43ms
step:597/1920 train_time:20553ms step_avg:34.43ms
step:598/1920 train_time:20587ms step_avg:34.43ms
step:599/1920 train_time:20621ms step_avg:34.43ms
step:600/1920 train_time:20656ms step_avg:34.43ms
step:601/1920 train_time:20690ms step_avg:34.43ms
step:602/1920 train_time:20724ms step_avg:34.43ms
step:603/1920 train_time:20759ms step_avg:34.43ms
step:604/1920 train_time:20793ms step_avg:34.43ms
step:605/1920 train_time:20827ms step_avg:34.43ms
step:606/1920 train_time:20862ms step_avg:34.43ms
step:607/1920 train_time:20896ms step_avg:34.42ms
step:608/1920 train_time:20930ms step_avg:34.42ms
step:609/1920 train_time:20964ms step_avg:34.42ms
step:610/1920 train_time:20998ms step_avg:34.42ms
step:611/1920 train_time:21032ms step_avg:34.42ms
step:612/1920 train_time:21066ms step_avg:34.42ms
step:613/1920 train_time:21101ms step_avg:34.42ms
step:614/1920 train_time:21135ms step_avg:34.42ms
step:615/1920 train_time:21169ms step_avg:34.42ms
step:616/1920 train_time:21203ms step_avg:34.42ms
step:617/1920 train_time:21238ms step_avg:34.42ms
step:618/1920 train_time:21272ms step_avg:34.42ms
step:619/1920 train_time:21306ms step_avg:34.42ms
step:620/1920 train_time:21340ms step_avg:34.42ms
step:621/1920 train_time:21375ms step_avg:34.42ms
step:622/1920 train_time:21409ms step_avg:34.42ms
step:623/1920 train_time:21444ms step_avg:34.42ms
step:624/1920 train_time:21478ms step_avg:34.42ms
step:625/1920 train_time:21512ms step_avg:34.42ms
step:626/1920 train_time:21547ms step_avg:34.42ms
step:627/1920 train_time:21582ms step_avg:34.42ms
step:628/1920 train_time:21616ms step_avg:34.42ms
step:629/1920 train_time:21677ms step_avg:34.46ms
step:630/1920 train_time:21739ms step_avg:34.51ms
step:631/1920 train_time:21802ms step_avg:34.55ms
step:632/1920 train_time:21863ms step_avg:34.59ms
step:633/1920 train_time:21926ms step_avg:34.64ms
step:634/1920 train_time:21987ms step_avg:34.68ms
step:635/1920 train_time:22049ms step_avg:34.72ms
step:636/1920 train_time:22111ms step_avg:34.77ms
step:637/1920 train_time:22173ms step_avg:34.81ms
step:638/1920 train_time:22235ms step_avg:34.85ms
step:639/1920 train_time:22297ms step_avg:34.89ms
step:640/1920 train_time:22358ms step_avg:34.93ms
step:641/1920 train_time:22421ms step_avg:34.98ms
step:642/1920 train_time:22483ms step_avg:35.02ms
step:643/1920 train_time:22546ms step_avg:35.06ms
step:644/1920 train_time:22608ms step_avg:35.10ms
step:645/1920 train_time:22670ms step_avg:35.15ms
step:646/1920 train_time:22732ms step_avg:35.19ms
step:647/1920 train_time:22795ms step_avg:35.23ms
step:648/1920 train_time:22857ms step_avg:35.27ms
step:649/1920 train_time:22919ms step_avg:35.31ms
step:650/1920 train_time:22981ms step_avg:35.36ms
step:651/1920 train_time:23044ms step_avg:35.40ms
step:652/1920 train_time:23106ms step_avg:35.44ms
step:653/1920 train_time:23168ms step_avg:35.48ms
step:654/1920 train_time:23230ms step_avg:35.52ms
step:655/1920 train_time:23292ms step_avg:35.56ms
step:656/1920 train_time:23354ms step_avg:35.60ms
step:657/1920 train_time:23417ms step_avg:35.64ms
step:658/1920 train_time:23479ms step_avg:35.68ms
step:659/1920 train_time:23541ms step_avg:35.72ms
step:660/1920 train_time:23603ms step_avg:35.76ms
step:661/1920 train_time:23666ms step_avg:35.80ms
step:662/1920 train_time:23728ms step_avg:35.84ms
step:663/1920 train_time:23791ms step_avg:35.88ms
step:664/1920 train_time:23853ms step_avg:35.92ms
step:665/1920 train_time:23916ms step_avg:35.96ms
step:666/1920 train_time:23978ms step_avg:36.00ms
step:667/1920 train_time:24040ms step_avg:36.04ms
step:668/1920 train_time:24102ms step_avg:36.08ms
step:669/1920 train_time:24165ms step_avg:36.12ms
step:670/1920 train_time:24227ms step_avg:36.16ms
step:671/1920 train_time:24289ms step_avg:36.20ms
step:672/1920 train_time:24351ms step_avg:36.24ms
step:673/1920 train_time:24415ms step_avg:36.28ms
step:674/1920 train_time:24476ms step_avg:36.31ms
step:675/1920 train_time:24539ms step_avg:36.35ms
step:676/1920 train_time:24601ms step_avg:36.39ms
step:677/1920 train_time:24663ms step_avg:36.43ms
step:678/1920 train_time:24725ms step_avg:36.47ms
step:679/1920 train_time:24789ms step_avg:36.51ms
step:680/1920 train_time:24851ms step_avg:36.55ms
step:681/1920 train_time:24913ms step_avg:36.58ms
step:682/1920 train_time:24974ms step_avg:36.62ms
step:683/1920 train_time:25037ms step_avg:36.66ms
step:684/1920 train_time:25099ms step_avg:36.69ms
step:685/1920 train_time:25161ms step_avg:36.73ms
step:686/1920 train_time:25222ms step_avg:36.77ms
step:687/1920 train_time:25286ms step_avg:36.81ms
step:688/1920 train_time:25348ms step_avg:36.84ms
step:689/1920 train_time:25411ms step_avg:36.88ms
step:690/1920 train_time:25472ms step_avg:36.92ms
step:691/1920 train_time:25535ms step_avg:36.95ms
step:692/1920 train_time:25596ms step_avg:36.99ms
step:693/1920 train_time:25659ms step_avg:37.03ms
step:694/1920 train_time:25720ms step_avg:37.06ms
step:695/1920 train_time:25783ms step_avg:37.10ms
step:696/1920 train_time:25845ms step_avg:37.13ms
step:697/1920 train_time:25909ms step_avg:37.17ms
step:698/1920 train_time:25971ms step_avg:37.21ms
step:699/1920 train_time:26033ms step_avg:37.24ms
step:700/1920 train_time:26095ms step_avg:37.28ms
step:701/1920 train_time:26158ms step_avg:37.32ms
step:702/1920 train_time:26220ms step_avg:37.35ms
step:703/1920 train_time:26283ms step_avg:37.39ms
step:704/1920 train_time:26345ms step_avg:37.42ms
step:705/1920 train_time:26408ms step_avg:37.46ms
step:706/1920 train_time:26470ms step_avg:37.49ms
step:707/1920 train_time:26532ms step_avg:37.53ms
step:708/1920 train_time:26594ms step_avg:37.56ms
step:709/1920 train_time:26656ms step_avg:37.60ms
step:710/1920 train_time:26718ms step_avg:37.63ms
step:711/1920 train_time:26780ms step_avg:37.67ms
step:712/1920 train_time:26842ms step_avg:37.70ms
step:713/1920 train_time:26906ms step_avg:37.74ms
step:714/1920 train_time:26968ms step_avg:37.77ms
step:715/1920 train_time:27030ms step_avg:37.80ms
step:716/1920 train_time:27092ms step_avg:37.84ms
step:717/1920 train_time:27154ms step_avg:37.87ms
step:718/1920 train_time:27216ms step_avg:37.91ms
step:719/1920 train_time:27279ms step_avg:37.94ms
step:720/1920 train_time:27341ms step_avg:37.97ms
step:721/1920 train_time:27404ms step_avg:38.01ms
step:722/1920 train_time:27465ms step_avg:38.04ms
step:723/1920 train_time:27529ms step_avg:38.08ms
step:724/1920 train_time:27590ms step_avg:38.11ms
step:725/1920 train_time:27652ms step_avg:38.14ms
step:726/1920 train_time:27714ms step_avg:38.17ms
step:727/1920 train_time:27776ms step_avg:38.21ms
step:728/1920 train_time:27838ms step_avg:38.24ms
step:729/1920 train_time:27900ms step_avg:38.27ms
step:730/1920 train_time:27961ms step_avg:38.30ms
step:731/1920 train_time:28024ms step_avg:38.34ms
step:732/1920 train_time:28086ms step_avg:38.37ms
step:733/1920 train_time:28149ms step_avg:38.40ms
step:734/1920 train_time:28211ms step_avg:38.43ms
step:735/1920 train_time:28273ms step_avg:38.47ms
step:736/1920 train_time:28335ms step_avg:38.50ms
step:737/1920 train_time:28398ms step_avg:38.53ms
step:738/1920 train_time:28460ms step_avg:38.56ms
step:739/1920 train_time:28523ms step_avg:38.60ms
step:740/1920 train_time:28584ms step_avg:38.63ms
step:741/1920 train_time:28647ms step_avg:38.66ms
step:742/1920 train_time:28709ms step_avg:38.69ms
step:743/1920 train_time:28772ms step_avg:38.72ms
step:744/1920 train_time:28834ms step_avg:38.76ms
step:745/1920 train_time:28897ms step_avg:38.79ms
step:746/1920 train_time:28958ms step_avg:38.82ms
step:747/1920 train_time:29020ms step_avg:38.85ms
step:748/1920 train_time:29082ms step_avg:38.88ms
step:749/1920 train_time:29145ms step_avg:38.91ms
step:750/1920 train_time:29208ms step_avg:38.94ms
step:750/1920 val_loss:4.0522 train_time:29273ms step_avg:39.03ms
step:751/1920 train_time:29291ms step_avg:39.00ms
step:752/1920 train_time:29334ms step_avg:39.01ms
step:753/1920 train_time:29402ms step_avg:39.05ms
step:754/1920 train_time:29468ms step_avg:39.08ms
step:755/1920 train_time:29532ms step_avg:39.12ms
step:756/1920 train_time:29594ms step_avg:39.14ms
step:757/1920 train_time:29655ms step_avg:39.17ms
step:758/1920 train_time:29716ms step_avg:39.20ms
step:759/1920 train_time:29778ms step_avg:39.23ms
step:760/1920 train_time:29839ms step_avg:39.26ms
step:761/1920 train_time:29901ms step_avg:39.29ms
step:762/1920 train_time:29962ms step_avg:39.32ms
step:763/1920 train_time:30024ms step_avg:39.35ms
step:764/1920 train_time:30085ms step_avg:39.38ms
step:765/1920 train_time:30147ms step_avg:39.41ms
step:766/1920 train_time:30210ms step_avg:39.44ms
step:767/1920 train_time:30273ms step_avg:39.47ms
step:768/1920 train_time:30337ms step_avg:39.50ms
step:769/1920 train_time:30401ms step_avg:39.53ms
step:770/1920 train_time:30464ms step_avg:39.56ms
step:771/1920 train_time:30528ms step_avg:39.60ms
step:772/1920 train_time:30590ms step_avg:39.62ms
step:773/1920 train_time:30652ms step_avg:39.65ms
step:774/1920 train_time:30714ms step_avg:39.68ms
step:775/1920 train_time:30776ms step_avg:39.71ms
step:776/1920 train_time:30837ms step_avg:39.74ms
step:777/1920 train_time:30899ms step_avg:39.77ms
step:778/1920 train_time:30960ms step_avg:39.79ms
step:779/1920 train_time:31022ms step_avg:39.82ms
step:780/1920 train_time:31083ms step_avg:39.85ms
step:781/1920 train_time:31146ms step_avg:39.88ms
step:782/1920 train_time:31208ms step_avg:39.91ms
step:783/1920 train_time:31271ms step_avg:39.94ms
step:784/1920 train_time:31333ms step_avg:39.97ms
step:785/1920 train_time:31396ms step_avg:40.00ms
step:786/1920 train_time:31459ms step_avg:40.02ms
step:787/1920 train_time:31523ms step_avg:40.06ms
step:788/1920 train_time:31586ms step_avg:40.08ms
step:789/1920 train_time:31649ms step_avg:40.11ms
step:790/1920 train_time:31710ms step_avg:40.14ms
step:791/1920 train_time:31773ms step_avg:40.17ms
step:792/1920 train_time:31834ms step_avg:40.19ms
step:793/1920 train_time:31896ms step_avg:40.22ms
step:794/1920 train_time:31958ms step_avg:40.25ms
step:795/1920 train_time:32020ms step_avg:40.28ms
step:796/1920 train_time:32081ms step_avg:40.30ms
step:797/1920 train_time:32144ms step_avg:40.33ms
step:798/1920 train_time:32206ms step_avg:40.36ms
step:799/1920 train_time:32269ms step_avg:40.39ms
step:800/1920 train_time:32331ms step_avg:40.41ms
step:801/1920 train_time:32394ms step_avg:40.44ms
step:802/1920 train_time:32456ms step_avg:40.47ms
step:803/1920 train_time:32520ms step_avg:40.50ms
step:804/1920 train_time:32582ms step_avg:40.52ms
step:805/1920 train_time:32646ms step_avg:40.55ms
step:806/1920 train_time:32707ms step_avg:40.58ms
step:807/1920 train_time:32770ms step_avg:40.61ms
step:808/1920 train_time:32832ms step_avg:40.63ms
step:809/1920 train_time:32894ms step_avg:40.66ms
step:810/1920 train_time:32956ms step_avg:40.69ms
step:811/1920 train_time:33018ms step_avg:40.71ms
step:812/1920 train_time:33080ms step_avg:40.74ms
step:813/1920 train_time:33143ms step_avg:40.77ms
step:814/1920 train_time:33205ms step_avg:40.79ms
step:815/1920 train_time:33267ms step_avg:40.82ms
step:816/1920 train_time:33329ms step_avg:40.84ms
step:817/1920 train_time:33392ms step_avg:40.87ms
step:818/1920 train_time:33454ms step_avg:40.90ms
step:819/1920 train_time:33516ms step_avg:40.92ms
step:820/1920 train_time:33578ms step_avg:40.95ms
step:821/1920 train_time:33642ms step_avg:40.98ms
step:822/1920 train_time:33704ms step_avg:41.00ms
step:823/1920 train_time:33767ms step_avg:41.03ms
step:824/1920 train_time:33829ms step_avg:41.05ms
step:825/1920 train_time:33891ms step_avg:41.08ms
step:826/1920 train_time:33953ms step_avg:41.11ms
step:827/1920 train_time:34015ms step_avg:41.13ms
step:828/1920 train_time:34076ms step_avg:41.16ms
step:829/1920 train_time:34138ms step_avg:41.18ms
step:830/1920 train_time:34200ms step_avg:41.20ms
step:831/1920 train_time:34263ms step_avg:41.23ms
step:832/1920 train_time:34324ms step_avg:41.26ms
step:833/1920 train_time:34387ms step_avg:41.28ms
step:834/1920 train_time:34449ms step_avg:41.31ms
step:835/1920 train_time:34512ms step_avg:41.33ms
step:836/1920 train_time:34574ms step_avg:41.36ms
step:837/1920 train_time:34637ms step_avg:41.38ms
step:838/1920 train_time:34699ms step_avg:41.41ms
step:839/1920 train_time:34763ms step_avg:41.43ms
step:840/1920 train_time:34825ms step_avg:41.46ms
step:841/1920 train_time:34888ms step_avg:41.48ms
step:842/1920 train_time:34949ms step_avg:41.51ms
step:843/1920 train_time:35012ms step_avg:41.53ms
step:844/1920 train_time:35074ms step_avg:41.56ms
step:845/1920 train_time:35136ms step_avg:41.58ms
step:846/1920 train_time:35198ms step_avg:41.60ms
step:847/1920 train_time:35260ms step_avg:41.63ms
step:848/1920 train_time:35322ms step_avg:41.65ms
step:849/1920 train_time:35386ms step_avg:41.68ms
step:850/1920 train_time:35447ms step_avg:41.70ms
step:851/1920 train_time:35510ms step_avg:41.73ms
step:852/1920 train_time:35571ms step_avg:41.75ms
step:853/1920 train_time:35633ms step_avg:41.77ms
step:854/1920 train_time:35695ms step_avg:41.80ms
step:855/1920 train_time:35759ms step_avg:41.82ms
step:856/1920 train_time:35821ms step_avg:41.85ms
step:857/1920 train_time:35884ms step_avg:41.87ms
step:858/1920 train_time:35946ms step_avg:41.90ms
step:859/1920 train_time:36009ms step_avg:41.92ms
step:860/1920 train_time:36070ms step_avg:41.94ms
step:861/1920 train_time:36133ms step_avg:41.97ms
step:862/1920 train_time:36195ms step_avg:41.99ms
step:863/1920 train_time:36257ms step_avg:42.01ms
step:864/1920 train_time:36319ms step_avg:42.04ms
step:865/1920 train_time:36381ms step_avg:42.06ms
step:866/1920 train_time:36444ms step_avg:42.08ms
step:867/1920 train_time:36507ms step_avg:42.11ms
step:868/1920 train_time:36568ms step_avg:42.13ms
step:869/1920 train_time:36631ms step_avg:42.15ms
step:870/1920 train_time:36693ms step_avg:42.18ms
step:871/1920 train_time:36755ms step_avg:42.20ms
step:872/1920 train_time:36817ms step_avg:42.22ms
step:873/1920 train_time:36880ms step_avg:42.24ms
step:874/1920 train_time:36942ms step_avg:42.27ms
step:875/1920 train_time:37006ms step_avg:42.29ms
step:876/1920 train_time:37068ms step_avg:42.31ms
step:877/1920 train_time:37130ms step_avg:42.34ms
step:878/1920 train_time:37192ms step_avg:42.36ms
step:879/1920 train_time:37255ms step_avg:42.38ms
step:880/1920 train_time:37316ms step_avg:42.40ms
step:881/1920 train_time:37379ms step_avg:42.43ms
step:882/1920 train_time:37441ms step_avg:42.45ms
step:883/1920 train_time:37504ms step_avg:42.47ms
step:884/1920 train_time:37566ms step_avg:42.50ms
step:885/1920 train_time:37629ms step_avg:42.52ms
step:886/1920 train_time:37691ms step_avg:42.54ms
step:887/1920 train_time:37754ms step_avg:42.56ms
step:888/1920 train_time:37815ms step_avg:42.58ms
step:889/1920 train_time:37878ms step_avg:42.61ms
step:890/1920 train_time:37940ms step_avg:42.63ms
step:891/1920 train_time:38003ms step_avg:42.65ms
step:892/1920 train_time:38065ms step_avg:42.67ms
step:893/1920 train_time:38127ms step_avg:42.70ms
step:894/1920 train_time:38190ms step_avg:42.72ms
step:895/1920 train_time:38252ms step_avg:42.74ms
step:896/1920 train_time:38315ms step_avg:42.76ms
step:897/1920 train_time:38377ms step_avg:42.78ms
step:898/1920 train_time:38438ms step_avg:42.80ms
step:899/1920 train_time:38502ms step_avg:42.83ms
step:900/1920 train_time:38564ms step_avg:42.85ms
step:901/1920 train_time:38627ms step_avg:42.87ms
step:902/1920 train_time:38689ms step_avg:42.89ms
step:903/1920 train_time:38751ms step_avg:42.91ms
step:904/1920 train_time:38813ms step_avg:42.94ms
step:905/1920 train_time:38876ms step_avg:42.96ms
step:906/1920 train_time:38937ms step_avg:42.98ms
step:907/1920 train_time:39000ms step_avg:43.00ms
step:908/1920 train_time:39063ms step_avg:43.02ms
step:909/1920 train_time:39126ms step_avg:43.04ms
step:910/1920 train_time:39188ms step_avg:43.06ms
step:911/1920 train_time:39251ms step_avg:43.09ms
step:912/1920 train_time:39313ms step_avg:43.11ms
step:913/1920 train_time:39375ms step_avg:43.13ms
step:914/1920 train_time:39437ms step_avg:43.15ms
step:915/1920 train_time:39500ms step_avg:43.17ms
step:916/1920 train_time:39562ms step_avg:43.19ms
step:917/1920 train_time:39625ms step_avg:43.21ms
step:918/1920 train_time:39687ms step_avg:43.23ms
step:919/1920 train_time:39750ms step_avg:43.25ms
step:920/1920 train_time:39811ms step_avg:43.27ms
step:921/1920 train_time:39873ms step_avg:43.29ms
step:922/1920 train_time:39935ms step_avg:43.31ms
step:923/1920 train_time:39998ms step_avg:43.33ms
step:924/1920 train_time:40060ms step_avg:43.36ms
step:925/1920 train_time:40124ms step_avg:43.38ms
step:926/1920 train_time:40186ms step_avg:43.40ms
step:927/1920 train_time:40249ms step_avg:43.42ms
step:928/1920 train_time:40311ms step_avg:43.44ms
step:929/1920 train_time:40373ms step_avg:43.46ms
step:930/1920 train_time:40435ms step_avg:43.48ms
step:931/1920 train_time:40497ms step_avg:43.50ms
step:932/1920 train_time:40558ms step_avg:43.52ms
step:933/1920 train_time:40621ms step_avg:43.54ms
step:934/1920 train_time:40684ms step_avg:43.56ms
step:935/1920 train_time:40747ms step_avg:43.58ms
step:936/1920 train_time:40808ms step_avg:43.60ms
step:937/1920 train_time:40871ms step_avg:43.62ms
step:938/1920 train_time:40933ms step_avg:43.64ms
step:939/1920 train_time:40995ms step_avg:43.66ms
step:940/1920 train_time:41056ms step_avg:43.68ms
step:941/1920 train_time:41119ms step_avg:43.70ms
step:942/1920 train_time:41182ms step_avg:43.72ms
step:943/1920 train_time:41246ms step_avg:43.74ms
step:944/1920 train_time:41308ms step_avg:43.76ms
step:945/1920 train_time:41370ms step_avg:43.78ms
step:946/1920 train_time:41432ms step_avg:43.80ms
step:947/1920 train_time:41494ms step_avg:43.82ms
step:948/1920 train_time:41556ms step_avg:43.84ms
step:949/1920 train_time:41619ms step_avg:43.86ms
step:950/1920 train_time:41681ms step_avg:43.87ms
step:951/1920 train_time:41745ms step_avg:43.90ms
step:952/1920 train_time:41806ms step_avg:43.91ms
step:953/1920 train_time:41869ms step_avg:43.93ms
step:954/1920 train_time:41931ms step_avg:43.95ms
step:955/1920 train_time:41994ms step_avg:43.97ms
step:956/1920 train_time:42056ms step_avg:43.99ms
step:957/1920 train_time:42118ms step_avg:44.01ms
step:958/1920 train_time:42180ms step_avg:44.03ms
step:959/1920 train_time:42243ms step_avg:44.05ms
step:960/1920 train_time:42305ms step_avg:44.07ms
step:961/1920 train_time:42368ms step_avg:44.09ms
step:962/1920 train_time:42430ms step_avg:44.11ms
step:963/1920 train_time:42492ms step_avg:44.12ms
step:964/1920 train_time:42554ms step_avg:44.14ms
step:965/1920 train_time:42617ms step_avg:44.16ms
step:966/1920 train_time:42678ms step_avg:44.18ms
step:967/1920 train_time:42741ms step_avg:44.20ms
step:968/1920 train_time:42804ms step_avg:44.22ms
step:969/1920 train_time:42867ms step_avg:44.24ms
step:970/1920 train_time:42928ms step_avg:44.26ms
step:971/1920 train_time:42991ms step_avg:44.27ms
step:972/1920 train_time:43053ms step_avg:44.29ms
step:973/1920 train_time:43116ms step_avg:44.31ms
step:974/1920 train_time:43177ms step_avg:44.33ms
step:975/1920 train_time:43240ms step_avg:44.35ms
step:976/1920 train_time:43303ms step_avg:44.37ms
step:977/1920 train_time:43365ms step_avg:44.39ms
step:978/1920 train_time:43427ms step_avg:44.40ms
step:979/1920 train_time:43490ms step_avg:44.42ms
step:980/1920 train_time:43551ms step_avg:44.44ms
step:981/1920 train_time:43614ms step_avg:44.46ms
step:982/1920 train_time:43676ms step_avg:44.48ms
step:983/1920 train_time:43739ms step_avg:44.50ms
step:984/1920 train_time:43801ms step_avg:44.51ms
step:985/1920 train_time:43864ms step_avg:44.53ms
step:986/1920 train_time:43926ms step_avg:44.55ms
step:987/1920 train_time:43988ms step_avg:44.57ms
step:988/1920 train_time:44050ms step_avg:44.59ms
step:989/1920 train_time:44112ms step_avg:44.60ms
step:990/1920 train_time:44174ms step_avg:44.62ms
step:991/1920 train_time:44236ms step_avg:44.64ms
step:992/1920 train_time:44298ms step_avg:44.65ms
step:993/1920 train_time:44361ms step_avg:44.67ms
step:994/1920 train_time:44423ms step_avg:44.69ms
step:995/1920 train_time:44486ms step_avg:44.71ms
step:996/1920 train_time:44548ms step_avg:44.73ms
step:997/1920 train_time:44611ms step_avg:44.74ms
step:998/1920 train_time:44673ms step_avg:44.76ms
step:999/1920 train_time:44735ms step_avg:44.78ms
step:1000/1920 train_time:44797ms step_avg:44.80ms
step:1000/1920 val_loss:3.7714 train_time:44863ms step_avg:44.86ms
step:1001/1920 train_time:44880ms step_avg:44.84ms
step:1002/1920 train_time:44923ms step_avg:44.83ms
step:1003/1920 train_time:44988ms step_avg:44.85ms
step:1004/1920 train_time:45051ms step_avg:44.87ms
step:1005/1920 train_time:45114ms step_avg:44.89ms
step:1006/1920 train_time:45175ms step_avg:44.91ms
step:1007/1920 train_time:45237ms step_avg:44.92ms
step:1008/1920 train_time:45299ms step_avg:44.94ms
step:1009/1920 train_time:45361ms step_avg:44.96ms
step:1010/1920 train_time:45422ms step_avg:44.97ms
step:1011/1920 train_time:45484ms step_avg:44.99ms
step:1012/1920 train_time:45546ms step_avg:45.01ms
step:1013/1920 train_time:45608ms step_avg:45.02ms
step:1014/1920 train_time:45671ms step_avg:45.04ms
step:1015/1920 train_time:45733ms step_avg:45.06ms
step:1016/1920 train_time:45795ms step_avg:45.07ms
step:1017/1920 train_time:45858ms step_avg:45.09ms
step:1018/1920 train_time:45921ms step_avg:45.11ms
step:1019/1920 train_time:45985ms step_avg:45.13ms
step:1020/1920 train_time:46047ms step_avg:45.14ms
step:1021/1920 train_time:46111ms step_avg:45.16ms
step:1022/1920 train_time:46173ms step_avg:45.18ms
step:1023/1920 train_time:46236ms step_avg:45.20ms
step:1024/1920 train_time:46297ms step_avg:45.21ms
step:1025/1920 train_time:46360ms step_avg:45.23ms
step:1026/1920 train_time:46422ms step_avg:45.25ms
step:1027/1920 train_time:46483ms step_avg:45.26ms
step:1028/1920 train_time:46545ms step_avg:45.28ms
step:1029/1920 train_time:46607ms step_avg:45.29ms
step:1030/1920 train_time:46669ms step_avg:45.31ms
step:1031/1920 train_time:46732ms step_avg:45.33ms
step:1032/1920 train_time:46794ms step_avg:45.34ms
step:1033/1920 train_time:46857ms step_avg:45.36ms
step:1034/1920 train_time:46919ms step_avg:45.38ms
step:1035/1920 train_time:46983ms step_avg:45.39ms
step:1036/1920 train_time:47045ms step_avg:45.41ms
step:1037/1920 train_time:47108ms step_avg:45.43ms
step:1038/1920 train_time:47171ms step_avg:45.44ms
step:1039/1920 train_time:47234ms step_avg:45.46ms
step:1040/1920 train_time:47295ms step_avg:45.48ms
step:1041/1920 train_time:47358ms step_avg:45.49ms
step:1042/1920 train_time:47420ms step_avg:45.51ms
step:1043/1920 train_time:47482ms step_avg:45.52ms
step:1044/1920 train_time:47543ms step_avg:45.54ms
step:1045/1920 train_time:47605ms step_avg:45.56ms
step:1046/1920 train_time:47667ms step_avg:45.57ms
step:1047/1920 train_time:47730ms step_avg:45.59ms
step:1048/1920 train_time:47792ms step_avg:45.60ms
step:1049/1920 train_time:47855ms step_avg:45.62ms
step:1050/1920 train_time:47917ms step_avg:45.64ms
step:1051/1920 train_time:47980ms step_avg:45.65ms
step:1052/1920 train_time:48042ms step_avg:45.67ms
step:1053/1920 train_time:48105ms step_avg:45.68ms
step:1054/1920 train_time:48167ms step_avg:45.70ms
step:1055/1920 train_time:48230ms step_avg:45.72ms
step:1056/1920 train_time:48292ms step_avg:45.73ms
step:1057/1920 train_time:48355ms step_avg:45.75ms
step:1058/1920 train_time:48417ms step_avg:45.76ms
step:1059/1920 train_time:48479ms step_avg:45.78ms
step:1060/1920 train_time:48541ms step_avg:45.79ms
step:1061/1920 train_time:48603ms step_avg:45.81ms
step:1062/1920 train_time:48665ms step_avg:45.82ms
step:1063/1920 train_time:48727ms step_avg:45.84ms
step:1064/1920 train_time:48789ms step_avg:45.85ms
step:1065/1920 train_time:48852ms step_avg:45.87ms
step:1066/1920 train_time:48915ms step_avg:45.89ms
step:1067/1920 train_time:48978ms step_avg:45.90ms
step:1068/1920 train_time:49040ms step_avg:45.92ms
step:1069/1920 train_time:49103ms step_avg:45.93ms
step:1070/1920 train_time:49165ms step_avg:45.95ms
step:1071/1920 train_time:49228ms step_avg:45.96ms
step:1072/1920 train_time:49290ms step_avg:45.98ms
step:1073/1920 train_time:49353ms step_avg:46.00ms
step:1074/1920 train_time:49415ms step_avg:46.01ms
step:1075/1920 train_time:49478ms step_avg:46.03ms
step:1076/1920 train_time:49540ms step_avg:46.04ms
step:1077/1920 train_time:49602ms step_avg:46.06ms
step:1078/1920 train_time:49664ms step_avg:46.07ms
step:1079/1920 train_time:49726ms step_avg:46.09ms
step:1080/1920 train_time:49788ms step_avg:46.10ms
step:1081/1920 train_time:49851ms step_avg:46.12ms
step:1082/1920 train_time:49913ms step_avg:46.13ms
step:1083/1920 train_time:49976ms step_avg:46.15ms
step:1084/1920 train_time:50038ms step_avg:46.16ms
step:1085/1920 train_time:50101ms step_avg:46.18ms
step:1086/1920 train_time:50164ms step_avg:46.19ms
step:1087/1920 train_time:50226ms step_avg:46.21ms
step:1088/1920 train_time:50289ms step_avg:46.22ms
step:1089/1920 train_time:50353ms step_avg:46.24ms
step:1090/1920 train_time:50415ms step_avg:46.25ms
step:1091/1920 train_time:50477ms step_avg:46.27ms
step:1092/1920 train_time:50538ms step_avg:46.28ms
step:1093/1920 train_time:50601ms step_avg:46.30ms
step:1094/1920 train_time:50663ms step_avg:46.31ms
step:1095/1920 train_time:50724ms step_avg:46.32ms
step:1096/1920 train_time:50786ms step_avg:46.34ms
step:1097/1920 train_time:50848ms step_avg:46.35ms
step:1098/1920 train_time:50911ms step_avg:46.37ms
step:1099/1920 train_time:50974ms step_avg:46.38ms
step:1100/1920 train_time:51035ms step_avg:46.40ms
step:1101/1920 train_time:51098ms step_avg:46.41ms
step:1102/1920 train_time:51160ms step_avg:46.43ms
step:1103/1920 train_time:51223ms step_avg:46.44ms
step:1104/1920 train_time:51285ms step_avg:46.45ms
step:1105/1920 train_time:51348ms step_avg:46.47ms
step:1106/1920 train_time:51411ms step_avg:46.48ms
step:1107/1920 train_time:51473ms step_avg:46.50ms
step:1108/1920 train_time:51535ms step_avg:46.51ms
step:1109/1920 train_time:51597ms step_avg:46.53ms
step:1110/1920 train_time:51659ms step_avg:46.54ms
step:1111/1920 train_time:51721ms step_avg:46.55ms
step:1112/1920 train_time:51783ms step_avg:46.57ms
step:1113/1920 train_time:51845ms step_avg:46.58ms
step:1114/1920 train_time:51907ms step_avg:46.60ms
step:1115/1920 train_time:51971ms step_avg:46.61ms
step:1116/1920 train_time:52033ms step_avg:46.62ms
step:1117/1920 train_time:52096ms step_avg:46.64ms
step:1118/1920 train_time:52158ms step_avg:46.65ms
step:1119/1920 train_time:52221ms step_avg:46.67ms
step:1120/1920 train_time:52283ms step_avg:46.68ms
step:1121/1920 train_time:52345ms step_avg:46.70ms
step:1122/1920 train_time:52407ms step_avg:46.71ms
step:1123/1920 train_time:52470ms step_avg:46.72ms
step:1124/1920 train_time:52532ms step_avg:46.74ms
step:1125/1920 train_time:52595ms step_avg:46.75ms
step:1126/1920 train_time:52658ms step_avg:46.77ms
step:1127/1920 train_time:52720ms step_avg:46.78ms
step:1128/1920 train_time:52782ms step_avg:46.79ms
step:1129/1920 train_time:52844ms step_avg:46.81ms
step:1130/1920 train_time:52905ms step_avg:46.82ms
step:1131/1920 train_time:52968ms step_avg:46.83ms
step:1132/1920 train_time:53030ms step_avg:46.85ms
step:1133/1920 train_time:53094ms step_avg:46.86ms
step:1134/1920 train_time:53155ms step_avg:46.87ms
step:1135/1920 train_time:53218ms step_avg:46.89ms
step:1136/1920 train_time:53280ms step_avg:46.90ms
step:1137/1920 train_time:53342ms step_avg:46.91ms
step:1138/1920 train_time:53405ms step_avg:46.93ms
step:1139/1920 train_time:53467ms step_avg:46.94ms
step:1140/1920 train_time:53529ms step_avg:46.96ms
step:1141/1920 train_time:53593ms step_avg:46.97ms
step:1142/1920 train_time:53654ms step_avg:46.98ms
step:1143/1920 train_time:53717ms step_avg:47.00ms
step:1144/1920 train_time:53779ms step_avg:47.01ms
step:1145/1920 train_time:53841ms step_avg:47.02ms
step:1146/1920 train_time:53903ms step_avg:47.04ms
step:1147/1920 train_time:53967ms step_avg:47.05ms
step:1148/1920 train_time:54029ms step_avg:47.06ms
step:1149/1920 train_time:54092ms step_avg:47.08ms
step:1150/1920 train_time:54154ms step_avg:47.09ms
step:1151/1920 train_time:54217ms step_avg:47.10ms
step:1152/1920 train_time:54279ms step_avg:47.12ms
step:1153/1920 train_time:54342ms step_avg:47.13ms
step:1154/1920 train_time:54403ms step_avg:47.14ms
step:1155/1920 train_time:54467ms step_avg:47.16ms
step:1156/1920 train_time:54529ms step_avg:47.17ms
step:1157/1920 train_time:54592ms step_avg:47.18ms
step:1158/1920 train_time:54654ms step_avg:47.20ms
step:1159/1920 train_time:54717ms step_avg:47.21ms
step:1160/1920 train_time:54779ms step_avg:47.22ms
step:1161/1920 train_time:54841ms step_avg:47.24ms
step:1162/1920 train_time:54903ms step_avg:47.25ms
step:1163/1920 train_time:54966ms step_avg:47.26ms
step:1164/1920 train_time:55027ms step_avg:47.27ms
step:1165/1920 train_time:55090ms step_avg:47.29ms
step:1166/1920 train_time:55153ms step_avg:47.30ms
step:1167/1920 train_time:55215ms step_avg:47.31ms
step:1168/1920 train_time:55278ms step_avg:47.33ms
step:1169/1920 train_time:55340ms step_avg:47.34ms
step:1170/1920 train_time:55403ms step_avg:47.35ms
step:1171/1920 train_time:55466ms step_avg:47.37ms
step:1172/1920 train_time:55528ms step_avg:47.38ms
step:1173/1920 train_time:55591ms step_avg:47.39ms
step:1174/1920 train_time:55653ms step_avg:47.40ms
step:1175/1920 train_time:55716ms step_avg:47.42ms
step:1176/1920 train_time:55777ms step_avg:47.43ms
step:1177/1920 train_time:55840ms step_avg:47.44ms
step:1178/1920 train_time:55902ms step_avg:47.46ms
step:1179/1920 train_time:55965ms step_avg:47.47ms
step:1180/1920 train_time:56027ms step_avg:47.48ms
step:1181/1920 train_time:56090ms step_avg:47.49ms
step:1182/1920 train_time:56152ms step_avg:47.51ms
step:1183/1920 train_time:56215ms step_avg:47.52ms
step:1184/1920 train_time:56276ms step_avg:47.53ms
step:1185/1920 train_time:56339ms step_avg:47.54ms
step:1186/1920 train_time:56401ms step_avg:47.56ms
step:1187/1920 train_time:56464ms step_avg:47.57ms
step:1188/1920 train_time:56525ms step_avg:47.58ms
step:1189/1920 train_time:56588ms step_avg:47.59ms
step:1190/1920 train_time:56650ms step_avg:47.60ms
step:1191/1920 train_time:56713ms step_avg:47.62ms
step:1192/1920 train_time:56775ms step_avg:47.63ms
step:1193/1920 train_time:56837ms step_avg:47.64ms
step:1194/1920 train_time:56899ms step_avg:47.65ms
step:1195/1920 train_time:56961ms step_avg:47.67ms
step:1196/1920 train_time:57024ms step_avg:47.68ms
step:1197/1920 train_time:57086ms step_avg:47.69ms
step:1198/1920 train_time:57149ms step_avg:47.70ms
step:1199/1920 train_time:57212ms step_avg:47.72ms
step:1200/1920 train_time:57274ms step_avg:47.73ms
step:1201/1920 train_time:57337ms step_avg:47.74ms
step:1202/1920 train_time:57398ms step_avg:47.75ms
step:1203/1920 train_time:57461ms step_avg:47.76ms
step:1204/1920 train_time:57523ms step_avg:47.78ms
step:1205/1920 train_time:57586ms step_avg:47.79ms
step:1206/1920 train_time:57648ms step_avg:47.80ms
step:1207/1920 train_time:57711ms step_avg:47.81ms
step:1208/1920 train_time:57773ms step_avg:47.83ms
step:1209/1920 train_time:57836ms step_avg:47.84ms
step:1210/1920 train_time:57898ms step_avg:47.85ms
step:1211/1920 train_time:57960ms step_avg:47.86ms
step:1212/1920 train_time:58023ms step_avg:47.87ms
step:1213/1920 train_time:58085ms step_avg:47.89ms
step:1214/1920 train_time:58147ms step_avg:47.90ms
step:1215/1920 train_time:58210ms step_avg:47.91ms
step:1216/1920 train_time:58273ms step_avg:47.92ms
step:1217/1920 train_time:58336ms step_avg:47.93ms
step:1218/1920 train_time:58397ms step_avg:47.95ms
step:1219/1920 train_time:58460ms step_avg:47.96ms
step:1220/1920 train_time:58522ms step_avg:47.97ms
step:1221/1920 train_time:58584ms step_avg:47.98ms
step:1222/1920 train_time:58647ms step_avg:47.99ms
step:1223/1920 train_time:58709ms step_avg:48.00ms
step:1224/1920 train_time:58771ms step_avg:48.02ms
step:1225/1920 train_time:58835ms step_avg:48.03ms
step:1226/1920 train_time:58896ms step_avg:48.04ms
step:1227/1920 train_time:58959ms step_avg:48.05ms
step:1228/1920 train_time:59021ms step_avg:48.06ms
step:1229/1920 train_time:59083ms step_avg:48.07ms
step:1230/1920 train_time:59145ms step_avg:48.09ms
step:1231/1920 train_time:59208ms step_avg:48.10ms
step:1232/1920 train_time:59271ms step_avg:48.11ms
step:1233/1920 train_time:59334ms step_avg:48.12ms
step:1234/1920 train_time:59396ms step_avg:48.13ms
step:1235/1920 train_time:59458ms step_avg:48.14ms
step:1236/1920 train_time:59520ms step_avg:48.16ms
step:1237/1920 train_time:59582ms step_avg:48.17ms
step:1238/1920 train_time:59644ms step_avg:48.18ms
step:1239/1920 train_time:59707ms step_avg:48.19ms
step:1240/1920 train_time:59769ms step_avg:48.20ms
step:1241/1920 train_time:59832ms step_avg:48.21ms
step:1242/1920 train_time:59894ms step_avg:48.22ms
step:1243/1920 train_time:59957ms step_avg:48.24ms
step:1244/1920 train_time:60019ms step_avg:48.25ms
step:1245/1920 train_time:60081ms step_avg:48.26ms
step:1246/1920 train_time:60143ms step_avg:48.27ms
step:1247/1920 train_time:60206ms step_avg:48.28ms
step:1248/1920 train_time:60268ms step_avg:48.29ms
step:1249/1920 train_time:60331ms step_avg:48.30ms
step:1250/1920 train_time:60393ms step_avg:48.31ms
step:1250/1920 val_loss:3.5508 train_time:60459ms step_avg:48.37ms
step:1251/1920 train_time:60478ms step_avg:48.34ms
step:1252/1920 train_time:60519ms step_avg:48.34ms
step:1253/1920 train_time:60583ms step_avg:48.35ms
step:1254/1920 train_time:60645ms step_avg:48.36ms
step:1255/1920 train_time:60708ms step_avg:48.37ms
step:1256/1920 train_time:60796ms step_avg:48.40ms
step:1257/1920 train_time:60884ms step_avg:48.44ms
step:1258/1920 train_time:60971ms step_avg:48.47ms
step:1259/1920 train_time:61060ms step_avg:48.50ms
step:1260/1920 train_time:61146ms step_avg:48.53ms
step:1261/1920 train_time:61234ms step_avg:48.56ms
step:1262/1920 train_time:61321ms step_avg:48.59ms
step:1263/1920 train_time:61412ms step_avg:48.62ms
step:1264/1920 train_time:61503ms step_avg:48.66ms
step:1265/1920 train_time:61593ms step_avg:48.69ms
step:1266/1920 train_time:61681ms step_avg:48.72ms
step:1267/1920 train_time:61769ms step_avg:48.75ms
step:1268/1920 train_time:61857ms step_avg:48.78ms
step:1269/1920 train_time:61947ms step_avg:48.82ms
step:1270/1920 train_time:62034ms step_avg:48.85ms
step:1271/1920 train_time:62122ms step_avg:48.88ms
step:1272/1920 train_time:62209ms step_avg:48.91ms
step:1273/1920 train_time:62297ms step_avg:48.94ms
step:1274/1920 train_time:62386ms step_avg:48.97ms
step:1275/1920 train_time:62476ms step_avg:49.00ms
step:1276/1920 train_time:62565ms step_avg:49.03ms
step:1277/1920 train_time:62654ms step_avg:49.06ms
step:1278/1920 train_time:62743ms step_avg:49.09ms
step:1279/1920 train_time:62830ms step_avg:49.12ms
step:1280/1920 train_time:62919ms step_avg:49.16ms
step:1281/1920 train_time:63007ms step_avg:49.19ms
step:1282/1920 train_time:63094ms step_avg:49.22ms
step:1283/1920 train_time:63182ms step_avg:49.25ms
step:1284/1920 train_time:63269ms step_avg:49.28ms
step:1285/1920 train_time:63359ms step_avg:49.31ms
step:1286/1920 train_time:63448ms step_avg:49.34ms
step:1287/1920 train_time:63537ms step_avg:49.37ms
step:1288/1920 train_time:63625ms step_avg:49.40ms
step:1289/1920 train_time:63714ms step_avg:49.43ms
step:1290/1920 train_time:63802ms step_avg:49.46ms
step:1291/1920 train_time:63890ms step_avg:49.49ms
step:1292/1920 train_time:63978ms step_avg:49.52ms
step:1293/1920 train_time:64068ms step_avg:49.55ms
step:1294/1920 train_time:64155ms step_avg:49.58ms
step:1295/1920 train_time:64244ms step_avg:49.61ms
step:1296/1920 train_time:64331ms step_avg:49.64ms
step:1297/1920 train_time:64420ms step_avg:49.67ms
step:1298/1920 train_time:64509ms step_avg:49.70ms
step:1299/1920 train_time:64597ms step_avg:49.73ms
step:1300/1920 train_time:64685ms step_avg:49.76ms
step:1301/1920 train_time:64773ms step_avg:49.79ms
step:1302/1920 train_time:64861ms step_avg:49.82ms
step:1303/1920 train_time:64949ms step_avg:49.85ms
step:1304/1920 train_time:65038ms step_avg:49.88ms
step:1305/1920 train_time:65127ms step_avg:49.91ms
step:1306/1920 train_time:65214ms step_avg:49.93ms
step:1307/1920 train_time:65303ms step_avg:49.96ms
step:1308/1920 train_time:65390ms step_avg:49.99ms
step:1309/1920 train_time:65480ms step_avg:50.02ms
step:1310/1920 train_time:65568ms step_avg:50.05ms
step:1311/1920 train_time:65657ms step_avg:50.08ms
step:1312/1920 train_time:65745ms step_avg:50.11ms
step:1313/1920 train_time:65834ms step_avg:50.14ms
step:1314/1920 train_time:65922ms step_avg:50.17ms
step:1315/1920 train_time:66010ms step_avg:50.20ms
step:1316/1920 train_time:66098ms step_avg:50.23ms
step:1317/1920 train_time:66187ms step_avg:50.26ms
step:1318/1920 train_time:66275ms step_avg:50.28ms
step:1319/1920 train_time:66364ms step_avg:50.31ms
step:1320/1920 train_time:66451ms step_avg:50.34ms
step:1321/1920 train_time:66541ms step_avg:50.37ms
step:1322/1920 train_time:66629ms step_avg:50.40ms
step:1323/1920 train_time:66717ms step_avg:50.43ms
step:1324/1920 train_time:66805ms step_avg:50.46ms
step:1325/1920 train_time:66893ms step_avg:50.49ms
step:1326/1920 train_time:66981ms step_avg:50.51ms
step:1327/1920 train_time:67069ms step_avg:50.54ms
step:1328/1920 train_time:67157ms step_avg:50.57ms
step:1329/1920 train_time:67246ms step_avg:50.60ms
step:1330/1920 train_time:67334ms step_avg:50.63ms
step:1331/1920 train_time:67423ms step_avg:50.66ms
step:1332/1920 train_time:67511ms step_avg:50.68ms
step:1333/1920 train_time:67599ms step_avg:50.71ms
step:1334/1920 train_time:67687ms step_avg:50.74ms
step:1335/1920 train_time:67776ms step_avg:50.77ms
step:1336/1920 train_time:67865ms step_avg:50.80ms
step:1337/1920 train_time:67953ms step_avg:50.83ms
step:1338/1920 train_time:68040ms step_avg:50.85ms
step:1339/1920 train_time:68129ms step_avg:50.88ms
step:1340/1920 train_time:68218ms step_avg:50.91ms
step:1341/1920 train_time:68307ms step_avg:50.94ms
step:1342/1920 train_time:68395ms step_avg:50.96ms
step:1343/1920 train_time:68485ms step_avg:50.99ms
step:1344/1920 train_time:68574ms step_avg:51.02ms
step:1345/1920 train_time:68663ms step_avg:51.05ms
step:1346/1920 train_time:68751ms step_avg:51.08ms
step:1347/1920 train_time:68841ms step_avg:51.11ms
step:1348/1920 train_time:68928ms step_avg:51.13ms
step:1349/1920 train_time:69017ms step_avg:51.16ms
step:1350/1920 train_time:69105ms step_avg:51.19ms
step:1351/1920 train_time:69193ms step_avg:51.22ms
step:1352/1920 train_time:69282ms step_avg:51.24ms
step:1353/1920 train_time:69370ms step_avg:51.27ms
step:1354/1920 train_time:69459ms step_avg:51.30ms
step:1355/1920 train_time:69548ms step_avg:51.33ms
step:1356/1920 train_time:69637ms step_avg:51.35ms
step:1357/1920 train_time:69727ms step_avg:51.38ms
step:1358/1920 train_time:69815ms step_avg:51.41ms
step:1359/1920 train_time:69904ms step_avg:51.44ms
step:1360/1920 train_time:69991ms step_avg:51.46ms
step:1361/1920 train_time:70080ms step_avg:51.49ms
step:1362/1920 train_time:70168ms step_avg:51.52ms
step:1363/1920 train_time:70256ms step_avg:51.55ms
step:1364/1920 train_time:70344ms step_avg:51.57ms
step:1365/1920 train_time:70433ms step_avg:51.60ms
step:1366/1920 train_time:70521ms step_avg:51.63ms
step:1367/1920 train_time:70609ms step_avg:51.65ms
step:1368/1920 train_time:70697ms step_avg:51.68ms
step:1369/1920 train_time:70786ms step_avg:51.71ms
step:1370/1920 train_time:70875ms step_avg:51.73ms
step:1371/1920 train_time:70963ms step_avg:51.76ms
step:1372/1920 train_time:71050ms step_avg:51.79ms
step:1373/1920 train_time:71139ms step_avg:51.81ms
step:1374/1920 train_time:71228ms step_avg:51.84ms
step:1375/1920 train_time:71317ms step_avg:51.87ms
step:1376/1920 train_time:71405ms step_avg:51.89ms
step:1377/1920 train_time:71494ms step_avg:51.92ms
step:1378/1920 train_time:71582ms step_avg:51.95ms
step:1379/1920 train_time:71670ms step_avg:51.97ms
step:1380/1920 train_time:71758ms step_avg:52.00ms
step:1381/1920 train_time:71847ms step_avg:52.03ms
step:1382/1920 train_time:71935ms step_avg:52.05ms
step:1383/1920 train_time:72024ms step_avg:52.08ms
step:1384/1920 train_time:72111ms step_avg:52.10ms
step:1385/1920 train_time:72200ms step_avg:52.13ms
step:1386/1920 train_time:72288ms step_avg:52.16ms
step:1387/1920 train_time:72377ms step_avg:52.18ms
step:1388/1920 train_time:72467ms step_avg:52.21ms
step:1389/1920 train_time:72555ms step_avg:52.24ms
step:1390/1920 train_time:72643ms step_avg:52.26ms
step:1391/1920 train_time:72732ms step_avg:52.29ms
step:1392/1920 train_time:72820ms step_avg:52.31ms
step:1393/1920 train_time:72908ms step_avg:52.34ms
step:1394/1920 train_time:72996ms step_avg:52.36ms
step:1395/1920 train_time:73085ms step_avg:52.39ms
step:1396/1920 train_time:73172ms step_avg:52.42ms
step:1397/1920 train_time:73261ms step_avg:52.44ms
step:1398/1920 train_time:73349ms step_avg:52.47ms
step:1399/1920 train_time:73439ms step_avg:52.49ms
step:1400/1920 train_time:73527ms step_avg:52.52ms
step:1401/1920 train_time:73616ms step_avg:52.55ms
step:1402/1920 train_time:73703ms step_avg:52.57ms
step:1403/1920 train_time:73791ms step_avg:52.60ms
step:1404/1920 train_time:73879ms step_avg:52.62ms
step:1405/1920 train_time:73967ms step_avg:52.65ms
step:1406/1920 train_time:74055ms step_avg:52.67ms
step:1407/1920 train_time:74145ms step_avg:52.70ms
step:1408/1920 train_time:74232ms step_avg:52.72ms
step:1409/1920 train_time:74321ms step_avg:52.75ms
step:1410/1920 train_time:74409ms step_avg:52.77ms
step:1411/1920 train_time:74498ms step_avg:52.80ms
step:1412/1920 train_time:74585ms step_avg:52.82ms
step:1413/1920 train_time:74674ms step_avg:52.85ms
step:1414/1920 train_time:74763ms step_avg:52.87ms
step:1415/1920 train_time:74851ms step_avg:52.90ms
step:1416/1920 train_time:74939ms step_avg:52.92ms
step:1417/1920 train_time:75027ms step_avg:52.95ms
step:1418/1920 train_time:75116ms step_avg:52.97ms
step:1419/1920 train_time:75205ms step_avg:53.00ms
step:1420/1920 train_time:75293ms step_avg:53.02ms
step:1421/1920 train_time:75382ms step_avg:53.05ms
step:1422/1920 train_time:75470ms step_avg:53.07ms
step:1423/1920 train_time:75559ms step_avg:53.10ms
step:1424/1920 train_time:75647ms step_avg:53.12ms
step:1425/1920 train_time:75736ms step_avg:53.15ms
step:1426/1920 train_time:75824ms step_avg:53.17ms
step:1427/1920 train_time:75912ms step_avg:53.20ms
step:1428/1920 train_time:76001ms step_avg:53.22ms
step:1429/1920 train_time:76090ms step_avg:53.25ms
step:1430/1920 train_time:76178ms step_avg:53.27ms
step:1431/1920 train_time:76267ms step_avg:53.30ms
step:1432/1920 train_time:76356ms step_avg:53.32ms
step:1433/1920 train_time:76444ms step_avg:53.35ms
step:1434/1920 train_time:76532ms step_avg:53.37ms
step:1435/1920 train_time:76621ms step_avg:53.39ms
step:1436/1920 train_time:76709ms step_avg:53.42ms
step:1437/1920 train_time:76797ms step_avg:53.44ms
step:1438/1920 train_time:76885ms step_avg:53.47ms
step:1439/1920 train_time:76974ms step_avg:53.49ms
step:1440/1920 train_time:77062ms step_avg:53.52ms
step:1441/1920 train_time:77150ms step_avg:53.54ms
step:1442/1920 train_time:77240ms step_avg:53.56ms
step:1443/1920 train_time:77329ms step_avg:53.59ms
step:1444/1920 train_time:77416ms step_avg:53.61ms
step:1445/1920 train_time:77505ms step_avg:53.64ms
step:1446/1920 train_time:77593ms step_avg:53.66ms
step:1447/1920 train_time:77682ms step_avg:53.68ms
step:1448/1920 train_time:77769ms step_avg:53.71ms
step:1449/1920 train_time:77858ms step_avg:53.73ms
step:1450/1920 train_time:77946ms step_avg:53.76ms
step:1451/1920 train_time:78035ms step_avg:53.78ms
step:1452/1920 train_time:78123ms step_avg:53.80ms
step:1453/1920 train_time:78211ms step_avg:53.83ms
step:1454/1920 train_time:78301ms step_avg:53.85ms
step:1455/1920 train_time:78389ms step_avg:53.88ms
step:1456/1920 train_time:78477ms step_avg:53.90ms
step:1457/1920 train_time:78567ms step_avg:53.92ms
step:1458/1920 train_time:78655ms step_avg:53.95ms
step:1459/1920 train_time:78744ms step_avg:53.97ms
step:1460/1920 train_time:78832ms step_avg:53.99ms
step:1461/1920 train_time:78921ms step_avg:54.02ms
step:1462/1920 train_time:79009ms step_avg:54.04ms
step:1463/1920 train_time:79097ms step_avg:54.07ms
step:1464/1920 train_time:79186ms step_avg:54.09ms
step:1465/1920 train_time:79275ms step_avg:54.11ms
step:1466/1920 train_time:79364ms step_avg:54.14ms
step:1467/1920 train_time:79452ms step_avg:54.16ms
step:1468/1920 train_time:79540ms step_avg:54.18ms
step:1469/1920 train_time:79628ms step_avg:54.21ms
step:1470/1920 train_time:79717ms step_avg:54.23ms
step:1471/1920 train_time:79806ms step_avg:54.25ms
step:1472/1920 train_time:79893ms step_avg:54.28ms
step:1473/1920 train_time:79982ms step_avg:54.30ms
step:1474/1920 train_time:80069ms step_avg:54.32ms
step:1475/1920 train_time:80158ms step_avg:54.34ms
step:1476/1920 train_time:80247ms step_avg:54.37ms
step:1477/1920 train_time:80336ms step_avg:54.39ms
step:1478/1920 train_time:80425ms step_avg:54.41ms
step:1479/1920 train_time:80514ms step_avg:54.44ms
step:1480/1920 train_time:80602ms step_avg:54.46ms
step:1481/1920 train_time:80690ms step_avg:54.48ms
step:1482/1920 train_time:80779ms step_avg:54.51ms
step:1483/1920 train_time:80868ms step_avg:54.53ms
step:1484/1920 train_time:80955ms step_avg:54.55ms
step:1485/1920 train_time:81044ms step_avg:54.57ms
step:1486/1920 train_time:81131ms step_avg:54.60ms
step:1487/1920 train_time:81219ms step_avg:54.62ms
step:1488/1920 train_time:81307ms step_avg:54.64ms
step:1489/1920 train_time:81396ms step_avg:54.66ms
step:1490/1920 train_time:81484ms step_avg:54.69ms
step:1491/1920 train_time:81572ms step_avg:54.71ms
step:1492/1920 train_time:81659ms step_avg:54.73ms
step:1493/1920 train_time:81748ms step_avg:54.75ms
step:1494/1920 train_time:81837ms step_avg:54.78ms
step:1495/1920 train_time:81927ms step_avg:54.80ms
step:1496/1920 train_time:82015ms step_avg:54.82ms
step:1497/1920 train_time:82105ms step_avg:54.85ms
step:1498/1920 train_time:82193ms step_avg:54.87ms
step:1499/1920 train_time:82282ms step_avg:54.89ms
step:1500/1920 train_time:82369ms step_avg:54.91ms
step:1500/1920 val_loss:3.4122 train_time:82460ms step_avg:54.97ms
step:1501/1920 train_time:82478ms step_avg:54.95ms
step:1502/1920 train_time:82549ms step_avg:54.96ms
step:1503/1920 train_time:82641ms step_avg:54.98ms
step:1504/1920 train_time:82729ms step_avg:55.01ms
step:1505/1920 train_time:82818ms step_avg:55.03ms
step:1506/1920 train_time:82905ms step_avg:55.05ms
step:1507/1920 train_time:82992ms step_avg:55.07ms
step:1508/1920 train_time:83079ms step_avg:55.09ms
step:1509/1920 train_time:83167ms step_avg:55.11ms
step:1510/1920 train_time:83255ms step_avg:55.14ms
step:1511/1920 train_time:83344ms step_avg:55.16ms
step:1512/1920 train_time:83435ms step_avg:55.18ms
step:1513/1920 train_time:83527ms step_avg:55.21ms
step:1514/1920 train_time:83617ms step_avg:55.23ms
step:1515/1920 train_time:83708ms step_avg:55.25ms
step:1516/1920 train_time:83797ms step_avg:55.27ms
step:1517/1920 train_time:83885ms step_avg:55.30ms
step:1518/1920 train_time:83972ms step_avg:55.32ms
step:1519/1920 train_time:84060ms step_avg:55.34ms
step:1520/1920 train_time:84147ms step_avg:55.36ms
step:1521/1920 train_time:84235ms step_avg:55.38ms
step:1522/1920 train_time:84323ms step_avg:55.40ms
step:1523/1920 train_time:84412ms step_avg:55.42ms
step:1524/1920 train_time:84501ms step_avg:55.45ms
step:1525/1920 train_time:84591ms step_avg:55.47ms
step:1526/1920 train_time:84680ms step_avg:55.49ms
step:1527/1920 train_time:84770ms step_avg:55.51ms
step:1528/1920 train_time:84858ms step_avg:55.54ms
step:1529/1920 train_time:84946ms step_avg:55.56ms
step:1530/1920 train_time:85033ms step_avg:55.58ms
step:1531/1920 train_time:85122ms step_avg:55.60ms
step:1532/1920 train_time:85208ms step_avg:55.62ms
step:1533/1920 train_time:85297ms step_avg:55.64ms
step:1534/1920 train_time:85386ms step_avg:55.66ms
step:1535/1920 train_time:85475ms step_avg:55.68ms
step:1536/1920 train_time:85564ms step_avg:55.71ms
step:1537/1920 train_time:85653ms step_avg:55.73ms
step:1538/1920 train_time:85742ms step_avg:55.75ms
step:1539/1920 train_time:85830ms step_avg:55.77ms
step:1540/1920 train_time:85919ms step_avg:55.79ms
step:1541/1920 train_time:86008ms step_avg:55.81ms
step:1542/1920 train_time:86096ms step_avg:55.83ms
step:1543/1920 train_time:86186ms step_avg:55.86ms
step:1544/1920 train_time:86272ms step_avg:55.88ms
step:1545/1920 train_time:86361ms step_avg:55.90ms
step:1546/1920 train_time:86449ms step_avg:55.92ms
step:1547/1920 train_time:86538ms step_avg:55.94ms
step:1548/1920 train_time:86627ms step_avg:55.96ms
step:1549/1920 train_time:86717ms step_avg:55.98ms
step:1550/1920 train_time:86805ms step_avg:56.00ms
step:1551/1920 train_time:86894ms step_avg:56.02ms
step:1552/1920 train_time:86983ms step_avg:56.05ms
step:1553/1920 train_time:87072ms step_avg:56.07ms
step:1554/1920 train_time:87160ms step_avg:56.09ms
step:1555/1920 train_time:87248ms step_avg:56.11ms
step:1556/1920 train_time:87335ms step_avg:56.13ms
step:1557/1920 train_time:87425ms step_avg:56.15ms
step:1558/1920 train_time:87513ms step_avg:56.17ms
step:1559/1920 train_time:87604ms step_avg:56.19ms
step:1560/1920 train_time:87693ms step_avg:56.21ms
step:1561/1920 train_time:87781ms step_avg:56.23ms
step:1562/1920 train_time:87869ms step_avg:56.25ms
step:1563/1920 train_time:87958ms step_avg:56.28ms
step:1564/1920 train_time:88045ms step_avg:56.29ms
step:1565/1920 train_time:88134ms step_avg:56.32ms
step:1566/1920 train_time:88221ms step_avg:56.34ms
step:1567/1920 train_time:88310ms step_avg:56.36ms
step:1568/1920 train_time:88398ms step_avg:56.38ms
step:1569/1920 train_time:88487ms step_avg:56.40ms
step:1570/1920 train_time:88576ms step_avg:56.42ms
step:1571/1920 train_time:88666ms step_avg:56.44ms
step:1572/1920 train_time:88754ms step_avg:56.46ms
step:1573/1920 train_time:88843ms step_avg:56.48ms
step:1574/1920 train_time:88930ms step_avg:56.50ms
step:1575/1920 train_time:89018ms step_avg:56.52ms
step:1576/1920 train_time:89106ms step_avg:56.54ms
step:1577/1920 train_time:89195ms step_avg:56.56ms
step:1578/1920 train_time:89282ms step_avg:56.58ms
step:1579/1920 train_time:89371ms step_avg:56.60ms
step:1580/1920 train_time:89460ms step_avg:56.62ms
step:1581/1920 train_time:89548ms step_avg:56.64ms
step:1582/1920 train_time:89638ms step_avg:56.66ms
step:1583/1920 train_time:89728ms step_avg:56.68ms
step:1584/1920 train_time:89815ms step_avg:56.70ms
step:1585/1920 train_time:89904ms step_avg:56.72ms
step:1586/1920 train_time:89991ms step_avg:56.74ms
step:1587/1920 train_time:90080ms step_avg:56.76ms
step:1588/1920 train_time:90169ms step_avg:56.78ms
step:1589/1920 train_time:90257ms step_avg:56.80ms
step:1590/1920 train_time:90345ms step_avg:56.82ms
step:1591/1920 train_time:90433ms step_avg:56.84ms
step:1592/1920 train_time:90523ms step_avg:56.86ms
step:1593/1920 train_time:90611ms step_avg:56.88ms
step:1594/1920 train_time:90701ms step_avg:56.90ms
step:1595/1920 train_time:90789ms step_avg:56.92ms
step:1596/1920 train_time:90877ms step_avg:56.94ms
step:1597/1920 train_time:90966ms step_avg:56.96ms
step:1598/1920 train_time:91055ms step_avg:56.98ms
step:1599/1920 train_time:91145ms step_avg:57.00ms
step:1600/1920 train_time:91233ms step_avg:57.02ms
step:1601/1920 train_time:91322ms step_avg:57.04ms
step:1602/1920 train_time:91410ms step_avg:57.06ms
step:1603/1920 train_time:91498ms step_avg:57.08ms
step:1604/1920 train_time:91586ms step_avg:57.10ms
step:1605/1920 train_time:91676ms step_avg:57.12ms
step:1606/1920 train_time:91764ms step_avg:57.14ms
step:1607/1920 train_time:91853ms step_avg:57.16ms
step:1608/1920 train_time:91941ms step_avg:57.18ms
step:1609/1920 train_time:92029ms step_avg:57.20ms
step:1610/1920 train_time:92117ms step_avg:57.22ms
step:1611/1920 train_time:92205ms step_avg:57.23ms
step:1612/1920 train_time:92293ms step_avg:57.25ms
step:1613/1920 train_time:92382ms step_avg:57.27ms
step:1614/1920 train_time:92469ms step_avg:57.29ms
step:1615/1920 train_time:92558ms step_avg:57.31ms
step:1616/1920 train_time:92648ms step_avg:57.33ms
step:1617/1920 train_time:92737ms step_avg:57.35ms
step:1618/1920 train_time:92824ms step_avg:57.37ms
step:1619/1920 train_time:92913ms step_avg:57.39ms
step:1620/1920 train_time:93001ms step_avg:57.41ms
step:1621/1920 train_time:93089ms step_avg:57.43ms
step:1622/1920 train_time:93177ms step_avg:57.45ms
step:1623/1920 train_time:93268ms step_avg:57.47ms
step:1624/1920 train_time:93357ms step_avg:57.49ms
step:1625/1920 train_time:93447ms step_avg:57.51ms
step:1626/1920 train_time:93535ms step_avg:57.52ms
step:1627/1920 train_time:93625ms step_avg:57.54ms
step:1628/1920 train_time:93713ms step_avg:57.56ms
step:1629/1920 train_time:93801ms step_avg:57.58ms
step:1630/1920 train_time:93889ms step_avg:57.60ms
step:1631/1920 train_time:93978ms step_avg:57.62ms
step:1632/1920 train_time:94067ms step_avg:57.64ms
step:1633/1920 train_time:94155ms step_avg:57.66ms
step:1634/1920 train_time:94244ms step_avg:57.68ms
step:1635/1920 train_time:94332ms step_avg:57.70ms
step:1636/1920 train_time:94421ms step_avg:57.71ms
step:1637/1920 train_time:94510ms step_avg:57.73ms
step:1638/1920 train_time:94600ms step_avg:57.75ms
step:1639/1920 train_time:94690ms step_avg:57.77ms
step:1640/1920 train_time:94777ms step_avg:57.79ms
step:1641/1920 train_time:94867ms step_avg:57.81ms
step:1642/1920 train_time:94955ms step_avg:57.83ms
step:1643/1920 train_time:95045ms step_avg:57.85ms
step:1644/1920 train_time:95133ms step_avg:57.87ms
step:1645/1920 train_time:95222ms step_avg:57.89ms
step:1646/1920 train_time:95309ms step_avg:57.90ms
step:1647/1920 train_time:95398ms step_avg:57.92ms
step:1648/1920 train_time:95487ms step_avg:57.94ms
step:1649/1920 train_time:95576ms step_avg:57.96ms
step:1650/1920 train_time:95664ms step_avg:57.98ms
step:1651/1920 train_time:95752ms step_avg:58.00ms
step:1652/1920 train_time:95839ms step_avg:58.01ms
step:1653/1920 train_time:95929ms step_avg:58.03ms
step:1654/1920 train_time:96019ms step_avg:58.05ms
step:1655/1920 train_time:96107ms step_avg:58.07ms
step:1656/1920 train_time:96196ms step_avg:58.09ms
step:1657/1920 train_time:96284ms step_avg:58.11ms
step:1658/1920 train_time:96372ms step_avg:58.13ms
step:1659/1920 train_time:96460ms step_avg:58.14ms
step:1660/1920 train_time:96548ms step_avg:58.16ms
step:1661/1920 train_time:96637ms step_avg:58.18ms
step:1662/1920 train_time:96725ms step_avg:58.20ms
step:1663/1920 train_time:96813ms step_avg:58.22ms
step:1664/1920 train_time:96902ms step_avg:58.23ms
step:1665/1920 train_time:96990ms step_avg:58.25ms
step:1666/1920 train_time:97079ms step_avg:58.27ms
step:1667/1920 train_time:97168ms step_avg:58.29ms
step:1668/1920 train_time:97257ms step_avg:58.31ms
step:1669/1920 train_time:97347ms step_avg:58.33ms
step:1670/1920 train_time:97436ms step_avg:58.35ms
step:1671/1920 train_time:97526ms step_avg:58.36ms
step:1672/1920 train_time:97614ms step_avg:58.38ms
step:1673/1920 train_time:97703ms step_avg:58.40ms
step:1674/1920 train_time:97790ms step_avg:58.42ms
step:1675/1920 train_time:97879ms step_avg:58.44ms
step:1676/1920 train_time:97967ms step_avg:58.45ms
step:1677/1920 train_time:98056ms step_avg:58.47ms
step:1678/1920 train_time:98144ms step_avg:58.49ms
step:1679/1920 train_time:98232ms step_avg:58.51ms
step:1680/1920 train_time:98320ms step_avg:58.52ms
step:1681/1920 train_time:98409ms step_avg:58.54ms
step:1682/1920 train_time:98498ms step_avg:58.56ms
step:1683/1920 train_time:98588ms step_avg:58.58ms
step:1684/1920 train_time:98676ms step_avg:58.60ms
step:1685/1920 train_time:98766ms step_avg:58.61ms
step:1686/1920 train_time:98854ms step_avg:58.63ms
step:1687/1920 train_time:98943ms step_avg:58.65ms
step:1688/1920 train_time:99030ms step_avg:58.67ms
step:1689/1920 train_time:99119ms step_avg:58.69ms
step:1690/1920 train_time:99208ms step_avg:58.70ms
step:1691/1920 train_time:99296ms step_avg:58.72ms
step:1692/1920 train_time:99385ms step_avg:58.74ms
step:1693/1920 train_time:99474ms step_avg:58.76ms
step:1694/1920 train_time:99562ms step_avg:58.77ms
step:1695/1920 train_time:99650ms step_avg:58.79ms
step:1696/1920 train_time:99738ms step_avg:58.81ms
step:1697/1920 train_time:99827ms step_avg:58.83ms
step:1698/1920 train_time:99915ms step_avg:58.84ms
step:1699/1920 train_time:100004ms step_avg:58.86ms
step:1700/1920 train_time:100091ms step_avg:58.88ms
step:1701/1920 train_time:100180ms step_avg:58.89ms
step:1702/1920 train_time:100267ms step_avg:58.91ms
step:1703/1920 train_time:100356ms step_avg:58.93ms
step:1704/1920 train_time:100444ms step_avg:58.95ms
step:1705/1920 train_time:100532ms step_avg:58.96ms
step:1706/1920 train_time:100620ms step_avg:58.98ms
step:1707/1920 train_time:100710ms step_avg:59.00ms
step:1708/1920 train_time:100798ms step_avg:59.02ms
step:1709/1920 train_time:100888ms step_avg:59.03ms
step:1710/1920 train_time:100977ms step_avg:59.05ms
step:1711/1920 train_time:101066ms step_avg:59.07ms
step:1712/1920 train_time:101155ms step_avg:59.09ms
step:1713/1920 train_time:101244ms step_avg:59.10ms
step:1714/1920 train_time:101331ms step_avg:59.12ms
step:1715/1920 train_time:101420ms step_avg:59.14ms
step:1716/1920 train_time:101508ms step_avg:59.15ms
step:1717/1920 train_time:101597ms step_avg:59.17ms
step:1718/1920 train_time:101684ms step_avg:59.19ms
step:1719/1920 train_time:101773ms step_avg:59.20ms
step:1720/1920 train_time:101862ms step_avg:59.22ms
step:1721/1920 train_time:101951ms step_avg:59.24ms
step:1722/1920 train_time:102039ms step_avg:59.26ms
step:1723/1920 train_time:102127ms step_avg:59.27ms
step:1724/1920 train_time:102215ms step_avg:59.29ms
step:1725/1920 train_time:102305ms step_avg:59.31ms
step:1726/1920 train_time:102392ms step_avg:59.32ms
step:1727/1920 train_time:102481ms step_avg:59.34ms
step:1728/1920 train_time:102568ms step_avg:59.36ms
step:1729/1920 train_time:102658ms step_avg:59.37ms
step:1730/1920 train_time:102746ms step_avg:59.39ms
step:1731/1920 train_time:102835ms step_avg:59.41ms
step:1732/1920 train_time:102923ms step_avg:59.42ms
step:1733/1920 train_time:103011ms step_avg:59.44ms
step:1734/1920 train_time:103099ms step_avg:59.46ms
step:1735/1920 train_time:103188ms step_avg:59.47ms
step:1736/1920 train_time:103277ms step_avg:59.49ms
step:1737/1920 train_time:103367ms step_avg:59.51ms
step:1738/1920 train_time:103454ms step_avg:59.52ms
step:1739/1920 train_time:103543ms step_avg:59.54ms
step:1740/1920 train_time:103630ms step_avg:59.56ms
step:1741/1920 train_time:103719ms step_avg:59.57ms
step:1742/1920 train_time:103808ms step_avg:59.59ms
step:1743/1920 train_time:103897ms step_avg:59.61ms
step:1744/1920 train_time:103986ms step_avg:59.62ms
step:1745/1920 train_time:104074ms step_avg:59.64ms
step:1746/1920 train_time:104163ms step_avg:59.66ms
step:1747/1920 train_time:104251ms step_avg:59.67ms
step:1748/1920 train_time:104340ms step_avg:59.69ms
step:1749/1920 train_time:104430ms step_avg:59.71ms
step:1750/1920 train_time:104519ms step_avg:59.73ms
step:1750/1920 val_loss:3.3213 train_time:104610ms step_avg:59.78ms
step:1751/1920 train_time:104628ms step_avg:59.75ms
step:1752/1920 train_time:104700ms step_avg:59.76ms
step:1753/1920 train_time:104790ms step_avg:59.78ms
step:1754/1920 train_time:104879ms step_avg:59.79ms
step:1755/1920 train_time:104967ms step_avg:59.81ms
step:1756/1920 train_time:105055ms step_avg:59.83ms
step:1757/1920 train_time:105143ms step_avg:59.84ms
step:1758/1920 train_time:105230ms step_avg:59.86ms
step:1759/1920 train_time:105319ms step_avg:59.87ms
step:1760/1920 train_time:105406ms step_avg:59.89ms
step:1761/1920 train_time:105495ms step_avg:59.91ms
step:1762/1920 train_time:105585ms step_avg:59.92ms
step:1763/1920 train_time:105677ms step_avg:59.94ms
step:1764/1920 train_time:105765ms step_avg:59.96ms
step:1765/1920 train_time:105855ms step_avg:59.97ms
step:1766/1920 train_time:105943ms step_avg:59.99ms
step:1767/1920 train_time:106031ms step_avg:60.01ms
step:1768/1920 train_time:106119ms step_avg:60.02ms
step:1769/1920 train_time:106206ms step_avg:60.04ms
step:1770/1920 train_time:106294ms step_avg:60.05ms
step:1771/1920 train_time:106382ms step_avg:60.07ms
step:1772/1920 train_time:106471ms step_avg:60.09ms
step:1773/1920 train_time:106560ms step_avg:60.10ms
step:1774/1920 train_time:106650ms step_avg:60.12ms
step:1775/1920 train_time:106739ms step_avg:60.13ms
step:1776/1920 train_time:106827ms step_avg:60.15ms
step:1777/1920 train_time:106917ms step_avg:60.17ms
step:1778/1920 train_time:107004ms step_avg:60.18ms
step:1779/1920 train_time:107093ms step_avg:60.20ms
step:1780/1920 train_time:107180ms step_avg:60.21ms
step:1781/1920 train_time:107268ms step_avg:60.23ms
step:1782/1920 train_time:107355ms step_avg:60.24ms
step:1783/1920 train_time:107445ms step_avg:60.26ms
step:1784/1920 train_time:107533ms step_avg:60.28ms
step:1785/1920 train_time:107624ms step_avg:60.29ms
step:1786/1920 train_time:107713ms step_avg:60.31ms
step:1787/1920 train_time:107803ms step_avg:60.33ms
step:1788/1920 train_time:107893ms step_avg:60.34ms
step:1789/1920 train_time:107982ms step_avg:60.36ms
step:1790/1920 train_time:108069ms step_avg:60.37ms
step:1791/1920 train_time:108157ms step_avg:60.39ms
step:1792/1920 train_time:108244ms step_avg:60.40ms
step:1793/1920 train_time:108332ms step_avg:60.42ms
step:1794/1920 train_time:108420ms step_avg:60.43ms
step:1795/1920 train_time:108509ms step_avg:60.45ms
step:1796/1920 train_time:108597ms step_avg:60.47ms
step:1797/1920 train_time:108687ms step_avg:60.48ms
step:1798/1920 train_time:108776ms step_avg:60.50ms
step:1799/1920 train_time:108866ms step_avg:60.51ms
step:1800/1920 train_time:108954ms step_avg:60.53ms
step:1801/1920 train_time:109044ms step_avg:60.55ms
step:1802/1920 train_time:109133ms step_avg:60.56ms
step:1803/1920 train_time:109221ms step_avg:60.58ms
step:1804/1920 train_time:109308ms step_avg:60.59ms
step:1805/1920 train_time:109396ms step_avg:60.61ms
step:1806/1920 train_time:109484ms step_avg:60.62ms
step:1807/1920 train_time:109574ms step_avg:60.64ms
step:1808/1920 train_time:109663ms step_avg:60.65ms
step:1809/1920 train_time:109752ms step_avg:60.67ms
step:1810/1920 train_time:109840ms step_avg:60.68ms
step:1811/1920 train_time:109929ms step_avg:60.70ms
step:1812/1920 train_time:110018ms step_avg:60.72ms
step:1813/1920 train_time:110108ms step_avg:60.73ms
step:1814/1920 train_time:110196ms step_avg:60.75ms
step:1815/1920 train_time:110284ms step_avg:60.76ms
step:1816/1920 train_time:110371ms step_avg:60.78ms
step:1817/1920 train_time:110461ms step_avg:60.79ms
step:1818/1920 train_time:110548ms step_avg:60.81ms
step:1819/1920 train_time:110637ms step_avg:60.82ms
step:1820/1920 train_time:110725ms step_avg:60.84ms
step:1821/1920 train_time:110815ms step_avg:60.85ms
step:1822/1920 train_time:110903ms step_avg:60.87ms
step:1823/1920 train_time:110992ms step_avg:60.88ms
step:1824/1920 train_time:111080ms step_avg:60.90ms
step:1825/1920 train_time:111168ms step_avg:60.91ms
step:1826/1920 train_time:111256ms step_avg:60.93ms
step:1827/1920 train_time:111344ms step_avg:60.94ms
step:1828/1920 train_time:111433ms step_avg:60.96ms
step:1829/1920 train_time:111522ms step_avg:60.97ms
step:1830/1920 train_time:111610ms step_avg:60.99ms
step:1831/1920 train_time:111699ms step_avg:61.00ms
step:1832/1920 train_time:111787ms step_avg:61.02ms
step:1833/1920 train_time:111876ms step_avg:61.03ms
step:1834/1920 train_time:111964ms step_avg:61.05ms
step:1835/1920 train_time:112053ms step_avg:61.06ms
step:1836/1920 train_time:112141ms step_avg:61.08ms
step:1837/1920 train_time:112230ms step_avg:61.09ms
step:1838/1920 train_time:112318ms step_avg:61.11ms
step:1839/1920 train_time:112406ms step_avg:61.12ms
step:1840/1920 train_time:112494ms step_avg:61.14ms
step:1841/1920 train_time:112584ms step_avg:61.15ms
step:1842/1920 train_time:112671ms step_avg:61.17ms
step:1843/1920 train_time:112760ms step_avg:61.18ms
step:1844/1920 train_time:112847ms step_avg:61.20ms
step:1845/1920 train_time:112936ms step_avg:61.21ms
step:1846/1920 train_time:113024ms step_avg:61.23ms
step:1847/1920 train_time:113113ms step_avg:61.24ms
step:1848/1920 train_time:113201ms step_avg:61.26ms
step:1849/1920 train_time:113290ms step_avg:61.27ms
step:1850/1920 train_time:113379ms step_avg:61.29ms
step:1851/1920 train_time:113467ms step_avg:61.30ms
step:1852/1920 train_time:113555ms step_avg:61.31ms
step:1853/1920 train_time:113645ms step_avg:61.33ms
step:1854/1920 train_time:113734ms step_avg:61.35ms
step:1855/1920 train_time:113823ms step_avg:61.36ms
step:1856/1920 train_time:113911ms step_avg:61.37ms
step:1857/1920 train_time:114000ms step_avg:61.39ms
step:1858/1920 train_time:114087ms step_avg:61.40ms
step:1859/1920 train_time:114176ms step_avg:61.42ms
step:1860/1920 train_time:114264ms step_avg:61.43ms
step:1861/1920 train_time:114353ms step_avg:61.45ms
step:1862/1920 train_time:114441ms step_avg:61.46ms
step:1863/1920 train_time:114529ms step_avg:61.48ms
step:1864/1920 train_time:114617ms step_avg:61.49ms
step:1865/1920 train_time:114707ms step_avg:61.51ms
step:1866/1920 train_time:114796ms step_avg:61.52ms
step:1867/1920 train_time:114886ms step_avg:61.54ms
step:1868/1920 train_time:114974ms step_avg:61.55ms
step:1869/1920 train_time:115063ms step_avg:61.56ms
step:1870/1920 train_time:115151ms step_avg:61.58ms
step:1871/1920 train_time:115240ms step_avg:61.59ms
step:1872/1920 train_time:115327ms step_avg:61.61ms
step:1873/1920 train_time:115416ms step_avg:61.62ms
step:1874/1920 train_time:115504ms step_avg:61.64ms
step:1875/1920 train_time:115592ms step_avg:61.65ms
step:1876/1920 train_time:115680ms step_avg:61.66ms
step:1877/1920 train_time:115768ms step_avg:61.68ms
step:1878/1920 train_time:115856ms step_avg:61.69ms
step:1879/1920 train_time:115945ms step_avg:61.71ms
step:1880/1920 train_time:116034ms step_avg:61.72ms
step:1881/1920 train_time:116123ms step_avg:61.73ms
step:1882/1920 train_time:116211ms step_avg:61.75ms
step:1883/1920 train_time:116301ms step_avg:61.76ms
step:1884/1920 train_time:116389ms step_avg:61.78ms
step:1885/1920 train_time:116479ms step_avg:61.79ms
step:1886/1920 train_time:116566ms step_avg:61.81ms
step:1887/1920 train_time:116655ms step_avg:61.82ms
step:1888/1920 train_time:116744ms step_avg:61.83ms
step:1889/1920 train_time:116832ms step_avg:61.85ms
step:1890/1920 train_time:116921ms step_avg:61.86ms
step:1891/1920 train_time:117011ms step_avg:61.88ms
step:1892/1920 train_time:117099ms step_avg:61.89ms
step:1893/1920 train_time:117187ms step_avg:61.91ms
step:1894/1920 train_time:117275ms step_avg:61.92ms
step:1895/1920 train_time:117365ms step_avg:61.93ms
step:1896/1920 train_time:117452ms step_avg:61.95ms
step:1897/1920 train_time:117543ms step_avg:61.96ms
step:1898/1920 train_time:117631ms step_avg:61.98ms
step:1899/1920 train_time:117721ms step_avg:61.99ms
step:1900/1920 train_time:117809ms step_avg:62.00ms
step:1901/1920 train_time:117898ms step_avg:62.02ms
step:1902/1920 train_time:117986ms step_avg:62.03ms
step:1903/1920 train_time:118075ms step_avg:62.05ms
step:1904/1920 train_time:118163ms step_avg:62.06ms
step:1905/1920 train_time:118254ms step_avg:62.08ms
step:1906/1920 train_time:118343ms step_avg:62.09ms
step:1907/1920 train_time:118432ms step_avg:62.10ms
step:1908/1920 train_time:118520ms step_avg:62.12ms
step:1909/1920 train_time:118610ms step_avg:62.13ms
step:1910/1920 train_time:118698ms step_avg:62.15ms
step:1911/1920 train_time:118787ms step_avg:62.16ms
step:1912/1920 train_time:118876ms step_avg:62.17ms
step:1913/1920 train_time:118965ms step_avg:62.19ms
step:1914/1920 train_time:119052ms step_avg:62.20ms
step:1915/1920 train_time:119143ms step_avg:62.22ms
step:1916/1920 train_time:119232ms step_avg:62.23ms
step:1917/1920 train_time:119321ms step_avg:62.24ms
step:1918/1920 train_time:119409ms step_avg:62.26ms
step:1919/1920 train_time:119498ms step_avg:62.27ms
step:1920/1920 train_time:119586ms step_avg:62.28ms
step:1920/1920 val_loss:3.2768 train_time:119677ms step_avg:62.33ms
peak memory allocated: 29863 MiB reserved: 43718 MiB

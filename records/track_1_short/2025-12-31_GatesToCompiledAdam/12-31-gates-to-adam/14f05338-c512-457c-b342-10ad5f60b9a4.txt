import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 17:44:40 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   26C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     79764      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A     79765      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A     79766      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A     79767      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A     79768      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A     79769      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A     79770      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A     79771      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8288 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:73ms step_avg:72.70ms
step:2/1845 train_time:94ms step_avg:47.22ms
step:3/1845 train_time:113ms step_avg:37.56ms
step:4/1845 train_time:147ms step_avg:36.83ms
step:5/1845 train_time:180ms step_avg:36.03ms
step:6/1845 train_time:261ms step_avg:43.51ms
step:7/1845 train_time:277ms step_avg:39.56ms
step:8/1845 train_time:316ms step_avg:39.47ms
step:9/1845 train_time:349ms step_avg:38.75ms
step:10/1845 train_time:384ms step_avg:38.41ms
step:11/1845 train_time:417ms step_avg:37.91ms
step:12/1845 train_time:452ms step_avg:37.70ms
step:13/1845 train_time:486ms step_avg:37.36ms
step:14/1845 train_time:521ms step_avg:37.22ms
step:15/1845 train_time:554ms step_avg:36.93ms
step:16/1845 train_time:589ms step_avg:36.84ms
step:17/1845 train_time:623ms step_avg:36.62ms
step:18/1845 train_time:658ms step_avg:36.55ms
step:19/1845 train_time:691ms step_avg:36.36ms
step:20/1845 train_time:726ms step_avg:36.30ms
step:21/1845 train_time:759ms step_avg:36.15ms
step:22/1845 train_time:795ms step_avg:36.11ms
step:23/1845 train_time:827ms step_avg:35.98ms
step:24/1845 train_time:863ms step_avg:35.96ms
step:25/1845 train_time:896ms step_avg:35.83ms
step:26/1845 train_time:931ms step_avg:35.81ms
step:27/1845 train_time:964ms step_avg:35.70ms
step:28/1845 train_time:999ms step_avg:35.70ms
step:29/1845 train_time:1032ms step_avg:35.60ms
step:30/1845 train_time:1068ms step_avg:35.59ms
step:31/1845 train_time:1101ms step_avg:35.51ms
step:32/1845 train_time:1136ms step_avg:35.51ms
step:33/1845 train_time:1169ms step_avg:35.43ms
step:34/1845 train_time:1205ms step_avg:35.44ms
step:35/1845 train_time:1238ms step_avg:35.37ms
step:36/1845 train_time:1273ms step_avg:35.37ms
step:37/1845 train_time:1306ms step_avg:35.30ms
step:38/1845 train_time:1341ms step_avg:35.30ms
step:39/1845 train_time:1374ms step_avg:35.24ms
step:40/1845 train_time:1410ms step_avg:35.25ms
step:41/1845 train_time:1443ms step_avg:35.20ms
step:42/1845 train_time:1478ms step_avg:35.20ms
step:43/1845 train_time:1511ms step_avg:35.14ms
step:44/1845 train_time:1547ms step_avg:35.15ms
step:45/1845 train_time:1580ms step_avg:35.11ms
step:46/1845 train_time:1615ms step_avg:35.12ms
step:47/1845 train_time:1649ms step_avg:35.08ms
step:48/1845 train_time:1684ms step_avg:35.08ms
step:49/1845 train_time:1717ms step_avg:35.04ms
step:50/1845 train_time:1752ms step_avg:35.04ms
step:51/1845 train_time:1785ms step_avg:35.01ms
step:52/1845 train_time:1821ms step_avg:35.01ms
step:53/1845 train_time:1854ms step_avg:34.97ms
step:54/1845 train_time:1889ms step_avg:34.98ms
step:55/1845 train_time:1922ms step_avg:34.95ms
step:56/1845 train_time:1957ms step_avg:34.95ms
step:57/1845 train_time:1990ms step_avg:34.92ms
step:58/1845 train_time:2026ms step_avg:34.93ms
step:59/1845 train_time:2059ms step_avg:34.90ms
step:60/1845 train_time:2094ms step_avg:34.90ms
step:61/1845 train_time:2127ms step_avg:34.87ms
step:62/1845 train_time:2163ms step_avg:34.88ms
step:63/1845 train_time:2196ms step_avg:34.85ms
step:64/1845 train_time:2231ms step_avg:34.86ms
step:65/1845 train_time:2264ms step_avg:34.83ms
step:66/1845 train_time:2299ms step_avg:34.84ms
step:67/1845 train_time:2332ms step_avg:34.81ms
step:68/1845 train_time:2368ms step_avg:34.82ms
step:69/1845 train_time:2400ms step_avg:34.79ms
step:70/1845 train_time:2436ms step_avg:34.80ms
step:71/1845 train_time:2469ms step_avg:34.77ms
step:72/1845 train_time:2504ms step_avg:34.78ms
step:73/1845 train_time:2537ms step_avg:34.76ms
step:74/1845 train_time:2572ms step_avg:34.76ms
step:75/1845 train_time:2605ms step_avg:34.74ms
step:76/1845 train_time:2641ms step_avg:34.74ms
step:77/1845 train_time:2673ms step_avg:34.72ms
step:78/1845 train_time:2709ms step_avg:34.73ms
step:79/1845 train_time:2742ms step_avg:34.71ms
step:80/1845 train_time:2777ms step_avg:34.71ms
step:81/1845 train_time:2810ms step_avg:34.69ms
step:82/1845 train_time:2846ms step_avg:34.70ms
step:83/1845 train_time:2879ms step_avg:34.68ms
step:84/1845 train_time:2914ms step_avg:34.69ms
step:85/1845 train_time:2947ms step_avg:34.67ms
step:86/1845 train_time:2982ms step_avg:34.68ms
step:87/1845 train_time:3015ms step_avg:34.66ms
step:88/1845 train_time:3050ms step_avg:34.66ms
step:89/1845 train_time:3083ms step_avg:34.64ms
step:90/1845 train_time:3119ms step_avg:34.65ms
step:91/1845 train_time:3152ms step_avg:34.63ms
step:92/1845 train_time:3187ms step_avg:34.64ms
step:93/1845 train_time:3220ms step_avg:34.62ms
step:94/1845 train_time:3255ms step_avg:34.63ms
step:95/1845 train_time:3288ms step_avg:34.61ms
step:96/1845 train_time:3323ms step_avg:34.62ms
step:97/1845 train_time:3356ms step_avg:34.60ms
step:98/1845 train_time:3392ms step_avg:34.61ms
step:99/1845 train_time:3425ms step_avg:34.60ms
step:100/1845 train_time:3460ms step_avg:34.60ms
step:101/1845 train_time:3493ms step_avg:34.59ms
step:102/1845 train_time:3529ms step_avg:34.60ms
step:103/1845 train_time:3562ms step_avg:34.58ms
step:104/1845 train_time:3597ms step_avg:34.59ms
step:105/1845 train_time:3630ms step_avg:34.57ms
step:106/1845 train_time:3665ms step_avg:34.58ms
step:107/1845 train_time:3698ms step_avg:34.56ms
step:108/1845 train_time:3733ms step_avg:34.57ms
step:109/1845 train_time:3767ms step_avg:34.56ms
step:110/1845 train_time:3802ms step_avg:34.57ms
step:111/1845 train_time:3835ms step_avg:34.55ms
step:112/1845 train_time:3871ms step_avg:34.56ms
step:113/1845 train_time:3904ms step_avg:34.55ms
step:114/1845 train_time:3939ms step_avg:34.55ms
step:115/1845 train_time:3972ms step_avg:34.54ms
step:116/1845 train_time:4007ms step_avg:34.55ms
step:117/1845 train_time:4041ms step_avg:34.53ms
step:118/1845 train_time:4076ms step_avg:34.54ms
step:119/1845 train_time:4109ms step_avg:34.53ms
step:120/1845 train_time:4144ms step_avg:34.54ms
step:121/1845 train_time:4177ms step_avg:34.52ms
step:122/1845 train_time:4212ms step_avg:34.53ms
step:123/1845 train_time:4245ms step_avg:34.51ms
step:124/1845 train_time:4280ms step_avg:34.52ms
step:125/1845 train_time:4313ms step_avg:34.51ms
step:126/1845 train_time:4349ms step_avg:34.51ms
step:127/1845 train_time:4382ms step_avg:34.50ms
step:128/1845 train_time:4417ms step_avg:34.51ms
step:129/1845 train_time:4450ms step_avg:34.50ms
step:130/1845 train_time:4485ms step_avg:34.50ms
step:131/1845 train_time:4518ms step_avg:34.49ms
step:132/1845 train_time:4554ms step_avg:34.50ms
step:133/1845 train_time:4586ms step_avg:34.48ms
step:134/1845 train_time:4622ms step_avg:34.49ms
step:135/1845 train_time:4655ms step_avg:34.48ms
step:136/1845 train_time:4690ms step_avg:34.49ms
step:137/1845 train_time:4723ms step_avg:34.47ms
step:138/1845 train_time:4758ms step_avg:34.48ms
step:139/1845 train_time:4791ms step_avg:34.47ms
step:140/1845 train_time:4826ms step_avg:34.47ms
step:141/1845 train_time:4859ms step_avg:34.46ms
step:142/1845 train_time:4895ms step_avg:34.47ms
step:143/1845 train_time:4927ms step_avg:34.46ms
step:144/1845 train_time:4963ms step_avg:34.47ms
step:145/1845 train_time:4996ms step_avg:34.46ms
step:146/1845 train_time:5031ms step_avg:34.46ms
step:147/1845 train_time:5064ms step_avg:34.45ms
step:148/1845 train_time:5100ms step_avg:34.46ms
step:149/1845 train_time:5132ms step_avg:34.45ms
step:150/1845 train_time:5168ms step_avg:34.45ms
step:151/1845 train_time:5201ms step_avg:34.44ms
step:152/1845 train_time:5236ms step_avg:34.45ms
step:153/1845 train_time:5269ms step_avg:34.44ms
step:154/1845 train_time:5304ms step_avg:34.44ms
step:155/1845 train_time:5337ms step_avg:34.43ms
step:156/1845 train_time:5372ms step_avg:34.44ms
step:157/1845 train_time:5405ms step_avg:34.43ms
step:158/1845 train_time:5440ms step_avg:34.43ms
step:159/1845 train_time:5473ms step_avg:34.42ms
step:160/1845 train_time:5508ms step_avg:34.43ms
step:161/1845 train_time:5541ms step_avg:34.42ms
step:162/1845 train_time:5576ms step_avg:34.42ms
step:163/1845 train_time:5609ms step_avg:34.41ms
step:164/1845 train_time:5644ms step_avg:34.42ms
step:165/1845 train_time:5677ms step_avg:34.41ms
step:166/1845 train_time:5713ms step_avg:34.41ms
step:167/1845 train_time:5746ms step_avg:34.40ms
step:168/1845 train_time:5781ms step_avg:34.41ms
step:169/1845 train_time:5814ms step_avg:34.40ms
step:170/1845 train_time:5849ms step_avg:34.41ms
step:171/1845 train_time:5882ms step_avg:34.40ms
step:172/1845 train_time:5917ms step_avg:34.40ms
step:173/1845 train_time:5950ms step_avg:34.39ms
step:174/1845 train_time:5985ms step_avg:34.40ms
step:175/1845 train_time:6018ms step_avg:34.39ms
step:176/1845 train_time:6054ms step_avg:34.40ms
step:177/1845 train_time:6087ms step_avg:34.39ms
step:178/1845 train_time:6122ms step_avg:34.39ms
step:179/1845 train_time:6155ms step_avg:34.38ms
step:180/1845 train_time:6190ms step_avg:34.39ms
step:181/1845 train_time:6223ms step_avg:34.38ms
step:182/1845 train_time:6258ms step_avg:34.39ms
step:183/1845 train_time:6291ms step_avg:34.38ms
step:184/1845 train_time:6327ms step_avg:34.38ms
step:185/1845 train_time:6359ms step_avg:34.37ms
step:186/1845 train_time:6395ms step_avg:34.38ms
step:187/1845 train_time:6427ms step_avg:34.37ms
step:188/1845 train_time:6463ms step_avg:34.38ms
step:189/1845 train_time:6496ms step_avg:34.37ms
step:190/1845 train_time:6531ms step_avg:34.37ms
step:191/1845 train_time:6564ms step_avg:34.37ms
step:192/1845 train_time:6599ms step_avg:34.37ms
step:193/1845 train_time:6632ms step_avg:34.36ms
step:194/1845 train_time:6667ms step_avg:34.37ms
step:195/1845 train_time:6700ms step_avg:34.36ms
step:196/1845 train_time:6735ms step_avg:34.36ms
step:197/1845 train_time:6768ms step_avg:34.36ms
step:198/1845 train_time:6803ms step_avg:34.36ms
step:199/1845 train_time:6836ms step_avg:34.35ms
step:200/1845 train_time:6872ms step_avg:34.36ms
step:201/1845 train_time:6905ms step_avg:34.35ms
step:202/1845 train_time:6940ms step_avg:34.36ms
step:203/1845 train_time:6973ms step_avg:34.35ms
step:204/1845 train_time:7008ms step_avg:34.35ms
step:205/1845 train_time:7041ms step_avg:34.35ms
step:206/1845 train_time:7076ms step_avg:34.35ms
step:207/1845 train_time:7109ms step_avg:34.34ms
step:208/1845 train_time:7144ms step_avg:34.35ms
step:209/1845 train_time:7177ms step_avg:34.34ms
step:210/1845 train_time:7212ms step_avg:34.34ms
step:211/1845 train_time:7245ms step_avg:34.34ms
step:212/1845 train_time:7281ms step_avg:34.34ms
step:213/1845 train_time:7313ms step_avg:34.34ms
step:214/1845 train_time:7349ms step_avg:34.34ms
step:215/1845 train_time:7382ms step_avg:34.33ms
step:216/1845 train_time:7417ms step_avg:34.34ms
step:217/1845 train_time:7450ms step_avg:34.33ms
step:218/1845 train_time:7485ms step_avg:34.34ms
step:219/1845 train_time:7518ms step_avg:34.33ms
step:220/1845 train_time:7553ms step_avg:34.33ms
step:221/1845 train_time:7586ms step_avg:34.33ms
step:222/1845 train_time:7621ms step_avg:34.33ms
step:223/1845 train_time:7654ms step_avg:34.32ms
step:224/1845 train_time:7690ms step_avg:34.33ms
step:225/1845 train_time:7723ms step_avg:34.32ms
step:226/1845 train_time:7758ms step_avg:34.33ms
step:227/1845 train_time:7791ms step_avg:34.32ms
step:228/1845 train_time:7826ms step_avg:34.32ms
step:229/1845 train_time:7859ms step_avg:34.32ms
step:230/1845 train_time:7894ms step_avg:34.32ms
step:231/1845 train_time:7926ms step_avg:34.31ms
step:232/1845 train_time:7962ms step_avg:34.32ms
step:233/1845 train_time:7995ms step_avg:34.31ms
step:234/1845 train_time:8030ms step_avg:34.32ms
step:235/1845 train_time:8063ms step_avg:34.31ms
step:236/1845 train_time:8098ms step_avg:34.31ms
step:237/1845 train_time:8131ms step_avg:34.31ms
step:238/1845 train_time:8166ms step_avg:34.31ms
step:239/1845 train_time:8199ms step_avg:34.31ms
step:240/1845 train_time:8234ms step_avg:34.31ms
step:241/1845 train_time:8267ms step_avg:34.30ms
step:242/1845 train_time:8302ms step_avg:34.31ms
step:243/1845 train_time:8335ms step_avg:34.30ms
step:244/1845 train_time:8370ms step_avg:34.31ms
step:245/1845 train_time:8404ms step_avg:34.30ms
step:246/1845 train_time:8439ms step_avg:34.30ms
step:247/1845 train_time:8472ms step_avg:34.30ms
step:248/1845 train_time:8507ms step_avg:34.30ms
step:249/1845 train_time:8540ms step_avg:34.30ms
step:250/1845 train_time:8575ms step_avg:34.30ms
step:250/1845 val_loss:4.6589 train_time:8617ms step_avg:34.47ms
step:251/1845 train_time:8634ms step_avg:34.40ms
step:252/1845 train_time:8651ms step_avg:34.33ms
step:253/1845 train_time:8678ms step_avg:34.30ms
step:254/1845 train_time:8714ms step_avg:34.31ms
step:255/1845 train_time:8747ms step_avg:34.30ms
step:256/1845 train_time:8784ms step_avg:34.31ms
step:257/1845 train_time:8817ms step_avg:34.31ms
step:258/1845 train_time:8852ms step_avg:34.31ms
step:259/1845 train_time:8885ms step_avg:34.31ms
step:260/1845 train_time:8921ms step_avg:34.31ms
step:261/1845 train_time:8954ms step_avg:34.31ms
step:262/1845 train_time:8989ms step_avg:34.31ms
step:263/1845 train_time:9022ms step_avg:34.30ms
step:264/1845 train_time:9057ms step_avg:34.31ms
step:265/1845 train_time:9090ms step_avg:34.30ms
step:266/1845 train_time:9125ms step_avg:34.31ms
step:267/1845 train_time:9158ms step_avg:34.30ms
step:268/1845 train_time:9193ms step_avg:34.30ms
step:269/1845 train_time:9226ms step_avg:34.30ms
step:270/1845 train_time:9262ms step_avg:34.30ms
step:271/1845 train_time:9294ms step_avg:34.30ms
step:272/1845 train_time:9330ms step_avg:34.30ms
step:273/1845 train_time:9362ms step_avg:34.29ms
step:274/1845 train_time:9398ms step_avg:34.30ms
step:275/1845 train_time:9431ms step_avg:34.29ms
step:276/1845 train_time:9466ms step_avg:34.30ms
step:277/1845 train_time:9499ms step_avg:34.29ms
step:278/1845 train_time:9534ms step_avg:34.29ms
step:279/1845 train_time:9567ms step_avg:34.29ms
step:280/1845 train_time:9602ms step_avg:34.29ms
step:281/1845 train_time:9635ms step_avg:34.29ms
step:282/1845 train_time:9670ms step_avg:34.29ms
step:283/1845 train_time:9703ms step_avg:34.29ms
step:284/1845 train_time:9738ms step_avg:34.29ms
step:285/1845 train_time:9771ms step_avg:34.28ms
step:286/1845 train_time:9806ms step_avg:34.29ms
step:287/1845 train_time:9839ms step_avg:34.28ms
step:288/1845 train_time:9875ms step_avg:34.29ms
step:289/1845 train_time:9907ms step_avg:34.28ms
step:290/1845 train_time:9943ms step_avg:34.29ms
step:291/1845 train_time:9976ms step_avg:34.28ms
step:292/1845 train_time:10011ms step_avg:34.28ms
step:293/1845 train_time:10044ms step_avg:34.28ms
step:294/1845 train_time:10079ms step_avg:34.28ms
step:295/1845 train_time:10112ms step_avg:34.28ms
step:296/1845 train_time:10147ms step_avg:34.28ms
step:297/1845 train_time:10180ms step_avg:34.28ms
step:298/1845 train_time:10215ms step_avg:34.28ms
step:299/1845 train_time:10248ms step_avg:34.27ms
step:300/1845 train_time:10283ms step_avg:34.28ms
step:301/1845 train_time:10316ms step_avg:34.27ms
step:302/1845 train_time:10351ms step_avg:34.28ms
step:303/1845 train_time:10384ms step_avg:34.27ms
step:304/1845 train_time:10419ms step_avg:34.27ms
step:305/1845 train_time:10452ms step_avg:34.27ms
step:306/1845 train_time:10487ms step_avg:34.27ms
step:307/1845 train_time:10520ms step_avg:34.27ms
step:308/1845 train_time:10555ms step_avg:34.27ms
step:309/1845 train_time:10588ms step_avg:34.27ms
step:310/1845 train_time:10623ms step_avg:34.27ms
step:311/1845 train_time:10656ms step_avg:34.26ms
step:312/1845 train_time:10691ms step_avg:34.27ms
step:313/1845 train_time:10724ms step_avg:34.26ms
step:314/1845 train_time:10759ms step_avg:34.26ms
step:315/1845 train_time:10792ms step_avg:34.26ms
step:316/1845 train_time:10827ms step_avg:34.26ms
step:317/1845 train_time:10860ms step_avg:34.26ms
step:318/1845 train_time:10895ms step_avg:34.26ms
step:319/1845 train_time:10928ms step_avg:34.26ms
step:320/1845 train_time:10964ms step_avg:34.26ms
step:321/1845 train_time:10996ms step_avg:34.26ms
step:322/1845 train_time:11032ms step_avg:34.26ms
step:323/1845 train_time:11064ms step_avg:34.26ms
step:324/1845 train_time:11100ms step_avg:34.26ms
step:325/1845 train_time:11133ms step_avg:34.25ms
step:326/1845 train_time:11168ms step_avg:34.26ms
step:327/1845 train_time:11201ms step_avg:34.25ms
step:328/1845 train_time:11236ms step_avg:34.26ms
step:329/1845 train_time:11269ms step_avg:34.25ms
step:330/1845 train_time:11304ms step_avg:34.25ms
step:331/1845 train_time:11337ms step_avg:34.25ms
step:332/1845 train_time:11372ms step_avg:34.25ms
step:333/1845 train_time:11405ms step_avg:34.25ms
step:334/1845 train_time:11440ms step_avg:34.25ms
step:335/1845 train_time:11473ms step_avg:34.25ms
step:336/1845 train_time:11508ms step_avg:34.25ms
step:337/1845 train_time:11541ms step_avg:34.25ms
step:338/1845 train_time:11576ms step_avg:34.25ms
step:339/1845 train_time:11609ms step_avg:34.25ms
step:340/1845 train_time:11644ms step_avg:34.25ms
step:341/1845 train_time:11677ms step_avg:34.24ms
step:342/1845 train_time:11712ms step_avg:34.25ms
step:343/1845 train_time:11745ms step_avg:34.24ms
step:344/1845 train_time:11780ms step_avg:34.25ms
step:345/1845 train_time:11813ms step_avg:34.24ms
step:346/1845 train_time:11848ms step_avg:34.24ms
step:347/1845 train_time:11881ms step_avg:34.24ms
step:348/1845 train_time:11916ms step_avg:34.24ms
step:349/1845 train_time:11949ms step_avg:34.24ms
step:350/1845 train_time:11985ms step_avg:34.24ms
step:351/1845 train_time:12017ms step_avg:34.24ms
step:352/1845 train_time:12053ms step_avg:34.24ms
step:353/1845 train_time:12085ms step_avg:34.24ms
step:354/1845 train_time:12121ms step_avg:34.24ms
step:355/1845 train_time:12153ms step_avg:34.23ms
step:356/1845 train_time:12188ms step_avg:34.24ms
step:357/1845 train_time:12221ms step_avg:34.23ms
step:358/1845 train_time:12256ms step_avg:34.24ms
step:359/1845 train_time:12289ms step_avg:34.23ms
step:360/1845 train_time:12324ms step_avg:34.23ms
step:361/1845 train_time:12357ms step_avg:34.23ms
step:362/1845 train_time:12392ms step_avg:34.23ms
step:363/1845 train_time:12425ms step_avg:34.23ms
step:364/1845 train_time:12461ms step_avg:34.23ms
step:365/1845 train_time:12493ms step_avg:34.23ms
step:366/1845 train_time:12529ms step_avg:34.23ms
step:367/1845 train_time:12562ms step_avg:34.23ms
step:368/1845 train_time:12597ms step_avg:34.23ms
step:369/1845 train_time:12630ms step_avg:34.23ms
step:370/1845 train_time:12665ms step_avg:34.23ms
step:371/1845 train_time:12698ms step_avg:34.23ms
step:372/1845 train_time:12733ms step_avg:34.23ms
step:373/1845 train_time:12766ms step_avg:34.23ms
step:374/1845 train_time:12801ms step_avg:34.23ms
step:375/1845 train_time:12834ms step_avg:34.22ms
step:376/1845 train_time:12870ms step_avg:34.23ms
step:377/1845 train_time:12902ms step_avg:34.22ms
step:378/1845 train_time:12938ms step_avg:34.23ms
step:379/1845 train_time:12970ms step_avg:34.22ms
step:380/1845 train_time:13006ms step_avg:34.23ms
step:381/1845 train_time:13039ms step_avg:34.22ms
step:382/1845 train_time:13074ms step_avg:34.22ms
step:383/1845 train_time:13106ms step_avg:34.22ms
step:384/1845 train_time:13142ms step_avg:34.22ms
step:385/1845 train_time:13175ms step_avg:34.22ms
step:386/1845 train_time:13210ms step_avg:34.22ms
step:387/1845 train_time:13243ms step_avg:34.22ms
step:388/1845 train_time:13278ms step_avg:34.22ms
step:389/1845 train_time:13311ms step_avg:34.22ms
step:390/1845 train_time:13346ms step_avg:34.22ms
step:391/1845 train_time:13379ms step_avg:34.22ms
step:392/1845 train_time:13414ms step_avg:34.22ms
step:393/1845 train_time:13447ms step_avg:34.22ms
step:394/1845 train_time:13483ms step_avg:34.22ms
step:395/1845 train_time:13515ms step_avg:34.22ms
step:396/1845 train_time:13550ms step_avg:34.22ms
step:397/1845 train_time:13584ms step_avg:34.22ms
step:398/1845 train_time:13619ms step_avg:34.22ms
step:399/1845 train_time:13652ms step_avg:34.21ms
step:400/1845 train_time:13687ms step_avg:34.22ms
step:401/1845 train_time:13720ms step_avg:34.21ms
step:402/1845 train_time:13755ms step_avg:34.22ms
step:403/1845 train_time:13788ms step_avg:34.21ms
step:404/1845 train_time:13823ms step_avg:34.21ms
step:405/1845 train_time:13855ms step_avg:34.21ms
step:406/1845 train_time:13891ms step_avg:34.21ms
step:407/1845 train_time:13923ms step_avg:34.21ms
step:408/1845 train_time:13959ms step_avg:34.21ms
step:409/1845 train_time:13992ms step_avg:34.21ms
step:410/1845 train_time:14027ms step_avg:34.21ms
step:411/1845 train_time:14060ms step_avg:34.21ms
step:412/1845 train_time:14095ms step_avg:34.21ms
step:413/1845 train_time:14128ms step_avg:34.21ms
step:414/1845 train_time:14164ms step_avg:34.21ms
step:415/1845 train_time:14197ms step_avg:34.21ms
step:416/1845 train_time:14232ms step_avg:34.21ms
step:417/1845 train_time:14265ms step_avg:34.21ms
step:418/1845 train_time:14300ms step_avg:34.21ms
step:419/1845 train_time:14333ms step_avg:34.21ms
step:420/1845 train_time:14368ms step_avg:34.21ms
step:421/1845 train_time:14401ms step_avg:34.21ms
step:422/1845 train_time:14436ms step_avg:34.21ms
step:423/1845 train_time:14469ms step_avg:34.20ms
step:424/1845 train_time:14504ms step_avg:34.21ms
step:425/1845 train_time:14537ms step_avg:34.20ms
step:426/1845 train_time:14572ms step_avg:34.21ms
step:427/1845 train_time:14605ms step_avg:34.20ms
step:428/1845 train_time:14640ms step_avg:34.21ms
step:429/1845 train_time:14673ms step_avg:34.20ms
step:430/1845 train_time:14709ms step_avg:34.21ms
step:431/1845 train_time:14742ms step_avg:34.20ms
step:432/1845 train_time:14777ms step_avg:34.21ms
step:433/1845 train_time:14810ms step_avg:34.20ms
step:434/1845 train_time:14845ms step_avg:34.21ms
step:435/1845 train_time:14878ms step_avg:34.20ms
step:436/1845 train_time:14913ms step_avg:34.21ms
step:437/1845 train_time:14946ms step_avg:34.20ms
step:438/1845 train_time:14981ms step_avg:34.20ms
step:439/1845 train_time:15014ms step_avg:34.20ms
step:440/1845 train_time:15049ms step_avg:34.20ms
step:441/1845 train_time:15082ms step_avg:34.20ms
step:442/1845 train_time:15118ms step_avg:34.20ms
step:443/1845 train_time:15150ms step_avg:34.20ms
step:444/1845 train_time:15186ms step_avg:34.20ms
step:445/1845 train_time:15219ms step_avg:34.20ms
step:446/1845 train_time:15254ms step_avg:34.20ms
step:447/1845 train_time:15287ms step_avg:34.20ms
step:448/1845 train_time:15322ms step_avg:34.20ms
step:449/1845 train_time:15355ms step_avg:34.20ms
step:450/1845 train_time:15390ms step_avg:34.20ms
step:451/1845 train_time:15423ms step_avg:34.20ms
step:452/1845 train_time:15458ms step_avg:34.20ms
step:453/1845 train_time:15491ms step_avg:34.20ms
step:454/1845 train_time:15526ms step_avg:34.20ms
step:455/1845 train_time:15559ms step_avg:34.20ms
step:456/1845 train_time:15594ms step_avg:34.20ms
step:457/1845 train_time:15627ms step_avg:34.20ms
step:458/1845 train_time:15662ms step_avg:34.20ms
step:459/1845 train_time:15695ms step_avg:34.19ms
step:460/1845 train_time:15731ms step_avg:34.20ms
step:461/1845 train_time:15763ms step_avg:34.19ms
step:462/1845 train_time:15799ms step_avg:34.20ms
step:463/1845 train_time:15832ms step_avg:34.19ms
step:464/1845 train_time:15867ms step_avg:34.20ms
step:465/1845 train_time:15900ms step_avg:34.19ms
step:466/1845 train_time:15935ms step_avg:34.20ms
step:467/1845 train_time:15968ms step_avg:34.19ms
step:468/1845 train_time:16003ms step_avg:34.19ms
step:469/1845 train_time:16036ms step_avg:34.19ms
step:470/1845 train_time:16071ms step_avg:34.19ms
step:471/1845 train_time:16104ms step_avg:34.19ms
step:472/1845 train_time:16139ms step_avg:34.19ms
step:473/1845 train_time:16172ms step_avg:34.19ms
step:474/1845 train_time:16207ms step_avg:34.19ms
step:475/1845 train_time:16240ms step_avg:34.19ms
step:476/1845 train_time:16275ms step_avg:34.19ms
step:477/1845 train_time:16308ms step_avg:34.19ms
step:478/1845 train_time:16343ms step_avg:34.19ms
step:479/1845 train_time:16376ms step_avg:34.19ms
step:480/1845 train_time:16411ms step_avg:34.19ms
step:481/1845 train_time:16444ms step_avg:34.19ms
step:482/1845 train_time:16479ms step_avg:34.19ms
step:483/1845 train_time:16512ms step_avg:34.19ms
step:484/1845 train_time:16548ms step_avg:34.19ms
step:485/1845 train_time:16581ms step_avg:34.19ms
step:486/1845 train_time:16616ms step_avg:34.19ms
step:487/1845 train_time:16649ms step_avg:34.19ms
step:488/1845 train_time:16684ms step_avg:34.19ms
step:489/1845 train_time:16717ms step_avg:34.19ms
step:490/1845 train_time:16752ms step_avg:34.19ms
step:491/1845 train_time:16785ms step_avg:34.18ms
step:492/1845 train_time:16820ms step_avg:34.19ms
step:493/1845 train_time:16853ms step_avg:34.18ms
step:494/1845 train_time:16888ms step_avg:34.19ms
step:495/1845 train_time:16921ms step_avg:34.18ms
step:496/1845 train_time:16956ms step_avg:34.19ms
step:497/1845 train_time:16989ms step_avg:34.18ms
step:498/1845 train_time:17024ms step_avg:34.18ms
step:499/1845 train_time:17057ms step_avg:34.18ms
step:500/1845 train_time:17092ms step_avg:34.18ms
step:500/1845 val_loss:4.2821 train_time:17134ms step_avg:34.27ms
step:501/1845 train_time:17152ms step_avg:34.24ms
step:502/1845 train_time:17170ms step_avg:34.20ms
step:503/1845 train_time:17195ms step_avg:34.18ms
step:504/1845 train_time:17230ms step_avg:34.19ms
step:505/1845 train_time:17263ms step_avg:34.18ms
step:506/1845 train_time:17300ms step_avg:34.19ms
step:507/1845 train_time:17333ms step_avg:34.19ms
step:508/1845 train_time:17369ms step_avg:34.19ms
step:509/1845 train_time:17402ms step_avg:34.19ms
step:510/1845 train_time:17437ms step_avg:34.19ms
step:511/1845 train_time:17470ms step_avg:34.19ms
step:512/1845 train_time:17505ms step_avg:34.19ms
step:513/1845 train_time:17538ms step_avg:34.19ms
step:514/1845 train_time:17573ms step_avg:34.19ms
step:515/1845 train_time:17606ms step_avg:34.19ms
step:516/1845 train_time:17641ms step_avg:34.19ms
step:517/1845 train_time:17674ms step_avg:34.19ms
step:518/1845 train_time:17709ms step_avg:34.19ms
step:519/1845 train_time:17742ms step_avg:34.18ms
step:520/1845 train_time:17777ms step_avg:34.19ms
step:521/1845 train_time:17810ms step_avg:34.18ms
step:522/1845 train_time:17845ms step_avg:34.19ms
step:523/1845 train_time:17878ms step_avg:34.18ms
step:524/1845 train_time:17913ms step_avg:34.19ms
step:525/1845 train_time:17946ms step_avg:34.18ms
step:526/1845 train_time:17981ms step_avg:34.18ms
step:527/1845 train_time:18014ms step_avg:34.18ms
step:528/1845 train_time:18049ms step_avg:34.18ms
step:529/1845 train_time:18082ms step_avg:34.18ms
step:530/1845 train_time:18117ms step_avg:34.18ms
step:531/1845 train_time:18150ms step_avg:34.18ms
step:532/1845 train_time:18185ms step_avg:34.18ms
step:533/1845 train_time:18218ms step_avg:34.18ms
step:534/1845 train_time:18253ms step_avg:34.18ms
step:535/1845 train_time:18286ms step_avg:34.18ms
step:536/1845 train_time:18321ms step_avg:34.18ms
step:537/1845 train_time:18354ms step_avg:34.18ms
step:538/1845 train_time:18389ms step_avg:34.18ms
step:539/1845 train_time:18422ms step_avg:34.18ms
step:540/1845 train_time:18457ms step_avg:34.18ms
step:541/1845 train_time:18490ms step_avg:34.18ms
step:542/1845 train_time:18526ms step_avg:34.18ms
step:543/1845 train_time:18559ms step_avg:34.18ms
step:544/1845 train_time:18594ms step_avg:34.18ms
step:545/1845 train_time:18626ms step_avg:34.18ms
step:546/1845 train_time:18662ms step_avg:34.18ms
step:547/1845 train_time:18695ms step_avg:34.18ms
step:548/1845 train_time:18730ms step_avg:34.18ms
step:549/1845 train_time:18763ms step_avg:34.18ms
step:550/1845 train_time:18798ms step_avg:34.18ms
step:551/1845 train_time:18831ms step_avg:34.18ms
step:552/1845 train_time:18866ms step_avg:34.18ms
step:553/1845 train_time:18899ms step_avg:34.18ms
step:554/1845 train_time:18934ms step_avg:34.18ms
step:555/1845 train_time:18967ms step_avg:34.18ms
step:556/1845 train_time:19002ms step_avg:34.18ms
step:557/1845 train_time:19035ms step_avg:34.17ms
step:558/1845 train_time:19071ms step_avg:34.18ms
step:559/1845 train_time:19103ms step_avg:34.17ms
step:560/1845 train_time:19139ms step_avg:34.18ms
step:561/1845 train_time:19171ms step_avg:34.17ms
step:562/1845 train_time:19207ms step_avg:34.18ms
step:563/1845 train_time:19239ms step_avg:34.17ms
step:564/1845 train_time:19275ms step_avg:34.17ms
step:565/1845 train_time:19307ms step_avg:34.17ms
step:566/1845 train_time:19342ms step_avg:34.17ms
step:567/1845 train_time:19375ms step_avg:34.17ms
step:568/1845 train_time:19410ms step_avg:34.17ms
step:569/1845 train_time:19443ms step_avg:34.17ms
step:570/1845 train_time:19479ms step_avg:34.17ms
step:571/1845 train_time:19511ms step_avg:34.17ms
step:572/1845 train_time:19547ms step_avg:34.17ms
step:573/1845 train_time:19580ms step_avg:34.17ms
step:574/1845 train_time:19615ms step_avg:34.17ms
step:575/1845 train_time:19648ms step_avg:34.17ms
step:576/1845 train_time:19683ms step_avg:34.17ms
step:577/1845 train_time:19716ms step_avg:34.17ms
step:578/1845 train_time:19752ms step_avg:34.17ms
step:579/1845 train_time:19784ms step_avg:34.17ms
step:580/1845 train_time:19820ms step_avg:34.17ms
step:581/1845 train_time:19852ms step_avg:34.17ms
step:582/1845 train_time:19888ms step_avg:34.17ms
step:583/1845 train_time:19921ms step_avg:34.17ms
step:584/1845 train_time:19956ms step_avg:34.17ms
step:585/1845 train_time:19989ms step_avg:34.17ms
step:586/1845 train_time:20024ms step_avg:34.17ms
step:587/1845 train_time:20057ms step_avg:34.17ms
step:588/1845 train_time:20092ms step_avg:34.17ms
step:589/1845 train_time:20125ms step_avg:34.17ms
step:590/1845 train_time:20161ms step_avg:34.17ms
step:591/1845 train_time:20193ms step_avg:34.17ms
step:592/1845 train_time:20228ms step_avg:34.17ms
step:593/1845 train_time:20261ms step_avg:34.17ms
step:594/1845 train_time:20296ms step_avg:34.17ms
step:595/1845 train_time:20329ms step_avg:34.17ms
step:596/1845 train_time:20365ms step_avg:34.17ms
step:597/1845 train_time:20398ms step_avg:34.17ms
step:598/1845 train_time:20433ms step_avg:34.17ms
step:599/1845 train_time:20466ms step_avg:34.17ms
step:600/1845 train_time:20501ms step_avg:34.17ms
step:601/1845 train_time:20534ms step_avg:34.17ms
step:602/1845 train_time:20569ms step_avg:34.17ms
step:603/1845 train_time:20604ms step_avg:34.17ms
step:604/1845 train_time:20663ms step_avg:34.21ms
step:605/1845 train_time:20722ms step_avg:34.25ms
step:606/1845 train_time:20785ms step_avg:34.30ms
step:607/1845 train_time:20846ms step_avg:34.34ms
step:608/1845 train_time:20907ms step_avg:34.39ms
step:609/1845 train_time:20967ms step_avg:34.43ms
step:610/1845 train_time:21031ms step_avg:34.48ms
step:611/1845 train_time:21091ms step_avg:34.52ms
step:612/1845 train_time:21154ms step_avg:34.57ms
step:613/1845 train_time:21215ms step_avg:34.61ms
step:614/1845 train_time:21278ms step_avg:34.65ms
step:615/1845 train_time:21338ms step_avg:34.70ms
step:616/1845 train_time:21401ms step_avg:34.74ms
step:617/1845 train_time:21461ms step_avg:34.78ms
step:618/1845 train_time:21524ms step_avg:34.83ms
step:619/1845 train_time:21584ms step_avg:34.87ms
step:620/1845 train_time:21647ms step_avg:34.91ms
step:621/1845 train_time:21707ms step_avg:34.95ms
step:622/1845 train_time:21770ms step_avg:35.00ms
step:623/1845 train_time:21830ms step_avg:35.04ms
step:624/1845 train_time:21892ms step_avg:35.08ms
step:625/1845 train_time:21952ms step_avg:35.12ms
step:626/1845 train_time:22014ms step_avg:35.17ms
step:627/1845 train_time:22074ms step_avg:35.21ms
step:628/1845 train_time:22138ms step_avg:35.25ms
step:629/1845 train_time:22198ms step_avg:35.29ms
step:630/1845 train_time:22262ms step_avg:35.34ms
step:631/1845 train_time:22322ms step_avg:35.38ms
step:632/1845 train_time:22386ms step_avg:35.42ms
step:633/1845 train_time:22446ms step_avg:35.46ms
step:634/1845 train_time:22509ms step_avg:35.50ms
step:635/1845 train_time:22569ms step_avg:35.54ms
step:636/1845 train_time:22632ms step_avg:35.59ms
step:637/1845 train_time:22693ms step_avg:35.62ms
step:638/1845 train_time:22756ms step_avg:35.67ms
step:639/1845 train_time:22815ms step_avg:35.70ms
step:640/1845 train_time:22878ms step_avg:35.75ms
step:641/1845 train_time:22938ms step_avg:35.78ms
step:642/1845 train_time:23001ms step_avg:35.83ms
step:643/1845 train_time:23061ms step_avg:35.86ms
step:644/1845 train_time:23123ms step_avg:35.91ms
step:645/1845 train_time:23183ms step_avg:35.94ms
step:646/1845 train_time:23246ms step_avg:35.99ms
step:647/1845 train_time:23307ms step_avg:36.02ms
step:648/1845 train_time:23370ms step_avg:36.06ms
step:649/1845 train_time:23430ms step_avg:36.10ms
step:650/1845 train_time:23492ms step_avg:36.14ms
step:651/1845 train_time:23553ms step_avg:36.18ms
step:652/1845 train_time:23616ms step_avg:36.22ms
step:653/1845 train_time:23676ms step_avg:36.26ms
step:654/1845 train_time:23739ms step_avg:36.30ms
step:655/1845 train_time:23799ms step_avg:36.33ms
step:656/1845 train_time:23862ms step_avg:36.38ms
step:657/1845 train_time:23922ms step_avg:36.41ms
step:658/1845 train_time:23986ms step_avg:36.45ms
step:659/1845 train_time:24045ms step_avg:36.49ms
step:660/1845 train_time:24108ms step_avg:36.53ms
step:661/1845 train_time:24168ms step_avg:36.56ms
step:662/1845 train_time:24231ms step_avg:36.60ms
step:663/1845 train_time:24291ms step_avg:36.64ms
step:664/1845 train_time:24354ms step_avg:36.68ms
step:665/1845 train_time:24414ms step_avg:36.71ms
step:666/1845 train_time:24478ms step_avg:36.75ms
step:667/1845 train_time:24538ms step_avg:36.79ms
step:668/1845 train_time:24601ms step_avg:36.83ms
step:669/1845 train_time:24662ms step_avg:36.86ms
step:670/1845 train_time:24725ms step_avg:36.90ms
step:671/1845 train_time:24785ms step_avg:36.94ms
step:672/1845 train_time:24848ms step_avg:36.98ms
step:673/1845 train_time:24908ms step_avg:37.01ms
step:674/1845 train_time:24970ms step_avg:37.05ms
step:675/1845 train_time:25031ms step_avg:37.08ms
step:676/1845 train_time:25093ms step_avg:37.12ms
step:677/1845 train_time:25153ms step_avg:37.15ms
step:678/1845 train_time:25216ms step_avg:37.19ms
step:679/1845 train_time:25276ms step_avg:37.23ms
step:680/1845 train_time:25340ms step_avg:37.26ms
step:681/1845 train_time:25401ms step_avg:37.30ms
step:682/1845 train_time:25463ms step_avg:37.34ms
step:683/1845 train_time:25523ms step_avg:37.37ms
step:684/1845 train_time:25586ms step_avg:37.41ms
step:685/1845 train_time:25646ms step_avg:37.44ms
step:686/1845 train_time:25709ms step_avg:37.48ms
step:687/1845 train_time:25769ms step_avg:37.51ms
step:688/1845 train_time:25832ms step_avg:37.55ms
step:689/1845 train_time:25892ms step_avg:37.58ms
step:690/1845 train_time:25955ms step_avg:37.62ms
step:691/1845 train_time:26015ms step_avg:37.65ms
step:692/1845 train_time:26078ms step_avg:37.69ms
step:693/1845 train_time:26138ms step_avg:37.72ms
step:694/1845 train_time:26202ms step_avg:37.75ms
step:695/1845 train_time:26262ms step_avg:37.79ms
step:696/1845 train_time:26324ms step_avg:37.82ms
step:697/1845 train_time:26384ms step_avg:37.85ms
step:698/1845 train_time:26447ms step_avg:37.89ms
step:699/1845 train_time:26507ms step_avg:37.92ms
step:700/1845 train_time:26570ms step_avg:37.96ms
step:701/1845 train_time:26630ms step_avg:37.99ms
step:702/1845 train_time:26693ms step_avg:38.02ms
step:703/1845 train_time:26753ms step_avg:38.06ms
step:704/1845 train_time:26816ms step_avg:38.09ms
step:705/1845 train_time:26875ms step_avg:38.12ms
step:706/1845 train_time:26940ms step_avg:38.16ms
step:707/1845 train_time:27000ms step_avg:38.19ms
step:708/1845 train_time:27063ms step_avg:38.22ms
step:709/1845 train_time:27122ms step_avg:38.25ms
step:710/1845 train_time:27186ms step_avg:38.29ms
step:711/1845 train_time:27246ms step_avg:38.32ms
step:712/1845 train_time:27309ms step_avg:38.35ms
step:713/1845 train_time:27369ms step_avg:38.39ms
step:714/1845 train_time:27432ms step_avg:38.42ms
step:715/1845 train_time:27492ms step_avg:38.45ms
step:716/1845 train_time:27555ms step_avg:38.48ms
step:717/1845 train_time:27614ms step_avg:38.51ms
step:718/1845 train_time:27678ms step_avg:38.55ms
step:719/1845 train_time:27738ms step_avg:38.58ms
step:720/1845 train_time:27800ms step_avg:38.61ms
step:721/1845 train_time:27861ms step_avg:38.64ms
step:722/1845 train_time:27924ms step_avg:38.68ms
step:723/1845 train_time:27984ms step_avg:38.71ms
step:724/1845 train_time:28047ms step_avg:38.74ms
step:725/1845 train_time:28107ms step_avg:38.77ms
step:726/1845 train_time:28170ms step_avg:38.80ms
step:727/1845 train_time:28229ms step_avg:38.83ms
step:728/1845 train_time:28292ms step_avg:38.86ms
step:729/1845 train_time:28354ms step_avg:38.89ms
step:730/1845 train_time:28416ms step_avg:38.93ms
step:731/1845 train_time:28476ms step_avg:38.95ms
step:732/1845 train_time:28539ms step_avg:38.99ms
step:733/1845 train_time:28600ms step_avg:39.02ms
step:734/1845 train_time:28662ms step_avg:39.05ms
step:735/1845 train_time:28722ms step_avg:39.08ms
step:736/1845 train_time:28785ms step_avg:39.11ms
step:737/1845 train_time:28845ms step_avg:39.14ms
step:738/1845 train_time:28908ms step_avg:39.17ms
step:739/1845 train_time:28968ms step_avg:39.20ms
step:740/1845 train_time:29030ms step_avg:39.23ms
step:741/1845 train_time:29090ms step_avg:39.26ms
step:742/1845 train_time:29153ms step_avg:39.29ms
step:743/1845 train_time:29213ms step_avg:39.32ms
step:744/1845 train_time:29276ms step_avg:39.35ms
step:745/1845 train_time:29336ms step_avg:39.38ms
step:746/1845 train_time:29399ms step_avg:39.41ms
step:747/1845 train_time:29459ms step_avg:39.44ms
step:748/1845 train_time:29522ms step_avg:39.47ms
step:749/1845 train_time:29582ms step_avg:39.50ms
step:750/1845 train_time:29645ms step_avg:39.53ms
step:750/1845 val_loss:4.0219 train_time:29714ms step_avg:39.62ms
step:751/1845 train_time:29732ms step_avg:39.59ms
step:752/1845 train_time:29768ms step_avg:39.58ms
step:753/1845 train_time:29832ms step_avg:39.62ms
step:754/1845 train_time:29896ms step_avg:39.65ms
step:755/1845 train_time:29956ms step_avg:39.68ms
step:756/1845 train_time:30020ms step_avg:39.71ms
step:757/1845 train_time:30080ms step_avg:39.74ms
step:758/1845 train_time:30142ms step_avg:39.77ms
step:759/1845 train_time:30202ms step_avg:39.79ms
step:760/1845 train_time:30265ms step_avg:39.82ms
step:761/1845 train_time:30325ms step_avg:39.85ms
step:762/1845 train_time:30387ms step_avg:39.88ms
step:763/1845 train_time:30447ms step_avg:39.90ms
step:764/1845 train_time:30509ms step_avg:39.93ms
step:765/1845 train_time:30569ms step_avg:39.96ms
step:766/1845 train_time:30632ms step_avg:39.99ms
step:767/1845 train_time:30692ms step_avg:40.02ms
step:768/1845 train_time:30756ms step_avg:40.05ms
step:769/1845 train_time:30817ms step_avg:40.07ms
step:770/1845 train_time:30880ms step_avg:40.10ms
step:771/1845 train_time:30941ms step_avg:40.13ms
step:772/1845 train_time:31004ms step_avg:40.16ms
step:773/1845 train_time:31065ms step_avg:40.19ms
step:774/1845 train_time:31127ms step_avg:40.22ms
step:775/1845 train_time:31187ms step_avg:40.24ms
step:776/1845 train_time:31250ms step_avg:40.27ms
step:777/1845 train_time:31310ms step_avg:40.30ms
step:778/1845 train_time:31373ms step_avg:40.32ms
step:779/1845 train_time:31432ms step_avg:40.35ms
step:780/1845 train_time:31495ms step_avg:40.38ms
step:781/1845 train_time:31555ms step_avg:40.40ms
step:782/1845 train_time:31617ms step_avg:40.43ms
step:783/1845 train_time:31677ms step_avg:40.46ms
step:784/1845 train_time:31741ms step_avg:40.49ms
step:785/1845 train_time:31802ms step_avg:40.51ms
step:786/1845 train_time:31865ms step_avg:40.54ms
step:787/1845 train_time:31926ms step_avg:40.57ms
step:788/1845 train_time:31988ms step_avg:40.59ms
step:789/1845 train_time:32049ms step_avg:40.62ms
step:790/1845 train_time:32113ms step_avg:40.65ms
step:791/1845 train_time:32174ms step_avg:40.67ms
step:792/1845 train_time:32237ms step_avg:40.70ms
step:793/1845 train_time:32296ms step_avg:40.73ms
step:794/1845 train_time:32359ms step_avg:40.75ms
step:795/1845 train_time:32419ms step_avg:40.78ms
step:796/1845 train_time:32482ms step_avg:40.81ms
step:797/1845 train_time:32542ms step_avg:40.83ms
step:798/1845 train_time:32604ms step_avg:40.86ms
step:799/1845 train_time:32664ms step_avg:40.88ms
step:800/1845 train_time:32727ms step_avg:40.91ms
step:801/1845 train_time:32787ms step_avg:40.93ms
step:802/1845 train_time:32851ms step_avg:40.96ms
step:803/1845 train_time:32911ms step_avg:40.98ms
step:804/1845 train_time:32974ms step_avg:41.01ms
step:805/1845 train_time:33033ms step_avg:41.04ms
step:806/1845 train_time:33096ms step_avg:41.06ms
step:807/1845 train_time:33157ms step_avg:41.09ms
step:808/1845 train_time:33220ms step_avg:41.11ms
step:809/1845 train_time:33280ms step_avg:41.14ms
step:810/1845 train_time:33343ms step_avg:41.16ms
step:811/1845 train_time:33403ms step_avg:41.19ms
step:812/1845 train_time:33465ms step_avg:41.21ms
step:813/1845 train_time:33525ms step_avg:41.24ms
step:814/1845 train_time:33588ms step_avg:41.26ms
step:815/1845 train_time:33648ms step_avg:41.29ms
step:816/1845 train_time:33711ms step_avg:41.31ms
step:817/1845 train_time:33771ms step_avg:41.34ms
step:818/1845 train_time:33833ms step_avg:41.36ms
step:819/1845 train_time:33894ms step_avg:41.38ms
step:820/1845 train_time:33956ms step_avg:41.41ms
step:821/1845 train_time:34016ms step_avg:41.43ms
step:822/1845 train_time:34079ms step_avg:41.46ms
step:823/1845 train_time:34140ms step_avg:41.48ms
step:824/1845 train_time:34203ms step_avg:41.51ms
step:825/1845 train_time:34263ms step_avg:41.53ms
step:826/1845 train_time:34326ms step_avg:41.56ms
step:827/1845 train_time:34386ms step_avg:41.58ms
step:828/1845 train_time:34450ms step_avg:41.61ms
step:829/1845 train_time:34510ms step_avg:41.63ms
step:830/1845 train_time:34573ms step_avg:41.65ms
step:831/1845 train_time:34634ms step_avg:41.68ms
step:832/1845 train_time:34697ms step_avg:41.70ms
step:833/1845 train_time:34757ms step_avg:41.73ms
step:834/1845 train_time:34819ms step_avg:41.75ms
step:835/1845 train_time:34879ms step_avg:41.77ms
step:836/1845 train_time:34943ms step_avg:41.80ms
step:837/1845 train_time:35003ms step_avg:41.82ms
step:838/1845 train_time:35067ms step_avg:41.85ms
step:839/1845 train_time:35127ms step_avg:41.87ms
step:840/1845 train_time:35190ms step_avg:41.89ms
step:841/1845 train_time:35251ms step_avg:41.92ms
step:842/1845 train_time:35314ms step_avg:41.94ms
step:843/1845 train_time:35373ms step_avg:41.96ms
step:844/1845 train_time:35437ms step_avg:41.99ms
step:845/1845 train_time:35497ms step_avg:42.01ms
step:846/1845 train_time:35559ms step_avg:42.03ms
step:847/1845 train_time:35619ms step_avg:42.05ms
step:848/1845 train_time:35683ms step_avg:42.08ms
step:849/1845 train_time:35743ms step_avg:42.10ms
step:850/1845 train_time:35805ms step_avg:42.12ms
step:851/1845 train_time:35865ms step_avg:42.14ms
step:852/1845 train_time:35928ms step_avg:42.17ms
step:853/1845 train_time:35988ms step_avg:42.19ms
step:854/1845 train_time:36051ms step_avg:42.21ms
step:855/1845 train_time:36112ms step_avg:42.24ms
step:856/1845 train_time:36175ms step_avg:42.26ms
step:857/1845 train_time:36235ms step_avg:42.28ms
step:858/1845 train_time:36298ms step_avg:42.30ms
step:859/1845 train_time:36358ms step_avg:42.33ms
step:860/1845 train_time:36422ms step_avg:42.35ms
step:861/1845 train_time:36483ms step_avg:42.37ms
step:862/1845 train_time:36545ms step_avg:42.40ms
step:863/1845 train_time:36605ms step_avg:42.42ms
step:864/1845 train_time:36668ms step_avg:42.44ms
step:865/1845 train_time:36728ms step_avg:42.46ms
step:866/1845 train_time:36791ms step_avg:42.48ms
step:867/1845 train_time:36851ms step_avg:42.50ms
step:868/1845 train_time:36913ms step_avg:42.53ms
step:869/1845 train_time:36974ms step_avg:42.55ms
step:870/1845 train_time:37037ms step_avg:42.57ms
step:871/1845 train_time:37097ms step_avg:42.59ms
step:872/1845 train_time:37159ms step_avg:42.61ms
step:873/1845 train_time:37220ms step_avg:42.63ms
step:874/1845 train_time:37283ms step_avg:42.66ms
step:875/1845 train_time:37344ms step_avg:42.68ms
step:876/1845 train_time:37407ms step_avg:42.70ms
step:877/1845 train_time:37467ms step_avg:42.72ms
step:878/1845 train_time:37529ms step_avg:42.74ms
step:879/1845 train_time:37589ms step_avg:42.76ms
step:880/1845 train_time:37652ms step_avg:42.79ms
step:881/1845 train_time:37712ms step_avg:42.81ms
step:882/1845 train_time:37775ms step_avg:42.83ms
step:883/1845 train_time:37835ms step_avg:42.85ms
step:884/1845 train_time:37898ms step_avg:42.87ms
step:885/1845 train_time:37957ms step_avg:42.89ms
step:886/1845 train_time:38020ms step_avg:42.91ms
step:887/1845 train_time:38081ms step_avg:42.93ms
step:888/1845 train_time:38144ms step_avg:42.95ms
step:889/1845 train_time:38205ms step_avg:42.97ms
step:890/1845 train_time:38267ms step_avg:43.00ms
step:891/1845 train_time:38327ms step_avg:43.02ms
step:892/1845 train_time:38390ms step_avg:43.04ms
step:893/1845 train_time:38451ms step_avg:43.06ms
step:894/1845 train_time:38513ms step_avg:43.08ms
step:895/1845 train_time:38573ms step_avg:43.10ms
step:896/1845 train_time:38636ms step_avg:43.12ms
step:897/1845 train_time:38696ms step_avg:43.14ms
step:898/1845 train_time:38759ms step_avg:43.16ms
step:899/1845 train_time:38819ms step_avg:43.18ms
step:900/1845 train_time:38882ms step_avg:43.20ms
step:901/1845 train_time:38943ms step_avg:43.22ms
step:902/1845 train_time:39006ms step_avg:43.24ms
step:903/1845 train_time:39065ms step_avg:43.26ms
step:904/1845 train_time:39129ms step_avg:43.28ms
step:905/1845 train_time:39189ms step_avg:43.30ms
step:906/1845 train_time:39252ms step_avg:43.32ms
step:907/1845 train_time:39312ms step_avg:43.34ms
step:908/1845 train_time:39375ms step_avg:43.36ms
step:909/1845 train_time:39436ms step_avg:43.38ms
step:910/1845 train_time:39498ms step_avg:43.40ms
step:911/1845 train_time:39558ms step_avg:43.42ms
step:912/1845 train_time:39620ms step_avg:43.44ms
step:913/1845 train_time:39681ms step_avg:43.46ms
step:914/1845 train_time:39744ms step_avg:43.48ms
step:915/1845 train_time:39805ms step_avg:43.50ms
step:916/1845 train_time:39868ms step_avg:43.52ms
step:917/1845 train_time:39928ms step_avg:43.54ms
step:918/1845 train_time:39990ms step_avg:43.56ms
step:919/1845 train_time:40051ms step_avg:43.58ms
step:920/1845 train_time:40114ms step_avg:43.60ms
step:921/1845 train_time:40174ms step_avg:43.62ms
step:922/1845 train_time:40236ms step_avg:43.64ms
step:923/1845 train_time:40296ms step_avg:43.66ms
step:924/1845 train_time:40358ms step_avg:43.68ms
step:925/1845 train_time:40418ms step_avg:43.70ms
step:926/1845 train_time:40482ms step_avg:43.72ms
step:927/1845 train_time:40542ms step_avg:43.73ms
step:928/1845 train_time:40605ms step_avg:43.76ms
step:929/1845 train_time:40664ms step_avg:43.77ms
step:930/1845 train_time:40727ms step_avg:43.79ms
step:931/1845 train_time:40787ms step_avg:43.81ms
step:932/1845 train_time:40850ms step_avg:43.83ms
step:933/1845 train_time:40910ms step_avg:43.85ms
step:934/1845 train_time:40973ms step_avg:43.87ms
step:935/1845 train_time:41033ms step_avg:43.89ms
step:936/1845 train_time:41096ms step_avg:43.91ms
step:937/1845 train_time:41157ms step_avg:43.92ms
step:938/1845 train_time:41219ms step_avg:43.94ms
step:939/1845 train_time:41279ms step_avg:43.96ms
step:940/1845 train_time:41341ms step_avg:43.98ms
step:941/1845 train_time:41401ms step_avg:44.00ms
step:942/1845 train_time:41464ms step_avg:44.02ms
step:943/1845 train_time:41524ms step_avg:44.03ms
step:944/1845 train_time:41588ms step_avg:44.05ms
step:945/1845 train_time:41648ms step_avg:44.07ms
step:946/1845 train_time:41710ms step_avg:44.09ms
step:947/1845 train_time:41770ms step_avg:44.11ms
step:948/1845 train_time:41833ms step_avg:44.13ms
step:949/1845 train_time:41893ms step_avg:44.14ms
step:950/1845 train_time:41956ms step_avg:44.16ms
step:951/1845 train_time:42016ms step_avg:44.18ms
step:952/1845 train_time:42079ms step_avg:44.20ms
step:953/1845 train_time:42139ms step_avg:44.22ms
step:954/1845 train_time:42202ms step_avg:44.24ms
step:955/1845 train_time:42262ms step_avg:44.25ms
step:956/1845 train_time:42325ms step_avg:44.27ms
step:957/1845 train_time:42385ms step_avg:44.29ms
step:958/1845 train_time:42448ms step_avg:44.31ms
step:959/1845 train_time:42508ms step_avg:44.32ms
step:960/1845 train_time:42570ms step_avg:44.34ms
step:961/1845 train_time:42630ms step_avg:44.36ms
step:962/1845 train_time:42694ms step_avg:44.38ms
step:963/1845 train_time:42754ms step_avg:44.40ms
step:964/1845 train_time:42816ms step_avg:44.42ms
step:965/1845 train_time:42876ms step_avg:44.43ms
step:966/1845 train_time:42939ms step_avg:44.45ms
step:967/1845 train_time:42999ms step_avg:44.47ms
step:968/1845 train_time:43062ms step_avg:44.49ms
step:969/1845 train_time:43122ms step_avg:44.50ms
step:970/1845 train_time:43186ms step_avg:44.52ms
step:971/1845 train_time:43246ms step_avg:44.54ms
step:972/1845 train_time:43309ms step_avg:44.56ms
step:973/1845 train_time:43369ms step_avg:44.57ms
step:974/1845 train_time:43431ms step_avg:44.59ms
step:975/1845 train_time:43492ms step_avg:44.61ms
step:976/1845 train_time:43555ms step_avg:44.63ms
step:977/1845 train_time:43615ms step_avg:44.64ms
step:978/1845 train_time:43677ms step_avg:44.66ms
step:979/1845 train_time:43737ms step_avg:44.68ms
step:980/1845 train_time:43801ms step_avg:44.69ms
step:981/1845 train_time:43861ms step_avg:44.71ms
step:982/1845 train_time:43924ms step_avg:44.73ms
step:983/1845 train_time:43984ms step_avg:44.74ms
step:984/1845 train_time:44047ms step_avg:44.76ms
step:985/1845 train_time:44107ms step_avg:44.78ms
step:986/1845 train_time:44169ms step_avg:44.80ms
step:987/1845 train_time:44229ms step_avg:44.81ms
step:988/1845 train_time:44292ms step_avg:44.83ms
step:989/1845 train_time:44353ms step_avg:44.85ms
step:990/1845 train_time:44415ms step_avg:44.86ms
step:991/1845 train_time:44475ms step_avg:44.88ms
step:992/1845 train_time:44538ms step_avg:44.90ms
step:993/1845 train_time:44598ms step_avg:44.91ms
step:994/1845 train_time:44660ms step_avg:44.93ms
step:995/1845 train_time:44720ms step_avg:44.94ms
step:996/1845 train_time:44784ms step_avg:44.96ms
step:997/1845 train_time:44844ms step_avg:44.98ms
step:998/1845 train_time:44908ms step_avg:45.00ms
step:999/1845 train_time:44968ms step_avg:45.01ms
step:1000/1845 train_time:45031ms step_avg:45.03ms
step:1000/1845 val_loss:3.7758 train_time:45100ms step_avg:45.10ms
step:1001/1845 train_time:45118ms step_avg:45.07ms
step:1002/1845 train_time:45155ms step_avg:45.07ms
step:1003/1845 train_time:45218ms step_avg:45.08ms
step:1004/1845 train_time:45281ms step_avg:45.10ms
step:1005/1845 train_time:45342ms step_avg:45.12ms
step:1006/1845 train_time:45404ms step_avg:45.13ms
step:1007/1845 train_time:45464ms step_avg:45.15ms
step:1008/1845 train_time:45526ms step_avg:45.16ms
step:1009/1845 train_time:45585ms step_avg:45.18ms
step:1010/1845 train_time:45648ms step_avg:45.20ms
step:1011/1845 train_time:45707ms step_avg:45.21ms
step:1012/1845 train_time:45770ms step_avg:45.23ms
step:1013/1845 train_time:45830ms step_avg:45.24ms
step:1014/1845 train_time:45893ms step_avg:45.26ms
step:1015/1845 train_time:45952ms step_avg:45.27ms
step:1016/1845 train_time:46015ms step_avg:45.29ms
step:1017/1845 train_time:46077ms step_avg:45.31ms
step:1018/1845 train_time:46140ms step_avg:45.32ms
step:1019/1845 train_time:46200ms step_avg:45.34ms
step:1020/1845 train_time:46264ms step_avg:45.36ms
step:1021/1845 train_time:46325ms step_avg:45.37ms
step:1022/1845 train_time:46388ms step_avg:45.39ms
step:1023/1845 train_time:46448ms step_avg:45.40ms
step:1024/1845 train_time:46511ms step_avg:45.42ms
step:1025/1845 train_time:46571ms step_avg:45.44ms
step:1026/1845 train_time:46634ms step_avg:45.45ms
step:1027/1845 train_time:46694ms step_avg:45.47ms
step:1028/1845 train_time:46756ms step_avg:45.48ms
step:1029/1845 train_time:46816ms step_avg:45.50ms
step:1030/1845 train_time:46878ms step_avg:45.51ms
step:1031/1845 train_time:46938ms step_avg:45.53ms
step:1032/1845 train_time:47001ms step_avg:45.54ms
step:1033/1845 train_time:47061ms step_avg:45.56ms
step:1034/1845 train_time:47125ms step_avg:45.58ms
step:1035/1845 train_time:47186ms step_avg:45.59ms
step:1036/1845 train_time:47249ms step_avg:45.61ms
step:1037/1845 train_time:47310ms step_avg:45.62ms
step:1038/1845 train_time:47374ms step_avg:45.64ms
step:1039/1845 train_time:47434ms step_avg:45.65ms
step:1040/1845 train_time:47497ms step_avg:45.67ms
step:1041/1845 train_time:47556ms step_avg:45.68ms
step:1042/1845 train_time:47619ms step_avg:45.70ms
step:1043/1845 train_time:47679ms step_avg:45.71ms
step:1044/1845 train_time:47743ms step_avg:45.73ms
step:1045/1845 train_time:47802ms step_avg:45.74ms
step:1046/1845 train_time:47865ms step_avg:45.76ms
step:1047/1845 train_time:47925ms step_avg:45.77ms
step:1048/1845 train_time:47989ms step_avg:45.79ms
step:1049/1845 train_time:48049ms step_avg:45.80ms
step:1050/1845 train_time:48112ms step_avg:45.82ms
step:1051/1845 train_time:48172ms step_avg:45.83ms
step:1052/1845 train_time:48235ms step_avg:45.85ms
step:1053/1845 train_time:48296ms step_avg:45.87ms
step:1054/1845 train_time:48359ms step_avg:45.88ms
step:1055/1845 train_time:48419ms step_avg:45.89ms
step:1056/1845 train_time:48482ms step_avg:45.91ms
step:1057/1845 train_time:48542ms step_avg:45.92ms
step:1058/1845 train_time:48605ms step_avg:45.94ms
step:1059/1845 train_time:48665ms step_avg:45.95ms
step:1060/1845 train_time:48728ms step_avg:45.97ms
step:1061/1845 train_time:48788ms step_avg:45.98ms
step:1062/1845 train_time:48851ms step_avg:46.00ms
step:1063/1845 train_time:48911ms step_avg:46.01ms
step:1064/1845 train_time:48973ms step_avg:46.03ms
step:1065/1845 train_time:49034ms step_avg:46.04ms
step:1066/1845 train_time:49096ms step_avg:46.06ms
step:1067/1845 train_time:49156ms step_avg:46.07ms
step:1068/1845 train_time:49219ms step_avg:46.08ms
step:1069/1845 train_time:49279ms step_avg:46.10ms
step:1070/1845 train_time:49342ms step_avg:46.11ms
step:1071/1845 train_time:49402ms step_avg:46.13ms
step:1072/1845 train_time:49466ms step_avg:46.14ms
step:1073/1845 train_time:49526ms step_avg:46.16ms
step:1074/1845 train_time:49588ms step_avg:46.17ms
step:1075/1845 train_time:49649ms step_avg:46.19ms
step:1076/1845 train_time:49712ms step_avg:46.20ms
step:1077/1845 train_time:49772ms step_avg:46.21ms
step:1078/1845 train_time:49834ms step_avg:46.23ms
step:1079/1845 train_time:49895ms step_avg:46.24ms
step:1080/1845 train_time:49957ms step_avg:46.26ms
step:1081/1845 train_time:50018ms step_avg:46.27ms
step:1082/1845 train_time:50080ms step_avg:46.28ms
step:1083/1845 train_time:50141ms step_avg:46.30ms
step:1084/1845 train_time:50203ms step_avg:46.31ms
step:1085/1845 train_time:50264ms step_avg:46.33ms
step:1086/1845 train_time:50328ms step_avg:46.34ms
step:1087/1845 train_time:50388ms step_avg:46.36ms
step:1088/1845 train_time:50450ms step_avg:46.37ms
step:1089/1845 train_time:50510ms step_avg:46.38ms
step:1090/1845 train_time:50574ms step_avg:46.40ms
step:1091/1845 train_time:50634ms step_avg:46.41ms
step:1092/1845 train_time:50697ms step_avg:46.43ms
step:1093/1845 train_time:50757ms step_avg:46.44ms
step:1094/1845 train_time:50820ms step_avg:46.45ms
step:1095/1845 train_time:50879ms step_avg:46.46ms
step:1096/1845 train_time:50942ms step_avg:46.48ms
step:1097/1845 train_time:51002ms step_avg:46.49ms
step:1098/1845 train_time:51065ms step_avg:46.51ms
step:1099/1845 train_time:51126ms step_avg:46.52ms
step:1100/1845 train_time:51190ms step_avg:46.54ms
step:1101/1845 train_time:51250ms step_avg:46.55ms
step:1102/1845 train_time:51312ms step_avg:46.56ms
step:1103/1845 train_time:51372ms step_avg:46.58ms
step:1104/1845 train_time:51435ms step_avg:46.59ms
step:1105/1845 train_time:51495ms step_avg:46.60ms
step:1106/1845 train_time:51558ms step_avg:46.62ms
step:1107/1845 train_time:51618ms step_avg:46.63ms
step:1108/1845 train_time:51681ms step_avg:46.64ms
step:1109/1845 train_time:51741ms step_avg:46.66ms
step:1110/1845 train_time:51804ms step_avg:46.67ms
step:1111/1845 train_time:51864ms step_avg:46.68ms
step:1112/1845 train_time:51927ms step_avg:46.70ms
step:1113/1845 train_time:51987ms step_avg:46.71ms
step:1114/1845 train_time:52050ms step_avg:46.72ms
step:1115/1845 train_time:52110ms step_avg:46.74ms
step:1116/1845 train_time:52173ms step_avg:46.75ms
step:1117/1845 train_time:52233ms step_avg:46.76ms
step:1118/1845 train_time:52296ms step_avg:46.78ms
step:1119/1845 train_time:52355ms step_avg:46.79ms
step:1120/1845 train_time:52418ms step_avg:46.80ms
step:1121/1845 train_time:52479ms step_avg:46.81ms
step:1122/1845 train_time:52543ms step_avg:46.83ms
step:1123/1845 train_time:52604ms step_avg:46.84ms
step:1124/1845 train_time:52667ms step_avg:46.86ms
step:1125/1845 train_time:52727ms step_avg:46.87ms
step:1126/1845 train_time:52790ms step_avg:46.88ms
step:1127/1845 train_time:52850ms step_avg:46.89ms
step:1128/1845 train_time:52913ms step_avg:46.91ms
step:1129/1845 train_time:52973ms step_avg:46.92ms
step:1130/1845 train_time:53036ms step_avg:46.93ms
step:1131/1845 train_time:53096ms step_avg:46.95ms
step:1132/1845 train_time:53158ms step_avg:46.96ms
step:1133/1845 train_time:53219ms step_avg:46.97ms
step:1134/1845 train_time:53281ms step_avg:46.98ms
step:1135/1845 train_time:53341ms step_avg:47.00ms
step:1136/1845 train_time:53403ms step_avg:47.01ms
step:1137/1845 train_time:53463ms step_avg:47.02ms
step:1138/1845 train_time:53526ms step_avg:47.04ms
step:1139/1845 train_time:53586ms step_avg:47.05ms
step:1140/1845 train_time:53650ms step_avg:47.06ms
step:1141/1845 train_time:53710ms step_avg:47.07ms
step:1142/1845 train_time:53772ms step_avg:47.09ms
step:1143/1845 train_time:53832ms step_avg:47.10ms
step:1144/1845 train_time:53895ms step_avg:47.11ms
step:1145/1845 train_time:53955ms step_avg:47.12ms
step:1146/1845 train_time:54018ms step_avg:47.14ms
step:1147/1845 train_time:54078ms step_avg:47.15ms
step:1148/1845 train_time:54140ms step_avg:47.16ms
step:1149/1845 train_time:54200ms step_avg:47.17ms
step:1150/1845 train_time:54263ms step_avg:47.19ms
step:1151/1845 train_time:54323ms step_avg:47.20ms
step:1152/1845 train_time:54386ms step_avg:47.21ms
step:1153/1845 train_time:54447ms step_avg:47.22ms
step:1154/1845 train_time:54509ms step_avg:47.23ms
step:1155/1845 train_time:54569ms step_avg:47.25ms
step:1156/1845 train_time:54632ms step_avg:47.26ms
step:1157/1845 train_time:54692ms step_avg:47.27ms
step:1158/1845 train_time:54755ms step_avg:47.28ms
step:1159/1845 train_time:54815ms step_avg:47.30ms
step:1160/1845 train_time:54878ms step_avg:47.31ms
step:1161/1845 train_time:54939ms step_avg:47.32ms
step:1162/1845 train_time:55001ms step_avg:47.33ms
step:1163/1845 train_time:55061ms step_avg:47.34ms
step:1164/1845 train_time:55124ms step_avg:47.36ms
step:1165/1845 train_time:55184ms step_avg:47.37ms
step:1166/1845 train_time:55247ms step_avg:47.38ms
step:1167/1845 train_time:55306ms step_avg:47.39ms
step:1168/1845 train_time:55369ms step_avg:47.41ms
step:1169/1845 train_time:55430ms step_avg:47.42ms
step:1170/1845 train_time:55493ms step_avg:47.43ms
step:1171/1845 train_time:55553ms step_avg:47.44ms
step:1172/1845 train_time:55616ms step_avg:47.45ms
step:1173/1845 train_time:55676ms step_avg:47.46ms
step:1174/1845 train_time:55739ms step_avg:47.48ms
step:1175/1845 train_time:55799ms step_avg:47.49ms
step:1176/1845 train_time:55862ms step_avg:47.50ms
step:1177/1845 train_time:55923ms step_avg:47.51ms
step:1178/1845 train_time:55986ms step_avg:47.53ms
step:1179/1845 train_time:56046ms step_avg:47.54ms
step:1180/1845 train_time:56108ms step_avg:47.55ms
step:1181/1845 train_time:56168ms step_avg:47.56ms
step:1182/1845 train_time:56231ms step_avg:47.57ms
step:1183/1845 train_time:56291ms step_avg:47.58ms
step:1184/1845 train_time:56354ms step_avg:47.60ms
step:1185/1845 train_time:56413ms step_avg:47.61ms
step:1186/1845 train_time:56476ms step_avg:47.62ms
step:1187/1845 train_time:56537ms step_avg:47.63ms
step:1188/1845 train_time:56599ms step_avg:47.64ms
step:1189/1845 train_time:56659ms step_avg:47.65ms
step:1190/1845 train_time:56722ms step_avg:47.67ms
step:1191/1845 train_time:56782ms step_avg:47.68ms
step:1192/1845 train_time:56846ms step_avg:47.69ms
step:1193/1845 train_time:56905ms step_avg:47.70ms
step:1194/1845 train_time:56968ms step_avg:47.71ms
step:1195/1845 train_time:57029ms step_avg:47.72ms
step:1196/1845 train_time:57091ms step_avg:47.74ms
step:1197/1845 train_time:57151ms step_avg:47.75ms
step:1198/1845 train_time:57214ms step_avg:47.76ms
step:1199/1845 train_time:57275ms step_avg:47.77ms
step:1200/1845 train_time:57337ms step_avg:47.78ms
step:1201/1845 train_time:57397ms step_avg:47.79ms
step:1202/1845 train_time:57460ms step_avg:47.80ms
step:1203/1845 train_time:57521ms step_avg:47.81ms
step:1204/1845 train_time:57584ms step_avg:47.83ms
step:1205/1845 train_time:57646ms step_avg:47.84ms
step:1206/1845 train_time:57732ms step_avg:47.87ms
step:1207/1845 train_time:57820ms step_avg:47.90ms
step:1208/1845 train_time:57909ms step_avg:47.94ms
step:1209/1845 train_time:57995ms step_avg:47.97ms
step:1210/1845 train_time:58085ms step_avg:48.00ms
step:1211/1845 train_time:58172ms step_avg:48.04ms
step:1212/1845 train_time:58262ms step_avg:48.07ms
step:1213/1845 train_time:58349ms step_avg:48.10ms
step:1214/1845 train_time:58437ms step_avg:48.14ms
step:1215/1845 train_time:58524ms step_avg:48.17ms
step:1216/1845 train_time:58613ms step_avg:48.20ms
step:1217/1845 train_time:58699ms step_avg:48.23ms
step:1218/1845 train_time:58788ms step_avg:48.27ms
step:1219/1845 train_time:58874ms step_avg:48.30ms
step:1220/1845 train_time:58963ms step_avg:48.33ms
step:1221/1845 train_time:59049ms step_avg:48.36ms
step:1222/1845 train_time:59140ms step_avg:48.40ms
step:1223/1845 train_time:59226ms step_avg:48.43ms
step:1224/1845 train_time:59314ms step_avg:48.46ms
step:1225/1845 train_time:59401ms step_avg:48.49ms
step:1226/1845 train_time:59490ms step_avg:48.52ms
step:1227/1845 train_time:59576ms step_avg:48.55ms
step:1228/1845 train_time:59666ms step_avg:48.59ms
step:1229/1845 train_time:59752ms step_avg:48.62ms
step:1230/1845 train_time:59841ms step_avg:48.65ms
step:1231/1845 train_time:59928ms step_avg:48.68ms
step:1232/1845 train_time:60016ms step_avg:48.71ms
step:1233/1845 train_time:60104ms step_avg:48.75ms
step:1234/1845 train_time:60192ms step_avg:48.78ms
step:1235/1845 train_time:60279ms step_avg:48.81ms
step:1236/1845 train_time:60369ms step_avg:48.84ms
step:1237/1845 train_time:60455ms step_avg:48.87ms
step:1238/1845 train_time:60544ms step_avg:48.90ms
step:1239/1845 train_time:60630ms step_avg:48.93ms
step:1240/1845 train_time:60720ms step_avg:48.97ms
step:1241/1845 train_time:60807ms step_avg:49.00ms
step:1242/1845 train_time:60895ms step_avg:49.03ms
step:1243/1845 train_time:60982ms step_avg:49.06ms
step:1244/1845 train_time:61072ms step_avg:49.09ms
step:1245/1845 train_time:61158ms step_avg:49.12ms
step:1246/1845 train_time:61247ms step_avg:49.15ms
step:1247/1845 train_time:61333ms step_avg:49.18ms
step:1248/1845 train_time:61423ms step_avg:49.22ms
step:1249/1845 train_time:61511ms step_avg:49.25ms
step:1250/1845 train_time:61600ms step_avg:49.28ms
step:1250/1845 val_loss:3.5349 train_time:61696ms step_avg:49.36ms
step:1251/1845 train_time:61714ms step_avg:49.33ms
step:1252/1845 train_time:61778ms step_avg:49.34ms
step:1253/1845 train_time:61867ms step_avg:49.37ms
step:1254/1845 train_time:61958ms step_avg:49.41ms
step:1255/1845 train_time:62045ms step_avg:49.44ms
step:1256/1845 train_time:62133ms step_avg:49.47ms
step:1257/1845 train_time:62218ms step_avg:49.50ms
step:1258/1845 train_time:62306ms step_avg:49.53ms
step:1259/1845 train_time:62391ms step_avg:49.56ms
step:1260/1845 train_time:62478ms step_avg:49.59ms
step:1261/1845 train_time:62563ms step_avg:49.61ms
step:1262/1845 train_time:62653ms step_avg:49.65ms
step:1263/1845 train_time:62741ms step_avg:49.68ms
step:1264/1845 train_time:62832ms step_avg:49.71ms
step:1265/1845 train_time:62920ms step_avg:49.74ms
step:1266/1845 train_time:63010ms step_avg:49.77ms
step:1267/1845 train_time:63097ms step_avg:49.80ms
step:1268/1845 train_time:63186ms step_avg:49.83ms
step:1269/1845 train_time:63272ms step_avg:49.86ms
step:1270/1845 train_time:63359ms step_avg:49.89ms
step:1271/1845 train_time:63445ms step_avg:49.92ms
step:1272/1845 train_time:63533ms step_avg:49.95ms
step:1273/1845 train_time:63619ms step_avg:49.98ms
step:1274/1845 train_time:63709ms step_avg:50.01ms
step:1275/1845 train_time:63797ms step_avg:50.04ms
step:1276/1845 train_time:63886ms step_avg:50.07ms
step:1277/1845 train_time:63975ms step_avg:50.10ms
step:1278/1845 train_time:64065ms step_avg:50.13ms
step:1279/1845 train_time:64151ms step_avg:50.16ms
step:1280/1845 train_time:64240ms step_avg:50.19ms
step:1281/1845 train_time:64326ms step_avg:50.22ms
step:1282/1845 train_time:64414ms step_avg:50.24ms
step:1283/1845 train_time:64498ms step_avg:50.27ms
step:1284/1845 train_time:64588ms step_avg:50.30ms
step:1285/1845 train_time:64675ms step_avg:50.33ms
step:1286/1845 train_time:64764ms step_avg:50.36ms
step:1287/1845 train_time:64850ms step_avg:50.39ms
step:1288/1845 train_time:64941ms step_avg:50.42ms
step:1289/1845 train_time:65028ms step_avg:50.45ms
step:1290/1845 train_time:65118ms step_avg:50.48ms
step:1291/1845 train_time:65204ms step_avg:50.51ms
step:1292/1845 train_time:65294ms step_avg:50.54ms
step:1293/1845 train_time:65380ms step_avg:50.56ms
step:1294/1845 train_time:65468ms step_avg:50.59ms
step:1295/1845 train_time:65555ms step_avg:50.62ms
step:1296/1845 train_time:65643ms step_avg:50.65ms
step:1297/1845 train_time:65730ms step_avg:50.68ms
step:1298/1845 train_time:65819ms step_avg:50.71ms
step:1299/1845 train_time:65906ms step_avg:50.74ms
step:1300/1845 train_time:65996ms step_avg:50.77ms
step:1301/1845 train_time:66082ms step_avg:50.79ms
step:1302/1845 train_time:66171ms step_avg:50.82ms
step:1303/1845 train_time:66258ms step_avg:50.85ms
step:1304/1845 train_time:66347ms step_avg:50.88ms
step:1305/1845 train_time:66433ms step_avg:50.91ms
step:1306/1845 train_time:66521ms step_avg:50.94ms
step:1307/1845 train_time:66608ms step_avg:50.96ms
step:1308/1845 train_time:66698ms step_avg:50.99ms
step:1309/1845 train_time:66783ms step_avg:51.02ms
step:1310/1845 train_time:66873ms step_avg:51.05ms
step:1311/1845 train_time:66960ms step_avg:51.08ms
step:1312/1845 train_time:67050ms step_avg:51.10ms
step:1313/1845 train_time:67136ms step_avg:51.13ms
step:1314/1845 train_time:67224ms step_avg:51.16ms
step:1315/1845 train_time:67310ms step_avg:51.19ms
step:1316/1845 train_time:67400ms step_avg:51.22ms
step:1317/1845 train_time:67486ms step_avg:51.24ms
step:1318/1845 train_time:67575ms step_avg:51.27ms
step:1319/1845 train_time:67660ms step_avg:51.30ms
step:1320/1845 train_time:67750ms step_avg:51.33ms
step:1321/1845 train_time:67837ms step_avg:51.35ms
step:1322/1845 train_time:67926ms step_avg:51.38ms
step:1323/1845 train_time:68012ms step_avg:51.41ms
step:1324/1845 train_time:68101ms step_avg:51.44ms
step:1325/1845 train_time:68188ms step_avg:51.46ms
step:1326/1845 train_time:68277ms step_avg:51.49ms
step:1327/1845 train_time:68363ms step_avg:51.52ms
step:1328/1845 train_time:68453ms step_avg:51.55ms
step:1329/1845 train_time:68539ms step_avg:51.57ms
step:1330/1845 train_time:68627ms step_avg:51.60ms
step:1331/1845 train_time:68714ms step_avg:51.63ms
step:1332/1845 train_time:68802ms step_avg:51.65ms
step:1333/1845 train_time:68888ms step_avg:51.68ms
step:1334/1845 train_time:68978ms step_avg:51.71ms
step:1335/1845 train_time:69064ms step_avg:51.73ms
step:1336/1845 train_time:69153ms step_avg:51.76ms
step:1337/1845 train_time:69239ms step_avg:51.79ms
step:1338/1845 train_time:69330ms step_avg:51.82ms
step:1339/1845 train_time:69416ms step_avg:51.84ms
step:1340/1845 train_time:69504ms step_avg:51.87ms
step:1341/1845 train_time:69590ms step_avg:51.89ms
step:1342/1845 train_time:69679ms step_avg:51.92ms
step:1343/1845 train_time:69766ms step_avg:51.95ms
step:1344/1845 train_time:69855ms step_avg:51.98ms
step:1345/1845 train_time:69942ms step_avg:52.00ms
step:1346/1845 train_time:70032ms step_avg:52.03ms
step:1347/1845 train_time:70118ms step_avg:52.06ms
step:1348/1845 train_time:70207ms step_avg:52.08ms
step:1349/1845 train_time:70295ms step_avg:52.11ms
step:1350/1845 train_time:70384ms step_avg:52.14ms
step:1351/1845 train_time:70470ms step_avg:52.16ms
step:1352/1845 train_time:70559ms step_avg:52.19ms
step:1353/1845 train_time:70645ms step_avg:52.21ms
step:1354/1845 train_time:70734ms step_avg:52.24ms
step:1355/1845 train_time:70820ms step_avg:52.27ms
step:1356/1845 train_time:70910ms step_avg:52.29ms
step:1357/1845 train_time:70997ms step_avg:52.32ms
step:1358/1845 train_time:71086ms step_avg:52.35ms
step:1359/1845 train_time:71172ms step_avg:52.37ms
step:1360/1845 train_time:71261ms step_avg:52.40ms
step:1361/1845 train_time:71348ms step_avg:52.42ms
step:1362/1845 train_time:71437ms step_avg:52.45ms
step:1363/1845 train_time:71523ms step_avg:52.47ms
step:1364/1845 train_time:71612ms step_avg:52.50ms
step:1365/1845 train_time:71699ms step_avg:52.53ms
step:1366/1845 train_time:71787ms step_avg:52.55ms
step:1367/1845 train_time:71874ms step_avg:52.58ms
step:1368/1845 train_time:71962ms step_avg:52.60ms
step:1369/1845 train_time:72050ms step_avg:52.63ms
step:1370/1845 train_time:72140ms step_avg:52.66ms
step:1371/1845 train_time:72226ms step_avg:52.68ms
step:1372/1845 train_time:72315ms step_avg:52.71ms
step:1373/1845 train_time:72401ms step_avg:52.73ms
step:1374/1845 train_time:72490ms step_avg:52.76ms
step:1375/1845 train_time:72576ms step_avg:52.78ms
step:1376/1845 train_time:72665ms step_avg:52.81ms
step:1377/1845 train_time:72751ms step_avg:52.83ms
step:1378/1845 train_time:72841ms step_avg:52.86ms
step:1379/1845 train_time:72927ms step_avg:52.88ms
step:1380/1845 train_time:73015ms step_avg:52.91ms
step:1381/1845 train_time:73102ms step_avg:52.93ms
step:1382/1845 train_time:73192ms step_avg:52.96ms
step:1383/1845 train_time:73278ms step_avg:52.99ms
step:1384/1845 train_time:73367ms step_avg:53.01ms
step:1385/1845 train_time:73455ms step_avg:53.04ms
step:1386/1845 train_time:73543ms step_avg:53.06ms
step:1387/1845 train_time:73631ms step_avg:53.09ms
step:1388/1845 train_time:73719ms step_avg:53.11ms
step:1389/1845 train_time:73805ms step_avg:53.14ms
step:1390/1845 train_time:73897ms step_avg:53.16ms
step:1391/1845 train_time:73983ms step_avg:53.19ms
step:1392/1845 train_time:74072ms step_avg:53.21ms
step:1393/1845 train_time:74158ms step_avg:53.24ms
step:1394/1845 train_time:74249ms step_avg:53.26ms
step:1395/1845 train_time:74336ms step_avg:53.29ms
step:1396/1845 train_time:74424ms step_avg:53.31ms
step:1397/1845 train_time:74511ms step_avg:53.34ms
step:1398/1845 train_time:74600ms step_avg:53.36ms
step:1399/1845 train_time:74686ms step_avg:53.39ms
step:1400/1845 train_time:74776ms step_avg:53.41ms
step:1401/1845 train_time:74862ms step_avg:53.43ms
step:1402/1845 train_time:74952ms step_avg:53.46ms
step:1403/1845 train_time:75039ms step_avg:53.48ms
step:1404/1845 train_time:75127ms step_avg:53.51ms
step:1405/1845 train_time:75213ms step_avg:53.53ms
step:1406/1845 train_time:75301ms step_avg:53.56ms
step:1407/1845 train_time:75387ms step_avg:53.58ms
step:1408/1845 train_time:75477ms step_avg:53.61ms
step:1409/1845 train_time:75562ms step_avg:53.63ms
step:1410/1845 train_time:75652ms step_avg:53.65ms
step:1411/1845 train_time:75740ms step_avg:53.68ms
step:1412/1845 train_time:75830ms step_avg:53.70ms
step:1413/1845 train_time:75916ms step_avg:53.73ms
step:1414/1845 train_time:76004ms step_avg:53.75ms
step:1415/1845 train_time:76092ms step_avg:53.78ms
step:1416/1845 train_time:76180ms step_avg:53.80ms
step:1417/1845 train_time:76267ms step_avg:53.82ms
step:1418/1845 train_time:76356ms step_avg:53.85ms
step:1419/1845 train_time:76442ms step_avg:53.87ms
step:1420/1845 train_time:76532ms step_avg:53.90ms
step:1421/1845 train_time:76619ms step_avg:53.92ms
step:1422/1845 train_time:76709ms step_avg:53.94ms
step:1423/1845 train_time:76796ms step_avg:53.97ms
step:1424/1845 train_time:76884ms step_avg:53.99ms
step:1425/1845 train_time:76971ms step_avg:54.01ms
step:1426/1845 train_time:77060ms step_avg:54.04ms
step:1427/1845 train_time:77145ms step_avg:54.06ms
step:1428/1845 train_time:77235ms step_avg:54.09ms
step:1429/1845 train_time:77320ms step_avg:54.11ms
step:1430/1845 train_time:77410ms step_avg:54.13ms
step:1431/1845 train_time:77496ms step_avg:54.16ms
step:1432/1845 train_time:77585ms step_avg:54.18ms
step:1433/1845 train_time:77673ms step_avg:54.20ms
step:1434/1845 train_time:77762ms step_avg:54.23ms
step:1435/1845 train_time:77847ms step_avg:54.25ms
step:1436/1845 train_time:77937ms step_avg:54.27ms
step:1437/1845 train_time:78023ms step_avg:54.30ms
step:1438/1845 train_time:78112ms step_avg:54.32ms
step:1439/1845 train_time:78199ms step_avg:54.34ms
step:1440/1845 train_time:78288ms step_avg:54.37ms
step:1441/1845 train_time:78375ms step_avg:54.39ms
step:1442/1845 train_time:78462ms step_avg:54.41ms
step:1443/1845 train_time:78548ms step_avg:54.43ms
step:1444/1845 train_time:78638ms step_avg:54.46ms
step:1445/1845 train_time:78723ms step_avg:54.48ms
step:1446/1845 train_time:78814ms step_avg:54.50ms
step:1447/1845 train_time:78901ms step_avg:54.53ms
step:1448/1845 train_time:78991ms step_avg:54.55ms
step:1449/1845 train_time:79077ms step_avg:54.57ms
step:1450/1845 train_time:79165ms step_avg:54.60ms
step:1451/1845 train_time:79251ms step_avg:54.62ms
step:1452/1845 train_time:79341ms step_avg:54.64ms
step:1453/1845 train_time:79427ms step_avg:54.66ms
step:1454/1845 train_time:79516ms step_avg:54.69ms
step:1455/1845 train_time:79603ms step_avg:54.71ms
step:1456/1845 train_time:79693ms step_avg:54.73ms
step:1457/1845 train_time:79779ms step_avg:54.76ms
step:1458/1845 train_time:79869ms step_avg:54.78ms
step:1459/1845 train_time:79956ms step_avg:54.80ms
step:1460/1845 train_time:80045ms step_avg:54.83ms
step:1461/1845 train_time:80132ms step_avg:54.85ms
step:1462/1845 train_time:80220ms step_avg:54.87ms
step:1463/1845 train_time:80306ms step_avg:54.89ms
step:1464/1845 train_time:80396ms step_avg:54.92ms
step:1465/1845 train_time:80482ms step_avg:54.94ms
step:1466/1845 train_time:80570ms step_avg:54.96ms
step:1467/1845 train_time:80657ms step_avg:54.98ms
step:1468/1845 train_time:80746ms step_avg:55.00ms
step:1469/1845 train_time:80834ms step_avg:55.03ms
step:1470/1845 train_time:80922ms step_avg:55.05ms
step:1471/1845 train_time:81009ms step_avg:55.07ms
step:1472/1845 train_time:81099ms step_avg:55.09ms
step:1473/1845 train_time:81186ms step_avg:55.12ms
step:1474/1845 train_time:81275ms step_avg:55.14ms
step:1475/1845 train_time:81360ms step_avg:55.16ms
step:1476/1845 train_time:81450ms step_avg:55.18ms
step:1477/1845 train_time:81536ms step_avg:55.20ms
step:1478/1845 train_time:81625ms step_avg:55.23ms
step:1479/1845 train_time:81712ms step_avg:55.25ms
step:1480/1845 train_time:81800ms step_avg:55.27ms
step:1481/1845 train_time:81887ms step_avg:55.29ms
step:1482/1845 train_time:81976ms step_avg:55.31ms
step:1483/1845 train_time:82063ms step_avg:55.34ms
step:1484/1845 train_time:82152ms step_avg:55.36ms
step:1485/1845 train_time:82239ms step_avg:55.38ms
step:1486/1845 train_time:82327ms step_avg:55.40ms
step:1487/1845 train_time:82413ms step_avg:55.42ms
step:1488/1845 train_time:82502ms step_avg:55.44ms
step:1489/1845 train_time:82588ms step_avg:55.47ms
step:1490/1845 train_time:82678ms step_avg:55.49ms
step:1491/1845 train_time:82763ms step_avg:55.51ms
step:1492/1845 train_time:82854ms step_avg:55.53ms
step:1493/1845 train_time:82939ms step_avg:55.55ms
step:1494/1845 train_time:83030ms step_avg:55.58ms
step:1495/1845 train_time:83117ms step_avg:55.60ms
step:1496/1845 train_time:83207ms step_avg:55.62ms
step:1497/1845 train_time:83294ms step_avg:55.64ms
step:1498/1845 train_time:83382ms step_avg:55.66ms
step:1499/1845 train_time:83468ms step_avg:55.68ms
step:1500/1845 train_time:83558ms step_avg:55.71ms
step:1500/1845 val_loss:3.4026 train_time:83654ms step_avg:55.77ms
step:1501/1845 train_time:83672ms step_avg:55.74ms
step:1502/1845 train_time:83736ms step_avg:55.75ms
step:1503/1845 train_time:83826ms step_avg:55.77ms
step:1504/1845 train_time:83915ms step_avg:55.79ms
step:1505/1845 train_time:84003ms step_avg:55.82ms
step:1506/1845 train_time:84091ms step_avg:55.84ms
step:1507/1845 train_time:84176ms step_avg:55.86ms
step:1508/1845 train_time:84265ms step_avg:55.88ms
step:1509/1845 train_time:84351ms step_avg:55.90ms
step:1510/1845 train_time:84441ms step_avg:55.92ms
step:1511/1845 train_time:84525ms step_avg:55.94ms
step:1512/1845 train_time:84615ms step_avg:55.96ms
step:1513/1845 train_time:84703ms step_avg:55.98ms
step:1514/1845 train_time:84793ms step_avg:56.01ms
step:1515/1845 train_time:84881ms step_avg:56.03ms
step:1516/1845 train_time:84972ms step_avg:56.05ms
step:1517/1845 train_time:85059ms step_avg:56.07ms
step:1518/1845 train_time:85148ms step_avg:56.09ms
step:1519/1845 train_time:85234ms step_avg:56.11ms
step:1520/1845 train_time:85322ms step_avg:56.13ms
step:1521/1845 train_time:85408ms step_avg:56.15ms
step:1522/1845 train_time:85496ms step_avg:56.17ms
step:1523/1845 train_time:85583ms step_avg:56.19ms
step:1524/1845 train_time:85672ms step_avg:56.21ms
step:1525/1845 train_time:85759ms step_avg:56.24ms
step:1526/1845 train_time:85849ms step_avg:56.26ms
step:1527/1845 train_time:85938ms step_avg:56.28ms
step:1528/1845 train_time:86027ms step_avg:56.30ms
step:1529/1845 train_time:86114ms step_avg:56.32ms
step:1530/1845 train_time:86202ms step_avg:56.34ms
step:1531/1845 train_time:86287ms step_avg:56.36ms
step:1532/1845 train_time:86377ms step_avg:56.38ms
step:1533/1845 train_time:86463ms step_avg:56.40ms
step:1534/1845 train_time:86553ms step_avg:56.42ms
step:1535/1845 train_time:86640ms step_avg:56.44ms
step:1536/1845 train_time:86729ms step_avg:56.46ms
step:1537/1845 train_time:86817ms step_avg:56.48ms
step:1538/1845 train_time:86905ms step_avg:56.51ms
step:1539/1845 train_time:86993ms step_avg:56.53ms
step:1540/1845 train_time:87083ms step_avg:56.55ms
step:1541/1845 train_time:87169ms step_avg:56.57ms
step:1542/1845 train_time:87259ms step_avg:56.59ms
step:1543/1845 train_time:87344ms step_avg:56.61ms
step:1544/1845 train_time:87433ms step_avg:56.63ms
step:1545/1845 train_time:87519ms step_avg:56.65ms
step:1546/1845 train_time:87609ms step_avg:56.67ms
step:1547/1845 train_time:87696ms step_avg:56.69ms
step:1548/1845 train_time:87785ms step_avg:56.71ms
step:1549/1845 train_time:87871ms step_avg:56.73ms
step:1550/1845 train_time:87961ms step_avg:56.75ms
step:1551/1845 train_time:88048ms step_avg:56.77ms
step:1552/1845 train_time:88138ms step_avg:56.79ms
step:1553/1845 train_time:88223ms step_avg:56.81ms
step:1554/1845 train_time:88312ms step_avg:56.83ms
step:1555/1845 train_time:88399ms step_avg:56.85ms
step:1556/1845 train_time:88487ms step_avg:56.87ms
step:1557/1845 train_time:88574ms step_avg:56.89ms
step:1558/1845 train_time:88662ms step_avg:56.91ms
step:1559/1845 train_time:88749ms step_avg:56.93ms
step:1560/1845 train_time:88840ms step_avg:56.95ms
step:1561/1845 train_time:88926ms step_avg:56.97ms
step:1562/1845 train_time:89018ms step_avg:56.99ms
step:1563/1845 train_time:89105ms step_avg:57.01ms
step:1564/1845 train_time:89194ms step_avg:57.03ms
step:1565/1845 train_time:89281ms step_avg:57.05ms
step:1566/1845 train_time:89369ms step_avg:57.07ms
step:1567/1845 train_time:89455ms step_avg:57.09ms
step:1568/1845 train_time:89544ms step_avg:57.11ms
step:1569/1845 train_time:89630ms step_avg:57.13ms
step:1570/1845 train_time:89719ms step_avg:57.15ms
step:1571/1845 train_time:89806ms step_avg:57.16ms
step:1572/1845 train_time:89895ms step_avg:57.19ms
step:1573/1845 train_time:89982ms step_avg:57.20ms
step:1574/1845 train_time:90071ms step_avg:57.22ms
step:1575/1845 train_time:90158ms step_avg:57.24ms
step:1576/1845 train_time:90247ms step_avg:57.26ms
step:1577/1845 train_time:90334ms step_avg:57.28ms
step:1578/1845 train_time:90422ms step_avg:57.30ms
step:1579/1845 train_time:90508ms step_avg:57.32ms
step:1580/1845 train_time:90598ms step_avg:57.34ms
step:1581/1845 train_time:90684ms step_avg:57.36ms
step:1582/1845 train_time:90774ms step_avg:57.38ms
step:1583/1845 train_time:90861ms step_avg:57.40ms
step:1584/1845 train_time:90950ms step_avg:57.42ms
step:1585/1845 train_time:91038ms step_avg:57.44ms
step:1586/1845 train_time:91127ms step_avg:57.46ms
step:1587/1845 train_time:91215ms step_avg:57.48ms
step:1588/1845 train_time:91303ms step_avg:57.50ms
step:1589/1845 train_time:91388ms step_avg:57.51ms
step:1590/1845 train_time:91478ms step_avg:57.53ms
step:1591/1845 train_time:91564ms step_avg:57.55ms
step:1592/1845 train_time:91654ms step_avg:57.57ms
step:1593/1845 train_time:91741ms step_avg:57.59ms
step:1594/1845 train_time:91830ms step_avg:57.61ms
step:1595/1845 train_time:91918ms step_avg:57.63ms
step:1596/1845 train_time:92007ms step_avg:57.65ms
step:1597/1845 train_time:92092ms step_avg:57.67ms
step:1598/1845 train_time:92182ms step_avg:57.69ms
step:1599/1845 train_time:92267ms step_avg:57.70ms
step:1600/1845 train_time:92358ms step_avg:57.72ms
step:1601/1845 train_time:92444ms step_avg:57.74ms
step:1602/1845 train_time:92535ms step_avg:57.76ms
step:1603/1845 train_time:92621ms step_avg:57.78ms
step:1604/1845 train_time:92710ms step_avg:57.80ms
step:1605/1845 train_time:92796ms step_avg:57.82ms
step:1606/1845 train_time:92885ms step_avg:57.84ms
step:1607/1845 train_time:92972ms step_avg:57.85ms
step:1608/1845 train_time:93062ms step_avg:57.87ms
step:1609/1845 train_time:93147ms step_avg:57.89ms
step:1610/1845 train_time:93237ms step_avg:57.91ms
step:1611/1845 train_time:93323ms step_avg:57.93ms
step:1612/1845 train_time:93412ms step_avg:57.95ms
step:1613/1845 train_time:93499ms step_avg:57.97ms
step:1614/1845 train_time:93586ms step_avg:57.98ms
step:1615/1845 train_time:93674ms step_avg:58.00ms
step:1616/1845 train_time:93762ms step_avg:58.02ms
step:1617/1845 train_time:93849ms step_avg:58.04ms
step:1618/1845 train_time:93940ms step_avg:58.06ms
step:1619/1845 train_time:94027ms step_avg:58.08ms
step:1620/1845 train_time:94118ms step_avg:58.10ms
step:1621/1845 train_time:94204ms step_avg:58.11ms
step:1622/1845 train_time:94293ms step_avg:58.13ms
step:1623/1845 train_time:94380ms step_avg:58.15ms
step:1624/1845 train_time:94467ms step_avg:58.17ms
step:1625/1845 train_time:94553ms step_avg:58.19ms
step:1626/1845 train_time:94642ms step_avg:58.21ms
step:1627/1845 train_time:94730ms step_avg:58.22ms
step:1628/1845 train_time:94819ms step_avg:58.24ms
step:1629/1845 train_time:94906ms step_avg:58.26ms
step:1630/1845 train_time:94994ms step_avg:58.28ms
step:1631/1845 train_time:95081ms step_avg:58.30ms
step:1632/1845 train_time:95170ms step_avg:58.32ms
step:1633/1845 train_time:95257ms step_avg:58.33ms
step:1634/1845 train_time:95346ms step_avg:58.35ms
step:1635/1845 train_time:95432ms step_avg:58.37ms
step:1636/1845 train_time:95521ms step_avg:58.39ms
step:1637/1845 train_time:95607ms step_avg:58.40ms
step:1638/1845 train_time:95696ms step_avg:58.42ms
step:1639/1845 train_time:95782ms step_avg:58.44ms
step:1640/1845 train_time:95872ms step_avg:58.46ms
step:1641/1845 train_time:95958ms step_avg:58.48ms
step:1642/1845 train_time:96047ms step_avg:58.49ms
step:1643/1845 train_time:96134ms step_avg:58.51ms
step:1644/1845 train_time:96223ms step_avg:58.53ms
step:1645/1845 train_time:96310ms step_avg:58.55ms
step:1646/1845 train_time:96400ms step_avg:58.57ms
step:1647/1845 train_time:96486ms step_avg:58.58ms
step:1648/1845 train_time:96575ms step_avg:58.60ms
step:1649/1845 train_time:96660ms step_avg:58.62ms
step:1650/1845 train_time:96751ms step_avg:58.64ms
step:1651/1845 train_time:96838ms step_avg:58.65ms
step:1652/1845 train_time:96926ms step_avg:58.67ms
step:1653/1845 train_time:97012ms step_avg:58.69ms
step:1654/1845 train_time:97102ms step_avg:58.71ms
step:1655/1845 train_time:97188ms step_avg:58.72ms
step:1656/1845 train_time:97278ms step_avg:58.74ms
step:1657/1845 train_time:97365ms step_avg:58.76ms
step:1658/1845 train_time:97455ms step_avg:58.78ms
step:1659/1845 train_time:97541ms step_avg:58.80ms
step:1660/1845 train_time:97630ms step_avg:58.81ms
step:1661/1845 train_time:97716ms step_avg:58.83ms
step:1662/1845 train_time:97804ms step_avg:58.85ms
step:1663/1845 train_time:97891ms step_avg:58.86ms
step:1664/1845 train_time:97983ms step_avg:58.88ms
step:1665/1845 train_time:98068ms step_avg:58.90ms
step:1666/1845 train_time:98157ms step_avg:58.92ms
step:1667/1845 train_time:98244ms step_avg:58.93ms
step:1668/1845 train_time:98335ms step_avg:58.95ms
step:1669/1845 train_time:98421ms step_avg:58.97ms
step:1670/1845 train_time:98511ms step_avg:58.99ms
step:1671/1845 train_time:98597ms step_avg:59.00ms
step:1672/1845 train_time:98686ms step_avg:59.02ms
step:1673/1845 train_time:98773ms step_avg:59.04ms
step:1674/1845 train_time:98862ms step_avg:59.06ms
step:1675/1845 train_time:98949ms step_avg:59.07ms
step:1676/1845 train_time:99039ms step_avg:59.09ms
step:1677/1845 train_time:99124ms step_avg:59.11ms
step:1678/1845 train_time:99214ms step_avg:59.13ms
step:1679/1845 train_time:99301ms step_avg:59.14ms
step:1680/1845 train_time:99390ms step_avg:59.16ms
step:1681/1845 train_time:99476ms step_avg:59.18ms
step:1682/1845 train_time:99564ms step_avg:59.19ms
step:1683/1845 train_time:99651ms step_avg:59.21ms
step:1684/1845 train_time:99740ms step_avg:59.23ms
step:1685/1845 train_time:99826ms step_avg:59.24ms
step:1686/1845 train_time:99917ms step_avg:59.26ms
step:1687/1845 train_time:100004ms step_avg:59.28ms
step:1688/1845 train_time:100092ms step_avg:59.30ms
step:1689/1845 train_time:100179ms step_avg:59.31ms
step:1690/1845 train_time:100267ms step_avg:59.33ms
step:1691/1845 train_time:100354ms step_avg:59.35ms
step:1692/1845 train_time:100443ms step_avg:59.36ms
step:1693/1845 train_time:100530ms step_avg:59.38ms
step:1694/1845 train_time:100620ms step_avg:59.40ms
step:1695/1845 train_time:100705ms step_avg:59.41ms
step:1696/1845 train_time:100795ms step_avg:59.43ms
step:1697/1845 train_time:100882ms step_avg:59.45ms
step:1698/1845 train_time:100972ms step_avg:59.47ms
step:1699/1845 train_time:101059ms step_avg:59.48ms
step:1700/1845 train_time:101147ms step_avg:59.50ms
step:1701/1845 train_time:101234ms step_avg:59.51ms
step:1702/1845 train_time:101322ms step_avg:59.53ms
step:1703/1845 train_time:101410ms step_avg:59.55ms
step:1704/1845 train_time:101500ms step_avg:59.57ms
step:1705/1845 train_time:101586ms step_avg:59.58ms
step:1706/1845 train_time:101677ms step_avg:59.60ms
step:1707/1845 train_time:101763ms step_avg:59.61ms
step:1708/1845 train_time:101852ms step_avg:59.63ms
step:1709/1845 train_time:101938ms step_avg:59.65ms
step:1710/1845 train_time:102027ms step_avg:59.66ms
step:1711/1845 train_time:102114ms step_avg:59.68ms
step:1712/1845 train_time:102203ms step_avg:59.70ms
step:1713/1845 train_time:102289ms step_avg:59.71ms
step:1714/1845 train_time:102379ms step_avg:59.73ms
step:1715/1845 train_time:102465ms step_avg:59.75ms
step:1716/1845 train_time:102555ms step_avg:59.76ms
step:1717/1845 train_time:102641ms step_avg:59.78ms
step:1718/1845 train_time:102730ms step_avg:59.80ms
step:1719/1845 train_time:102817ms step_avg:59.81ms
step:1720/1845 train_time:102905ms step_avg:59.83ms
step:1721/1845 train_time:102992ms step_avg:59.84ms
step:1722/1845 train_time:103082ms step_avg:59.86ms
step:1723/1845 train_time:103168ms step_avg:59.88ms
step:1724/1845 train_time:103259ms step_avg:59.90ms
step:1725/1845 train_time:103345ms step_avg:59.91ms
step:1726/1845 train_time:103435ms step_avg:59.93ms
step:1727/1845 train_time:103522ms step_avg:59.94ms
step:1728/1845 train_time:103611ms step_avg:59.96ms
step:1729/1845 train_time:103697ms step_avg:59.97ms
step:1730/1845 train_time:103785ms step_avg:59.99ms
step:1731/1845 train_time:103871ms step_avg:60.01ms
step:1732/1845 train_time:103960ms step_avg:60.02ms
step:1733/1845 train_time:104046ms step_avg:60.04ms
step:1734/1845 train_time:104137ms step_avg:60.06ms
step:1735/1845 train_time:104223ms step_avg:60.07ms
step:1736/1845 train_time:104312ms step_avg:60.09ms
step:1737/1845 train_time:104399ms step_avg:60.10ms
step:1738/1845 train_time:104487ms step_avg:60.12ms
step:1739/1845 train_time:104574ms step_avg:60.13ms
step:1740/1845 train_time:104663ms step_avg:60.15ms
step:1741/1845 train_time:104749ms step_avg:60.17ms
step:1742/1845 train_time:104840ms step_avg:60.18ms
step:1743/1845 train_time:104925ms step_avg:60.20ms
step:1744/1845 train_time:105015ms step_avg:60.21ms
step:1745/1845 train_time:105101ms step_avg:60.23ms
step:1746/1845 train_time:105190ms step_avg:60.25ms
step:1747/1845 train_time:105278ms step_avg:60.26ms
step:1748/1845 train_time:105366ms step_avg:60.28ms
step:1749/1845 train_time:105453ms step_avg:60.29ms
step:1750/1845 train_time:105543ms step_avg:60.31ms
step:1750/1845 val_loss:3.3039 train_time:105639ms step_avg:60.37ms
step:1751/1845 train_time:105657ms step_avg:60.34ms
step:1752/1845 train_time:105719ms step_avg:60.34ms
step:1753/1845 train_time:105806ms step_avg:60.36ms
step:1754/1845 train_time:105896ms step_avg:60.37ms
step:1755/1845 train_time:105983ms step_avg:60.39ms
step:1756/1845 train_time:106073ms step_avg:60.41ms
step:1757/1845 train_time:106159ms step_avg:60.42ms
step:1758/1845 train_time:106247ms step_avg:60.44ms
step:1759/1845 train_time:106332ms step_avg:60.45ms
step:1760/1845 train_time:106420ms step_avg:60.47ms
step:1761/1845 train_time:106508ms step_avg:60.48ms
step:1762/1845 train_time:106598ms step_avg:60.50ms
step:1763/1845 train_time:106684ms step_avg:60.51ms
step:1764/1845 train_time:106778ms step_avg:60.53ms
step:1765/1845 train_time:106866ms step_avg:60.55ms
step:1766/1845 train_time:106955ms step_avg:60.56ms
step:1767/1845 train_time:107041ms step_avg:60.58ms
step:1768/1845 train_time:107131ms step_avg:60.59ms
step:1769/1845 train_time:107217ms step_avg:60.61ms
step:1770/1845 train_time:107306ms step_avg:60.62ms
step:1771/1845 train_time:107392ms step_avg:60.64ms
step:1772/1845 train_time:107480ms step_avg:60.65ms
step:1773/1845 train_time:107566ms step_avg:60.67ms
step:1774/1845 train_time:107656ms step_avg:60.69ms
step:1775/1845 train_time:107743ms step_avg:60.70ms
step:1776/1845 train_time:107834ms step_avg:60.72ms
step:1777/1845 train_time:107921ms step_avg:60.73ms
step:1778/1845 train_time:108011ms step_avg:60.75ms
step:1779/1845 train_time:108097ms step_avg:60.76ms
step:1780/1845 train_time:108186ms step_avg:60.78ms
step:1781/1845 train_time:108273ms step_avg:60.79ms
step:1782/1845 train_time:108361ms step_avg:60.81ms
step:1783/1845 train_time:108447ms step_avg:60.82ms
step:1784/1845 train_time:108537ms step_avg:60.84ms
step:1785/1845 train_time:108624ms step_avg:60.85ms
step:1786/1845 train_time:108715ms step_avg:60.87ms
step:1787/1845 train_time:108802ms step_avg:60.89ms
step:1788/1845 train_time:108892ms step_avg:60.90ms
step:1789/1845 train_time:108978ms step_avg:60.92ms
step:1790/1845 train_time:109066ms step_avg:60.93ms
step:1791/1845 train_time:109153ms step_avg:60.95ms
step:1792/1845 train_time:109241ms step_avg:60.96ms
step:1793/1845 train_time:109327ms step_avg:60.97ms
step:1794/1845 train_time:109416ms step_avg:60.99ms
step:1795/1845 train_time:109502ms step_avg:61.00ms
step:1796/1845 train_time:109593ms step_avg:61.02ms
step:1797/1845 train_time:109681ms step_avg:61.04ms
step:1798/1845 train_time:109770ms step_avg:61.05ms
step:1799/1845 train_time:109856ms step_avg:61.07ms
step:1800/1845 train_time:109946ms step_avg:61.08ms
step:1801/1845 train_time:110032ms step_avg:61.10ms
step:1802/1845 train_time:110120ms step_avg:61.11ms
step:1803/1845 train_time:110208ms step_avg:61.12ms
step:1804/1845 train_time:110297ms step_avg:61.14ms
step:1805/1845 train_time:110383ms step_avg:61.15ms
step:1806/1845 train_time:110473ms step_avg:61.17ms
step:1807/1845 train_time:110559ms step_avg:61.18ms
step:1808/1845 train_time:110648ms step_avg:61.20ms
step:1809/1845 train_time:110736ms step_avg:61.21ms
step:1810/1845 train_time:110824ms step_avg:61.23ms
step:1811/1845 train_time:110912ms step_avg:61.24ms
step:1812/1845 train_time:111000ms step_avg:61.26ms
step:1813/1845 train_time:111088ms step_avg:61.27ms
step:1814/1845 train_time:111177ms step_avg:61.29ms
step:1815/1845 train_time:111264ms step_avg:61.30ms
step:1816/1845 train_time:111353ms step_avg:61.32ms
step:1817/1845 train_time:111439ms step_avg:61.33ms
step:1818/1845 train_time:111528ms step_avg:61.35ms
step:1819/1845 train_time:111615ms step_avg:61.36ms
step:1820/1845 train_time:111704ms step_avg:61.38ms
step:1821/1845 train_time:111792ms step_avg:61.39ms
step:1822/1845 train_time:111881ms step_avg:61.41ms
step:1823/1845 train_time:111968ms step_avg:61.42ms
step:1824/1845 train_time:112057ms step_avg:61.43ms
step:1825/1845 train_time:112143ms step_avg:61.45ms
step:1826/1845 train_time:112233ms step_avg:61.46ms
step:1827/1845 train_time:112320ms step_avg:61.48ms
step:1828/1845 train_time:112411ms step_avg:61.49ms
step:1829/1845 train_time:112497ms step_avg:61.51ms
step:1830/1845 train_time:112587ms step_avg:61.52ms
step:1831/1845 train_time:112673ms step_avg:61.54ms
step:1832/1845 train_time:112762ms step_avg:61.55ms
step:1833/1845 train_time:112850ms step_avg:61.57ms
step:1834/1845 train_time:112939ms step_avg:61.58ms
step:1835/1845 train_time:113025ms step_avg:61.59ms
step:1836/1845 train_time:113114ms step_avg:61.61ms
step:1837/1845 train_time:113200ms step_avg:61.62ms
step:1838/1845 train_time:113292ms step_avg:61.64ms
step:1839/1845 train_time:113379ms step_avg:61.65ms
step:1840/1845 train_time:113470ms step_avg:61.67ms
step:1841/1845 train_time:113557ms step_avg:61.68ms
step:1842/1845 train_time:113645ms step_avg:61.70ms
step:1843/1845 train_time:113732ms step_avg:61.71ms
step:1844/1845 train_time:113820ms step_avg:61.72ms
step:1845/1845 train_time:113908ms step_avg:61.74ms
step:1845/1845 val_loss:3.2772 train_time:114005ms step_avg:61.79ms
peak memory allocated: 29801 MiB reserved: 45438 MiB

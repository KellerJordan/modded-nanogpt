import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 17:51:57 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            124W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    107146      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    107147      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    107148      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    107149      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    107150      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    107151      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    107152      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    107153      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8289 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:73ms step_avg:73.23ms
step:2/1845 train_time:94ms step_avg:47.16ms
step:3/1845 train_time:112ms step_avg:37.45ms
step:4/1845 train_time:147ms step_avg:36.80ms
step:5/1845 train_time:180ms step_avg:36.04ms
step:6/1845 train_time:274ms step_avg:45.60ms
step:7/1845 train_time:290ms step_avg:41.38ms
step:8/1845 train_time:318ms step_avg:39.71ms
step:9/1845 train_time:351ms step_avg:38.98ms
step:10/1845 train_time:386ms step_avg:38.61ms
step:11/1845 train_time:419ms step_avg:38.09ms
step:12/1845 train_time:454ms step_avg:37.86ms
step:13/1845 train_time:487ms step_avg:37.49ms
step:14/1845 train_time:523ms step_avg:37.34ms
step:15/1845 train_time:556ms step_avg:37.05ms
step:16/1845 train_time:591ms step_avg:36.96ms
step:17/1845 train_time:625ms step_avg:36.74ms
step:18/1845 train_time:660ms step_avg:36.68ms
step:19/1845 train_time:693ms step_avg:36.49ms
step:20/1845 train_time:729ms step_avg:36.44ms
step:21/1845 train_time:762ms step_avg:36.27ms
step:22/1845 train_time:797ms step_avg:36.23ms
step:23/1845 train_time:830ms step_avg:36.10ms
step:24/1845 train_time:866ms step_avg:36.08ms
step:25/1845 train_time:899ms step_avg:35.95ms
step:26/1845 train_time:934ms step_avg:35.93ms
step:27/1845 train_time:967ms step_avg:35.83ms
step:28/1845 train_time:1003ms step_avg:35.81ms
step:29/1845 train_time:1036ms step_avg:35.72ms
step:30/1845 train_time:1071ms step_avg:35.71ms
step:31/1845 train_time:1104ms step_avg:35.62ms
step:32/1845 train_time:1140ms step_avg:35.62ms
step:33/1845 train_time:1173ms step_avg:35.54ms
step:34/1845 train_time:1208ms step_avg:35.54ms
step:35/1845 train_time:1241ms step_avg:35.47ms
step:36/1845 train_time:1277ms step_avg:35.48ms
step:37/1845 train_time:1310ms step_avg:35.41ms
step:38/1845 train_time:1346ms step_avg:35.41ms
step:39/1845 train_time:1379ms step_avg:35.35ms
step:40/1845 train_time:1414ms step_avg:35.35ms
step:41/1845 train_time:1447ms step_avg:35.30ms
step:42/1845 train_time:1483ms step_avg:35.30ms
step:43/1845 train_time:1516ms step_avg:35.25ms
step:44/1845 train_time:1551ms step_avg:35.25ms
step:45/1845 train_time:1584ms step_avg:35.20ms
step:46/1845 train_time:1620ms step_avg:35.21ms
step:47/1845 train_time:1653ms step_avg:35.16ms
step:48/1845 train_time:1688ms step_avg:35.17ms
step:49/1845 train_time:1721ms step_avg:35.13ms
step:50/1845 train_time:1757ms step_avg:35.13ms
step:51/1845 train_time:1790ms step_avg:35.10ms
step:52/1845 train_time:1825ms step_avg:35.10ms
step:53/1845 train_time:1858ms step_avg:35.06ms
step:54/1845 train_time:1894ms step_avg:35.07ms
step:55/1845 train_time:1927ms step_avg:35.03ms
step:56/1845 train_time:1962ms step_avg:35.04ms
step:57/1845 train_time:1995ms step_avg:35.00ms
step:58/1845 train_time:2031ms step_avg:35.02ms
step:59/1845 train_time:2064ms step_avg:34.99ms
step:60/1845 train_time:2099ms step_avg:34.99ms
step:61/1845 train_time:2133ms step_avg:34.96ms
step:62/1845 train_time:2168ms step_avg:34.97ms
step:63/1845 train_time:2202ms step_avg:34.95ms
step:64/1845 train_time:2237ms step_avg:34.95ms
step:65/1845 train_time:2270ms step_avg:34.93ms
step:66/1845 train_time:2306ms step_avg:34.93ms
step:67/1845 train_time:2339ms step_avg:34.90ms
step:68/1845 train_time:2374ms step_avg:34.91ms
step:69/1845 train_time:2407ms step_avg:34.88ms
step:70/1845 train_time:2442ms step_avg:34.89ms
step:71/1845 train_time:2476ms step_avg:34.87ms
step:72/1845 train_time:2511ms step_avg:34.88ms
step:73/1845 train_time:2544ms step_avg:34.85ms
step:74/1845 train_time:2579ms step_avg:34.86ms
step:75/1845 train_time:2612ms step_avg:34.83ms
step:76/1845 train_time:2648ms step_avg:34.84ms
step:77/1845 train_time:2681ms step_avg:34.82ms
step:78/1845 train_time:2716ms step_avg:34.83ms
step:79/1845 train_time:2750ms step_avg:34.81ms
step:80/1845 train_time:2785ms step_avg:34.81ms
step:81/1845 train_time:2818ms step_avg:34.79ms
step:82/1845 train_time:2853ms step_avg:34.80ms
step:83/1845 train_time:2886ms step_avg:34.78ms
step:84/1845 train_time:2922ms step_avg:34.78ms
step:85/1845 train_time:2955ms step_avg:34.76ms
step:86/1845 train_time:2990ms step_avg:34.77ms
step:87/1845 train_time:3023ms step_avg:34.75ms
step:88/1845 train_time:3059ms step_avg:34.76ms
step:89/1845 train_time:3092ms step_avg:34.74ms
step:90/1845 train_time:3127ms step_avg:34.75ms
step:91/1845 train_time:3160ms step_avg:34.73ms
step:92/1845 train_time:3195ms step_avg:34.73ms
step:93/1845 train_time:3229ms step_avg:34.72ms
step:94/1845 train_time:3264ms step_avg:34.72ms
step:95/1845 train_time:3297ms step_avg:34.70ms
step:96/1845 train_time:3333ms step_avg:34.71ms
step:97/1845 train_time:3366ms step_avg:34.70ms
step:98/1845 train_time:3401ms step_avg:34.71ms
step:99/1845 train_time:3434ms step_avg:34.69ms
step:100/1845 train_time:3470ms step_avg:34.70ms
step:101/1845 train_time:3503ms step_avg:34.68ms
step:102/1845 train_time:3538ms step_avg:34.69ms
step:103/1845 train_time:3571ms step_avg:34.67ms
step:104/1845 train_time:3606ms step_avg:34.68ms
step:105/1845 train_time:3639ms step_avg:34.66ms
step:106/1845 train_time:3675ms step_avg:34.67ms
step:107/1845 train_time:3708ms step_avg:34.65ms
step:108/1845 train_time:3743ms step_avg:34.66ms
step:109/1845 train_time:3776ms step_avg:34.64ms
step:110/1845 train_time:3811ms step_avg:34.65ms
step:111/1845 train_time:3844ms step_avg:34.63ms
step:112/1845 train_time:3880ms step_avg:34.64ms
step:113/1845 train_time:3913ms step_avg:34.62ms
step:114/1845 train_time:3948ms step_avg:34.63ms
step:115/1845 train_time:3981ms step_avg:34.62ms
step:116/1845 train_time:4017ms step_avg:34.63ms
step:117/1845 train_time:4050ms step_avg:34.62ms
step:118/1845 train_time:4085ms step_avg:34.62ms
step:119/1845 train_time:4118ms step_avg:34.61ms
step:120/1845 train_time:4154ms step_avg:34.62ms
step:121/1845 train_time:4187ms step_avg:34.60ms
step:122/1845 train_time:4222ms step_avg:34.61ms
step:123/1845 train_time:4255ms step_avg:34.59ms
step:124/1845 train_time:4290ms step_avg:34.60ms
step:125/1845 train_time:4323ms step_avg:34.59ms
step:126/1845 train_time:4359ms step_avg:34.60ms
step:127/1845 train_time:4392ms step_avg:34.58ms
step:128/1845 train_time:4427ms step_avg:34.59ms
step:129/1845 train_time:4460ms step_avg:34.58ms
step:130/1845 train_time:4496ms step_avg:34.58ms
step:131/1845 train_time:4529ms step_avg:34.57ms
step:132/1845 train_time:4564ms step_avg:34.58ms
step:133/1845 train_time:4597ms step_avg:34.57ms
step:134/1845 train_time:4633ms step_avg:34.57ms
step:135/1845 train_time:4666ms step_avg:34.56ms
step:136/1845 train_time:4701ms step_avg:34.57ms
step:137/1845 train_time:4734ms step_avg:34.56ms
step:138/1845 train_time:4770ms step_avg:34.56ms
step:139/1845 train_time:4803ms step_avg:34.55ms
step:140/1845 train_time:4838ms step_avg:34.56ms
step:141/1845 train_time:4871ms step_avg:34.54ms
step:142/1845 train_time:4906ms step_avg:34.55ms
step:143/1845 train_time:4939ms step_avg:34.54ms
step:144/1845 train_time:4974ms step_avg:34.54ms
step:145/1845 train_time:5007ms step_avg:34.53ms
step:146/1845 train_time:5042ms step_avg:34.54ms
step:147/1845 train_time:5075ms step_avg:34.52ms
step:148/1845 train_time:5111ms step_avg:34.53ms
step:149/1845 train_time:5143ms step_avg:34.52ms
step:150/1845 train_time:5179ms step_avg:34.53ms
step:151/1845 train_time:5212ms step_avg:34.52ms
step:152/1845 train_time:5247ms step_avg:34.52ms
step:153/1845 train_time:5280ms step_avg:34.51ms
step:154/1845 train_time:5315ms step_avg:34.51ms
step:155/1845 train_time:5348ms step_avg:34.50ms
step:156/1845 train_time:5383ms step_avg:34.51ms
step:157/1845 train_time:5416ms step_avg:34.50ms
step:158/1845 train_time:5452ms step_avg:34.50ms
step:159/1845 train_time:5485ms step_avg:34.50ms
step:160/1845 train_time:5520ms step_avg:34.50ms
step:161/1845 train_time:5553ms step_avg:34.49ms
step:162/1845 train_time:5588ms step_avg:34.50ms
step:163/1845 train_time:5621ms step_avg:34.49ms
step:164/1845 train_time:5657ms step_avg:34.49ms
step:165/1845 train_time:5689ms step_avg:34.48ms
step:166/1845 train_time:5725ms step_avg:34.49ms
step:167/1845 train_time:5758ms step_avg:34.48ms
step:168/1845 train_time:5793ms step_avg:34.48ms
step:169/1845 train_time:5826ms step_avg:34.47ms
step:170/1845 train_time:5861ms step_avg:34.48ms
step:171/1845 train_time:5894ms step_avg:34.47ms
step:172/1845 train_time:5929ms step_avg:34.47ms
step:173/1845 train_time:5962ms step_avg:34.46ms
step:174/1845 train_time:5998ms step_avg:34.47ms
step:175/1845 train_time:6031ms step_avg:34.46ms
step:176/1845 train_time:6066ms step_avg:34.47ms
step:177/1845 train_time:6099ms step_avg:34.46ms
step:178/1845 train_time:6134ms step_avg:34.46ms
step:179/1845 train_time:6167ms step_avg:34.45ms
step:180/1845 train_time:6202ms step_avg:34.46ms
step:181/1845 train_time:6235ms step_avg:34.45ms
step:182/1845 train_time:6271ms step_avg:34.45ms
step:183/1845 train_time:6304ms step_avg:34.45ms
step:184/1845 train_time:6339ms step_avg:34.45ms
step:185/1845 train_time:6372ms step_avg:34.44ms
step:186/1845 train_time:6407ms step_avg:34.45ms
step:187/1845 train_time:6440ms step_avg:34.44ms
step:188/1845 train_time:6475ms step_avg:34.44ms
step:189/1845 train_time:6508ms step_avg:34.43ms
step:190/1845 train_time:6543ms step_avg:34.44ms
step:191/1845 train_time:6576ms step_avg:34.43ms
step:192/1845 train_time:6612ms step_avg:34.44ms
step:193/1845 train_time:6645ms step_avg:34.43ms
step:194/1845 train_time:6680ms step_avg:34.43ms
step:195/1845 train_time:6713ms step_avg:34.42ms
step:196/1845 train_time:6748ms step_avg:34.43ms
step:197/1845 train_time:6781ms step_avg:34.42ms
step:198/1845 train_time:6816ms step_avg:34.43ms
step:199/1845 train_time:6849ms step_avg:34.42ms
step:200/1845 train_time:6884ms step_avg:34.42ms
step:201/1845 train_time:6917ms step_avg:34.41ms
step:202/1845 train_time:6952ms step_avg:34.42ms
step:203/1845 train_time:6986ms step_avg:34.41ms
step:204/1845 train_time:7021ms step_avg:34.42ms
step:205/1845 train_time:7054ms step_avg:34.41ms
step:206/1845 train_time:7089ms step_avg:34.41ms
step:207/1845 train_time:7122ms step_avg:34.41ms
step:208/1845 train_time:7157ms step_avg:34.41ms
step:209/1845 train_time:7190ms step_avg:34.40ms
step:210/1845 train_time:7226ms step_avg:34.41ms
step:211/1845 train_time:7258ms step_avg:34.40ms
step:212/1845 train_time:7294ms step_avg:34.40ms
step:213/1845 train_time:7327ms step_avg:34.40ms
step:214/1845 train_time:7362ms step_avg:34.40ms
step:215/1845 train_time:7395ms step_avg:34.39ms
step:216/1845 train_time:7430ms step_avg:34.40ms
step:217/1845 train_time:7463ms step_avg:34.39ms
step:218/1845 train_time:7499ms step_avg:34.40ms
step:219/1845 train_time:7532ms step_avg:34.39ms
step:220/1845 train_time:7567ms step_avg:34.40ms
step:221/1845 train_time:7600ms step_avg:34.39ms
step:222/1845 train_time:7635ms step_avg:34.39ms
step:223/1845 train_time:7668ms step_avg:34.39ms
step:224/1845 train_time:7703ms step_avg:34.39ms
step:225/1845 train_time:7736ms step_avg:34.38ms
step:226/1845 train_time:7772ms step_avg:34.39ms
step:227/1845 train_time:7804ms step_avg:34.38ms
step:228/1845 train_time:7840ms step_avg:34.38ms
step:229/1845 train_time:7872ms step_avg:34.38ms
step:230/1845 train_time:7908ms step_avg:34.38ms
step:231/1845 train_time:7941ms step_avg:34.38ms
step:232/1845 train_time:7976ms step_avg:34.38ms
step:233/1845 train_time:8009ms step_avg:34.37ms
step:234/1845 train_time:8044ms step_avg:34.38ms
step:235/1845 train_time:8077ms step_avg:34.37ms
step:236/1845 train_time:8113ms step_avg:34.38ms
step:237/1845 train_time:8145ms step_avg:34.37ms
step:238/1845 train_time:8181ms step_avg:34.37ms
step:239/1845 train_time:8214ms step_avg:34.37ms
step:240/1845 train_time:8249ms step_avg:34.37ms
step:241/1845 train_time:8282ms step_avg:34.37ms
step:242/1845 train_time:8317ms step_avg:34.37ms
step:243/1845 train_time:8350ms step_avg:34.36ms
step:244/1845 train_time:8386ms step_avg:34.37ms
step:245/1845 train_time:8419ms step_avg:34.36ms
step:246/1845 train_time:8454ms step_avg:34.37ms
step:247/1845 train_time:8487ms step_avg:34.36ms
step:248/1845 train_time:8522ms step_avg:34.36ms
step:249/1845 train_time:8555ms step_avg:34.36ms
step:250/1845 train_time:8590ms step_avg:34.36ms
step:250/1845 val_loss:4.6085 train_time:8632ms step_avg:34.53ms
step:251/1845 train_time:8649ms step_avg:34.46ms
step:252/1845 train_time:8666ms step_avg:34.39ms
step:253/1845 train_time:8694ms step_avg:34.36ms
step:254/1845 train_time:8729ms step_avg:34.37ms
step:255/1845 train_time:8763ms step_avg:34.36ms
step:256/1845 train_time:8798ms step_avg:34.37ms
step:257/1845 train_time:8831ms step_avg:34.36ms
step:258/1845 train_time:8867ms step_avg:34.37ms
step:259/1845 train_time:8900ms step_avg:34.36ms
step:260/1845 train_time:8935ms step_avg:34.37ms
step:261/1845 train_time:8968ms step_avg:34.36ms
step:262/1845 train_time:9003ms step_avg:34.36ms
step:263/1845 train_time:9036ms step_avg:34.36ms
step:264/1845 train_time:9071ms step_avg:34.36ms
step:265/1845 train_time:9104ms step_avg:34.36ms
step:266/1845 train_time:9139ms step_avg:34.36ms
step:267/1845 train_time:9172ms step_avg:34.35ms
step:268/1845 train_time:9208ms step_avg:34.36ms
step:269/1845 train_time:9241ms step_avg:34.35ms
step:270/1845 train_time:9276ms step_avg:34.36ms
step:271/1845 train_time:9309ms step_avg:34.35ms
step:272/1845 train_time:9344ms step_avg:34.35ms
step:273/1845 train_time:9377ms step_avg:34.35ms
step:274/1845 train_time:9412ms step_avg:34.35ms
step:275/1845 train_time:9445ms step_avg:34.35ms
step:276/1845 train_time:9480ms step_avg:34.35ms
step:277/1845 train_time:9513ms step_avg:34.34ms
step:278/1845 train_time:9548ms step_avg:34.35ms
step:279/1845 train_time:9581ms step_avg:34.34ms
step:280/1845 train_time:9617ms step_avg:34.35ms
step:281/1845 train_time:9650ms step_avg:34.34ms
step:282/1845 train_time:9685ms step_avg:34.34ms
step:283/1845 train_time:9718ms step_avg:34.34ms
step:284/1845 train_time:9753ms step_avg:34.34ms
step:285/1845 train_time:9786ms step_avg:34.34ms
step:286/1845 train_time:9822ms step_avg:34.34ms
step:287/1845 train_time:9855ms step_avg:34.34ms
step:288/1845 train_time:9890ms step_avg:34.34ms
step:289/1845 train_time:9923ms step_avg:34.34ms
step:290/1845 train_time:9958ms step_avg:34.34ms
step:291/1845 train_time:9991ms step_avg:34.33ms
step:292/1845 train_time:10026ms step_avg:34.34ms
step:293/1845 train_time:10059ms step_avg:34.33ms
step:294/1845 train_time:10094ms step_avg:34.33ms
step:295/1845 train_time:10128ms step_avg:34.33ms
step:296/1845 train_time:10163ms step_avg:34.33ms
step:297/1845 train_time:10196ms step_avg:34.33ms
step:298/1845 train_time:10231ms step_avg:34.33ms
step:299/1845 train_time:10264ms step_avg:34.33ms
step:300/1845 train_time:10299ms step_avg:34.33ms
step:301/1845 train_time:10332ms step_avg:34.33ms
step:302/1845 train_time:10367ms step_avg:34.33ms
step:303/1845 train_time:10400ms step_avg:34.32ms
step:304/1845 train_time:10436ms step_avg:34.33ms
step:305/1845 train_time:10469ms step_avg:34.32ms
step:306/1845 train_time:10504ms step_avg:34.33ms
step:307/1845 train_time:10537ms step_avg:34.32ms
step:308/1845 train_time:10572ms step_avg:34.32ms
step:309/1845 train_time:10605ms step_avg:34.32ms
step:310/1845 train_time:10640ms step_avg:34.32ms
step:311/1845 train_time:10673ms step_avg:34.32ms
step:312/1845 train_time:10708ms step_avg:34.32ms
step:313/1845 train_time:10741ms step_avg:34.32ms
step:314/1845 train_time:10777ms step_avg:34.32ms
step:315/1845 train_time:10809ms step_avg:34.32ms
step:316/1845 train_time:10845ms step_avg:34.32ms
step:317/1845 train_time:10877ms step_avg:34.31ms
step:318/1845 train_time:10913ms step_avg:34.32ms
step:319/1845 train_time:10946ms step_avg:34.31ms
step:320/1845 train_time:10981ms step_avg:34.32ms
step:321/1845 train_time:11014ms step_avg:34.31ms
step:322/1845 train_time:11049ms step_avg:34.31ms
step:323/1845 train_time:11082ms step_avg:34.31ms
step:324/1845 train_time:11117ms step_avg:34.31ms
step:325/1845 train_time:11150ms step_avg:34.31ms
step:326/1845 train_time:11186ms step_avg:34.31ms
step:327/1845 train_time:11219ms step_avg:34.31ms
step:328/1845 train_time:11254ms step_avg:34.31ms
step:329/1845 train_time:11287ms step_avg:34.31ms
step:330/1845 train_time:11322ms step_avg:34.31ms
step:331/1845 train_time:11355ms step_avg:34.31ms
step:332/1845 train_time:11390ms step_avg:34.31ms
step:333/1845 train_time:11423ms step_avg:34.30ms
step:334/1845 train_time:11458ms step_avg:34.31ms
step:335/1845 train_time:11491ms step_avg:34.30ms
step:336/1845 train_time:11527ms step_avg:34.31ms
step:337/1845 train_time:11559ms step_avg:34.30ms
step:338/1845 train_time:11595ms step_avg:34.30ms
step:339/1845 train_time:11627ms step_avg:34.30ms
step:340/1845 train_time:11663ms step_avg:34.30ms
step:341/1845 train_time:11696ms step_avg:34.30ms
step:342/1845 train_time:11731ms step_avg:34.30ms
step:343/1845 train_time:11764ms step_avg:34.30ms
step:344/1845 train_time:11799ms step_avg:34.30ms
step:345/1845 train_time:11832ms step_avg:34.30ms
step:346/1845 train_time:11867ms step_avg:34.30ms
step:347/1845 train_time:11900ms step_avg:34.29ms
step:348/1845 train_time:11935ms step_avg:34.30ms
step:349/1845 train_time:11968ms step_avg:34.29ms
step:350/1845 train_time:12003ms step_avg:34.29ms
step:351/1845 train_time:12036ms step_avg:34.29ms
step:352/1845 train_time:12071ms step_avg:34.29ms
step:353/1845 train_time:12104ms step_avg:34.29ms
step:354/1845 train_time:12139ms step_avg:34.29ms
step:355/1845 train_time:12172ms step_avg:34.29ms
step:356/1845 train_time:12207ms step_avg:34.29ms
step:357/1845 train_time:12240ms step_avg:34.29ms
step:358/1845 train_time:12275ms step_avg:34.29ms
step:359/1845 train_time:12308ms step_avg:34.29ms
step:360/1845 train_time:12344ms step_avg:34.29ms
step:361/1845 train_time:12377ms step_avg:34.28ms
step:362/1845 train_time:12412ms step_avg:34.29ms
step:363/1845 train_time:12445ms step_avg:34.28ms
step:364/1845 train_time:12480ms step_avg:34.29ms
step:365/1845 train_time:12513ms step_avg:34.28ms
step:366/1845 train_time:12548ms step_avg:34.28ms
step:367/1845 train_time:12581ms step_avg:34.28ms
step:368/1845 train_time:12616ms step_avg:34.28ms
step:369/1845 train_time:12649ms step_avg:34.28ms
step:370/1845 train_time:12684ms step_avg:34.28ms
step:371/1845 train_time:12717ms step_avg:34.28ms
step:372/1845 train_time:12753ms step_avg:34.28ms
step:373/1845 train_time:12785ms step_avg:34.28ms
step:374/1845 train_time:12821ms step_avg:34.28ms
step:375/1845 train_time:12854ms step_avg:34.28ms
step:376/1845 train_time:12889ms step_avg:34.28ms
step:377/1845 train_time:12922ms step_avg:34.28ms
step:378/1845 train_time:12957ms step_avg:34.28ms
step:379/1845 train_time:12990ms step_avg:34.27ms
step:380/1845 train_time:13025ms step_avg:34.28ms
step:381/1845 train_time:13058ms step_avg:34.27ms
step:382/1845 train_time:13094ms step_avg:34.28ms
step:383/1845 train_time:13126ms step_avg:34.27ms
step:384/1845 train_time:13162ms step_avg:34.28ms
step:385/1845 train_time:13194ms step_avg:34.27ms
step:386/1845 train_time:13230ms step_avg:34.27ms
step:387/1845 train_time:13263ms step_avg:34.27ms
step:388/1845 train_time:13298ms step_avg:34.27ms
step:389/1845 train_time:13331ms step_avg:34.27ms
step:390/1845 train_time:13366ms step_avg:34.27ms
step:391/1845 train_time:13399ms step_avg:34.27ms
step:392/1845 train_time:13434ms step_avg:34.27ms
step:393/1845 train_time:13467ms step_avg:34.27ms
step:394/1845 train_time:13502ms step_avg:34.27ms
step:395/1845 train_time:13535ms step_avg:34.27ms
step:396/1845 train_time:13570ms step_avg:34.27ms
step:397/1845 train_time:13603ms step_avg:34.26ms
step:398/1845 train_time:13638ms step_avg:34.27ms
step:399/1845 train_time:13671ms step_avg:34.26ms
step:400/1845 train_time:13706ms step_avg:34.27ms
step:401/1845 train_time:13739ms step_avg:34.26ms
step:402/1845 train_time:13774ms step_avg:34.26ms
step:403/1845 train_time:13807ms step_avg:34.26ms
step:404/1845 train_time:13842ms step_avg:34.26ms
step:405/1845 train_time:13875ms step_avg:34.26ms
step:406/1845 train_time:13911ms step_avg:34.26ms
step:407/1845 train_time:13943ms step_avg:34.26ms
step:408/1845 train_time:13979ms step_avg:34.26ms
step:409/1845 train_time:14012ms step_avg:34.26ms
step:410/1845 train_time:14047ms step_avg:34.26ms
step:411/1845 train_time:14080ms step_avg:34.26ms
step:412/1845 train_time:14115ms step_avg:34.26ms
step:413/1845 train_time:14148ms step_avg:34.26ms
step:414/1845 train_time:14183ms step_avg:34.26ms
step:415/1845 train_time:14216ms step_avg:34.26ms
step:416/1845 train_time:14252ms step_avg:34.26ms
step:417/1845 train_time:14284ms step_avg:34.26ms
step:418/1845 train_time:14320ms step_avg:34.26ms
step:419/1845 train_time:14353ms step_avg:34.25ms
step:420/1845 train_time:14388ms step_avg:34.26ms
step:421/1845 train_time:14421ms step_avg:34.25ms
step:422/1845 train_time:14456ms step_avg:34.26ms
step:423/1845 train_time:14489ms step_avg:34.25ms
step:424/1845 train_time:14524ms step_avg:34.26ms
step:425/1845 train_time:14558ms step_avg:34.25ms
step:426/1845 train_time:14593ms step_avg:34.26ms
step:427/1845 train_time:14626ms step_avg:34.25ms
step:428/1845 train_time:14661ms step_avg:34.26ms
step:429/1845 train_time:14694ms step_avg:34.25ms
step:430/1845 train_time:14729ms step_avg:34.25ms
step:431/1845 train_time:14762ms step_avg:34.25ms
step:432/1845 train_time:14798ms step_avg:34.25ms
step:433/1845 train_time:14831ms step_avg:34.25ms
step:434/1845 train_time:14866ms step_avg:34.25ms
step:435/1845 train_time:14899ms step_avg:34.25ms
step:436/1845 train_time:14934ms step_avg:34.25ms
step:437/1845 train_time:14967ms step_avg:34.25ms
step:438/1845 train_time:15002ms step_avg:34.25ms
step:439/1845 train_time:15035ms step_avg:34.25ms
step:440/1845 train_time:15070ms step_avg:34.25ms
step:441/1845 train_time:15103ms step_avg:34.25ms
step:442/1845 train_time:15139ms step_avg:34.25ms
step:443/1845 train_time:15171ms step_avg:34.25ms
step:444/1845 train_time:15207ms step_avg:34.25ms
step:445/1845 train_time:15240ms step_avg:34.25ms
step:446/1845 train_time:15275ms step_avg:34.25ms
step:447/1845 train_time:15308ms step_avg:34.25ms
step:448/1845 train_time:15344ms step_avg:34.25ms
step:449/1845 train_time:15377ms step_avg:34.25ms
step:450/1845 train_time:15412ms step_avg:34.25ms
step:451/1845 train_time:15445ms step_avg:34.25ms
step:452/1845 train_time:15480ms step_avg:34.25ms
step:453/1845 train_time:15513ms step_avg:34.24ms
step:454/1845 train_time:15548ms step_avg:34.25ms
step:455/1845 train_time:15581ms step_avg:34.24ms
step:456/1845 train_time:15616ms step_avg:34.25ms
step:457/1845 train_time:15649ms step_avg:34.24ms
step:458/1845 train_time:15684ms step_avg:34.25ms
step:459/1845 train_time:15717ms step_avg:34.24ms
step:460/1845 train_time:15753ms step_avg:34.24ms
step:461/1845 train_time:15785ms step_avg:34.24ms
step:462/1845 train_time:15821ms step_avg:34.24ms
step:463/1845 train_time:15854ms step_avg:34.24ms
step:464/1845 train_time:15889ms step_avg:34.24ms
step:465/1845 train_time:15922ms step_avg:34.24ms
step:466/1845 train_time:15957ms step_avg:34.24ms
step:467/1845 train_time:15990ms step_avg:34.24ms
step:468/1845 train_time:16025ms step_avg:34.24ms
step:469/1845 train_time:16058ms step_avg:34.24ms
step:470/1845 train_time:16093ms step_avg:34.24ms
step:471/1845 train_time:16126ms step_avg:34.24ms
step:472/1845 train_time:16161ms step_avg:34.24ms
step:473/1845 train_time:16194ms step_avg:34.24ms
step:474/1845 train_time:16229ms step_avg:34.24ms
step:475/1845 train_time:16262ms step_avg:34.24ms
step:476/1845 train_time:16298ms step_avg:34.24ms
step:477/1845 train_time:16330ms step_avg:34.24ms
step:478/1845 train_time:16366ms step_avg:34.24ms
step:479/1845 train_time:16399ms step_avg:34.24ms
step:480/1845 train_time:16434ms step_avg:34.24ms
step:481/1845 train_time:16467ms step_avg:34.23ms
step:482/1845 train_time:16502ms step_avg:34.24ms
step:483/1845 train_time:16535ms step_avg:34.23ms
step:484/1845 train_time:16571ms step_avg:34.24ms
step:485/1845 train_time:16604ms step_avg:34.23ms
step:486/1845 train_time:16639ms step_avg:34.24ms
step:487/1845 train_time:16672ms step_avg:34.23ms
step:488/1845 train_time:16707ms step_avg:34.24ms
step:489/1845 train_time:16740ms step_avg:34.23ms
step:490/1845 train_time:16775ms step_avg:34.24ms
step:491/1845 train_time:16808ms step_avg:34.23ms
step:492/1845 train_time:16844ms step_avg:34.23ms
step:493/1845 train_time:16876ms step_avg:34.23ms
step:494/1845 train_time:16912ms step_avg:34.23ms
step:495/1845 train_time:16945ms step_avg:34.23ms
step:496/1845 train_time:16980ms step_avg:34.23ms
step:497/1845 train_time:17013ms step_avg:34.23ms
step:498/1845 train_time:17048ms step_avg:34.23ms
step:499/1845 train_time:17081ms step_avg:34.23ms
step:500/1845 train_time:17116ms step_avg:34.23ms
step:500/1845 val_loss:4.2925 train_time:17158ms step_avg:34.32ms
step:501/1845 train_time:17175ms step_avg:34.28ms
step:502/1845 train_time:17193ms step_avg:34.25ms
step:503/1845 train_time:17220ms step_avg:34.23ms
step:504/1845 train_time:17255ms step_avg:34.24ms
step:505/1845 train_time:17289ms step_avg:34.23ms
step:506/1845 train_time:17325ms step_avg:34.24ms
step:507/1845 train_time:17358ms step_avg:34.24ms
step:508/1845 train_time:17394ms step_avg:34.24ms
step:509/1845 train_time:17427ms step_avg:34.24ms
step:510/1845 train_time:17463ms step_avg:34.24ms
step:511/1845 train_time:17495ms step_avg:34.24ms
step:512/1845 train_time:17531ms step_avg:34.24ms
step:513/1845 train_time:17563ms step_avg:34.24ms
step:514/1845 train_time:17599ms step_avg:34.24ms
step:515/1845 train_time:17632ms step_avg:34.24ms
step:516/1845 train_time:17667ms step_avg:34.24ms
step:517/1845 train_time:17700ms step_avg:34.24ms
step:518/1845 train_time:17735ms step_avg:34.24ms
step:519/1845 train_time:17768ms step_avg:34.23ms
step:520/1845 train_time:17803ms step_avg:34.24ms
step:521/1845 train_time:17836ms step_avg:34.23ms
step:522/1845 train_time:17871ms step_avg:34.24ms
step:523/1845 train_time:17904ms step_avg:34.23ms
step:524/1845 train_time:17939ms step_avg:34.24ms
step:525/1845 train_time:17972ms step_avg:34.23ms
step:526/1845 train_time:18008ms step_avg:34.23ms
step:527/1845 train_time:18040ms step_avg:34.23ms
step:528/1845 train_time:18076ms step_avg:34.23ms
step:529/1845 train_time:18109ms step_avg:34.23ms
step:530/1845 train_time:18144ms step_avg:34.23ms
step:531/1845 train_time:18177ms step_avg:34.23ms
step:532/1845 train_time:18212ms step_avg:34.23ms
step:533/1845 train_time:18245ms step_avg:34.23ms
step:534/1845 train_time:18280ms step_avg:34.23ms
step:535/1845 train_time:18313ms step_avg:34.23ms
step:536/1845 train_time:18348ms step_avg:34.23ms
step:537/1845 train_time:18382ms step_avg:34.23ms
step:538/1845 train_time:18417ms step_avg:34.23ms
step:539/1845 train_time:18450ms step_avg:34.23ms
step:540/1845 train_time:18485ms step_avg:34.23ms
step:541/1845 train_time:18518ms step_avg:34.23ms
step:542/1845 train_time:18553ms step_avg:34.23ms
step:543/1845 train_time:18586ms step_avg:34.23ms
step:544/1845 train_time:18622ms step_avg:34.23ms
step:545/1845 train_time:18655ms step_avg:34.23ms
step:546/1845 train_time:18690ms step_avg:34.23ms
step:547/1845 train_time:18723ms step_avg:34.23ms
step:548/1845 train_time:18758ms step_avg:34.23ms
step:549/1845 train_time:18791ms step_avg:34.23ms
step:550/1845 train_time:18826ms step_avg:34.23ms
step:551/1845 train_time:18859ms step_avg:34.23ms
step:552/1845 train_time:18894ms step_avg:34.23ms
step:553/1845 train_time:18927ms step_avg:34.23ms
step:554/1845 train_time:18963ms step_avg:34.23ms
step:555/1845 train_time:18996ms step_avg:34.23ms
step:556/1845 train_time:19031ms step_avg:34.23ms
step:557/1845 train_time:19064ms step_avg:34.23ms
step:558/1845 train_time:19099ms step_avg:34.23ms
step:559/1845 train_time:19132ms step_avg:34.23ms
step:560/1845 train_time:19167ms step_avg:34.23ms
step:561/1845 train_time:19200ms step_avg:34.22ms
step:562/1845 train_time:19235ms step_avg:34.23ms
step:563/1845 train_time:19268ms step_avg:34.22ms
step:564/1845 train_time:19303ms step_avg:34.23ms
step:565/1845 train_time:19337ms step_avg:34.22ms
step:566/1845 train_time:19372ms step_avg:34.23ms
step:567/1845 train_time:19405ms step_avg:34.22ms
step:568/1845 train_time:19440ms step_avg:34.23ms
step:569/1845 train_time:19473ms step_avg:34.22ms
step:570/1845 train_time:19508ms step_avg:34.23ms
step:571/1845 train_time:19541ms step_avg:34.22ms
step:572/1845 train_time:19576ms step_avg:34.22ms
step:573/1845 train_time:19609ms step_avg:34.22ms
step:574/1845 train_time:19645ms step_avg:34.22ms
step:575/1845 train_time:19677ms step_avg:34.22ms
step:576/1845 train_time:19712ms step_avg:34.22ms
step:577/1845 train_time:19745ms step_avg:34.22ms
step:578/1845 train_time:19781ms step_avg:34.22ms
step:579/1845 train_time:19814ms step_avg:34.22ms
step:580/1845 train_time:19849ms step_avg:34.22ms
step:581/1845 train_time:19882ms step_avg:34.22ms
step:582/1845 train_time:19917ms step_avg:34.22ms
step:583/1845 train_time:19950ms step_avg:34.22ms
step:584/1845 train_time:19985ms step_avg:34.22ms
step:585/1845 train_time:20018ms step_avg:34.22ms
step:586/1845 train_time:20054ms step_avg:34.22ms
step:587/1845 train_time:20087ms step_avg:34.22ms
step:588/1845 train_time:20122ms step_avg:34.22ms
step:589/1845 train_time:20155ms step_avg:34.22ms
step:590/1845 train_time:20190ms step_avg:34.22ms
step:591/1845 train_time:20223ms step_avg:34.22ms
step:592/1845 train_time:20258ms step_avg:34.22ms
step:593/1845 train_time:20291ms step_avg:34.22ms
step:594/1845 train_time:20327ms step_avg:34.22ms
step:595/1845 train_time:20359ms step_avg:34.22ms
step:596/1845 train_time:20395ms step_avg:34.22ms
step:597/1845 train_time:20428ms step_avg:34.22ms
step:598/1845 train_time:20463ms step_avg:34.22ms
step:599/1845 train_time:20496ms step_avg:34.22ms
step:600/1845 train_time:20531ms step_avg:34.22ms
step:601/1845 train_time:20564ms step_avg:34.22ms
step:602/1845 train_time:20600ms step_avg:34.22ms
step:603/1845 train_time:20634ms step_avg:34.22ms
step:604/1845 train_time:20694ms step_avg:34.26ms
step:605/1845 train_time:20753ms step_avg:34.30ms
step:606/1845 train_time:20816ms step_avg:34.35ms
step:607/1845 train_time:20876ms step_avg:34.39ms
step:608/1845 train_time:20939ms step_avg:34.44ms
step:609/1845 train_time:20999ms step_avg:34.48ms
step:610/1845 train_time:21062ms step_avg:34.53ms
step:611/1845 train_time:21122ms step_avg:34.57ms
step:612/1845 train_time:21185ms step_avg:34.62ms
step:613/1845 train_time:21246ms step_avg:34.66ms
step:614/1845 train_time:21308ms step_avg:34.70ms
step:615/1845 train_time:21368ms step_avg:34.75ms
step:616/1845 train_time:21431ms step_avg:34.79ms
step:617/1845 train_time:21492ms step_avg:34.83ms
step:618/1845 train_time:21556ms step_avg:34.88ms
step:619/1845 train_time:21616ms step_avg:34.92ms
step:620/1845 train_time:21678ms step_avg:34.96ms
step:621/1845 train_time:21738ms step_avg:35.01ms
step:622/1845 train_time:21801ms step_avg:35.05ms
step:623/1845 train_time:21861ms step_avg:35.09ms
step:624/1845 train_time:21924ms step_avg:35.13ms
step:625/1845 train_time:21984ms step_avg:35.17ms
step:626/1845 train_time:22046ms step_avg:35.22ms
step:627/1845 train_time:22106ms step_avg:35.26ms
step:628/1845 train_time:22169ms step_avg:35.30ms
step:629/1845 train_time:22229ms step_avg:35.34ms
step:630/1845 train_time:22292ms step_avg:35.38ms
step:631/1845 train_time:22352ms step_avg:35.42ms
step:632/1845 train_time:22415ms step_avg:35.47ms
step:633/1845 train_time:22475ms step_avg:35.51ms
step:634/1845 train_time:22538ms step_avg:35.55ms
step:635/1845 train_time:22598ms step_avg:35.59ms
step:636/1845 train_time:22660ms step_avg:35.63ms
step:637/1845 train_time:22721ms step_avg:35.67ms
step:638/1845 train_time:22783ms step_avg:35.71ms
step:639/1845 train_time:22843ms step_avg:35.75ms
step:640/1845 train_time:22906ms step_avg:35.79ms
step:641/1845 train_time:22966ms step_avg:35.83ms
step:642/1845 train_time:23029ms step_avg:35.87ms
step:643/1845 train_time:23089ms step_avg:35.91ms
step:644/1845 train_time:23151ms step_avg:35.95ms
step:645/1845 train_time:23213ms step_avg:35.99ms
step:646/1845 train_time:23276ms step_avg:36.03ms
step:647/1845 train_time:23336ms step_avg:36.07ms
step:648/1845 train_time:23400ms step_avg:36.11ms
step:649/1845 train_time:23460ms step_avg:36.15ms
step:650/1845 train_time:23523ms step_avg:36.19ms
step:651/1845 train_time:23583ms step_avg:36.23ms
step:652/1845 train_time:23646ms step_avg:36.27ms
step:653/1845 train_time:23706ms step_avg:36.30ms
step:654/1845 train_time:23769ms step_avg:36.34ms
step:655/1845 train_time:23829ms step_avg:36.38ms
step:656/1845 train_time:23892ms step_avg:36.42ms
step:657/1845 train_time:23951ms step_avg:36.46ms
step:658/1845 train_time:24014ms step_avg:36.50ms
step:659/1845 train_time:24074ms step_avg:36.53ms
step:660/1845 train_time:24137ms step_avg:36.57ms
step:661/1845 train_time:24197ms step_avg:36.61ms
step:662/1845 train_time:24260ms step_avg:36.65ms
step:663/1845 train_time:24320ms step_avg:36.68ms
step:664/1845 train_time:24383ms step_avg:36.72ms
step:665/1845 train_time:24443ms step_avg:36.76ms
step:666/1845 train_time:24506ms step_avg:36.80ms
step:667/1845 train_time:24566ms step_avg:36.83ms
step:668/1845 train_time:24629ms step_avg:36.87ms
step:669/1845 train_time:24689ms step_avg:36.90ms
step:670/1845 train_time:24753ms step_avg:36.94ms
step:671/1845 train_time:24814ms step_avg:36.98ms
step:672/1845 train_time:24877ms step_avg:37.02ms
step:673/1845 train_time:24936ms step_avg:37.05ms
step:674/1845 train_time:25000ms step_avg:37.09ms
step:675/1845 train_time:25060ms step_avg:37.13ms
step:676/1845 train_time:25123ms step_avg:37.16ms
step:677/1845 train_time:25182ms step_avg:37.20ms
step:678/1845 train_time:25245ms step_avg:37.23ms
step:679/1845 train_time:25305ms step_avg:37.27ms
step:680/1845 train_time:25368ms step_avg:37.31ms
step:681/1845 train_time:25428ms step_avg:37.34ms
step:682/1845 train_time:25490ms step_avg:37.38ms
step:683/1845 train_time:25550ms step_avg:37.41ms
step:684/1845 train_time:25614ms step_avg:37.45ms
step:685/1845 train_time:25675ms step_avg:37.48ms
step:686/1845 train_time:25738ms step_avg:37.52ms
step:687/1845 train_time:25798ms step_avg:37.55ms
step:688/1845 train_time:25861ms step_avg:37.59ms
step:689/1845 train_time:25921ms step_avg:37.62ms
step:690/1845 train_time:25984ms step_avg:37.66ms
step:691/1845 train_time:26044ms step_avg:37.69ms
step:692/1845 train_time:26107ms step_avg:37.73ms
step:693/1845 train_time:26167ms step_avg:37.76ms
step:694/1845 train_time:26230ms step_avg:37.79ms
step:695/1845 train_time:26290ms step_avg:37.83ms
step:696/1845 train_time:26353ms step_avg:37.86ms
step:697/1845 train_time:26413ms step_avg:37.90ms
step:698/1845 train_time:26476ms step_avg:37.93ms
step:699/1845 train_time:26536ms step_avg:37.96ms
step:700/1845 train_time:26599ms step_avg:38.00ms
step:701/1845 train_time:26660ms step_avg:38.03ms
step:702/1845 train_time:26722ms step_avg:38.07ms
step:703/1845 train_time:26782ms step_avg:38.10ms
step:704/1845 train_time:26846ms step_avg:38.13ms
step:705/1845 train_time:26906ms step_avg:38.16ms
step:706/1845 train_time:26969ms step_avg:38.20ms
step:707/1845 train_time:27029ms step_avg:38.23ms
step:708/1845 train_time:27092ms step_avg:38.27ms
step:709/1845 train_time:27152ms step_avg:38.30ms
step:710/1845 train_time:27215ms step_avg:38.33ms
step:711/1845 train_time:27275ms step_avg:38.36ms
step:712/1845 train_time:27338ms step_avg:38.40ms
step:713/1845 train_time:27397ms step_avg:38.43ms
step:714/1845 train_time:27460ms step_avg:38.46ms
step:715/1845 train_time:27520ms step_avg:38.49ms
step:716/1845 train_time:27583ms step_avg:38.52ms
step:717/1845 train_time:27643ms step_avg:38.55ms
step:718/1845 train_time:27706ms step_avg:38.59ms
step:719/1845 train_time:27767ms step_avg:38.62ms
step:720/1845 train_time:27829ms step_avg:38.65ms
step:721/1845 train_time:27890ms step_avg:38.68ms
step:722/1845 train_time:27953ms step_avg:38.72ms
step:723/1845 train_time:28013ms step_avg:38.75ms
step:724/1845 train_time:28076ms step_avg:38.78ms
step:725/1845 train_time:28137ms step_avg:38.81ms
step:726/1845 train_time:28200ms step_avg:38.84ms
step:727/1845 train_time:28260ms step_avg:38.87ms
step:728/1845 train_time:28323ms step_avg:38.90ms
step:729/1845 train_time:28382ms step_avg:38.93ms
step:730/1845 train_time:28446ms step_avg:38.97ms
step:731/1845 train_time:28506ms step_avg:39.00ms
step:732/1845 train_time:28568ms step_avg:39.03ms
step:733/1845 train_time:28628ms step_avg:39.06ms
step:734/1845 train_time:28691ms step_avg:39.09ms
step:735/1845 train_time:28751ms step_avg:39.12ms
step:736/1845 train_time:28814ms step_avg:39.15ms
step:737/1845 train_time:28873ms step_avg:39.18ms
step:738/1845 train_time:28936ms step_avg:39.21ms
step:739/1845 train_time:28997ms step_avg:39.24ms
step:740/1845 train_time:29059ms step_avg:39.27ms
step:741/1845 train_time:29120ms step_avg:39.30ms
step:742/1845 train_time:29183ms step_avg:39.33ms
step:743/1845 train_time:29243ms step_avg:39.36ms
step:744/1845 train_time:29306ms step_avg:39.39ms
step:745/1845 train_time:29366ms step_avg:39.42ms
step:746/1845 train_time:29429ms step_avg:39.45ms
step:747/1845 train_time:29489ms step_avg:39.48ms
step:748/1845 train_time:29552ms step_avg:39.51ms
step:749/1845 train_time:29612ms step_avg:39.54ms
step:750/1845 train_time:29675ms step_avg:39.57ms
step:750/1845 val_loss:4.0231 train_time:29745ms step_avg:39.66ms
step:751/1845 train_time:29762ms step_avg:39.63ms
step:752/1845 train_time:29799ms step_avg:39.63ms
step:753/1845 train_time:29861ms step_avg:39.66ms
step:754/1845 train_time:29925ms step_avg:39.69ms
step:755/1845 train_time:29986ms step_avg:39.72ms
step:756/1845 train_time:30049ms step_avg:39.75ms
step:757/1845 train_time:30109ms step_avg:39.77ms
step:758/1845 train_time:30171ms step_avg:39.80ms
step:759/1845 train_time:30233ms step_avg:39.83ms
step:760/1845 train_time:30296ms step_avg:39.86ms
step:761/1845 train_time:30356ms step_avg:39.89ms
step:762/1845 train_time:30418ms step_avg:39.92ms
step:763/1845 train_time:30478ms step_avg:39.94ms
step:764/1845 train_time:30540ms step_avg:39.97ms
step:765/1845 train_time:30600ms step_avg:40.00ms
step:766/1845 train_time:30662ms step_avg:40.03ms
step:767/1845 train_time:30723ms step_avg:40.06ms
step:768/1845 train_time:30787ms step_avg:40.09ms
step:769/1845 train_time:30849ms step_avg:40.12ms
step:770/1845 train_time:30912ms step_avg:40.15ms
step:771/1845 train_time:30973ms step_avg:40.17ms
step:772/1845 train_time:31037ms step_avg:40.20ms
step:773/1845 train_time:31097ms step_avg:40.23ms
step:774/1845 train_time:31160ms step_avg:40.26ms
step:775/1845 train_time:31221ms step_avg:40.29ms
step:776/1845 train_time:31284ms step_avg:40.31ms
step:777/1845 train_time:31344ms step_avg:40.34ms
step:778/1845 train_time:31406ms step_avg:40.37ms
step:779/1845 train_time:31466ms step_avg:40.39ms
step:780/1845 train_time:31528ms step_avg:40.42ms
step:781/1845 train_time:31588ms step_avg:40.45ms
step:782/1845 train_time:31650ms step_avg:40.47ms
step:783/1845 train_time:31711ms step_avg:40.50ms
step:784/1845 train_time:31774ms step_avg:40.53ms
step:785/1845 train_time:31835ms step_avg:40.55ms
step:786/1845 train_time:31898ms step_avg:40.58ms
step:787/1845 train_time:31959ms step_avg:40.61ms
step:788/1845 train_time:32023ms step_avg:40.64ms
step:789/1845 train_time:32083ms step_avg:40.66ms
step:790/1845 train_time:32146ms step_avg:40.69ms
step:791/1845 train_time:32207ms step_avg:40.72ms
step:792/1845 train_time:32270ms step_avg:40.74ms
step:793/1845 train_time:32330ms step_avg:40.77ms
step:794/1845 train_time:32392ms step_avg:40.80ms
step:795/1845 train_time:32453ms step_avg:40.82ms
step:796/1845 train_time:32516ms step_avg:40.85ms
step:797/1845 train_time:32576ms step_avg:40.87ms
step:798/1845 train_time:32639ms step_avg:40.90ms
step:799/1845 train_time:32699ms step_avg:40.92ms
step:800/1845 train_time:32761ms step_avg:40.95ms
step:801/1845 train_time:32822ms step_avg:40.98ms
step:802/1845 train_time:32885ms step_avg:41.00ms
step:803/1845 train_time:32945ms step_avg:41.03ms
step:804/1845 train_time:33008ms step_avg:41.05ms
step:805/1845 train_time:33068ms step_avg:41.08ms
step:806/1845 train_time:33130ms step_avg:41.10ms
step:807/1845 train_time:33191ms step_avg:41.13ms
step:808/1845 train_time:33253ms step_avg:41.16ms
step:809/1845 train_time:33314ms step_avg:41.18ms
step:810/1845 train_time:33377ms step_avg:41.21ms
step:811/1845 train_time:33437ms step_avg:41.23ms
step:812/1845 train_time:33500ms step_avg:41.26ms
step:813/1845 train_time:33560ms step_avg:41.28ms
step:814/1845 train_time:33622ms step_avg:41.31ms
step:815/1845 train_time:33682ms step_avg:41.33ms
step:816/1845 train_time:33745ms step_avg:41.35ms
step:817/1845 train_time:33805ms step_avg:41.38ms
step:818/1845 train_time:33868ms step_avg:41.40ms
step:819/1845 train_time:33929ms step_avg:41.43ms
step:820/1845 train_time:33991ms step_avg:41.45ms
step:821/1845 train_time:34051ms step_avg:41.47ms
step:822/1845 train_time:34114ms step_avg:41.50ms
step:823/1845 train_time:34175ms step_avg:41.52ms
step:824/1845 train_time:34238ms step_avg:41.55ms
step:825/1845 train_time:34297ms step_avg:41.57ms
step:826/1845 train_time:34361ms step_avg:41.60ms
step:827/1845 train_time:34420ms step_avg:41.62ms
step:828/1845 train_time:34484ms step_avg:41.65ms
step:829/1845 train_time:34544ms step_avg:41.67ms
step:830/1845 train_time:34607ms step_avg:41.69ms
step:831/1845 train_time:34667ms step_avg:41.72ms
step:832/1845 train_time:34730ms step_avg:41.74ms
step:833/1845 train_time:34790ms step_avg:41.76ms
step:834/1845 train_time:34852ms step_avg:41.79ms
step:835/1845 train_time:34913ms step_avg:41.81ms
step:836/1845 train_time:34977ms step_avg:41.84ms
step:837/1845 train_time:35036ms step_avg:41.86ms
step:838/1845 train_time:35099ms step_avg:41.88ms
step:839/1845 train_time:35160ms step_avg:41.91ms
step:840/1845 train_time:35223ms step_avg:41.93ms
step:841/1845 train_time:35284ms step_avg:41.95ms
step:842/1845 train_time:35347ms step_avg:41.98ms
step:843/1845 train_time:35408ms step_avg:42.00ms
step:844/1845 train_time:35471ms step_avg:42.03ms
step:845/1845 train_time:35530ms step_avg:42.05ms
step:846/1845 train_time:35593ms step_avg:42.07ms
step:847/1845 train_time:35653ms step_avg:42.09ms
step:848/1845 train_time:35717ms step_avg:42.12ms
step:849/1845 train_time:35778ms step_avg:42.14ms
step:850/1845 train_time:35841ms step_avg:42.17ms
step:851/1845 train_time:35901ms step_avg:42.19ms
step:852/1845 train_time:35965ms step_avg:42.21ms
step:853/1845 train_time:36025ms step_avg:42.23ms
step:854/1845 train_time:36088ms step_avg:42.26ms
step:855/1845 train_time:36148ms step_avg:42.28ms
step:856/1845 train_time:36211ms step_avg:42.30ms
step:857/1845 train_time:36271ms step_avg:42.32ms
step:858/1845 train_time:36334ms step_avg:42.35ms
step:859/1845 train_time:36394ms step_avg:42.37ms
step:860/1845 train_time:36457ms step_avg:42.39ms
step:861/1845 train_time:36517ms step_avg:42.41ms
step:862/1845 train_time:36580ms step_avg:42.44ms
step:863/1845 train_time:36641ms step_avg:42.46ms
step:864/1845 train_time:36703ms step_avg:42.48ms
step:865/1845 train_time:36764ms step_avg:42.50ms
step:866/1845 train_time:36827ms step_avg:42.53ms
step:867/1845 train_time:36888ms step_avg:42.55ms
step:868/1845 train_time:36950ms step_avg:42.57ms
step:869/1845 train_time:37011ms step_avg:42.59ms
step:870/1845 train_time:37074ms step_avg:42.61ms
step:871/1845 train_time:37134ms step_avg:42.63ms
step:872/1845 train_time:37196ms step_avg:42.66ms
step:873/1845 train_time:37256ms step_avg:42.68ms
step:874/1845 train_time:37319ms step_avg:42.70ms
step:875/1845 train_time:37380ms step_avg:42.72ms
step:876/1845 train_time:37443ms step_avg:42.74ms
step:877/1845 train_time:37503ms step_avg:42.76ms
step:878/1845 train_time:37565ms step_avg:42.79ms
step:879/1845 train_time:37625ms step_avg:42.80ms
step:880/1845 train_time:37688ms step_avg:42.83ms
step:881/1845 train_time:37748ms step_avg:42.85ms
step:882/1845 train_time:37812ms step_avg:42.87ms
step:883/1845 train_time:37872ms step_avg:42.89ms
step:884/1845 train_time:37935ms step_avg:42.91ms
step:885/1845 train_time:37995ms step_avg:42.93ms
step:886/1845 train_time:38059ms step_avg:42.96ms
step:887/1845 train_time:38119ms step_avg:42.98ms
step:888/1845 train_time:38182ms step_avg:43.00ms
step:889/1845 train_time:38242ms step_avg:43.02ms
step:890/1845 train_time:38305ms step_avg:43.04ms
step:891/1845 train_time:38366ms step_avg:43.06ms
step:892/1845 train_time:38429ms step_avg:43.08ms
step:893/1845 train_time:38489ms step_avg:43.10ms
step:894/1845 train_time:38551ms step_avg:43.12ms
step:895/1845 train_time:38612ms step_avg:43.14ms
step:896/1845 train_time:38674ms step_avg:43.16ms
step:897/1845 train_time:38735ms step_avg:43.18ms
step:898/1845 train_time:38798ms step_avg:43.21ms
step:899/1845 train_time:38858ms step_avg:43.22ms
step:900/1845 train_time:38921ms step_avg:43.25ms
step:901/1845 train_time:38982ms step_avg:43.26ms
step:902/1845 train_time:39045ms step_avg:43.29ms
step:903/1845 train_time:39105ms step_avg:43.31ms
step:904/1845 train_time:39167ms step_avg:43.33ms
step:905/1845 train_time:39228ms step_avg:43.35ms
step:906/1845 train_time:39290ms step_avg:43.37ms
step:907/1845 train_time:39350ms step_avg:43.38ms
step:908/1845 train_time:39413ms step_avg:43.41ms
step:909/1845 train_time:39474ms step_avg:43.43ms
step:910/1845 train_time:39537ms step_avg:43.45ms
step:911/1845 train_time:39597ms step_avg:43.47ms
step:912/1845 train_time:39660ms step_avg:43.49ms
step:913/1845 train_time:39720ms step_avg:43.51ms
step:914/1845 train_time:39783ms step_avg:43.53ms
step:915/1845 train_time:39843ms step_avg:43.54ms
step:916/1845 train_time:39906ms step_avg:43.57ms
step:917/1845 train_time:39967ms step_avg:43.58ms
step:918/1845 train_time:40031ms step_avg:43.61ms
step:919/1845 train_time:40090ms step_avg:43.62ms
step:920/1845 train_time:40153ms step_avg:43.64ms
step:921/1845 train_time:40213ms step_avg:43.66ms
step:922/1845 train_time:40276ms step_avg:43.68ms
step:923/1845 train_time:40337ms step_avg:43.70ms
step:924/1845 train_time:40400ms step_avg:43.72ms
step:925/1845 train_time:40460ms step_avg:43.74ms
step:926/1845 train_time:40523ms step_avg:43.76ms
step:927/1845 train_time:40584ms step_avg:43.78ms
step:928/1845 train_time:40646ms step_avg:43.80ms
step:929/1845 train_time:40706ms step_avg:43.82ms
step:930/1845 train_time:40769ms step_avg:43.84ms
step:931/1845 train_time:40829ms step_avg:43.86ms
step:932/1845 train_time:40893ms step_avg:43.88ms
step:933/1845 train_time:40953ms step_avg:43.89ms
step:934/1845 train_time:41016ms step_avg:43.91ms
step:935/1845 train_time:41077ms step_avg:43.93ms
step:936/1845 train_time:41139ms step_avg:43.95ms
step:937/1845 train_time:41199ms step_avg:43.97ms
step:938/1845 train_time:41262ms step_avg:43.99ms
step:939/1845 train_time:41322ms step_avg:44.01ms
step:940/1845 train_time:41385ms step_avg:44.03ms
step:941/1845 train_time:41445ms step_avg:44.04ms
step:942/1845 train_time:41508ms step_avg:44.06ms
step:943/1845 train_time:41569ms step_avg:44.08ms
step:944/1845 train_time:41632ms step_avg:44.10ms
step:945/1845 train_time:41692ms step_avg:44.12ms
step:946/1845 train_time:41755ms step_avg:44.14ms
step:947/1845 train_time:41815ms step_avg:44.16ms
step:948/1845 train_time:41879ms step_avg:44.18ms
step:949/1845 train_time:41939ms step_avg:44.19ms
step:950/1845 train_time:42002ms step_avg:44.21ms
step:951/1845 train_time:42063ms step_avg:44.23ms
step:952/1845 train_time:42126ms step_avg:44.25ms
step:953/1845 train_time:42186ms step_avg:44.27ms
step:954/1845 train_time:42248ms step_avg:44.29ms
step:955/1845 train_time:42309ms step_avg:44.30ms
step:956/1845 train_time:42372ms step_avg:44.32ms
step:957/1845 train_time:42432ms step_avg:44.34ms
step:958/1845 train_time:42495ms step_avg:44.36ms
step:959/1845 train_time:42555ms step_avg:44.37ms
step:960/1845 train_time:42618ms step_avg:44.39ms
step:961/1845 train_time:42679ms step_avg:44.41ms
step:962/1845 train_time:42741ms step_avg:44.43ms
step:963/1845 train_time:42802ms step_avg:44.45ms
step:964/1845 train_time:42865ms step_avg:44.47ms
step:965/1845 train_time:42925ms step_avg:44.48ms
step:966/1845 train_time:42988ms step_avg:44.50ms
step:967/1845 train_time:43048ms step_avg:44.52ms
step:968/1845 train_time:43110ms step_avg:44.54ms
step:969/1845 train_time:43171ms step_avg:44.55ms
step:970/1845 train_time:43234ms step_avg:44.57ms
step:971/1845 train_time:43294ms step_avg:44.59ms
step:972/1845 train_time:43357ms step_avg:44.61ms
step:973/1845 train_time:43417ms step_avg:44.62ms
step:974/1845 train_time:43480ms step_avg:44.64ms
step:975/1845 train_time:43541ms step_avg:44.66ms
step:976/1845 train_time:43603ms step_avg:44.68ms
step:977/1845 train_time:43664ms step_avg:44.69ms
step:978/1845 train_time:43727ms step_avg:44.71ms
step:979/1845 train_time:43787ms step_avg:44.73ms
step:980/1845 train_time:43850ms step_avg:44.74ms
step:981/1845 train_time:43909ms step_avg:44.76ms
step:982/1845 train_time:43973ms step_avg:44.78ms
step:983/1845 train_time:44033ms step_avg:44.79ms
step:984/1845 train_time:44097ms step_avg:44.81ms
step:985/1845 train_time:44157ms step_avg:44.83ms
step:986/1845 train_time:44219ms step_avg:44.85ms
step:987/1845 train_time:44280ms step_avg:44.86ms
step:988/1845 train_time:44343ms step_avg:44.88ms
step:989/1845 train_time:44403ms step_avg:44.90ms
step:990/1845 train_time:44466ms step_avg:44.92ms
step:991/1845 train_time:44527ms step_avg:44.93ms
step:992/1845 train_time:44590ms step_avg:44.95ms
step:993/1845 train_time:44650ms step_avg:44.96ms
step:994/1845 train_time:44713ms step_avg:44.98ms
step:995/1845 train_time:44773ms step_avg:45.00ms
step:996/1845 train_time:44837ms step_avg:45.02ms
step:997/1845 train_time:44897ms step_avg:45.03ms
step:998/1845 train_time:44960ms step_avg:45.05ms
step:999/1845 train_time:45020ms step_avg:45.06ms
step:1000/1845 train_time:45083ms step_avg:45.08ms
step:1000/1845 val_loss:3.7798 train_time:45154ms step_avg:45.15ms
step:1001/1845 train_time:45171ms step_avg:45.13ms
step:1002/1845 train_time:45209ms step_avg:45.12ms
step:1003/1845 train_time:45269ms step_avg:45.13ms
step:1004/1845 train_time:45334ms step_avg:45.15ms
step:1005/1845 train_time:45394ms step_avg:45.17ms
step:1006/1845 train_time:45458ms step_avg:45.19ms
step:1007/1845 train_time:45518ms step_avg:45.20ms
step:1008/1845 train_time:45581ms step_avg:45.22ms
step:1009/1845 train_time:45640ms step_avg:45.23ms
step:1010/1845 train_time:45703ms step_avg:45.25ms
step:1011/1845 train_time:45762ms step_avg:45.26ms
step:1012/1845 train_time:45824ms step_avg:45.28ms
step:1013/1845 train_time:45884ms step_avg:45.30ms
step:1014/1845 train_time:45946ms step_avg:45.31ms
step:1015/1845 train_time:46006ms step_avg:45.33ms
step:1016/1845 train_time:46069ms step_avg:45.34ms
step:1017/1845 train_time:46131ms step_avg:45.36ms
step:1018/1845 train_time:46194ms step_avg:45.38ms
step:1019/1845 train_time:46255ms step_avg:45.39ms
step:1020/1845 train_time:46319ms step_avg:45.41ms
step:1021/1845 train_time:46379ms step_avg:45.43ms
step:1022/1845 train_time:46442ms step_avg:45.44ms
step:1023/1845 train_time:46503ms step_avg:45.46ms
step:1024/1845 train_time:46565ms step_avg:45.47ms
step:1025/1845 train_time:46625ms step_avg:45.49ms
step:1026/1845 train_time:46687ms step_avg:45.50ms
step:1027/1845 train_time:46747ms step_avg:45.52ms
step:1028/1845 train_time:46809ms step_avg:45.53ms
step:1029/1845 train_time:46868ms step_avg:45.55ms
step:1030/1845 train_time:46931ms step_avg:45.56ms
step:1031/1845 train_time:46990ms step_avg:45.58ms
step:1032/1845 train_time:47053ms step_avg:45.59ms
step:1033/1845 train_time:47114ms step_avg:45.61ms
step:1034/1845 train_time:47178ms step_avg:45.63ms
step:1035/1845 train_time:47239ms step_avg:45.64ms
step:1036/1845 train_time:47301ms step_avg:45.66ms
step:1037/1845 train_time:47362ms step_avg:45.67ms
step:1038/1845 train_time:47425ms step_avg:45.69ms
step:1039/1845 train_time:47486ms step_avg:45.70ms
step:1040/1845 train_time:47548ms step_avg:45.72ms
step:1041/1845 train_time:47608ms step_avg:45.73ms
step:1042/1845 train_time:47671ms step_avg:45.75ms
step:1043/1845 train_time:47730ms step_avg:45.76ms
step:1044/1845 train_time:47792ms step_avg:45.78ms
step:1045/1845 train_time:47852ms step_avg:45.79ms
step:1046/1845 train_time:47916ms step_avg:45.81ms
step:1047/1845 train_time:47975ms step_avg:45.82ms
step:1048/1845 train_time:48038ms step_avg:45.84ms
step:1049/1845 train_time:48098ms step_avg:45.85ms
step:1050/1845 train_time:48161ms step_avg:45.87ms
step:1051/1845 train_time:48221ms step_avg:45.88ms
step:1052/1845 train_time:48284ms step_avg:45.90ms
step:1053/1845 train_time:48345ms step_avg:45.91ms
step:1054/1845 train_time:48408ms step_avg:45.93ms
step:1055/1845 train_time:48469ms step_avg:45.94ms
step:1056/1845 train_time:48531ms step_avg:45.96ms
step:1057/1845 train_time:48591ms step_avg:45.97ms
step:1058/1845 train_time:48654ms step_avg:45.99ms
step:1059/1845 train_time:48714ms step_avg:46.00ms
step:1060/1845 train_time:48777ms step_avg:46.02ms
step:1061/1845 train_time:48837ms step_avg:46.03ms
step:1062/1845 train_time:48899ms step_avg:46.04ms
step:1063/1845 train_time:48959ms step_avg:46.06ms
step:1064/1845 train_time:49022ms step_avg:46.07ms
step:1065/1845 train_time:49082ms step_avg:46.09ms
step:1066/1845 train_time:49144ms step_avg:46.10ms
step:1067/1845 train_time:49204ms step_avg:46.11ms
step:1068/1845 train_time:49267ms step_avg:46.13ms
step:1069/1845 train_time:49327ms step_avg:46.14ms
step:1070/1845 train_time:49389ms step_avg:46.16ms
step:1071/1845 train_time:49450ms step_avg:46.17ms
step:1072/1845 train_time:49513ms step_avg:46.19ms
step:1073/1845 train_time:49573ms step_avg:46.20ms
step:1074/1845 train_time:49636ms step_avg:46.22ms
step:1075/1845 train_time:49696ms step_avg:46.23ms
step:1076/1845 train_time:49759ms step_avg:46.24ms
step:1077/1845 train_time:49819ms step_avg:46.26ms
step:1078/1845 train_time:49883ms step_avg:46.27ms
step:1079/1845 train_time:49943ms step_avg:46.29ms
step:1080/1845 train_time:50006ms step_avg:46.30ms
step:1081/1845 train_time:50065ms step_avg:46.31ms
step:1082/1845 train_time:50129ms step_avg:46.33ms
step:1083/1845 train_time:50189ms step_avg:46.34ms
step:1084/1845 train_time:50251ms step_avg:46.36ms
step:1085/1845 train_time:50311ms step_avg:46.37ms
step:1086/1845 train_time:50375ms step_avg:46.39ms
step:1087/1845 train_time:50435ms step_avg:46.40ms
step:1088/1845 train_time:50498ms step_avg:46.41ms
step:1089/1845 train_time:50558ms step_avg:46.43ms
step:1090/1845 train_time:50622ms step_avg:46.44ms
step:1091/1845 train_time:50682ms step_avg:46.45ms
step:1092/1845 train_time:50745ms step_avg:46.47ms
step:1093/1845 train_time:50806ms step_avg:46.48ms
step:1094/1845 train_time:50868ms step_avg:46.50ms
step:1095/1845 train_time:50928ms step_avg:46.51ms
step:1096/1845 train_time:50991ms step_avg:46.52ms
step:1097/1845 train_time:51051ms step_avg:46.54ms
step:1098/1845 train_time:51114ms step_avg:46.55ms
step:1099/1845 train_time:51174ms step_avg:46.56ms
step:1100/1845 train_time:51236ms step_avg:46.58ms
step:1101/1845 train_time:51297ms step_avg:46.59ms
step:1102/1845 train_time:51360ms step_avg:46.61ms
step:1103/1845 train_time:51421ms step_avg:46.62ms
step:1104/1845 train_time:51484ms step_avg:46.63ms
step:1105/1845 train_time:51544ms step_avg:46.65ms
step:1106/1845 train_time:51607ms step_avg:46.66ms
step:1107/1845 train_time:51666ms step_avg:46.67ms
step:1108/1845 train_time:51729ms step_avg:46.69ms
step:1109/1845 train_time:51790ms step_avg:46.70ms
step:1110/1845 train_time:51852ms step_avg:46.71ms
step:1111/1845 train_time:51912ms step_avg:46.73ms
step:1112/1845 train_time:51975ms step_avg:46.74ms
step:1113/1845 train_time:52035ms step_avg:46.75ms
step:1114/1845 train_time:52097ms step_avg:46.77ms
step:1115/1845 train_time:52157ms step_avg:46.78ms
step:1116/1845 train_time:52220ms step_avg:46.79ms
step:1117/1845 train_time:52281ms step_avg:46.80ms
step:1118/1845 train_time:52344ms step_avg:46.82ms
step:1119/1845 train_time:52403ms step_avg:46.83ms
step:1120/1845 train_time:52466ms step_avg:46.84ms
step:1121/1845 train_time:52526ms step_avg:46.86ms
step:1122/1845 train_time:52589ms step_avg:46.87ms
step:1123/1845 train_time:52649ms step_avg:46.88ms
step:1124/1845 train_time:52712ms step_avg:46.90ms
step:1125/1845 train_time:52772ms step_avg:46.91ms
step:1126/1845 train_time:52835ms step_avg:46.92ms
step:1127/1845 train_time:52895ms step_avg:46.93ms
step:1128/1845 train_time:52959ms step_avg:46.95ms
step:1129/1845 train_time:53019ms step_avg:46.96ms
step:1130/1845 train_time:53081ms step_avg:46.97ms
step:1131/1845 train_time:53141ms step_avg:46.99ms
step:1132/1845 train_time:53204ms step_avg:47.00ms
step:1133/1845 train_time:53264ms step_avg:47.01ms
step:1134/1845 train_time:53328ms step_avg:47.03ms
step:1135/1845 train_time:53388ms step_avg:47.04ms
step:1136/1845 train_time:53450ms step_avg:47.05ms
step:1137/1845 train_time:53510ms step_avg:47.06ms
step:1138/1845 train_time:53573ms step_avg:47.08ms
step:1139/1845 train_time:53634ms step_avg:47.09ms
step:1140/1845 train_time:53697ms step_avg:47.10ms
step:1141/1845 train_time:53758ms step_avg:47.11ms
step:1142/1845 train_time:53820ms step_avg:47.13ms
step:1143/1845 train_time:53880ms step_avg:47.14ms
step:1144/1845 train_time:53943ms step_avg:47.15ms
step:1145/1845 train_time:54004ms step_avg:47.16ms
step:1146/1845 train_time:54067ms step_avg:47.18ms
step:1147/1845 train_time:54126ms step_avg:47.19ms
step:1148/1845 train_time:54189ms step_avg:47.20ms
step:1149/1845 train_time:54249ms step_avg:47.21ms
step:1150/1845 train_time:54312ms step_avg:47.23ms
step:1151/1845 train_time:54372ms step_avg:47.24ms
step:1152/1845 train_time:54434ms step_avg:47.25ms
step:1153/1845 train_time:54494ms step_avg:47.26ms
step:1154/1845 train_time:54557ms step_avg:47.28ms
step:1155/1845 train_time:54617ms step_avg:47.29ms
step:1156/1845 train_time:54680ms step_avg:47.30ms
step:1157/1845 train_time:54741ms step_avg:47.31ms
step:1158/1845 train_time:54804ms step_avg:47.33ms
step:1159/1845 train_time:54864ms step_avg:47.34ms
step:1160/1845 train_time:54927ms step_avg:47.35ms
step:1161/1845 train_time:54987ms step_avg:47.36ms
step:1162/1845 train_time:55050ms step_avg:47.37ms
step:1163/1845 train_time:55110ms step_avg:47.39ms
step:1164/1845 train_time:55172ms step_avg:47.40ms
step:1165/1845 train_time:55232ms step_avg:47.41ms
step:1166/1845 train_time:55295ms step_avg:47.42ms
step:1167/1845 train_time:55355ms step_avg:47.43ms
step:1168/1845 train_time:55419ms step_avg:47.45ms
step:1169/1845 train_time:55479ms step_avg:47.46ms
step:1170/1845 train_time:55542ms step_avg:47.47ms
step:1171/1845 train_time:55602ms step_avg:47.48ms
step:1172/1845 train_time:55665ms step_avg:47.50ms
step:1173/1845 train_time:55725ms step_avg:47.51ms
step:1174/1845 train_time:55788ms step_avg:47.52ms
step:1175/1845 train_time:55849ms step_avg:47.53ms
step:1176/1845 train_time:55911ms step_avg:47.54ms
step:1177/1845 train_time:55971ms step_avg:47.55ms
step:1178/1845 train_time:56034ms step_avg:47.57ms
step:1179/1845 train_time:56094ms step_avg:47.58ms
step:1180/1845 train_time:56156ms step_avg:47.59ms
step:1181/1845 train_time:56217ms step_avg:47.60ms
step:1182/1845 train_time:56280ms step_avg:47.61ms
step:1183/1845 train_time:56340ms step_avg:47.62ms
step:1184/1845 train_time:56402ms step_avg:47.64ms
step:1185/1845 train_time:56462ms step_avg:47.65ms
step:1186/1845 train_time:56525ms step_avg:47.66ms
step:1187/1845 train_time:56585ms step_avg:47.67ms
step:1188/1845 train_time:56648ms step_avg:47.68ms
step:1189/1845 train_time:56708ms step_avg:47.69ms
step:1190/1845 train_time:56770ms step_avg:47.71ms
step:1191/1845 train_time:56831ms step_avg:47.72ms
step:1192/1845 train_time:56893ms step_avg:47.73ms
step:1193/1845 train_time:56953ms step_avg:47.74ms
step:1194/1845 train_time:57016ms step_avg:47.75ms
step:1195/1845 train_time:57077ms step_avg:47.76ms
step:1196/1845 train_time:57140ms step_avg:47.78ms
step:1197/1845 train_time:57200ms step_avg:47.79ms
step:1198/1845 train_time:57263ms step_avg:47.80ms
step:1199/1845 train_time:57323ms step_avg:47.81ms
step:1200/1845 train_time:57386ms step_avg:47.82ms
step:1201/1845 train_time:57446ms step_avg:47.83ms
step:1202/1845 train_time:57509ms step_avg:47.84ms
step:1203/1845 train_time:57569ms step_avg:47.85ms
step:1204/1845 train_time:57632ms step_avg:47.87ms
step:1205/1845 train_time:57693ms step_avg:47.88ms
step:1206/1845 train_time:57780ms step_avg:47.91ms
step:1207/1845 train_time:57868ms step_avg:47.94ms
step:1208/1845 train_time:57957ms step_avg:47.98ms
step:1209/1845 train_time:58045ms step_avg:48.01ms
step:1210/1845 train_time:58134ms step_avg:48.04ms
step:1211/1845 train_time:58220ms step_avg:48.08ms
step:1212/1845 train_time:58309ms step_avg:48.11ms
step:1213/1845 train_time:58396ms step_avg:48.14ms
step:1214/1845 train_time:58485ms step_avg:48.18ms
step:1215/1845 train_time:58572ms step_avg:48.21ms
step:1216/1845 train_time:58661ms step_avg:48.24ms
step:1217/1845 train_time:58747ms step_avg:48.27ms
step:1218/1845 train_time:58836ms step_avg:48.31ms
step:1219/1845 train_time:58922ms step_avg:48.34ms
step:1220/1845 train_time:59012ms step_avg:48.37ms
step:1221/1845 train_time:59098ms step_avg:48.40ms
step:1222/1845 train_time:59187ms step_avg:48.43ms
step:1223/1845 train_time:59274ms step_avg:48.47ms
step:1224/1845 train_time:59363ms step_avg:48.50ms
step:1225/1845 train_time:59450ms step_avg:48.53ms
step:1226/1845 train_time:59539ms step_avg:48.56ms
step:1227/1845 train_time:59625ms step_avg:48.59ms
step:1228/1845 train_time:59715ms step_avg:48.63ms
step:1229/1845 train_time:59802ms step_avg:48.66ms
step:1230/1845 train_time:59891ms step_avg:48.69ms
step:1231/1845 train_time:59978ms step_avg:48.72ms
step:1232/1845 train_time:60066ms step_avg:48.76ms
step:1233/1845 train_time:60154ms step_avg:48.79ms
step:1234/1845 train_time:60243ms step_avg:48.82ms
step:1235/1845 train_time:60329ms step_avg:48.85ms
step:1236/1845 train_time:60419ms step_avg:48.88ms
step:1237/1845 train_time:60506ms step_avg:48.91ms
step:1238/1845 train_time:60595ms step_avg:48.95ms
step:1239/1845 train_time:60682ms step_avg:48.98ms
step:1240/1845 train_time:60772ms step_avg:49.01ms
step:1241/1845 train_time:60858ms step_avg:49.04ms
step:1242/1845 train_time:60946ms step_avg:49.07ms
step:1243/1845 train_time:61033ms step_avg:49.10ms
step:1244/1845 train_time:61123ms step_avg:49.13ms
step:1245/1845 train_time:61209ms step_avg:49.16ms
step:1246/1845 train_time:61300ms step_avg:49.20ms
step:1247/1845 train_time:61386ms step_avg:49.23ms
step:1248/1845 train_time:61476ms step_avg:49.26ms
step:1249/1845 train_time:61562ms step_avg:49.29ms
step:1250/1845 train_time:61651ms step_avg:49.32ms
step:1250/1845 val_loss:3.5338 train_time:61747ms step_avg:49.40ms
step:1251/1845 train_time:61765ms step_avg:49.37ms
step:1252/1845 train_time:61828ms step_avg:49.38ms
step:1253/1845 train_time:61918ms step_avg:49.42ms
step:1254/1845 train_time:62009ms step_avg:49.45ms
step:1255/1845 train_time:62097ms step_avg:49.48ms
step:1256/1845 train_time:62184ms step_avg:49.51ms
step:1257/1845 train_time:62269ms step_avg:49.54ms
step:1258/1845 train_time:62359ms step_avg:49.57ms
step:1259/1845 train_time:62443ms step_avg:49.60ms
step:1260/1845 train_time:62532ms step_avg:49.63ms
step:1261/1845 train_time:62618ms step_avg:49.66ms
step:1262/1845 train_time:62707ms step_avg:49.69ms
step:1263/1845 train_time:62797ms step_avg:49.72ms
step:1264/1845 train_time:62886ms step_avg:49.75ms
step:1265/1845 train_time:62976ms step_avg:49.78ms
step:1266/1845 train_time:63064ms step_avg:49.81ms
step:1267/1845 train_time:63151ms step_avg:49.84ms
step:1268/1845 train_time:63240ms step_avg:49.87ms
step:1269/1845 train_time:63325ms step_avg:49.90ms
step:1270/1845 train_time:63414ms step_avg:49.93ms
step:1271/1845 train_time:63499ms step_avg:49.96ms
step:1272/1845 train_time:63587ms step_avg:49.99ms
step:1273/1845 train_time:63674ms step_avg:50.02ms
step:1274/1845 train_time:63764ms step_avg:50.05ms
step:1275/1845 train_time:63852ms step_avg:50.08ms
step:1276/1845 train_time:63942ms step_avg:50.11ms
step:1277/1845 train_time:64029ms step_avg:50.14ms
step:1278/1845 train_time:64119ms step_avg:50.17ms
step:1279/1845 train_time:64206ms step_avg:50.20ms
step:1280/1845 train_time:64296ms step_avg:50.23ms
step:1281/1845 train_time:64381ms step_avg:50.26ms
step:1282/1845 train_time:64469ms step_avg:50.29ms
step:1283/1845 train_time:64555ms step_avg:50.32ms
step:1284/1845 train_time:64643ms step_avg:50.34ms
step:1285/1845 train_time:64730ms step_avg:50.37ms
step:1286/1845 train_time:64820ms step_avg:50.40ms
step:1287/1845 train_time:64907ms step_avg:50.43ms
step:1288/1845 train_time:64997ms step_avg:50.46ms
step:1289/1845 train_time:65083ms step_avg:50.49ms
step:1290/1845 train_time:65172ms step_avg:50.52ms
step:1291/1845 train_time:65258ms step_avg:50.55ms
step:1292/1845 train_time:65346ms step_avg:50.58ms
step:1293/1845 train_time:65432ms step_avg:50.61ms
step:1294/1845 train_time:65521ms step_avg:50.63ms
step:1295/1845 train_time:65606ms step_avg:50.66ms
step:1296/1845 train_time:65696ms step_avg:50.69ms
step:1297/1845 train_time:65782ms step_avg:50.72ms
step:1298/1845 train_time:65872ms step_avg:50.75ms
step:1299/1845 train_time:65959ms step_avg:50.78ms
step:1300/1845 train_time:66048ms step_avg:50.81ms
step:1301/1845 train_time:66134ms step_avg:50.83ms
step:1302/1845 train_time:66222ms step_avg:50.86ms
step:1303/1845 train_time:66309ms step_avg:50.89ms
step:1304/1845 train_time:66399ms step_avg:50.92ms
step:1305/1845 train_time:66484ms step_avg:50.95ms
step:1306/1845 train_time:66573ms step_avg:50.97ms
step:1307/1845 train_time:66659ms step_avg:51.00ms
step:1308/1845 train_time:66747ms step_avg:51.03ms
step:1309/1845 train_time:66834ms step_avg:51.06ms
step:1310/1845 train_time:66923ms step_avg:51.09ms
step:1311/1845 train_time:67011ms step_avg:51.11ms
step:1312/1845 train_time:67100ms step_avg:51.14ms
step:1313/1845 train_time:67187ms step_avg:51.17ms
step:1314/1845 train_time:67275ms step_avg:51.20ms
step:1315/1845 train_time:67361ms step_avg:51.22ms
step:1316/1845 train_time:67450ms step_avg:51.25ms
step:1317/1845 train_time:67536ms step_avg:51.28ms
step:1318/1845 train_time:67623ms step_avg:51.31ms
step:1319/1845 train_time:67710ms step_avg:51.33ms
step:1320/1845 train_time:67800ms step_avg:51.36ms
step:1321/1845 train_time:67886ms step_avg:51.39ms
step:1322/1845 train_time:67977ms step_avg:51.42ms
step:1323/1845 train_time:68062ms step_avg:51.45ms
step:1324/1845 train_time:68153ms step_avg:51.47ms
step:1325/1845 train_time:68239ms step_avg:51.50ms
step:1326/1845 train_time:68327ms step_avg:51.53ms
step:1327/1845 train_time:68415ms step_avg:51.56ms
step:1328/1845 train_time:68504ms step_avg:51.58ms
step:1329/1845 train_time:68590ms step_avg:51.61ms
step:1330/1845 train_time:68679ms step_avg:51.64ms
step:1331/1845 train_time:68765ms step_avg:51.66ms
step:1332/1845 train_time:68854ms step_avg:51.69ms
step:1333/1845 train_time:68940ms step_avg:51.72ms
step:1334/1845 train_time:69030ms step_avg:51.75ms
step:1335/1845 train_time:69116ms step_avg:51.77ms
step:1336/1845 train_time:69204ms step_avg:51.80ms
step:1337/1845 train_time:69291ms step_avg:51.83ms
step:1338/1845 train_time:69381ms step_avg:51.85ms
step:1339/1845 train_time:69465ms step_avg:51.88ms
step:1340/1845 train_time:69557ms step_avg:51.91ms
step:1341/1845 train_time:69643ms step_avg:51.93ms
step:1342/1845 train_time:69732ms step_avg:51.96ms
step:1343/1845 train_time:69819ms step_avg:51.99ms
step:1344/1845 train_time:69907ms step_avg:52.01ms
step:1345/1845 train_time:69993ms step_avg:52.04ms
step:1346/1845 train_time:70082ms step_avg:52.07ms
step:1347/1845 train_time:70169ms step_avg:52.09ms
step:1348/1845 train_time:70259ms step_avg:52.12ms
step:1349/1845 train_time:70346ms step_avg:52.15ms
step:1350/1845 train_time:70435ms step_avg:52.17ms
step:1351/1845 train_time:70521ms step_avg:52.20ms
step:1352/1845 train_time:70610ms step_avg:52.23ms
step:1353/1845 train_time:70697ms step_avg:52.25ms
step:1354/1845 train_time:70784ms step_avg:52.28ms
step:1355/1845 train_time:70871ms step_avg:52.30ms
step:1356/1845 train_time:70960ms step_avg:52.33ms
step:1357/1845 train_time:71046ms step_avg:52.36ms
step:1358/1845 train_time:71134ms step_avg:52.38ms
step:1359/1845 train_time:71221ms step_avg:52.41ms
step:1360/1845 train_time:71311ms step_avg:52.43ms
step:1361/1845 train_time:71398ms step_avg:52.46ms
step:1362/1845 train_time:71486ms step_avg:52.49ms
step:1363/1845 train_time:71572ms step_avg:52.51ms
step:1364/1845 train_time:71661ms step_avg:52.54ms
step:1365/1845 train_time:71748ms step_avg:52.56ms
step:1366/1845 train_time:71837ms step_avg:52.59ms
step:1367/1845 train_time:71922ms step_avg:52.61ms
step:1368/1845 train_time:72012ms step_avg:52.64ms
step:1369/1845 train_time:72098ms step_avg:52.66ms
step:1370/1845 train_time:72186ms step_avg:52.69ms
step:1371/1845 train_time:72273ms step_avg:52.72ms
step:1372/1845 train_time:72362ms step_avg:52.74ms
step:1373/1845 train_time:72448ms step_avg:52.77ms
step:1374/1845 train_time:72539ms step_avg:52.79ms
step:1375/1845 train_time:72624ms step_avg:52.82ms
step:1376/1845 train_time:72713ms step_avg:52.84ms
step:1377/1845 train_time:72800ms step_avg:52.87ms
step:1378/1845 train_time:72889ms step_avg:52.89ms
step:1379/1845 train_time:72975ms step_avg:52.92ms
step:1380/1845 train_time:73063ms step_avg:52.94ms
step:1381/1845 train_time:73150ms step_avg:52.97ms
step:1382/1845 train_time:73239ms step_avg:53.00ms
step:1383/1845 train_time:73326ms step_avg:53.02ms
step:1384/1845 train_time:73416ms step_avg:53.05ms
step:1385/1845 train_time:73502ms step_avg:53.07ms
step:1386/1845 train_time:73590ms step_avg:53.10ms
step:1387/1845 train_time:73677ms step_avg:53.12ms
step:1388/1845 train_time:73765ms step_avg:53.15ms
step:1389/1845 train_time:73853ms step_avg:53.17ms
step:1390/1845 train_time:73941ms step_avg:53.19ms
step:1391/1845 train_time:74027ms step_avg:53.22ms
step:1392/1845 train_time:74116ms step_avg:53.24ms
step:1393/1845 train_time:74202ms step_avg:53.27ms
step:1394/1845 train_time:74292ms step_avg:53.29ms
step:1395/1845 train_time:74378ms step_avg:53.32ms
step:1396/1845 train_time:74466ms step_avg:53.34ms
step:1397/1845 train_time:74552ms step_avg:53.37ms
step:1398/1845 train_time:74641ms step_avg:53.39ms
step:1399/1845 train_time:74725ms step_avg:53.41ms
step:1400/1845 train_time:74816ms step_avg:53.44ms
step:1401/1845 train_time:74902ms step_avg:53.46ms
step:1402/1845 train_time:74991ms step_avg:53.49ms
step:1403/1845 train_time:75078ms step_avg:53.51ms
step:1404/1845 train_time:75166ms step_avg:53.54ms
step:1405/1845 train_time:75252ms step_avg:53.56ms
step:1406/1845 train_time:75340ms step_avg:53.58ms
step:1407/1845 train_time:75427ms step_avg:53.61ms
step:1408/1845 train_time:75516ms step_avg:53.63ms
step:1409/1845 train_time:75602ms step_avg:53.66ms
step:1410/1845 train_time:75691ms step_avg:53.68ms
step:1411/1845 train_time:75779ms step_avg:53.71ms
step:1412/1845 train_time:75868ms step_avg:53.73ms
step:1413/1845 train_time:75954ms step_avg:53.75ms
step:1414/1845 train_time:76042ms step_avg:53.78ms
step:1415/1845 train_time:76128ms step_avg:53.80ms
step:1416/1845 train_time:76218ms step_avg:53.83ms
step:1417/1845 train_time:76304ms step_avg:53.85ms
step:1418/1845 train_time:76393ms step_avg:53.87ms
step:1419/1845 train_time:76480ms step_avg:53.90ms
step:1420/1845 train_time:76569ms step_avg:53.92ms
step:1421/1845 train_time:76654ms step_avg:53.94ms
step:1422/1845 train_time:76743ms step_avg:53.97ms
step:1423/1845 train_time:76830ms step_avg:53.99ms
step:1424/1845 train_time:76920ms step_avg:54.02ms
step:1425/1845 train_time:77005ms step_avg:54.04ms
step:1426/1845 train_time:77094ms step_avg:54.06ms
step:1427/1845 train_time:77180ms step_avg:54.09ms
step:1428/1845 train_time:77270ms step_avg:54.11ms
step:1429/1845 train_time:77356ms step_avg:54.13ms
step:1430/1845 train_time:77444ms step_avg:54.16ms
step:1431/1845 train_time:77530ms step_avg:54.18ms
step:1432/1845 train_time:77619ms step_avg:54.20ms
step:1433/1845 train_time:77705ms step_avg:54.23ms
step:1434/1845 train_time:77795ms step_avg:54.25ms
step:1435/1845 train_time:77881ms step_avg:54.27ms
step:1436/1845 train_time:77970ms step_avg:54.30ms
step:1437/1845 train_time:78056ms step_avg:54.32ms
step:1438/1845 train_time:78144ms step_avg:54.34ms
step:1439/1845 train_time:78232ms step_avg:54.37ms
step:1440/1845 train_time:78321ms step_avg:54.39ms
step:1441/1845 train_time:78407ms step_avg:54.41ms
step:1442/1845 train_time:78497ms step_avg:54.44ms
step:1443/1845 train_time:78583ms step_avg:54.46ms
step:1444/1845 train_time:78673ms step_avg:54.48ms
step:1445/1845 train_time:78759ms step_avg:54.50ms
step:1446/1845 train_time:78847ms step_avg:54.53ms
step:1447/1845 train_time:78934ms step_avg:54.55ms
step:1448/1845 train_time:79022ms step_avg:54.57ms
step:1449/1845 train_time:79109ms step_avg:54.60ms
step:1450/1845 train_time:79197ms step_avg:54.62ms
step:1451/1845 train_time:79283ms step_avg:54.64ms
step:1452/1845 train_time:79374ms step_avg:54.67ms
step:1453/1845 train_time:79459ms step_avg:54.69ms
step:1454/1845 train_time:79548ms step_avg:54.71ms
step:1455/1845 train_time:79635ms step_avg:54.73ms
step:1456/1845 train_time:79723ms step_avg:54.75ms
step:1457/1845 train_time:79811ms step_avg:54.78ms
step:1458/1845 train_time:79900ms step_avg:54.80ms
step:1459/1845 train_time:79986ms step_avg:54.82ms
step:1460/1845 train_time:80076ms step_avg:54.85ms
step:1461/1845 train_time:80162ms step_avg:54.87ms
step:1462/1845 train_time:80251ms step_avg:54.89ms
step:1463/1845 train_time:80338ms step_avg:54.91ms
step:1464/1845 train_time:80427ms step_avg:54.94ms
step:1465/1845 train_time:80513ms step_avg:54.96ms
step:1466/1845 train_time:80601ms step_avg:54.98ms
step:1467/1845 train_time:80688ms step_avg:55.00ms
step:1468/1845 train_time:80777ms step_avg:55.03ms
step:1469/1845 train_time:80863ms step_avg:55.05ms
step:1470/1845 train_time:80952ms step_avg:55.07ms
step:1471/1845 train_time:81038ms step_avg:55.09ms
step:1472/1845 train_time:81128ms step_avg:55.11ms
step:1473/1845 train_time:81213ms step_avg:55.13ms
step:1474/1845 train_time:81302ms step_avg:55.16ms
step:1475/1845 train_time:81388ms step_avg:55.18ms
step:1476/1845 train_time:81479ms step_avg:55.20ms
step:1477/1845 train_time:81564ms step_avg:55.22ms
step:1478/1845 train_time:81655ms step_avg:55.25ms
step:1479/1845 train_time:81741ms step_avg:55.27ms
step:1480/1845 train_time:81829ms step_avg:55.29ms
step:1481/1845 train_time:81916ms step_avg:55.31ms
step:1482/1845 train_time:82004ms step_avg:55.33ms
step:1483/1845 train_time:82091ms step_avg:55.35ms
step:1484/1845 train_time:82180ms step_avg:55.38ms
step:1485/1845 train_time:82265ms step_avg:55.40ms
step:1486/1845 train_time:82356ms step_avg:55.42ms
step:1487/1845 train_time:82442ms step_avg:55.44ms
step:1488/1845 train_time:82532ms step_avg:55.46ms
step:1489/1845 train_time:82618ms step_avg:55.49ms
step:1490/1845 train_time:82707ms step_avg:55.51ms
step:1491/1845 train_time:82793ms step_avg:55.53ms
step:1492/1845 train_time:82882ms step_avg:55.55ms
step:1493/1845 train_time:82968ms step_avg:55.57ms
step:1494/1845 train_time:83058ms step_avg:55.59ms
step:1495/1845 train_time:83144ms step_avg:55.61ms
step:1496/1845 train_time:83232ms step_avg:55.64ms
step:1497/1845 train_time:83319ms step_avg:55.66ms
step:1498/1845 train_time:83408ms step_avg:55.68ms
step:1499/1845 train_time:83494ms step_avg:55.70ms
step:1500/1845 train_time:83583ms step_avg:55.72ms
step:1500/1845 val_loss:3.4018 train_time:83680ms step_avg:55.79ms
step:1501/1845 train_time:83699ms step_avg:55.76ms
step:1502/1845 train_time:83760ms step_avg:55.77ms
step:1503/1845 train_time:83854ms step_avg:55.79ms
step:1504/1845 train_time:83946ms step_avg:55.82ms
step:1505/1845 train_time:84032ms step_avg:55.84ms
step:1506/1845 train_time:84118ms step_avg:55.86ms
step:1507/1845 train_time:84205ms step_avg:55.88ms
step:1508/1845 train_time:84293ms step_avg:55.90ms
step:1509/1845 train_time:84378ms step_avg:55.92ms
step:1510/1845 train_time:84467ms step_avg:55.94ms
step:1511/1845 train_time:84553ms step_avg:55.96ms
step:1512/1845 train_time:84644ms step_avg:55.98ms
step:1513/1845 train_time:84733ms step_avg:56.00ms
step:1514/1845 train_time:84823ms step_avg:56.03ms
step:1515/1845 train_time:84912ms step_avg:56.05ms
step:1516/1845 train_time:85002ms step_avg:56.07ms
step:1517/1845 train_time:85090ms step_avg:56.09ms
step:1518/1845 train_time:85178ms step_avg:56.11ms
step:1519/1845 train_time:85264ms step_avg:56.13ms
step:1520/1845 train_time:85352ms step_avg:56.15ms
step:1521/1845 train_time:85438ms step_avg:56.17ms
step:1522/1845 train_time:85526ms step_avg:56.19ms
step:1523/1845 train_time:85613ms step_avg:56.21ms
step:1524/1845 train_time:85704ms step_avg:56.24ms
step:1525/1845 train_time:85792ms step_avg:56.26ms
step:1526/1845 train_time:85881ms step_avg:56.28ms
step:1527/1845 train_time:85970ms step_avg:56.30ms
step:1528/1845 train_time:86058ms step_avg:56.32ms
step:1529/1845 train_time:86145ms step_avg:56.34ms
step:1530/1845 train_time:86234ms step_avg:56.36ms
step:1531/1845 train_time:86319ms step_avg:56.38ms
step:1532/1845 train_time:86408ms step_avg:56.40ms
step:1533/1845 train_time:86495ms step_avg:56.42ms
step:1534/1845 train_time:86584ms step_avg:56.44ms
step:1535/1845 train_time:86672ms step_avg:56.46ms
step:1536/1845 train_time:86760ms step_avg:56.48ms
step:1537/1845 train_time:86850ms step_avg:56.51ms
step:1538/1845 train_time:86940ms step_avg:56.53ms
step:1539/1845 train_time:87027ms step_avg:56.55ms
step:1540/1845 train_time:87116ms step_avg:56.57ms
step:1541/1845 train_time:87203ms step_avg:56.59ms
step:1542/1845 train_time:87292ms step_avg:56.61ms
step:1543/1845 train_time:87377ms step_avg:56.63ms
step:1544/1845 train_time:87466ms step_avg:56.65ms
step:1545/1845 train_time:87551ms step_avg:56.67ms
step:1546/1845 train_time:87641ms step_avg:56.69ms
step:1547/1845 train_time:87729ms step_avg:56.71ms
step:1548/1845 train_time:87818ms step_avg:56.73ms
step:1549/1845 train_time:87905ms step_avg:56.75ms
step:1550/1845 train_time:87994ms step_avg:56.77ms
step:1551/1845 train_time:88081ms step_avg:56.79ms
step:1552/1845 train_time:88170ms step_avg:56.81ms
step:1553/1845 train_time:88255ms step_avg:56.83ms
step:1554/1845 train_time:88344ms step_avg:56.85ms
step:1555/1845 train_time:88429ms step_avg:56.87ms
step:1556/1845 train_time:88518ms step_avg:56.89ms
step:1557/1845 train_time:88604ms step_avg:56.91ms
step:1558/1845 train_time:88695ms step_avg:56.93ms
step:1559/1845 train_time:88782ms step_avg:56.95ms
step:1560/1845 train_time:88873ms step_avg:56.97ms
step:1561/1845 train_time:88959ms step_avg:56.99ms
step:1562/1845 train_time:89051ms step_avg:57.01ms
step:1563/1845 train_time:89137ms step_avg:57.03ms
step:1564/1845 train_time:89226ms step_avg:57.05ms
step:1565/1845 train_time:89312ms step_avg:57.07ms
step:1566/1845 train_time:89400ms step_avg:57.09ms
step:1567/1845 train_time:89487ms step_avg:57.11ms
step:1568/1845 train_time:89576ms step_avg:57.13ms
step:1569/1845 train_time:89662ms step_avg:57.15ms
step:1570/1845 train_time:89751ms step_avg:57.17ms
step:1571/1845 train_time:89838ms step_avg:57.19ms
step:1572/1845 train_time:89928ms step_avg:57.21ms
step:1573/1845 train_time:90014ms step_avg:57.22ms
step:1574/1845 train_time:90104ms step_avg:57.25ms
step:1575/1845 train_time:90190ms step_avg:57.26ms
step:1576/1845 train_time:90279ms step_avg:57.28ms
step:1577/1845 train_time:90366ms step_avg:57.30ms
step:1578/1845 train_time:90455ms step_avg:57.32ms
step:1579/1845 train_time:90540ms step_avg:57.34ms
step:1580/1845 train_time:90630ms step_avg:57.36ms
step:1581/1845 train_time:90716ms step_avg:57.38ms
step:1582/1845 train_time:90806ms step_avg:57.40ms
step:1583/1845 train_time:90892ms step_avg:57.42ms
step:1584/1845 train_time:90981ms step_avg:57.44ms
step:1585/1845 train_time:91069ms step_avg:57.46ms
step:1586/1845 train_time:91157ms step_avg:57.48ms
step:1587/1845 train_time:91244ms step_avg:57.49ms
step:1588/1845 train_time:91333ms step_avg:57.51ms
step:1589/1845 train_time:91418ms step_avg:57.53ms
step:1590/1845 train_time:91509ms step_avg:57.55ms
step:1591/1845 train_time:91596ms step_avg:57.57ms
step:1592/1845 train_time:91685ms step_avg:57.59ms
step:1593/1845 train_time:91772ms step_avg:57.61ms
step:1594/1845 train_time:91860ms step_avg:57.63ms
step:1595/1845 train_time:91949ms step_avg:57.65ms
step:1596/1845 train_time:92038ms step_avg:57.67ms
step:1597/1845 train_time:92125ms step_avg:57.69ms
step:1598/1845 train_time:92213ms step_avg:57.71ms
step:1599/1845 train_time:92298ms step_avg:57.72ms
step:1600/1845 train_time:92389ms step_avg:57.74ms
step:1601/1845 train_time:92475ms step_avg:57.76ms
step:1602/1845 train_time:92565ms step_avg:57.78ms
step:1603/1845 train_time:92652ms step_avg:57.80ms
step:1604/1845 train_time:92740ms step_avg:57.82ms
step:1605/1845 train_time:92827ms step_avg:57.84ms
step:1606/1845 train_time:92916ms step_avg:57.86ms
step:1607/1845 train_time:93001ms step_avg:57.87ms
step:1608/1845 train_time:93092ms step_avg:57.89ms
step:1609/1845 train_time:93177ms step_avg:57.91ms
step:1610/1845 train_time:93267ms step_avg:57.93ms
step:1611/1845 train_time:93355ms step_avg:57.95ms
step:1612/1845 train_time:93444ms step_avg:57.97ms
step:1613/1845 train_time:93530ms step_avg:57.99ms
step:1614/1845 train_time:93618ms step_avg:58.00ms
step:1615/1845 train_time:93706ms step_avg:58.02ms
step:1616/1845 train_time:93795ms step_avg:58.04ms
step:1617/1845 train_time:93882ms step_avg:58.06ms
step:1618/1845 train_time:93972ms step_avg:58.08ms
step:1619/1845 train_time:94058ms step_avg:58.10ms
step:1620/1845 train_time:94148ms step_avg:58.12ms
step:1621/1845 train_time:94235ms step_avg:58.13ms
step:1622/1845 train_time:94325ms step_avg:58.15ms
step:1623/1845 train_time:94411ms step_avg:58.17ms
step:1624/1845 train_time:94499ms step_avg:58.19ms
step:1625/1845 train_time:94586ms step_avg:58.21ms
step:1626/1845 train_time:94674ms step_avg:58.23ms
step:1627/1845 train_time:94760ms step_avg:58.24ms
step:1628/1845 train_time:94850ms step_avg:58.26ms
step:1629/1845 train_time:94936ms step_avg:58.28ms
step:1630/1845 train_time:95026ms step_avg:58.30ms
step:1631/1845 train_time:95113ms step_avg:58.32ms
step:1632/1845 train_time:95202ms step_avg:58.33ms
step:1633/1845 train_time:95288ms step_avg:58.35ms
step:1634/1845 train_time:95378ms step_avg:58.37ms
step:1635/1845 train_time:95464ms step_avg:58.39ms
step:1636/1845 train_time:95553ms step_avg:58.41ms
step:1637/1845 train_time:95640ms step_avg:58.42ms
step:1638/1845 train_time:95730ms step_avg:58.44ms
step:1639/1845 train_time:95816ms step_avg:58.46ms
step:1640/1845 train_time:95905ms step_avg:58.48ms
step:1641/1845 train_time:95992ms step_avg:58.50ms
step:1642/1845 train_time:96081ms step_avg:58.51ms
step:1643/1845 train_time:96168ms step_avg:58.53ms
step:1644/1845 train_time:96257ms step_avg:58.55ms
step:1645/1845 train_time:96345ms step_avg:58.57ms
step:1646/1845 train_time:96434ms step_avg:58.59ms
step:1647/1845 train_time:96520ms step_avg:58.60ms
step:1648/1845 train_time:96609ms step_avg:58.62ms
step:1649/1845 train_time:96695ms step_avg:58.64ms
step:1650/1845 train_time:96784ms step_avg:58.66ms
step:1651/1845 train_time:96870ms step_avg:58.67ms
step:1652/1845 train_time:96959ms step_avg:58.69ms
step:1653/1845 train_time:97046ms step_avg:58.71ms
step:1654/1845 train_time:97136ms step_avg:58.73ms
step:1655/1845 train_time:97223ms step_avg:58.75ms
step:1656/1845 train_time:97313ms step_avg:58.76ms
step:1657/1845 train_time:97398ms step_avg:58.78ms
step:1658/1845 train_time:97489ms step_avg:58.80ms
step:1659/1845 train_time:97575ms step_avg:58.82ms
step:1660/1845 train_time:97664ms step_avg:58.83ms
step:1661/1845 train_time:97750ms step_avg:58.85ms
step:1662/1845 train_time:97840ms step_avg:58.87ms
step:1663/1845 train_time:97927ms step_avg:58.89ms
step:1664/1845 train_time:98016ms step_avg:58.90ms
step:1665/1845 train_time:98102ms step_avg:58.92ms
step:1666/1845 train_time:98193ms step_avg:58.94ms
step:1667/1845 train_time:98279ms step_avg:58.96ms
step:1668/1845 train_time:98370ms step_avg:58.97ms
step:1669/1845 train_time:98456ms step_avg:58.99ms
step:1670/1845 train_time:98545ms step_avg:59.01ms
step:1671/1845 train_time:98632ms step_avg:59.03ms
step:1672/1845 train_time:98719ms step_avg:59.04ms
step:1673/1845 train_time:98807ms step_avg:59.06ms
step:1674/1845 train_time:98896ms step_avg:59.08ms
step:1675/1845 train_time:98983ms step_avg:59.09ms
step:1676/1845 train_time:99072ms step_avg:59.11ms
step:1677/1845 train_time:99158ms step_avg:59.13ms
step:1678/1845 train_time:99249ms step_avg:59.15ms
step:1679/1845 train_time:99335ms step_avg:59.16ms
step:1680/1845 train_time:99425ms step_avg:59.18ms
step:1681/1845 train_time:99512ms step_avg:59.20ms
step:1682/1845 train_time:99600ms step_avg:59.21ms
step:1683/1845 train_time:99687ms step_avg:59.23ms
step:1684/1845 train_time:99776ms step_avg:59.25ms
step:1685/1845 train_time:99861ms step_avg:59.26ms
step:1686/1845 train_time:99953ms step_avg:59.28ms
step:1687/1845 train_time:100039ms step_avg:59.30ms
step:1688/1845 train_time:100130ms step_avg:59.32ms
step:1689/1845 train_time:100216ms step_avg:59.33ms
step:1690/1845 train_time:100307ms step_avg:59.35ms
step:1691/1845 train_time:100394ms step_avg:59.37ms
step:1692/1845 train_time:100484ms step_avg:59.39ms
step:1693/1845 train_time:100570ms step_avg:59.40ms
step:1694/1845 train_time:100659ms step_avg:59.42ms
step:1695/1845 train_time:100747ms step_avg:59.44ms
step:1696/1845 train_time:100835ms step_avg:59.45ms
step:1697/1845 train_time:100923ms step_avg:59.47ms
step:1698/1845 train_time:101013ms step_avg:59.49ms
step:1699/1845 train_time:101099ms step_avg:59.50ms
step:1700/1845 train_time:101188ms step_avg:59.52ms
step:1701/1845 train_time:101275ms step_avg:59.54ms
step:1702/1845 train_time:101365ms step_avg:59.56ms
step:1703/1845 train_time:101451ms step_avg:59.57ms
step:1704/1845 train_time:101540ms step_avg:59.59ms
step:1705/1845 train_time:101625ms step_avg:59.60ms
step:1706/1845 train_time:101717ms step_avg:59.62ms
step:1707/1845 train_time:101803ms step_avg:59.64ms
step:1708/1845 train_time:101892ms step_avg:59.66ms
step:1709/1845 train_time:101978ms step_avg:59.67ms
step:1710/1845 train_time:102069ms step_avg:59.69ms
step:1711/1845 train_time:102154ms step_avg:59.70ms
step:1712/1845 train_time:102245ms step_avg:59.72ms
step:1713/1845 train_time:102333ms step_avg:59.74ms
step:1714/1845 train_time:102421ms step_avg:59.76ms
step:1715/1845 train_time:102508ms step_avg:59.77ms
step:1716/1845 train_time:102596ms step_avg:59.79ms
step:1717/1845 train_time:102683ms step_avg:59.80ms
step:1718/1845 train_time:102774ms step_avg:59.82ms
step:1719/1845 train_time:102860ms step_avg:59.84ms
step:1720/1845 train_time:102950ms step_avg:59.85ms
step:1721/1845 train_time:103036ms step_avg:59.87ms
step:1722/1845 train_time:103125ms step_avg:59.89ms
step:1723/1845 train_time:103212ms step_avg:59.90ms
step:1724/1845 train_time:103302ms step_avg:59.92ms
step:1725/1845 train_time:103388ms step_avg:59.94ms
step:1726/1845 train_time:103477ms step_avg:59.95ms
step:1727/1845 train_time:103564ms step_avg:59.97ms
step:1728/1845 train_time:103654ms step_avg:59.98ms
step:1729/1845 train_time:103740ms step_avg:60.00ms
step:1730/1845 train_time:103830ms step_avg:60.02ms
step:1731/1845 train_time:103916ms step_avg:60.03ms
step:1732/1845 train_time:104006ms step_avg:60.05ms
step:1733/1845 train_time:104092ms step_avg:60.06ms
step:1734/1845 train_time:104181ms step_avg:60.08ms
step:1735/1845 train_time:104270ms step_avg:60.10ms
step:1736/1845 train_time:104358ms step_avg:60.11ms
step:1737/1845 train_time:104447ms step_avg:60.13ms
step:1738/1845 train_time:104536ms step_avg:60.15ms
step:1739/1845 train_time:104622ms step_avg:60.16ms
step:1740/1845 train_time:104712ms step_avg:60.18ms
step:1741/1845 train_time:104798ms step_avg:60.19ms
step:1742/1845 train_time:104888ms step_avg:60.21ms
step:1743/1845 train_time:104974ms step_avg:60.23ms
step:1744/1845 train_time:105064ms step_avg:60.24ms
step:1745/1845 train_time:105151ms step_avg:60.26ms
step:1746/1845 train_time:105239ms step_avg:60.27ms
step:1747/1845 train_time:105326ms step_avg:60.29ms
step:1748/1845 train_time:105415ms step_avg:60.31ms
step:1749/1845 train_time:105501ms step_avg:60.32ms
step:1750/1845 train_time:105593ms step_avg:60.34ms
step:1750/1845 val_loss:3.3032 train_time:105690ms step_avg:60.39ms
step:1751/1845 train_time:105708ms step_avg:60.37ms
step:1752/1845 train_time:105769ms step_avg:60.37ms
step:1753/1845 train_time:105860ms step_avg:60.39ms
step:1754/1845 train_time:105949ms step_avg:60.40ms
step:1755/1845 train_time:106036ms step_avg:60.42ms
step:1756/1845 train_time:106124ms step_avg:60.44ms
step:1757/1845 train_time:106210ms step_avg:60.45ms
step:1758/1845 train_time:106300ms step_avg:60.47ms
step:1759/1845 train_time:106385ms step_avg:60.48ms
step:1760/1845 train_time:106474ms step_avg:60.50ms
step:1761/1845 train_time:106560ms step_avg:60.51ms
step:1762/1845 train_time:106651ms step_avg:60.53ms
step:1763/1845 train_time:106741ms step_avg:60.55ms
step:1764/1845 train_time:106832ms step_avg:60.56ms
step:1765/1845 train_time:106920ms step_avg:60.58ms
step:1766/1845 train_time:107009ms step_avg:60.59ms
step:1767/1845 train_time:107095ms step_avg:60.61ms
step:1768/1845 train_time:107182ms step_avg:60.62ms
step:1769/1845 train_time:107269ms step_avg:60.64ms
step:1770/1845 train_time:107357ms step_avg:60.65ms
step:1771/1845 train_time:107442ms step_avg:60.67ms
step:1772/1845 train_time:107532ms step_avg:60.68ms
step:1773/1845 train_time:107620ms step_avg:60.70ms
step:1774/1845 train_time:107709ms step_avg:60.72ms
step:1775/1845 train_time:107798ms step_avg:60.73ms
step:1776/1845 train_time:107888ms step_avg:60.75ms
step:1777/1845 train_time:107976ms step_avg:60.76ms
step:1778/1845 train_time:108064ms step_avg:60.78ms
step:1779/1845 train_time:108151ms step_avg:60.79ms
step:1780/1845 train_time:108240ms step_avg:60.81ms
step:1781/1845 train_time:108326ms step_avg:60.82ms
step:1782/1845 train_time:108415ms step_avg:60.84ms
step:1783/1845 train_time:108500ms step_avg:60.85ms
step:1784/1845 train_time:108591ms step_avg:60.87ms
step:1785/1845 train_time:108677ms step_avg:60.88ms
step:1786/1845 train_time:108767ms step_avg:60.90ms
step:1787/1845 train_time:108855ms step_avg:60.91ms
step:1788/1845 train_time:108946ms step_avg:60.93ms
step:1789/1845 train_time:109034ms step_avg:60.95ms
step:1790/1845 train_time:109122ms step_avg:60.96ms
step:1791/1845 train_time:109209ms step_avg:60.98ms
step:1792/1845 train_time:109299ms step_avg:60.99ms
step:1793/1845 train_time:109384ms step_avg:61.01ms
step:1794/1845 train_time:109474ms step_avg:61.02ms
step:1795/1845 train_time:109560ms step_avg:61.04ms
step:1796/1845 train_time:109649ms step_avg:61.05ms
step:1797/1845 train_time:109738ms step_avg:61.07ms
step:1798/1845 train_time:109826ms step_avg:61.08ms
step:1799/1845 train_time:109913ms step_avg:61.10ms
step:1800/1845 train_time:110003ms step_avg:61.11ms
step:1801/1845 train_time:110089ms step_avg:61.13ms
step:1802/1845 train_time:110178ms step_avg:61.14ms
step:1803/1845 train_time:110264ms step_avg:61.16ms
step:1804/1845 train_time:110353ms step_avg:61.17ms
step:1805/1845 train_time:110440ms step_avg:61.19ms
step:1806/1845 train_time:110531ms step_avg:61.20ms
step:1807/1845 train_time:110617ms step_avg:61.22ms
step:1808/1845 train_time:110705ms step_avg:61.23ms
step:1809/1845 train_time:110794ms step_avg:61.25ms
step:1810/1845 train_time:110883ms step_avg:61.26ms
step:1811/1845 train_time:110971ms step_avg:61.28ms
step:1812/1845 train_time:111060ms step_avg:61.29ms
step:1813/1845 train_time:111146ms step_avg:61.30ms
step:1814/1845 train_time:111235ms step_avg:61.32ms
step:1815/1845 train_time:111323ms step_avg:61.33ms
step:1816/1845 train_time:111413ms step_avg:61.35ms
step:1817/1845 train_time:111499ms step_avg:61.36ms
step:1818/1845 train_time:111588ms step_avg:61.38ms
step:1819/1845 train_time:111674ms step_avg:61.39ms
step:1820/1845 train_time:111764ms step_avg:61.41ms
step:1821/1845 train_time:111852ms step_avg:61.42ms
step:1822/1845 train_time:111942ms step_avg:61.44ms
step:1823/1845 train_time:112029ms step_avg:61.45ms
step:1824/1845 train_time:112119ms step_avg:61.47ms
step:1825/1845 train_time:112205ms step_avg:61.48ms
step:1826/1845 train_time:112295ms step_avg:61.50ms
step:1827/1845 train_time:112381ms step_avg:61.51ms
step:1828/1845 train_time:112471ms step_avg:61.53ms
step:1829/1845 train_time:112559ms step_avg:61.54ms
step:1830/1845 train_time:112648ms step_avg:61.56ms
step:1831/1845 train_time:112735ms step_avg:61.57ms
step:1832/1845 train_time:112824ms step_avg:61.59ms
step:1833/1845 train_time:112912ms step_avg:61.60ms
step:1834/1845 train_time:113002ms step_avg:61.62ms
step:1835/1845 train_time:113088ms step_avg:61.63ms
step:1836/1845 train_time:113178ms step_avg:61.64ms
step:1837/1845 train_time:113264ms step_avg:61.66ms
step:1838/1845 train_time:113354ms step_avg:61.67ms
step:1839/1845 train_time:113441ms step_avg:61.69ms
step:1840/1845 train_time:113531ms step_avg:61.70ms
step:1841/1845 train_time:113617ms step_avg:61.71ms
step:1842/1845 train_time:113706ms step_avg:61.73ms
step:1843/1845 train_time:113795ms step_avg:61.74ms
step:1844/1845 train_time:113883ms step_avg:61.76ms
step:1845/1845 train_time:113972ms step_avg:61.77ms
step:1845/1845 val_loss:3.2771 train_time:114069ms step_avg:61.83ms
peak memory allocated: 29801 MiB reserved: 45178 MiB

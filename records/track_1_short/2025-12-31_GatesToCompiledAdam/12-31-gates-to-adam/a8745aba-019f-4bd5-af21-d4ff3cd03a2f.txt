import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 18:02:55 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    147331      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    147332      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    147333      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    147334      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    147335      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    147336      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    147337      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    147338      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8308 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:99ms step_avg:98.80ms
step:2/1845 train_time:119ms step_avg:59.67ms
step:3/1845 train_time:139ms step_avg:46.32ms
step:4/1845 train_time:174ms step_avg:43.56ms
step:5/1845 train_time:207ms step_avg:41.44ms
step:6/1845 train_time:294ms step_avg:49.01ms
step:7/1845 train_time:310ms step_avg:44.34ms
step:8/1845 train_time:350ms step_avg:43.76ms
step:9/1845 train_time:383ms step_avg:42.56ms
step:10/1845 train_time:418ms step_avg:41.82ms
step:11/1845 train_time:451ms step_avg:41.02ms
step:12/1845 train_time:487ms step_avg:40.54ms
step:13/1845 train_time:519ms step_avg:39.96ms
step:14/1845 train_time:555ms step_avg:39.62ms
step:15/1845 train_time:588ms step_avg:39.18ms
step:16/1845 train_time:623ms step_avg:38.94ms
step:17/1845 train_time:656ms step_avg:38.59ms
step:18/1845 train_time:691ms step_avg:38.41ms
step:19/1845 train_time:724ms step_avg:38.13ms
step:20/1845 train_time:760ms step_avg:37.99ms
step:21/1845 train_time:793ms step_avg:37.76ms
step:22/1845 train_time:828ms step_avg:37.65ms
step:23/1845 train_time:861ms step_avg:37.45ms
step:24/1845 train_time:897ms step_avg:37.36ms
step:25/1845 train_time:930ms step_avg:37.19ms
step:26/1845 train_time:965ms step_avg:37.12ms
step:27/1845 train_time:998ms step_avg:36.97ms
step:28/1845 train_time:1033ms step_avg:36.91ms
step:29/1845 train_time:1067ms step_avg:36.78ms
step:30/1845 train_time:1102ms step_avg:36.73ms
step:31/1845 train_time:1135ms step_avg:36.61ms
step:32/1845 train_time:1170ms step_avg:36.58ms
step:33/1845 train_time:1203ms step_avg:36.47ms
step:34/1845 train_time:1239ms step_avg:36.43ms
step:35/1845 train_time:1272ms step_avg:36.34ms
step:36/1845 train_time:1308ms step_avg:36.34ms
step:37/1845 train_time:1342ms step_avg:36.26ms
step:38/1845 train_time:1377ms step_avg:36.24ms
step:39/1845 train_time:1410ms step_avg:36.16ms
step:40/1845 train_time:1446ms step_avg:36.15ms
step:41/1845 train_time:1479ms step_avg:36.08ms
step:42/1845 train_time:1515ms step_avg:36.06ms
step:43/1845 train_time:1548ms step_avg:36.00ms
step:44/1845 train_time:1584ms step_avg:35.99ms
step:45/1845 train_time:1617ms step_avg:35.93ms
step:46/1845 train_time:1652ms step_avg:35.92ms
step:47/1845 train_time:1685ms step_avg:35.86ms
step:48/1845 train_time:1721ms step_avg:35.86ms
step:49/1845 train_time:1754ms step_avg:35.80ms
step:50/1845 train_time:1790ms step_avg:35.79ms
step:51/1845 train_time:1823ms step_avg:35.74ms
step:52/1845 train_time:1858ms step_avg:35.73ms
step:53/1845 train_time:1891ms step_avg:35.68ms
step:54/1845 train_time:1926ms step_avg:35.68ms
step:55/1845 train_time:1960ms step_avg:35.63ms
step:56/1845 train_time:1995ms step_avg:35.62ms
step:57/1845 train_time:2028ms step_avg:35.58ms
step:58/1845 train_time:2064ms step_avg:35.58ms
step:59/1845 train_time:2097ms step_avg:35.54ms
step:60/1845 train_time:2132ms step_avg:35.54ms
step:61/1845 train_time:2165ms step_avg:35.49ms
step:62/1845 train_time:2200ms step_avg:35.49ms
step:63/1845 train_time:2234ms step_avg:35.45ms
step:64/1845 train_time:2269ms step_avg:35.45ms
step:65/1845 train_time:2302ms step_avg:35.42ms
step:66/1845 train_time:2338ms step_avg:35.42ms
step:67/1845 train_time:2371ms step_avg:35.38ms
step:68/1845 train_time:2406ms step_avg:35.38ms
step:69/1845 train_time:2439ms step_avg:35.35ms
step:70/1845 train_time:2474ms step_avg:35.35ms
step:71/1845 train_time:2508ms step_avg:35.32ms
step:72/1845 train_time:2543ms step_avg:35.32ms
step:73/1845 train_time:2576ms step_avg:35.29ms
step:74/1845 train_time:2612ms step_avg:35.30ms
step:75/1845 train_time:2645ms step_avg:35.27ms
step:76/1845 train_time:2681ms step_avg:35.28ms
step:77/1845 train_time:2714ms step_avg:35.25ms
step:78/1845 train_time:2750ms step_avg:35.25ms
step:79/1845 train_time:2783ms step_avg:35.22ms
step:80/1845 train_time:2818ms step_avg:35.22ms
step:81/1845 train_time:2851ms step_avg:35.20ms
step:82/1845 train_time:2886ms step_avg:35.20ms
step:83/1845 train_time:2919ms step_avg:35.17ms
step:84/1845 train_time:2955ms step_avg:35.17ms
step:85/1845 train_time:2988ms step_avg:35.15ms
step:86/1845 train_time:3023ms step_avg:35.15ms
step:87/1845 train_time:3056ms step_avg:35.13ms
step:88/1845 train_time:3091ms step_avg:35.13ms
step:89/1845 train_time:3125ms step_avg:35.11ms
step:90/1845 train_time:3160ms step_avg:35.11ms
step:91/1845 train_time:3193ms step_avg:35.09ms
step:92/1845 train_time:3228ms step_avg:35.09ms
step:93/1845 train_time:3262ms step_avg:35.07ms
step:94/1845 train_time:3297ms step_avg:35.07ms
step:95/1845 train_time:3330ms step_avg:35.05ms
step:96/1845 train_time:3365ms step_avg:35.06ms
step:97/1845 train_time:3399ms step_avg:35.04ms
step:98/1845 train_time:3434ms step_avg:35.04ms
step:99/1845 train_time:3467ms step_avg:35.02ms
step:100/1845 train_time:3502ms step_avg:35.02ms
step:101/1845 train_time:3535ms step_avg:35.00ms
step:102/1845 train_time:3571ms step_avg:35.01ms
step:103/1845 train_time:3604ms step_avg:34.99ms
step:104/1845 train_time:3640ms step_avg:35.00ms
step:105/1845 train_time:3673ms step_avg:34.98ms
step:106/1845 train_time:3708ms step_avg:34.98ms
step:107/1845 train_time:3741ms step_avg:34.96ms
step:108/1845 train_time:3777ms step_avg:34.97ms
step:109/1845 train_time:3810ms step_avg:34.95ms
step:110/1845 train_time:3845ms step_avg:34.95ms
step:111/1845 train_time:3878ms step_avg:34.94ms
step:112/1845 train_time:3913ms step_avg:34.94ms
step:113/1845 train_time:3946ms step_avg:34.92ms
step:114/1845 train_time:3982ms step_avg:34.93ms
step:115/1845 train_time:4015ms step_avg:34.91ms
step:116/1845 train_time:4050ms step_avg:34.91ms
step:117/1845 train_time:4083ms step_avg:34.90ms
step:118/1845 train_time:4119ms step_avg:34.90ms
step:119/1845 train_time:4152ms step_avg:34.89ms
step:120/1845 train_time:4187ms step_avg:34.89ms
step:121/1845 train_time:4220ms step_avg:34.88ms
step:122/1845 train_time:4256ms step_avg:34.88ms
step:123/1845 train_time:4288ms step_avg:34.87ms
step:124/1845 train_time:4324ms step_avg:34.87ms
step:125/1845 train_time:4357ms step_avg:34.86ms
step:126/1845 train_time:4392ms step_avg:34.86ms
step:127/1845 train_time:4425ms step_avg:34.85ms
step:128/1845 train_time:4461ms step_avg:34.85ms
step:129/1845 train_time:4494ms step_avg:34.84ms
step:130/1845 train_time:4530ms step_avg:34.84ms
step:131/1845 train_time:4562ms step_avg:34.83ms
step:132/1845 train_time:4598ms step_avg:34.83ms
step:133/1845 train_time:4631ms step_avg:34.82ms
step:134/1845 train_time:4666ms step_avg:34.82ms
step:135/1845 train_time:4700ms step_avg:34.81ms
step:136/1845 train_time:4735ms step_avg:34.82ms
step:137/1845 train_time:4768ms step_avg:34.80ms
step:138/1845 train_time:4804ms step_avg:34.81ms
step:139/1845 train_time:4836ms step_avg:34.79ms
step:140/1845 train_time:4872ms step_avg:34.80ms
step:141/1845 train_time:4905ms step_avg:34.79ms
step:142/1845 train_time:4940ms step_avg:34.79ms
step:143/1845 train_time:4973ms step_avg:34.78ms
step:144/1845 train_time:5009ms step_avg:34.78ms
step:145/1845 train_time:5042ms step_avg:34.77ms
step:146/1845 train_time:5077ms step_avg:34.77ms
step:147/1845 train_time:5110ms step_avg:34.76ms
step:148/1845 train_time:5145ms step_avg:34.76ms
step:149/1845 train_time:5178ms step_avg:34.75ms
step:150/1845 train_time:5214ms step_avg:34.76ms
step:151/1845 train_time:5247ms step_avg:34.75ms
step:152/1845 train_time:5282ms step_avg:34.75ms
step:153/1845 train_time:5315ms step_avg:34.74ms
step:154/1845 train_time:5350ms step_avg:34.74ms
step:155/1845 train_time:5383ms step_avg:34.73ms
step:156/1845 train_time:5418ms step_avg:34.73ms
step:157/1845 train_time:5451ms step_avg:34.72ms
step:158/1845 train_time:5486ms step_avg:34.72ms
step:159/1845 train_time:5519ms step_avg:34.71ms
step:160/1845 train_time:5554ms step_avg:34.72ms
step:161/1845 train_time:5587ms step_avg:34.70ms
step:162/1845 train_time:5623ms step_avg:34.71ms
step:163/1845 train_time:5655ms step_avg:34.70ms
step:164/1845 train_time:5691ms step_avg:34.70ms
step:165/1845 train_time:5724ms step_avg:34.69ms
step:166/1845 train_time:5759ms step_avg:34.69ms
step:167/1845 train_time:5792ms step_avg:34.68ms
step:168/1845 train_time:5827ms step_avg:34.69ms
step:169/1845 train_time:5860ms step_avg:34.68ms
step:170/1845 train_time:5896ms step_avg:34.68ms
step:171/1845 train_time:5929ms step_avg:34.67ms
step:172/1845 train_time:5964ms step_avg:34.67ms
step:173/1845 train_time:5997ms step_avg:34.66ms
step:174/1845 train_time:6032ms step_avg:34.67ms
step:175/1845 train_time:6065ms step_avg:34.66ms
step:176/1845 train_time:6101ms step_avg:34.66ms
step:177/1845 train_time:6134ms step_avg:34.65ms
step:178/1845 train_time:6169ms step_avg:34.66ms
step:179/1845 train_time:6202ms step_avg:34.65ms
step:180/1845 train_time:6237ms step_avg:34.65ms
step:181/1845 train_time:6270ms step_avg:34.64ms
step:182/1845 train_time:6306ms step_avg:34.65ms
step:183/1845 train_time:6339ms step_avg:34.64ms
step:184/1845 train_time:6374ms step_avg:34.64ms
step:185/1845 train_time:6407ms step_avg:34.63ms
step:186/1845 train_time:6442ms step_avg:34.64ms
step:187/1845 train_time:6475ms step_avg:34.63ms
step:188/1845 train_time:6510ms step_avg:34.63ms
step:189/1845 train_time:6543ms step_avg:34.62ms
step:190/1845 train_time:6579ms step_avg:34.62ms
step:191/1845 train_time:6612ms step_avg:34.62ms
step:192/1845 train_time:6647ms step_avg:34.62ms
step:193/1845 train_time:6680ms step_avg:34.61ms
step:194/1845 train_time:6715ms step_avg:34.61ms
step:195/1845 train_time:6748ms step_avg:34.61ms
step:196/1845 train_time:6783ms step_avg:34.61ms
step:197/1845 train_time:6816ms step_avg:34.60ms
step:198/1845 train_time:6852ms step_avg:34.60ms
step:199/1845 train_time:6884ms step_avg:34.60ms
step:200/1845 train_time:6920ms step_avg:34.60ms
step:201/1845 train_time:6953ms step_avg:34.59ms
step:202/1845 train_time:6988ms step_avg:34.59ms
step:203/1845 train_time:7021ms step_avg:34.59ms
step:204/1845 train_time:7056ms step_avg:34.59ms
step:205/1845 train_time:7089ms step_avg:34.58ms
step:206/1845 train_time:7125ms step_avg:34.59ms
step:207/1845 train_time:7158ms step_avg:34.58ms
step:208/1845 train_time:7193ms step_avg:34.58ms
step:209/1845 train_time:7226ms step_avg:34.57ms
step:210/1845 train_time:7261ms step_avg:34.58ms
step:211/1845 train_time:7294ms step_avg:34.57ms
step:212/1845 train_time:7330ms step_avg:34.57ms
step:213/1845 train_time:7363ms step_avg:34.57ms
step:214/1845 train_time:7398ms step_avg:34.57ms
step:215/1845 train_time:7431ms step_avg:34.56ms
step:216/1845 train_time:7466ms step_avg:34.56ms
step:217/1845 train_time:7499ms step_avg:34.56ms
step:218/1845 train_time:7534ms step_avg:34.56ms
step:219/1845 train_time:7567ms step_avg:34.55ms
step:220/1845 train_time:7602ms step_avg:34.56ms
step:221/1845 train_time:7635ms step_avg:34.55ms
step:222/1845 train_time:7671ms step_avg:34.55ms
step:223/1845 train_time:7704ms step_avg:34.55ms
step:224/1845 train_time:7739ms step_avg:34.55ms
step:225/1845 train_time:7772ms step_avg:34.54ms
step:226/1845 train_time:7807ms step_avg:34.55ms
step:227/1845 train_time:7840ms step_avg:34.54ms
step:228/1845 train_time:7876ms step_avg:34.54ms
step:229/1845 train_time:7909ms step_avg:34.54ms
step:230/1845 train_time:7944ms step_avg:34.54ms
step:231/1845 train_time:7977ms step_avg:34.53ms
step:232/1845 train_time:8012ms step_avg:34.54ms
step:233/1845 train_time:8045ms step_avg:34.53ms
step:234/1845 train_time:8081ms step_avg:34.53ms
step:235/1845 train_time:8113ms step_avg:34.53ms
step:236/1845 train_time:8149ms step_avg:34.53ms
step:237/1845 train_time:8182ms step_avg:34.52ms
step:238/1845 train_time:8217ms step_avg:34.53ms
step:239/1845 train_time:8250ms step_avg:34.52ms
step:240/1845 train_time:8285ms step_avg:34.52ms
step:241/1845 train_time:8318ms step_avg:34.52ms
step:242/1845 train_time:8353ms step_avg:34.52ms
step:243/1845 train_time:8386ms step_avg:34.51ms
step:244/1845 train_time:8422ms step_avg:34.52ms
step:245/1845 train_time:8455ms step_avg:34.51ms
step:246/1845 train_time:8490ms step_avg:34.51ms
step:247/1845 train_time:8523ms step_avg:34.51ms
step:248/1845 train_time:8559ms step_avg:34.51ms
step:249/1845 train_time:8591ms step_avg:34.50ms
step:250/1845 train_time:8627ms step_avg:34.51ms
step:250/1845 val_loss:4.6127 train_time:8668ms step_avg:34.67ms
step:251/1845 train_time:8686ms step_avg:34.60ms
step:252/1845 train_time:8703ms step_avg:34.54ms
step:253/1845 train_time:8730ms step_avg:34.51ms
step:254/1845 train_time:8766ms step_avg:34.51ms
step:255/1845 train_time:8800ms step_avg:34.51ms
step:256/1845 train_time:8837ms step_avg:34.52ms
step:257/1845 train_time:8870ms step_avg:34.51ms
step:258/1845 train_time:8906ms step_avg:34.52ms
step:259/1845 train_time:8939ms step_avg:34.52ms
step:260/1845 train_time:8975ms step_avg:34.52ms
step:261/1845 train_time:9008ms step_avg:34.51ms
step:262/1845 train_time:9044ms step_avg:34.52ms
step:263/1845 train_time:9077ms step_avg:34.51ms
step:264/1845 train_time:9112ms step_avg:34.51ms
step:265/1845 train_time:9145ms step_avg:34.51ms
step:266/1845 train_time:9180ms step_avg:34.51ms
step:267/1845 train_time:9213ms step_avg:34.51ms
step:268/1845 train_time:9249ms step_avg:34.51ms
step:269/1845 train_time:9281ms step_avg:34.50ms
step:270/1845 train_time:9317ms step_avg:34.51ms
step:271/1845 train_time:9350ms step_avg:34.50ms
step:272/1845 train_time:9385ms step_avg:34.50ms
step:273/1845 train_time:9418ms step_avg:34.50ms
step:274/1845 train_time:9453ms step_avg:34.50ms
step:275/1845 train_time:9486ms step_avg:34.49ms
step:276/1845 train_time:9521ms step_avg:34.50ms
step:277/1845 train_time:9554ms step_avg:34.49ms
step:278/1845 train_time:9589ms step_avg:34.49ms
step:279/1845 train_time:9622ms step_avg:34.49ms
step:280/1845 train_time:9657ms step_avg:34.49ms
step:281/1845 train_time:9690ms step_avg:34.49ms
step:282/1845 train_time:9726ms step_avg:34.49ms
step:283/1845 train_time:9759ms step_avg:34.48ms
step:284/1845 train_time:9794ms step_avg:34.49ms
step:285/1845 train_time:9827ms step_avg:34.48ms
step:286/1845 train_time:9862ms step_avg:34.48ms
step:287/1845 train_time:9895ms step_avg:34.48ms
step:288/1845 train_time:9930ms step_avg:34.48ms
step:289/1845 train_time:9963ms step_avg:34.47ms
step:290/1845 train_time:9998ms step_avg:34.48ms
step:291/1845 train_time:10031ms step_avg:34.47ms
step:292/1845 train_time:10067ms step_avg:34.48ms
step:293/1845 train_time:10100ms step_avg:34.47ms
step:294/1845 train_time:10135ms step_avg:34.47ms
step:295/1845 train_time:10168ms step_avg:34.47ms
step:296/1845 train_time:10203ms step_avg:34.47ms
step:297/1845 train_time:10236ms step_avg:34.47ms
step:298/1845 train_time:10272ms step_avg:34.47ms
step:299/1845 train_time:10305ms step_avg:34.46ms
step:300/1845 train_time:10340ms step_avg:34.47ms
step:301/1845 train_time:10373ms step_avg:34.46ms
step:302/1845 train_time:10408ms step_avg:34.46ms
step:303/1845 train_time:10441ms step_avg:34.46ms
step:304/1845 train_time:10476ms step_avg:34.46ms
step:305/1845 train_time:10509ms step_avg:34.46ms
step:306/1845 train_time:10545ms step_avg:34.46ms
step:307/1845 train_time:10577ms step_avg:34.45ms
step:308/1845 train_time:10613ms step_avg:34.46ms
step:309/1845 train_time:10645ms step_avg:34.45ms
step:310/1845 train_time:10681ms step_avg:34.45ms
step:311/1845 train_time:10714ms step_avg:34.45ms
step:312/1845 train_time:10749ms step_avg:34.45ms
step:313/1845 train_time:10782ms step_avg:34.45ms
step:314/1845 train_time:10817ms step_avg:34.45ms
step:315/1845 train_time:10850ms step_avg:34.44ms
step:316/1845 train_time:10885ms step_avg:34.45ms
step:317/1845 train_time:10918ms step_avg:34.44ms
step:318/1845 train_time:10953ms step_avg:34.44ms
step:319/1845 train_time:10986ms step_avg:34.44ms
step:320/1845 train_time:11022ms step_avg:34.44ms
step:321/1845 train_time:11054ms step_avg:34.44ms
step:322/1845 train_time:11090ms step_avg:34.44ms
step:323/1845 train_time:11123ms step_avg:34.44ms
step:324/1845 train_time:11158ms step_avg:34.44ms
step:325/1845 train_time:11191ms step_avg:34.43ms
step:326/1845 train_time:11226ms step_avg:34.44ms
step:327/1845 train_time:11259ms step_avg:34.43ms
step:328/1845 train_time:11294ms step_avg:34.43ms
step:329/1845 train_time:11327ms step_avg:34.43ms
step:330/1845 train_time:11363ms step_avg:34.43ms
step:331/1845 train_time:11395ms step_avg:34.43ms
step:332/1845 train_time:11431ms step_avg:34.43ms
step:333/1845 train_time:11463ms step_avg:34.42ms
step:334/1845 train_time:11499ms step_avg:34.43ms
step:335/1845 train_time:11532ms step_avg:34.42ms
step:336/1845 train_time:11567ms step_avg:34.43ms
step:337/1845 train_time:11600ms step_avg:34.42ms
step:338/1845 train_time:11635ms step_avg:34.42ms
step:339/1845 train_time:11668ms step_avg:34.42ms
step:340/1845 train_time:11703ms step_avg:34.42ms
step:341/1845 train_time:11736ms step_avg:34.42ms
step:342/1845 train_time:11771ms step_avg:34.42ms
step:343/1845 train_time:11804ms step_avg:34.41ms
step:344/1845 train_time:11839ms step_avg:34.42ms
step:345/1845 train_time:11872ms step_avg:34.41ms
step:346/1845 train_time:11907ms step_avg:34.41ms
step:347/1845 train_time:11940ms step_avg:34.41ms
step:348/1845 train_time:11976ms step_avg:34.41ms
step:349/1845 train_time:12009ms step_avg:34.41ms
step:350/1845 train_time:12044ms step_avg:34.41ms
step:351/1845 train_time:12077ms step_avg:34.41ms
step:352/1845 train_time:12112ms step_avg:34.41ms
step:353/1845 train_time:12145ms step_avg:34.41ms
step:354/1845 train_time:12181ms step_avg:34.41ms
step:355/1845 train_time:12213ms step_avg:34.40ms
step:356/1845 train_time:12249ms step_avg:34.41ms
step:357/1845 train_time:12282ms step_avg:34.40ms
step:358/1845 train_time:12317ms step_avg:34.41ms
step:359/1845 train_time:12350ms step_avg:34.40ms
step:360/1845 train_time:12385ms step_avg:34.40ms
step:361/1845 train_time:12418ms step_avg:34.40ms
step:362/1845 train_time:12453ms step_avg:34.40ms
step:363/1845 train_time:12486ms step_avg:34.40ms
step:364/1845 train_time:12521ms step_avg:34.40ms
step:365/1845 train_time:12554ms step_avg:34.40ms
step:366/1845 train_time:12590ms step_avg:34.40ms
step:367/1845 train_time:12623ms step_avg:34.39ms
step:368/1845 train_time:12658ms step_avg:34.40ms
step:369/1845 train_time:12691ms step_avg:34.39ms
step:370/1845 train_time:12726ms step_avg:34.39ms
step:371/1845 train_time:12759ms step_avg:34.39ms
step:372/1845 train_time:12794ms step_avg:34.39ms
step:373/1845 train_time:12827ms step_avg:34.39ms
step:374/1845 train_time:12862ms step_avg:34.39ms
step:375/1845 train_time:12895ms step_avg:34.39ms
step:376/1845 train_time:12930ms step_avg:34.39ms
step:377/1845 train_time:12963ms step_avg:34.38ms
step:378/1845 train_time:12998ms step_avg:34.39ms
step:379/1845 train_time:13031ms step_avg:34.38ms
step:380/1845 train_time:13066ms step_avg:34.38ms
step:381/1845 train_time:13099ms step_avg:34.38ms
step:382/1845 train_time:13134ms step_avg:34.38ms
step:383/1845 train_time:13167ms step_avg:34.38ms
step:384/1845 train_time:13203ms step_avg:34.38ms
step:385/1845 train_time:13236ms step_avg:34.38ms
step:386/1845 train_time:13271ms step_avg:34.38ms
step:387/1845 train_time:13304ms step_avg:34.38ms
step:388/1845 train_time:13339ms step_avg:34.38ms
step:389/1845 train_time:13372ms step_avg:34.37ms
step:390/1845 train_time:13407ms step_avg:34.38ms
step:391/1845 train_time:13440ms step_avg:34.37ms
step:392/1845 train_time:13476ms step_avg:34.38ms
step:393/1845 train_time:13508ms step_avg:34.37ms
step:394/1845 train_time:13544ms step_avg:34.38ms
step:395/1845 train_time:13577ms step_avg:34.37ms
step:396/1845 train_time:13612ms step_avg:34.37ms
step:397/1845 train_time:13645ms step_avg:34.37ms
step:398/1845 train_time:13681ms step_avg:34.37ms
step:399/1845 train_time:13714ms step_avg:34.37ms
step:400/1845 train_time:13749ms step_avg:34.37ms
step:401/1845 train_time:13782ms step_avg:34.37ms
step:402/1845 train_time:13817ms step_avg:34.37ms
step:403/1845 train_time:13850ms step_avg:34.37ms
step:404/1845 train_time:13885ms step_avg:34.37ms
step:405/1845 train_time:13918ms step_avg:34.37ms
step:406/1845 train_time:13954ms step_avg:34.37ms
step:407/1845 train_time:13987ms step_avg:34.36ms
step:408/1845 train_time:14022ms step_avg:34.37ms
step:409/1845 train_time:14055ms step_avg:34.36ms
step:410/1845 train_time:14090ms step_avg:34.37ms
step:411/1845 train_time:14123ms step_avg:34.36ms
step:412/1845 train_time:14159ms step_avg:34.37ms
step:413/1845 train_time:14192ms step_avg:34.36ms
step:414/1845 train_time:14227ms step_avg:34.36ms
step:415/1845 train_time:14260ms step_avg:34.36ms
step:416/1845 train_time:14295ms step_avg:34.36ms
step:417/1845 train_time:14328ms step_avg:34.36ms
step:418/1845 train_time:14364ms step_avg:34.36ms
step:419/1845 train_time:14396ms step_avg:34.36ms
step:420/1845 train_time:14432ms step_avg:34.36ms
step:421/1845 train_time:14465ms step_avg:34.36ms
step:422/1845 train_time:14500ms step_avg:34.36ms
step:423/1845 train_time:14533ms step_avg:34.36ms
step:424/1845 train_time:14568ms step_avg:34.36ms
step:425/1845 train_time:14601ms step_avg:34.35ms
step:426/1845 train_time:14636ms step_avg:34.36ms
step:427/1845 train_time:14669ms step_avg:34.35ms
step:428/1845 train_time:14704ms step_avg:34.36ms
step:429/1845 train_time:14737ms step_avg:34.35ms
step:430/1845 train_time:14773ms step_avg:34.35ms
step:431/1845 train_time:14806ms step_avg:34.35ms
step:432/1845 train_time:14841ms step_avg:34.35ms
step:433/1845 train_time:14874ms step_avg:34.35ms
step:434/1845 train_time:14909ms step_avg:34.35ms
step:435/1845 train_time:14942ms step_avg:34.35ms
step:436/1845 train_time:14977ms step_avg:34.35ms
step:437/1845 train_time:15010ms step_avg:34.35ms
step:438/1845 train_time:15045ms step_avg:34.35ms
step:439/1845 train_time:15078ms step_avg:34.35ms
step:440/1845 train_time:15113ms step_avg:34.35ms
step:441/1845 train_time:15146ms step_avg:34.35ms
step:442/1845 train_time:15182ms step_avg:34.35ms
step:443/1845 train_time:15215ms step_avg:34.34ms
step:444/1845 train_time:15250ms step_avg:34.35ms
step:445/1845 train_time:15283ms step_avg:34.34ms
step:446/1845 train_time:15318ms step_avg:34.35ms
step:447/1845 train_time:15351ms step_avg:34.34ms
step:448/1845 train_time:15387ms step_avg:34.35ms
step:449/1845 train_time:15420ms step_avg:34.34ms
step:450/1845 train_time:15455ms step_avg:34.34ms
step:451/1845 train_time:15488ms step_avg:34.34ms
step:452/1845 train_time:15523ms step_avg:34.34ms
step:453/1845 train_time:15556ms step_avg:34.34ms
step:454/1845 train_time:15591ms step_avg:34.34ms
step:455/1845 train_time:15624ms step_avg:34.34ms
step:456/1845 train_time:15659ms step_avg:34.34ms
step:457/1845 train_time:15692ms step_avg:34.34ms
step:458/1845 train_time:15727ms step_avg:34.34ms
step:459/1845 train_time:15760ms step_avg:34.34ms
step:460/1845 train_time:15796ms step_avg:34.34ms
step:461/1845 train_time:15828ms step_avg:34.33ms
step:462/1845 train_time:15864ms step_avg:34.34ms
step:463/1845 train_time:15897ms step_avg:34.33ms
step:464/1845 train_time:15932ms step_avg:34.34ms
step:465/1845 train_time:15965ms step_avg:34.33ms
step:466/1845 train_time:16000ms step_avg:34.34ms
step:467/1845 train_time:16033ms step_avg:34.33ms
step:468/1845 train_time:16068ms step_avg:34.33ms
step:469/1845 train_time:16101ms step_avg:34.33ms
step:470/1845 train_time:16137ms step_avg:34.33ms
step:471/1845 train_time:16170ms step_avg:34.33ms
step:472/1845 train_time:16205ms step_avg:34.33ms
step:473/1845 train_time:16238ms step_avg:34.33ms
step:474/1845 train_time:16273ms step_avg:34.33ms
step:475/1845 train_time:16306ms step_avg:34.33ms
step:476/1845 train_time:16341ms step_avg:34.33ms
step:477/1845 train_time:16374ms step_avg:34.33ms
step:478/1845 train_time:16409ms step_avg:34.33ms
step:479/1845 train_time:16442ms step_avg:34.33ms
step:480/1845 train_time:16477ms step_avg:34.33ms
step:481/1845 train_time:16510ms step_avg:34.32ms
step:482/1845 train_time:16546ms step_avg:34.33ms
step:483/1845 train_time:16579ms step_avg:34.32ms
step:484/1845 train_time:16614ms step_avg:34.33ms
step:485/1845 train_time:16647ms step_avg:34.32ms
step:486/1845 train_time:16682ms step_avg:34.33ms
step:487/1845 train_time:16715ms step_avg:34.32ms
step:488/1845 train_time:16751ms step_avg:34.32ms
step:489/1845 train_time:16783ms step_avg:34.32ms
step:490/1845 train_time:16819ms step_avg:34.32ms
step:491/1845 train_time:16851ms step_avg:34.32ms
step:492/1845 train_time:16887ms step_avg:34.32ms
step:493/1845 train_time:16920ms step_avg:34.32ms
step:494/1845 train_time:16955ms step_avg:34.32ms
step:495/1845 train_time:16988ms step_avg:34.32ms
step:496/1845 train_time:17024ms step_avg:34.32ms
step:497/1845 train_time:17056ms step_avg:34.32ms
step:498/1845 train_time:17092ms step_avg:34.32ms
step:499/1845 train_time:17125ms step_avg:34.32ms
step:500/1845 train_time:17160ms step_avg:34.32ms
step:500/1845 val_loss:4.2906 train_time:17202ms step_avg:34.40ms
step:501/1845 train_time:17219ms step_avg:34.37ms
step:502/1845 train_time:17236ms step_avg:34.34ms
step:503/1845 train_time:17263ms step_avg:34.32ms
step:504/1845 train_time:17300ms step_avg:34.32ms
step:505/1845 train_time:17334ms step_avg:34.32ms
step:506/1845 train_time:17371ms step_avg:34.33ms
step:507/1845 train_time:17404ms step_avg:34.33ms
step:508/1845 train_time:17440ms step_avg:34.33ms
step:509/1845 train_time:17473ms step_avg:34.33ms
step:510/1845 train_time:17508ms step_avg:34.33ms
step:511/1845 train_time:17541ms step_avg:34.33ms
step:512/1845 train_time:17577ms step_avg:34.33ms
step:513/1845 train_time:17609ms step_avg:34.33ms
step:514/1845 train_time:17645ms step_avg:34.33ms
step:515/1845 train_time:17678ms step_avg:34.33ms
step:516/1845 train_time:17713ms step_avg:34.33ms
step:517/1845 train_time:17746ms step_avg:34.33ms
step:518/1845 train_time:17782ms step_avg:34.33ms
step:519/1845 train_time:17814ms step_avg:34.32ms
step:520/1845 train_time:17850ms step_avg:34.33ms
step:521/1845 train_time:17882ms step_avg:34.32ms
step:522/1845 train_time:17918ms step_avg:34.33ms
step:523/1845 train_time:17951ms step_avg:34.32ms
step:524/1845 train_time:17986ms step_avg:34.32ms
step:525/1845 train_time:18019ms step_avg:34.32ms
step:526/1845 train_time:18054ms step_avg:34.32ms
step:527/1845 train_time:18087ms step_avg:34.32ms
step:528/1845 train_time:18122ms step_avg:34.32ms
step:529/1845 train_time:18155ms step_avg:34.32ms
step:530/1845 train_time:18190ms step_avg:34.32ms
step:531/1845 train_time:18223ms step_avg:34.32ms
step:532/1845 train_time:18259ms step_avg:34.32ms
step:533/1845 train_time:18291ms step_avg:34.32ms
step:534/1845 train_time:18327ms step_avg:34.32ms
step:535/1845 train_time:18360ms step_avg:34.32ms
step:536/1845 train_time:18395ms step_avg:34.32ms
step:537/1845 train_time:18428ms step_avg:34.32ms
step:538/1845 train_time:18464ms step_avg:34.32ms
step:539/1845 train_time:18496ms step_avg:34.32ms
step:540/1845 train_time:18532ms step_avg:34.32ms
step:541/1845 train_time:18565ms step_avg:34.32ms
step:542/1845 train_time:18600ms step_avg:34.32ms
step:543/1845 train_time:18633ms step_avg:34.31ms
step:544/1845 train_time:18668ms step_avg:34.32ms
step:545/1845 train_time:18701ms step_avg:34.31ms
step:546/1845 train_time:18737ms step_avg:34.32ms
step:547/1845 train_time:18769ms step_avg:34.31ms
step:548/1845 train_time:18805ms step_avg:34.32ms
step:549/1845 train_time:18838ms step_avg:34.31ms
step:550/1845 train_time:18873ms step_avg:34.31ms
step:551/1845 train_time:18906ms step_avg:34.31ms
step:552/1845 train_time:18941ms step_avg:34.31ms
step:553/1845 train_time:18975ms step_avg:34.31ms
step:554/1845 train_time:19010ms step_avg:34.31ms
step:555/1845 train_time:19043ms step_avg:34.31ms
step:556/1845 train_time:19078ms step_avg:34.31ms
step:557/1845 train_time:19111ms step_avg:34.31ms
step:558/1845 train_time:19146ms step_avg:34.31ms
step:559/1845 train_time:19179ms step_avg:34.31ms
step:560/1845 train_time:19214ms step_avg:34.31ms
step:561/1845 train_time:19247ms step_avg:34.31ms
step:562/1845 train_time:19282ms step_avg:34.31ms
step:563/1845 train_time:19315ms step_avg:34.31ms
step:564/1845 train_time:19350ms step_avg:34.31ms
step:565/1845 train_time:19383ms step_avg:34.31ms
step:566/1845 train_time:19418ms step_avg:34.31ms
step:567/1845 train_time:19451ms step_avg:34.31ms
step:568/1845 train_time:19487ms step_avg:34.31ms
step:569/1845 train_time:19519ms step_avg:34.30ms
step:570/1845 train_time:19555ms step_avg:34.31ms
step:571/1845 train_time:19587ms step_avg:34.30ms
step:572/1845 train_time:19623ms step_avg:34.31ms
step:573/1845 train_time:19656ms step_avg:34.30ms
step:574/1845 train_time:19691ms step_avg:34.30ms
step:575/1845 train_time:19724ms step_avg:34.30ms
step:576/1845 train_time:19759ms step_avg:34.30ms
step:577/1845 train_time:19792ms step_avg:34.30ms
step:578/1845 train_time:19827ms step_avg:34.30ms
step:579/1845 train_time:19860ms step_avg:34.30ms
step:580/1845 train_time:19895ms step_avg:34.30ms
step:581/1845 train_time:19928ms step_avg:34.30ms
step:582/1845 train_time:19964ms step_avg:34.30ms
step:583/1845 train_time:19996ms step_avg:34.30ms
step:584/1845 train_time:20032ms step_avg:34.30ms
step:585/1845 train_time:20065ms step_avg:34.30ms
step:586/1845 train_time:20100ms step_avg:34.30ms
step:587/1845 train_time:20133ms step_avg:34.30ms
step:588/1845 train_time:20168ms step_avg:34.30ms
step:589/1845 train_time:20201ms step_avg:34.30ms
step:590/1845 train_time:20236ms step_avg:34.30ms
step:591/1845 train_time:20269ms step_avg:34.30ms
step:592/1845 train_time:20304ms step_avg:34.30ms
step:593/1845 train_time:20337ms step_avg:34.30ms
step:594/1845 train_time:20372ms step_avg:34.30ms
step:595/1845 train_time:20406ms step_avg:34.29ms
step:596/1845 train_time:20441ms step_avg:34.30ms
step:597/1845 train_time:20474ms step_avg:34.29ms
step:598/1845 train_time:20509ms step_avg:34.30ms
step:599/1845 train_time:20542ms step_avg:34.29ms
step:600/1845 train_time:20577ms step_avg:34.30ms
step:601/1845 train_time:20610ms step_avg:34.29ms
step:602/1845 train_time:20645ms step_avg:34.29ms
step:603/1845 train_time:20680ms step_avg:34.30ms
step:604/1845 train_time:20740ms step_avg:34.34ms
step:605/1845 train_time:20800ms step_avg:34.38ms
step:606/1845 train_time:20863ms step_avg:34.43ms
step:607/1845 train_time:20924ms step_avg:34.47ms
step:608/1845 train_time:20987ms step_avg:34.52ms
step:609/1845 train_time:21047ms step_avg:34.56ms
step:610/1845 train_time:21110ms step_avg:34.61ms
step:611/1845 train_time:21170ms step_avg:34.65ms
step:612/1845 train_time:21233ms step_avg:34.69ms
step:613/1845 train_time:21293ms step_avg:34.74ms
step:614/1845 train_time:21356ms step_avg:34.78ms
step:615/1845 train_time:21416ms step_avg:34.82ms
step:616/1845 train_time:21478ms step_avg:34.87ms
step:617/1845 train_time:21539ms step_avg:34.91ms
step:618/1845 train_time:21602ms step_avg:34.95ms
step:619/1845 train_time:21662ms step_avg:35.00ms
step:620/1845 train_time:21725ms step_avg:35.04ms
step:621/1845 train_time:21785ms step_avg:35.08ms
step:622/1845 train_time:21848ms step_avg:35.12ms
step:623/1845 train_time:21909ms step_avg:35.17ms
step:624/1845 train_time:21971ms step_avg:35.21ms
step:625/1845 train_time:22031ms step_avg:35.25ms
step:626/1845 train_time:22095ms step_avg:35.30ms
step:627/1845 train_time:22155ms step_avg:35.33ms
step:628/1845 train_time:22217ms step_avg:35.38ms
step:629/1845 train_time:22277ms step_avg:35.42ms
step:630/1845 train_time:22340ms step_avg:35.46ms
step:631/1845 train_time:22401ms step_avg:35.50ms
step:632/1845 train_time:22463ms step_avg:35.54ms
step:633/1845 train_time:22524ms step_avg:35.58ms
step:634/1845 train_time:22587ms step_avg:35.63ms
step:635/1845 train_time:22646ms step_avg:35.66ms
step:636/1845 train_time:22710ms step_avg:35.71ms
step:637/1845 train_time:22770ms step_avg:35.75ms
step:638/1845 train_time:22833ms step_avg:35.79ms
step:639/1845 train_time:22893ms step_avg:35.83ms
step:640/1845 train_time:22955ms step_avg:35.87ms
step:641/1845 train_time:23016ms step_avg:35.91ms
step:642/1845 train_time:23078ms step_avg:35.95ms
step:643/1845 train_time:23138ms step_avg:35.98ms
step:644/1845 train_time:23201ms step_avg:36.03ms
step:645/1845 train_time:23261ms step_avg:36.06ms
step:646/1845 train_time:23324ms step_avg:36.10ms
step:647/1845 train_time:23384ms step_avg:36.14ms
step:648/1845 train_time:23447ms step_avg:36.18ms
step:649/1845 train_time:23508ms step_avg:36.22ms
step:650/1845 train_time:23571ms step_avg:36.26ms
step:651/1845 train_time:23631ms step_avg:36.30ms
step:652/1845 train_time:23694ms step_avg:36.34ms
step:653/1845 train_time:23754ms step_avg:36.38ms
step:654/1845 train_time:23817ms step_avg:36.42ms
step:655/1845 train_time:23876ms step_avg:36.45ms
step:656/1845 train_time:23940ms step_avg:36.49ms
step:657/1845 train_time:24000ms step_avg:36.53ms
step:658/1845 train_time:24063ms step_avg:36.57ms
step:659/1845 train_time:24122ms step_avg:36.60ms
step:660/1845 train_time:24185ms step_avg:36.64ms
step:661/1845 train_time:24246ms step_avg:36.68ms
step:662/1845 train_time:24309ms step_avg:36.72ms
step:663/1845 train_time:24369ms step_avg:36.76ms
step:664/1845 train_time:24432ms step_avg:36.80ms
step:665/1845 train_time:24492ms step_avg:36.83ms
step:666/1845 train_time:24556ms step_avg:36.87ms
step:667/1845 train_time:24616ms step_avg:36.91ms
step:668/1845 train_time:24678ms step_avg:36.94ms
step:669/1845 train_time:24738ms step_avg:36.98ms
step:670/1845 train_time:24801ms step_avg:37.02ms
step:671/1845 train_time:24861ms step_avg:37.05ms
step:672/1845 train_time:24924ms step_avg:37.09ms
step:673/1845 train_time:24984ms step_avg:37.12ms
step:674/1845 train_time:25047ms step_avg:37.16ms
step:675/1845 train_time:25107ms step_avg:37.20ms
step:676/1845 train_time:25170ms step_avg:37.23ms
step:677/1845 train_time:25230ms step_avg:37.27ms
step:678/1845 train_time:25293ms step_avg:37.31ms
step:679/1845 train_time:25353ms step_avg:37.34ms
step:680/1845 train_time:25416ms step_avg:37.38ms
step:681/1845 train_time:25477ms step_avg:37.41ms
step:682/1845 train_time:25539ms step_avg:37.45ms
step:683/1845 train_time:25600ms step_avg:37.48ms
step:684/1845 train_time:25663ms step_avg:37.52ms
step:685/1845 train_time:25723ms step_avg:37.55ms
step:686/1845 train_time:25787ms step_avg:37.59ms
step:687/1845 train_time:25847ms step_avg:37.62ms
step:688/1845 train_time:25910ms step_avg:37.66ms
step:689/1845 train_time:25970ms step_avg:37.69ms
step:690/1845 train_time:26033ms step_avg:37.73ms
step:691/1845 train_time:26094ms step_avg:37.76ms
step:692/1845 train_time:26157ms step_avg:37.80ms
step:693/1845 train_time:26217ms step_avg:37.83ms
step:694/1845 train_time:26280ms step_avg:37.87ms
step:695/1845 train_time:26340ms step_avg:37.90ms
step:696/1845 train_time:26404ms step_avg:37.94ms
step:697/1845 train_time:26464ms step_avg:37.97ms
step:698/1845 train_time:26527ms step_avg:38.00ms
step:699/1845 train_time:26587ms step_avg:38.04ms
step:700/1845 train_time:26650ms step_avg:38.07ms
step:701/1845 train_time:26711ms step_avg:38.10ms
step:702/1845 train_time:26774ms step_avg:38.14ms
step:703/1845 train_time:26834ms step_avg:38.17ms
step:704/1845 train_time:26897ms step_avg:38.21ms
step:705/1845 train_time:26957ms step_avg:38.24ms
step:706/1845 train_time:27020ms step_avg:38.27ms
step:707/1845 train_time:27081ms step_avg:38.30ms
step:708/1845 train_time:27144ms step_avg:38.34ms
step:709/1845 train_time:27203ms step_avg:38.37ms
step:710/1845 train_time:27266ms step_avg:38.40ms
step:711/1845 train_time:27327ms step_avg:38.43ms
step:712/1845 train_time:27390ms step_avg:38.47ms
step:713/1845 train_time:27450ms step_avg:38.50ms
step:714/1845 train_time:27512ms step_avg:38.53ms
step:715/1845 train_time:27573ms step_avg:38.56ms
step:716/1845 train_time:27636ms step_avg:38.60ms
step:717/1845 train_time:27697ms step_avg:38.63ms
step:718/1845 train_time:27759ms step_avg:38.66ms
step:719/1845 train_time:27819ms step_avg:38.69ms
step:720/1845 train_time:27882ms step_avg:38.73ms
step:721/1845 train_time:27943ms step_avg:38.76ms
step:722/1845 train_time:28007ms step_avg:38.79ms
step:723/1845 train_time:28067ms step_avg:38.82ms
step:724/1845 train_time:28130ms step_avg:38.85ms
step:725/1845 train_time:28191ms step_avg:38.88ms
step:726/1845 train_time:28254ms step_avg:38.92ms
step:727/1845 train_time:28315ms step_avg:38.95ms
step:728/1845 train_time:28377ms step_avg:38.98ms
step:729/1845 train_time:28437ms step_avg:39.01ms
step:730/1845 train_time:28500ms step_avg:39.04ms
step:731/1845 train_time:28560ms step_avg:39.07ms
step:732/1845 train_time:28624ms step_avg:39.10ms
step:733/1845 train_time:28684ms step_avg:39.13ms
step:734/1845 train_time:28747ms step_avg:39.17ms
step:735/1845 train_time:28808ms step_avg:39.19ms
step:736/1845 train_time:28871ms step_avg:39.23ms
step:737/1845 train_time:28932ms step_avg:39.26ms
step:738/1845 train_time:28995ms step_avg:39.29ms
step:739/1845 train_time:29055ms step_avg:39.32ms
step:740/1845 train_time:29117ms step_avg:39.35ms
step:741/1845 train_time:29178ms step_avg:39.38ms
step:742/1845 train_time:29241ms step_avg:39.41ms
step:743/1845 train_time:29302ms step_avg:39.44ms
step:744/1845 train_time:29364ms step_avg:39.47ms
step:745/1845 train_time:29424ms step_avg:39.50ms
step:746/1845 train_time:29487ms step_avg:39.53ms
step:747/1845 train_time:29548ms step_avg:39.56ms
step:748/1845 train_time:29611ms step_avg:39.59ms
step:749/1845 train_time:29672ms step_avg:39.62ms
step:750/1845 train_time:29735ms step_avg:39.65ms
step:750/1845 val_loss:4.0244 train_time:29805ms step_avg:39.74ms
step:751/1845 train_time:29823ms step_avg:39.71ms
step:752/1845 train_time:29859ms step_avg:39.71ms
step:753/1845 train_time:29921ms step_avg:39.74ms
step:754/1845 train_time:29987ms step_avg:39.77ms
step:755/1845 train_time:30050ms step_avg:39.80ms
step:756/1845 train_time:30114ms step_avg:39.83ms
step:757/1845 train_time:30175ms step_avg:39.86ms
step:758/1845 train_time:30238ms step_avg:39.89ms
step:759/1845 train_time:30297ms step_avg:39.92ms
step:760/1845 train_time:30360ms step_avg:39.95ms
step:761/1845 train_time:30419ms step_avg:39.97ms
step:762/1845 train_time:30482ms step_avg:40.00ms
step:763/1845 train_time:30541ms step_avg:40.03ms
step:764/1845 train_time:30603ms step_avg:40.06ms
step:765/1845 train_time:30663ms step_avg:40.08ms
step:766/1845 train_time:30726ms step_avg:40.11ms
step:767/1845 train_time:30786ms step_avg:40.14ms
step:768/1845 train_time:30849ms step_avg:40.17ms
step:769/1845 train_time:30911ms step_avg:40.20ms
step:770/1845 train_time:30974ms step_avg:40.23ms
step:771/1845 train_time:31036ms step_avg:40.25ms
step:772/1845 train_time:31098ms step_avg:40.28ms
step:773/1845 train_time:31160ms step_avg:40.31ms
step:774/1845 train_time:31223ms step_avg:40.34ms
step:775/1845 train_time:31282ms step_avg:40.36ms
step:776/1845 train_time:31345ms step_avg:40.39ms
step:777/1845 train_time:31405ms step_avg:40.42ms
step:778/1845 train_time:31468ms step_avg:40.45ms
step:779/1845 train_time:31527ms step_avg:40.47ms
step:780/1845 train_time:31590ms step_avg:40.50ms
step:781/1845 train_time:31650ms step_avg:40.53ms
step:782/1845 train_time:31712ms step_avg:40.55ms
step:783/1845 train_time:31772ms step_avg:40.58ms
step:784/1845 train_time:31836ms step_avg:40.61ms
step:785/1845 train_time:31897ms step_avg:40.63ms
step:786/1845 train_time:31960ms step_avg:40.66ms
step:787/1845 train_time:32021ms step_avg:40.69ms
step:788/1845 train_time:32084ms step_avg:40.72ms
step:789/1845 train_time:32145ms step_avg:40.74ms
step:790/1845 train_time:32209ms step_avg:40.77ms
step:791/1845 train_time:32269ms step_avg:40.80ms
step:792/1845 train_time:32332ms step_avg:40.82ms
step:793/1845 train_time:32392ms step_avg:40.85ms
step:794/1845 train_time:32454ms step_avg:40.87ms
step:795/1845 train_time:32514ms step_avg:40.90ms
step:796/1845 train_time:32577ms step_avg:40.93ms
step:797/1845 train_time:32638ms step_avg:40.95ms
step:798/1845 train_time:32700ms step_avg:40.98ms
step:799/1845 train_time:32761ms step_avg:41.00ms
step:800/1845 train_time:32823ms step_avg:41.03ms
step:801/1845 train_time:32884ms step_avg:41.05ms
step:802/1845 train_time:32947ms step_avg:41.08ms
step:803/1845 train_time:33007ms step_avg:41.10ms
step:804/1845 train_time:33070ms step_avg:41.13ms
step:805/1845 train_time:33132ms step_avg:41.16ms
step:806/1845 train_time:33195ms step_avg:41.18ms
step:807/1845 train_time:33255ms step_avg:41.21ms
step:808/1845 train_time:33318ms step_avg:41.23ms
step:809/1845 train_time:33378ms step_avg:41.26ms
step:810/1845 train_time:33441ms step_avg:41.29ms
step:811/1845 train_time:33501ms step_avg:41.31ms
step:812/1845 train_time:33564ms step_avg:41.33ms
step:813/1845 train_time:33624ms step_avg:41.36ms
step:814/1845 train_time:33686ms step_avg:41.38ms
step:815/1845 train_time:33746ms step_avg:41.41ms
step:816/1845 train_time:33809ms step_avg:41.43ms
step:817/1845 train_time:33869ms step_avg:41.46ms
step:818/1845 train_time:33932ms step_avg:41.48ms
step:819/1845 train_time:33992ms step_avg:41.50ms
step:820/1845 train_time:34056ms step_avg:41.53ms
step:821/1845 train_time:34117ms step_avg:41.56ms
step:822/1845 train_time:34181ms step_avg:41.58ms
step:823/1845 train_time:34241ms step_avg:41.61ms
step:824/1845 train_time:34304ms step_avg:41.63ms
step:825/1845 train_time:34365ms step_avg:41.65ms
step:826/1845 train_time:34428ms step_avg:41.68ms
step:827/1845 train_time:34489ms step_avg:41.70ms
step:828/1845 train_time:34552ms step_avg:41.73ms
step:829/1845 train_time:34612ms step_avg:41.75ms
step:830/1845 train_time:34675ms step_avg:41.78ms
step:831/1845 train_time:34735ms step_avg:41.80ms
step:832/1845 train_time:34798ms step_avg:41.82ms
step:833/1845 train_time:34857ms step_avg:41.85ms
step:834/1845 train_time:34921ms step_avg:41.87ms
step:835/1845 train_time:34981ms step_avg:41.89ms
step:836/1845 train_time:35045ms step_avg:41.92ms
step:837/1845 train_time:35105ms step_avg:41.94ms
step:838/1845 train_time:35168ms step_avg:41.97ms
step:839/1845 train_time:35229ms step_avg:41.99ms
step:840/1845 train_time:35292ms step_avg:42.01ms
step:841/1845 train_time:35352ms step_avg:42.04ms
step:842/1845 train_time:35415ms step_avg:42.06ms
step:843/1845 train_time:35475ms step_avg:42.08ms
step:844/1845 train_time:35539ms step_avg:42.11ms
step:845/1845 train_time:35598ms step_avg:42.13ms
step:846/1845 train_time:35662ms step_avg:42.15ms
step:847/1845 train_time:35722ms step_avg:42.17ms
step:848/1845 train_time:35784ms step_avg:42.20ms
step:849/1845 train_time:35844ms step_avg:42.22ms
step:850/1845 train_time:35907ms step_avg:42.24ms
step:851/1845 train_time:35967ms step_avg:42.26ms
step:852/1845 train_time:36030ms step_avg:42.29ms
step:853/1845 train_time:36089ms step_avg:42.31ms
step:854/1845 train_time:36153ms step_avg:42.33ms
step:855/1845 train_time:36213ms step_avg:42.35ms
step:856/1845 train_time:36276ms step_avg:42.38ms
step:857/1845 train_time:36336ms step_avg:42.40ms
step:858/1845 train_time:36399ms step_avg:42.42ms
step:859/1845 train_time:36459ms step_avg:42.44ms
step:860/1845 train_time:36522ms step_avg:42.47ms
step:861/1845 train_time:36583ms step_avg:42.49ms
step:862/1845 train_time:36646ms step_avg:42.51ms
step:863/1845 train_time:36707ms step_avg:42.53ms
step:864/1845 train_time:36770ms step_avg:42.56ms
step:865/1845 train_time:36831ms step_avg:42.58ms
step:866/1845 train_time:36893ms step_avg:42.60ms
step:867/1845 train_time:36953ms step_avg:42.62ms
step:868/1845 train_time:37016ms step_avg:42.65ms
step:869/1845 train_time:37076ms step_avg:42.67ms
step:870/1845 train_time:37138ms step_avg:42.69ms
step:871/1845 train_time:37198ms step_avg:42.71ms
step:872/1845 train_time:37262ms step_avg:42.73ms
step:873/1845 train_time:37322ms step_avg:42.75ms
step:874/1845 train_time:37385ms step_avg:42.77ms
step:875/1845 train_time:37446ms step_avg:42.79ms
step:876/1845 train_time:37509ms step_avg:42.82ms
step:877/1845 train_time:37569ms step_avg:42.84ms
step:878/1845 train_time:37631ms step_avg:42.86ms
step:879/1845 train_time:37691ms step_avg:42.88ms
step:880/1845 train_time:37754ms step_avg:42.90ms
step:881/1845 train_time:37815ms step_avg:42.92ms
step:882/1845 train_time:37877ms step_avg:42.94ms
step:883/1845 train_time:37938ms step_avg:42.97ms
step:884/1845 train_time:38002ms step_avg:42.99ms
step:885/1845 train_time:38062ms step_avg:43.01ms
step:886/1845 train_time:38124ms step_avg:43.03ms
step:887/1845 train_time:38185ms step_avg:43.05ms
step:888/1845 train_time:38248ms step_avg:43.07ms
step:889/1845 train_time:38308ms step_avg:43.09ms
step:890/1845 train_time:38371ms step_avg:43.11ms
step:891/1845 train_time:38432ms step_avg:43.13ms
step:892/1845 train_time:38495ms step_avg:43.16ms
step:893/1845 train_time:38555ms step_avg:43.17ms
step:894/1845 train_time:38618ms step_avg:43.20ms
step:895/1845 train_time:38678ms step_avg:43.22ms
step:896/1845 train_time:38741ms step_avg:43.24ms
step:897/1845 train_time:38801ms step_avg:43.26ms
step:898/1845 train_time:38865ms step_avg:43.28ms
step:899/1845 train_time:38926ms step_avg:43.30ms
step:900/1845 train_time:38989ms step_avg:43.32ms
step:901/1845 train_time:39049ms step_avg:43.34ms
step:902/1845 train_time:39111ms step_avg:43.36ms
step:903/1845 train_time:39172ms step_avg:43.38ms
step:904/1845 train_time:39235ms step_avg:43.40ms
step:905/1845 train_time:39294ms step_avg:43.42ms
step:906/1845 train_time:39358ms step_avg:43.44ms
step:907/1845 train_time:39418ms step_avg:43.46ms
step:908/1845 train_time:39481ms step_avg:43.48ms
step:909/1845 train_time:39541ms step_avg:43.50ms
step:910/1845 train_time:39604ms step_avg:43.52ms
step:911/1845 train_time:39665ms step_avg:43.54ms
step:912/1845 train_time:39728ms step_avg:43.56ms
step:913/1845 train_time:39789ms step_avg:43.58ms
step:914/1845 train_time:39851ms step_avg:43.60ms
step:915/1845 train_time:39912ms step_avg:43.62ms
step:916/1845 train_time:39975ms step_avg:43.64ms
step:917/1845 train_time:40035ms step_avg:43.66ms
step:918/1845 train_time:40098ms step_avg:43.68ms
step:919/1845 train_time:40158ms step_avg:43.70ms
step:920/1845 train_time:40221ms step_avg:43.72ms
step:921/1845 train_time:40281ms step_avg:43.74ms
step:922/1845 train_time:40344ms step_avg:43.76ms
step:923/1845 train_time:40403ms step_avg:43.77ms
step:924/1845 train_time:40467ms step_avg:43.80ms
step:925/1845 train_time:40527ms step_avg:43.81ms
step:926/1845 train_time:40589ms step_avg:43.83ms
step:927/1845 train_time:40650ms step_avg:43.85ms
step:928/1845 train_time:40712ms step_avg:43.87ms
step:929/1845 train_time:40773ms step_avg:43.89ms
step:930/1845 train_time:40835ms step_avg:43.91ms
step:931/1845 train_time:40896ms step_avg:43.93ms
step:932/1845 train_time:40960ms step_avg:43.95ms
step:933/1845 train_time:41020ms step_avg:43.97ms
step:934/1845 train_time:41084ms step_avg:43.99ms
step:935/1845 train_time:41144ms step_avg:44.00ms
step:936/1845 train_time:41207ms step_avg:44.02ms
step:937/1845 train_time:41267ms step_avg:44.04ms
step:938/1845 train_time:41330ms step_avg:44.06ms
step:939/1845 train_time:41389ms step_avg:44.08ms
step:940/1845 train_time:41452ms step_avg:44.10ms
step:941/1845 train_time:41513ms step_avg:44.12ms
step:942/1845 train_time:41576ms step_avg:44.14ms
step:943/1845 train_time:41636ms step_avg:44.15ms
step:944/1845 train_time:41699ms step_avg:44.17ms
step:945/1845 train_time:41759ms step_avg:44.19ms
step:946/1845 train_time:41822ms step_avg:44.21ms
step:947/1845 train_time:41882ms step_avg:44.23ms
step:948/1845 train_time:41945ms step_avg:44.25ms
step:949/1845 train_time:42006ms step_avg:44.26ms
step:950/1845 train_time:42068ms step_avg:44.28ms
step:951/1845 train_time:42129ms step_avg:44.30ms
step:952/1845 train_time:42191ms step_avg:44.32ms
step:953/1845 train_time:42252ms step_avg:44.34ms
step:954/1845 train_time:42314ms step_avg:44.35ms
step:955/1845 train_time:42375ms step_avg:44.37ms
step:956/1845 train_time:42438ms step_avg:44.39ms
step:957/1845 train_time:42498ms step_avg:44.41ms
step:958/1845 train_time:42561ms step_avg:44.43ms
step:959/1845 train_time:42621ms step_avg:44.44ms
step:960/1845 train_time:42684ms step_avg:44.46ms
step:961/1845 train_time:42745ms step_avg:44.48ms
step:962/1845 train_time:42808ms step_avg:44.50ms
step:963/1845 train_time:42868ms step_avg:44.52ms
step:964/1845 train_time:42931ms step_avg:44.53ms
step:965/1845 train_time:42991ms step_avg:44.55ms
step:966/1845 train_time:43054ms step_avg:44.57ms
step:967/1845 train_time:43114ms step_avg:44.59ms
step:968/1845 train_time:43177ms step_avg:44.60ms
step:969/1845 train_time:43238ms step_avg:44.62ms
step:970/1845 train_time:43300ms step_avg:44.64ms
step:971/1845 train_time:43360ms step_avg:44.66ms
step:972/1845 train_time:43423ms step_avg:44.67ms
step:973/1845 train_time:43484ms step_avg:44.69ms
step:974/1845 train_time:43546ms step_avg:44.71ms
step:975/1845 train_time:43607ms step_avg:44.73ms
step:976/1845 train_time:43670ms step_avg:44.74ms
step:977/1845 train_time:43731ms step_avg:44.76ms
step:978/1845 train_time:43794ms step_avg:44.78ms
step:979/1845 train_time:43854ms step_avg:44.79ms
step:980/1845 train_time:43916ms step_avg:44.81ms
step:981/1845 train_time:43977ms step_avg:44.83ms
step:982/1845 train_time:44040ms step_avg:44.85ms
step:983/1845 train_time:44099ms step_avg:44.86ms
step:984/1845 train_time:44163ms step_avg:44.88ms
step:985/1845 train_time:44223ms step_avg:44.90ms
step:986/1845 train_time:44286ms step_avg:44.91ms
step:987/1845 train_time:44345ms step_avg:44.93ms
step:988/1845 train_time:44408ms step_avg:44.95ms
step:989/1845 train_time:44469ms step_avg:44.96ms
step:990/1845 train_time:44531ms step_avg:44.98ms
step:991/1845 train_time:44591ms step_avg:45.00ms
step:992/1845 train_time:44654ms step_avg:45.01ms
step:993/1845 train_time:44714ms step_avg:45.03ms
step:994/1845 train_time:44777ms step_avg:45.05ms
step:995/1845 train_time:44838ms step_avg:45.06ms
step:996/1845 train_time:44901ms step_avg:45.08ms
step:997/1845 train_time:44961ms step_avg:45.10ms
step:998/1845 train_time:45024ms step_avg:45.11ms
step:999/1845 train_time:45085ms step_avg:45.13ms
step:1000/1845 train_time:45148ms step_avg:45.15ms
step:1000/1845 val_loss:3.7784 train_time:45218ms step_avg:45.22ms
step:1001/1845 train_time:45235ms step_avg:45.19ms
step:1002/1845 train_time:45272ms step_avg:45.18ms
step:1003/1845 train_time:45337ms step_avg:45.20ms
step:1004/1845 train_time:45402ms step_avg:45.22ms
step:1005/1845 train_time:45464ms step_avg:45.24ms
step:1006/1845 train_time:45528ms step_avg:45.26ms
step:1007/1845 train_time:45588ms step_avg:45.27ms
step:1008/1845 train_time:45650ms step_avg:45.29ms
step:1009/1845 train_time:45710ms step_avg:45.30ms
step:1010/1845 train_time:45773ms step_avg:45.32ms
step:1011/1845 train_time:45832ms step_avg:45.33ms
step:1012/1845 train_time:45895ms step_avg:45.35ms
step:1013/1845 train_time:45954ms step_avg:45.36ms
step:1014/1845 train_time:46017ms step_avg:45.38ms
step:1015/1845 train_time:46076ms step_avg:45.40ms
step:1016/1845 train_time:46138ms step_avg:45.41ms
step:1017/1845 train_time:46198ms step_avg:45.43ms
step:1018/1845 train_time:46261ms step_avg:45.44ms
step:1019/1845 train_time:46323ms step_avg:45.46ms
step:1020/1845 train_time:46388ms step_avg:45.48ms
step:1021/1845 train_time:46449ms step_avg:45.49ms
step:1022/1845 train_time:46513ms step_avg:45.51ms
step:1023/1845 train_time:46573ms step_avg:45.53ms
step:1024/1845 train_time:46636ms step_avg:45.54ms
step:1025/1845 train_time:46696ms step_avg:45.56ms
step:1026/1845 train_time:46759ms step_avg:45.57ms
step:1027/1845 train_time:46819ms step_avg:45.59ms
step:1028/1845 train_time:46881ms step_avg:45.60ms
step:1029/1845 train_time:46941ms step_avg:45.62ms
step:1030/1845 train_time:47004ms step_avg:45.63ms
step:1031/1845 train_time:47064ms step_avg:45.65ms
step:1032/1845 train_time:47126ms step_avg:45.66ms
step:1033/1845 train_time:47186ms step_avg:45.68ms
step:1034/1845 train_time:47249ms step_avg:45.70ms
step:1035/1845 train_time:47309ms step_avg:45.71ms
step:1036/1845 train_time:47373ms step_avg:45.73ms
step:1037/1845 train_time:47434ms step_avg:45.74ms
step:1038/1845 train_time:47498ms step_avg:45.76ms
step:1039/1845 train_time:47558ms step_avg:45.77ms
step:1040/1845 train_time:47621ms step_avg:45.79ms
step:1041/1845 train_time:47681ms step_avg:45.80ms
step:1042/1845 train_time:47744ms step_avg:45.82ms
step:1043/1845 train_time:47804ms step_avg:45.83ms
step:1044/1845 train_time:47866ms step_avg:45.85ms
step:1045/1845 train_time:47926ms step_avg:45.86ms
step:1046/1845 train_time:47989ms step_avg:45.88ms
step:1047/1845 train_time:48049ms step_avg:45.89ms
step:1048/1845 train_time:48112ms step_avg:45.91ms
step:1049/1845 train_time:48173ms step_avg:45.92ms
step:1050/1845 train_time:48235ms step_avg:45.94ms
step:1051/1845 train_time:48296ms step_avg:45.95ms
step:1052/1845 train_time:48360ms step_avg:45.97ms
step:1053/1845 train_time:48420ms step_avg:45.98ms
step:1054/1845 train_time:48484ms step_avg:46.00ms
step:1055/1845 train_time:48544ms step_avg:46.01ms
step:1056/1845 train_time:48607ms step_avg:46.03ms
step:1057/1845 train_time:48666ms step_avg:46.04ms
step:1058/1845 train_time:48730ms step_avg:46.06ms
step:1059/1845 train_time:48790ms step_avg:46.07ms
step:1060/1845 train_time:48853ms step_avg:46.09ms
step:1061/1845 train_time:48914ms step_avg:46.10ms
step:1062/1845 train_time:48977ms step_avg:46.12ms
step:1063/1845 train_time:49037ms step_avg:46.13ms
step:1064/1845 train_time:49099ms step_avg:46.15ms
step:1065/1845 train_time:49160ms step_avg:46.16ms
step:1066/1845 train_time:49223ms step_avg:46.18ms
step:1067/1845 train_time:49283ms step_avg:46.19ms
step:1068/1845 train_time:49346ms step_avg:46.20ms
step:1069/1845 train_time:49407ms step_avg:46.22ms
step:1070/1845 train_time:49470ms step_avg:46.23ms
step:1071/1845 train_time:49531ms step_avg:46.25ms
step:1072/1845 train_time:49593ms step_avg:46.26ms
step:1073/1845 train_time:49654ms step_avg:46.28ms
step:1074/1845 train_time:49718ms step_avg:46.29ms
step:1075/1845 train_time:49778ms step_avg:46.31ms
step:1076/1845 train_time:49841ms step_avg:46.32ms
step:1077/1845 train_time:49901ms step_avg:46.33ms
step:1078/1845 train_time:49964ms step_avg:46.35ms
step:1079/1845 train_time:50024ms step_avg:46.36ms
step:1080/1845 train_time:50087ms step_avg:46.38ms
step:1081/1845 train_time:50147ms step_avg:46.39ms
step:1082/1845 train_time:50210ms step_avg:46.40ms
step:1083/1845 train_time:50270ms step_avg:46.42ms
step:1084/1845 train_time:50333ms step_avg:46.43ms
step:1085/1845 train_time:50394ms step_avg:46.45ms
step:1086/1845 train_time:50458ms step_avg:46.46ms
step:1087/1845 train_time:50518ms step_avg:46.48ms
step:1088/1845 train_time:50581ms step_avg:46.49ms
step:1089/1845 train_time:50642ms step_avg:46.50ms
step:1090/1845 train_time:50705ms step_avg:46.52ms
step:1091/1845 train_time:50765ms step_avg:46.53ms
step:1092/1845 train_time:50828ms step_avg:46.55ms
step:1093/1845 train_time:50888ms step_avg:46.56ms
step:1094/1845 train_time:50951ms step_avg:46.57ms
step:1095/1845 train_time:51011ms step_avg:46.59ms
step:1096/1845 train_time:51075ms step_avg:46.60ms
step:1097/1845 train_time:51135ms step_avg:46.61ms
step:1098/1845 train_time:51198ms step_avg:46.63ms
step:1099/1845 train_time:51258ms step_avg:46.64ms
step:1100/1845 train_time:51321ms step_avg:46.66ms
step:1101/1845 train_time:51381ms step_avg:46.67ms
step:1102/1845 train_time:51444ms step_avg:46.68ms
step:1103/1845 train_time:51504ms step_avg:46.69ms
step:1104/1845 train_time:51567ms step_avg:46.71ms
step:1105/1845 train_time:51628ms step_avg:46.72ms
step:1106/1845 train_time:51691ms step_avg:46.74ms
step:1107/1845 train_time:51752ms step_avg:46.75ms
step:1108/1845 train_time:51815ms step_avg:46.76ms
step:1109/1845 train_time:51875ms step_avg:46.78ms
step:1110/1845 train_time:51938ms step_avg:46.79ms
step:1111/1845 train_time:51999ms step_avg:46.80ms
step:1112/1845 train_time:52061ms step_avg:46.82ms
step:1113/1845 train_time:52121ms step_avg:46.83ms
step:1114/1845 train_time:52184ms step_avg:46.84ms
step:1115/1845 train_time:52244ms step_avg:46.86ms
step:1116/1845 train_time:52307ms step_avg:46.87ms
step:1117/1845 train_time:52367ms step_avg:46.88ms
step:1118/1845 train_time:52430ms step_avg:46.90ms
step:1119/1845 train_time:52491ms step_avg:46.91ms
step:1120/1845 train_time:52554ms step_avg:46.92ms
step:1121/1845 train_time:52615ms step_avg:46.94ms
step:1122/1845 train_time:52678ms step_avg:46.95ms
step:1123/1845 train_time:52738ms step_avg:46.96ms
step:1124/1845 train_time:52802ms step_avg:46.98ms
step:1125/1845 train_time:52863ms step_avg:46.99ms
step:1126/1845 train_time:52927ms step_avg:47.00ms
step:1127/1845 train_time:52987ms step_avg:47.02ms
step:1128/1845 train_time:53050ms step_avg:47.03ms
step:1129/1845 train_time:53110ms step_avg:47.04ms
step:1130/1845 train_time:53173ms step_avg:47.06ms
step:1131/1845 train_time:53233ms step_avg:47.07ms
step:1132/1845 train_time:53296ms step_avg:47.08ms
step:1133/1845 train_time:53356ms step_avg:47.09ms
step:1134/1845 train_time:53419ms step_avg:47.11ms
step:1135/1845 train_time:53480ms step_avg:47.12ms
step:1136/1845 train_time:53543ms step_avg:47.13ms
step:1137/1845 train_time:53604ms step_avg:47.14ms
step:1138/1845 train_time:53667ms step_avg:47.16ms
step:1139/1845 train_time:53727ms step_avg:47.17ms
step:1140/1845 train_time:53789ms step_avg:47.18ms
step:1141/1845 train_time:53850ms step_avg:47.20ms
step:1142/1845 train_time:53914ms step_avg:47.21ms
step:1143/1845 train_time:53974ms step_avg:47.22ms
step:1144/1845 train_time:54037ms step_avg:47.24ms
step:1145/1845 train_time:54098ms step_avg:47.25ms
step:1146/1845 train_time:54161ms step_avg:47.26ms
step:1147/1845 train_time:54221ms step_avg:47.27ms
step:1148/1845 train_time:54284ms step_avg:47.29ms
step:1149/1845 train_time:54343ms step_avg:47.30ms
step:1150/1845 train_time:54406ms step_avg:47.31ms
step:1151/1845 train_time:54467ms step_avg:47.32ms
step:1152/1845 train_time:54529ms step_avg:47.33ms
step:1153/1845 train_time:54590ms step_avg:47.35ms
step:1154/1845 train_time:54653ms step_avg:47.36ms
step:1155/1845 train_time:54713ms step_avg:47.37ms
step:1156/1845 train_time:54776ms step_avg:47.38ms
step:1157/1845 train_time:54836ms step_avg:47.39ms
step:1158/1845 train_time:54899ms step_avg:47.41ms
step:1159/1845 train_time:54960ms step_avg:47.42ms
step:1160/1845 train_time:55023ms step_avg:47.43ms
step:1161/1845 train_time:55083ms step_avg:47.44ms
step:1162/1845 train_time:55146ms step_avg:47.46ms
step:1163/1845 train_time:55207ms step_avg:47.47ms
step:1164/1845 train_time:55270ms step_avg:47.48ms
step:1165/1845 train_time:55329ms step_avg:47.49ms
step:1166/1845 train_time:55391ms step_avg:47.51ms
step:1167/1845 train_time:55452ms step_avg:47.52ms
step:1168/1845 train_time:55515ms step_avg:47.53ms
step:1169/1845 train_time:55575ms step_avg:47.54ms
step:1170/1845 train_time:55639ms step_avg:47.55ms
step:1171/1845 train_time:55699ms step_avg:47.57ms
step:1172/1845 train_time:55762ms step_avg:47.58ms
step:1173/1845 train_time:55822ms step_avg:47.59ms
step:1174/1845 train_time:55885ms step_avg:47.60ms
step:1175/1845 train_time:55945ms step_avg:47.61ms
step:1176/1845 train_time:56008ms step_avg:47.63ms
step:1177/1845 train_time:56068ms step_avg:47.64ms
step:1178/1845 train_time:56130ms step_avg:47.65ms
step:1179/1845 train_time:56191ms step_avg:47.66ms
step:1180/1845 train_time:56255ms step_avg:47.67ms
step:1181/1845 train_time:56315ms step_avg:47.68ms
step:1182/1845 train_time:56379ms step_avg:47.70ms
step:1183/1845 train_time:56439ms step_avg:47.71ms
step:1184/1845 train_time:56502ms step_avg:47.72ms
step:1185/1845 train_time:56562ms step_avg:47.73ms
step:1186/1845 train_time:56625ms step_avg:47.74ms
step:1187/1845 train_time:56685ms step_avg:47.76ms
step:1188/1845 train_time:56748ms step_avg:47.77ms
step:1189/1845 train_time:56808ms step_avg:47.78ms
step:1190/1845 train_time:56871ms step_avg:47.79ms
step:1191/1845 train_time:56931ms step_avg:47.80ms
step:1192/1845 train_time:56994ms step_avg:47.81ms
step:1193/1845 train_time:57054ms step_avg:47.82ms
step:1194/1845 train_time:57117ms step_avg:47.84ms
step:1195/1845 train_time:57178ms step_avg:47.85ms
step:1196/1845 train_time:57241ms step_avg:47.86ms
step:1197/1845 train_time:57301ms step_avg:47.87ms
step:1198/1845 train_time:57364ms step_avg:47.88ms
step:1199/1845 train_time:57424ms step_avg:47.89ms
step:1200/1845 train_time:57487ms step_avg:47.91ms
step:1201/1845 train_time:57548ms step_avg:47.92ms
step:1202/1845 train_time:57611ms step_avg:47.93ms
step:1203/1845 train_time:57671ms step_avg:47.94ms
step:1204/1845 train_time:57733ms step_avg:47.95ms
step:1205/1845 train_time:57795ms step_avg:47.96ms
step:1206/1845 train_time:57882ms step_avg:48.00ms
step:1207/1845 train_time:57972ms step_avg:48.03ms
step:1208/1845 train_time:58061ms step_avg:48.06ms
step:1209/1845 train_time:58146ms step_avg:48.09ms
step:1210/1845 train_time:58236ms step_avg:48.13ms
step:1211/1845 train_time:58323ms step_avg:48.16ms
step:1212/1845 train_time:58413ms step_avg:48.20ms
step:1213/1845 train_time:58500ms step_avg:48.23ms
step:1214/1845 train_time:58591ms step_avg:48.26ms
step:1215/1845 train_time:58679ms step_avg:48.30ms
step:1216/1845 train_time:58770ms step_avg:48.33ms
step:1217/1845 train_time:58856ms step_avg:48.36ms
step:1218/1845 train_time:58944ms step_avg:48.39ms
step:1219/1845 train_time:59030ms step_avg:48.42ms
step:1220/1845 train_time:59119ms step_avg:48.46ms
step:1221/1845 train_time:59205ms step_avg:48.49ms
step:1222/1845 train_time:59296ms step_avg:48.52ms
step:1223/1845 train_time:59382ms step_avg:48.55ms
step:1224/1845 train_time:59472ms step_avg:48.59ms
step:1225/1845 train_time:59559ms step_avg:48.62ms
step:1226/1845 train_time:59647ms step_avg:48.65ms
step:1227/1845 train_time:59733ms step_avg:48.68ms
step:1228/1845 train_time:59822ms step_avg:48.72ms
step:1229/1845 train_time:59909ms step_avg:48.75ms
step:1230/1845 train_time:59999ms step_avg:48.78ms
step:1231/1845 train_time:60085ms step_avg:48.81ms
step:1232/1845 train_time:60174ms step_avg:48.84ms
step:1233/1845 train_time:60261ms step_avg:48.87ms
step:1234/1845 train_time:60350ms step_avg:48.91ms
step:1235/1845 train_time:60436ms step_avg:48.94ms
step:1236/1845 train_time:60525ms step_avg:48.97ms
step:1237/1845 train_time:60612ms step_avg:49.00ms
step:1238/1845 train_time:60702ms step_avg:49.03ms
step:1239/1845 train_time:60788ms step_avg:49.06ms
step:1240/1845 train_time:60879ms step_avg:49.10ms
step:1241/1845 train_time:60966ms step_avg:49.13ms
step:1242/1845 train_time:61055ms step_avg:49.16ms
step:1243/1845 train_time:61141ms step_avg:49.19ms
step:1244/1845 train_time:61230ms step_avg:49.22ms
step:1245/1845 train_time:61316ms step_avg:49.25ms
step:1246/1845 train_time:61404ms step_avg:49.28ms
step:1247/1845 train_time:61492ms step_avg:49.31ms
step:1248/1845 train_time:61581ms step_avg:49.34ms
step:1249/1845 train_time:61669ms step_avg:49.37ms
step:1250/1845 train_time:61758ms step_avg:49.41ms
step:1250/1845 val_loss:3.5341 train_time:61856ms step_avg:49.48ms
step:1251/1845 train_time:61874ms step_avg:49.46ms
step:1252/1845 train_time:61935ms step_avg:49.47ms
step:1253/1845 train_time:62030ms step_avg:49.51ms
step:1254/1845 train_time:62124ms step_avg:49.54ms
step:1255/1845 train_time:62210ms step_avg:49.57ms
step:1256/1845 train_time:62299ms step_avg:49.60ms
step:1257/1845 train_time:62384ms step_avg:49.63ms
step:1258/1845 train_time:62471ms step_avg:49.66ms
step:1259/1845 train_time:62557ms step_avg:49.69ms
step:1260/1845 train_time:62645ms step_avg:49.72ms
step:1261/1845 train_time:62731ms step_avg:49.75ms
step:1262/1845 train_time:62821ms step_avg:49.78ms
step:1263/1845 train_time:62910ms step_avg:49.81ms
step:1264/1845 train_time:63001ms step_avg:49.84ms
step:1265/1845 train_time:63090ms step_avg:49.87ms
step:1266/1845 train_time:63181ms step_avg:49.91ms
step:1267/1845 train_time:63267ms step_avg:49.93ms
step:1268/1845 train_time:63355ms step_avg:49.96ms
step:1269/1845 train_time:63441ms step_avg:49.99ms
step:1270/1845 train_time:63529ms step_avg:50.02ms
step:1271/1845 train_time:63615ms step_avg:50.05ms
step:1272/1845 train_time:63703ms step_avg:50.08ms
step:1273/1845 train_time:63789ms step_avg:50.11ms
step:1274/1845 train_time:63879ms step_avg:50.14ms
step:1275/1845 train_time:63968ms step_avg:50.17ms
step:1276/1845 train_time:64058ms step_avg:50.20ms
step:1277/1845 train_time:64147ms step_avg:50.23ms
step:1278/1845 train_time:64238ms step_avg:50.26ms
step:1279/1845 train_time:64324ms step_avg:50.29ms
step:1280/1845 train_time:64412ms step_avg:50.32ms
step:1281/1845 train_time:64498ms step_avg:50.35ms
step:1282/1845 train_time:64586ms step_avg:50.38ms
step:1283/1845 train_time:64671ms step_avg:50.41ms
step:1284/1845 train_time:64760ms step_avg:50.44ms
step:1285/1845 train_time:64847ms step_avg:50.46ms
step:1286/1845 train_time:64937ms step_avg:50.50ms
step:1287/1845 train_time:65025ms step_avg:50.52ms
step:1288/1845 train_time:65115ms step_avg:50.55ms
step:1289/1845 train_time:65203ms step_avg:50.58ms
step:1290/1845 train_time:65291ms step_avg:50.61ms
step:1291/1845 train_time:65377ms step_avg:50.64ms
step:1292/1845 train_time:65466ms step_avg:50.67ms
step:1293/1845 train_time:65552ms step_avg:50.70ms
step:1294/1845 train_time:65642ms step_avg:50.73ms
step:1295/1845 train_time:65728ms step_avg:50.76ms
step:1296/1845 train_time:65818ms step_avg:50.79ms
step:1297/1845 train_time:65905ms step_avg:50.81ms
step:1298/1845 train_time:65995ms step_avg:50.84ms
step:1299/1845 train_time:66082ms step_avg:50.87ms
step:1300/1845 train_time:66171ms step_avg:50.90ms
step:1301/1845 train_time:66258ms step_avg:50.93ms
step:1302/1845 train_time:66346ms step_avg:50.96ms
step:1303/1845 train_time:66433ms step_avg:50.98ms
step:1304/1845 train_time:66522ms step_avg:51.01ms
step:1305/1845 train_time:66607ms step_avg:51.04ms
step:1306/1845 train_time:66697ms step_avg:51.07ms
step:1307/1845 train_time:66783ms step_avg:51.10ms
step:1308/1845 train_time:66871ms step_avg:51.12ms
step:1309/1845 train_time:66959ms step_avg:51.15ms
step:1310/1845 train_time:67049ms step_avg:51.18ms
step:1311/1845 train_time:67136ms step_avg:51.21ms
step:1312/1845 train_time:67227ms step_avg:51.24ms
step:1313/1845 train_time:67313ms step_avg:51.27ms
step:1314/1845 train_time:67402ms step_avg:51.30ms
step:1315/1845 train_time:67488ms step_avg:51.32ms
step:1316/1845 train_time:67577ms step_avg:51.35ms
step:1317/1845 train_time:67664ms step_avg:51.38ms
step:1318/1845 train_time:67752ms step_avg:51.41ms
step:1319/1845 train_time:67839ms step_avg:51.43ms
step:1320/1845 train_time:67928ms step_avg:51.46ms
step:1321/1845 train_time:68015ms step_avg:51.49ms
step:1322/1845 train_time:68105ms step_avg:51.52ms
step:1323/1845 train_time:68192ms step_avg:51.54ms
step:1324/1845 train_time:68282ms step_avg:51.57ms
step:1325/1845 train_time:68368ms step_avg:51.60ms
step:1326/1845 train_time:68457ms step_avg:51.63ms
step:1327/1845 train_time:68543ms step_avg:51.65ms
step:1328/1845 train_time:68632ms step_avg:51.68ms
step:1329/1845 train_time:68719ms step_avg:51.71ms
step:1330/1845 train_time:68808ms step_avg:51.74ms
step:1331/1845 train_time:68896ms step_avg:51.76ms
step:1332/1845 train_time:68986ms step_avg:51.79ms
step:1333/1845 train_time:69073ms step_avg:51.82ms
step:1334/1845 train_time:69163ms step_avg:51.85ms
step:1335/1845 train_time:69250ms step_avg:51.87ms
step:1336/1845 train_time:69340ms step_avg:51.90ms
step:1337/1845 train_time:69427ms step_avg:51.93ms
step:1338/1845 train_time:69516ms step_avg:51.96ms
step:1339/1845 train_time:69602ms step_avg:51.98ms
step:1340/1845 train_time:69691ms step_avg:52.01ms
step:1341/1845 train_time:69777ms step_avg:52.03ms
step:1342/1845 train_time:69866ms step_avg:52.06ms
step:1343/1845 train_time:69953ms step_avg:52.09ms
step:1344/1845 train_time:70045ms step_avg:52.12ms
step:1345/1845 train_time:70131ms step_avg:52.14ms
step:1346/1845 train_time:70221ms step_avg:52.17ms
step:1347/1845 train_time:70307ms step_avg:52.20ms
step:1348/1845 train_time:70398ms step_avg:52.22ms
step:1349/1845 train_time:70485ms step_avg:52.25ms
step:1350/1845 train_time:70573ms step_avg:52.28ms
step:1351/1845 train_time:70660ms step_avg:52.30ms
step:1352/1845 train_time:70749ms step_avg:52.33ms
step:1353/1845 train_time:70835ms step_avg:52.35ms
step:1354/1845 train_time:70926ms step_avg:52.38ms
step:1355/1845 train_time:71011ms step_avg:52.41ms
step:1356/1845 train_time:71103ms step_avg:52.44ms
step:1357/1845 train_time:71190ms step_avg:52.46ms
step:1358/1845 train_time:71280ms step_avg:52.49ms
step:1359/1845 train_time:71367ms step_avg:52.51ms
step:1360/1845 train_time:71456ms step_avg:52.54ms
step:1361/1845 train_time:71542ms step_avg:52.57ms
step:1362/1845 train_time:71631ms step_avg:52.59ms
step:1363/1845 train_time:71717ms step_avg:52.62ms
step:1364/1845 train_time:71806ms step_avg:52.64ms
step:1365/1845 train_time:71893ms step_avg:52.67ms
step:1366/1845 train_time:71982ms step_avg:52.70ms
step:1367/1845 train_time:72069ms step_avg:52.72ms
step:1368/1845 train_time:72160ms step_avg:52.75ms
step:1369/1845 train_time:72246ms step_avg:52.77ms
step:1370/1845 train_time:72336ms step_avg:52.80ms
step:1371/1845 train_time:72423ms step_avg:52.82ms
step:1372/1845 train_time:72511ms step_avg:52.85ms
step:1373/1845 train_time:72599ms step_avg:52.88ms
step:1374/1845 train_time:72688ms step_avg:52.90ms
step:1375/1845 train_time:72773ms step_avg:52.93ms
step:1376/1845 train_time:72863ms step_avg:52.95ms
step:1377/1845 train_time:72950ms step_avg:52.98ms
step:1378/1845 train_time:73041ms step_avg:53.00ms
step:1379/1845 train_time:73127ms step_avg:53.03ms
step:1380/1845 train_time:73217ms step_avg:53.06ms
step:1381/1845 train_time:73303ms step_avg:53.08ms
step:1382/1845 train_time:73392ms step_avg:53.11ms
step:1383/1845 train_time:73479ms step_avg:53.13ms
step:1384/1845 train_time:73569ms step_avg:53.16ms
step:1385/1845 train_time:73656ms step_avg:53.18ms
step:1386/1845 train_time:73744ms step_avg:53.21ms
step:1387/1845 train_time:73830ms step_avg:53.23ms
step:1388/1845 train_time:73919ms step_avg:53.26ms
step:1389/1845 train_time:74006ms step_avg:53.28ms
step:1390/1845 train_time:74097ms step_avg:53.31ms
step:1391/1845 train_time:74183ms step_avg:53.33ms
step:1392/1845 train_time:74271ms step_avg:53.36ms
step:1393/1845 train_time:74359ms step_avg:53.38ms
step:1394/1845 train_time:74448ms step_avg:53.41ms
step:1395/1845 train_time:74534ms step_avg:53.43ms
step:1396/1845 train_time:74623ms step_avg:53.46ms
step:1397/1845 train_time:74710ms step_avg:53.48ms
step:1398/1845 train_time:74799ms step_avg:53.50ms
step:1399/1845 train_time:74886ms step_avg:53.53ms
step:1400/1845 train_time:74976ms step_avg:53.55ms
step:1401/1845 train_time:75062ms step_avg:53.58ms
step:1402/1845 train_time:75151ms step_avg:53.60ms
step:1403/1845 train_time:75239ms step_avg:53.63ms
step:1404/1845 train_time:75329ms step_avg:53.65ms
step:1405/1845 train_time:75415ms step_avg:53.68ms
step:1406/1845 train_time:75504ms step_avg:53.70ms
step:1407/1845 train_time:75590ms step_avg:53.72ms
step:1408/1845 train_time:75679ms step_avg:53.75ms
step:1409/1845 train_time:75766ms step_avg:53.77ms
step:1410/1845 train_time:75856ms step_avg:53.80ms
step:1411/1845 train_time:75943ms step_avg:53.82ms
step:1412/1845 train_time:76032ms step_avg:53.85ms
step:1413/1845 train_time:76119ms step_avg:53.87ms
step:1414/1845 train_time:76208ms step_avg:53.90ms
step:1415/1845 train_time:76295ms step_avg:53.92ms
step:1416/1845 train_time:76384ms step_avg:53.94ms
step:1417/1845 train_time:76469ms step_avg:53.97ms
step:1418/1845 train_time:76560ms step_avg:53.99ms
step:1419/1845 train_time:76647ms step_avg:54.01ms
step:1420/1845 train_time:76735ms step_avg:54.04ms
step:1421/1845 train_time:76822ms step_avg:54.06ms
step:1422/1845 train_time:76911ms step_avg:54.09ms
step:1423/1845 train_time:76998ms step_avg:54.11ms
step:1424/1845 train_time:77087ms step_avg:54.13ms
step:1425/1845 train_time:77173ms step_avg:54.16ms
step:1426/1845 train_time:77263ms step_avg:54.18ms
step:1427/1845 train_time:77350ms step_avg:54.20ms
step:1428/1845 train_time:77441ms step_avg:54.23ms
step:1429/1845 train_time:77527ms step_avg:54.25ms
step:1430/1845 train_time:77618ms step_avg:54.28ms
step:1431/1845 train_time:77704ms step_avg:54.30ms
step:1432/1845 train_time:77792ms step_avg:54.32ms
step:1433/1845 train_time:77878ms step_avg:54.35ms
step:1434/1845 train_time:77969ms step_avg:54.37ms
step:1435/1845 train_time:78055ms step_avg:54.39ms
step:1436/1845 train_time:78144ms step_avg:54.42ms
step:1437/1845 train_time:78231ms step_avg:54.44ms
step:1438/1845 train_time:78321ms step_avg:54.46ms
step:1439/1845 train_time:78407ms step_avg:54.49ms
step:1440/1845 train_time:78498ms step_avg:54.51ms
step:1441/1845 train_time:78584ms step_avg:54.53ms
step:1442/1845 train_time:78673ms step_avg:54.56ms
step:1443/1845 train_time:78760ms step_avg:54.58ms
step:1444/1845 train_time:78848ms step_avg:54.60ms
step:1445/1845 train_time:78935ms step_avg:54.63ms
step:1446/1845 train_time:79025ms step_avg:54.65ms
step:1447/1845 train_time:79111ms step_avg:54.67ms
step:1448/1845 train_time:79201ms step_avg:54.70ms
step:1449/1845 train_time:79288ms step_avg:54.72ms
step:1450/1845 train_time:79376ms step_avg:54.74ms
step:1451/1845 train_time:79463ms step_avg:54.76ms
step:1452/1845 train_time:79552ms step_avg:54.79ms
step:1453/1845 train_time:79639ms step_avg:54.81ms
step:1454/1845 train_time:79730ms step_avg:54.83ms
step:1455/1845 train_time:79817ms step_avg:54.86ms
step:1456/1845 train_time:79906ms step_avg:54.88ms
step:1457/1845 train_time:79993ms step_avg:54.90ms
step:1458/1845 train_time:80082ms step_avg:54.93ms
step:1459/1845 train_time:80168ms step_avg:54.95ms
step:1460/1845 train_time:80258ms step_avg:54.97ms
step:1461/1845 train_time:80346ms step_avg:54.99ms
step:1462/1845 train_time:80435ms step_avg:55.02ms
step:1463/1845 train_time:80521ms step_avg:55.04ms
step:1464/1845 train_time:80610ms step_avg:55.06ms
step:1465/1845 train_time:80698ms step_avg:55.08ms
step:1466/1845 train_time:80787ms step_avg:55.11ms
step:1467/1845 train_time:80873ms step_avg:55.13ms
step:1468/1845 train_time:80964ms step_avg:55.15ms
step:1469/1845 train_time:81050ms step_avg:55.17ms
step:1470/1845 train_time:81139ms step_avg:55.20ms
step:1471/1845 train_time:81225ms step_avg:55.22ms
step:1472/1845 train_time:81316ms step_avg:55.24ms
step:1473/1845 train_time:81403ms step_avg:55.26ms
step:1474/1845 train_time:81491ms step_avg:55.29ms
step:1475/1845 train_time:81578ms step_avg:55.31ms
step:1476/1845 train_time:81667ms step_avg:55.33ms
step:1477/1845 train_time:81753ms step_avg:55.35ms
step:1478/1845 train_time:81843ms step_avg:55.37ms
step:1479/1845 train_time:81929ms step_avg:55.40ms
step:1480/1845 train_time:82019ms step_avg:55.42ms
step:1481/1845 train_time:82105ms step_avg:55.44ms
step:1482/1845 train_time:82195ms step_avg:55.46ms
step:1483/1845 train_time:82282ms step_avg:55.48ms
step:1484/1845 train_time:82370ms step_avg:55.51ms
step:1485/1845 train_time:82457ms step_avg:55.53ms
step:1486/1845 train_time:82546ms step_avg:55.55ms
step:1487/1845 train_time:82633ms step_avg:55.57ms
step:1488/1845 train_time:82725ms step_avg:55.59ms
step:1489/1845 train_time:82810ms step_avg:55.61ms
step:1490/1845 train_time:82901ms step_avg:55.64ms
step:1491/1845 train_time:82988ms step_avg:55.66ms
step:1492/1845 train_time:83077ms step_avg:55.68ms
step:1493/1845 train_time:83163ms step_avg:55.70ms
step:1494/1845 train_time:83251ms step_avg:55.72ms
step:1495/1845 train_time:83340ms step_avg:55.75ms
step:1496/1845 train_time:83429ms step_avg:55.77ms
step:1497/1845 train_time:83516ms step_avg:55.79ms
step:1498/1845 train_time:83605ms step_avg:55.81ms
step:1499/1845 train_time:83691ms step_avg:55.83ms
step:1500/1845 train_time:83781ms step_avg:55.85ms
step:1500/1845 val_loss:3.4022 train_time:83877ms step_avg:55.92ms
step:1501/1845 train_time:83895ms step_avg:55.89ms
step:1502/1845 train_time:83958ms step_avg:55.90ms
step:1503/1845 train_time:84050ms step_avg:55.92ms
step:1504/1845 train_time:84138ms step_avg:55.94ms
step:1505/1845 train_time:84226ms step_avg:55.96ms
step:1506/1845 train_time:84315ms step_avg:55.99ms
step:1507/1845 train_time:84401ms step_avg:56.01ms
step:1508/1845 train_time:84491ms step_avg:56.03ms
step:1509/1845 train_time:84575ms step_avg:56.05ms
step:1510/1845 train_time:84664ms step_avg:56.07ms
step:1511/1845 train_time:84748ms step_avg:56.09ms
step:1512/1845 train_time:84837ms step_avg:56.11ms
step:1513/1845 train_time:84927ms step_avg:56.13ms
step:1514/1845 train_time:85021ms step_avg:56.16ms
step:1515/1845 train_time:85112ms step_avg:56.18ms
step:1516/1845 train_time:85201ms step_avg:56.20ms
step:1517/1845 train_time:85288ms step_avg:56.22ms
step:1518/1845 train_time:85376ms step_avg:56.24ms
step:1519/1845 train_time:85462ms step_avg:56.26ms
step:1520/1845 train_time:85551ms step_avg:56.28ms
step:1521/1845 train_time:85636ms step_avg:56.30ms
step:1522/1845 train_time:85724ms step_avg:56.32ms
step:1523/1845 train_time:85810ms step_avg:56.34ms
step:1524/1845 train_time:85899ms step_avg:56.36ms
step:1525/1845 train_time:85989ms step_avg:56.39ms
step:1526/1845 train_time:86079ms step_avg:56.41ms
step:1527/1845 train_time:86168ms step_avg:56.43ms
step:1528/1845 train_time:86257ms step_avg:56.45ms
step:1529/1845 train_time:86343ms step_avg:56.47ms
step:1530/1845 train_time:86432ms step_avg:56.49ms
step:1531/1845 train_time:86518ms step_avg:56.51ms
step:1532/1845 train_time:86607ms step_avg:56.53ms
step:1533/1845 train_time:86693ms step_avg:56.55ms
step:1534/1845 train_time:86782ms step_avg:56.57ms
step:1535/1845 train_time:86868ms step_avg:56.59ms
step:1536/1845 train_time:86956ms step_avg:56.61ms
step:1537/1845 train_time:87045ms step_avg:56.63ms
step:1538/1845 train_time:87137ms step_avg:56.66ms
step:1539/1845 train_time:87223ms step_avg:56.68ms
step:1540/1845 train_time:87312ms step_avg:56.70ms
step:1541/1845 train_time:87399ms step_avg:56.72ms
step:1542/1845 train_time:87489ms step_avg:56.74ms
step:1543/1845 train_time:87574ms step_avg:56.76ms
step:1544/1845 train_time:87665ms step_avg:56.78ms
step:1545/1845 train_time:87752ms step_avg:56.80ms
step:1546/1845 train_time:87840ms step_avg:56.82ms
step:1547/1845 train_time:87927ms step_avg:56.84ms
step:1548/1845 train_time:88017ms step_avg:56.86ms
step:1549/1845 train_time:88104ms step_avg:56.88ms
step:1550/1845 train_time:88194ms step_avg:56.90ms
step:1551/1845 train_time:88280ms step_avg:56.92ms
step:1552/1845 train_time:88370ms step_avg:56.94ms
step:1553/1845 train_time:88456ms step_avg:56.96ms
step:1554/1845 train_time:88545ms step_avg:56.98ms
step:1555/1845 train_time:88631ms step_avg:57.00ms
step:1556/1845 train_time:88720ms step_avg:57.02ms
step:1557/1845 train_time:88807ms step_avg:57.04ms
step:1558/1845 train_time:88896ms step_avg:57.06ms
step:1559/1845 train_time:88983ms step_avg:57.08ms
step:1560/1845 train_time:89073ms step_avg:57.10ms
step:1561/1845 train_time:89160ms step_avg:57.12ms
step:1562/1845 train_time:89250ms step_avg:57.14ms
step:1563/1845 train_time:89336ms step_avg:57.16ms
step:1564/1845 train_time:89426ms step_avg:57.18ms
step:1565/1845 train_time:89512ms step_avg:57.20ms
step:1566/1845 train_time:89600ms step_avg:57.22ms
step:1567/1845 train_time:89687ms step_avg:57.23ms
step:1568/1845 train_time:89776ms step_avg:57.25ms
step:1569/1845 train_time:89862ms step_avg:57.27ms
step:1570/1845 train_time:89953ms step_avg:57.29ms
step:1571/1845 train_time:90039ms step_avg:57.31ms
step:1572/1845 train_time:90128ms step_avg:57.33ms
step:1573/1845 train_time:90214ms step_avg:57.35ms
step:1574/1845 train_time:90304ms step_avg:57.37ms
step:1575/1845 train_time:90392ms step_avg:57.39ms
step:1576/1845 train_time:90481ms step_avg:57.41ms
step:1577/1845 train_time:90567ms step_avg:57.43ms
step:1578/1845 train_time:90656ms step_avg:57.45ms
step:1579/1845 train_time:90743ms step_avg:57.47ms
step:1580/1845 train_time:90832ms step_avg:57.49ms
step:1581/1845 train_time:90918ms step_avg:57.51ms
step:1582/1845 train_time:91008ms step_avg:57.53ms
step:1583/1845 train_time:91095ms step_avg:57.55ms
step:1584/1845 train_time:91186ms step_avg:57.57ms
step:1585/1845 train_time:91273ms step_avg:57.59ms
step:1586/1845 train_time:91361ms step_avg:57.60ms
step:1587/1845 train_time:91448ms step_avg:57.62ms
step:1588/1845 train_time:91536ms step_avg:57.64ms
step:1589/1845 train_time:91623ms step_avg:57.66ms
step:1590/1845 train_time:91712ms step_avg:57.68ms
step:1591/1845 train_time:91798ms step_avg:57.70ms
step:1592/1845 train_time:91888ms step_avg:57.72ms
step:1593/1845 train_time:91975ms step_avg:57.74ms
step:1594/1845 train_time:92066ms step_avg:57.76ms
step:1595/1845 train_time:92153ms step_avg:57.78ms
step:1596/1845 train_time:92243ms step_avg:57.80ms
step:1597/1845 train_time:92329ms step_avg:57.81ms
step:1598/1845 train_time:92419ms step_avg:57.83ms
step:1599/1845 train_time:92505ms step_avg:57.85ms
step:1600/1845 train_time:92595ms step_avg:57.87ms
step:1601/1845 train_time:92681ms step_avg:57.89ms
step:1602/1845 train_time:92771ms step_avg:57.91ms
step:1603/1845 train_time:92856ms step_avg:57.93ms
step:1604/1845 train_time:92946ms step_avg:57.95ms
step:1605/1845 train_time:93033ms step_avg:57.96ms
step:1606/1845 train_time:93123ms step_avg:57.98ms
step:1607/1845 train_time:93210ms step_avg:58.00ms
step:1608/1845 train_time:93299ms step_avg:58.02ms
step:1609/1845 train_time:93385ms step_avg:58.04ms
step:1610/1845 train_time:93474ms step_avg:58.06ms
step:1611/1845 train_time:93560ms step_avg:58.08ms
step:1612/1845 train_time:93650ms step_avg:58.10ms
step:1613/1845 train_time:93735ms step_avg:58.11ms
step:1614/1845 train_time:93825ms step_avg:58.13ms
step:1615/1845 train_time:93912ms step_avg:58.15ms
step:1616/1845 train_time:93999ms step_avg:58.17ms
step:1617/1845 train_time:94088ms step_avg:58.19ms
step:1618/1845 train_time:94177ms step_avg:58.21ms
step:1619/1845 train_time:94264ms step_avg:58.22ms
step:1620/1845 train_time:94354ms step_avg:58.24ms
step:1621/1845 train_time:94441ms step_avg:58.26ms
step:1622/1845 train_time:94530ms step_avg:58.28ms
step:1623/1845 train_time:94617ms step_avg:58.30ms
step:1624/1845 train_time:94706ms step_avg:58.32ms
step:1625/1845 train_time:94793ms step_avg:58.33ms
step:1626/1845 train_time:94881ms step_avg:58.35ms
step:1627/1845 train_time:94968ms step_avg:58.37ms
step:1628/1845 train_time:95056ms step_avg:58.39ms
step:1629/1845 train_time:95143ms step_avg:58.41ms
step:1630/1845 train_time:95233ms step_avg:58.43ms
step:1631/1845 train_time:95320ms step_avg:58.44ms
step:1632/1845 train_time:95409ms step_avg:58.46ms
step:1633/1845 train_time:95495ms step_avg:58.48ms
step:1634/1845 train_time:95586ms step_avg:58.50ms
step:1635/1845 train_time:95672ms step_avg:58.51ms
step:1636/1845 train_time:95760ms step_avg:58.53ms
step:1637/1845 train_time:95848ms step_avg:58.55ms
step:1638/1845 train_time:95937ms step_avg:58.57ms
step:1639/1845 train_time:96024ms step_avg:58.59ms
step:1640/1845 train_time:96114ms step_avg:58.61ms
step:1641/1845 train_time:96199ms step_avg:58.62ms
step:1642/1845 train_time:96290ms step_avg:58.64ms
step:1643/1845 train_time:96376ms step_avg:58.66ms
step:1644/1845 train_time:96465ms step_avg:58.68ms
step:1645/1845 train_time:96552ms step_avg:58.69ms
step:1646/1845 train_time:96642ms step_avg:58.71ms
step:1647/1845 train_time:96729ms step_avg:58.73ms
step:1648/1845 train_time:96817ms step_avg:58.75ms
step:1649/1845 train_time:96903ms step_avg:58.76ms
step:1650/1845 train_time:96993ms step_avg:58.78ms
step:1651/1845 train_time:97078ms step_avg:58.80ms
step:1652/1845 train_time:97169ms step_avg:58.82ms
step:1653/1845 train_time:97256ms step_avg:58.84ms
step:1654/1845 train_time:97345ms step_avg:58.85ms
step:1655/1845 train_time:97433ms step_avg:58.87ms
step:1656/1845 train_time:97521ms step_avg:58.89ms
step:1657/1845 train_time:97607ms step_avg:58.91ms
step:1658/1845 train_time:97696ms step_avg:58.92ms
step:1659/1845 train_time:97783ms step_avg:58.94ms
step:1660/1845 train_time:97872ms step_avg:58.96ms
step:1661/1845 train_time:97958ms step_avg:58.98ms
step:1662/1845 train_time:98048ms step_avg:58.99ms
step:1663/1845 train_time:98135ms step_avg:59.01ms
step:1664/1845 train_time:98226ms step_avg:59.03ms
step:1665/1845 train_time:98312ms step_avg:59.05ms
step:1666/1845 train_time:98402ms step_avg:59.06ms
step:1667/1845 train_time:98488ms step_avg:59.08ms
step:1668/1845 train_time:98577ms step_avg:59.10ms
step:1669/1845 train_time:98664ms step_avg:59.12ms
step:1670/1845 train_time:98754ms step_avg:59.13ms
step:1671/1845 train_time:98841ms step_avg:59.15ms
step:1672/1845 train_time:98930ms step_avg:59.17ms
step:1673/1845 train_time:99017ms step_avg:59.19ms
step:1674/1845 train_time:99107ms step_avg:59.20ms
step:1675/1845 train_time:99194ms step_avg:59.22ms
step:1676/1845 train_time:99283ms step_avg:59.24ms
step:1677/1845 train_time:99371ms step_avg:59.26ms
step:1678/1845 train_time:99459ms step_avg:59.27ms
step:1679/1845 train_time:99545ms step_avg:59.29ms
step:1680/1845 train_time:99635ms step_avg:59.31ms
step:1681/1845 train_time:99721ms step_avg:59.32ms
step:1682/1845 train_time:99810ms step_avg:59.34ms
step:1683/1845 train_time:99896ms step_avg:59.36ms
step:1684/1845 train_time:99986ms step_avg:59.37ms
step:1685/1845 train_time:100072ms step_avg:59.39ms
step:1686/1845 train_time:100162ms step_avg:59.41ms
step:1687/1845 train_time:100249ms step_avg:59.42ms
step:1688/1845 train_time:100338ms step_avg:59.44ms
step:1689/1845 train_time:100424ms step_avg:59.46ms
step:1690/1845 train_time:100514ms step_avg:59.48ms
step:1691/1845 train_time:100601ms step_avg:59.49ms
step:1692/1845 train_time:100690ms step_avg:59.51ms
step:1693/1845 train_time:100777ms step_avg:59.53ms
step:1694/1845 train_time:100867ms step_avg:59.54ms
step:1695/1845 train_time:100954ms step_avg:59.56ms
step:1696/1845 train_time:101043ms step_avg:59.58ms
step:1697/1845 train_time:101129ms step_avg:59.59ms
step:1698/1845 train_time:101218ms step_avg:59.61ms
step:1699/1845 train_time:101304ms step_avg:59.63ms
step:1700/1845 train_time:101394ms step_avg:59.64ms
step:1701/1845 train_time:101480ms step_avg:59.66ms
step:1702/1845 train_time:101570ms step_avg:59.68ms
step:1703/1845 train_time:101656ms step_avg:59.69ms
step:1704/1845 train_time:101745ms step_avg:59.71ms
step:1705/1845 train_time:101832ms step_avg:59.73ms
step:1706/1845 train_time:101921ms step_avg:59.74ms
step:1707/1845 train_time:102008ms step_avg:59.76ms
step:1708/1845 train_time:102097ms step_avg:59.78ms
step:1709/1845 train_time:102184ms step_avg:59.79ms
step:1710/1845 train_time:102273ms step_avg:59.81ms
step:1711/1845 train_time:102360ms step_avg:59.82ms
step:1712/1845 train_time:102451ms step_avg:59.84ms
step:1713/1845 train_time:102537ms step_avg:59.86ms
step:1714/1845 train_time:102627ms step_avg:59.88ms
step:1715/1845 train_time:102713ms step_avg:59.89ms
step:1716/1845 train_time:102802ms step_avg:59.91ms
step:1717/1845 train_time:102889ms step_avg:59.92ms
step:1718/1845 train_time:102977ms step_avg:59.94ms
step:1719/1845 train_time:103064ms step_avg:59.96ms
step:1720/1845 train_time:103154ms step_avg:59.97ms
step:1721/1845 train_time:103240ms step_avg:59.99ms
step:1722/1845 train_time:103331ms step_avg:60.01ms
step:1723/1845 train_time:103417ms step_avg:60.02ms
step:1724/1845 train_time:103507ms step_avg:60.04ms
step:1725/1845 train_time:103594ms step_avg:60.05ms
step:1726/1845 train_time:103683ms step_avg:60.07ms
step:1727/1845 train_time:103770ms step_avg:60.09ms
step:1728/1845 train_time:103859ms step_avg:60.10ms
step:1729/1845 train_time:103946ms step_avg:60.12ms
step:1730/1845 train_time:104035ms step_avg:60.14ms
step:1731/1845 train_time:104121ms step_avg:60.15ms
step:1732/1845 train_time:104212ms step_avg:60.17ms
step:1733/1845 train_time:104298ms step_avg:60.18ms
step:1734/1845 train_time:104390ms step_avg:60.20ms
step:1735/1845 train_time:104476ms step_avg:60.22ms
step:1736/1845 train_time:104565ms step_avg:60.23ms
step:1737/1845 train_time:104652ms step_avg:60.25ms
step:1738/1845 train_time:104741ms step_avg:60.27ms
step:1739/1845 train_time:104827ms step_avg:60.28ms
step:1740/1845 train_time:104917ms step_avg:60.30ms
step:1741/1845 train_time:105003ms step_avg:60.31ms
step:1742/1845 train_time:105093ms step_avg:60.33ms
step:1743/1845 train_time:105178ms step_avg:60.34ms
step:1744/1845 train_time:105269ms step_avg:60.36ms
step:1745/1845 train_time:105355ms step_avg:60.38ms
step:1746/1845 train_time:105445ms step_avg:60.39ms
step:1747/1845 train_time:105531ms step_avg:60.41ms
step:1748/1845 train_time:105620ms step_avg:60.42ms
step:1749/1845 train_time:105708ms step_avg:60.44ms
step:1750/1845 train_time:105798ms step_avg:60.46ms
step:1750/1845 val_loss:3.3038 train_time:105896ms step_avg:60.51ms
step:1751/1845 train_time:105914ms step_avg:60.49ms
step:1752/1845 train_time:105977ms step_avg:60.49ms
step:1753/1845 train_time:106069ms step_avg:60.51ms
step:1754/1845 train_time:106160ms step_avg:60.52ms
step:1755/1845 train_time:106251ms step_avg:60.54ms
step:1756/1845 train_time:106339ms step_avg:60.56ms
step:1757/1845 train_time:106424ms step_avg:60.57ms
step:1758/1845 train_time:106513ms step_avg:60.59ms
step:1759/1845 train_time:106597ms step_avg:60.60ms
step:1760/1845 train_time:106686ms step_avg:60.62ms
step:1761/1845 train_time:106772ms step_avg:60.63ms
step:1762/1845 train_time:106862ms step_avg:60.65ms
step:1763/1845 train_time:106953ms step_avg:60.67ms
step:1764/1845 train_time:107047ms step_avg:60.68ms
step:1765/1845 train_time:107135ms step_avg:60.70ms
step:1766/1845 train_time:107224ms step_avg:60.72ms
step:1767/1845 train_time:107310ms step_avg:60.73ms
step:1768/1845 train_time:107398ms step_avg:60.75ms
step:1769/1845 train_time:107485ms step_avg:60.76ms
step:1770/1845 train_time:107573ms step_avg:60.78ms
step:1771/1845 train_time:107658ms step_avg:60.79ms
step:1772/1845 train_time:107746ms step_avg:60.80ms
step:1773/1845 train_time:107833ms step_avg:60.82ms
step:1774/1845 train_time:107923ms step_avg:60.84ms
step:1775/1845 train_time:108013ms step_avg:60.85ms
step:1776/1845 train_time:108103ms step_avg:60.87ms
step:1777/1845 train_time:108192ms step_avg:60.88ms
step:1778/1845 train_time:108281ms step_avg:60.90ms
step:1779/1845 train_time:108367ms step_avg:60.91ms
step:1780/1845 train_time:108455ms step_avg:60.93ms
step:1781/1845 train_time:108540ms step_avg:60.94ms
step:1782/1845 train_time:108630ms step_avg:60.96ms
step:1783/1845 train_time:108715ms step_avg:60.97ms
step:1784/1845 train_time:108804ms step_avg:60.99ms
step:1785/1845 train_time:108893ms step_avg:61.00ms
step:1786/1845 train_time:108983ms step_avg:61.02ms
step:1787/1845 train_time:109072ms step_avg:61.04ms
step:1788/1845 train_time:109163ms step_avg:61.05ms
step:1789/1845 train_time:109253ms step_avg:61.07ms
step:1790/1845 train_time:109341ms step_avg:61.08ms
step:1791/1845 train_time:109427ms step_avg:61.10ms
step:1792/1845 train_time:109515ms step_avg:61.11ms
step:1793/1845 train_time:109600ms step_avg:61.13ms
step:1794/1845 train_time:109690ms step_avg:61.14ms
step:1795/1845 train_time:109776ms step_avg:61.16ms
step:1796/1845 train_time:109866ms step_avg:61.17ms
step:1797/1845 train_time:109953ms step_avg:61.19ms
step:1798/1845 train_time:110042ms step_avg:61.20ms
step:1799/1845 train_time:110131ms step_avg:61.22ms
step:1800/1845 train_time:110220ms step_avg:61.23ms
step:1801/1845 train_time:110307ms step_avg:61.25ms
step:1802/1845 train_time:110396ms step_avg:61.26ms
step:1803/1845 train_time:110481ms step_avg:61.28ms
step:1804/1845 train_time:110570ms step_avg:61.29ms
step:1805/1845 train_time:110656ms step_avg:61.31ms
step:1806/1845 train_time:110748ms step_avg:61.32ms
step:1807/1845 train_time:110834ms step_avg:61.34ms
step:1808/1845 train_time:110922ms step_avg:61.35ms
step:1809/1845 train_time:111011ms step_avg:61.37ms
step:1810/1845 train_time:111100ms step_avg:61.38ms
step:1811/1845 train_time:111189ms step_avg:61.40ms
step:1812/1845 train_time:111278ms step_avg:61.41ms
step:1813/1845 train_time:111365ms step_avg:61.43ms
step:1814/1845 train_time:111454ms step_avg:61.44ms
step:1815/1845 train_time:111541ms step_avg:61.45ms
step:1816/1845 train_time:111630ms step_avg:61.47ms
step:1817/1845 train_time:111717ms step_avg:61.48ms
step:1818/1845 train_time:111806ms step_avg:61.50ms
step:1819/1845 train_time:111892ms step_avg:61.51ms
step:1820/1845 train_time:111982ms step_avg:61.53ms
step:1821/1845 train_time:112069ms step_avg:61.54ms
step:1822/1845 train_time:112158ms step_avg:61.56ms
step:1823/1845 train_time:112247ms step_avg:61.57ms
step:1824/1845 train_time:112337ms step_avg:61.59ms
step:1825/1845 train_time:112423ms step_avg:61.60ms
step:1826/1845 train_time:112514ms step_avg:61.62ms
step:1827/1845 train_time:112599ms step_avg:61.63ms
step:1828/1845 train_time:112689ms step_avg:61.65ms
step:1829/1845 train_time:112776ms step_avg:61.66ms
step:1830/1845 train_time:112865ms step_avg:61.67ms
step:1831/1845 train_time:112952ms step_avg:61.69ms
step:1832/1845 train_time:113040ms step_avg:61.70ms
step:1833/1845 train_time:113129ms step_avg:61.72ms
step:1834/1845 train_time:113220ms step_avg:61.73ms
step:1835/1845 train_time:113308ms step_avg:61.75ms
step:1836/1845 train_time:113396ms step_avg:61.76ms
step:1837/1845 train_time:113484ms step_avg:61.78ms
step:1838/1845 train_time:113573ms step_avg:61.79ms
step:1839/1845 train_time:113658ms step_avg:61.80ms
step:1840/1845 train_time:113748ms step_avg:61.82ms
step:1841/1845 train_time:113836ms step_avg:61.83ms
step:1842/1845 train_time:113926ms step_avg:61.85ms
step:1843/1845 train_time:114014ms step_avg:61.86ms
step:1844/1845 train_time:114103ms step_avg:61.88ms
step:1845/1845 train_time:114193ms step_avg:61.89ms
step:1845/1845 val_loss:3.2772 train_time:114290ms step_avg:61.95ms
peak memory allocated: 29524 MiB reserved: 44558 MiB

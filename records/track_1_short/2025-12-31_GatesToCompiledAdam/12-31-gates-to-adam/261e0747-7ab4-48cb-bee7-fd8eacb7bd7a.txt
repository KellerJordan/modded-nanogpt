import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 17:48:18 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     93193      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A     93194      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A     93195      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A     93196      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A     93197      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A     93198      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A     93199      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A     93200      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8312 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:72ms step_avg:71.81ms
step:2/1845 train_time:92ms step_avg:45.97ms
step:3/1845 train_time:112ms step_avg:37.22ms
step:4/1845 train_time:147ms step_avg:36.69ms
step:5/1845 train_time:180ms step_avg:35.94ms
step:6/1845 train_time:259ms step_avg:43.24ms
step:7/1845 train_time:275ms step_avg:39.32ms
step:8/1845 train_time:312ms step_avg:39.03ms
step:9/1845 train_time:345ms step_avg:38.35ms
step:10/1845 train_time:380ms step_avg:38.04ms
step:11/1845 train_time:413ms step_avg:37.58ms
step:12/1845 train_time:449ms step_avg:37.39ms
step:13/1845 train_time:482ms step_avg:37.05ms
step:14/1845 train_time:518ms step_avg:36.97ms
step:15/1845 train_time:551ms step_avg:36.72ms
step:16/1845 train_time:586ms step_avg:36.64ms
step:17/1845 train_time:619ms step_avg:36.42ms
step:18/1845 train_time:655ms step_avg:36.36ms
step:19/1845 train_time:688ms step_avg:36.19ms
step:20/1845 train_time:723ms step_avg:36.14ms
step:21/1845 train_time:756ms step_avg:35.99ms
step:22/1845 train_time:791ms step_avg:35.96ms
step:23/1845 train_time:824ms step_avg:35.83ms
step:24/1845 train_time:859ms step_avg:35.81ms
step:25/1845 train_time:892ms step_avg:35.70ms
step:26/1845 train_time:928ms step_avg:35.68ms
step:27/1845 train_time:961ms step_avg:35.58ms
step:28/1845 train_time:996ms step_avg:35.58ms
step:29/1845 train_time:1029ms step_avg:35.49ms
step:30/1845 train_time:1064ms step_avg:35.48ms
step:31/1845 train_time:1097ms step_avg:35.40ms
step:32/1845 train_time:1133ms step_avg:35.40ms
step:33/1845 train_time:1166ms step_avg:35.33ms
step:34/1845 train_time:1201ms step_avg:35.33ms
step:35/1845 train_time:1235ms step_avg:35.28ms
step:36/1845 train_time:1270ms step_avg:35.28ms
step:37/1845 train_time:1303ms step_avg:35.22ms
step:38/1845 train_time:1339ms step_avg:35.23ms
step:39/1845 train_time:1372ms step_avg:35.17ms
step:40/1845 train_time:1407ms step_avg:35.18ms
step:41/1845 train_time:1441ms step_avg:35.14ms
step:42/1845 train_time:1476ms step_avg:35.15ms
step:43/1845 train_time:1509ms step_avg:35.10ms
step:44/1845 train_time:1545ms step_avg:35.11ms
step:45/1845 train_time:1578ms step_avg:35.06ms
step:46/1845 train_time:1613ms step_avg:35.07ms
step:47/1845 train_time:1646ms step_avg:35.02ms
step:48/1845 train_time:1682ms step_avg:35.03ms
step:49/1845 train_time:1715ms step_avg:35.00ms
step:50/1845 train_time:1750ms step_avg:35.00ms
step:51/1845 train_time:1783ms step_avg:34.96ms
step:52/1845 train_time:1818ms step_avg:34.97ms
step:53/1845 train_time:1851ms step_avg:34.93ms
step:54/1845 train_time:1887ms step_avg:34.94ms
step:55/1845 train_time:1920ms step_avg:34.90ms
step:56/1845 train_time:1955ms step_avg:34.91ms
step:57/1845 train_time:1988ms step_avg:34.88ms
step:58/1845 train_time:2024ms step_avg:34.89ms
step:59/1845 train_time:2057ms step_avg:34.86ms
step:60/1845 train_time:2092ms step_avg:34.87ms
step:61/1845 train_time:2125ms step_avg:34.84ms
step:62/1845 train_time:2160ms step_avg:34.85ms
step:63/1845 train_time:2193ms step_avg:34.82ms
step:64/1845 train_time:2229ms step_avg:34.82ms
step:65/1845 train_time:2262ms step_avg:34.79ms
step:66/1845 train_time:2297ms step_avg:34.80ms
step:67/1845 train_time:2330ms step_avg:34.77ms
step:68/1845 train_time:2365ms step_avg:34.78ms
step:69/1845 train_time:2398ms step_avg:34.75ms
step:70/1845 train_time:2433ms step_avg:34.76ms
step:71/1845 train_time:2466ms step_avg:34.74ms
step:72/1845 train_time:2502ms step_avg:34.75ms
step:73/1845 train_time:2535ms step_avg:34.73ms
step:74/1845 train_time:2570ms step_avg:34.73ms
step:75/1845 train_time:2603ms step_avg:34.71ms
step:76/1845 train_time:2639ms step_avg:34.72ms
step:77/1845 train_time:2672ms step_avg:34.70ms
step:78/1845 train_time:2707ms step_avg:34.71ms
step:79/1845 train_time:2740ms step_avg:34.68ms
step:80/1845 train_time:2776ms step_avg:34.69ms
step:81/1845 train_time:2809ms step_avg:34.67ms
step:82/1845 train_time:2844ms step_avg:34.68ms
step:83/1845 train_time:2877ms step_avg:34.66ms
step:84/1845 train_time:2912ms step_avg:34.67ms
step:85/1845 train_time:2945ms step_avg:34.65ms
step:86/1845 train_time:2981ms step_avg:34.66ms
step:87/1845 train_time:3014ms step_avg:34.64ms
step:88/1845 train_time:3049ms step_avg:34.65ms
step:89/1845 train_time:3082ms step_avg:34.63ms
step:90/1845 train_time:3118ms step_avg:34.64ms
step:91/1845 train_time:3151ms step_avg:34.62ms
step:92/1845 train_time:3186ms step_avg:34.63ms
step:93/1845 train_time:3219ms step_avg:34.61ms
step:94/1845 train_time:3255ms step_avg:34.63ms
step:95/1845 train_time:3288ms step_avg:34.61ms
step:96/1845 train_time:3323ms step_avg:34.62ms
step:97/1845 train_time:3356ms step_avg:34.60ms
step:98/1845 train_time:3391ms step_avg:34.61ms
step:99/1845 train_time:3424ms step_avg:34.59ms
step:100/1845 train_time:3460ms step_avg:34.60ms
step:101/1845 train_time:3493ms step_avg:34.58ms
step:102/1845 train_time:3528ms step_avg:34.59ms
step:103/1845 train_time:3561ms step_avg:34.57ms
step:104/1845 train_time:3596ms step_avg:34.58ms
step:105/1845 train_time:3629ms step_avg:34.56ms
step:106/1845 train_time:3664ms step_avg:34.57ms
step:107/1845 train_time:3697ms step_avg:34.55ms
step:108/1845 train_time:3732ms step_avg:34.56ms
step:109/1845 train_time:3766ms step_avg:34.55ms
step:110/1845 train_time:3801ms step_avg:34.56ms
step:111/1845 train_time:3835ms step_avg:34.55ms
step:112/1845 train_time:3870ms step_avg:34.55ms
step:113/1845 train_time:3903ms step_avg:34.54ms
step:114/1845 train_time:3938ms step_avg:34.55ms
step:115/1845 train_time:3971ms step_avg:34.53ms
step:116/1845 train_time:4007ms step_avg:34.54ms
step:117/1845 train_time:4040ms step_avg:34.53ms
step:118/1845 train_time:4075ms step_avg:34.53ms
step:119/1845 train_time:4108ms step_avg:34.52ms
step:120/1845 train_time:4143ms step_avg:34.53ms
step:121/1845 train_time:4176ms step_avg:34.51ms
step:122/1845 train_time:4212ms step_avg:34.52ms
step:123/1845 train_time:4244ms step_avg:34.51ms
step:124/1845 train_time:4280ms step_avg:34.51ms
step:125/1845 train_time:4313ms step_avg:34.50ms
step:126/1845 train_time:4348ms step_avg:34.51ms
step:127/1845 train_time:4381ms step_avg:34.50ms
step:128/1845 train_time:4417ms step_avg:34.51ms
step:129/1845 train_time:4450ms step_avg:34.49ms
step:130/1845 train_time:4485ms step_avg:34.50ms
step:131/1845 train_time:4518ms step_avg:34.49ms
step:132/1845 train_time:4553ms step_avg:34.49ms
step:133/1845 train_time:4586ms step_avg:34.48ms
step:134/1845 train_time:4622ms step_avg:34.49ms
step:135/1845 train_time:4654ms step_avg:34.48ms
step:136/1845 train_time:4690ms step_avg:34.48ms
step:137/1845 train_time:4723ms step_avg:34.47ms
step:138/1845 train_time:4758ms step_avg:34.48ms
step:139/1845 train_time:4791ms step_avg:34.47ms
step:140/1845 train_time:4826ms step_avg:34.47ms
step:141/1845 train_time:4859ms step_avg:34.46ms
step:142/1845 train_time:4894ms step_avg:34.47ms
step:143/1845 train_time:4927ms step_avg:34.46ms
step:144/1845 train_time:4963ms step_avg:34.46ms
step:145/1845 train_time:4996ms step_avg:34.45ms
step:146/1845 train_time:5031ms step_avg:34.46ms
step:147/1845 train_time:5064ms step_avg:34.45ms
step:148/1845 train_time:5099ms step_avg:34.46ms
step:149/1845 train_time:5133ms step_avg:34.45ms
step:150/1845 train_time:5168ms step_avg:34.45ms
step:151/1845 train_time:5201ms step_avg:34.44ms
step:152/1845 train_time:5236ms step_avg:34.45ms
step:153/1845 train_time:5269ms step_avg:34.44ms
step:154/1845 train_time:5304ms step_avg:34.44ms
step:155/1845 train_time:5337ms step_avg:34.43ms
step:156/1845 train_time:5373ms step_avg:34.44ms
step:157/1845 train_time:5405ms step_avg:34.43ms
step:158/1845 train_time:5441ms step_avg:34.44ms
step:159/1845 train_time:5474ms step_avg:34.43ms
step:160/1845 train_time:5509ms step_avg:34.43ms
step:161/1845 train_time:5542ms step_avg:34.42ms
step:162/1845 train_time:5577ms step_avg:34.43ms
step:163/1845 train_time:5610ms step_avg:34.42ms
step:164/1845 train_time:5646ms step_avg:34.43ms
step:165/1845 train_time:5679ms step_avg:34.42ms
step:166/1845 train_time:5714ms step_avg:34.42ms
step:167/1845 train_time:5747ms step_avg:34.41ms
step:168/1845 train_time:5782ms step_avg:34.42ms
step:169/1845 train_time:5815ms step_avg:34.41ms
step:170/1845 train_time:5851ms step_avg:34.42ms
step:171/1845 train_time:5884ms step_avg:34.41ms
step:172/1845 train_time:5919ms step_avg:34.41ms
step:173/1845 train_time:5952ms step_avg:34.40ms
step:174/1845 train_time:5987ms step_avg:34.41ms
step:175/1845 train_time:6020ms step_avg:34.40ms
step:176/1845 train_time:6055ms step_avg:34.41ms
step:177/1845 train_time:6088ms step_avg:34.40ms
step:178/1845 train_time:6124ms step_avg:34.40ms
step:179/1845 train_time:6157ms step_avg:34.39ms
step:180/1845 train_time:6192ms step_avg:34.40ms
step:181/1845 train_time:6225ms step_avg:34.39ms
step:182/1845 train_time:6260ms step_avg:34.40ms
step:183/1845 train_time:6293ms step_avg:34.39ms
step:184/1845 train_time:6329ms step_avg:34.39ms
step:185/1845 train_time:6362ms step_avg:34.39ms
step:186/1845 train_time:6397ms step_avg:34.39ms
step:187/1845 train_time:6430ms step_avg:34.38ms
step:188/1845 train_time:6465ms step_avg:34.39ms
step:189/1845 train_time:6498ms step_avg:34.38ms
step:190/1845 train_time:6534ms step_avg:34.39ms
step:191/1845 train_time:6567ms step_avg:34.38ms
step:192/1845 train_time:6602ms step_avg:34.38ms
step:193/1845 train_time:6635ms step_avg:34.38ms
step:194/1845 train_time:6670ms step_avg:34.38ms
step:195/1845 train_time:6703ms step_avg:34.37ms
step:196/1845 train_time:6738ms step_avg:34.38ms
step:197/1845 train_time:6771ms step_avg:34.37ms
step:198/1845 train_time:6807ms step_avg:34.38ms
step:199/1845 train_time:6839ms step_avg:34.37ms
step:200/1845 train_time:6875ms step_avg:34.37ms
step:201/1845 train_time:6908ms step_avg:34.37ms
step:202/1845 train_time:6943ms step_avg:34.37ms
step:203/1845 train_time:6976ms step_avg:34.36ms
step:204/1845 train_time:7011ms step_avg:34.37ms
step:205/1845 train_time:7044ms step_avg:34.36ms
step:206/1845 train_time:7079ms step_avg:34.37ms
step:207/1845 train_time:7112ms step_avg:34.36ms
step:208/1845 train_time:7148ms step_avg:34.36ms
step:209/1845 train_time:7181ms step_avg:34.36ms
step:210/1845 train_time:7216ms step_avg:34.36ms
step:211/1845 train_time:7249ms step_avg:34.35ms
step:212/1845 train_time:7284ms step_avg:34.36ms
step:213/1845 train_time:7317ms step_avg:34.35ms
step:214/1845 train_time:7352ms step_avg:34.35ms
step:215/1845 train_time:7385ms step_avg:34.35ms
step:216/1845 train_time:7420ms step_avg:34.35ms
step:217/1845 train_time:7453ms step_avg:34.35ms
step:218/1845 train_time:7488ms step_avg:34.35ms
step:219/1845 train_time:7521ms step_avg:34.34ms
step:220/1845 train_time:7557ms step_avg:34.35ms
step:221/1845 train_time:7590ms step_avg:34.34ms
step:222/1845 train_time:7625ms step_avg:34.35ms
step:223/1845 train_time:7658ms step_avg:34.34ms
step:224/1845 train_time:7693ms step_avg:34.34ms
step:225/1845 train_time:7726ms step_avg:34.34ms
step:226/1845 train_time:7761ms step_avg:34.34ms
step:227/1845 train_time:7794ms step_avg:34.34ms
step:228/1845 train_time:7830ms step_avg:34.34ms
step:229/1845 train_time:7863ms step_avg:34.33ms
step:230/1845 train_time:7898ms step_avg:34.34ms
step:231/1845 train_time:7931ms step_avg:34.33ms
step:232/1845 train_time:7966ms step_avg:34.34ms
step:233/1845 train_time:7999ms step_avg:34.33ms
step:234/1845 train_time:8034ms step_avg:34.33ms
step:235/1845 train_time:8067ms step_avg:34.33ms
step:236/1845 train_time:8102ms step_avg:34.33ms
step:237/1845 train_time:8135ms step_avg:34.32ms
step:238/1845 train_time:8170ms step_avg:34.33ms
step:239/1845 train_time:8203ms step_avg:34.32ms
step:240/1845 train_time:8238ms step_avg:34.33ms
step:241/1845 train_time:8271ms step_avg:34.32ms
step:242/1845 train_time:8307ms step_avg:34.32ms
step:243/1845 train_time:8340ms step_avg:34.32ms
step:244/1845 train_time:8375ms step_avg:34.32ms
step:245/1845 train_time:8408ms step_avg:34.32ms
step:246/1845 train_time:8443ms step_avg:34.32ms
step:247/1845 train_time:8476ms step_avg:34.32ms
step:248/1845 train_time:8512ms step_avg:34.32ms
step:249/1845 train_time:8545ms step_avg:34.32ms
step:250/1845 train_time:8580ms step_avg:34.32ms
step:250/1845 val_loss:4.6108 train_time:8622ms step_avg:34.49ms
step:251/1845 train_time:8639ms step_avg:34.42ms
step:252/1845 train_time:8656ms step_avg:34.35ms
step:253/1845 train_time:8683ms step_avg:34.32ms
step:254/1845 train_time:8718ms step_avg:34.32ms
step:255/1845 train_time:8752ms step_avg:34.32ms
step:256/1845 train_time:8787ms step_avg:34.32ms
step:257/1845 train_time:8821ms step_avg:34.32ms
step:258/1845 train_time:8856ms step_avg:34.33ms
step:259/1845 train_time:8889ms step_avg:34.32ms
step:260/1845 train_time:8924ms step_avg:34.32ms
step:261/1845 train_time:8957ms step_avg:34.32ms
step:262/1845 train_time:8993ms step_avg:34.32ms
step:263/1845 train_time:9026ms step_avg:34.32ms
step:264/1845 train_time:9061ms step_avg:34.32ms
step:265/1845 train_time:9094ms step_avg:34.32ms
step:266/1845 train_time:9129ms step_avg:34.32ms
step:267/1845 train_time:9162ms step_avg:34.31ms
step:268/1845 train_time:9197ms step_avg:34.32ms
step:269/1845 train_time:9230ms step_avg:34.31ms
step:270/1845 train_time:9265ms step_avg:34.32ms
step:271/1845 train_time:9298ms step_avg:34.31ms
step:272/1845 train_time:9333ms step_avg:34.31ms
step:273/1845 train_time:9366ms step_avg:34.31ms
step:274/1845 train_time:9401ms step_avg:34.31ms
step:275/1845 train_time:9434ms step_avg:34.31ms
step:276/1845 train_time:9470ms step_avg:34.31ms
step:277/1845 train_time:9502ms step_avg:34.30ms
step:278/1845 train_time:9538ms step_avg:34.31ms
step:279/1845 train_time:9571ms step_avg:34.30ms
step:280/1845 train_time:9606ms step_avg:34.31ms
step:281/1845 train_time:9639ms step_avg:34.30ms
step:282/1845 train_time:9674ms step_avg:34.30ms
step:283/1845 train_time:9707ms step_avg:34.30ms
step:284/1845 train_time:9742ms step_avg:34.30ms
step:285/1845 train_time:9775ms step_avg:34.30ms
step:286/1845 train_time:9810ms step_avg:34.30ms
step:287/1845 train_time:9843ms step_avg:34.30ms
step:288/1845 train_time:9879ms step_avg:34.30ms
step:289/1845 train_time:9911ms step_avg:34.30ms
step:290/1845 train_time:9947ms step_avg:34.30ms
step:291/1845 train_time:9980ms step_avg:34.29ms
step:292/1845 train_time:10015ms step_avg:34.30ms
step:293/1845 train_time:10048ms step_avg:34.29ms
step:294/1845 train_time:10083ms step_avg:34.30ms
step:295/1845 train_time:10116ms step_avg:34.29ms
step:296/1845 train_time:10151ms step_avg:34.29ms
step:297/1845 train_time:10184ms step_avg:34.29ms
step:298/1845 train_time:10220ms step_avg:34.29ms
step:299/1845 train_time:10253ms step_avg:34.29ms
step:300/1845 train_time:10288ms step_avg:34.29ms
step:301/1845 train_time:10321ms step_avg:34.29ms
step:302/1845 train_time:10357ms step_avg:34.29ms
step:303/1845 train_time:10389ms step_avg:34.29ms
step:304/1845 train_time:10424ms step_avg:34.29ms
step:305/1845 train_time:10457ms step_avg:34.29ms
step:306/1845 train_time:10493ms step_avg:34.29ms
step:307/1845 train_time:10526ms step_avg:34.29ms
step:308/1845 train_time:10561ms step_avg:34.29ms
step:309/1845 train_time:10594ms step_avg:34.28ms
step:310/1845 train_time:10629ms step_avg:34.29ms
step:311/1845 train_time:10662ms step_avg:34.28ms
step:312/1845 train_time:10697ms step_avg:34.29ms
step:313/1845 train_time:10730ms step_avg:34.28ms
step:314/1845 train_time:10766ms step_avg:34.29ms
step:315/1845 train_time:10799ms step_avg:34.28ms
step:316/1845 train_time:10834ms step_avg:34.28ms
step:317/1845 train_time:10867ms step_avg:34.28ms
step:318/1845 train_time:10902ms step_avg:34.28ms
step:319/1845 train_time:10935ms step_avg:34.28ms
step:320/1845 train_time:10971ms step_avg:34.28ms
step:321/1845 train_time:11003ms step_avg:34.28ms
step:322/1845 train_time:11039ms step_avg:34.28ms
step:323/1845 train_time:11072ms step_avg:34.28ms
step:324/1845 train_time:11107ms step_avg:34.28ms
step:325/1845 train_time:11140ms step_avg:34.28ms
step:326/1845 train_time:11175ms step_avg:34.28ms
step:327/1845 train_time:11208ms step_avg:34.27ms
step:328/1845 train_time:11243ms step_avg:34.28ms
step:329/1845 train_time:11276ms step_avg:34.27ms
step:330/1845 train_time:11311ms step_avg:34.28ms
step:331/1845 train_time:11344ms step_avg:34.27ms
step:332/1845 train_time:11379ms step_avg:34.28ms
step:333/1845 train_time:11412ms step_avg:34.27ms
step:334/1845 train_time:11447ms step_avg:34.27ms
step:335/1845 train_time:11480ms step_avg:34.27ms
step:336/1845 train_time:11516ms step_avg:34.27ms
step:337/1845 train_time:11549ms step_avg:34.27ms
step:338/1845 train_time:11584ms step_avg:34.27ms
step:339/1845 train_time:11617ms step_avg:34.27ms
step:340/1845 train_time:11652ms step_avg:34.27ms
step:341/1845 train_time:11685ms step_avg:34.27ms
step:342/1845 train_time:11721ms step_avg:34.27ms
step:343/1845 train_time:11754ms step_avg:34.27ms
step:344/1845 train_time:11789ms step_avg:34.27ms
step:345/1845 train_time:11822ms step_avg:34.27ms
step:346/1845 train_time:11857ms step_avg:34.27ms
step:347/1845 train_time:11890ms step_avg:34.27ms
step:348/1845 train_time:11926ms step_avg:34.27ms
step:349/1845 train_time:11959ms step_avg:34.27ms
step:350/1845 train_time:11994ms step_avg:34.27ms
step:351/1845 train_time:12027ms step_avg:34.26ms
step:352/1845 train_time:12062ms step_avg:34.27ms
step:353/1845 train_time:12095ms step_avg:34.26ms
step:354/1845 train_time:12130ms step_avg:34.27ms
step:355/1845 train_time:12163ms step_avg:34.26ms
step:356/1845 train_time:12198ms step_avg:34.26ms
step:357/1845 train_time:12231ms step_avg:34.26ms
step:358/1845 train_time:12266ms step_avg:34.26ms
step:359/1845 train_time:12299ms step_avg:34.26ms
step:360/1845 train_time:12334ms step_avg:34.26ms
step:361/1845 train_time:12367ms step_avg:34.26ms
step:362/1845 train_time:12402ms step_avg:34.26ms
step:363/1845 train_time:12435ms step_avg:34.26ms
step:364/1845 train_time:12471ms step_avg:34.26ms
step:365/1845 train_time:12503ms step_avg:34.26ms
step:366/1845 train_time:12539ms step_avg:34.26ms
step:367/1845 train_time:12572ms step_avg:34.26ms
step:368/1845 train_time:12607ms step_avg:34.26ms
step:369/1845 train_time:12640ms step_avg:34.25ms
step:370/1845 train_time:12675ms step_avg:34.26ms
step:371/1845 train_time:12708ms step_avg:34.25ms
step:372/1845 train_time:12743ms step_avg:34.26ms
step:373/1845 train_time:12776ms step_avg:34.25ms
step:374/1845 train_time:12812ms step_avg:34.26ms
step:375/1845 train_time:12844ms step_avg:34.25ms
step:376/1845 train_time:12880ms step_avg:34.25ms
step:377/1845 train_time:12913ms step_avg:34.25ms
step:378/1845 train_time:12948ms step_avg:34.25ms
step:379/1845 train_time:12981ms step_avg:34.25ms
step:380/1845 train_time:13016ms step_avg:34.25ms
step:381/1845 train_time:13049ms step_avg:34.25ms
step:382/1845 train_time:13084ms step_avg:34.25ms
step:383/1845 train_time:13117ms step_avg:34.25ms
step:384/1845 train_time:13152ms step_avg:34.25ms
step:385/1845 train_time:13185ms step_avg:34.25ms
step:386/1845 train_time:13220ms step_avg:34.25ms
step:387/1845 train_time:13253ms step_avg:34.25ms
step:388/1845 train_time:13288ms step_avg:34.25ms
step:389/1845 train_time:13321ms step_avg:34.24ms
step:390/1845 train_time:13356ms step_avg:34.25ms
step:391/1845 train_time:13389ms step_avg:34.24ms
step:392/1845 train_time:13425ms step_avg:34.25ms
step:393/1845 train_time:13457ms step_avg:34.24ms
step:394/1845 train_time:13493ms step_avg:34.25ms
step:395/1845 train_time:13526ms step_avg:34.24ms
step:396/1845 train_time:13561ms step_avg:34.24ms
step:397/1845 train_time:13594ms step_avg:34.24ms
step:398/1845 train_time:13629ms step_avg:34.24ms
step:399/1845 train_time:13662ms step_avg:34.24ms
step:400/1845 train_time:13697ms step_avg:34.24ms
step:401/1845 train_time:13730ms step_avg:34.24ms
step:402/1845 train_time:13765ms step_avg:34.24ms
step:403/1845 train_time:13798ms step_avg:34.24ms
step:404/1845 train_time:13834ms step_avg:34.24ms
step:405/1845 train_time:13866ms step_avg:34.24ms
step:406/1845 train_time:13902ms step_avg:34.24ms
step:407/1845 train_time:13935ms step_avg:34.24ms
step:408/1845 train_time:13970ms step_avg:34.24ms
step:409/1845 train_time:14003ms step_avg:34.24ms
step:410/1845 train_time:14038ms step_avg:34.24ms
step:411/1845 train_time:14071ms step_avg:34.24ms
step:412/1845 train_time:14106ms step_avg:34.24ms
step:413/1845 train_time:14139ms step_avg:34.24ms
step:414/1845 train_time:14174ms step_avg:34.24ms
step:415/1845 train_time:14207ms step_avg:34.23ms
step:416/1845 train_time:14242ms step_avg:34.24ms
step:417/1845 train_time:14276ms step_avg:34.23ms
step:418/1845 train_time:14311ms step_avg:34.24ms
step:419/1845 train_time:14344ms step_avg:34.23ms
step:420/1845 train_time:14379ms step_avg:34.24ms
step:421/1845 train_time:14412ms step_avg:34.23ms
step:422/1845 train_time:14447ms step_avg:34.24ms
step:423/1845 train_time:14480ms step_avg:34.23ms
step:424/1845 train_time:14515ms step_avg:34.23ms
step:425/1845 train_time:14548ms step_avg:34.23ms
step:426/1845 train_time:14583ms step_avg:34.23ms
step:427/1845 train_time:14617ms step_avg:34.23ms
step:428/1845 train_time:14652ms step_avg:34.23ms
step:429/1845 train_time:14685ms step_avg:34.23ms
step:430/1845 train_time:14720ms step_avg:34.23ms
step:431/1845 train_time:14753ms step_avg:34.23ms
step:432/1845 train_time:14788ms step_avg:34.23ms
step:433/1845 train_time:14821ms step_avg:34.23ms
step:434/1845 train_time:14856ms step_avg:34.23ms
step:435/1845 train_time:14889ms step_avg:34.23ms
step:436/1845 train_time:14924ms step_avg:34.23ms
step:437/1845 train_time:14957ms step_avg:34.23ms
step:438/1845 train_time:14992ms step_avg:34.23ms
step:439/1845 train_time:15025ms step_avg:34.23ms
step:440/1845 train_time:15060ms step_avg:34.23ms
step:441/1845 train_time:15093ms step_avg:34.22ms
step:442/1845 train_time:15128ms step_avg:34.23ms
step:443/1845 train_time:15161ms step_avg:34.22ms
step:444/1845 train_time:15197ms step_avg:34.23ms
step:445/1845 train_time:15229ms step_avg:34.22ms
step:446/1845 train_time:15265ms step_avg:34.23ms
step:447/1845 train_time:15298ms step_avg:34.22ms
step:448/1845 train_time:15333ms step_avg:34.22ms
step:449/1845 train_time:15366ms step_avg:34.22ms
step:450/1845 train_time:15401ms step_avg:34.22ms
step:451/1845 train_time:15434ms step_avg:34.22ms
step:452/1845 train_time:15470ms step_avg:34.22ms
step:453/1845 train_time:15502ms step_avg:34.22ms
step:454/1845 train_time:15538ms step_avg:34.22ms
step:455/1845 train_time:15571ms step_avg:34.22ms
step:456/1845 train_time:15606ms step_avg:34.22ms
step:457/1845 train_time:15639ms step_avg:34.22ms
step:458/1845 train_time:15674ms step_avg:34.22ms
step:459/1845 train_time:15707ms step_avg:34.22ms
step:460/1845 train_time:15742ms step_avg:34.22ms
step:461/1845 train_time:15775ms step_avg:34.22ms
step:462/1845 train_time:15810ms step_avg:34.22ms
step:463/1845 train_time:15843ms step_avg:34.22ms
step:464/1845 train_time:15878ms step_avg:34.22ms
step:465/1845 train_time:15911ms step_avg:34.22ms
step:466/1845 train_time:15946ms step_avg:34.22ms
step:467/1845 train_time:15980ms step_avg:34.22ms
step:468/1845 train_time:16015ms step_avg:34.22ms
step:469/1845 train_time:16048ms step_avg:34.22ms
step:470/1845 train_time:16083ms step_avg:34.22ms
step:471/1845 train_time:16116ms step_avg:34.22ms
step:472/1845 train_time:16151ms step_avg:34.22ms
step:473/1845 train_time:16184ms step_avg:34.22ms
step:474/1845 train_time:16220ms step_avg:34.22ms
step:475/1845 train_time:16253ms step_avg:34.22ms
step:476/1845 train_time:16288ms step_avg:34.22ms
step:477/1845 train_time:16321ms step_avg:34.22ms
step:478/1845 train_time:16356ms step_avg:34.22ms
step:479/1845 train_time:16389ms step_avg:34.22ms
step:480/1845 train_time:16424ms step_avg:34.22ms
step:481/1845 train_time:16457ms step_avg:34.21ms
step:482/1845 train_time:16493ms step_avg:34.22ms
step:483/1845 train_time:16526ms step_avg:34.22ms
step:484/1845 train_time:16561ms step_avg:34.22ms
step:485/1845 train_time:16594ms step_avg:34.21ms
step:486/1845 train_time:16629ms step_avg:34.22ms
step:487/1845 train_time:16662ms step_avg:34.21ms
step:488/1845 train_time:16698ms step_avg:34.22ms
step:489/1845 train_time:16731ms step_avg:34.21ms
step:490/1845 train_time:16766ms step_avg:34.22ms
step:491/1845 train_time:16799ms step_avg:34.21ms
step:492/1845 train_time:16834ms step_avg:34.22ms
step:493/1845 train_time:16867ms step_avg:34.21ms
step:494/1845 train_time:16902ms step_avg:34.22ms
step:495/1845 train_time:16935ms step_avg:34.21ms
step:496/1845 train_time:16970ms step_avg:34.21ms
step:497/1845 train_time:17003ms step_avg:34.21ms
step:498/1845 train_time:17038ms step_avg:34.21ms
step:499/1845 train_time:17071ms step_avg:34.21ms
step:500/1845 train_time:17107ms step_avg:34.21ms
step:500/1845 val_loss:4.2848 train_time:17148ms step_avg:34.30ms
step:501/1845 train_time:17167ms step_avg:34.26ms
step:502/1845 train_time:17184ms step_avg:34.23ms
step:503/1845 train_time:17211ms step_avg:34.22ms
step:504/1845 train_time:17247ms step_avg:34.22ms
step:505/1845 train_time:17280ms step_avg:34.22ms
step:506/1845 train_time:17316ms step_avg:34.22ms
step:507/1845 train_time:17350ms step_avg:34.22ms
step:508/1845 train_time:17385ms step_avg:34.22ms
step:509/1845 train_time:17418ms step_avg:34.22ms
step:510/1845 train_time:17454ms step_avg:34.22ms
step:511/1845 train_time:17487ms step_avg:34.22ms
step:512/1845 train_time:17522ms step_avg:34.22ms
step:513/1845 train_time:17555ms step_avg:34.22ms
step:514/1845 train_time:17590ms step_avg:34.22ms
step:515/1845 train_time:17623ms step_avg:34.22ms
step:516/1845 train_time:17658ms step_avg:34.22ms
step:517/1845 train_time:17691ms step_avg:34.22ms
step:518/1845 train_time:17726ms step_avg:34.22ms
step:519/1845 train_time:17759ms step_avg:34.22ms
step:520/1845 train_time:17794ms step_avg:34.22ms
step:521/1845 train_time:17827ms step_avg:34.22ms
step:522/1845 train_time:17862ms step_avg:34.22ms
step:523/1845 train_time:17895ms step_avg:34.22ms
step:524/1845 train_time:17930ms step_avg:34.22ms
step:525/1845 train_time:17963ms step_avg:34.22ms
step:526/1845 train_time:17998ms step_avg:34.22ms
step:527/1845 train_time:18031ms step_avg:34.21ms
step:528/1845 train_time:18067ms step_avg:34.22ms
step:529/1845 train_time:18099ms step_avg:34.21ms
step:530/1845 train_time:18135ms step_avg:34.22ms
step:531/1845 train_time:18168ms step_avg:34.21ms
step:532/1845 train_time:18203ms step_avg:34.22ms
step:533/1845 train_time:18236ms step_avg:34.21ms
step:534/1845 train_time:18271ms step_avg:34.22ms
step:535/1845 train_time:18304ms step_avg:34.21ms
step:536/1845 train_time:18339ms step_avg:34.22ms
step:537/1845 train_time:18372ms step_avg:34.21ms
step:538/1845 train_time:18408ms step_avg:34.22ms
step:539/1845 train_time:18441ms step_avg:34.21ms
step:540/1845 train_time:18476ms step_avg:34.22ms
step:541/1845 train_time:18509ms step_avg:34.21ms
step:542/1845 train_time:18545ms step_avg:34.22ms
step:543/1845 train_time:18578ms step_avg:34.21ms
step:544/1845 train_time:18613ms step_avg:34.22ms
step:545/1845 train_time:18646ms step_avg:34.21ms
step:546/1845 train_time:18681ms step_avg:34.22ms
step:547/1845 train_time:18714ms step_avg:34.21ms
step:548/1845 train_time:18749ms step_avg:34.21ms
step:549/1845 train_time:18782ms step_avg:34.21ms
step:550/1845 train_time:18817ms step_avg:34.21ms
step:551/1845 train_time:18850ms step_avg:34.21ms
step:552/1845 train_time:18885ms step_avg:34.21ms
step:553/1845 train_time:18918ms step_avg:34.21ms
step:554/1845 train_time:18953ms step_avg:34.21ms
step:555/1845 train_time:18986ms step_avg:34.21ms
step:556/1845 train_time:19022ms step_avg:34.21ms
step:557/1845 train_time:19055ms step_avg:34.21ms
step:558/1845 train_time:19090ms step_avg:34.21ms
step:559/1845 train_time:19123ms step_avg:34.21ms
step:560/1845 train_time:19158ms step_avg:34.21ms
step:561/1845 train_time:19191ms step_avg:34.21ms
step:562/1845 train_time:19226ms step_avg:34.21ms
step:563/1845 train_time:19259ms step_avg:34.21ms
step:564/1845 train_time:19294ms step_avg:34.21ms
step:565/1845 train_time:19327ms step_avg:34.21ms
step:566/1845 train_time:19362ms step_avg:34.21ms
step:567/1845 train_time:19395ms step_avg:34.21ms
step:568/1845 train_time:19430ms step_avg:34.21ms
step:569/1845 train_time:19464ms step_avg:34.21ms
step:570/1845 train_time:19499ms step_avg:34.21ms
step:571/1845 train_time:19532ms step_avg:34.21ms
step:572/1845 train_time:19567ms step_avg:34.21ms
step:573/1845 train_time:19600ms step_avg:34.21ms
step:574/1845 train_time:19636ms step_avg:34.21ms
step:575/1845 train_time:19668ms step_avg:34.21ms
step:576/1845 train_time:19703ms step_avg:34.21ms
step:577/1845 train_time:19736ms step_avg:34.21ms
step:578/1845 train_time:19772ms step_avg:34.21ms
step:579/1845 train_time:19805ms step_avg:34.21ms
step:580/1845 train_time:19840ms step_avg:34.21ms
step:581/1845 train_time:19873ms step_avg:34.20ms
step:582/1845 train_time:19908ms step_avg:34.21ms
step:583/1845 train_time:19941ms step_avg:34.20ms
step:584/1845 train_time:19976ms step_avg:34.21ms
step:585/1845 train_time:20009ms step_avg:34.20ms
step:586/1845 train_time:20045ms step_avg:34.21ms
step:587/1845 train_time:20078ms step_avg:34.20ms
step:588/1845 train_time:20113ms step_avg:34.21ms
step:589/1845 train_time:20146ms step_avg:34.20ms
step:590/1845 train_time:20181ms step_avg:34.21ms
step:591/1845 train_time:20214ms step_avg:34.20ms
step:592/1845 train_time:20250ms step_avg:34.21ms
step:593/1845 train_time:20283ms step_avg:34.20ms
step:594/1845 train_time:20318ms step_avg:34.21ms
step:595/1845 train_time:20351ms step_avg:34.20ms
step:596/1845 train_time:20386ms step_avg:34.20ms
step:597/1845 train_time:20419ms step_avg:34.20ms
step:598/1845 train_time:20454ms step_avg:34.20ms
step:599/1845 train_time:20487ms step_avg:34.20ms
step:600/1845 train_time:20523ms step_avg:34.20ms
step:601/1845 train_time:20556ms step_avg:34.20ms
step:602/1845 train_time:20591ms step_avg:34.20ms
step:603/1845 train_time:20625ms step_avg:34.20ms
step:604/1845 train_time:20685ms step_avg:34.25ms
step:605/1845 train_time:20745ms step_avg:34.29ms
step:606/1845 train_time:20808ms step_avg:34.34ms
step:607/1845 train_time:20868ms step_avg:34.38ms
step:608/1845 train_time:20931ms step_avg:34.43ms
step:609/1845 train_time:20992ms step_avg:34.47ms
step:610/1845 train_time:21055ms step_avg:34.52ms
step:611/1845 train_time:21115ms step_avg:34.56ms
step:612/1845 train_time:21178ms step_avg:34.60ms
step:613/1845 train_time:21238ms step_avg:34.65ms
step:614/1845 train_time:21301ms step_avg:34.69ms
step:615/1845 train_time:21360ms step_avg:34.73ms
step:616/1845 train_time:21424ms step_avg:34.78ms
step:617/1845 train_time:21484ms step_avg:34.82ms
step:618/1845 train_time:21548ms step_avg:34.87ms
step:619/1845 train_time:21608ms step_avg:34.91ms
step:620/1845 train_time:21671ms step_avg:34.95ms
step:621/1845 train_time:21731ms step_avg:34.99ms
step:622/1845 train_time:21793ms step_avg:35.04ms
step:623/1845 train_time:21854ms step_avg:35.08ms
step:624/1845 train_time:21916ms step_avg:35.12ms
step:625/1845 train_time:21977ms step_avg:35.16ms
step:626/1845 train_time:22039ms step_avg:35.21ms
step:627/1845 train_time:22100ms step_avg:35.25ms
step:628/1845 train_time:22162ms step_avg:35.29ms
step:629/1845 train_time:22223ms step_avg:35.33ms
step:630/1845 train_time:22286ms step_avg:35.37ms
step:631/1845 train_time:22346ms step_avg:35.41ms
step:632/1845 train_time:22409ms step_avg:35.46ms
step:633/1845 train_time:22470ms step_avg:35.50ms
step:634/1845 train_time:22533ms step_avg:35.54ms
step:635/1845 train_time:22593ms step_avg:35.58ms
step:636/1845 train_time:22656ms step_avg:35.62ms
step:637/1845 train_time:22716ms step_avg:35.66ms
step:638/1845 train_time:22779ms step_avg:35.70ms
step:639/1845 train_time:22839ms step_avg:35.74ms
step:640/1845 train_time:22902ms step_avg:35.78ms
step:641/1845 train_time:22962ms step_avg:35.82ms
step:642/1845 train_time:23026ms step_avg:35.87ms
step:643/1845 train_time:23086ms step_avg:35.90ms
step:644/1845 train_time:23149ms step_avg:35.95ms
step:645/1845 train_time:23210ms step_avg:35.98ms
step:646/1845 train_time:23273ms step_avg:36.03ms
step:647/1845 train_time:23332ms step_avg:36.06ms
step:648/1845 train_time:23395ms step_avg:36.10ms
step:649/1845 train_time:23455ms step_avg:36.14ms
step:650/1845 train_time:23518ms step_avg:36.18ms
step:651/1845 train_time:23579ms step_avg:36.22ms
step:652/1845 train_time:23642ms step_avg:36.26ms
step:653/1845 train_time:23702ms step_avg:36.30ms
step:654/1845 train_time:23765ms step_avg:36.34ms
step:655/1845 train_time:23825ms step_avg:36.37ms
step:656/1845 train_time:23888ms step_avg:36.41ms
step:657/1845 train_time:23949ms step_avg:36.45ms
step:658/1845 train_time:24013ms step_avg:36.49ms
step:659/1845 train_time:24073ms step_avg:36.53ms
step:660/1845 train_time:24136ms step_avg:36.57ms
step:661/1845 train_time:24197ms step_avg:36.61ms
step:662/1845 train_time:24260ms step_avg:36.65ms
step:663/1845 train_time:24320ms step_avg:36.68ms
step:664/1845 train_time:24384ms step_avg:36.72ms
step:665/1845 train_time:24444ms step_avg:36.76ms
step:666/1845 train_time:24507ms step_avg:36.80ms
step:667/1845 train_time:24567ms step_avg:36.83ms
step:668/1845 train_time:24630ms step_avg:36.87ms
step:669/1845 train_time:24690ms step_avg:36.91ms
step:670/1845 train_time:24753ms step_avg:36.94ms
step:671/1845 train_time:24813ms step_avg:36.98ms
step:672/1845 train_time:24875ms step_avg:37.02ms
step:673/1845 train_time:24935ms step_avg:37.05ms
step:674/1845 train_time:24998ms step_avg:37.09ms
step:675/1845 train_time:25059ms step_avg:37.12ms
step:676/1845 train_time:25123ms step_avg:37.16ms
step:677/1845 train_time:25183ms step_avg:37.20ms
step:678/1845 train_time:25246ms step_avg:37.24ms
step:679/1845 train_time:25306ms step_avg:37.27ms
step:680/1845 train_time:25369ms step_avg:37.31ms
step:681/1845 train_time:25430ms step_avg:37.34ms
step:682/1845 train_time:25493ms step_avg:37.38ms
step:683/1845 train_time:25553ms step_avg:37.41ms
step:684/1845 train_time:25616ms step_avg:37.45ms
step:685/1845 train_time:25677ms step_avg:37.48ms
step:686/1845 train_time:25740ms step_avg:37.52ms
step:687/1845 train_time:25801ms step_avg:37.56ms
step:688/1845 train_time:25863ms step_avg:37.59ms
step:689/1845 train_time:25923ms step_avg:37.62ms
step:690/1845 train_time:25987ms step_avg:37.66ms
step:691/1845 train_time:26047ms step_avg:37.69ms
step:692/1845 train_time:26109ms step_avg:37.73ms
step:693/1845 train_time:26170ms step_avg:37.76ms
step:694/1845 train_time:26233ms step_avg:37.80ms
step:695/1845 train_time:26293ms step_avg:37.83ms
step:696/1845 train_time:26356ms step_avg:37.87ms
step:697/1845 train_time:26417ms step_avg:37.90ms
step:698/1845 train_time:26479ms step_avg:37.94ms
step:699/1845 train_time:26539ms step_avg:37.97ms
step:700/1845 train_time:26602ms step_avg:38.00ms
step:701/1845 train_time:26661ms step_avg:38.03ms
step:702/1845 train_time:26725ms step_avg:38.07ms
step:703/1845 train_time:26785ms step_avg:38.10ms
step:704/1845 train_time:26848ms step_avg:38.14ms
step:705/1845 train_time:26908ms step_avg:38.17ms
step:706/1845 train_time:26971ms step_avg:38.20ms
step:707/1845 train_time:27031ms step_avg:38.23ms
step:708/1845 train_time:27094ms step_avg:38.27ms
step:709/1845 train_time:27154ms step_avg:38.30ms
step:710/1845 train_time:27217ms step_avg:38.33ms
step:711/1845 train_time:27277ms step_avg:38.36ms
step:712/1845 train_time:27340ms step_avg:38.40ms
step:713/1845 train_time:27402ms step_avg:38.43ms
step:714/1845 train_time:27465ms step_avg:38.47ms
step:715/1845 train_time:27524ms step_avg:38.50ms
step:716/1845 train_time:27587ms step_avg:38.53ms
step:717/1845 train_time:27647ms step_avg:38.56ms
step:718/1845 train_time:27710ms step_avg:38.59ms
step:719/1845 train_time:27771ms step_avg:38.62ms
step:720/1845 train_time:27833ms step_avg:38.66ms
step:721/1845 train_time:27893ms step_avg:38.69ms
step:722/1845 train_time:27956ms step_avg:38.72ms
step:723/1845 train_time:28017ms step_avg:38.75ms
step:724/1845 train_time:28079ms step_avg:38.78ms
step:725/1845 train_time:28139ms step_avg:38.81ms
step:726/1845 train_time:28202ms step_avg:38.85ms
step:727/1845 train_time:28262ms step_avg:38.87ms
step:728/1845 train_time:28325ms step_avg:38.91ms
step:729/1845 train_time:28385ms step_avg:38.94ms
step:730/1845 train_time:28448ms step_avg:38.97ms
step:731/1845 train_time:28508ms step_avg:39.00ms
step:732/1845 train_time:28571ms step_avg:39.03ms
step:733/1845 train_time:28632ms step_avg:39.06ms
step:734/1845 train_time:28695ms step_avg:39.09ms
step:735/1845 train_time:28755ms step_avg:39.12ms
step:736/1845 train_time:28818ms step_avg:39.15ms
step:737/1845 train_time:28878ms step_avg:39.18ms
step:738/1845 train_time:28942ms step_avg:39.22ms
step:739/1845 train_time:29002ms step_avg:39.24ms
step:740/1845 train_time:29065ms step_avg:39.28ms
step:741/1845 train_time:29125ms step_avg:39.30ms
step:742/1845 train_time:29188ms step_avg:39.34ms
step:743/1845 train_time:29247ms step_avg:39.36ms
step:744/1845 train_time:29310ms step_avg:39.40ms
step:745/1845 train_time:29370ms step_avg:39.42ms
step:746/1845 train_time:29433ms step_avg:39.45ms
step:747/1845 train_time:29493ms step_avg:39.48ms
step:748/1845 train_time:29556ms step_avg:39.51ms
step:749/1845 train_time:29617ms step_avg:39.54ms
step:750/1845 train_time:29680ms step_avg:39.57ms
step:750/1845 val_loss:4.0207 train_time:29750ms step_avg:39.67ms
step:751/1845 train_time:29769ms step_avg:39.64ms
step:752/1845 train_time:29805ms step_avg:39.63ms
step:753/1845 train_time:29866ms step_avg:39.66ms
step:754/1845 train_time:29930ms step_avg:39.69ms
step:755/1845 train_time:29992ms step_avg:39.72ms
step:756/1845 train_time:30055ms step_avg:39.76ms
step:757/1845 train_time:30114ms step_avg:39.78ms
step:758/1845 train_time:30178ms step_avg:39.81ms
step:759/1845 train_time:30238ms step_avg:39.84ms
step:760/1845 train_time:30300ms step_avg:39.87ms
step:761/1845 train_time:30361ms step_avg:39.90ms
step:762/1845 train_time:30423ms step_avg:39.93ms
step:763/1845 train_time:30483ms step_avg:39.95ms
step:764/1845 train_time:30545ms step_avg:39.98ms
step:765/1845 train_time:30604ms step_avg:40.01ms
step:766/1845 train_time:30667ms step_avg:40.04ms
step:767/1845 train_time:30728ms step_avg:40.06ms
step:768/1845 train_time:30792ms step_avg:40.09ms
step:769/1845 train_time:30853ms step_avg:40.12ms
step:770/1845 train_time:30916ms step_avg:40.15ms
step:771/1845 train_time:30977ms step_avg:40.18ms
step:772/1845 train_time:31040ms step_avg:40.21ms
step:773/1845 train_time:31101ms step_avg:40.23ms
step:774/1845 train_time:31163ms step_avg:40.26ms
step:775/1845 train_time:31223ms step_avg:40.29ms
step:776/1845 train_time:31286ms step_avg:40.32ms
step:777/1845 train_time:31347ms step_avg:40.34ms
step:778/1845 train_time:31409ms step_avg:40.37ms
step:779/1845 train_time:31469ms step_avg:40.40ms
step:780/1845 train_time:31531ms step_avg:40.42ms
step:781/1845 train_time:31591ms step_avg:40.45ms
step:782/1845 train_time:31654ms step_avg:40.48ms
step:783/1845 train_time:31715ms step_avg:40.50ms
step:784/1845 train_time:31778ms step_avg:40.53ms
step:785/1845 train_time:31839ms step_avg:40.56ms
step:786/1845 train_time:31902ms step_avg:40.59ms
step:787/1845 train_time:31962ms step_avg:40.61ms
step:788/1845 train_time:32025ms step_avg:40.64ms
step:789/1845 train_time:32086ms step_avg:40.67ms
step:790/1845 train_time:32149ms step_avg:40.70ms
step:791/1845 train_time:32209ms step_avg:40.72ms
step:792/1845 train_time:32272ms step_avg:40.75ms
step:793/1845 train_time:32332ms step_avg:40.77ms
step:794/1845 train_time:32395ms step_avg:40.80ms
step:795/1845 train_time:32454ms step_avg:40.82ms
step:796/1845 train_time:32518ms step_avg:40.85ms
step:797/1845 train_time:32577ms step_avg:40.87ms
step:798/1845 train_time:32639ms step_avg:40.90ms
step:799/1845 train_time:32700ms step_avg:40.93ms
step:800/1845 train_time:32762ms step_avg:40.95ms
step:801/1845 train_time:32823ms step_avg:40.98ms
step:802/1845 train_time:32885ms step_avg:41.00ms
step:803/1845 train_time:32945ms step_avg:41.03ms
step:804/1845 train_time:33008ms step_avg:41.05ms
step:805/1845 train_time:33069ms step_avg:41.08ms
step:806/1845 train_time:33132ms step_avg:41.11ms
step:807/1845 train_time:33192ms step_avg:41.13ms
step:808/1845 train_time:33255ms step_avg:41.16ms
step:809/1845 train_time:33315ms step_avg:41.18ms
step:810/1845 train_time:33377ms step_avg:41.21ms
step:811/1845 train_time:33437ms step_avg:41.23ms
step:812/1845 train_time:33500ms step_avg:41.26ms
step:813/1845 train_time:33560ms step_avg:41.28ms
step:814/1845 train_time:33623ms step_avg:41.31ms
step:815/1845 train_time:33683ms step_avg:41.33ms
step:816/1845 train_time:33746ms step_avg:41.36ms
step:817/1845 train_time:33806ms step_avg:41.38ms
step:818/1845 train_time:33869ms step_avg:41.40ms
step:819/1845 train_time:33930ms step_avg:41.43ms
step:820/1845 train_time:33993ms step_avg:41.45ms
step:821/1845 train_time:34053ms step_avg:41.48ms
step:822/1845 train_time:34116ms step_avg:41.50ms
step:823/1845 train_time:34176ms step_avg:41.53ms
step:824/1845 train_time:34239ms step_avg:41.55ms
step:825/1845 train_time:34299ms step_avg:41.57ms
step:826/1845 train_time:34362ms step_avg:41.60ms
step:827/1845 train_time:34422ms step_avg:41.62ms
step:828/1845 train_time:34485ms step_avg:41.65ms
step:829/1845 train_time:34546ms step_avg:41.67ms
step:830/1845 train_time:34608ms step_avg:41.70ms
step:831/1845 train_time:34669ms step_avg:41.72ms
step:832/1845 train_time:34731ms step_avg:41.74ms
step:833/1845 train_time:34791ms step_avg:41.77ms
step:834/1845 train_time:34855ms step_avg:41.79ms
step:835/1845 train_time:34916ms step_avg:41.82ms
step:836/1845 train_time:34979ms step_avg:41.84ms
step:837/1845 train_time:35038ms step_avg:41.86ms
step:838/1845 train_time:35101ms step_avg:41.89ms
step:839/1845 train_time:35162ms step_avg:41.91ms
step:840/1845 train_time:35224ms step_avg:41.93ms
step:841/1845 train_time:35284ms step_avg:41.96ms
step:842/1845 train_time:35348ms step_avg:41.98ms
step:843/1845 train_time:35408ms step_avg:42.00ms
step:844/1845 train_time:35471ms step_avg:42.03ms
step:845/1845 train_time:35531ms step_avg:42.05ms
step:846/1845 train_time:35594ms step_avg:42.07ms
step:847/1845 train_time:35655ms step_avg:42.10ms
step:848/1845 train_time:35718ms step_avg:42.12ms
step:849/1845 train_time:35778ms step_avg:42.14ms
step:850/1845 train_time:35841ms step_avg:42.17ms
step:851/1845 train_time:35902ms step_avg:42.19ms
step:852/1845 train_time:35964ms step_avg:42.21ms
step:853/1845 train_time:36024ms step_avg:42.23ms
step:854/1845 train_time:36087ms step_avg:42.26ms
step:855/1845 train_time:36147ms step_avg:42.28ms
step:856/1845 train_time:36210ms step_avg:42.30ms
step:857/1845 train_time:36271ms step_avg:42.32ms
step:858/1845 train_time:36334ms step_avg:42.35ms
step:859/1845 train_time:36394ms step_avg:42.37ms
step:860/1845 train_time:36456ms step_avg:42.39ms
step:861/1845 train_time:36516ms step_avg:42.41ms
step:862/1845 train_time:36579ms step_avg:42.44ms
step:863/1845 train_time:36640ms step_avg:42.46ms
step:864/1845 train_time:36702ms step_avg:42.48ms
step:865/1845 train_time:36762ms step_avg:42.50ms
step:866/1845 train_time:36825ms step_avg:42.52ms
step:867/1845 train_time:36885ms step_avg:42.54ms
step:868/1845 train_time:36948ms step_avg:42.57ms
step:869/1845 train_time:37008ms step_avg:42.59ms
step:870/1845 train_time:37072ms step_avg:42.61ms
step:871/1845 train_time:37132ms step_avg:42.63ms
step:872/1845 train_time:37195ms step_avg:42.65ms
step:873/1845 train_time:37255ms step_avg:42.67ms
step:874/1845 train_time:37318ms step_avg:42.70ms
step:875/1845 train_time:37379ms step_avg:42.72ms
step:876/1845 train_time:37442ms step_avg:42.74ms
step:877/1845 train_time:37501ms step_avg:42.76ms
step:878/1845 train_time:37564ms step_avg:42.78ms
step:879/1845 train_time:37624ms step_avg:42.80ms
step:880/1845 train_time:37687ms step_avg:42.83ms
step:881/1845 train_time:37747ms step_avg:42.85ms
step:882/1845 train_time:37810ms step_avg:42.87ms
step:883/1845 train_time:37870ms step_avg:42.89ms
step:884/1845 train_time:37933ms step_avg:42.91ms
step:885/1845 train_time:37993ms step_avg:42.93ms
step:886/1845 train_time:38056ms step_avg:42.95ms
step:887/1845 train_time:38117ms step_avg:42.97ms
step:888/1845 train_time:38179ms step_avg:42.99ms
step:889/1845 train_time:38240ms step_avg:43.02ms
step:890/1845 train_time:38303ms step_avg:43.04ms
step:891/1845 train_time:38362ms step_avg:43.06ms
step:892/1845 train_time:38425ms step_avg:43.08ms
step:893/1845 train_time:38486ms step_avg:43.10ms
step:894/1845 train_time:38549ms step_avg:43.12ms
step:895/1845 train_time:38609ms step_avg:43.14ms
step:896/1845 train_time:38673ms step_avg:43.16ms
step:897/1845 train_time:38733ms step_avg:43.18ms
step:898/1845 train_time:38796ms step_avg:43.20ms
step:899/1845 train_time:38856ms step_avg:43.22ms
step:900/1845 train_time:38918ms step_avg:43.24ms
step:901/1845 train_time:38979ms step_avg:43.26ms
step:902/1845 train_time:39041ms step_avg:43.28ms
step:903/1845 train_time:39101ms step_avg:43.30ms
step:904/1845 train_time:39165ms step_avg:43.32ms
step:905/1845 train_time:39226ms step_avg:43.34ms
step:906/1845 train_time:39288ms step_avg:43.36ms
step:907/1845 train_time:39348ms step_avg:43.38ms
step:908/1845 train_time:39412ms step_avg:43.40ms
step:909/1845 train_time:39471ms step_avg:43.42ms
step:910/1845 train_time:39534ms step_avg:43.44ms
step:911/1845 train_time:39594ms step_avg:43.46ms
step:912/1845 train_time:39657ms step_avg:43.48ms
step:913/1845 train_time:39717ms step_avg:43.50ms
step:914/1845 train_time:39780ms step_avg:43.52ms
step:915/1845 train_time:39840ms step_avg:43.54ms
step:916/1845 train_time:39903ms step_avg:43.56ms
step:917/1845 train_time:39963ms step_avg:43.58ms
step:918/1845 train_time:40026ms step_avg:43.60ms
step:919/1845 train_time:40086ms step_avg:43.62ms
step:920/1845 train_time:40149ms step_avg:43.64ms
step:921/1845 train_time:40209ms step_avg:43.66ms
step:922/1845 train_time:40273ms step_avg:43.68ms
step:923/1845 train_time:40333ms step_avg:43.70ms
step:924/1845 train_time:40396ms step_avg:43.72ms
step:925/1845 train_time:40456ms step_avg:43.74ms
step:926/1845 train_time:40519ms step_avg:43.76ms
step:927/1845 train_time:40579ms step_avg:43.77ms
step:928/1845 train_time:40642ms step_avg:43.80ms
step:929/1845 train_time:40702ms step_avg:43.81ms
step:930/1845 train_time:40765ms step_avg:43.83ms
step:931/1845 train_time:40825ms step_avg:43.85ms
step:932/1845 train_time:40889ms step_avg:43.87ms
step:933/1845 train_time:40949ms step_avg:43.89ms
step:934/1845 train_time:41012ms step_avg:43.91ms
step:935/1845 train_time:41072ms step_avg:43.93ms
step:936/1845 train_time:41134ms step_avg:43.95ms
step:937/1845 train_time:41195ms step_avg:43.96ms
step:938/1845 train_time:41258ms step_avg:43.98ms
step:939/1845 train_time:41317ms step_avg:44.00ms
step:940/1845 train_time:41380ms step_avg:44.02ms
step:941/1845 train_time:41441ms step_avg:44.04ms
step:942/1845 train_time:41504ms step_avg:44.06ms
step:943/1845 train_time:41563ms step_avg:44.08ms
step:944/1845 train_time:41626ms step_avg:44.10ms
step:945/1845 train_time:41686ms step_avg:44.11ms
step:946/1845 train_time:41750ms step_avg:44.13ms
step:947/1845 train_time:41810ms step_avg:44.15ms
step:948/1845 train_time:41873ms step_avg:44.17ms
step:949/1845 train_time:41933ms step_avg:44.19ms
step:950/1845 train_time:41996ms step_avg:44.21ms
step:951/1845 train_time:42056ms step_avg:44.22ms
step:952/1845 train_time:42119ms step_avg:44.24ms
step:953/1845 train_time:42180ms step_avg:44.26ms
step:954/1845 train_time:42243ms step_avg:44.28ms
step:955/1845 train_time:42302ms step_avg:44.30ms
step:956/1845 train_time:42366ms step_avg:44.32ms
step:957/1845 train_time:42426ms step_avg:44.33ms
step:958/1845 train_time:42489ms step_avg:44.35ms
step:959/1845 train_time:42549ms step_avg:44.37ms
step:960/1845 train_time:42611ms step_avg:44.39ms
step:961/1845 train_time:42672ms step_avg:44.40ms
step:962/1845 train_time:42734ms step_avg:44.42ms
step:963/1845 train_time:42795ms step_avg:44.44ms
step:964/1845 train_time:42857ms step_avg:44.46ms
step:965/1845 train_time:42918ms step_avg:44.47ms
step:966/1845 train_time:42980ms step_avg:44.49ms
step:967/1845 train_time:43040ms step_avg:44.51ms
step:968/1845 train_time:43103ms step_avg:44.53ms
step:969/1845 train_time:43163ms step_avg:44.54ms
step:970/1845 train_time:43226ms step_avg:44.56ms
step:971/1845 train_time:43286ms step_avg:44.58ms
step:972/1845 train_time:43348ms step_avg:44.60ms
step:973/1845 train_time:43408ms step_avg:44.61ms
step:974/1845 train_time:43472ms step_avg:44.63ms
step:975/1845 train_time:43532ms step_avg:44.65ms
step:976/1845 train_time:43595ms step_avg:44.67ms
step:977/1845 train_time:43655ms step_avg:44.68ms
step:978/1845 train_time:43718ms step_avg:44.70ms
step:979/1845 train_time:43778ms step_avg:44.72ms
step:980/1845 train_time:43841ms step_avg:44.74ms
step:981/1845 train_time:43901ms step_avg:44.75ms
step:982/1845 train_time:43964ms step_avg:44.77ms
step:983/1845 train_time:44024ms step_avg:44.79ms
step:984/1845 train_time:44087ms step_avg:44.80ms
step:985/1845 train_time:44147ms step_avg:44.82ms
step:986/1845 train_time:44210ms step_avg:44.84ms
step:987/1845 train_time:44270ms step_avg:44.85ms
step:988/1845 train_time:44333ms step_avg:44.87ms
step:989/1845 train_time:44394ms step_avg:44.89ms
step:990/1845 train_time:44457ms step_avg:44.91ms
step:991/1845 train_time:44518ms step_avg:44.92ms
step:992/1845 train_time:44581ms step_avg:44.94ms
step:993/1845 train_time:44641ms step_avg:44.96ms
step:994/1845 train_time:44703ms step_avg:44.97ms
step:995/1845 train_time:44763ms step_avg:44.99ms
step:996/1845 train_time:44826ms step_avg:45.01ms
step:997/1845 train_time:44887ms step_avg:45.02ms
step:998/1845 train_time:44950ms step_avg:45.04ms
step:999/1845 train_time:45009ms step_avg:45.05ms
step:1000/1845 train_time:45072ms step_avg:45.07ms
step:1000/1845 val_loss:3.7826 train_time:45142ms step_avg:45.14ms
step:1001/1845 train_time:45163ms step_avg:45.12ms
step:1002/1845 train_time:45197ms step_avg:45.11ms
step:1003/1845 train_time:45257ms step_avg:45.12ms
step:1004/1845 train_time:45323ms step_avg:45.14ms
step:1005/1845 train_time:45386ms step_avg:45.16ms
step:1006/1845 train_time:45449ms step_avg:45.18ms
step:1007/1845 train_time:45509ms step_avg:45.19ms
step:1008/1845 train_time:45571ms step_avg:45.21ms
step:1009/1845 train_time:45631ms step_avg:45.22ms
step:1010/1845 train_time:45694ms step_avg:45.24ms
step:1011/1845 train_time:45754ms step_avg:45.26ms
step:1012/1845 train_time:45816ms step_avg:45.27ms
step:1013/1845 train_time:45875ms step_avg:45.29ms
step:1014/1845 train_time:45938ms step_avg:45.30ms
step:1015/1845 train_time:45997ms step_avg:45.32ms
step:1016/1845 train_time:46060ms step_avg:45.33ms
step:1017/1845 train_time:46121ms step_avg:45.35ms
step:1018/1845 train_time:46184ms step_avg:45.37ms
step:1019/1845 train_time:46245ms step_avg:45.38ms
step:1020/1845 train_time:46309ms step_avg:45.40ms
step:1021/1845 train_time:46370ms step_avg:45.42ms
step:1022/1845 train_time:46432ms step_avg:45.43ms
step:1023/1845 train_time:46493ms step_avg:45.45ms
step:1024/1845 train_time:46556ms step_avg:45.46ms
step:1025/1845 train_time:46615ms step_avg:45.48ms
step:1026/1845 train_time:46678ms step_avg:45.50ms
step:1027/1845 train_time:46738ms step_avg:45.51ms
step:1028/1845 train_time:46801ms step_avg:45.53ms
step:1029/1845 train_time:46860ms step_avg:45.54ms
step:1030/1845 train_time:46923ms step_avg:45.56ms
step:1031/1845 train_time:46984ms step_avg:45.57ms
step:1032/1845 train_time:47047ms step_avg:45.59ms
step:1033/1845 train_time:47107ms step_avg:45.60ms
step:1034/1845 train_time:47170ms step_avg:45.62ms
step:1035/1845 train_time:47230ms step_avg:45.63ms
step:1036/1845 train_time:47292ms step_avg:45.65ms
step:1037/1845 train_time:47353ms step_avg:45.66ms
step:1038/1845 train_time:47416ms step_avg:45.68ms
step:1039/1845 train_time:47477ms step_avg:45.69ms
step:1040/1845 train_time:47540ms step_avg:45.71ms
step:1041/1845 train_time:47600ms step_avg:45.72ms
step:1042/1845 train_time:47662ms step_avg:45.74ms
step:1043/1845 train_time:47723ms step_avg:45.76ms
step:1044/1845 train_time:47785ms step_avg:45.77ms
step:1045/1845 train_time:47845ms step_avg:45.78ms
step:1046/1845 train_time:47908ms step_avg:45.80ms
step:1047/1845 train_time:47968ms step_avg:45.82ms
step:1048/1845 train_time:48031ms step_avg:45.83ms
step:1049/1845 train_time:48092ms step_avg:45.85ms
step:1050/1845 train_time:48154ms step_avg:45.86ms
step:1051/1845 train_time:48214ms step_avg:45.87ms
step:1052/1845 train_time:48277ms step_avg:45.89ms
step:1053/1845 train_time:48337ms step_avg:45.90ms
step:1054/1845 train_time:48401ms step_avg:45.92ms
step:1055/1845 train_time:48461ms step_avg:45.93ms
step:1056/1845 train_time:48524ms step_avg:45.95ms
step:1057/1845 train_time:48585ms step_avg:45.96ms
step:1058/1845 train_time:48647ms step_avg:45.98ms
step:1059/1845 train_time:48707ms step_avg:45.99ms
step:1060/1845 train_time:48769ms step_avg:46.01ms
step:1061/1845 train_time:48829ms step_avg:46.02ms
step:1062/1845 train_time:48892ms step_avg:46.04ms
step:1063/1845 train_time:48952ms step_avg:46.05ms
step:1064/1845 train_time:49015ms step_avg:46.07ms
step:1065/1845 train_time:49076ms step_avg:46.08ms
step:1066/1845 train_time:49138ms step_avg:46.10ms
step:1067/1845 train_time:49198ms step_avg:46.11ms
step:1068/1845 train_time:49260ms step_avg:46.12ms
step:1069/1845 train_time:49321ms step_avg:46.14ms
step:1070/1845 train_time:49384ms step_avg:46.15ms
step:1071/1845 train_time:49444ms step_avg:46.17ms
step:1072/1845 train_time:49506ms step_avg:46.18ms
step:1073/1845 train_time:49567ms step_avg:46.19ms
step:1074/1845 train_time:49630ms step_avg:46.21ms
step:1075/1845 train_time:49690ms step_avg:46.22ms
step:1076/1845 train_time:49752ms step_avg:46.24ms
step:1077/1845 train_time:49812ms step_avg:46.25ms
step:1078/1845 train_time:49875ms step_avg:46.27ms
step:1079/1845 train_time:49934ms step_avg:46.28ms
step:1080/1845 train_time:49998ms step_avg:46.29ms
step:1081/1845 train_time:50057ms step_avg:46.31ms
step:1082/1845 train_time:50120ms step_avg:46.32ms
step:1083/1845 train_time:50180ms step_avg:46.33ms
step:1084/1845 train_time:50243ms step_avg:46.35ms
step:1085/1845 train_time:50303ms step_avg:46.36ms
step:1086/1845 train_time:50365ms step_avg:46.38ms
step:1087/1845 train_time:50426ms step_avg:46.39ms
step:1088/1845 train_time:50489ms step_avg:46.41ms
step:1089/1845 train_time:50549ms step_avg:46.42ms
step:1090/1845 train_time:50611ms step_avg:46.43ms
step:1091/1845 train_time:50670ms step_avg:46.44ms
step:1092/1845 train_time:50733ms step_avg:46.46ms
step:1093/1845 train_time:50794ms step_avg:46.47ms
step:1094/1845 train_time:50856ms step_avg:46.49ms
step:1095/1845 train_time:50915ms step_avg:46.50ms
step:1096/1845 train_time:50979ms step_avg:46.51ms
step:1097/1845 train_time:51038ms step_avg:46.53ms
step:1098/1845 train_time:51101ms step_avg:46.54ms
step:1099/1845 train_time:51161ms step_avg:46.55ms
step:1100/1845 train_time:51224ms step_avg:46.57ms
step:1101/1845 train_time:51285ms step_avg:46.58ms
step:1102/1845 train_time:51347ms step_avg:46.59ms
step:1103/1845 train_time:51408ms step_avg:46.61ms
step:1104/1845 train_time:51470ms step_avg:46.62ms
step:1105/1845 train_time:51531ms step_avg:46.63ms
step:1106/1845 train_time:51593ms step_avg:46.65ms
step:1107/1845 train_time:51653ms step_avg:46.66ms
step:1108/1845 train_time:51715ms step_avg:46.67ms
step:1109/1845 train_time:51776ms step_avg:46.69ms
step:1110/1845 train_time:51838ms step_avg:46.70ms
step:1111/1845 train_time:51898ms step_avg:46.71ms
step:1112/1845 train_time:51961ms step_avg:46.73ms
step:1113/1845 train_time:52021ms step_avg:46.74ms
step:1114/1845 train_time:52084ms step_avg:46.75ms
step:1115/1845 train_time:52144ms step_avg:46.77ms
step:1116/1845 train_time:52207ms step_avg:46.78ms
step:1117/1845 train_time:52268ms step_avg:46.79ms
step:1118/1845 train_time:52330ms step_avg:46.81ms
step:1119/1845 train_time:52390ms step_avg:46.82ms
step:1120/1845 train_time:52452ms step_avg:46.83ms
step:1121/1845 train_time:52513ms step_avg:46.84ms
step:1122/1845 train_time:52576ms step_avg:46.86ms
step:1123/1845 train_time:52636ms step_avg:46.87ms
step:1124/1845 train_time:52699ms step_avg:46.89ms
step:1125/1845 train_time:52759ms step_avg:46.90ms
step:1126/1845 train_time:52822ms step_avg:46.91ms
step:1127/1845 train_time:52883ms step_avg:46.92ms
step:1128/1845 train_time:52945ms step_avg:46.94ms
step:1129/1845 train_time:53006ms step_avg:46.95ms
step:1130/1845 train_time:53068ms step_avg:46.96ms
step:1131/1845 train_time:53128ms step_avg:46.97ms
step:1132/1845 train_time:53192ms step_avg:46.99ms
step:1133/1845 train_time:53251ms step_avg:47.00ms
step:1134/1845 train_time:53314ms step_avg:47.01ms
step:1135/1845 train_time:53373ms step_avg:47.03ms
step:1136/1845 train_time:53436ms step_avg:47.04ms
step:1137/1845 train_time:53496ms step_avg:47.05ms
step:1138/1845 train_time:53559ms step_avg:47.06ms
step:1139/1845 train_time:53619ms step_avg:47.08ms
step:1140/1845 train_time:53682ms step_avg:47.09ms
step:1141/1845 train_time:53742ms step_avg:47.10ms
step:1142/1845 train_time:53805ms step_avg:47.11ms
step:1143/1845 train_time:53866ms step_avg:47.13ms
step:1144/1845 train_time:53928ms step_avg:47.14ms
step:1145/1845 train_time:53989ms step_avg:47.15ms
step:1146/1845 train_time:54051ms step_avg:47.16ms
step:1147/1845 train_time:54111ms step_avg:47.18ms
step:1148/1845 train_time:54173ms step_avg:47.19ms
step:1149/1845 train_time:54233ms step_avg:47.20ms
step:1150/1845 train_time:54297ms step_avg:47.21ms
step:1151/1845 train_time:54356ms step_avg:47.22ms
step:1152/1845 train_time:54419ms step_avg:47.24ms
step:1153/1845 train_time:54479ms step_avg:47.25ms
step:1154/1845 train_time:54542ms step_avg:47.26ms
step:1155/1845 train_time:54602ms step_avg:47.27ms
step:1156/1845 train_time:54665ms step_avg:47.29ms
step:1157/1845 train_time:54725ms step_avg:47.30ms
step:1158/1845 train_time:54788ms step_avg:47.31ms
step:1159/1845 train_time:54848ms step_avg:47.32ms
step:1160/1845 train_time:54911ms step_avg:47.34ms
step:1161/1845 train_time:54971ms step_avg:47.35ms
step:1162/1845 train_time:55033ms step_avg:47.36ms
step:1163/1845 train_time:55094ms step_avg:47.37ms
step:1164/1845 train_time:55156ms step_avg:47.38ms
step:1165/1845 train_time:55216ms step_avg:47.40ms
step:1166/1845 train_time:55279ms step_avg:47.41ms
step:1167/1845 train_time:55339ms step_avg:47.42ms
step:1168/1845 train_time:55402ms step_avg:47.43ms
step:1169/1845 train_time:55462ms step_avg:47.44ms
step:1170/1845 train_time:55525ms step_avg:47.46ms
step:1171/1845 train_time:55585ms step_avg:47.47ms
step:1172/1845 train_time:55648ms step_avg:47.48ms
step:1173/1845 train_time:55708ms step_avg:47.49ms
step:1174/1845 train_time:55770ms step_avg:47.50ms
step:1175/1845 train_time:55830ms step_avg:47.52ms
step:1176/1845 train_time:55893ms step_avg:47.53ms
step:1177/1845 train_time:55953ms step_avg:47.54ms
step:1178/1845 train_time:56016ms step_avg:47.55ms
step:1179/1845 train_time:56077ms step_avg:47.56ms
step:1180/1845 train_time:56139ms step_avg:47.58ms
step:1181/1845 train_time:56200ms step_avg:47.59ms
step:1182/1845 train_time:56262ms step_avg:47.60ms
step:1183/1845 train_time:56322ms step_avg:47.61ms
step:1184/1845 train_time:56385ms step_avg:47.62ms
step:1185/1845 train_time:56445ms step_avg:47.63ms
step:1186/1845 train_time:56507ms step_avg:47.64ms
step:1187/1845 train_time:56567ms step_avg:47.66ms
step:1188/1845 train_time:56629ms step_avg:47.67ms
step:1189/1845 train_time:56690ms step_avg:47.68ms
step:1190/1845 train_time:56752ms step_avg:47.69ms
step:1191/1845 train_time:56812ms step_avg:47.70ms
step:1192/1845 train_time:56875ms step_avg:47.71ms
step:1193/1845 train_time:56935ms step_avg:47.72ms
step:1194/1845 train_time:56999ms step_avg:47.74ms
step:1195/1845 train_time:57058ms step_avg:47.75ms
step:1196/1845 train_time:57122ms step_avg:47.76ms
step:1197/1845 train_time:57182ms step_avg:47.77ms
step:1198/1845 train_time:57245ms step_avg:47.78ms
step:1199/1845 train_time:57305ms step_avg:47.79ms
step:1200/1845 train_time:57368ms step_avg:47.81ms
step:1201/1845 train_time:57428ms step_avg:47.82ms
step:1202/1845 train_time:57490ms step_avg:47.83ms
step:1203/1845 train_time:57550ms step_avg:47.84ms
step:1204/1845 train_time:57613ms step_avg:47.85ms
step:1205/1845 train_time:57674ms step_avg:47.86ms
step:1206/1845 train_time:57762ms step_avg:47.90ms
step:1207/1845 train_time:57848ms step_avg:47.93ms
step:1208/1845 train_time:57938ms step_avg:47.96ms
step:1209/1845 train_time:58025ms step_avg:47.99ms
step:1210/1845 train_time:58115ms step_avg:48.03ms
step:1211/1845 train_time:58202ms step_avg:48.06ms
step:1212/1845 train_time:58291ms step_avg:48.10ms
step:1213/1845 train_time:58378ms step_avg:48.13ms
step:1214/1845 train_time:58465ms step_avg:48.16ms
step:1215/1845 train_time:58553ms step_avg:48.19ms
step:1216/1845 train_time:58643ms step_avg:48.23ms
step:1217/1845 train_time:58729ms step_avg:48.26ms
step:1218/1845 train_time:58817ms step_avg:48.29ms
step:1219/1845 train_time:58903ms step_avg:48.32ms
step:1220/1845 train_time:58993ms step_avg:48.35ms
step:1221/1845 train_time:59079ms step_avg:48.39ms
step:1222/1845 train_time:59168ms step_avg:48.42ms
step:1223/1845 train_time:59255ms step_avg:48.45ms
step:1224/1845 train_time:59344ms step_avg:48.48ms
step:1225/1845 train_time:59430ms step_avg:48.51ms
step:1226/1845 train_time:59520ms step_avg:48.55ms
step:1227/1845 train_time:59605ms step_avg:48.58ms
step:1228/1845 train_time:59695ms step_avg:48.61ms
step:1229/1845 train_time:59782ms step_avg:48.64ms
step:1230/1845 train_time:59872ms step_avg:48.68ms
step:1231/1845 train_time:59958ms step_avg:48.71ms
step:1232/1845 train_time:60047ms step_avg:48.74ms
step:1233/1845 train_time:60134ms step_avg:48.77ms
step:1234/1845 train_time:60223ms step_avg:48.80ms
step:1235/1845 train_time:60308ms step_avg:48.83ms
step:1236/1845 train_time:60399ms step_avg:48.87ms
step:1237/1845 train_time:60485ms step_avg:48.90ms
step:1238/1845 train_time:60574ms step_avg:48.93ms
step:1239/1845 train_time:60662ms step_avg:48.96ms
step:1240/1845 train_time:60751ms step_avg:48.99ms
step:1241/1845 train_time:60838ms step_avg:49.02ms
step:1242/1845 train_time:60926ms step_avg:49.06ms
step:1243/1845 train_time:61013ms step_avg:49.09ms
step:1244/1845 train_time:61103ms step_avg:49.12ms
step:1245/1845 train_time:61189ms step_avg:49.15ms
step:1246/1845 train_time:61279ms step_avg:49.18ms
step:1247/1845 train_time:61365ms step_avg:49.21ms
step:1248/1845 train_time:61455ms step_avg:49.24ms
step:1249/1845 train_time:61542ms step_avg:49.27ms
step:1250/1845 train_time:61631ms step_avg:49.30ms
step:1250/1845 val_loss:3.5348 train_time:61727ms step_avg:49.38ms
step:1251/1845 train_time:61746ms step_avg:49.36ms
step:1252/1845 train_time:61805ms step_avg:49.37ms
step:1253/1845 train_time:61898ms step_avg:49.40ms
step:1254/1845 train_time:61987ms step_avg:49.43ms
step:1255/1845 train_time:62076ms step_avg:49.46ms
step:1256/1845 train_time:62163ms step_avg:49.49ms
step:1257/1845 train_time:62249ms step_avg:49.52ms
step:1258/1845 train_time:62338ms step_avg:49.55ms
step:1259/1845 train_time:62422ms step_avg:49.58ms
step:1260/1845 train_time:62511ms step_avg:49.61ms
step:1261/1845 train_time:62596ms step_avg:49.64ms
step:1262/1845 train_time:62685ms step_avg:49.67ms
step:1263/1845 train_time:62775ms step_avg:49.70ms
step:1264/1845 train_time:62866ms step_avg:49.74ms
step:1265/1845 train_time:62953ms step_avg:49.77ms
step:1266/1845 train_time:63042ms step_avg:49.80ms
step:1267/1845 train_time:63129ms step_avg:49.83ms
step:1268/1845 train_time:63219ms step_avg:49.86ms
step:1269/1845 train_time:63305ms step_avg:49.89ms
step:1270/1845 train_time:63394ms step_avg:49.92ms
step:1271/1845 train_time:63480ms step_avg:49.94ms
step:1272/1845 train_time:63569ms step_avg:49.98ms
step:1273/1845 train_time:63655ms step_avg:50.00ms
step:1274/1845 train_time:63745ms step_avg:50.04ms
step:1275/1845 train_time:63833ms step_avg:50.06ms
step:1276/1845 train_time:63923ms step_avg:50.10ms
step:1277/1845 train_time:64010ms step_avg:50.13ms
step:1278/1845 train_time:64100ms step_avg:50.16ms
step:1279/1845 train_time:64186ms step_avg:50.18ms
step:1280/1845 train_time:64275ms step_avg:50.21ms
step:1281/1845 train_time:64361ms step_avg:50.24ms
step:1282/1845 train_time:64448ms step_avg:50.27ms
step:1283/1845 train_time:64534ms step_avg:50.30ms
step:1284/1845 train_time:64622ms step_avg:50.33ms
step:1285/1845 train_time:64709ms step_avg:50.36ms
step:1286/1845 train_time:64800ms step_avg:50.39ms
step:1287/1845 train_time:64887ms step_avg:50.42ms
step:1288/1845 train_time:64979ms step_avg:50.45ms
step:1289/1845 train_time:65066ms step_avg:50.48ms
step:1290/1845 train_time:65156ms step_avg:50.51ms
step:1291/1845 train_time:65242ms step_avg:50.54ms
step:1292/1845 train_time:65330ms step_avg:50.57ms
step:1293/1845 train_time:65416ms step_avg:50.59ms
step:1294/1845 train_time:65505ms step_avg:50.62ms
step:1295/1845 train_time:65592ms step_avg:50.65ms
step:1296/1845 train_time:65681ms step_avg:50.68ms
step:1297/1845 train_time:65767ms step_avg:50.71ms
step:1298/1845 train_time:65859ms step_avg:50.74ms
step:1299/1845 train_time:65945ms step_avg:50.77ms
step:1300/1845 train_time:66035ms step_avg:50.80ms
step:1301/1845 train_time:66121ms step_avg:50.82ms
step:1302/1845 train_time:66210ms step_avg:50.85ms
step:1303/1845 train_time:66297ms step_avg:50.88ms
step:1304/1845 train_time:66385ms step_avg:50.91ms
step:1305/1845 train_time:66470ms step_avg:50.93ms
step:1306/1845 train_time:66560ms step_avg:50.96ms
step:1307/1845 train_time:66646ms step_avg:50.99ms
step:1308/1845 train_time:66735ms step_avg:51.02ms
step:1309/1845 train_time:66823ms step_avg:51.05ms
step:1310/1845 train_time:66912ms step_avg:51.08ms
step:1311/1845 train_time:66999ms step_avg:51.11ms
step:1312/1845 train_time:67087ms step_avg:51.13ms
step:1313/1845 train_time:67175ms step_avg:51.16ms
step:1314/1845 train_time:67263ms step_avg:51.19ms
step:1315/1845 train_time:67349ms step_avg:51.22ms
step:1316/1845 train_time:67438ms step_avg:51.24ms
step:1317/1845 train_time:67523ms step_avg:51.27ms
step:1318/1845 train_time:67613ms step_avg:51.30ms
step:1319/1845 train_time:67700ms step_avg:51.33ms
step:1320/1845 train_time:67789ms step_avg:51.36ms
step:1321/1845 train_time:67875ms step_avg:51.38ms
step:1322/1845 train_time:67964ms step_avg:51.41ms
step:1323/1845 train_time:68051ms step_avg:51.44ms
step:1324/1845 train_time:68141ms step_avg:51.47ms
step:1325/1845 train_time:68227ms step_avg:51.49ms
step:1326/1845 train_time:68316ms step_avg:51.52ms
step:1327/1845 train_time:68402ms step_avg:51.55ms
step:1328/1845 train_time:68491ms step_avg:51.57ms
step:1329/1845 train_time:68578ms step_avg:51.60ms
step:1330/1845 train_time:68666ms step_avg:51.63ms
step:1331/1845 train_time:68753ms step_avg:51.66ms
step:1332/1845 train_time:68842ms step_avg:51.68ms
step:1333/1845 train_time:68928ms step_avg:51.71ms
step:1334/1845 train_time:69018ms step_avg:51.74ms
step:1335/1845 train_time:69104ms step_avg:51.76ms
step:1336/1845 train_time:69195ms step_avg:51.79ms
step:1337/1845 train_time:69280ms step_avg:51.82ms
step:1338/1845 train_time:69369ms step_avg:51.85ms
step:1339/1845 train_time:69456ms step_avg:51.87ms
step:1340/1845 train_time:69545ms step_avg:51.90ms
step:1341/1845 train_time:69632ms step_avg:51.93ms
step:1342/1845 train_time:69720ms step_avg:51.95ms
step:1343/1845 train_time:69806ms step_avg:51.98ms
step:1344/1845 train_time:69896ms step_avg:52.01ms
step:1345/1845 train_time:69983ms step_avg:52.03ms
step:1346/1845 train_time:70073ms step_avg:52.06ms
step:1347/1845 train_time:70161ms step_avg:52.09ms
step:1348/1845 train_time:70249ms step_avg:52.11ms
step:1349/1845 train_time:70336ms step_avg:52.14ms
step:1350/1845 train_time:70424ms step_avg:52.17ms
step:1351/1845 train_time:70511ms step_avg:52.19ms
step:1352/1845 train_time:70601ms step_avg:52.22ms
step:1353/1845 train_time:70686ms step_avg:52.24ms
step:1354/1845 train_time:70778ms step_avg:52.27ms
step:1355/1845 train_time:70864ms step_avg:52.30ms
step:1356/1845 train_time:70953ms step_avg:52.33ms
step:1357/1845 train_time:71040ms step_avg:52.35ms
step:1358/1845 train_time:71128ms step_avg:52.38ms
step:1359/1845 train_time:71214ms step_avg:52.40ms
step:1360/1845 train_time:71304ms step_avg:52.43ms
step:1361/1845 train_time:71390ms step_avg:52.45ms
step:1362/1845 train_time:71480ms step_avg:52.48ms
step:1363/1845 train_time:71565ms step_avg:52.51ms
step:1364/1845 train_time:71655ms step_avg:52.53ms
step:1365/1845 train_time:71741ms step_avg:52.56ms
step:1366/1845 train_time:71830ms step_avg:52.58ms
step:1367/1845 train_time:71917ms step_avg:52.61ms
step:1368/1845 train_time:72005ms step_avg:52.63ms
step:1369/1845 train_time:72092ms step_avg:52.66ms
step:1370/1845 train_time:72181ms step_avg:52.69ms
step:1371/1845 train_time:72266ms step_avg:52.71ms
step:1372/1845 train_time:72356ms step_avg:52.74ms
step:1373/1845 train_time:72442ms step_avg:52.76ms
step:1374/1845 train_time:72532ms step_avg:52.79ms
step:1375/1845 train_time:72618ms step_avg:52.81ms
step:1376/1845 train_time:72707ms step_avg:52.84ms
step:1377/1845 train_time:72794ms step_avg:52.86ms
step:1378/1845 train_time:72883ms step_avg:52.89ms
step:1379/1845 train_time:72969ms step_avg:52.91ms
step:1380/1845 train_time:73060ms step_avg:52.94ms
step:1381/1845 train_time:73146ms step_avg:52.97ms
step:1382/1845 train_time:73236ms step_avg:52.99ms
step:1383/1845 train_time:73321ms step_avg:53.02ms
step:1384/1845 train_time:73411ms step_avg:53.04ms
step:1385/1845 train_time:73499ms step_avg:53.07ms
step:1386/1845 train_time:73588ms step_avg:53.09ms
step:1387/1845 train_time:73674ms step_avg:53.12ms
step:1388/1845 train_time:73763ms step_avg:53.14ms
step:1389/1845 train_time:73849ms step_avg:53.17ms
step:1390/1845 train_time:73939ms step_avg:53.19ms
step:1391/1845 train_time:74025ms step_avg:53.22ms
step:1392/1845 train_time:74114ms step_avg:53.24ms
step:1393/1845 train_time:74201ms step_avg:53.27ms
step:1394/1845 train_time:74289ms step_avg:53.29ms
step:1395/1845 train_time:74375ms step_avg:53.32ms
step:1396/1845 train_time:74463ms step_avg:53.34ms
step:1397/1845 train_time:74550ms step_avg:53.36ms
step:1398/1845 train_time:74640ms step_avg:53.39ms
step:1399/1845 train_time:74725ms step_avg:53.41ms
step:1400/1845 train_time:74816ms step_avg:53.44ms
step:1401/1845 train_time:74903ms step_avg:53.46ms
step:1402/1845 train_time:74992ms step_avg:53.49ms
step:1403/1845 train_time:75079ms step_avg:53.51ms
step:1404/1845 train_time:75168ms step_avg:53.54ms
step:1405/1845 train_time:75254ms step_avg:53.56ms
step:1406/1845 train_time:75343ms step_avg:53.59ms
step:1407/1845 train_time:75431ms step_avg:53.61ms
step:1408/1845 train_time:75520ms step_avg:53.64ms
step:1409/1845 train_time:75605ms step_avg:53.66ms
step:1410/1845 train_time:75695ms step_avg:53.68ms
step:1411/1845 train_time:75781ms step_avg:53.71ms
step:1412/1845 train_time:75871ms step_avg:53.73ms
step:1413/1845 train_time:75958ms step_avg:53.76ms
step:1414/1845 train_time:76046ms step_avg:53.78ms
step:1415/1845 train_time:76134ms step_avg:53.80ms
step:1416/1845 train_time:76223ms step_avg:53.83ms
step:1417/1845 train_time:76309ms step_avg:53.85ms
step:1418/1845 train_time:76399ms step_avg:53.88ms
step:1419/1845 train_time:76485ms step_avg:53.90ms
step:1420/1845 train_time:76574ms step_avg:53.93ms
step:1421/1845 train_time:76660ms step_avg:53.95ms
step:1422/1845 train_time:76749ms step_avg:53.97ms
step:1423/1845 train_time:76836ms step_avg:54.00ms
step:1424/1845 train_time:76925ms step_avg:54.02ms
step:1425/1845 train_time:77011ms step_avg:54.04ms
step:1426/1845 train_time:77101ms step_avg:54.07ms
step:1427/1845 train_time:77187ms step_avg:54.09ms
step:1428/1845 train_time:77277ms step_avg:54.12ms
step:1429/1845 train_time:77362ms step_avg:54.14ms
step:1430/1845 train_time:77452ms step_avg:54.16ms
step:1431/1845 train_time:77539ms step_avg:54.19ms
step:1432/1845 train_time:77627ms step_avg:54.21ms
step:1433/1845 train_time:77713ms step_avg:54.23ms
step:1434/1845 train_time:77803ms step_avg:54.26ms
step:1435/1845 train_time:77889ms step_avg:54.28ms
step:1436/1845 train_time:77979ms step_avg:54.30ms
step:1437/1845 train_time:78065ms step_avg:54.32ms
step:1438/1845 train_time:78155ms step_avg:54.35ms
step:1439/1845 train_time:78242ms step_avg:54.37ms
step:1440/1845 train_time:78331ms step_avg:54.40ms
step:1441/1845 train_time:78418ms step_avg:54.42ms
step:1442/1845 train_time:78506ms step_avg:54.44ms
step:1443/1845 train_time:78593ms step_avg:54.46ms
step:1444/1845 train_time:78682ms step_avg:54.49ms
step:1445/1845 train_time:78768ms step_avg:54.51ms
step:1446/1845 train_time:78858ms step_avg:54.54ms
step:1447/1845 train_time:78944ms step_avg:54.56ms
step:1448/1845 train_time:79034ms step_avg:54.58ms
step:1449/1845 train_time:79121ms step_avg:54.60ms
step:1450/1845 train_time:79209ms step_avg:54.63ms
step:1451/1845 train_time:79296ms step_avg:54.65ms
step:1452/1845 train_time:79385ms step_avg:54.67ms
step:1453/1845 train_time:79472ms step_avg:54.70ms
step:1454/1845 train_time:79562ms step_avg:54.72ms
step:1455/1845 train_time:79649ms step_avg:54.74ms
step:1456/1845 train_time:79738ms step_avg:54.76ms
step:1457/1845 train_time:79824ms step_avg:54.79ms
step:1458/1845 train_time:79913ms step_avg:54.81ms
step:1459/1845 train_time:80000ms step_avg:54.83ms
step:1460/1845 train_time:80089ms step_avg:54.86ms
step:1461/1845 train_time:80176ms step_avg:54.88ms
step:1462/1845 train_time:80264ms step_avg:54.90ms
step:1463/1845 train_time:80350ms step_avg:54.92ms
step:1464/1845 train_time:80440ms step_avg:54.95ms
step:1465/1845 train_time:80527ms step_avg:54.97ms
step:1466/1845 train_time:80616ms step_avg:54.99ms
step:1467/1845 train_time:80702ms step_avg:55.01ms
step:1468/1845 train_time:80791ms step_avg:55.03ms
step:1469/1845 train_time:80877ms step_avg:55.06ms
step:1470/1845 train_time:80965ms step_avg:55.08ms
step:1471/1845 train_time:81053ms step_avg:55.10ms
step:1472/1845 train_time:81142ms step_avg:55.12ms
step:1473/1845 train_time:81228ms step_avg:55.14ms
step:1474/1845 train_time:81317ms step_avg:55.17ms
step:1475/1845 train_time:81404ms step_avg:55.19ms
step:1476/1845 train_time:81495ms step_avg:55.21ms
step:1477/1845 train_time:81582ms step_avg:55.24ms
step:1478/1845 train_time:81672ms step_avg:55.26ms
step:1479/1845 train_time:81758ms step_avg:55.28ms
step:1480/1845 train_time:81846ms step_avg:55.30ms
step:1481/1845 train_time:81933ms step_avg:55.32ms
step:1482/1845 train_time:82022ms step_avg:55.35ms
step:1483/1845 train_time:82109ms step_avg:55.37ms
step:1484/1845 train_time:82200ms step_avg:55.39ms
step:1485/1845 train_time:82286ms step_avg:55.41ms
step:1486/1845 train_time:82377ms step_avg:55.44ms
step:1487/1845 train_time:82463ms step_avg:55.46ms
step:1488/1845 train_time:82553ms step_avg:55.48ms
step:1489/1845 train_time:82639ms step_avg:55.50ms
step:1490/1845 train_time:82729ms step_avg:55.52ms
step:1491/1845 train_time:82815ms step_avg:55.54ms
step:1492/1845 train_time:82903ms step_avg:55.57ms
step:1493/1845 train_time:82990ms step_avg:55.59ms
step:1494/1845 train_time:83079ms step_avg:55.61ms
step:1495/1845 train_time:83165ms step_avg:55.63ms
step:1496/1845 train_time:83254ms step_avg:55.65ms
step:1497/1845 train_time:83341ms step_avg:55.67ms
step:1498/1845 train_time:83430ms step_avg:55.69ms
step:1499/1845 train_time:83517ms step_avg:55.72ms
step:1500/1845 train_time:83605ms step_avg:55.74ms
step:1500/1845 val_loss:3.4037 train_time:83702ms step_avg:55.80ms
step:1501/1845 train_time:83720ms step_avg:55.78ms
step:1502/1845 train_time:83783ms step_avg:55.78ms
step:1503/1845 train_time:83875ms step_avg:55.81ms
step:1504/1845 train_time:83967ms step_avg:55.83ms
step:1505/1845 train_time:84053ms step_avg:55.85ms
step:1506/1845 train_time:84140ms step_avg:55.87ms
step:1507/1845 train_time:84226ms step_avg:55.89ms
step:1508/1845 train_time:84316ms step_avg:55.91ms
step:1509/1845 train_time:84402ms step_avg:55.93ms
step:1510/1845 train_time:84491ms step_avg:55.95ms
step:1511/1845 train_time:84576ms step_avg:55.97ms
step:1512/1845 train_time:84666ms step_avg:56.00ms
step:1513/1845 train_time:84755ms step_avg:56.02ms
step:1514/1845 train_time:84845ms step_avg:56.04ms
step:1515/1845 train_time:84933ms step_avg:56.06ms
step:1516/1845 train_time:85022ms step_avg:56.08ms
step:1517/1845 train_time:85109ms step_avg:56.10ms
step:1518/1845 train_time:85198ms step_avg:56.12ms
step:1519/1845 train_time:85284ms step_avg:56.14ms
step:1520/1845 train_time:85373ms step_avg:56.17ms
step:1521/1845 train_time:85457ms step_avg:56.19ms
step:1522/1845 train_time:85545ms step_avg:56.21ms
step:1523/1845 train_time:85632ms step_avg:56.23ms
step:1524/1845 train_time:85722ms step_avg:56.25ms
step:1525/1845 train_time:85809ms step_avg:56.27ms
step:1526/1845 train_time:85900ms step_avg:56.29ms
step:1527/1845 train_time:85986ms step_avg:56.31ms
step:1528/1845 train_time:86076ms step_avg:56.33ms
step:1529/1845 train_time:86162ms step_avg:56.35ms
step:1530/1845 train_time:86251ms step_avg:56.37ms
step:1531/1845 train_time:86336ms step_avg:56.39ms
step:1532/1845 train_time:86426ms step_avg:56.41ms
step:1533/1845 train_time:86512ms step_avg:56.43ms
step:1534/1845 train_time:86601ms step_avg:56.45ms
step:1535/1845 train_time:86687ms step_avg:56.47ms
step:1536/1845 train_time:86777ms step_avg:56.50ms
step:1537/1845 train_time:86864ms step_avg:56.52ms
step:1538/1845 train_time:86955ms step_avg:56.54ms
step:1539/1845 train_time:87041ms step_avg:56.56ms
step:1540/1845 train_time:87129ms step_avg:56.58ms
step:1541/1845 train_time:87215ms step_avg:56.60ms
step:1542/1845 train_time:87304ms step_avg:56.62ms
step:1543/1845 train_time:87390ms step_avg:56.64ms
step:1544/1845 train_time:87479ms step_avg:56.66ms
step:1545/1845 train_time:87565ms step_avg:56.68ms
step:1546/1845 train_time:87655ms step_avg:56.70ms
step:1547/1845 train_time:87741ms step_avg:56.72ms
step:1548/1845 train_time:87832ms step_avg:56.74ms
step:1549/1845 train_time:87919ms step_avg:56.76ms
step:1550/1845 train_time:88009ms step_avg:56.78ms
step:1551/1845 train_time:88095ms step_avg:56.80ms
step:1552/1845 train_time:88183ms step_avg:56.82ms
step:1553/1845 train_time:88270ms step_avg:56.84ms
step:1554/1845 train_time:88359ms step_avg:56.86ms
step:1555/1845 train_time:88444ms step_avg:56.88ms
step:1556/1845 train_time:88535ms step_avg:56.90ms
step:1557/1845 train_time:88622ms step_avg:56.92ms
step:1558/1845 train_time:88711ms step_avg:56.94ms
step:1559/1845 train_time:88799ms step_avg:56.96ms
step:1560/1845 train_time:88889ms step_avg:56.98ms
step:1561/1845 train_time:88976ms step_avg:57.00ms
step:1562/1845 train_time:89065ms step_avg:57.02ms
step:1563/1845 train_time:89151ms step_avg:57.04ms
step:1564/1845 train_time:89240ms step_avg:57.06ms
step:1565/1845 train_time:89326ms step_avg:57.08ms
step:1566/1845 train_time:89415ms step_avg:57.10ms
step:1567/1845 train_time:89501ms step_avg:57.12ms
step:1568/1845 train_time:89591ms step_avg:57.14ms
step:1569/1845 train_time:89678ms step_avg:57.16ms
step:1570/1845 train_time:89767ms step_avg:57.18ms
step:1571/1845 train_time:89854ms step_avg:57.20ms
step:1572/1845 train_time:89942ms step_avg:57.21ms
step:1573/1845 train_time:90029ms step_avg:57.23ms
step:1574/1845 train_time:90118ms step_avg:57.25ms
step:1575/1845 train_time:90204ms step_avg:57.27ms
step:1576/1845 train_time:90295ms step_avg:57.29ms
step:1577/1845 train_time:90380ms step_avg:57.31ms
step:1578/1845 train_time:90471ms step_avg:57.33ms
step:1579/1845 train_time:90558ms step_avg:57.35ms
step:1580/1845 train_time:90646ms step_avg:57.37ms
step:1581/1845 train_time:90734ms step_avg:57.39ms
step:1582/1845 train_time:90822ms step_avg:57.41ms
step:1583/1845 train_time:90909ms step_avg:57.43ms
step:1584/1845 train_time:90998ms step_avg:57.45ms
step:1585/1845 train_time:91086ms step_avg:57.47ms
step:1586/1845 train_time:91175ms step_avg:57.49ms
step:1587/1845 train_time:91260ms step_avg:57.50ms
step:1588/1845 train_time:91351ms step_avg:57.53ms
step:1589/1845 train_time:91437ms step_avg:57.54ms
step:1590/1845 train_time:91526ms step_avg:57.56ms
step:1591/1845 train_time:91612ms step_avg:57.58ms
step:1592/1845 train_time:91702ms step_avg:57.60ms
step:1593/1845 train_time:91788ms step_avg:57.62ms
step:1594/1845 train_time:91876ms step_avg:57.64ms
step:1595/1845 train_time:91963ms step_avg:57.66ms
step:1596/1845 train_time:92052ms step_avg:57.68ms
step:1597/1845 train_time:92139ms step_avg:57.70ms
step:1598/1845 train_time:92228ms step_avg:57.71ms
step:1599/1845 train_time:92315ms step_avg:57.73ms
step:1600/1845 train_time:92403ms step_avg:57.75ms
step:1601/1845 train_time:92490ms step_avg:57.77ms
step:1602/1845 train_time:92579ms step_avg:57.79ms
step:1603/1845 train_time:92666ms step_avg:57.81ms
step:1604/1845 train_time:92754ms step_avg:57.83ms
step:1605/1845 train_time:92840ms step_avg:57.84ms
step:1606/1845 train_time:92929ms step_avg:57.86ms
step:1607/1845 train_time:93015ms step_avg:57.88ms
step:1608/1845 train_time:93103ms step_avg:57.90ms
step:1609/1845 train_time:93191ms step_avg:57.92ms
step:1610/1845 train_time:93280ms step_avg:57.94ms
step:1611/1845 train_time:93367ms step_avg:57.96ms
step:1612/1845 train_time:93456ms step_avg:57.98ms
step:1613/1845 train_time:93543ms step_avg:57.99ms
step:1614/1845 train_time:93633ms step_avg:58.01ms
step:1615/1845 train_time:93719ms step_avg:58.03ms
step:1616/1845 train_time:93808ms step_avg:58.05ms
step:1617/1845 train_time:93895ms step_avg:58.07ms
step:1618/1845 train_time:93983ms step_avg:58.09ms
step:1619/1845 train_time:94072ms step_avg:58.11ms
step:1620/1845 train_time:94160ms step_avg:58.12ms
step:1621/1845 train_time:94247ms step_avg:58.14ms
step:1622/1845 train_time:94336ms step_avg:58.16ms
step:1623/1845 train_time:94422ms step_avg:58.18ms
step:1624/1845 train_time:94514ms step_avg:58.20ms
step:1625/1845 train_time:94600ms step_avg:58.22ms
step:1626/1845 train_time:94688ms step_avg:58.23ms
step:1627/1845 train_time:94775ms step_avg:58.25ms
step:1628/1845 train_time:94863ms step_avg:58.27ms
step:1629/1845 train_time:94949ms step_avg:58.29ms
step:1630/1845 train_time:95038ms step_avg:58.31ms
step:1631/1845 train_time:95125ms step_avg:58.32ms
step:1632/1845 train_time:95215ms step_avg:58.34ms
step:1633/1845 train_time:95302ms step_avg:58.36ms
step:1634/1845 train_time:95393ms step_avg:58.38ms
step:1635/1845 train_time:95479ms step_avg:58.40ms
step:1636/1845 train_time:95569ms step_avg:58.42ms
step:1637/1845 train_time:95655ms step_avg:58.43ms
step:1638/1845 train_time:95744ms step_avg:58.45ms
step:1639/1845 train_time:95831ms step_avg:58.47ms
step:1640/1845 train_time:95919ms step_avg:58.49ms
step:1641/1845 train_time:96005ms step_avg:58.50ms
step:1642/1845 train_time:96095ms step_avg:58.52ms
step:1643/1845 train_time:96181ms step_avg:58.54ms
step:1644/1845 train_time:96270ms step_avg:58.56ms
step:1645/1845 train_time:96357ms step_avg:58.58ms
step:1646/1845 train_time:96445ms step_avg:58.59ms
step:1647/1845 train_time:96532ms step_avg:58.61ms
step:1648/1845 train_time:96620ms step_avg:58.63ms
step:1649/1845 train_time:96706ms step_avg:58.65ms
step:1650/1845 train_time:96797ms step_avg:58.66ms
step:1651/1845 train_time:96882ms step_avg:58.68ms
step:1652/1845 train_time:96972ms step_avg:58.70ms
step:1653/1845 train_time:97059ms step_avg:58.72ms
step:1654/1845 train_time:97148ms step_avg:58.74ms
step:1655/1845 train_time:97236ms step_avg:58.75ms
step:1656/1845 train_time:97324ms step_avg:58.77ms
step:1657/1845 train_time:97411ms step_avg:58.79ms
step:1658/1845 train_time:97500ms step_avg:58.81ms
step:1659/1845 train_time:97586ms step_avg:58.82ms
step:1660/1845 train_time:97676ms step_avg:58.84ms
step:1661/1845 train_time:97763ms step_avg:58.86ms
step:1662/1845 train_time:97853ms step_avg:58.88ms
step:1663/1845 train_time:97940ms step_avg:58.89ms
step:1664/1845 train_time:98031ms step_avg:58.91ms
step:1665/1845 train_time:98118ms step_avg:58.93ms
step:1666/1845 train_time:98207ms step_avg:58.95ms
step:1667/1845 train_time:98294ms step_avg:58.96ms
step:1668/1845 train_time:98382ms step_avg:58.98ms
step:1669/1845 train_time:98468ms step_avg:59.00ms
step:1670/1845 train_time:98557ms step_avg:59.02ms
step:1671/1845 train_time:98643ms step_avg:59.03ms
step:1672/1845 train_time:98732ms step_avg:59.05ms
step:1673/1845 train_time:98818ms step_avg:59.07ms
step:1674/1845 train_time:98907ms step_avg:59.08ms
step:1675/1845 train_time:98994ms step_avg:59.10ms
step:1676/1845 train_time:99082ms step_avg:59.12ms
step:1677/1845 train_time:99170ms step_avg:59.14ms
step:1678/1845 train_time:99259ms step_avg:59.15ms
step:1679/1845 train_time:99345ms step_avg:59.17ms
step:1680/1845 train_time:99435ms step_avg:59.19ms
step:1681/1845 train_time:99521ms step_avg:59.20ms
step:1682/1845 train_time:99611ms step_avg:59.22ms
step:1683/1845 train_time:99698ms step_avg:59.24ms
step:1684/1845 train_time:99787ms step_avg:59.26ms
step:1685/1845 train_time:99872ms step_avg:59.27ms
step:1686/1845 train_time:99961ms step_avg:59.29ms
step:1687/1845 train_time:100047ms step_avg:59.30ms
step:1688/1845 train_time:100137ms step_avg:59.32ms
step:1689/1845 train_time:100223ms step_avg:59.34ms
step:1690/1845 train_time:100315ms step_avg:59.36ms
step:1691/1845 train_time:100401ms step_avg:59.37ms
step:1692/1845 train_time:100490ms step_avg:59.39ms
step:1693/1845 train_time:100576ms step_avg:59.41ms
step:1694/1845 train_time:100666ms step_avg:59.42ms
step:1695/1845 train_time:100752ms step_avg:59.44ms
step:1696/1845 train_time:100840ms step_avg:59.46ms
step:1697/1845 train_time:100927ms step_avg:59.47ms
step:1698/1845 train_time:101016ms step_avg:59.49ms
step:1699/1845 train_time:101102ms step_avg:59.51ms
step:1700/1845 train_time:101193ms step_avg:59.53ms
step:1701/1845 train_time:101278ms step_avg:59.54ms
step:1702/1845 train_time:101368ms step_avg:59.56ms
step:1703/1845 train_time:101454ms step_avg:59.57ms
step:1704/1845 train_time:101542ms step_avg:59.59ms
step:1705/1845 train_time:101629ms step_avg:59.61ms
step:1706/1845 train_time:101718ms step_avg:59.62ms
step:1707/1845 train_time:101805ms step_avg:59.64ms
step:1708/1845 train_time:101894ms step_avg:59.66ms
step:1709/1845 train_time:101979ms step_avg:59.67ms
step:1710/1845 train_time:102070ms step_avg:59.69ms
step:1711/1845 train_time:102156ms step_avg:59.71ms
step:1712/1845 train_time:102246ms step_avg:59.72ms
step:1713/1845 train_time:102333ms step_avg:59.74ms
step:1714/1845 train_time:102421ms step_avg:59.76ms
step:1715/1845 train_time:102508ms step_avg:59.77ms
step:1716/1845 train_time:102597ms step_avg:59.79ms
step:1717/1845 train_time:102684ms step_avg:59.80ms
step:1718/1845 train_time:102774ms step_avg:59.82ms
step:1719/1845 train_time:102860ms step_avg:59.84ms
step:1720/1845 train_time:102950ms step_avg:59.85ms
step:1721/1845 train_time:103037ms step_avg:59.87ms
step:1722/1845 train_time:103126ms step_avg:59.89ms
step:1723/1845 train_time:103212ms step_avg:59.90ms
step:1724/1845 train_time:103302ms step_avg:59.92ms
step:1725/1845 train_time:103388ms step_avg:59.93ms
step:1726/1845 train_time:103477ms step_avg:59.95ms
step:1727/1845 train_time:103563ms step_avg:59.97ms
step:1728/1845 train_time:103654ms step_avg:59.98ms
step:1729/1845 train_time:103740ms step_avg:60.00ms
step:1730/1845 train_time:103831ms step_avg:60.02ms
step:1731/1845 train_time:103917ms step_avg:60.03ms
step:1732/1845 train_time:104006ms step_avg:60.05ms
step:1733/1845 train_time:104093ms step_avg:60.07ms
step:1734/1845 train_time:104183ms step_avg:60.08ms
step:1735/1845 train_time:104271ms step_avg:60.10ms
step:1736/1845 train_time:104359ms step_avg:60.11ms
step:1737/1845 train_time:104444ms step_avg:60.13ms
step:1738/1845 train_time:104535ms step_avg:60.15ms
step:1739/1845 train_time:104621ms step_avg:60.16ms
step:1740/1845 train_time:104711ms step_avg:60.18ms
step:1741/1845 train_time:104797ms step_avg:60.19ms
step:1742/1845 train_time:104886ms step_avg:60.21ms
step:1743/1845 train_time:104973ms step_avg:60.23ms
step:1744/1845 train_time:105062ms step_avg:60.24ms
step:1745/1845 train_time:105148ms step_avg:60.26ms
step:1746/1845 train_time:105239ms step_avg:60.27ms
step:1747/1845 train_time:105326ms step_avg:60.29ms
step:1748/1845 train_time:105415ms step_avg:60.31ms
step:1749/1845 train_time:105500ms step_avg:60.32ms
step:1750/1845 train_time:105591ms step_avg:60.34ms
step:1750/1845 val_loss:3.3045 train_time:105687ms step_avg:60.39ms
step:1751/1845 train_time:105705ms step_avg:60.37ms
step:1752/1845 train_time:105766ms step_avg:60.37ms
step:1753/1845 train_time:105858ms step_avg:60.39ms
step:1754/1845 train_time:105949ms step_avg:60.40ms
step:1755/1845 train_time:106037ms step_avg:60.42ms
step:1756/1845 train_time:106125ms step_avg:60.44ms
step:1757/1845 train_time:106210ms step_avg:60.45ms
step:1758/1845 train_time:106297ms step_avg:60.46ms
step:1759/1845 train_time:106383ms step_avg:60.48ms
step:1760/1845 train_time:106473ms step_avg:60.50ms
step:1761/1845 train_time:106558ms step_avg:60.51ms
step:1762/1845 train_time:106649ms step_avg:60.53ms
step:1763/1845 train_time:106737ms step_avg:60.54ms
step:1764/1845 train_time:106828ms step_avg:60.56ms
step:1765/1845 train_time:106917ms step_avg:60.58ms
step:1766/1845 train_time:107006ms step_avg:60.59ms
step:1767/1845 train_time:107092ms step_avg:60.61ms
step:1768/1845 train_time:107181ms step_avg:60.62ms
step:1769/1845 train_time:107267ms step_avg:60.64ms
step:1770/1845 train_time:107356ms step_avg:60.65ms
step:1771/1845 train_time:107442ms step_avg:60.67ms
step:1772/1845 train_time:107531ms step_avg:60.68ms
step:1773/1845 train_time:107617ms step_avg:60.70ms
step:1774/1845 train_time:107708ms step_avg:60.71ms
step:1775/1845 train_time:107796ms step_avg:60.73ms
step:1776/1845 train_time:107885ms step_avg:60.75ms
step:1777/1845 train_time:107973ms step_avg:60.76ms
step:1778/1845 train_time:108062ms step_avg:60.78ms
step:1779/1845 train_time:108149ms step_avg:60.79ms
step:1780/1845 train_time:108237ms step_avg:60.81ms
step:1781/1845 train_time:108323ms step_avg:60.82ms
step:1782/1845 train_time:108413ms step_avg:60.84ms
step:1783/1845 train_time:108498ms step_avg:60.85ms
step:1784/1845 train_time:108588ms step_avg:60.87ms
step:1785/1845 train_time:108674ms step_avg:60.88ms
step:1786/1845 train_time:108765ms step_avg:60.90ms
step:1787/1845 train_time:108852ms step_avg:60.91ms
step:1788/1845 train_time:108941ms step_avg:60.93ms
step:1789/1845 train_time:109028ms step_avg:60.94ms
step:1790/1845 train_time:109117ms step_avg:60.96ms
step:1791/1845 train_time:109202ms step_avg:60.97ms
step:1792/1845 train_time:109291ms step_avg:60.99ms
step:1793/1845 train_time:109377ms step_avg:61.00ms
step:1794/1845 train_time:109466ms step_avg:61.02ms
step:1795/1845 train_time:109552ms step_avg:61.03ms
step:1796/1845 train_time:109641ms step_avg:61.05ms
step:1797/1845 train_time:109729ms step_avg:61.06ms
step:1798/1845 train_time:109817ms step_avg:61.08ms
step:1799/1845 train_time:109904ms step_avg:61.09ms
step:1800/1845 train_time:109994ms step_avg:61.11ms
step:1801/1845 train_time:110082ms step_avg:61.12ms
step:1802/1845 train_time:110171ms step_avg:61.14ms
step:1803/1845 train_time:110257ms step_avg:61.15ms
step:1804/1845 train_time:110345ms step_avg:61.17ms
step:1805/1845 train_time:110431ms step_avg:61.18ms
step:1806/1845 train_time:110521ms step_avg:61.20ms
step:1807/1845 train_time:110607ms step_avg:61.21ms
step:1808/1845 train_time:110696ms step_avg:61.23ms
step:1809/1845 train_time:110783ms step_avg:61.24ms
step:1810/1845 train_time:110873ms step_avg:61.26ms
step:1811/1845 train_time:110960ms step_avg:61.27ms
step:1812/1845 train_time:111050ms step_avg:61.29ms
step:1813/1845 train_time:111137ms step_avg:61.30ms
step:1814/1845 train_time:111227ms step_avg:61.32ms
step:1815/1845 train_time:111314ms step_avg:61.33ms
step:1816/1845 train_time:111403ms step_avg:61.35ms
step:1817/1845 train_time:111489ms step_avg:61.36ms
step:1818/1845 train_time:111578ms step_avg:61.37ms
step:1819/1845 train_time:111664ms step_avg:61.39ms
step:1820/1845 train_time:111756ms step_avg:61.40ms
step:1821/1845 train_time:111842ms step_avg:61.42ms
step:1822/1845 train_time:111932ms step_avg:61.43ms
step:1823/1845 train_time:112018ms step_avg:61.45ms
step:1824/1845 train_time:112109ms step_avg:61.46ms
step:1825/1845 train_time:112195ms step_avg:61.48ms
step:1826/1845 train_time:112284ms step_avg:61.49ms
step:1827/1845 train_time:112370ms step_avg:61.51ms
step:1828/1845 train_time:112459ms step_avg:61.52ms
step:1829/1845 train_time:112546ms step_avg:61.53ms
step:1830/1845 train_time:112637ms step_avg:61.55ms
step:1831/1845 train_time:112722ms step_avg:61.56ms
step:1832/1845 train_time:112813ms step_avg:61.58ms
step:1833/1845 train_time:112900ms step_avg:61.59ms
step:1834/1845 train_time:112991ms step_avg:61.61ms
step:1835/1845 train_time:113078ms step_avg:61.62ms
step:1836/1845 train_time:113168ms step_avg:61.64ms
step:1837/1845 train_time:113254ms step_avg:61.65ms
step:1838/1845 train_time:113342ms step_avg:61.67ms
step:1839/1845 train_time:113429ms step_avg:61.68ms
step:1840/1845 train_time:113518ms step_avg:61.69ms
step:1841/1845 train_time:113606ms step_avg:61.71ms
step:1842/1845 train_time:113695ms step_avg:61.72ms
step:1843/1845 train_time:113783ms step_avg:61.74ms
step:1844/1845 train_time:113871ms step_avg:61.75ms
step:1845/1845 train_time:113958ms step_avg:61.77ms
step:1845/1845 val_loss:3.2783 train_time:114055ms step_avg:61.82ms
peak memory allocated: 29524 MiB reserved: 45518 MiB

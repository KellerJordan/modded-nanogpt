import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 18:17:54 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    197467      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    197468      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    197469      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    197470      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    197471      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    197472      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    197473      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    197474      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8299 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:79ms step_avg:79.17ms
step:2/1845 train_time:103ms step_avg:51.41ms
step:3/1845 train_time:130ms step_avg:43.18ms
step:4/1845 train_time:164ms step_avg:40.94ms
step:5/1845 train_time:198ms step_avg:39.56ms
step:6/1845 train_time:261ms step_avg:43.56ms
step:7/1845 train_time:314ms step_avg:44.87ms
step:8/1845 train_time:348ms step_avg:43.52ms
step:9/1845 train_time:382ms step_avg:42.47ms
step:10/1845 train_time:416ms step_avg:41.64ms
step:11/1845 train_time:451ms step_avg:40.96ms
step:12/1845 train_time:485ms step_avg:40.40ms
step:13/1845 train_time:519ms step_avg:39.92ms
step:14/1845 train_time:553ms step_avg:39.54ms
step:15/1845 train_time:588ms step_avg:39.18ms
step:16/1845 train_time:622ms step_avg:38.89ms
step:17/1845 train_time:656ms step_avg:38.61ms
step:18/1845 train_time:691ms step_avg:38.38ms
step:19/1845 train_time:725ms step_avg:38.16ms
step:20/1845 train_time:760ms step_avg:37.98ms
step:21/1845 train_time:794ms step_avg:37.80ms
step:22/1845 train_time:828ms step_avg:37.66ms
step:23/1845 train_time:863ms step_avg:37.50ms
step:24/1845 train_time:897ms step_avg:37.38ms
step:25/1845 train_time:932ms step_avg:37.26ms
step:26/1845 train_time:966ms step_avg:37.15ms
step:27/1845 train_time:1000ms step_avg:37.04ms
step:28/1845 train_time:1035ms step_avg:36.95ms
step:29/1845 train_time:1069ms step_avg:36.86ms
step:30/1845 train_time:1103ms step_avg:36.78ms
step:31/1845 train_time:1138ms step_avg:36.69ms
step:32/1845 train_time:1173ms step_avg:36.64ms
step:33/1845 train_time:1206ms step_avg:36.55ms
step:34/1845 train_time:1241ms step_avg:36.49ms
step:35/1845 train_time:1275ms step_avg:36.44ms
step:36/1845 train_time:1310ms step_avg:36.39ms
step:37/1845 train_time:1344ms step_avg:36.33ms
step:38/1845 train_time:1379ms step_avg:36.29ms
step:39/1845 train_time:1413ms step_avg:36.23ms
step:40/1845 train_time:1447ms step_avg:36.19ms
step:41/1845 train_time:1482ms step_avg:36.15ms
step:42/1845 train_time:1516ms step_avg:36.11ms
step:43/1845 train_time:1551ms step_avg:36.06ms
step:44/1845 train_time:1585ms step_avg:36.03ms
step:45/1845 train_time:1619ms step_avg:35.99ms
step:46/1845 train_time:1654ms step_avg:35.96ms
step:47/1845 train_time:1688ms step_avg:35.92ms
step:48/1845 train_time:1723ms step_avg:35.89ms
step:49/1845 train_time:1757ms step_avg:35.85ms
step:50/1845 train_time:1791ms step_avg:35.83ms
step:51/1845 train_time:1825ms step_avg:35.79ms
step:52/1845 train_time:1860ms step_avg:35.76ms
step:53/1845 train_time:1894ms step_avg:35.73ms
step:54/1845 train_time:1928ms step_avg:35.71ms
step:55/1845 train_time:1962ms step_avg:35.68ms
step:56/1845 train_time:1997ms step_avg:35.66ms
step:57/1845 train_time:2033ms step_avg:35.66ms
step:58/1845 train_time:2066ms step_avg:35.61ms
step:59/1845 train_time:2100ms step_avg:35.59ms
step:60/1845 train_time:2134ms step_avg:35.57ms
step:61/1845 train_time:2168ms step_avg:35.55ms
step:62/1845 train_time:2203ms step_avg:35.53ms
step:63/1845 train_time:2237ms step_avg:35.51ms
step:64/1845 train_time:2272ms step_avg:35.49ms
step:65/1845 train_time:2306ms step_avg:35.47ms
step:66/1845 train_time:2340ms step_avg:35.46ms
step:67/1845 train_time:2375ms step_avg:35.44ms
step:68/1845 train_time:2409ms step_avg:35.43ms
step:69/1845 train_time:2443ms step_avg:35.41ms
step:70/1845 train_time:2478ms step_avg:35.40ms
step:71/1845 train_time:2512ms step_avg:35.38ms
step:72/1845 train_time:2546ms step_avg:35.37ms
step:73/1845 train_time:2581ms step_avg:35.35ms
step:74/1845 train_time:2615ms step_avg:35.34ms
step:75/1845 train_time:2649ms step_avg:35.32ms
step:76/1845 train_time:2683ms step_avg:35.31ms
step:77/1845 train_time:2718ms step_avg:35.30ms
step:78/1845 train_time:2752ms step_avg:35.28ms
step:79/1845 train_time:2786ms step_avg:35.27ms
step:80/1845 train_time:2821ms step_avg:35.26ms
step:81/1845 train_time:2855ms step_avg:35.25ms
step:82/1845 train_time:2890ms step_avg:35.24ms
step:83/1845 train_time:2924ms step_avg:35.22ms
step:84/1845 train_time:2958ms step_avg:35.21ms
step:85/1845 train_time:2992ms step_avg:35.20ms
step:86/1845 train_time:3026ms step_avg:35.19ms
step:87/1845 train_time:3061ms step_avg:35.18ms
step:88/1845 train_time:3095ms step_avg:35.17ms
step:89/1845 train_time:3129ms step_avg:35.16ms
step:90/1845 train_time:3164ms step_avg:35.15ms
step:91/1845 train_time:3198ms step_avg:35.14ms
step:92/1845 train_time:3232ms step_avg:35.13ms
step:93/1845 train_time:3266ms step_avg:35.12ms
step:94/1845 train_time:3301ms step_avg:35.11ms
step:95/1845 train_time:3335ms step_avg:35.10ms
step:96/1845 train_time:3369ms step_avg:35.10ms
step:97/1845 train_time:3403ms step_avg:35.09ms
step:98/1845 train_time:3438ms step_avg:35.08ms
step:99/1845 train_time:3472ms step_avg:35.07ms
step:100/1845 train_time:3506ms step_avg:35.06ms
step:101/1845 train_time:3540ms step_avg:35.05ms
step:102/1845 train_time:3575ms step_avg:35.05ms
step:103/1845 train_time:3609ms step_avg:35.04ms
step:104/1845 train_time:3644ms step_avg:35.03ms
step:105/1845 train_time:3678ms step_avg:35.03ms
step:106/1845 train_time:3712ms step_avg:35.02ms
step:107/1845 train_time:3746ms step_avg:35.01ms
step:108/1845 train_time:3781ms step_avg:35.01ms
step:109/1845 train_time:3815ms step_avg:35.00ms
step:110/1845 train_time:3849ms step_avg:34.99ms
step:111/1845 train_time:3884ms step_avg:34.99ms
step:112/1845 train_time:3918ms step_avg:34.98ms
step:113/1845 train_time:3952ms step_avg:34.97ms
step:114/1845 train_time:3986ms step_avg:34.97ms
step:115/1845 train_time:4021ms step_avg:34.96ms
step:116/1845 train_time:4055ms step_avg:34.96ms
step:117/1845 train_time:4089ms step_avg:34.95ms
step:118/1845 train_time:4124ms step_avg:34.95ms
step:119/1845 train_time:4158ms step_avg:34.94ms
step:120/1845 train_time:4192ms step_avg:34.94ms
step:121/1845 train_time:4227ms step_avg:34.93ms
step:122/1845 train_time:4261ms step_avg:34.93ms
step:123/1845 train_time:4295ms step_avg:34.92ms
step:124/1845 train_time:4329ms step_avg:34.91ms
step:125/1845 train_time:4363ms step_avg:34.91ms
step:126/1845 train_time:4398ms step_avg:34.90ms
step:127/1845 train_time:4432ms step_avg:34.90ms
step:128/1845 train_time:4466ms step_avg:34.89ms
step:129/1845 train_time:4500ms step_avg:34.89ms
step:130/1845 train_time:4535ms step_avg:34.88ms
step:131/1845 train_time:4569ms step_avg:34.88ms
step:132/1845 train_time:4603ms step_avg:34.87ms
step:133/1845 train_time:4638ms step_avg:34.87ms
step:134/1845 train_time:4672ms step_avg:34.87ms
step:135/1845 train_time:4706ms step_avg:34.86ms
step:136/1845 train_time:4741ms step_avg:34.86ms
step:137/1845 train_time:4775ms step_avg:34.85ms
step:138/1845 train_time:4809ms step_avg:34.85ms
step:139/1845 train_time:4843ms step_avg:34.84ms
step:140/1845 train_time:4878ms step_avg:34.84ms
step:141/1845 train_time:4912ms step_avg:34.84ms
step:142/1845 train_time:4946ms step_avg:34.83ms
step:143/1845 train_time:4980ms step_avg:34.83ms
step:144/1845 train_time:5015ms step_avg:34.82ms
step:145/1845 train_time:5049ms step_avg:34.82ms
step:146/1845 train_time:5083ms step_avg:34.82ms
step:147/1845 train_time:5117ms step_avg:34.81ms
step:148/1845 train_time:5152ms step_avg:34.81ms
step:149/1845 train_time:5186ms step_avg:34.81ms
step:150/1845 train_time:5221ms step_avg:34.80ms
step:151/1845 train_time:5255ms step_avg:34.80ms
step:152/1845 train_time:5289ms step_avg:34.79ms
step:153/1845 train_time:5323ms step_avg:34.79ms
step:154/1845 train_time:5357ms step_avg:34.79ms
step:155/1845 train_time:5391ms step_avg:34.78ms
step:156/1845 train_time:5426ms step_avg:34.78ms
step:157/1845 train_time:5460ms step_avg:34.77ms
step:158/1845 train_time:5494ms step_avg:34.77ms
step:159/1845 train_time:5528ms step_avg:34.77ms
step:160/1845 train_time:5562ms step_avg:34.76ms
step:161/1845 train_time:5596ms step_avg:34.76ms
step:162/1845 train_time:5631ms step_avg:34.76ms
step:163/1845 train_time:5665ms step_avg:34.75ms
step:164/1845 train_time:5699ms step_avg:34.75ms
step:165/1845 train_time:5733ms step_avg:34.75ms
step:166/1845 train_time:5768ms step_avg:34.75ms
step:167/1845 train_time:5802ms step_avg:34.74ms
step:168/1845 train_time:5836ms step_avg:34.74ms
step:169/1845 train_time:5870ms step_avg:34.74ms
step:170/1845 train_time:5905ms step_avg:34.73ms
step:171/1845 train_time:5938ms step_avg:34.73ms
step:172/1845 train_time:5973ms step_avg:34.73ms
step:173/1845 train_time:6007ms step_avg:34.72ms
step:174/1845 train_time:6042ms step_avg:34.73ms
step:175/1845 train_time:6076ms step_avg:34.72ms
step:176/1845 train_time:6111ms step_avg:34.72ms
step:177/1845 train_time:6145ms step_avg:34.72ms
step:178/1845 train_time:6179ms step_avg:34.71ms
step:179/1845 train_time:6213ms step_avg:34.71ms
step:180/1845 train_time:6248ms step_avg:34.71ms
step:181/1845 train_time:6282ms step_avg:34.71ms
step:182/1845 train_time:6316ms step_avg:34.70ms
step:183/1845 train_time:6350ms step_avg:34.70ms
step:184/1845 train_time:6384ms step_avg:34.70ms
step:185/1845 train_time:6418ms step_avg:34.69ms
step:186/1845 train_time:6453ms step_avg:34.69ms
step:187/1845 train_time:6487ms step_avg:34.69ms
step:188/1845 train_time:6521ms step_avg:34.69ms
step:189/1845 train_time:6556ms step_avg:34.69ms
step:190/1845 train_time:6590ms step_avg:34.68ms
step:191/1845 train_time:6624ms step_avg:34.68ms
step:192/1845 train_time:6658ms step_avg:34.68ms
step:193/1845 train_time:6693ms step_avg:34.68ms
step:194/1845 train_time:6727ms step_avg:34.67ms
step:195/1845 train_time:6761ms step_avg:34.67ms
step:196/1845 train_time:6796ms step_avg:34.67ms
step:197/1845 train_time:6830ms step_avg:34.67ms
step:198/1845 train_time:6864ms step_avg:34.67ms
step:199/1845 train_time:6898ms step_avg:34.66ms
step:200/1845 train_time:6933ms step_avg:34.66ms
step:201/1845 train_time:6966ms step_avg:34.66ms
step:202/1845 train_time:7000ms step_avg:34.66ms
step:203/1845 train_time:7034ms step_avg:34.65ms
step:204/1845 train_time:7069ms step_avg:34.65ms
step:205/1845 train_time:7103ms step_avg:34.65ms
step:206/1845 train_time:7137ms step_avg:34.65ms
step:207/1845 train_time:7171ms step_avg:34.64ms
step:208/1845 train_time:7205ms step_avg:34.64ms
step:209/1845 train_time:7239ms step_avg:34.64ms
step:210/1845 train_time:7274ms step_avg:34.64ms
step:211/1845 train_time:7308ms step_avg:34.64ms
step:212/1845 train_time:7342ms step_avg:34.63ms
step:213/1845 train_time:7376ms step_avg:34.63ms
step:214/1845 train_time:7411ms step_avg:34.63ms
step:215/1845 train_time:7445ms step_avg:34.63ms
step:216/1845 train_time:7479ms step_avg:34.63ms
step:217/1845 train_time:7513ms step_avg:34.62ms
step:218/1845 train_time:7547ms step_avg:34.62ms
step:219/1845 train_time:7582ms step_avg:34.62ms
step:220/1845 train_time:7616ms step_avg:34.62ms
step:221/1845 train_time:7650ms step_avg:34.62ms
step:222/1845 train_time:7684ms step_avg:34.61ms
step:223/1845 train_time:7718ms step_avg:34.61ms
step:224/1845 train_time:7753ms step_avg:34.61ms
step:225/1845 train_time:7787ms step_avg:34.61ms
step:226/1845 train_time:7821ms step_avg:34.61ms
step:227/1845 train_time:7855ms step_avg:34.61ms
step:228/1845 train_time:7890ms step_avg:34.60ms
step:229/1845 train_time:7924ms step_avg:34.60ms
step:230/1845 train_time:7958ms step_avg:34.60ms
step:231/1845 train_time:7992ms step_avg:34.60ms
step:232/1845 train_time:8026ms step_avg:34.60ms
step:233/1845 train_time:8060ms step_avg:34.59ms
step:234/1845 train_time:8095ms step_avg:34.59ms
step:235/1845 train_time:8129ms step_avg:34.59ms
step:236/1845 train_time:8163ms step_avg:34.59ms
step:237/1845 train_time:8197ms step_avg:34.59ms
step:238/1845 train_time:8231ms step_avg:34.59ms
step:239/1845 train_time:8265ms step_avg:34.58ms
step:240/1845 train_time:8300ms step_avg:34.58ms
step:241/1845 train_time:8334ms step_avg:34.58ms
step:242/1845 train_time:8368ms step_avg:34.58ms
step:243/1845 train_time:8402ms step_avg:34.58ms
step:244/1845 train_time:8436ms step_avg:34.58ms
step:245/1845 train_time:8471ms step_avg:34.57ms
step:246/1845 train_time:8505ms step_avg:34.57ms
step:247/1845 train_time:8539ms step_avg:34.57ms
step:248/1845 train_time:8573ms step_avg:34.57ms
step:249/1845 train_time:8607ms step_avg:34.57ms
step:250/1845 train_time:8642ms step_avg:34.57ms
step:250/1845 val_loss:4.6073 train_time:8678ms step_avg:34.71ms
step:251/1845 train_time:8699ms step_avg:34.66ms
step:252/1845 train_time:8718ms step_avg:34.59ms
step:253/1845 train_time:8748ms step_avg:34.58ms
step:254/1845 train_time:8783ms step_avg:34.58ms
step:255/1845 train_time:8818ms step_avg:34.58ms
step:256/1845 train_time:8853ms step_avg:34.58ms
step:257/1845 train_time:8888ms step_avg:34.58ms
step:258/1845 train_time:8922ms step_avg:34.58ms
step:259/1845 train_time:8957ms step_avg:34.58ms
step:260/1845 train_time:8991ms step_avg:34.58ms
step:261/1845 train_time:9025ms step_avg:34.58ms
step:262/1845 train_time:9060ms step_avg:34.58ms
step:263/1845 train_time:9094ms step_avg:34.58ms
step:264/1845 train_time:9128ms step_avg:34.58ms
step:265/1845 train_time:9162ms step_avg:34.57ms
step:266/1845 train_time:9196ms step_avg:34.57ms
step:267/1845 train_time:9230ms step_avg:34.57ms
step:268/1845 train_time:9265ms step_avg:34.57ms
step:269/1845 train_time:9299ms step_avg:34.57ms
step:270/1845 train_time:9333ms step_avg:34.57ms
step:271/1845 train_time:9367ms step_avg:34.57ms
step:272/1845 train_time:9403ms step_avg:34.57ms
step:273/1845 train_time:9435ms step_avg:34.56ms
step:274/1845 train_time:9470ms step_avg:34.56ms
step:275/1845 train_time:9504ms step_avg:34.56ms
step:276/1845 train_time:9538ms step_avg:34.56ms
step:277/1845 train_time:9572ms step_avg:34.56ms
step:278/1845 train_time:9606ms step_avg:34.56ms
step:279/1845 train_time:9640ms step_avg:34.55ms
step:280/1845 train_time:9675ms step_avg:34.55ms
step:281/1845 train_time:9709ms step_avg:34.55ms
step:282/1845 train_time:9743ms step_avg:34.55ms
step:283/1845 train_time:9777ms step_avg:34.55ms
step:284/1845 train_time:9812ms step_avg:34.55ms
step:285/1845 train_time:9846ms step_avg:34.55ms
step:286/1845 train_time:9880ms step_avg:34.55ms
step:287/1845 train_time:9915ms step_avg:34.55ms
step:288/1845 train_time:9949ms step_avg:34.55ms
step:289/1845 train_time:9984ms step_avg:34.55ms
step:290/1845 train_time:10018ms step_avg:34.54ms
step:291/1845 train_time:10052ms step_avg:34.54ms
step:292/1845 train_time:10086ms step_avg:34.54ms
step:293/1845 train_time:10121ms step_avg:34.54ms
step:294/1845 train_time:10155ms step_avg:34.54ms
step:295/1845 train_time:10189ms step_avg:34.54ms
step:296/1845 train_time:10223ms step_avg:34.54ms
step:297/1845 train_time:10257ms step_avg:34.54ms
step:298/1845 train_time:10292ms step_avg:34.54ms
step:299/1845 train_time:10326ms step_avg:34.53ms
step:300/1845 train_time:10360ms step_avg:34.53ms
step:301/1845 train_time:10394ms step_avg:34.53ms
step:302/1845 train_time:10428ms step_avg:34.53ms
step:303/1845 train_time:10462ms step_avg:34.53ms
step:304/1845 train_time:10496ms step_avg:34.53ms
step:305/1845 train_time:10530ms step_avg:34.53ms
step:306/1845 train_time:10564ms step_avg:34.52ms
step:307/1845 train_time:10599ms step_avg:34.52ms
step:308/1845 train_time:10633ms step_avg:34.52ms
step:309/1845 train_time:10667ms step_avg:34.52ms
step:310/1845 train_time:10701ms step_avg:34.52ms
step:311/1845 train_time:10735ms step_avg:34.52ms
step:312/1845 train_time:10769ms step_avg:34.52ms
step:313/1845 train_time:10803ms step_avg:34.52ms
step:314/1845 train_time:10838ms step_avg:34.52ms
step:315/1845 train_time:10872ms step_avg:34.51ms
step:316/1845 train_time:10906ms step_avg:34.51ms
step:317/1845 train_time:10940ms step_avg:34.51ms
step:318/1845 train_time:10974ms step_avg:34.51ms
step:319/1845 train_time:11008ms step_avg:34.51ms
step:320/1845 train_time:11042ms step_avg:34.51ms
step:321/1845 train_time:11077ms step_avg:34.51ms
step:322/1845 train_time:11111ms step_avg:34.50ms
step:323/1845 train_time:11144ms step_avg:34.50ms
step:324/1845 train_time:11179ms step_avg:34.50ms
step:325/1845 train_time:11213ms step_avg:34.50ms
step:326/1845 train_time:11247ms step_avg:34.50ms
step:327/1845 train_time:11281ms step_avg:34.50ms
step:328/1845 train_time:11316ms step_avg:34.50ms
step:329/1845 train_time:11350ms step_avg:34.50ms
step:330/1845 train_time:11384ms step_avg:34.50ms
step:331/1845 train_time:11418ms step_avg:34.50ms
step:332/1845 train_time:11452ms step_avg:34.49ms
step:333/1845 train_time:11486ms step_avg:34.49ms
step:334/1845 train_time:11520ms step_avg:34.49ms
step:335/1845 train_time:11554ms step_avg:34.49ms
step:336/1845 train_time:11589ms step_avg:34.49ms
step:337/1845 train_time:11623ms step_avg:34.49ms
step:338/1845 train_time:11657ms step_avg:34.49ms
step:339/1845 train_time:11691ms step_avg:34.49ms
step:340/1845 train_time:11725ms step_avg:34.49ms
step:341/1845 train_time:11759ms step_avg:34.48ms
step:342/1845 train_time:11794ms step_avg:34.48ms
step:343/1845 train_time:11828ms step_avg:34.48ms
step:344/1845 train_time:11862ms step_avg:34.48ms
step:345/1845 train_time:11896ms step_avg:34.48ms
step:346/1845 train_time:11931ms step_avg:34.48ms
step:347/1845 train_time:11965ms step_avg:34.48ms
step:348/1845 train_time:11999ms step_avg:34.48ms
step:349/1845 train_time:12033ms step_avg:34.48ms
step:350/1845 train_time:12067ms step_avg:34.48ms
step:351/1845 train_time:12101ms step_avg:34.48ms
step:352/1845 train_time:12135ms step_avg:34.48ms
step:353/1845 train_time:12169ms step_avg:34.47ms
step:354/1845 train_time:12204ms step_avg:34.47ms
step:355/1845 train_time:12238ms step_avg:34.47ms
step:356/1845 train_time:12272ms step_avg:34.47ms
step:357/1845 train_time:12306ms step_avg:34.47ms
step:358/1845 train_time:12340ms step_avg:34.47ms
step:359/1845 train_time:12374ms step_avg:34.47ms
step:360/1845 train_time:12409ms step_avg:34.47ms
step:361/1845 train_time:12443ms step_avg:34.47ms
step:362/1845 train_time:12477ms step_avg:34.47ms
step:363/1845 train_time:12511ms step_avg:34.47ms
step:364/1845 train_time:12545ms step_avg:34.47ms
step:365/1845 train_time:12579ms step_avg:34.46ms
step:366/1845 train_time:12614ms step_avg:34.46ms
step:367/1845 train_time:12648ms step_avg:34.46ms
step:368/1845 train_time:12682ms step_avg:34.46ms
step:369/1845 train_time:12716ms step_avg:34.46ms
step:370/1845 train_time:12750ms step_avg:34.46ms
step:371/1845 train_time:12785ms step_avg:34.46ms
step:372/1845 train_time:12819ms step_avg:34.46ms
step:373/1845 train_time:12853ms step_avg:34.46ms
step:374/1845 train_time:12887ms step_avg:34.46ms
step:375/1845 train_time:12921ms step_avg:34.46ms
step:376/1845 train_time:12956ms step_avg:34.46ms
step:377/1845 train_time:12989ms step_avg:34.45ms
step:378/1845 train_time:13024ms step_avg:34.45ms
step:379/1845 train_time:13058ms step_avg:34.45ms
step:380/1845 train_time:13092ms step_avg:34.45ms
step:381/1845 train_time:13126ms step_avg:34.45ms
step:382/1845 train_time:13161ms step_avg:34.45ms
step:383/1845 train_time:13194ms step_avg:34.45ms
step:384/1845 train_time:13229ms step_avg:34.45ms
step:385/1845 train_time:13263ms step_avg:34.45ms
step:386/1845 train_time:13297ms step_avg:34.45ms
step:387/1845 train_time:13331ms step_avg:34.45ms
step:388/1845 train_time:13365ms step_avg:34.45ms
step:389/1845 train_time:13400ms step_avg:34.45ms
step:390/1845 train_time:13434ms step_avg:34.45ms
step:391/1845 train_time:13468ms step_avg:34.45ms
step:392/1845 train_time:13502ms step_avg:34.44ms
step:393/1845 train_time:13537ms step_avg:34.44ms
step:394/1845 train_time:13571ms step_avg:34.44ms
step:395/1845 train_time:13605ms step_avg:34.44ms
step:396/1845 train_time:13639ms step_avg:34.44ms
step:397/1845 train_time:13673ms step_avg:34.44ms
step:398/1845 train_time:13707ms step_avg:34.44ms
step:399/1845 train_time:13742ms step_avg:34.44ms
step:400/1845 train_time:13776ms step_avg:34.44ms
step:401/1845 train_time:13810ms step_avg:34.44ms
step:402/1845 train_time:13844ms step_avg:34.44ms
step:403/1845 train_time:13878ms step_avg:34.44ms
step:404/1845 train_time:13913ms step_avg:34.44ms
step:405/1845 train_time:13947ms step_avg:34.44ms
step:406/1845 train_time:13981ms step_avg:34.44ms
step:407/1845 train_time:14015ms step_avg:34.44ms
step:408/1845 train_time:14049ms step_avg:34.43ms
step:409/1845 train_time:14083ms step_avg:34.43ms
step:410/1845 train_time:14118ms step_avg:34.43ms
step:411/1845 train_time:14152ms step_avg:34.43ms
step:412/1845 train_time:14186ms step_avg:34.43ms
step:413/1845 train_time:14220ms step_avg:34.43ms
step:414/1845 train_time:14254ms step_avg:34.43ms
step:415/1845 train_time:14288ms step_avg:34.43ms
step:416/1845 train_time:14322ms step_avg:34.43ms
step:417/1845 train_time:14356ms step_avg:34.43ms
step:418/1845 train_time:14391ms step_avg:34.43ms
step:419/1845 train_time:14425ms step_avg:34.43ms
step:420/1845 train_time:14459ms step_avg:34.43ms
step:421/1845 train_time:14493ms step_avg:34.42ms
step:422/1845 train_time:14527ms step_avg:34.42ms
step:423/1845 train_time:14561ms step_avg:34.42ms
step:424/1845 train_time:14595ms step_avg:34.42ms
step:425/1845 train_time:14629ms step_avg:34.42ms
step:426/1845 train_time:14663ms step_avg:34.42ms
step:427/1845 train_time:14697ms step_avg:34.42ms
step:428/1845 train_time:14732ms step_avg:34.42ms
step:429/1845 train_time:14766ms step_avg:34.42ms
step:430/1845 train_time:14800ms step_avg:34.42ms
step:431/1845 train_time:14834ms step_avg:34.42ms
step:432/1845 train_time:14868ms step_avg:34.42ms
step:433/1845 train_time:14902ms step_avg:34.42ms
step:434/1845 train_time:14937ms step_avg:34.42ms
step:435/1845 train_time:14971ms step_avg:34.42ms
step:436/1845 train_time:15005ms step_avg:34.41ms
step:437/1845 train_time:15039ms step_avg:34.41ms
step:438/1845 train_time:15073ms step_avg:34.41ms
step:439/1845 train_time:15108ms step_avg:34.41ms
step:440/1845 train_time:15142ms step_avg:34.41ms
step:441/1845 train_time:15176ms step_avg:34.41ms
step:442/1845 train_time:15210ms step_avg:34.41ms
step:443/1845 train_time:15244ms step_avg:34.41ms
step:444/1845 train_time:15278ms step_avg:34.41ms
step:445/1845 train_time:15312ms step_avg:34.41ms
step:446/1845 train_time:15347ms step_avg:34.41ms
step:447/1845 train_time:15381ms step_avg:34.41ms
step:448/1845 train_time:15415ms step_avg:34.41ms
step:449/1845 train_time:15449ms step_avg:34.41ms
step:450/1845 train_time:15483ms step_avg:34.41ms
step:451/1845 train_time:15517ms step_avg:34.41ms
step:452/1845 train_time:15551ms step_avg:34.41ms
step:453/1845 train_time:15586ms step_avg:34.41ms
step:454/1845 train_time:15620ms step_avg:34.41ms
step:455/1845 train_time:15654ms step_avg:34.40ms
step:456/1845 train_time:15688ms step_avg:34.40ms
step:457/1845 train_time:15722ms step_avg:34.40ms
step:458/1845 train_time:15757ms step_avg:34.40ms
step:459/1845 train_time:15791ms step_avg:34.40ms
step:460/1845 train_time:15825ms step_avg:34.40ms
step:461/1845 train_time:15859ms step_avg:34.40ms
step:462/1845 train_time:15893ms step_avg:34.40ms
step:463/1845 train_time:15927ms step_avg:34.40ms
step:464/1845 train_time:15961ms step_avg:34.40ms
step:465/1845 train_time:15995ms step_avg:34.40ms
step:466/1845 train_time:16030ms step_avg:34.40ms
step:467/1845 train_time:16064ms step_avg:34.40ms
step:468/1845 train_time:16098ms step_avg:34.40ms
step:469/1845 train_time:16132ms step_avg:34.40ms
step:470/1845 train_time:16166ms step_avg:34.40ms
step:471/1845 train_time:16200ms step_avg:34.40ms
step:472/1845 train_time:16234ms step_avg:34.39ms
step:473/1845 train_time:16268ms step_avg:34.39ms
step:474/1845 train_time:16303ms step_avg:34.39ms
step:475/1845 train_time:16337ms step_avg:34.39ms
step:476/1845 train_time:16371ms step_avg:34.39ms
step:477/1845 train_time:16405ms step_avg:34.39ms
step:478/1845 train_time:16440ms step_avg:34.39ms
step:479/1845 train_time:16474ms step_avg:34.39ms
step:480/1845 train_time:16508ms step_avg:34.39ms
step:481/1845 train_time:16542ms step_avg:34.39ms
step:482/1845 train_time:16577ms step_avg:34.39ms
step:483/1845 train_time:16611ms step_avg:34.39ms
step:484/1845 train_time:16645ms step_avg:34.39ms
step:485/1845 train_time:16679ms step_avg:34.39ms
step:486/1845 train_time:16714ms step_avg:34.39ms
step:487/1845 train_time:16748ms step_avg:34.39ms
step:488/1845 train_time:16782ms step_avg:34.39ms
step:489/1845 train_time:16817ms step_avg:34.39ms
step:490/1845 train_time:16851ms step_avg:34.39ms
step:491/1845 train_time:16885ms step_avg:34.39ms
step:492/1845 train_time:16919ms step_avg:34.39ms
step:493/1845 train_time:16953ms step_avg:34.39ms
step:494/1845 train_time:16987ms step_avg:34.39ms
step:495/1845 train_time:17022ms step_avg:34.39ms
step:496/1845 train_time:17056ms step_avg:34.39ms
step:497/1845 train_time:17090ms step_avg:34.39ms
step:498/1845 train_time:17124ms step_avg:34.39ms
step:499/1845 train_time:17158ms step_avg:34.38ms
step:500/1845 train_time:17192ms step_avg:34.38ms
step:500/1845 val_loss:4.2990 train_time:17228ms step_avg:34.46ms
step:501/1845 train_time:17250ms step_avg:34.43ms
step:502/1845 train_time:17269ms step_avg:34.40ms
step:503/1845 train_time:17297ms step_avg:34.39ms
step:504/1845 train_time:17333ms step_avg:34.39ms
step:505/1845 train_time:17369ms step_avg:34.39ms
step:506/1845 train_time:17404ms step_avg:34.40ms
step:507/1845 train_time:17439ms step_avg:34.40ms
step:508/1845 train_time:17474ms step_avg:34.40ms
step:509/1845 train_time:17508ms step_avg:34.40ms
step:510/1845 train_time:17542ms step_avg:34.40ms
step:511/1845 train_time:17576ms step_avg:34.40ms
step:512/1845 train_time:17610ms step_avg:34.40ms
step:513/1845 train_time:17644ms step_avg:34.39ms
step:514/1845 train_time:17679ms step_avg:34.39ms
step:515/1845 train_time:17713ms step_avg:34.39ms
step:516/1845 train_time:17747ms step_avg:34.39ms
step:517/1845 train_time:17781ms step_avg:34.39ms
step:518/1845 train_time:17815ms step_avg:34.39ms
step:519/1845 train_time:17849ms step_avg:34.39ms
step:520/1845 train_time:17883ms step_avg:34.39ms
step:521/1845 train_time:17917ms step_avg:34.39ms
step:522/1845 train_time:17952ms step_avg:34.39ms
step:523/1845 train_time:17986ms step_avg:34.39ms
step:524/1845 train_time:18020ms step_avg:34.39ms
step:525/1845 train_time:18054ms step_avg:34.39ms
step:526/1845 train_time:18088ms step_avg:34.39ms
step:527/1845 train_time:18122ms step_avg:34.39ms
step:528/1845 train_time:18156ms step_avg:34.39ms
step:529/1845 train_time:18190ms step_avg:34.39ms
step:530/1845 train_time:18224ms step_avg:34.39ms
step:531/1845 train_time:18258ms step_avg:34.38ms
step:532/1845 train_time:18293ms step_avg:34.38ms
step:533/1845 train_time:18327ms step_avg:34.38ms
step:534/1845 train_time:18361ms step_avg:34.38ms
step:535/1845 train_time:18395ms step_avg:34.38ms
step:536/1845 train_time:18429ms step_avg:34.38ms
step:537/1845 train_time:18463ms step_avg:34.38ms
step:538/1845 train_time:18498ms step_avg:34.38ms
step:539/1845 train_time:18532ms step_avg:34.38ms
step:540/1845 train_time:18566ms step_avg:34.38ms
step:541/1845 train_time:18600ms step_avg:34.38ms
step:542/1845 train_time:18634ms step_avg:34.38ms
step:543/1845 train_time:18669ms step_avg:34.38ms
step:544/1845 train_time:18703ms step_avg:34.38ms
step:545/1845 train_time:18738ms step_avg:34.38ms
step:546/1845 train_time:18772ms step_avg:34.38ms
step:547/1845 train_time:18806ms step_avg:34.38ms
step:548/1845 train_time:18840ms step_avg:34.38ms
step:549/1845 train_time:18874ms step_avg:34.38ms
step:550/1845 train_time:18909ms step_avg:34.38ms
step:551/1845 train_time:18943ms step_avg:34.38ms
step:552/1845 train_time:18977ms step_avg:34.38ms
step:553/1845 train_time:19012ms step_avg:34.38ms
step:554/1845 train_time:19046ms step_avg:34.38ms
step:555/1845 train_time:19080ms step_avg:34.38ms
step:556/1845 train_time:19115ms step_avg:34.38ms
step:557/1845 train_time:19149ms step_avg:34.38ms
step:558/1845 train_time:19184ms step_avg:34.38ms
step:559/1845 train_time:19218ms step_avg:34.38ms
step:560/1845 train_time:19252ms step_avg:34.38ms
step:561/1845 train_time:19286ms step_avg:34.38ms
step:562/1845 train_time:19321ms step_avg:34.38ms
step:563/1845 train_time:19355ms step_avg:34.38ms
step:564/1845 train_time:19390ms step_avg:34.38ms
step:565/1845 train_time:19423ms step_avg:34.38ms
step:566/1845 train_time:19457ms step_avg:34.38ms
step:567/1845 train_time:19491ms step_avg:34.38ms
step:568/1845 train_time:19526ms step_avg:34.38ms
step:569/1845 train_time:19560ms step_avg:34.38ms
step:570/1845 train_time:19594ms step_avg:34.38ms
step:571/1845 train_time:19628ms step_avg:34.38ms
step:572/1845 train_time:19663ms step_avg:34.38ms
step:573/1845 train_time:19697ms step_avg:34.37ms
step:574/1845 train_time:19731ms step_avg:34.37ms
step:575/1845 train_time:19765ms step_avg:34.37ms
step:576/1845 train_time:19799ms step_avg:34.37ms
step:577/1845 train_time:19833ms step_avg:34.37ms
step:578/1845 train_time:19868ms step_avg:34.37ms
step:579/1845 train_time:19902ms step_avg:34.37ms
step:580/1845 train_time:19936ms step_avg:34.37ms
step:581/1845 train_time:19970ms step_avg:34.37ms
step:582/1845 train_time:20004ms step_avg:34.37ms
step:583/1845 train_time:20038ms step_avg:34.37ms
step:584/1845 train_time:20073ms step_avg:34.37ms
step:585/1845 train_time:20107ms step_avg:34.37ms
step:586/1845 train_time:20141ms step_avg:34.37ms
step:587/1845 train_time:20175ms step_avg:34.37ms
step:588/1845 train_time:20209ms step_avg:34.37ms
step:589/1845 train_time:20243ms step_avg:34.37ms
step:590/1845 train_time:20278ms step_avg:34.37ms
step:591/1845 train_time:20312ms step_avg:34.37ms
step:592/1845 train_time:20346ms step_avg:34.37ms
step:593/1845 train_time:20380ms step_avg:34.37ms
step:594/1845 train_time:20414ms step_avg:34.37ms
step:595/1845 train_time:20448ms step_avg:34.37ms
step:596/1845 train_time:20483ms step_avg:34.37ms
step:597/1845 train_time:20517ms step_avg:34.37ms
step:598/1845 train_time:20551ms step_avg:34.37ms
step:599/1845 train_time:20585ms step_avg:34.37ms
step:600/1845 train_time:20620ms step_avg:34.37ms
step:601/1845 train_time:20654ms step_avg:34.37ms
step:602/1845 train_time:20688ms step_avg:34.37ms
step:603/1845 train_time:20723ms step_avg:34.37ms
step:604/1845 train_time:20783ms step_avg:34.41ms
step:605/1845 train_time:20845ms step_avg:34.46ms
step:606/1845 train_time:20906ms step_avg:34.50ms
step:607/1845 train_time:20969ms step_avg:34.54ms
step:608/1845 train_time:21030ms step_avg:34.59ms
step:609/1845 train_time:21092ms step_avg:34.63ms
step:610/1845 train_time:21153ms step_avg:34.68ms
step:611/1845 train_time:21215ms step_avg:34.72ms
step:612/1845 train_time:21276ms step_avg:34.76ms
step:613/1845 train_time:21339ms step_avg:34.81ms
step:614/1845 train_time:21400ms step_avg:34.85ms
step:615/1845 train_time:21462ms step_avg:34.90ms
step:616/1845 train_time:21524ms step_avg:34.94ms
step:617/1845 train_time:21586ms step_avg:34.98ms
step:618/1845 train_time:21647ms step_avg:35.03ms
step:619/1845 train_time:21709ms step_avg:35.07ms
step:620/1845 train_time:21770ms step_avg:35.11ms
step:621/1845 train_time:21832ms step_avg:35.16ms
step:622/1845 train_time:21893ms step_avg:35.20ms
step:623/1845 train_time:21955ms step_avg:35.24ms
step:624/1845 train_time:22016ms step_avg:35.28ms
step:625/1845 train_time:22079ms step_avg:35.33ms
step:626/1845 train_time:22140ms step_avg:35.37ms
step:627/1845 train_time:22202ms step_avg:35.41ms
step:628/1845 train_time:22263ms step_avg:35.45ms
step:629/1845 train_time:22325ms step_avg:35.49ms
step:630/1845 train_time:22386ms step_avg:35.53ms
step:631/1845 train_time:22449ms step_avg:35.58ms
step:632/1845 train_time:22510ms step_avg:35.62ms
step:633/1845 train_time:22572ms step_avg:35.66ms
step:634/1845 train_time:22634ms step_avg:35.70ms
step:635/1845 train_time:22696ms step_avg:35.74ms
step:636/1845 train_time:22758ms step_avg:35.78ms
step:637/1845 train_time:22820ms step_avg:35.82ms
step:638/1845 train_time:22881ms step_avg:35.86ms
step:639/1845 train_time:22944ms step_avg:35.91ms
step:640/1845 train_time:23005ms step_avg:35.94ms
step:641/1845 train_time:23067ms step_avg:35.99ms
step:642/1845 train_time:23129ms step_avg:36.03ms
step:643/1845 train_time:23191ms step_avg:36.07ms
step:644/1845 train_time:23253ms step_avg:36.11ms
step:645/1845 train_time:23315ms step_avg:36.15ms
step:646/1845 train_time:23376ms step_avg:36.19ms
step:647/1845 train_time:23439ms step_avg:36.23ms
step:648/1845 train_time:23500ms step_avg:36.27ms
step:649/1845 train_time:23563ms step_avg:36.31ms
step:650/1845 train_time:23624ms step_avg:36.35ms
step:651/1845 train_time:23687ms step_avg:36.39ms
step:652/1845 train_time:23748ms step_avg:36.42ms
step:653/1845 train_time:23811ms step_avg:36.46ms
step:654/1845 train_time:23872ms step_avg:36.50ms
step:655/1845 train_time:23934ms step_avg:36.54ms
step:656/1845 train_time:23994ms step_avg:36.58ms
step:657/1845 train_time:24056ms step_avg:36.62ms
step:658/1845 train_time:24118ms step_avg:36.65ms
step:659/1845 train_time:24180ms step_avg:36.69ms
step:660/1845 train_time:24241ms step_avg:36.73ms
step:661/1845 train_time:24303ms step_avg:36.77ms
step:662/1845 train_time:24365ms step_avg:36.81ms
step:663/1845 train_time:24427ms step_avg:36.84ms
step:664/1845 train_time:24489ms step_avg:36.88ms
step:665/1845 train_time:24551ms step_avg:36.92ms
step:666/1845 train_time:24613ms step_avg:36.96ms
step:667/1845 train_time:24675ms step_avg:36.99ms
step:668/1845 train_time:24735ms step_avg:37.03ms
step:669/1845 train_time:24798ms step_avg:37.07ms
step:670/1845 train_time:24859ms step_avg:37.10ms
step:671/1845 train_time:24922ms step_avg:37.14ms
step:672/1845 train_time:24982ms step_avg:37.18ms
step:673/1845 train_time:25045ms step_avg:37.21ms
step:674/1845 train_time:25106ms step_avg:37.25ms
step:675/1845 train_time:25168ms step_avg:37.29ms
step:676/1845 train_time:25229ms step_avg:37.32ms
step:677/1845 train_time:25291ms step_avg:37.36ms
step:678/1845 train_time:25353ms step_avg:37.39ms
step:679/1845 train_time:25416ms step_avg:37.43ms
step:680/1845 train_time:25477ms step_avg:37.47ms
step:681/1845 train_time:25539ms step_avg:37.50ms
step:682/1845 train_time:25600ms step_avg:37.54ms
step:683/1845 train_time:25663ms step_avg:37.57ms
step:684/1845 train_time:25725ms step_avg:37.61ms
step:685/1845 train_time:25787ms step_avg:37.65ms
step:686/1845 train_time:25849ms step_avg:37.68ms
step:687/1845 train_time:25911ms step_avg:37.72ms
step:688/1845 train_time:25972ms step_avg:37.75ms
step:689/1845 train_time:26035ms step_avg:37.79ms
step:690/1845 train_time:26096ms step_avg:37.82ms
step:691/1845 train_time:26159ms step_avg:37.86ms
step:692/1845 train_time:26220ms step_avg:37.89ms
step:693/1845 train_time:26283ms step_avg:37.93ms
step:694/1845 train_time:26344ms step_avg:37.96ms
step:695/1845 train_time:26406ms step_avg:37.99ms
step:696/1845 train_time:26467ms step_avg:38.03ms
step:697/1845 train_time:26529ms step_avg:38.06ms
step:698/1845 train_time:26591ms step_avg:38.10ms
step:699/1845 train_time:26654ms step_avg:38.13ms
step:700/1845 train_time:26715ms step_avg:38.16ms
step:701/1845 train_time:26778ms step_avg:38.20ms
step:702/1845 train_time:26839ms step_avg:38.23ms
step:703/1845 train_time:26902ms step_avg:38.27ms
step:704/1845 train_time:26963ms step_avg:38.30ms
step:705/1845 train_time:27025ms step_avg:38.33ms
step:706/1845 train_time:27086ms step_avg:38.37ms
step:707/1845 train_time:27149ms step_avg:38.40ms
step:708/1845 train_time:27211ms step_avg:38.43ms
step:709/1845 train_time:27272ms step_avg:38.47ms
step:710/1845 train_time:27333ms step_avg:38.50ms
step:711/1845 train_time:27397ms step_avg:38.53ms
step:712/1845 train_time:27458ms step_avg:38.56ms
step:713/1845 train_time:27521ms step_avg:38.60ms
step:714/1845 train_time:27582ms step_avg:38.63ms
step:715/1845 train_time:27644ms step_avg:38.66ms
step:716/1845 train_time:27705ms step_avg:38.69ms
step:717/1845 train_time:27767ms step_avg:38.73ms
step:718/1845 train_time:27829ms step_avg:38.76ms
step:719/1845 train_time:27891ms step_avg:38.79ms
step:720/1845 train_time:27952ms step_avg:38.82ms
step:721/1845 train_time:28015ms step_avg:38.86ms
step:722/1845 train_time:28075ms step_avg:38.89ms
step:723/1845 train_time:28138ms step_avg:38.92ms
step:724/1845 train_time:28199ms step_avg:38.95ms
step:725/1845 train_time:28262ms step_avg:38.98ms
step:726/1845 train_time:28323ms step_avg:39.01ms
step:727/1845 train_time:28384ms step_avg:39.04ms
step:728/1845 train_time:28446ms step_avg:39.07ms
step:729/1845 train_time:28508ms step_avg:39.11ms
step:730/1845 train_time:28570ms step_avg:39.14ms
step:731/1845 train_time:28632ms step_avg:39.17ms
step:732/1845 train_time:28693ms step_avg:39.20ms
step:733/1845 train_time:28755ms step_avg:39.23ms
step:734/1845 train_time:28817ms step_avg:39.26ms
step:735/1845 train_time:28879ms step_avg:39.29ms
step:736/1845 train_time:28940ms step_avg:39.32ms
step:737/1845 train_time:29002ms step_avg:39.35ms
step:738/1845 train_time:29063ms step_avg:39.38ms
step:739/1845 train_time:29125ms step_avg:39.41ms
step:740/1845 train_time:29187ms step_avg:39.44ms
step:741/1845 train_time:29248ms step_avg:39.47ms
step:742/1845 train_time:29310ms step_avg:39.50ms
step:743/1845 train_time:29372ms step_avg:39.53ms
step:744/1845 train_time:29433ms step_avg:39.56ms
step:745/1845 train_time:29495ms step_avg:39.59ms
step:746/1845 train_time:29556ms step_avg:39.62ms
step:747/1845 train_time:29619ms step_avg:39.65ms
step:748/1845 train_time:29681ms step_avg:39.68ms
step:749/1845 train_time:29743ms step_avg:39.71ms
step:750/1845 train_time:29804ms step_avg:39.74ms
step:750/1845 val_loss:4.0290 train_time:29867ms step_avg:39.82ms
step:751/1845 train_time:29889ms step_avg:39.80ms
step:752/1845 train_time:29929ms step_avg:39.80ms
step:753/1845 train_time:29995ms step_avg:39.83ms
step:754/1845 train_time:30057ms step_avg:39.86ms
step:755/1845 train_time:30120ms step_avg:39.89ms
step:756/1845 train_time:30182ms step_avg:39.92ms
step:757/1845 train_time:30243ms step_avg:39.95ms
step:758/1845 train_time:30304ms step_avg:39.98ms
step:759/1845 train_time:30366ms step_avg:40.01ms
step:760/1845 train_time:30427ms step_avg:40.04ms
step:761/1845 train_time:30489ms step_avg:40.06ms
step:762/1845 train_time:30550ms step_avg:40.09ms
step:763/1845 train_time:30611ms step_avg:40.12ms
step:764/1845 train_time:30672ms step_avg:40.15ms
step:765/1845 train_time:30733ms step_avg:40.17ms
step:766/1845 train_time:30794ms step_avg:40.20ms
step:767/1845 train_time:30858ms step_avg:40.23ms
step:768/1845 train_time:30920ms step_avg:40.26ms
step:769/1845 train_time:30983ms step_avg:40.29ms
step:770/1845 train_time:31045ms step_avg:40.32ms
step:771/1845 train_time:31109ms step_avg:40.35ms
step:772/1845 train_time:31170ms step_avg:40.38ms
step:773/1845 train_time:31233ms step_avg:40.40ms
step:774/1845 train_time:31294ms step_avg:40.43ms
step:775/1845 train_time:31357ms step_avg:40.46ms
step:776/1845 train_time:31417ms step_avg:40.49ms
step:777/1845 train_time:31479ms step_avg:40.51ms
step:778/1845 train_time:31540ms step_avg:40.54ms
step:779/1845 train_time:31602ms step_avg:40.57ms
step:780/1845 train_time:31663ms step_avg:40.59ms
step:781/1845 train_time:31725ms step_avg:40.62ms
step:782/1845 train_time:31785ms step_avg:40.65ms
step:783/1845 train_time:31847ms step_avg:40.67ms
step:784/1845 train_time:31909ms step_avg:40.70ms
step:785/1845 train_time:31971ms step_avg:40.73ms
step:786/1845 train_time:32033ms step_avg:40.75ms
step:787/1845 train_time:32095ms step_avg:40.78ms
step:788/1845 train_time:32157ms step_avg:40.81ms
step:789/1845 train_time:32220ms step_avg:40.84ms
step:790/1845 train_time:32281ms step_avg:40.86ms
step:791/1845 train_time:32343ms step_avg:40.89ms
step:792/1845 train_time:32405ms step_avg:40.92ms
step:793/1845 train_time:32467ms step_avg:40.94ms
step:794/1845 train_time:32528ms step_avg:40.97ms
step:795/1845 train_time:32590ms step_avg:40.99ms
step:796/1845 train_time:32651ms step_avg:41.02ms
step:797/1845 train_time:32712ms step_avg:41.04ms
step:798/1845 train_time:32773ms step_avg:41.07ms
step:799/1845 train_time:32836ms step_avg:41.10ms
step:800/1845 train_time:32897ms step_avg:41.12ms
step:801/1845 train_time:32960ms step_avg:41.15ms
step:802/1845 train_time:33021ms step_avg:41.17ms
step:803/1845 train_time:33084ms step_avg:41.20ms
step:804/1845 train_time:33145ms step_avg:41.22ms
step:805/1845 train_time:33206ms step_avg:41.25ms
step:806/1845 train_time:33268ms step_avg:41.28ms
step:807/1845 train_time:33330ms step_avg:41.30ms
step:808/1845 train_time:33392ms step_avg:41.33ms
step:809/1845 train_time:33455ms step_avg:41.35ms
step:810/1845 train_time:33517ms step_avg:41.38ms
step:811/1845 train_time:33579ms step_avg:41.40ms
step:812/1845 train_time:33641ms step_avg:41.43ms
step:813/1845 train_time:33703ms step_avg:41.45ms
step:814/1845 train_time:33764ms step_avg:41.48ms
step:815/1845 train_time:33826ms step_avg:41.50ms
step:816/1845 train_time:33887ms step_avg:41.53ms
step:817/1845 train_time:33949ms step_avg:41.55ms
step:818/1845 train_time:34011ms step_avg:41.58ms
step:819/1845 train_time:34073ms step_avg:41.60ms
step:820/1845 train_time:34134ms step_avg:41.63ms
step:821/1845 train_time:34196ms step_avg:41.65ms
step:822/1845 train_time:34258ms step_avg:41.68ms
step:823/1845 train_time:34321ms step_avg:41.70ms
step:824/1845 train_time:34382ms step_avg:41.73ms
step:825/1845 train_time:34445ms step_avg:41.75ms
step:826/1845 train_time:34506ms step_avg:41.78ms
step:827/1845 train_time:34568ms step_avg:41.80ms
step:828/1845 train_time:34631ms step_avg:41.82ms
step:829/1845 train_time:34693ms step_avg:41.85ms
step:830/1845 train_time:34755ms step_avg:41.87ms
step:831/1845 train_time:34817ms step_avg:41.90ms
step:832/1845 train_time:34878ms step_avg:41.92ms
step:833/1845 train_time:34941ms step_avg:41.95ms
step:834/1845 train_time:35002ms step_avg:41.97ms
step:835/1845 train_time:35064ms step_avg:41.99ms
step:836/1845 train_time:35125ms step_avg:42.02ms
step:837/1845 train_time:35187ms step_avg:42.04ms
step:838/1845 train_time:35250ms step_avg:42.06ms
step:839/1845 train_time:35312ms step_avg:42.09ms
step:840/1845 train_time:35373ms step_avg:42.11ms
step:841/1845 train_time:35436ms step_avg:42.14ms
step:842/1845 train_time:35497ms step_avg:42.16ms
step:843/1845 train_time:35560ms step_avg:42.18ms
step:844/1845 train_time:35621ms step_avg:42.21ms
step:845/1845 train_time:35683ms step_avg:42.23ms
step:846/1845 train_time:35745ms step_avg:42.25ms
step:847/1845 train_time:35807ms step_avg:42.28ms
step:848/1845 train_time:35869ms step_avg:42.30ms
step:849/1845 train_time:35932ms step_avg:42.32ms
step:850/1845 train_time:35992ms step_avg:42.34ms
step:851/1845 train_time:36055ms step_avg:42.37ms
step:852/1845 train_time:36116ms step_avg:42.39ms
step:853/1845 train_time:36179ms step_avg:42.41ms
step:854/1845 train_time:36240ms step_avg:42.44ms
step:855/1845 train_time:36303ms step_avg:42.46ms
step:856/1845 train_time:36364ms step_avg:42.48ms
step:857/1845 train_time:36425ms step_avg:42.50ms
step:858/1845 train_time:36487ms step_avg:42.53ms
step:859/1845 train_time:36550ms step_avg:42.55ms
step:860/1845 train_time:36612ms step_avg:42.57ms
step:861/1845 train_time:36673ms step_avg:42.59ms
step:862/1845 train_time:36735ms step_avg:42.62ms
step:863/1845 train_time:36798ms step_avg:42.64ms
step:864/1845 train_time:36859ms step_avg:42.66ms
step:865/1845 train_time:36922ms step_avg:42.68ms
step:866/1845 train_time:36982ms step_avg:42.70ms
step:867/1845 train_time:37044ms step_avg:42.73ms
step:868/1845 train_time:37106ms step_avg:42.75ms
step:869/1845 train_time:37168ms step_avg:42.77ms
step:870/1845 train_time:37230ms step_avg:42.79ms
step:871/1845 train_time:37292ms step_avg:42.82ms
step:872/1845 train_time:37354ms step_avg:42.84ms
step:873/1845 train_time:37416ms step_avg:42.86ms
step:874/1845 train_time:37477ms step_avg:42.88ms
step:875/1845 train_time:37539ms step_avg:42.90ms
step:876/1845 train_time:37600ms step_avg:42.92ms
step:877/1845 train_time:37662ms step_avg:42.94ms
step:878/1845 train_time:37723ms step_avg:42.96ms
step:879/1845 train_time:37785ms step_avg:42.99ms
step:880/1845 train_time:37847ms step_avg:43.01ms
step:881/1845 train_time:37909ms step_avg:43.03ms
step:882/1845 train_time:37970ms step_avg:43.05ms
step:883/1845 train_time:38032ms step_avg:43.07ms
step:884/1845 train_time:38093ms step_avg:43.09ms
step:885/1845 train_time:38156ms step_avg:43.11ms
step:886/1845 train_time:38217ms step_avg:43.13ms
step:887/1845 train_time:38280ms step_avg:43.16ms
step:888/1845 train_time:38341ms step_avg:43.18ms
step:889/1845 train_time:38403ms step_avg:43.20ms
step:890/1845 train_time:38465ms step_avg:43.22ms
step:891/1845 train_time:38527ms step_avg:43.24ms
step:892/1845 train_time:38588ms step_avg:43.26ms
step:893/1845 train_time:38650ms step_avg:43.28ms
step:894/1845 train_time:38711ms step_avg:43.30ms
step:895/1845 train_time:38773ms step_avg:43.32ms
step:896/1845 train_time:38834ms step_avg:43.34ms
step:897/1845 train_time:38897ms step_avg:43.36ms
step:898/1845 train_time:38958ms step_avg:43.38ms
step:899/1845 train_time:39020ms step_avg:43.40ms
step:900/1845 train_time:39081ms step_avg:43.42ms
step:901/1845 train_time:39144ms step_avg:43.44ms
step:902/1845 train_time:39205ms step_avg:43.46ms
step:903/1845 train_time:39267ms step_avg:43.48ms
step:904/1845 train_time:39328ms step_avg:43.50ms
step:905/1845 train_time:39390ms step_avg:43.53ms
step:906/1845 train_time:39452ms step_avg:43.55ms
step:907/1845 train_time:39514ms step_avg:43.57ms
step:908/1845 train_time:39576ms step_avg:43.59ms
step:909/1845 train_time:39639ms step_avg:43.61ms
step:910/1845 train_time:39700ms step_avg:43.63ms
step:911/1845 train_time:39762ms step_avg:43.65ms
step:912/1845 train_time:39824ms step_avg:43.67ms
step:913/1845 train_time:39886ms step_avg:43.69ms
step:914/1845 train_time:39948ms step_avg:43.71ms
step:915/1845 train_time:40010ms step_avg:43.73ms
step:916/1845 train_time:40072ms step_avg:43.75ms
step:917/1845 train_time:40134ms step_avg:43.77ms
step:918/1845 train_time:40195ms step_avg:43.79ms
step:919/1845 train_time:40258ms step_avg:43.81ms
step:920/1845 train_time:40320ms step_avg:43.83ms
step:921/1845 train_time:40382ms step_avg:43.85ms
step:922/1845 train_time:40443ms step_avg:43.86ms
step:923/1845 train_time:40506ms step_avg:43.89ms
step:924/1845 train_time:40568ms step_avg:43.90ms
step:925/1845 train_time:40630ms step_avg:43.92ms
step:926/1845 train_time:40691ms step_avg:43.94ms
step:927/1845 train_time:40754ms step_avg:43.96ms
step:928/1845 train_time:40816ms step_avg:43.98ms
step:929/1845 train_time:40878ms step_avg:44.00ms
step:930/1845 train_time:40940ms step_avg:44.02ms
step:931/1845 train_time:41003ms step_avg:44.04ms
step:932/1845 train_time:41064ms step_avg:44.06ms
step:933/1845 train_time:41126ms step_avg:44.08ms
step:934/1845 train_time:41187ms step_avg:44.10ms
step:935/1845 train_time:41249ms step_avg:44.12ms
step:936/1845 train_time:41311ms step_avg:44.14ms
step:937/1845 train_time:41373ms step_avg:44.15ms
step:938/1845 train_time:41434ms step_avg:44.17ms
step:939/1845 train_time:41497ms step_avg:44.19ms
step:940/1845 train_time:41558ms step_avg:44.21ms
step:941/1845 train_time:41621ms step_avg:44.23ms
step:942/1845 train_time:41682ms step_avg:44.25ms
step:943/1845 train_time:41744ms step_avg:44.27ms
step:944/1845 train_time:41805ms step_avg:44.29ms
step:945/1845 train_time:41867ms step_avg:44.30ms
step:946/1845 train_time:41929ms step_avg:44.32ms
step:947/1845 train_time:41991ms step_avg:44.34ms
step:948/1845 train_time:42053ms step_avg:44.36ms
step:949/1845 train_time:42115ms step_avg:44.38ms
step:950/1845 train_time:42175ms step_avg:44.40ms
step:951/1845 train_time:42239ms step_avg:44.42ms
step:952/1845 train_time:42300ms step_avg:44.43ms
step:953/1845 train_time:42363ms step_avg:44.45ms
step:954/1845 train_time:42424ms step_avg:44.47ms
step:955/1845 train_time:42486ms step_avg:44.49ms
step:956/1845 train_time:42547ms step_avg:44.51ms
step:957/1845 train_time:42609ms step_avg:44.52ms
step:958/1845 train_time:42670ms step_avg:44.54ms
step:959/1845 train_time:42732ms step_avg:44.56ms
step:960/1845 train_time:42793ms step_avg:44.58ms
step:961/1845 train_time:42856ms step_avg:44.60ms
step:962/1845 train_time:42918ms step_avg:44.61ms
step:963/1845 train_time:42980ms step_avg:44.63ms
step:964/1845 train_time:43041ms step_avg:44.65ms
step:965/1845 train_time:43103ms step_avg:44.67ms
step:966/1845 train_time:43165ms step_avg:44.68ms
step:967/1845 train_time:43227ms step_avg:44.70ms
step:968/1845 train_time:43289ms step_avg:44.72ms
step:969/1845 train_time:43351ms step_avg:44.74ms
step:970/1845 train_time:43412ms step_avg:44.75ms
step:971/1845 train_time:43474ms step_avg:44.77ms
step:972/1845 train_time:43535ms step_avg:44.79ms
step:973/1845 train_time:43597ms step_avg:44.81ms
step:974/1845 train_time:43659ms step_avg:44.82ms
step:975/1845 train_time:43722ms step_avg:44.84ms
step:976/1845 train_time:43782ms step_avg:44.86ms
step:977/1845 train_time:43845ms step_avg:44.88ms
step:978/1845 train_time:43906ms step_avg:44.89ms
step:979/1845 train_time:43968ms step_avg:44.91ms
step:980/1845 train_time:44030ms step_avg:44.93ms
step:981/1845 train_time:44092ms step_avg:44.95ms
step:982/1845 train_time:44154ms step_avg:44.96ms
step:983/1845 train_time:44217ms step_avg:44.98ms
step:984/1845 train_time:44278ms step_avg:45.00ms
step:985/1845 train_time:44340ms step_avg:45.02ms
step:986/1845 train_time:44401ms step_avg:45.03ms
step:987/1845 train_time:44463ms step_avg:45.05ms
step:988/1845 train_time:44524ms step_avg:45.07ms
step:989/1845 train_time:44587ms step_avg:45.08ms
step:990/1845 train_time:44648ms step_avg:45.10ms
step:991/1845 train_time:44710ms step_avg:45.12ms
step:992/1845 train_time:44771ms step_avg:45.13ms
step:993/1845 train_time:44833ms step_avg:45.15ms
step:994/1845 train_time:44895ms step_avg:45.17ms
step:995/1845 train_time:44958ms step_avg:45.18ms
step:996/1845 train_time:45019ms step_avg:45.20ms
step:997/1845 train_time:45082ms step_avg:45.22ms
step:998/1845 train_time:45142ms step_avg:45.23ms
step:999/1845 train_time:45204ms step_avg:45.25ms
step:1000/1845 train_time:45266ms step_avg:45.27ms
step:1000/1845 val_loss:3.7815 train_time:45330ms step_avg:45.33ms
step:1001/1845 train_time:45353ms step_avg:45.31ms
step:1002/1845 train_time:45393ms step_avg:45.30ms
step:1003/1845 train_time:45457ms step_avg:45.32ms
step:1004/1845 train_time:45520ms step_avg:45.34ms
step:1005/1845 train_time:45583ms step_avg:45.36ms
step:1006/1845 train_time:45644ms step_avg:45.37ms
step:1007/1845 train_time:45705ms step_avg:45.39ms
step:1008/1845 train_time:45766ms step_avg:45.40ms
step:1009/1845 train_time:45828ms step_avg:45.42ms
step:1010/1845 train_time:45889ms step_avg:45.43ms
step:1011/1845 train_time:45950ms step_avg:45.45ms
step:1012/1845 train_time:46011ms step_avg:45.47ms
step:1013/1845 train_time:46073ms step_avg:45.48ms
step:1014/1845 train_time:46134ms step_avg:45.50ms
step:1015/1845 train_time:46195ms step_avg:45.51ms
step:1016/1845 train_time:46256ms step_avg:45.53ms
step:1017/1845 train_time:46319ms step_avg:45.54ms
step:1018/1845 train_time:46381ms step_avg:45.56ms
step:1019/1845 train_time:46444ms step_avg:45.58ms
step:1020/1845 train_time:46507ms step_avg:45.60ms
step:1021/1845 train_time:46569ms step_avg:45.61ms
step:1022/1845 train_time:46630ms step_avg:45.63ms
step:1023/1845 train_time:46693ms step_avg:45.64ms
step:1024/1845 train_time:46754ms step_avg:45.66ms
step:1025/1845 train_time:46816ms step_avg:45.67ms
step:1026/1845 train_time:46877ms step_avg:45.69ms
step:1027/1845 train_time:46939ms step_avg:45.70ms
step:1028/1845 train_time:46999ms step_avg:45.72ms
step:1029/1845 train_time:47062ms step_avg:45.74ms
step:1030/1845 train_time:47122ms step_avg:45.75ms
step:1031/1845 train_time:47185ms step_avg:45.77ms
step:1032/1845 train_time:47245ms step_avg:45.78ms
step:1033/1845 train_time:47307ms step_avg:45.80ms
step:1034/1845 train_time:47369ms step_avg:45.81ms
step:1035/1845 train_time:47432ms step_avg:45.83ms
step:1036/1845 train_time:47494ms step_avg:45.84ms
step:1037/1845 train_time:47557ms step_avg:45.86ms
step:1038/1845 train_time:47618ms step_avg:45.87ms
step:1039/1845 train_time:47681ms step_avg:45.89ms
step:1040/1845 train_time:47742ms step_avg:45.91ms
step:1041/1845 train_time:47804ms step_avg:45.92ms
step:1042/1845 train_time:47865ms step_avg:45.94ms
step:1043/1845 train_time:47926ms step_avg:45.95ms
step:1044/1845 train_time:47988ms step_avg:45.97ms
step:1045/1845 train_time:48049ms step_avg:45.98ms
step:1046/1845 train_time:48111ms step_avg:46.00ms
step:1047/1845 train_time:48172ms step_avg:46.01ms
step:1048/1845 train_time:48233ms step_avg:46.02ms
step:1049/1845 train_time:48295ms step_avg:46.04ms
step:1050/1845 train_time:48355ms step_avg:46.05ms
step:1051/1845 train_time:48418ms step_avg:46.07ms
step:1052/1845 train_time:48480ms step_avg:46.08ms
step:1053/1845 train_time:48542ms step_avg:46.10ms
step:1054/1845 train_time:48604ms step_avg:46.11ms
step:1055/1845 train_time:48667ms step_avg:46.13ms
step:1056/1845 train_time:48728ms step_avg:46.14ms
step:1057/1845 train_time:48790ms step_avg:46.16ms
step:1058/1845 train_time:48851ms step_avg:46.17ms
step:1059/1845 train_time:48914ms step_avg:46.19ms
step:1060/1845 train_time:48975ms step_avg:46.20ms
step:1061/1845 train_time:49037ms step_avg:46.22ms
step:1062/1845 train_time:49098ms step_avg:46.23ms
step:1063/1845 train_time:49160ms step_avg:46.25ms
step:1064/1845 train_time:49222ms step_avg:46.26ms
step:1065/1845 train_time:49284ms step_avg:46.28ms
step:1066/1845 train_time:49345ms step_avg:46.29ms
step:1067/1845 train_time:49407ms step_avg:46.31ms
step:1068/1845 train_time:49469ms step_avg:46.32ms
step:1069/1845 train_time:49532ms step_avg:46.34ms
step:1070/1845 train_time:49594ms step_avg:46.35ms
step:1071/1845 train_time:49657ms step_avg:46.37ms
step:1072/1845 train_time:49718ms step_avg:46.38ms
step:1073/1845 train_time:49780ms step_avg:46.39ms
step:1074/1845 train_time:49841ms step_avg:46.41ms
step:1075/1845 train_time:49904ms step_avg:46.42ms
step:1076/1845 train_time:49965ms step_avg:46.44ms
step:1077/1845 train_time:50026ms step_avg:46.45ms
step:1078/1845 train_time:50088ms step_avg:46.46ms
step:1079/1845 train_time:50151ms step_avg:46.48ms
step:1080/1845 train_time:50212ms step_avg:46.49ms
step:1081/1845 train_time:50274ms step_avg:46.51ms
step:1082/1845 train_time:50334ms step_avg:46.52ms
step:1083/1845 train_time:50397ms step_avg:46.53ms
step:1084/1845 train_time:50458ms step_avg:46.55ms
step:1085/1845 train_time:50521ms step_avg:46.56ms
step:1086/1845 train_time:50582ms step_avg:46.58ms
step:1087/1845 train_time:50645ms step_avg:46.59ms
step:1088/1845 train_time:50706ms step_avg:46.61ms
step:1089/1845 train_time:50768ms step_avg:46.62ms
step:1090/1845 train_time:50830ms step_avg:46.63ms
step:1091/1845 train_time:50892ms step_avg:46.65ms
step:1092/1845 train_time:50953ms step_avg:46.66ms
step:1093/1845 train_time:51016ms step_avg:46.68ms
step:1094/1845 train_time:51078ms step_avg:46.69ms
step:1095/1845 train_time:51140ms step_avg:46.70ms
step:1096/1845 train_time:51203ms step_avg:46.72ms
step:1097/1845 train_time:51263ms step_avg:46.73ms
step:1098/1845 train_time:51324ms step_avg:46.74ms
step:1099/1845 train_time:51387ms step_avg:46.76ms
step:1100/1845 train_time:51449ms step_avg:46.77ms
step:1101/1845 train_time:51511ms step_avg:46.79ms
step:1102/1845 train_time:51572ms step_avg:46.80ms
step:1103/1845 train_time:51635ms step_avg:46.81ms
step:1104/1845 train_time:51696ms step_avg:46.83ms
step:1105/1845 train_time:51758ms step_avg:46.84ms
step:1106/1845 train_time:51819ms step_avg:46.85ms
step:1107/1845 train_time:51882ms step_avg:46.87ms
step:1108/1845 train_time:51943ms step_avg:46.88ms
step:1109/1845 train_time:52004ms step_avg:46.89ms
step:1110/1845 train_time:52066ms step_avg:46.91ms
step:1111/1845 train_time:52128ms step_avg:46.92ms
step:1112/1845 train_time:52190ms step_avg:46.93ms
step:1113/1845 train_time:52252ms step_avg:46.95ms
step:1114/1845 train_time:52313ms step_avg:46.96ms
step:1115/1845 train_time:52375ms step_avg:46.97ms
step:1116/1845 train_time:52436ms step_avg:46.99ms
step:1117/1845 train_time:52499ms step_avg:47.00ms
step:1118/1845 train_time:52560ms step_avg:47.01ms
step:1119/1845 train_time:52622ms step_avg:47.03ms
step:1120/1845 train_time:52683ms step_avg:47.04ms
step:1121/1845 train_time:52745ms step_avg:47.05ms
step:1122/1845 train_time:52807ms step_avg:47.06ms
step:1123/1845 train_time:52869ms step_avg:47.08ms
step:1124/1845 train_time:52931ms step_avg:47.09ms
step:1125/1845 train_time:52993ms step_avg:47.11ms
step:1126/1845 train_time:53054ms step_avg:47.12ms
step:1127/1845 train_time:53117ms step_avg:47.13ms
step:1128/1845 train_time:53178ms step_avg:47.14ms
step:1129/1845 train_time:53240ms step_avg:47.16ms
step:1130/1845 train_time:53301ms step_avg:47.17ms
step:1131/1845 train_time:53363ms step_avg:47.18ms
step:1132/1845 train_time:53424ms step_avg:47.19ms
step:1133/1845 train_time:53487ms step_avg:47.21ms
step:1134/1845 train_time:53548ms step_avg:47.22ms
step:1135/1845 train_time:53611ms step_avg:47.23ms
step:1136/1845 train_time:53672ms step_avg:47.25ms
step:1137/1845 train_time:53734ms step_avg:47.26ms
step:1138/1845 train_time:53796ms step_avg:47.27ms
step:1139/1845 train_time:53859ms step_avg:47.29ms
step:1140/1845 train_time:53920ms step_avg:47.30ms
step:1141/1845 train_time:53983ms step_avg:47.31ms
step:1142/1845 train_time:54043ms step_avg:47.32ms
step:1143/1845 train_time:54106ms step_avg:47.34ms
step:1144/1845 train_time:54168ms step_avg:47.35ms
step:1145/1845 train_time:54229ms step_avg:47.36ms
step:1146/1845 train_time:54293ms step_avg:47.38ms
step:1147/1845 train_time:54353ms step_avg:47.39ms
step:1148/1845 train_time:54414ms step_avg:47.40ms
step:1149/1845 train_time:54476ms step_avg:47.41ms
step:1150/1845 train_time:54537ms step_avg:47.42ms
step:1151/1845 train_time:54599ms step_avg:47.44ms
step:1152/1845 train_time:54660ms step_avg:47.45ms
step:1153/1845 train_time:54723ms step_avg:47.46ms
step:1154/1845 train_time:54784ms step_avg:47.47ms
step:1155/1845 train_time:54847ms step_avg:47.49ms
step:1156/1845 train_time:54908ms step_avg:47.50ms
step:1157/1845 train_time:54969ms step_avg:47.51ms
step:1158/1845 train_time:55031ms step_avg:47.52ms
step:1159/1845 train_time:55093ms step_avg:47.54ms
step:1160/1845 train_time:55154ms step_avg:47.55ms
step:1161/1845 train_time:55216ms step_avg:47.56ms
step:1162/1845 train_time:55277ms step_avg:47.57ms
step:1163/1845 train_time:55340ms step_avg:47.58ms
step:1164/1845 train_time:55401ms step_avg:47.60ms
step:1165/1845 train_time:55463ms step_avg:47.61ms
step:1166/1845 train_time:55524ms step_avg:47.62ms
step:1167/1845 train_time:55586ms step_avg:47.63ms
step:1168/1845 train_time:55648ms step_avg:47.64ms
step:1169/1845 train_time:55710ms step_avg:47.66ms
step:1170/1845 train_time:55771ms step_avg:47.67ms
step:1171/1845 train_time:55833ms step_avg:47.68ms
step:1172/1845 train_time:55894ms step_avg:47.69ms
step:1173/1845 train_time:55956ms step_avg:47.70ms
step:1174/1845 train_time:56018ms step_avg:47.72ms
step:1175/1845 train_time:56081ms step_avg:47.73ms
step:1176/1845 train_time:56141ms step_avg:47.74ms
step:1177/1845 train_time:56204ms step_avg:47.75ms
step:1178/1845 train_time:56264ms step_avg:47.76ms
step:1179/1845 train_time:56327ms step_avg:47.78ms
step:1180/1845 train_time:56388ms step_avg:47.79ms
step:1181/1845 train_time:56450ms step_avg:47.80ms
step:1182/1845 train_time:56511ms step_avg:47.81ms
step:1183/1845 train_time:56573ms step_avg:47.82ms
step:1184/1845 train_time:56635ms step_avg:47.83ms
step:1185/1845 train_time:56697ms step_avg:47.85ms
step:1186/1845 train_time:56758ms step_avg:47.86ms
step:1187/1845 train_time:56822ms step_avg:47.87ms
step:1188/1845 train_time:56882ms step_avg:47.88ms
step:1189/1845 train_time:56944ms step_avg:47.89ms
step:1190/1845 train_time:57005ms step_avg:47.90ms
step:1191/1845 train_time:57067ms step_avg:47.92ms
step:1192/1845 train_time:57129ms step_avg:47.93ms
step:1193/1845 train_time:57191ms step_avg:47.94ms
step:1194/1845 train_time:57252ms step_avg:47.95ms
step:1195/1845 train_time:57315ms step_avg:47.96ms
step:1196/1845 train_time:57376ms step_avg:47.97ms
step:1197/1845 train_time:57438ms step_avg:47.99ms
step:1198/1845 train_time:57500ms step_avg:48.00ms
step:1199/1845 train_time:57562ms step_avg:48.01ms
step:1200/1845 train_time:57623ms step_avg:48.02ms
step:1201/1845 train_time:57685ms step_avg:48.03ms
step:1202/1845 train_time:57747ms step_avg:48.04ms
step:1203/1845 train_time:57809ms step_avg:48.05ms
step:1204/1845 train_time:57871ms step_avg:48.07ms
step:1205/1845 train_time:57933ms step_avg:48.08ms
step:1206/1845 train_time:58021ms step_avg:48.11ms
step:1207/1845 train_time:58111ms step_avg:48.15ms
step:1208/1845 train_time:58197ms step_avg:48.18ms
step:1209/1845 train_time:58286ms step_avg:48.21ms
step:1210/1845 train_time:58374ms step_avg:48.24ms
step:1211/1845 train_time:58463ms step_avg:48.28ms
step:1212/1845 train_time:58549ms step_avg:48.31ms
step:1213/1845 train_time:58639ms step_avg:48.34ms
step:1214/1845 train_time:58726ms step_avg:48.37ms
step:1215/1845 train_time:58815ms step_avg:48.41ms
step:1216/1845 train_time:58903ms step_avg:48.44ms
step:1217/1845 train_time:58991ms step_avg:48.47ms
step:1218/1845 train_time:59078ms step_avg:48.50ms
step:1219/1845 train_time:59166ms step_avg:48.54ms
step:1220/1845 train_time:59254ms step_avg:48.57ms
step:1221/1845 train_time:59342ms step_avg:48.60ms
step:1222/1845 train_time:59429ms step_avg:48.63ms
step:1223/1845 train_time:59517ms step_avg:48.67ms
step:1224/1845 train_time:59604ms step_avg:48.70ms
step:1225/1845 train_time:59694ms step_avg:48.73ms
step:1226/1845 train_time:59781ms step_avg:48.76ms
step:1227/1845 train_time:59870ms step_avg:48.79ms
step:1228/1845 train_time:59959ms step_avg:48.83ms
step:1229/1845 train_time:60046ms step_avg:48.86ms
step:1230/1845 train_time:60134ms step_avg:48.89ms
step:1231/1845 train_time:60222ms step_avg:48.92ms
step:1232/1845 train_time:60310ms step_avg:48.95ms
step:1233/1845 train_time:60399ms step_avg:48.99ms
step:1234/1845 train_time:60487ms step_avg:49.02ms
step:1235/1845 train_time:60575ms step_avg:49.05ms
step:1236/1845 train_time:60662ms step_avg:49.08ms
step:1237/1845 train_time:60751ms step_avg:49.11ms
step:1238/1845 train_time:60839ms step_avg:49.14ms
step:1239/1845 train_time:60928ms step_avg:49.17ms
step:1240/1845 train_time:61016ms step_avg:49.21ms
step:1241/1845 train_time:61103ms step_avg:49.24ms
step:1242/1845 train_time:61190ms step_avg:49.27ms
step:1243/1845 train_time:61279ms step_avg:49.30ms
step:1244/1845 train_time:61366ms step_avg:49.33ms
step:1245/1845 train_time:61455ms step_avg:49.36ms
step:1246/1845 train_time:61544ms step_avg:49.39ms
step:1247/1845 train_time:61632ms step_avg:49.42ms
step:1248/1845 train_time:61719ms step_avg:49.45ms
step:1249/1845 train_time:61808ms step_avg:49.49ms
step:1250/1845 train_time:61896ms step_avg:49.52ms
step:1250/1845 val_loss:3.5402 train_time:61986ms step_avg:49.59ms
step:1251/1845 train_time:62010ms step_avg:49.57ms
step:1252/1845 train_time:62073ms step_avg:49.58ms
step:1253/1845 train_time:62165ms step_avg:49.61ms
step:1254/1845 train_time:62259ms step_avg:49.65ms
step:1255/1845 train_time:62352ms step_avg:49.68ms
step:1256/1845 train_time:62438ms step_avg:49.71ms
step:1257/1845 train_time:62526ms step_avg:49.74ms
step:1258/1845 train_time:62612ms step_avg:49.77ms
step:1259/1845 train_time:62700ms step_avg:49.80ms
step:1260/1845 train_time:62787ms step_avg:49.83ms
step:1261/1845 train_time:62874ms step_avg:49.86ms
step:1262/1845 train_time:62962ms step_avg:49.89ms
step:1263/1845 train_time:63052ms step_avg:49.92ms
step:1264/1845 train_time:63140ms step_avg:49.95ms
step:1265/1845 train_time:63232ms step_avg:49.99ms
step:1266/1845 train_time:63319ms step_avg:50.01ms
step:1267/1845 train_time:63408ms step_avg:50.05ms
step:1268/1845 train_time:63495ms step_avg:50.07ms
step:1269/1845 train_time:63583ms step_avg:50.10ms
step:1270/1845 train_time:63669ms step_avg:50.13ms
step:1271/1845 train_time:63757ms step_avg:50.16ms
step:1272/1845 train_time:63844ms step_avg:50.19ms
step:1273/1845 train_time:63932ms step_avg:50.22ms
step:1274/1845 train_time:64021ms step_avg:50.25ms
step:1275/1845 train_time:64110ms step_avg:50.28ms
step:1276/1845 train_time:64199ms step_avg:50.31ms
step:1277/1845 train_time:64289ms step_avg:50.34ms
step:1278/1845 train_time:64377ms step_avg:50.37ms
step:1279/1845 train_time:64465ms step_avg:50.40ms
step:1280/1845 train_time:64552ms step_avg:50.43ms
step:1281/1845 train_time:64640ms step_avg:50.46ms
step:1282/1845 train_time:64727ms step_avg:50.49ms
step:1283/1845 train_time:64814ms step_avg:50.52ms
step:1284/1845 train_time:64902ms step_avg:50.55ms
step:1285/1845 train_time:64990ms step_avg:50.58ms
step:1286/1845 train_time:65078ms step_avg:50.61ms
step:1287/1845 train_time:65167ms step_avg:50.63ms
step:1288/1845 train_time:65255ms step_avg:50.66ms
step:1289/1845 train_time:65345ms step_avg:50.69ms
step:1290/1845 train_time:65432ms step_avg:50.72ms
step:1291/1845 train_time:65519ms step_avg:50.75ms
step:1292/1845 train_time:65607ms step_avg:50.78ms
step:1293/1845 train_time:65695ms step_avg:50.81ms
step:1294/1845 train_time:65782ms step_avg:50.84ms
step:1295/1845 train_time:65870ms step_avg:50.87ms
step:1296/1845 train_time:65959ms step_avg:50.89ms
step:1297/1845 train_time:66047ms step_avg:50.92ms
step:1298/1845 train_time:66136ms step_avg:50.95ms
step:1299/1845 train_time:66225ms step_avg:50.98ms
step:1300/1845 train_time:66313ms step_avg:51.01ms
step:1301/1845 train_time:66402ms step_avg:51.04ms
step:1302/1845 train_time:66489ms step_avg:51.07ms
step:1303/1845 train_time:66577ms step_avg:51.10ms
step:1304/1845 train_time:66665ms step_avg:51.12ms
step:1305/1845 train_time:66753ms step_avg:51.15ms
step:1306/1845 train_time:66840ms step_avg:51.18ms
step:1307/1845 train_time:66928ms step_avg:51.21ms
step:1308/1845 train_time:67016ms step_avg:51.24ms
step:1309/1845 train_time:67106ms step_avg:51.26ms
step:1310/1845 train_time:67194ms step_avg:51.29ms
step:1311/1845 train_time:67283ms step_avg:51.32ms
step:1312/1845 train_time:67370ms step_avg:51.35ms
step:1313/1845 train_time:67458ms step_avg:51.38ms
step:1314/1845 train_time:67545ms step_avg:51.40ms
step:1315/1845 train_time:67633ms step_avg:51.43ms
step:1316/1845 train_time:67721ms step_avg:51.46ms
step:1317/1845 train_time:67808ms step_avg:51.49ms
step:1318/1845 train_time:67896ms step_avg:51.51ms
step:1319/1845 train_time:67984ms step_avg:51.54ms
step:1320/1845 train_time:68072ms step_avg:51.57ms
step:1321/1845 train_time:68160ms step_avg:51.60ms
step:1322/1845 train_time:68249ms step_avg:51.63ms
step:1323/1845 train_time:68338ms step_avg:51.65ms
step:1324/1845 train_time:68425ms step_avg:51.68ms
step:1325/1845 train_time:68514ms step_avg:51.71ms
step:1326/1845 train_time:68601ms step_avg:51.74ms
step:1327/1845 train_time:68689ms step_avg:51.76ms
step:1328/1845 train_time:68776ms step_avg:51.79ms
step:1329/1845 train_time:68864ms step_avg:51.82ms
step:1330/1845 train_time:68951ms step_avg:51.84ms
step:1331/1845 train_time:69042ms step_avg:51.87ms
step:1332/1845 train_time:69129ms step_avg:51.90ms
step:1333/1845 train_time:69219ms step_avg:51.93ms
step:1334/1845 train_time:69307ms step_avg:51.95ms
step:1335/1845 train_time:69395ms step_avg:51.98ms
step:1336/1845 train_time:69482ms step_avg:52.01ms
step:1337/1845 train_time:69570ms step_avg:52.03ms
step:1338/1845 train_time:69659ms step_avg:52.06ms
step:1339/1845 train_time:69746ms step_avg:52.09ms
step:1340/1845 train_time:69835ms step_avg:52.12ms
step:1341/1845 train_time:69922ms step_avg:52.14ms
step:1342/1845 train_time:70011ms step_avg:52.17ms
step:1343/1845 train_time:70101ms step_avg:52.20ms
step:1344/1845 train_time:70189ms step_avg:52.22ms
step:1345/1845 train_time:70278ms step_avg:52.25ms
step:1346/1845 train_time:70366ms step_avg:52.28ms
step:1347/1845 train_time:70454ms step_avg:52.30ms
step:1348/1845 train_time:70543ms step_avg:52.33ms
step:1349/1845 train_time:70631ms step_avg:52.36ms
step:1350/1845 train_time:70719ms step_avg:52.38ms
step:1351/1845 train_time:70807ms step_avg:52.41ms
step:1352/1845 train_time:70895ms step_avg:52.44ms
step:1353/1845 train_time:70983ms step_avg:52.46ms
step:1354/1845 train_time:71071ms step_avg:52.49ms
step:1355/1845 train_time:71160ms step_avg:52.52ms
step:1356/1845 train_time:71248ms step_avg:52.54ms
step:1357/1845 train_time:71337ms step_avg:52.57ms
step:1358/1845 train_time:71425ms step_avg:52.60ms
step:1359/1845 train_time:71514ms step_avg:52.62ms
step:1360/1845 train_time:71602ms step_avg:52.65ms
step:1361/1845 train_time:71690ms step_avg:52.67ms
step:1362/1845 train_time:71779ms step_avg:52.70ms
step:1363/1845 train_time:71867ms step_avg:52.73ms
step:1364/1845 train_time:71954ms step_avg:52.75ms
step:1365/1845 train_time:72043ms step_avg:52.78ms
step:1366/1845 train_time:72131ms step_avg:52.80ms
step:1367/1845 train_time:72219ms step_avg:52.83ms
step:1368/1845 train_time:72306ms step_avg:52.86ms
step:1369/1845 train_time:72395ms step_avg:52.88ms
step:1370/1845 train_time:72482ms step_avg:52.91ms
step:1371/1845 train_time:72570ms step_avg:52.93ms
step:1372/1845 train_time:72657ms step_avg:52.96ms
step:1373/1845 train_time:72746ms step_avg:52.98ms
step:1374/1845 train_time:72834ms step_avg:53.01ms
step:1375/1845 train_time:72921ms step_avg:53.03ms
step:1376/1845 train_time:73008ms step_avg:53.06ms
step:1377/1845 train_time:73097ms step_avg:53.08ms
step:1378/1845 train_time:73186ms step_avg:53.11ms
step:1379/1845 train_time:73274ms step_avg:53.14ms
step:1380/1845 train_time:73361ms step_avg:53.16ms
step:1381/1845 train_time:73450ms step_avg:53.19ms
step:1382/1845 train_time:73539ms step_avg:53.21ms
step:1383/1845 train_time:73627ms step_avg:53.24ms
step:1384/1845 train_time:73714ms step_avg:53.26ms
step:1385/1845 train_time:73803ms step_avg:53.29ms
step:1386/1845 train_time:73891ms step_avg:53.31ms
step:1387/1845 train_time:73978ms step_avg:53.34ms
step:1388/1845 train_time:74067ms step_avg:53.36ms
step:1389/1845 train_time:74156ms step_avg:53.39ms
step:1390/1845 train_time:74243ms step_avg:53.41ms
step:1391/1845 train_time:74331ms step_avg:53.44ms
step:1392/1845 train_time:74419ms step_avg:53.46ms
step:1393/1845 train_time:74507ms step_avg:53.49ms
step:1394/1845 train_time:74593ms step_avg:53.51ms
step:1395/1845 train_time:74681ms step_avg:53.53ms
step:1396/1845 train_time:74768ms step_avg:53.56ms
step:1397/1845 train_time:74857ms step_avg:53.58ms
step:1398/1845 train_time:74944ms step_avg:53.61ms
step:1399/1845 train_time:75032ms step_avg:53.63ms
step:1400/1845 train_time:75120ms step_avg:53.66ms
step:1401/1845 train_time:75209ms step_avg:53.68ms
step:1402/1845 train_time:75297ms step_avg:53.71ms
step:1403/1845 train_time:75385ms step_avg:53.73ms
step:1404/1845 train_time:75473ms step_avg:53.76ms
step:1405/1845 train_time:75561ms step_avg:53.78ms
step:1406/1845 train_time:75649ms step_avg:53.80ms
step:1407/1845 train_time:75737ms step_avg:53.83ms
step:1408/1845 train_time:75824ms step_avg:53.85ms
step:1409/1845 train_time:75912ms step_avg:53.88ms
step:1410/1845 train_time:76000ms step_avg:53.90ms
step:1411/1845 train_time:76089ms step_avg:53.93ms
step:1412/1845 train_time:76176ms step_avg:53.95ms
step:1413/1845 train_time:76264ms step_avg:53.97ms
step:1414/1845 train_time:76351ms step_avg:54.00ms
step:1415/1845 train_time:76441ms step_avg:54.02ms
step:1416/1845 train_time:76530ms step_avg:54.05ms
step:1417/1845 train_time:76619ms step_avg:54.07ms
step:1418/1845 train_time:76706ms step_avg:54.09ms
step:1419/1845 train_time:76794ms step_avg:54.12ms
step:1420/1845 train_time:76882ms step_avg:54.14ms
step:1421/1845 train_time:76970ms step_avg:54.17ms
step:1422/1845 train_time:77058ms step_avg:54.19ms
step:1423/1845 train_time:77146ms step_avg:54.21ms
step:1424/1845 train_time:77234ms step_avg:54.24ms
step:1425/1845 train_time:77323ms step_avg:54.26ms
step:1426/1845 train_time:77409ms step_avg:54.28ms
step:1427/1845 train_time:77498ms step_avg:54.31ms
step:1428/1845 train_time:77585ms step_avg:54.33ms
step:1429/1845 train_time:77673ms step_avg:54.35ms
step:1430/1845 train_time:77761ms step_avg:54.38ms
step:1431/1845 train_time:77848ms step_avg:54.40ms
step:1432/1845 train_time:77936ms step_avg:54.42ms
step:1433/1845 train_time:78024ms step_avg:54.45ms
step:1434/1845 train_time:78112ms step_avg:54.47ms
step:1435/1845 train_time:78200ms step_avg:54.49ms
step:1436/1845 train_time:78288ms step_avg:54.52ms
step:1437/1845 train_time:78378ms step_avg:54.54ms
step:1438/1845 train_time:78466ms step_avg:54.57ms
step:1439/1845 train_time:78555ms step_avg:54.59ms
step:1440/1845 train_time:78642ms step_avg:54.61ms
step:1441/1845 train_time:78731ms step_avg:54.64ms
step:1442/1845 train_time:78818ms step_avg:54.66ms
step:1443/1845 train_time:78907ms step_avg:54.68ms
step:1444/1845 train_time:78994ms step_avg:54.70ms
step:1445/1845 train_time:79082ms step_avg:54.73ms
step:1446/1845 train_time:79169ms step_avg:54.75ms
step:1447/1845 train_time:79258ms step_avg:54.77ms
step:1448/1845 train_time:79348ms step_avg:54.80ms
step:1449/1845 train_time:79436ms step_avg:54.82ms
step:1450/1845 train_time:79525ms step_avg:54.84ms
step:1451/1845 train_time:79612ms step_avg:54.87ms
step:1452/1845 train_time:79700ms step_avg:54.89ms
step:1453/1845 train_time:79788ms step_avg:54.91ms
step:1454/1845 train_time:79875ms step_avg:54.93ms
step:1455/1845 train_time:79964ms step_avg:54.96ms
step:1456/1845 train_time:80052ms step_avg:54.98ms
step:1457/1845 train_time:80140ms step_avg:55.00ms
step:1458/1845 train_time:80228ms step_avg:55.03ms
step:1459/1845 train_time:80317ms step_avg:55.05ms
step:1460/1845 train_time:80404ms step_avg:55.07ms
step:1461/1845 train_time:80493ms step_avg:55.09ms
step:1462/1845 train_time:80580ms step_avg:55.12ms
step:1463/1845 train_time:80669ms step_avg:55.14ms
step:1464/1845 train_time:80757ms step_avg:55.16ms
step:1465/1845 train_time:80845ms step_avg:55.18ms
step:1466/1845 train_time:80932ms step_avg:55.21ms
step:1467/1845 train_time:81021ms step_avg:55.23ms
step:1468/1845 train_time:81109ms step_avg:55.25ms
step:1469/1845 train_time:81196ms step_avg:55.27ms
step:1470/1845 train_time:81284ms step_avg:55.30ms
step:1471/1845 train_time:81372ms step_avg:55.32ms
step:1472/1845 train_time:81459ms step_avg:55.34ms
step:1473/1845 train_time:81548ms step_avg:55.36ms
step:1474/1845 train_time:81636ms step_avg:55.38ms
step:1475/1845 train_time:81725ms step_avg:55.41ms
step:1476/1845 train_time:81812ms step_avg:55.43ms
step:1477/1845 train_time:81901ms step_avg:55.45ms
step:1478/1845 train_time:81988ms step_avg:55.47ms
step:1479/1845 train_time:82075ms step_avg:55.49ms
step:1480/1845 train_time:82162ms step_avg:55.51ms
step:1481/1845 train_time:82251ms step_avg:55.54ms
step:1482/1845 train_time:82340ms step_avg:55.56ms
step:1483/1845 train_time:82428ms step_avg:55.58ms
step:1484/1845 train_time:82516ms step_avg:55.60ms
step:1485/1845 train_time:82604ms step_avg:55.63ms
step:1486/1845 train_time:82692ms step_avg:55.65ms
step:1487/1845 train_time:82781ms step_avg:55.67ms
step:1488/1845 train_time:82869ms step_avg:55.69ms
step:1489/1845 train_time:82958ms step_avg:55.71ms
step:1490/1845 train_time:83046ms step_avg:55.74ms
step:1491/1845 train_time:83134ms step_avg:55.76ms
step:1492/1845 train_time:83222ms step_avg:55.78ms
step:1493/1845 train_time:83310ms step_avg:55.80ms
step:1494/1845 train_time:83398ms step_avg:55.82ms
step:1495/1845 train_time:83487ms step_avg:55.84ms
step:1496/1845 train_time:83573ms step_avg:55.86ms
step:1497/1845 train_time:83662ms step_avg:55.89ms
step:1498/1845 train_time:83750ms step_avg:55.91ms
step:1499/1845 train_time:83838ms step_avg:55.93ms
step:1500/1845 train_time:83926ms step_avg:55.95ms
step:1500/1845 val_loss:3.4060 train_time:84016ms step_avg:56.01ms
step:1501/1845 train_time:84040ms step_avg:55.99ms
step:1502/1845 train_time:84106ms step_avg:56.00ms
step:1503/1845 train_time:84198ms step_avg:56.02ms
step:1504/1845 train_time:84285ms step_avg:56.04ms
step:1505/1845 train_time:84374ms step_avg:56.06ms
step:1506/1845 train_time:84461ms step_avg:56.08ms
step:1507/1845 train_time:84548ms step_avg:56.10ms
step:1508/1845 train_time:84635ms step_avg:56.12ms
step:1509/1845 train_time:84722ms step_avg:56.14ms
step:1510/1845 train_time:84809ms step_avg:56.17ms
step:1511/1845 train_time:84897ms step_avg:56.19ms
step:1512/1845 train_time:84988ms step_avg:56.21ms
step:1513/1845 train_time:85078ms step_avg:56.23ms
step:1514/1845 train_time:85169ms step_avg:56.25ms
step:1515/1845 train_time:85259ms step_avg:56.28ms
step:1516/1845 train_time:85347ms step_avg:56.30ms
step:1517/1845 train_time:85435ms step_avg:56.32ms
step:1518/1845 train_time:85522ms step_avg:56.34ms
step:1519/1845 train_time:85609ms step_avg:56.36ms
step:1520/1845 train_time:85696ms step_avg:56.38ms
step:1521/1845 train_time:85783ms step_avg:56.40ms
step:1522/1845 train_time:85871ms step_avg:56.42ms
step:1523/1845 train_time:85959ms step_avg:56.44ms
step:1524/1845 train_time:86048ms step_avg:56.46ms
step:1525/1845 train_time:86139ms step_avg:56.48ms
step:1526/1845 train_time:86227ms step_avg:56.51ms
step:1527/1845 train_time:86316ms step_avg:56.53ms
step:1528/1845 train_time:86403ms step_avg:56.55ms
step:1529/1845 train_time:86492ms step_avg:56.57ms
step:1530/1845 train_time:86579ms step_avg:56.59ms
step:1531/1845 train_time:86667ms step_avg:56.61ms
step:1532/1845 train_time:86753ms step_avg:56.63ms
step:1533/1845 train_time:86841ms step_avg:56.65ms
step:1534/1845 train_time:86929ms step_avg:56.67ms
step:1535/1845 train_time:87018ms step_avg:56.69ms
step:1536/1845 train_time:87105ms step_avg:56.71ms
step:1537/1845 train_time:87195ms step_avg:56.73ms
step:1538/1845 train_time:87284ms step_avg:56.75ms
step:1539/1845 train_time:87372ms step_avg:56.77ms
step:1540/1845 train_time:87460ms step_avg:56.79ms
step:1541/1845 train_time:87548ms step_avg:56.81ms
step:1542/1845 train_time:87635ms step_avg:56.83ms
step:1543/1845 train_time:87722ms step_avg:56.85ms
step:1544/1845 train_time:87810ms step_avg:56.87ms
step:1545/1845 train_time:87898ms step_avg:56.89ms
step:1546/1845 train_time:87986ms step_avg:56.91ms
step:1547/1845 train_time:88075ms step_avg:56.93ms
step:1548/1845 train_time:88163ms step_avg:56.95ms
step:1549/1845 train_time:88253ms step_avg:56.97ms
step:1550/1845 train_time:88341ms step_avg:56.99ms
step:1551/1845 train_time:88429ms step_avg:57.01ms
step:1552/1845 train_time:88516ms step_avg:57.03ms
step:1553/1845 train_time:88604ms step_avg:57.05ms
step:1554/1845 train_time:88691ms step_avg:57.07ms
step:1555/1845 train_time:88780ms step_avg:57.09ms
step:1556/1845 train_time:88867ms step_avg:57.11ms
step:1557/1845 train_time:88956ms step_avg:57.13ms
step:1558/1845 train_time:89044ms step_avg:57.15ms
step:1559/1845 train_time:89133ms step_avg:57.17ms
step:1560/1845 train_time:89221ms step_avg:57.19ms
step:1561/1845 train_time:89310ms step_avg:57.21ms
step:1562/1845 train_time:89396ms step_avg:57.23ms
step:1563/1845 train_time:89485ms step_avg:57.25ms
step:1564/1845 train_time:89573ms step_avg:57.27ms
step:1565/1845 train_time:89661ms step_avg:57.29ms
step:1566/1845 train_time:89749ms step_avg:57.31ms
step:1567/1845 train_time:89836ms step_avg:57.33ms
step:1568/1845 train_time:89924ms step_avg:57.35ms
step:1569/1845 train_time:90012ms step_avg:57.37ms
step:1570/1845 train_time:90100ms step_avg:57.39ms
step:1571/1845 train_time:90189ms step_avg:57.41ms
step:1572/1845 train_time:90277ms step_avg:57.43ms
step:1573/1845 train_time:90365ms step_avg:57.45ms
step:1574/1845 train_time:90454ms step_avg:57.47ms
step:1575/1845 train_time:90542ms step_avg:57.49ms
step:1576/1845 train_time:90631ms step_avg:57.51ms
step:1577/1845 train_time:90719ms step_avg:57.53ms
step:1578/1845 train_time:90807ms step_avg:57.55ms
step:1579/1845 train_time:90894ms step_avg:57.56ms
step:1580/1845 train_time:90982ms step_avg:57.58ms
step:1581/1845 train_time:91071ms step_avg:57.60ms
step:1582/1845 train_time:91159ms step_avg:57.62ms
step:1583/1845 train_time:91247ms step_avg:57.64ms
step:1584/1845 train_time:91335ms step_avg:57.66ms
step:1585/1845 train_time:91424ms step_avg:57.68ms
step:1586/1845 train_time:91511ms step_avg:57.70ms
step:1587/1845 train_time:91601ms step_avg:57.72ms
step:1588/1845 train_time:91689ms step_avg:57.74ms
step:1589/1845 train_time:91778ms step_avg:57.76ms
step:1590/1845 train_time:91866ms step_avg:57.78ms
step:1591/1845 train_time:91953ms step_avg:57.80ms
step:1592/1845 train_time:92041ms step_avg:57.81ms
step:1593/1845 train_time:92130ms step_avg:57.83ms
step:1594/1845 train_time:92218ms step_avg:57.85ms
step:1595/1845 train_time:92307ms step_avg:57.87ms
step:1596/1845 train_time:92395ms step_avg:57.89ms
step:1597/1845 train_time:92484ms step_avg:57.91ms
step:1598/1845 train_time:92571ms step_avg:57.93ms
step:1599/1845 train_time:92659ms step_avg:57.95ms
step:1600/1845 train_time:92746ms step_avg:57.97ms
step:1601/1845 train_time:92834ms step_avg:57.99ms
step:1602/1845 train_time:92923ms step_avg:58.00ms
step:1603/1845 train_time:93011ms step_avg:58.02ms
step:1604/1845 train_time:93098ms step_avg:58.04ms
step:1605/1845 train_time:93186ms step_avg:58.06ms
step:1606/1845 train_time:93274ms step_avg:58.08ms
step:1607/1845 train_time:93362ms step_avg:58.10ms
step:1608/1845 train_time:93451ms step_avg:58.12ms
step:1609/1845 train_time:93539ms step_avg:58.13ms
step:1610/1845 train_time:93627ms step_avg:58.15ms
step:1611/1845 train_time:93715ms step_avg:58.17ms
step:1612/1845 train_time:93803ms step_avg:58.19ms
step:1613/1845 train_time:93890ms step_avg:58.21ms
step:1614/1845 train_time:93979ms step_avg:58.23ms
step:1615/1845 train_time:94066ms step_avg:58.25ms
step:1616/1845 train_time:94154ms step_avg:58.26ms
step:1617/1845 train_time:94242ms step_avg:58.28ms
step:1618/1845 train_time:94331ms step_avg:58.30ms
step:1619/1845 train_time:94420ms step_avg:58.32ms
step:1620/1845 train_time:94507ms step_avg:58.34ms
step:1621/1845 train_time:94596ms step_avg:58.36ms
step:1622/1845 train_time:94684ms step_avg:58.37ms
step:1623/1845 train_time:94772ms step_avg:58.39ms
step:1624/1845 train_time:94859ms step_avg:58.41ms
step:1625/1845 train_time:94948ms step_avg:58.43ms
step:1626/1845 train_time:95034ms step_avg:58.45ms
step:1627/1845 train_time:95123ms step_avg:58.47ms
step:1628/1845 train_time:95211ms step_avg:58.48ms
step:1629/1845 train_time:95300ms step_avg:58.50ms
step:1630/1845 train_time:95388ms step_avg:58.52ms
step:1631/1845 train_time:95477ms step_avg:58.54ms
step:1632/1845 train_time:95565ms step_avg:58.56ms
step:1633/1845 train_time:95653ms step_avg:58.58ms
step:1634/1845 train_time:95741ms step_avg:58.59ms
step:1635/1845 train_time:95829ms step_avg:58.61ms
step:1636/1845 train_time:95916ms step_avg:58.63ms
step:1637/1845 train_time:96005ms step_avg:58.65ms
step:1638/1845 train_time:96092ms step_avg:58.66ms
step:1639/1845 train_time:96181ms step_avg:58.68ms
step:1640/1845 train_time:96270ms step_avg:58.70ms
step:1641/1845 train_time:96358ms step_avg:58.72ms
step:1642/1845 train_time:96446ms step_avg:58.74ms
step:1643/1845 train_time:96535ms step_avg:58.76ms
step:1644/1845 train_time:96622ms step_avg:58.77ms
step:1645/1845 train_time:96711ms step_avg:58.79ms
step:1646/1845 train_time:96798ms step_avg:58.81ms
step:1647/1845 train_time:96886ms step_avg:58.83ms
step:1648/1845 train_time:96974ms step_avg:58.84ms
step:1649/1845 train_time:97062ms step_avg:58.86ms
step:1650/1845 train_time:97150ms step_avg:58.88ms
step:1651/1845 train_time:97239ms step_avg:58.90ms
step:1652/1845 train_time:97327ms step_avg:58.91ms
step:1653/1845 train_time:97414ms step_avg:58.93ms
step:1654/1845 train_time:97502ms step_avg:58.95ms
step:1655/1845 train_time:97591ms step_avg:58.97ms
step:1656/1845 train_time:97678ms step_avg:58.98ms
step:1657/1845 train_time:97768ms step_avg:59.00ms
step:1658/1845 train_time:97855ms step_avg:59.02ms
step:1659/1845 train_time:97944ms step_avg:59.04ms
step:1660/1845 train_time:98032ms step_avg:59.06ms
step:1661/1845 train_time:98120ms step_avg:59.07ms
step:1662/1845 train_time:98208ms step_avg:59.09ms
step:1663/1845 train_time:98297ms step_avg:59.11ms
step:1664/1845 train_time:98385ms step_avg:59.13ms
step:1665/1845 train_time:98473ms step_avg:59.14ms
step:1666/1845 train_time:98561ms step_avg:59.16ms
step:1667/1845 train_time:98649ms step_avg:59.18ms
step:1668/1845 train_time:98737ms step_avg:59.20ms
step:1669/1845 train_time:98825ms step_avg:59.21ms
step:1670/1845 train_time:98913ms step_avg:59.23ms
step:1671/1845 train_time:99002ms step_avg:59.25ms
step:1672/1845 train_time:99090ms step_avg:59.26ms
step:1673/1845 train_time:99179ms step_avg:59.28ms
step:1674/1845 train_time:99266ms step_avg:59.30ms
step:1675/1845 train_time:99354ms step_avg:59.32ms
step:1676/1845 train_time:99442ms step_avg:59.33ms
step:1677/1845 train_time:99531ms step_avg:59.35ms
step:1678/1845 train_time:99618ms step_avg:59.37ms
step:1679/1845 train_time:99706ms step_avg:59.38ms
step:1680/1845 train_time:99794ms step_avg:59.40ms
step:1681/1845 train_time:99882ms step_avg:59.42ms
step:1682/1845 train_time:99971ms step_avg:59.44ms
step:1683/1845 train_time:100059ms step_avg:59.45ms
step:1684/1845 train_time:100147ms step_avg:59.47ms
step:1685/1845 train_time:100235ms step_avg:59.49ms
step:1686/1845 train_time:100324ms step_avg:59.50ms
step:1687/1845 train_time:100413ms step_avg:59.52ms
step:1688/1845 train_time:100501ms step_avg:59.54ms
step:1689/1845 train_time:100589ms step_avg:59.56ms
step:1690/1845 train_time:100676ms step_avg:59.57ms
step:1691/1845 train_time:100763ms step_avg:59.59ms
step:1692/1845 train_time:100851ms step_avg:59.60ms
step:1693/1845 train_time:100941ms step_avg:59.62ms
step:1694/1845 train_time:101029ms step_avg:59.64ms
step:1695/1845 train_time:101117ms step_avg:59.66ms
step:1696/1845 train_time:101204ms step_avg:59.67ms
step:1697/1845 train_time:101293ms step_avg:59.69ms
step:1698/1845 train_time:101381ms step_avg:59.71ms
step:1699/1845 train_time:101470ms step_avg:59.72ms
step:1700/1845 train_time:101557ms step_avg:59.74ms
step:1701/1845 train_time:101645ms step_avg:59.76ms
step:1702/1845 train_time:101732ms step_avg:59.77ms
step:1703/1845 train_time:101820ms step_avg:59.79ms
step:1704/1845 train_time:101908ms step_avg:59.81ms
step:1705/1845 train_time:101997ms step_avg:59.82ms
step:1706/1845 train_time:102085ms step_avg:59.84ms
step:1707/1845 train_time:102173ms step_avg:59.86ms
step:1708/1845 train_time:102261ms step_avg:59.87ms
step:1709/1845 train_time:102350ms step_avg:59.89ms
step:1710/1845 train_time:102438ms step_avg:59.91ms
step:1711/1845 train_time:102525ms step_avg:59.92ms
step:1712/1845 train_time:102614ms step_avg:59.94ms
step:1713/1845 train_time:102701ms step_avg:59.95ms
step:1714/1845 train_time:102790ms step_avg:59.97ms
step:1715/1845 train_time:102877ms step_avg:59.99ms
step:1716/1845 train_time:102965ms step_avg:60.00ms
step:1717/1845 train_time:103053ms step_avg:60.02ms
step:1718/1845 train_time:103141ms step_avg:60.04ms
step:1719/1845 train_time:103228ms step_avg:60.05ms
step:1720/1845 train_time:103316ms step_avg:60.07ms
step:1721/1845 train_time:103406ms step_avg:60.08ms
step:1722/1845 train_time:103492ms step_avg:60.10ms
step:1723/1845 train_time:103581ms step_avg:60.12ms
step:1724/1845 train_time:103669ms step_avg:60.13ms
step:1725/1845 train_time:103757ms step_avg:60.15ms
step:1726/1845 train_time:103844ms step_avg:60.16ms
step:1727/1845 train_time:103933ms step_avg:60.18ms
step:1728/1845 train_time:104021ms step_avg:60.20ms
step:1729/1845 train_time:104110ms step_avg:60.21ms
step:1730/1845 train_time:104198ms step_avg:60.23ms
step:1731/1845 train_time:104286ms step_avg:60.25ms
step:1732/1845 train_time:104375ms step_avg:60.26ms
step:1733/1845 train_time:104464ms step_avg:60.28ms
step:1734/1845 train_time:104552ms step_avg:60.30ms
step:1735/1845 train_time:104641ms step_avg:60.31ms
step:1736/1845 train_time:104727ms step_avg:60.33ms
step:1737/1845 train_time:104815ms step_avg:60.34ms
step:1738/1845 train_time:104903ms step_avg:60.36ms
step:1739/1845 train_time:104991ms step_avg:60.37ms
step:1740/1845 train_time:105080ms step_avg:60.39ms
step:1741/1845 train_time:105169ms step_avg:60.41ms
step:1742/1845 train_time:105256ms step_avg:60.42ms
step:1743/1845 train_time:105344ms step_avg:60.44ms
step:1744/1845 train_time:105432ms step_avg:60.45ms
step:1745/1845 train_time:105520ms step_avg:60.47ms
step:1746/1845 train_time:105607ms step_avg:60.49ms
step:1747/1845 train_time:105695ms step_avg:60.50ms
step:1748/1845 train_time:105783ms step_avg:60.52ms
step:1749/1845 train_time:105871ms step_avg:60.53ms
step:1750/1845 train_time:105959ms step_avg:60.55ms
step:1750/1845 val_loss:3.3066 train_time:106049ms step_avg:60.60ms
step:1751/1845 train_time:106073ms step_avg:60.58ms
step:1752/1845 train_time:106140ms step_avg:60.58ms
step:1753/1845 train_time:106235ms step_avg:60.60ms
step:1754/1845 train_time:106324ms step_avg:60.62ms
step:1755/1845 train_time:106412ms step_avg:60.63ms
step:1756/1845 train_time:106498ms step_avg:60.65ms
step:1757/1845 train_time:106586ms step_avg:60.66ms
step:1758/1845 train_time:106672ms step_avg:60.68ms
step:1759/1845 train_time:106759ms step_avg:60.69ms
step:1760/1845 train_time:106845ms step_avg:60.71ms
step:1761/1845 train_time:106932ms step_avg:60.72ms
step:1762/1845 train_time:107021ms step_avg:60.74ms
step:1763/1845 train_time:107111ms step_avg:60.76ms
step:1764/1845 train_time:107202ms step_avg:60.77ms
step:1765/1845 train_time:107292ms step_avg:60.79ms
step:1766/1845 train_time:107380ms step_avg:60.80ms
step:1767/1845 train_time:107467ms step_avg:60.82ms
step:1768/1845 train_time:107554ms step_avg:60.83ms
step:1769/1845 train_time:107642ms step_avg:60.85ms
step:1770/1845 train_time:107728ms step_avg:60.86ms
step:1771/1845 train_time:107816ms step_avg:60.88ms
step:1772/1845 train_time:107902ms step_avg:60.89ms
step:1773/1845 train_time:107990ms step_avg:60.91ms
step:1774/1845 train_time:108079ms step_avg:60.92ms
step:1775/1845 train_time:108169ms step_avg:60.94ms
step:1776/1845 train_time:108257ms step_avg:60.96ms
step:1777/1845 train_time:108345ms step_avg:60.97ms
step:1778/1845 train_time:108433ms step_avg:60.99ms
step:1779/1845 train_time:108522ms step_avg:61.00ms
step:1780/1845 train_time:108608ms step_avg:61.02ms
step:1781/1845 train_time:108696ms step_avg:61.03ms
step:1782/1845 train_time:108782ms step_avg:61.05ms
step:1783/1845 train_time:108870ms step_avg:61.06ms
step:1784/1845 train_time:108957ms step_avg:61.07ms
step:1785/1845 train_time:109046ms step_avg:61.09ms
step:1786/1845 train_time:109134ms step_avg:61.11ms
step:1787/1845 train_time:109223ms step_avg:61.12ms
step:1788/1845 train_time:109313ms step_avg:61.14ms
step:1789/1845 train_time:109402ms step_avg:61.15ms
step:1790/1845 train_time:109489ms step_avg:61.17ms
step:1791/1845 train_time:109577ms step_avg:61.18ms
step:1792/1845 train_time:109664ms step_avg:61.20ms
step:1793/1845 train_time:109752ms step_avg:61.21ms
step:1794/1845 train_time:109838ms step_avg:61.23ms
step:1795/1845 train_time:109927ms step_avg:61.24ms
step:1796/1845 train_time:110015ms step_avg:61.26ms
step:1797/1845 train_time:110103ms step_avg:61.27ms
step:1798/1845 train_time:110191ms step_avg:61.29ms
step:1799/1845 train_time:110280ms step_avg:61.30ms
step:1800/1845 train_time:110368ms step_avg:61.32ms
step:1801/1845 train_time:110455ms step_avg:61.33ms
step:1802/1845 train_time:110542ms step_avg:61.34ms
step:1803/1845 train_time:110631ms step_avg:61.36ms
step:1804/1845 train_time:110718ms step_avg:61.37ms
step:1805/1845 train_time:110805ms step_avg:61.39ms
step:1806/1845 train_time:110893ms step_avg:61.40ms
step:1807/1845 train_time:110982ms step_avg:61.42ms
step:1808/1845 train_time:111070ms step_avg:61.43ms
step:1809/1845 train_time:111158ms step_avg:61.45ms
step:1810/1845 train_time:111246ms step_avg:61.46ms
step:1811/1845 train_time:111335ms step_avg:61.48ms
step:1812/1845 train_time:111422ms step_avg:61.49ms
step:1813/1845 train_time:111511ms step_avg:61.51ms
step:1814/1845 train_time:111598ms step_avg:61.52ms
step:1815/1845 train_time:111687ms step_avg:61.54ms
step:1816/1845 train_time:111774ms step_avg:61.55ms
step:1817/1845 train_time:111862ms step_avg:61.56ms
step:1818/1845 train_time:111950ms step_avg:61.58ms
step:1819/1845 train_time:112039ms step_avg:61.59ms
step:1820/1845 train_time:112127ms step_avg:61.61ms
step:1821/1845 train_time:112216ms step_avg:61.62ms
step:1822/1845 train_time:112304ms step_avg:61.64ms
step:1823/1845 train_time:112393ms step_avg:61.65ms
step:1824/1845 train_time:112481ms step_avg:61.67ms
step:1825/1845 train_time:112570ms step_avg:61.68ms
step:1826/1845 train_time:112658ms step_avg:61.70ms
step:1827/1845 train_time:112747ms step_avg:61.71ms
step:1828/1845 train_time:112835ms step_avg:61.73ms
step:1829/1845 train_time:112923ms step_avg:61.74ms
step:1830/1845 train_time:113012ms step_avg:61.76ms
step:1831/1845 train_time:113100ms step_avg:61.77ms
step:1832/1845 train_time:113189ms step_avg:61.78ms
step:1833/1845 train_time:113278ms step_avg:61.80ms
step:1834/1845 train_time:113366ms step_avg:61.81ms
step:1835/1845 train_time:113456ms step_avg:61.83ms
step:1836/1845 train_time:113543ms step_avg:61.84ms
step:1837/1845 train_time:113631ms step_avg:61.86ms
step:1838/1845 train_time:113718ms step_avg:61.87ms
step:1839/1845 train_time:113807ms step_avg:61.89ms
step:1840/1845 train_time:113894ms step_avg:61.90ms
step:1841/1845 train_time:113983ms step_avg:61.91ms
step:1842/1845 train_time:114072ms step_avg:61.93ms
step:1843/1845 train_time:114161ms step_avg:61.94ms
step:1844/1845 train_time:114248ms step_avg:61.96ms
step:1845/1845 train_time:114337ms step_avg:61.97ms
step:1845/1845 val_loss:3.2798 train_time:114424ms step_avg:62.02ms
peak memory allocated: 29405 MiB reserved: 44818 MiB

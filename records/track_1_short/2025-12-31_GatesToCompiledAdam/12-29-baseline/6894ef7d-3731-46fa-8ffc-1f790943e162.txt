import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 18:14:11 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    180190      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    180191      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    180192      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    180193      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    180194      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    180195      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    180196      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    180197      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8307 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:77ms step_avg:76.82ms
step:2/1845 train_time:100ms step_avg:49.90ms
step:3/1845 train_time:121ms step_avg:40.20ms
step:4/1845 train_time:155ms step_avg:38.75ms
step:5/1845 train_time:189ms step_avg:37.76ms
step:6/1845 train_time:264ms step_avg:43.97ms
step:7/1845 train_time:364ms step_avg:52.03ms
step:8/1845 train_time:399ms step_avg:49.82ms
step:9/1845 train_time:433ms step_avg:48.06ms
step:10/1845 train_time:467ms step_avg:46.71ms
step:11/1845 train_time:501ms step_avg:45.53ms
step:12/1845 train_time:535ms step_avg:44.60ms
step:13/1845 train_time:569ms step_avg:43.81ms
step:14/1845 train_time:604ms step_avg:43.15ms
step:15/1845 train_time:638ms step_avg:42.54ms
step:16/1845 train_time:673ms step_avg:42.05ms
step:17/1845 train_time:707ms step_avg:41.58ms
step:18/1845 train_time:741ms step_avg:41.18ms
step:19/1845 train_time:775ms step_avg:40.81ms
step:20/1845 train_time:810ms step_avg:40.49ms
step:21/1845 train_time:844ms step_avg:40.19ms
step:22/1845 train_time:878ms step_avg:39.93ms
step:23/1845 train_time:913ms step_avg:39.68ms
step:24/1845 train_time:947ms step_avg:39.46ms
step:25/1845 train_time:981ms step_avg:39.24ms
step:26/1845 train_time:1015ms step_avg:39.06ms
step:27/1845 train_time:1050ms step_avg:38.87ms
step:28/1845 train_time:1084ms step_avg:38.73ms
step:29/1845 train_time:1118ms step_avg:38.57ms
step:30/1845 train_time:1153ms step_avg:38.43ms
step:31/1845 train_time:1187ms step_avg:38.30ms
step:32/1845 train_time:1222ms step_avg:38.18ms
step:33/1845 train_time:1256ms step_avg:38.06ms
step:34/1845 train_time:1290ms step_avg:37.95ms
step:35/1845 train_time:1325ms step_avg:37.84ms
step:36/1845 train_time:1359ms step_avg:37.75ms
step:37/1845 train_time:1393ms step_avg:37.66ms
step:38/1845 train_time:1428ms step_avg:37.58ms
step:39/1845 train_time:1462ms step_avg:37.49ms
step:40/1845 train_time:1497ms step_avg:37.41ms
step:41/1845 train_time:1531ms step_avg:37.34ms
step:42/1845 train_time:1566ms step_avg:37.28ms
step:43/1845 train_time:1600ms step_avg:37.20ms
step:44/1845 train_time:1634ms step_avg:37.14ms
step:45/1845 train_time:1668ms step_avg:37.07ms
step:46/1845 train_time:1703ms step_avg:37.02ms
step:47/1845 train_time:1737ms step_avg:36.95ms
step:48/1845 train_time:1772ms step_avg:36.91ms
step:49/1845 train_time:1806ms step_avg:36.86ms
step:50/1845 train_time:1840ms step_avg:36.81ms
step:51/1845 train_time:1875ms step_avg:36.76ms
step:52/1845 train_time:1909ms step_avg:36.71ms
step:53/1845 train_time:1943ms step_avg:36.66ms
step:54/1845 train_time:1978ms step_avg:36.62ms
step:55/1845 train_time:2012ms step_avg:36.58ms
step:56/1845 train_time:2047ms step_avg:36.54ms
step:57/1845 train_time:2081ms step_avg:36.51ms
step:58/1845 train_time:2115ms step_avg:36.47ms
step:59/1845 train_time:2149ms step_avg:36.43ms
step:60/1845 train_time:2184ms step_avg:36.39ms
step:61/1845 train_time:2218ms step_avg:36.35ms
step:62/1845 train_time:2252ms step_avg:36.32ms
step:63/1845 train_time:2286ms step_avg:36.29ms
step:64/1845 train_time:2321ms step_avg:36.26ms
step:65/1845 train_time:2355ms step_avg:36.23ms
step:66/1845 train_time:2389ms step_avg:36.20ms
step:67/1845 train_time:2423ms step_avg:36.17ms
step:68/1845 train_time:2458ms step_avg:36.14ms
step:69/1845 train_time:2492ms step_avg:36.11ms
step:70/1845 train_time:2526ms step_avg:36.09ms
step:71/1845 train_time:2560ms step_avg:36.06ms
step:72/1845 train_time:2595ms step_avg:36.04ms
step:73/1845 train_time:2629ms step_avg:36.01ms
step:74/1845 train_time:2663ms step_avg:35.99ms
step:75/1845 train_time:2698ms step_avg:35.97ms
step:76/1845 train_time:2732ms step_avg:35.95ms
step:77/1845 train_time:2766ms step_avg:35.93ms
step:78/1845 train_time:2801ms step_avg:35.91ms
step:79/1845 train_time:2835ms step_avg:35.88ms
step:80/1845 train_time:2870ms step_avg:35.87ms
step:81/1845 train_time:2904ms step_avg:35.85ms
step:82/1845 train_time:2938ms step_avg:35.83ms
step:83/1845 train_time:2972ms step_avg:35.81ms
step:84/1845 train_time:3007ms step_avg:35.79ms
step:85/1845 train_time:3041ms step_avg:35.78ms
step:86/1845 train_time:3076ms step_avg:35.76ms
step:87/1845 train_time:3110ms step_avg:35.74ms
step:88/1845 train_time:3144ms step_avg:35.73ms
step:89/1845 train_time:3178ms step_avg:35.71ms
step:90/1845 train_time:3213ms step_avg:35.70ms
step:91/1845 train_time:3247ms step_avg:35.68ms
step:92/1845 train_time:3281ms step_avg:35.67ms
step:93/1845 train_time:3315ms step_avg:35.65ms
step:94/1845 train_time:3350ms step_avg:35.64ms
step:95/1845 train_time:3384ms step_avg:35.62ms
step:96/1845 train_time:3418ms step_avg:35.61ms
step:97/1845 train_time:3453ms step_avg:35.59ms
step:98/1845 train_time:3487ms step_avg:35.58ms
step:99/1845 train_time:3521ms step_avg:35.57ms
step:100/1845 train_time:3556ms step_avg:35.56ms
step:101/1845 train_time:3590ms step_avg:35.55ms
step:102/1845 train_time:3625ms step_avg:35.53ms
step:103/1845 train_time:3659ms step_avg:35.52ms
step:104/1845 train_time:3693ms step_avg:35.51ms
step:105/1845 train_time:3728ms step_avg:35.50ms
step:106/1845 train_time:3762ms step_avg:35.49ms
step:107/1845 train_time:3796ms step_avg:35.48ms
step:108/1845 train_time:3831ms step_avg:35.47ms
step:109/1845 train_time:3865ms step_avg:35.46ms
step:110/1845 train_time:3900ms step_avg:35.45ms
step:111/1845 train_time:3934ms step_avg:35.44ms
step:112/1845 train_time:3968ms step_avg:35.43ms
step:113/1845 train_time:4003ms step_avg:35.42ms
step:114/1845 train_time:4037ms step_avg:35.41ms
step:115/1845 train_time:4071ms step_avg:35.40ms
step:116/1845 train_time:4106ms step_avg:35.39ms
step:117/1845 train_time:4140ms step_avg:35.38ms
step:118/1845 train_time:4174ms step_avg:35.38ms
step:119/1845 train_time:4208ms step_avg:35.37ms
step:120/1845 train_time:4243ms step_avg:35.36ms
step:121/1845 train_time:4277ms step_avg:35.35ms
step:122/1845 train_time:4311ms step_avg:35.34ms
step:123/1845 train_time:4345ms step_avg:35.33ms
step:124/1845 train_time:4380ms step_avg:35.32ms
step:125/1845 train_time:4414ms step_avg:35.31ms
step:126/1845 train_time:4448ms step_avg:35.30ms
step:127/1845 train_time:4482ms step_avg:35.29ms
step:128/1845 train_time:4516ms step_avg:35.28ms
step:129/1845 train_time:4551ms step_avg:35.28ms
step:130/1845 train_time:4585ms step_avg:35.27ms
step:131/1845 train_time:4619ms step_avg:35.26ms
step:132/1845 train_time:4653ms step_avg:35.25ms
step:133/1845 train_time:4688ms step_avg:35.24ms
step:134/1845 train_time:4722ms step_avg:35.24ms
step:135/1845 train_time:4756ms step_avg:35.23ms
step:136/1845 train_time:4791ms step_avg:35.23ms
step:137/1845 train_time:4825ms step_avg:35.22ms
step:138/1845 train_time:4859ms step_avg:35.21ms
step:139/1845 train_time:4893ms step_avg:35.20ms
step:140/1845 train_time:4928ms step_avg:35.20ms
step:141/1845 train_time:4962ms step_avg:35.19ms
step:142/1845 train_time:4997ms step_avg:35.19ms
step:143/1845 train_time:5031ms step_avg:35.18ms
step:144/1845 train_time:5065ms step_avg:35.17ms
step:145/1845 train_time:5099ms step_avg:35.17ms
step:146/1845 train_time:5133ms step_avg:35.16ms
step:147/1845 train_time:5167ms step_avg:35.15ms
step:148/1845 train_time:5202ms step_avg:35.15ms
step:149/1845 train_time:5236ms step_avg:35.14ms
step:150/1845 train_time:5271ms step_avg:35.14ms
step:151/1845 train_time:5305ms step_avg:35.13ms
step:152/1845 train_time:5340ms step_avg:35.13ms
step:153/1845 train_time:5374ms step_avg:35.12ms
step:154/1845 train_time:5408ms step_avg:35.12ms
step:155/1845 train_time:5442ms step_avg:35.11ms
step:156/1845 train_time:5476ms step_avg:35.11ms
step:157/1845 train_time:5511ms step_avg:35.10ms
step:158/1845 train_time:5545ms step_avg:35.09ms
step:159/1845 train_time:5579ms step_avg:35.09ms
step:160/1845 train_time:5613ms step_avg:35.08ms
step:161/1845 train_time:5647ms step_avg:35.08ms
step:162/1845 train_time:5682ms step_avg:35.07ms
step:163/1845 train_time:5716ms step_avg:35.07ms
step:164/1845 train_time:5750ms step_avg:35.06ms
step:165/1845 train_time:5784ms step_avg:35.06ms
step:166/1845 train_time:5818ms step_avg:35.05ms
step:167/1845 train_time:5853ms step_avg:35.05ms
step:168/1845 train_time:5887ms step_avg:35.04ms
step:169/1845 train_time:5921ms step_avg:35.04ms
step:170/1845 train_time:5955ms step_avg:35.03ms
step:171/1845 train_time:5990ms step_avg:35.03ms
step:172/1845 train_time:6024ms step_avg:35.02ms
step:173/1845 train_time:6058ms step_avg:35.02ms
step:174/1845 train_time:6092ms step_avg:35.01ms
step:175/1845 train_time:6127ms step_avg:35.01ms
step:176/1845 train_time:6161ms step_avg:35.00ms
step:177/1845 train_time:6195ms step_avg:35.00ms
step:178/1845 train_time:6229ms step_avg:35.00ms
step:179/1845 train_time:6263ms step_avg:34.99ms
step:180/1845 train_time:6298ms step_avg:34.99ms
step:181/1845 train_time:6332ms step_avg:34.98ms
step:182/1845 train_time:6366ms step_avg:34.98ms
step:183/1845 train_time:6400ms step_avg:34.97ms
step:184/1845 train_time:6434ms step_avg:34.97ms
step:185/1845 train_time:6468ms step_avg:34.96ms
step:186/1845 train_time:6503ms step_avg:34.96ms
step:187/1845 train_time:6537ms step_avg:34.96ms
step:188/1845 train_time:6571ms step_avg:34.95ms
step:189/1845 train_time:6605ms step_avg:34.95ms
step:190/1845 train_time:6640ms step_avg:34.94ms
step:191/1845 train_time:6673ms step_avg:34.94ms
step:192/1845 train_time:6708ms step_avg:34.94ms
step:193/1845 train_time:6742ms step_avg:34.93ms
step:194/1845 train_time:6776ms step_avg:34.93ms
step:195/1845 train_time:6810ms step_avg:34.92ms
step:196/1845 train_time:6845ms step_avg:34.92ms
step:197/1845 train_time:6878ms step_avg:34.92ms
step:198/1845 train_time:6913ms step_avg:34.91ms
step:199/1845 train_time:6947ms step_avg:34.91ms
step:200/1845 train_time:6981ms step_avg:34.91ms
step:201/1845 train_time:7015ms step_avg:34.90ms
step:202/1845 train_time:7050ms step_avg:34.90ms
step:203/1845 train_time:7084ms step_avg:34.90ms
step:204/1845 train_time:7118ms step_avg:34.89ms
step:205/1845 train_time:7152ms step_avg:34.89ms
step:206/1845 train_time:7187ms step_avg:34.89ms
step:207/1845 train_time:7221ms step_avg:34.88ms
step:208/1845 train_time:7255ms step_avg:34.88ms
step:209/1845 train_time:7290ms step_avg:34.88ms
step:210/1845 train_time:7324ms step_avg:34.87ms
step:211/1845 train_time:7358ms step_avg:34.87ms
step:212/1845 train_time:7392ms step_avg:34.87ms
step:213/1845 train_time:7426ms step_avg:34.87ms
step:214/1845 train_time:7461ms step_avg:34.86ms
step:215/1845 train_time:7495ms step_avg:34.86ms
step:216/1845 train_time:7529ms step_avg:34.86ms
step:217/1845 train_time:7563ms step_avg:34.85ms
step:218/1845 train_time:7597ms step_avg:34.85ms
step:219/1845 train_time:7632ms step_avg:34.85ms
step:220/1845 train_time:7666ms step_avg:34.84ms
step:221/1845 train_time:7700ms step_avg:34.84ms
step:222/1845 train_time:7734ms step_avg:34.84ms
step:223/1845 train_time:7768ms step_avg:34.84ms
step:224/1845 train_time:7803ms step_avg:34.83ms
step:225/1845 train_time:7837ms step_avg:34.83ms
step:226/1845 train_time:7871ms step_avg:34.83ms
step:227/1845 train_time:7905ms step_avg:34.82ms
step:228/1845 train_time:7939ms step_avg:34.82ms
step:229/1845 train_time:7973ms step_avg:34.82ms
step:230/1845 train_time:8008ms step_avg:34.82ms
step:231/1845 train_time:8042ms step_avg:34.81ms
step:232/1845 train_time:8076ms step_avg:34.81ms
step:233/1845 train_time:8110ms step_avg:34.81ms
step:234/1845 train_time:8144ms step_avg:34.81ms
step:235/1845 train_time:8178ms step_avg:34.80ms
step:236/1845 train_time:8213ms step_avg:34.80ms
step:237/1845 train_time:8247ms step_avg:34.80ms
step:238/1845 train_time:8281ms step_avg:34.79ms
step:239/1845 train_time:8315ms step_avg:34.79ms
step:240/1845 train_time:8349ms step_avg:34.79ms
step:241/1845 train_time:8383ms step_avg:34.79ms
step:242/1845 train_time:8417ms step_avg:34.78ms
step:243/1845 train_time:8451ms step_avg:34.78ms
step:244/1845 train_time:8486ms step_avg:34.78ms
step:245/1845 train_time:8520ms step_avg:34.77ms
step:246/1845 train_time:8554ms step_avg:34.77ms
step:247/1845 train_time:8588ms step_avg:34.77ms
step:248/1845 train_time:8622ms step_avg:34.77ms
step:249/1845 train_time:8656ms step_avg:34.76ms
step:250/1845 train_time:8691ms step_avg:34.76ms
step:250/1845 val_loss:4.6390 train_time:8727ms step_avg:34.91ms
step:251/1845 train_time:8747ms step_avg:34.85ms
step:252/1845 train_time:8765ms step_avg:34.78ms
step:253/1845 train_time:8796ms step_avg:34.77ms
step:254/1845 train_time:8831ms step_avg:34.77ms
step:255/1845 train_time:8866ms step_avg:34.77ms
step:256/1845 train_time:8901ms step_avg:34.77ms
step:257/1845 train_time:8936ms step_avg:34.77ms
step:258/1845 train_time:8970ms step_avg:34.77ms
step:259/1845 train_time:9004ms step_avg:34.77ms
step:260/1845 train_time:9039ms step_avg:34.76ms
step:261/1845 train_time:9073ms step_avg:34.76ms
step:262/1845 train_time:9107ms step_avg:34.76ms
step:263/1845 train_time:9141ms step_avg:34.76ms
step:264/1845 train_time:9175ms step_avg:34.76ms
step:265/1845 train_time:9210ms step_avg:34.75ms
step:266/1845 train_time:9244ms step_avg:34.75ms
step:267/1845 train_time:9278ms step_avg:34.75ms
step:268/1845 train_time:9312ms step_avg:34.75ms
step:269/1845 train_time:9347ms step_avg:34.75ms
step:270/1845 train_time:9381ms step_avg:34.74ms
step:271/1845 train_time:9415ms step_avg:34.74ms
step:272/1845 train_time:9449ms step_avg:34.74ms
step:273/1845 train_time:9483ms step_avg:34.74ms
step:274/1845 train_time:9517ms step_avg:34.73ms
step:275/1845 train_time:9551ms step_avg:34.73ms
step:276/1845 train_time:9585ms step_avg:34.73ms
step:277/1845 train_time:9619ms step_avg:34.73ms
step:278/1845 train_time:9654ms step_avg:34.73ms
step:279/1845 train_time:9688ms step_avg:34.72ms
step:280/1845 train_time:9722ms step_avg:34.72ms
step:281/1845 train_time:9756ms step_avg:34.72ms
step:282/1845 train_time:9790ms step_avg:34.72ms
step:283/1845 train_time:9824ms step_avg:34.72ms
step:284/1845 train_time:9859ms step_avg:34.71ms
step:285/1845 train_time:9893ms step_avg:34.71ms
step:286/1845 train_time:9927ms step_avg:34.71ms
step:287/1845 train_time:9961ms step_avg:34.71ms
step:288/1845 train_time:9995ms step_avg:34.71ms
step:289/1845 train_time:10029ms step_avg:34.70ms
step:290/1845 train_time:10064ms step_avg:34.70ms
step:291/1845 train_time:10098ms step_avg:34.70ms
step:292/1845 train_time:10132ms step_avg:34.70ms
step:293/1845 train_time:10167ms step_avg:34.70ms
step:294/1845 train_time:10201ms step_avg:34.70ms
step:295/1845 train_time:10235ms step_avg:34.69ms
step:296/1845 train_time:10269ms step_avg:34.69ms
step:297/1845 train_time:10303ms step_avg:34.69ms
step:298/1845 train_time:10337ms step_avg:34.69ms
step:299/1845 train_time:10371ms step_avg:34.69ms
step:300/1845 train_time:10406ms step_avg:34.69ms
step:301/1845 train_time:10440ms step_avg:34.68ms
step:302/1845 train_time:10474ms step_avg:34.68ms
step:303/1845 train_time:10508ms step_avg:34.68ms
step:304/1845 train_time:10543ms step_avg:34.68ms
step:305/1845 train_time:10577ms step_avg:34.68ms
step:306/1845 train_time:10611ms step_avg:34.68ms
step:307/1845 train_time:10645ms step_avg:34.67ms
step:308/1845 train_time:10679ms step_avg:34.67ms
step:309/1845 train_time:10713ms step_avg:34.67ms
step:310/1845 train_time:10748ms step_avg:34.67ms
step:311/1845 train_time:10782ms step_avg:34.67ms
step:312/1845 train_time:10816ms step_avg:34.67ms
step:313/1845 train_time:10850ms step_avg:34.67ms
step:314/1845 train_time:10884ms step_avg:34.66ms
step:315/1845 train_time:10918ms step_avg:34.66ms
step:316/1845 train_time:10953ms step_avg:34.66ms
step:317/1845 train_time:10987ms step_avg:34.66ms
step:318/1845 train_time:11021ms step_avg:34.66ms
step:319/1845 train_time:11055ms step_avg:34.66ms
step:320/1845 train_time:11089ms step_avg:34.65ms
step:321/1845 train_time:11123ms step_avg:34.65ms
step:322/1845 train_time:11158ms step_avg:34.65ms
step:323/1845 train_time:11192ms step_avg:34.65ms
step:324/1845 train_time:11226ms step_avg:34.65ms
step:325/1845 train_time:11260ms step_avg:34.65ms
step:326/1845 train_time:11295ms step_avg:34.65ms
step:327/1845 train_time:11329ms step_avg:34.64ms
step:328/1845 train_time:11364ms step_avg:34.65ms
step:329/1845 train_time:11397ms step_avg:34.64ms
step:330/1845 train_time:11431ms step_avg:34.64ms
step:331/1845 train_time:11465ms step_avg:34.64ms
step:332/1845 train_time:11500ms step_avg:34.64ms
step:333/1845 train_time:11534ms step_avg:34.64ms
step:334/1845 train_time:11568ms step_avg:34.63ms
step:335/1845 train_time:11602ms step_avg:34.63ms
step:336/1845 train_time:11636ms step_avg:34.63ms
step:337/1845 train_time:11670ms step_avg:34.63ms
step:338/1845 train_time:11704ms step_avg:34.63ms
step:339/1845 train_time:11739ms step_avg:34.63ms
step:340/1845 train_time:11773ms step_avg:34.63ms
step:341/1845 train_time:11807ms step_avg:34.63ms
step:342/1845 train_time:11841ms step_avg:34.62ms
step:343/1845 train_time:11875ms step_avg:34.62ms
step:344/1845 train_time:11910ms step_avg:34.62ms
step:345/1845 train_time:11944ms step_avg:34.62ms
step:346/1845 train_time:11978ms step_avg:34.62ms
step:347/1845 train_time:12012ms step_avg:34.62ms
step:348/1845 train_time:12046ms step_avg:34.62ms
step:349/1845 train_time:12080ms step_avg:34.61ms
step:350/1845 train_time:12115ms step_avg:34.61ms
step:351/1845 train_time:12149ms step_avg:34.61ms
step:352/1845 train_time:12183ms step_avg:34.61ms
step:353/1845 train_time:12217ms step_avg:34.61ms
step:354/1845 train_time:12251ms step_avg:34.61ms
step:355/1845 train_time:12285ms step_avg:34.61ms
step:356/1845 train_time:12320ms step_avg:34.61ms
step:357/1845 train_time:12354ms step_avg:34.60ms
step:358/1845 train_time:12388ms step_avg:34.60ms
step:359/1845 train_time:12422ms step_avg:34.60ms
step:360/1845 train_time:12456ms step_avg:34.60ms
step:361/1845 train_time:12490ms step_avg:34.60ms
step:362/1845 train_time:12525ms step_avg:34.60ms
step:363/1845 train_time:12559ms step_avg:34.60ms
step:364/1845 train_time:12593ms step_avg:34.60ms
step:365/1845 train_time:12627ms step_avg:34.59ms
step:366/1845 train_time:12661ms step_avg:34.59ms
step:367/1845 train_time:12695ms step_avg:34.59ms
step:368/1845 train_time:12730ms step_avg:34.59ms
step:369/1845 train_time:12764ms step_avg:34.59ms
step:370/1845 train_time:12798ms step_avg:34.59ms
step:371/1845 train_time:12832ms step_avg:34.59ms
step:372/1845 train_time:12866ms step_avg:34.59ms
step:373/1845 train_time:12900ms step_avg:34.59ms
step:374/1845 train_time:12935ms step_avg:34.59ms
step:375/1845 train_time:12969ms step_avg:34.58ms
step:376/1845 train_time:13003ms step_avg:34.58ms
step:377/1845 train_time:13037ms step_avg:34.58ms
step:378/1845 train_time:13071ms step_avg:34.58ms
step:379/1845 train_time:13105ms step_avg:34.58ms
step:380/1845 train_time:13140ms step_avg:34.58ms
step:381/1845 train_time:13174ms step_avg:34.58ms
step:382/1845 train_time:13208ms step_avg:34.58ms
step:383/1845 train_time:13242ms step_avg:34.57ms
step:384/1845 train_time:13276ms step_avg:34.57ms
step:385/1845 train_time:13310ms step_avg:34.57ms
step:386/1845 train_time:13344ms step_avg:34.57ms
step:387/1845 train_time:13378ms step_avg:34.57ms
step:388/1845 train_time:13412ms step_avg:34.57ms
step:389/1845 train_time:13446ms step_avg:34.57ms
step:390/1845 train_time:13481ms step_avg:34.57ms
step:391/1845 train_time:13515ms step_avg:34.56ms
step:392/1845 train_time:13549ms step_avg:34.56ms
step:393/1845 train_time:13583ms step_avg:34.56ms
step:394/1845 train_time:13618ms step_avg:34.56ms
step:395/1845 train_time:13652ms step_avg:34.56ms
step:396/1845 train_time:13686ms step_avg:34.56ms
step:397/1845 train_time:13720ms step_avg:34.56ms
step:398/1845 train_time:13754ms step_avg:34.56ms
step:399/1845 train_time:13788ms step_avg:34.56ms
step:400/1845 train_time:13822ms step_avg:34.56ms
step:401/1845 train_time:13856ms step_avg:34.55ms
step:402/1845 train_time:13891ms step_avg:34.55ms
step:403/1845 train_time:13925ms step_avg:34.55ms
step:404/1845 train_time:13959ms step_avg:34.55ms
step:405/1845 train_time:13993ms step_avg:34.55ms
step:406/1845 train_time:14027ms step_avg:34.55ms
step:407/1845 train_time:14061ms step_avg:34.55ms
step:408/1845 train_time:14096ms step_avg:34.55ms
step:409/1845 train_time:14130ms step_avg:34.55ms
step:410/1845 train_time:14164ms step_avg:34.55ms
step:411/1845 train_time:14198ms step_avg:34.55ms
step:412/1845 train_time:14232ms step_avg:34.54ms
step:413/1845 train_time:14267ms step_avg:34.54ms
step:414/1845 train_time:14301ms step_avg:34.54ms
step:415/1845 train_time:14335ms step_avg:34.54ms
step:416/1845 train_time:14369ms step_avg:34.54ms
step:417/1845 train_time:14403ms step_avg:34.54ms
step:418/1845 train_time:14438ms step_avg:34.54ms
step:419/1845 train_time:14471ms step_avg:34.54ms
step:420/1845 train_time:14506ms step_avg:34.54ms
step:421/1845 train_time:14540ms step_avg:34.54ms
step:422/1845 train_time:14574ms step_avg:34.54ms
step:423/1845 train_time:14608ms step_avg:34.53ms
step:424/1845 train_time:14642ms step_avg:34.53ms
step:425/1845 train_time:14676ms step_avg:34.53ms
step:426/1845 train_time:14710ms step_avg:34.53ms
step:427/1845 train_time:14744ms step_avg:34.53ms
step:428/1845 train_time:14779ms step_avg:34.53ms
step:429/1845 train_time:14813ms step_avg:34.53ms
step:430/1845 train_time:14847ms step_avg:34.53ms
step:431/1845 train_time:14881ms step_avg:34.53ms
step:432/1845 train_time:14915ms step_avg:34.53ms
step:433/1845 train_time:14949ms step_avg:34.52ms
step:434/1845 train_time:14983ms step_avg:34.52ms
step:435/1845 train_time:15017ms step_avg:34.52ms
step:436/1845 train_time:15052ms step_avg:34.52ms
step:437/1845 train_time:15086ms step_avg:34.52ms
step:438/1845 train_time:15120ms step_avg:34.52ms
step:439/1845 train_time:15154ms step_avg:34.52ms
step:440/1845 train_time:15188ms step_avg:34.52ms
step:441/1845 train_time:15222ms step_avg:34.52ms
step:442/1845 train_time:15257ms step_avg:34.52ms
step:443/1845 train_time:15291ms step_avg:34.52ms
step:444/1845 train_time:15325ms step_avg:34.52ms
step:445/1845 train_time:15359ms step_avg:34.51ms
step:446/1845 train_time:15393ms step_avg:34.51ms
step:447/1845 train_time:15428ms step_avg:34.51ms
step:448/1845 train_time:15462ms step_avg:34.51ms
step:449/1845 train_time:15496ms step_avg:34.51ms
step:450/1845 train_time:15530ms step_avg:34.51ms
step:451/1845 train_time:15564ms step_avg:34.51ms
step:452/1845 train_time:15599ms step_avg:34.51ms
step:453/1845 train_time:15633ms step_avg:34.51ms
step:454/1845 train_time:15667ms step_avg:34.51ms
step:455/1845 train_time:15701ms step_avg:34.51ms
step:456/1845 train_time:15735ms step_avg:34.51ms
step:457/1845 train_time:15769ms step_avg:34.51ms
step:458/1845 train_time:15803ms step_avg:34.51ms
step:459/1845 train_time:15837ms step_avg:34.50ms
step:460/1845 train_time:15872ms step_avg:34.50ms
step:461/1845 train_time:15906ms step_avg:34.50ms
step:462/1845 train_time:15940ms step_avg:34.50ms
step:463/1845 train_time:15974ms step_avg:34.50ms
step:464/1845 train_time:16008ms step_avg:34.50ms
step:465/1845 train_time:16042ms step_avg:34.50ms
step:466/1845 train_time:16077ms step_avg:34.50ms
step:467/1845 train_time:16110ms step_avg:34.50ms
step:468/1845 train_time:16145ms step_avg:34.50ms
step:469/1845 train_time:16179ms step_avg:34.50ms
step:470/1845 train_time:16213ms step_avg:34.50ms
step:471/1845 train_time:16247ms step_avg:34.49ms
step:472/1845 train_time:16281ms step_avg:34.49ms
step:473/1845 train_time:16315ms step_avg:34.49ms
step:474/1845 train_time:16350ms step_avg:34.49ms
step:475/1845 train_time:16384ms step_avg:34.49ms
step:476/1845 train_time:16418ms step_avg:34.49ms
step:477/1845 train_time:16452ms step_avg:34.49ms
step:478/1845 train_time:16486ms step_avg:34.49ms
step:479/1845 train_time:16520ms step_avg:34.49ms
step:480/1845 train_time:16554ms step_avg:34.49ms
step:481/1845 train_time:16589ms step_avg:34.49ms
step:482/1845 train_time:16623ms step_avg:34.49ms
step:483/1845 train_time:16657ms step_avg:34.49ms
step:484/1845 train_time:16691ms step_avg:34.49ms
step:485/1845 train_time:16725ms step_avg:34.48ms
step:486/1845 train_time:16759ms step_avg:34.48ms
step:487/1845 train_time:16793ms step_avg:34.48ms
step:488/1845 train_time:16828ms step_avg:34.48ms
step:489/1845 train_time:16862ms step_avg:34.48ms
step:490/1845 train_time:16896ms step_avg:34.48ms
step:491/1845 train_time:16930ms step_avg:34.48ms
step:492/1845 train_time:16965ms step_avg:34.48ms
step:493/1845 train_time:16999ms step_avg:34.48ms
step:494/1845 train_time:17033ms step_avg:34.48ms
step:495/1845 train_time:17067ms step_avg:34.48ms
step:496/1845 train_time:17101ms step_avg:34.48ms
step:497/1845 train_time:17135ms step_avg:34.48ms
step:498/1845 train_time:17169ms step_avg:34.48ms
step:499/1845 train_time:17203ms step_avg:34.48ms
step:500/1845 train_time:17238ms step_avg:34.48ms
step:500/1845 val_loss:4.3007 train_time:17274ms step_avg:34.55ms
step:501/1845 train_time:17293ms step_avg:34.52ms
step:502/1845 train_time:17312ms step_avg:34.49ms
step:503/1845 train_time:17342ms step_avg:34.48ms
step:504/1845 train_time:17377ms step_avg:34.48ms
step:505/1845 train_time:17413ms step_avg:34.48ms
step:506/1845 train_time:17448ms step_avg:34.48ms
step:507/1845 train_time:17483ms step_avg:34.48ms
step:508/1845 train_time:17517ms step_avg:34.48ms
step:509/1845 train_time:17551ms step_avg:34.48ms
step:510/1845 train_time:17586ms step_avg:34.48ms
step:511/1845 train_time:17620ms step_avg:34.48ms
step:512/1845 train_time:17654ms step_avg:34.48ms
step:513/1845 train_time:17688ms step_avg:34.48ms
step:514/1845 train_time:17722ms step_avg:34.48ms
step:515/1845 train_time:17757ms step_avg:34.48ms
step:516/1845 train_time:17791ms step_avg:34.48ms
step:517/1845 train_time:17825ms step_avg:34.48ms
step:518/1845 train_time:17859ms step_avg:34.48ms
step:519/1845 train_time:17893ms step_avg:34.48ms
step:520/1845 train_time:17927ms step_avg:34.48ms
step:521/1845 train_time:17961ms step_avg:34.47ms
step:522/1845 train_time:17996ms step_avg:34.47ms
step:523/1845 train_time:18030ms step_avg:34.47ms
step:524/1845 train_time:18064ms step_avg:34.47ms
step:525/1845 train_time:18098ms step_avg:34.47ms
step:526/1845 train_time:18132ms step_avg:34.47ms
step:527/1845 train_time:18166ms step_avg:34.47ms
step:528/1845 train_time:18200ms step_avg:34.47ms
step:529/1845 train_time:18234ms step_avg:34.47ms
step:530/1845 train_time:18269ms step_avg:34.47ms
step:531/1845 train_time:18303ms step_avg:34.47ms
step:532/1845 train_time:18337ms step_avg:34.47ms
step:533/1845 train_time:18371ms step_avg:34.47ms
step:534/1845 train_time:18405ms step_avg:34.47ms
step:535/1845 train_time:18439ms step_avg:34.47ms
step:536/1845 train_time:18474ms step_avg:34.47ms
step:537/1845 train_time:18508ms step_avg:34.47ms
step:538/1845 train_time:18542ms step_avg:34.47ms
step:539/1845 train_time:18577ms step_avg:34.46ms
step:540/1845 train_time:18611ms step_avg:34.46ms
step:541/1845 train_time:18645ms step_avg:34.46ms
step:542/1845 train_time:18679ms step_avg:34.46ms
step:543/1845 train_time:18713ms step_avg:34.46ms
step:544/1845 train_time:18747ms step_avg:34.46ms
step:545/1845 train_time:18781ms step_avg:34.46ms
step:546/1845 train_time:18816ms step_avg:34.46ms
step:547/1845 train_time:18850ms step_avg:34.46ms
step:548/1845 train_time:18884ms step_avg:34.46ms
step:549/1845 train_time:18918ms step_avg:34.46ms
step:550/1845 train_time:18952ms step_avg:34.46ms
step:551/1845 train_time:18986ms step_avg:34.46ms
step:552/1845 train_time:19020ms step_avg:34.46ms
step:553/1845 train_time:19054ms step_avg:34.46ms
step:554/1845 train_time:19089ms step_avg:34.46ms
step:555/1845 train_time:19123ms step_avg:34.46ms
step:556/1845 train_time:19157ms step_avg:34.46ms
step:557/1845 train_time:19191ms step_avg:34.45ms
step:558/1845 train_time:19226ms step_avg:34.45ms
step:559/1845 train_time:19260ms step_avg:34.45ms
step:560/1845 train_time:19294ms step_avg:34.45ms
step:561/1845 train_time:19328ms step_avg:34.45ms
step:562/1845 train_time:19362ms step_avg:34.45ms
step:563/1845 train_time:19396ms step_avg:34.45ms
step:564/1845 train_time:19431ms step_avg:34.45ms
step:565/1845 train_time:19465ms step_avg:34.45ms
step:566/1845 train_time:19499ms step_avg:34.45ms
step:567/1845 train_time:19533ms step_avg:34.45ms
step:568/1845 train_time:19568ms step_avg:34.45ms
step:569/1845 train_time:19602ms step_avg:34.45ms
step:570/1845 train_time:19636ms step_avg:34.45ms
step:571/1845 train_time:19670ms step_avg:34.45ms
step:572/1845 train_time:19704ms step_avg:34.45ms
step:573/1845 train_time:19738ms step_avg:34.45ms
step:574/1845 train_time:19773ms step_avg:34.45ms
step:575/1845 train_time:19807ms step_avg:34.45ms
step:576/1845 train_time:19841ms step_avg:34.45ms
step:577/1845 train_time:19875ms step_avg:34.44ms
step:578/1845 train_time:19909ms step_avg:34.44ms
step:579/1845 train_time:19943ms step_avg:34.44ms
step:580/1845 train_time:19978ms step_avg:34.44ms
step:581/1845 train_time:20012ms step_avg:34.44ms
step:582/1845 train_time:20046ms step_avg:34.44ms
step:583/1845 train_time:20080ms step_avg:34.44ms
step:584/1845 train_time:20115ms step_avg:34.44ms
step:585/1845 train_time:20149ms step_avg:34.44ms
step:586/1845 train_time:20183ms step_avg:34.44ms
step:587/1845 train_time:20217ms step_avg:34.44ms
step:588/1845 train_time:20252ms step_avg:34.44ms
step:589/1845 train_time:20286ms step_avg:34.44ms
step:590/1845 train_time:20320ms step_avg:34.44ms
step:591/1845 train_time:20354ms step_avg:34.44ms
step:592/1845 train_time:20388ms step_avg:34.44ms
step:593/1845 train_time:20422ms step_avg:34.44ms
step:594/1845 train_time:20456ms step_avg:34.44ms
step:595/1845 train_time:20490ms step_avg:34.44ms
step:596/1845 train_time:20525ms step_avg:34.44ms
step:597/1845 train_time:20559ms step_avg:34.44ms
step:598/1845 train_time:20593ms step_avg:34.44ms
step:599/1845 train_time:20627ms step_avg:34.44ms
step:600/1845 train_time:20661ms step_avg:34.44ms
step:601/1845 train_time:20695ms step_avg:34.43ms
step:602/1845 train_time:20730ms step_avg:34.43ms
step:603/1845 train_time:20764ms step_avg:34.44ms
step:604/1845 train_time:20825ms step_avg:34.48ms
step:605/1845 train_time:20887ms step_avg:34.52ms
step:606/1845 train_time:20947ms step_avg:34.57ms
step:607/1845 train_time:21010ms step_avg:34.61ms
step:608/1845 train_time:21072ms step_avg:34.66ms
step:609/1845 train_time:21134ms step_avg:34.70ms
step:610/1845 train_time:21195ms step_avg:34.75ms
step:611/1845 train_time:21257ms step_avg:34.79ms
step:612/1845 train_time:21318ms step_avg:34.83ms
step:613/1845 train_time:21381ms step_avg:34.88ms
step:614/1845 train_time:21442ms step_avg:34.92ms
step:615/1845 train_time:21504ms step_avg:34.97ms
step:616/1845 train_time:21566ms step_avg:35.01ms
step:617/1845 train_time:21628ms step_avg:35.05ms
step:618/1845 train_time:21689ms step_avg:35.10ms
step:619/1845 train_time:21751ms step_avg:35.14ms
step:620/1845 train_time:21813ms step_avg:35.18ms
step:621/1845 train_time:21874ms step_avg:35.22ms
step:622/1845 train_time:21935ms step_avg:35.27ms
step:623/1845 train_time:21997ms step_avg:35.31ms
step:624/1845 train_time:22059ms step_avg:35.35ms
step:625/1845 train_time:22120ms step_avg:35.39ms
step:626/1845 train_time:22181ms step_avg:35.43ms
step:627/1845 train_time:22243ms step_avg:35.48ms
step:628/1845 train_time:22305ms step_avg:35.52ms
step:629/1845 train_time:22368ms step_avg:35.56ms
step:630/1845 train_time:22430ms step_avg:35.60ms
step:631/1845 train_time:22493ms step_avg:35.65ms
step:632/1845 train_time:22553ms step_avg:35.69ms
step:633/1845 train_time:22615ms step_avg:35.73ms
step:634/1845 train_time:22677ms step_avg:35.77ms
step:635/1845 train_time:22739ms step_avg:35.81ms
step:636/1845 train_time:22801ms step_avg:35.85ms
step:637/1845 train_time:22862ms step_avg:35.89ms
step:638/1845 train_time:22924ms step_avg:35.93ms
step:639/1845 train_time:22986ms step_avg:35.97ms
step:640/1845 train_time:23047ms step_avg:36.01ms
step:641/1845 train_time:23109ms step_avg:36.05ms
step:642/1845 train_time:23170ms step_avg:36.09ms
step:643/1845 train_time:23232ms step_avg:36.13ms
step:644/1845 train_time:23293ms step_avg:36.17ms
step:645/1845 train_time:23356ms step_avg:36.21ms
step:646/1845 train_time:23417ms step_avg:36.25ms
step:647/1845 train_time:23479ms step_avg:36.29ms
step:648/1845 train_time:23541ms step_avg:36.33ms
step:649/1845 train_time:23603ms step_avg:36.37ms
step:650/1845 train_time:23665ms step_avg:36.41ms
step:651/1845 train_time:23728ms step_avg:36.45ms
step:652/1845 train_time:23790ms step_avg:36.49ms
step:653/1845 train_time:23851ms step_avg:36.53ms
step:654/1845 train_time:23912ms step_avg:36.56ms
step:655/1845 train_time:23974ms step_avg:36.60ms
step:656/1845 train_time:24035ms step_avg:36.64ms
step:657/1845 train_time:24096ms step_avg:36.68ms
step:658/1845 train_time:24158ms step_avg:36.71ms
step:659/1845 train_time:24221ms step_avg:36.75ms
step:660/1845 train_time:24282ms step_avg:36.79ms
step:661/1845 train_time:24344ms step_avg:36.83ms
step:662/1845 train_time:24405ms step_avg:36.87ms
step:663/1845 train_time:24468ms step_avg:36.90ms
step:664/1845 train_time:24529ms step_avg:36.94ms
step:665/1845 train_time:24592ms step_avg:36.98ms
step:666/1845 train_time:24653ms step_avg:37.02ms
step:667/1845 train_time:24715ms step_avg:37.05ms
step:668/1845 train_time:24777ms step_avg:37.09ms
step:669/1845 train_time:24838ms step_avg:37.13ms
step:670/1845 train_time:24901ms step_avg:37.17ms
step:671/1845 train_time:24962ms step_avg:37.20ms
step:672/1845 train_time:25023ms step_avg:37.24ms
step:673/1845 train_time:25085ms step_avg:37.27ms
step:674/1845 train_time:25146ms step_avg:37.31ms
step:675/1845 train_time:25209ms step_avg:37.35ms
step:676/1845 train_time:25270ms step_avg:37.38ms
step:677/1845 train_time:25333ms step_avg:37.42ms
step:678/1845 train_time:25394ms step_avg:37.45ms
step:679/1845 train_time:25456ms step_avg:37.49ms
step:680/1845 train_time:25518ms step_avg:37.53ms
step:681/1845 train_time:25579ms step_avg:37.56ms
step:682/1845 train_time:25641ms step_avg:37.60ms
step:683/1845 train_time:25703ms step_avg:37.63ms
step:684/1845 train_time:25764ms step_avg:37.67ms
step:685/1845 train_time:25827ms step_avg:37.70ms
step:686/1845 train_time:25888ms step_avg:37.74ms
step:687/1845 train_time:25951ms step_avg:37.77ms
step:688/1845 train_time:26012ms step_avg:37.81ms
step:689/1845 train_time:26074ms step_avg:37.84ms
step:690/1845 train_time:26135ms step_avg:37.88ms
step:691/1845 train_time:26196ms step_avg:37.91ms
step:692/1845 train_time:26258ms step_avg:37.95ms
step:693/1845 train_time:26320ms step_avg:37.98ms
step:694/1845 train_time:26382ms step_avg:38.01ms
step:695/1845 train_time:26444ms step_avg:38.05ms
step:696/1845 train_time:26505ms step_avg:38.08ms
step:697/1845 train_time:26568ms step_avg:38.12ms
step:698/1845 train_time:26629ms step_avg:38.15ms
step:699/1845 train_time:26692ms step_avg:38.19ms
step:700/1845 train_time:26753ms step_avg:38.22ms
step:701/1845 train_time:26815ms step_avg:38.25ms
step:702/1845 train_time:26876ms step_avg:38.29ms
step:703/1845 train_time:26938ms step_avg:38.32ms
step:704/1845 train_time:27000ms step_avg:38.35ms
step:705/1845 train_time:27061ms step_avg:38.38ms
step:706/1845 train_time:27123ms step_avg:38.42ms
step:707/1845 train_time:27185ms step_avg:38.45ms
step:708/1845 train_time:27246ms step_avg:38.48ms
step:709/1845 train_time:27308ms step_avg:38.52ms
step:710/1845 train_time:27369ms step_avg:38.55ms
step:711/1845 train_time:27432ms step_avg:38.58ms
step:712/1845 train_time:27493ms step_avg:38.61ms
step:713/1845 train_time:27555ms step_avg:38.65ms
step:714/1845 train_time:27617ms step_avg:38.68ms
step:715/1845 train_time:27679ms step_avg:38.71ms
step:716/1845 train_time:27741ms step_avg:38.74ms
step:717/1845 train_time:27802ms step_avg:38.78ms
step:718/1845 train_time:27864ms step_avg:38.81ms
step:719/1845 train_time:27927ms step_avg:38.84ms
step:720/1845 train_time:27988ms step_avg:38.87ms
step:721/1845 train_time:28051ms step_avg:38.91ms
step:722/1845 train_time:28112ms step_avg:38.94ms
step:723/1845 train_time:28174ms step_avg:38.97ms
step:724/1845 train_time:28236ms step_avg:39.00ms
step:725/1845 train_time:28298ms step_avg:39.03ms
step:726/1845 train_time:28359ms step_avg:39.06ms
step:727/1845 train_time:28421ms step_avg:39.09ms
step:728/1845 train_time:28483ms step_avg:39.12ms
step:729/1845 train_time:28545ms step_avg:39.16ms
step:730/1845 train_time:28607ms step_avg:39.19ms
step:731/1845 train_time:28670ms step_avg:39.22ms
step:732/1845 train_time:28731ms step_avg:39.25ms
step:733/1845 train_time:28793ms step_avg:39.28ms
step:734/1845 train_time:28854ms step_avg:39.31ms
step:735/1845 train_time:28916ms step_avg:39.34ms
step:736/1845 train_time:28978ms step_avg:39.37ms
step:737/1845 train_time:29040ms step_avg:39.40ms
step:738/1845 train_time:29102ms step_avg:39.43ms
step:739/1845 train_time:29164ms step_avg:39.46ms
step:740/1845 train_time:29224ms step_avg:39.49ms
step:741/1845 train_time:29287ms step_avg:39.52ms
step:742/1845 train_time:29348ms step_avg:39.55ms
step:743/1845 train_time:29411ms step_avg:39.58ms
step:744/1845 train_time:29472ms step_avg:39.61ms
step:745/1845 train_time:29534ms step_avg:39.64ms
step:746/1845 train_time:29595ms step_avg:39.67ms
step:747/1845 train_time:29657ms step_avg:39.70ms
step:748/1845 train_time:29719ms step_avg:39.73ms
step:749/1845 train_time:29781ms step_avg:39.76ms
step:750/1845 train_time:29843ms step_avg:39.79ms
step:750/1845 val_loss:4.0329 train_time:29907ms step_avg:39.88ms
step:751/1845 train_time:29925ms step_avg:39.85ms
step:752/1845 train_time:29970ms step_avg:39.85ms
step:753/1845 train_time:30034ms step_avg:39.89ms
step:754/1845 train_time:30099ms step_avg:39.92ms
step:755/1845 train_time:30162ms step_avg:39.95ms
step:756/1845 train_time:30223ms step_avg:39.98ms
step:757/1845 train_time:30284ms step_avg:40.01ms
step:758/1845 train_time:30345ms step_avg:40.03ms
step:759/1845 train_time:30406ms step_avg:40.06ms
step:760/1845 train_time:30468ms step_avg:40.09ms
step:761/1845 train_time:30529ms step_avg:40.12ms
step:762/1845 train_time:30590ms step_avg:40.14ms
step:763/1845 train_time:30651ms step_avg:40.17ms
step:764/1845 train_time:30712ms step_avg:40.20ms
step:765/1845 train_time:30773ms step_avg:40.23ms
step:766/1845 train_time:30834ms step_avg:40.25ms
step:767/1845 train_time:30897ms step_avg:40.28ms
step:768/1845 train_time:30959ms step_avg:40.31ms
step:769/1845 train_time:31022ms step_avg:40.34ms
step:770/1845 train_time:31084ms step_avg:40.37ms
step:771/1845 train_time:31148ms step_avg:40.40ms
step:772/1845 train_time:31210ms step_avg:40.43ms
step:773/1845 train_time:31272ms step_avg:40.46ms
step:774/1845 train_time:31334ms step_avg:40.48ms
step:775/1845 train_time:31396ms step_avg:40.51ms
step:776/1845 train_time:31457ms step_avg:40.54ms
step:777/1845 train_time:31520ms step_avg:40.57ms
step:778/1845 train_time:31581ms step_avg:40.59ms
step:779/1845 train_time:31643ms step_avg:40.62ms
step:780/1845 train_time:31703ms step_avg:40.65ms
step:781/1845 train_time:31765ms step_avg:40.67ms
step:782/1845 train_time:31826ms step_avg:40.70ms
step:783/1845 train_time:31888ms step_avg:40.73ms
step:784/1845 train_time:31949ms step_avg:40.75ms
step:785/1845 train_time:32012ms step_avg:40.78ms
step:786/1845 train_time:32074ms step_avg:40.81ms
step:787/1845 train_time:32138ms step_avg:40.84ms
step:788/1845 train_time:32199ms step_avg:40.86ms
step:789/1845 train_time:32262ms step_avg:40.89ms
step:790/1845 train_time:32322ms step_avg:40.91ms
step:791/1845 train_time:32384ms step_avg:40.94ms
step:792/1845 train_time:32446ms step_avg:40.97ms
step:793/1845 train_time:32508ms step_avg:40.99ms
step:794/1845 train_time:32569ms step_avg:41.02ms
step:795/1845 train_time:32630ms step_avg:41.04ms
step:796/1845 train_time:32691ms step_avg:41.07ms
step:797/1845 train_time:32753ms step_avg:41.10ms
step:798/1845 train_time:32814ms step_avg:41.12ms
step:799/1845 train_time:32876ms step_avg:41.15ms
step:800/1845 train_time:32938ms step_avg:41.17ms
step:801/1845 train_time:33000ms step_avg:41.20ms
step:802/1845 train_time:33061ms step_avg:41.22ms
step:803/1845 train_time:33124ms step_avg:41.25ms
step:804/1845 train_time:33185ms step_avg:41.27ms
step:805/1845 train_time:33247ms step_avg:41.30ms
step:806/1845 train_time:33309ms step_avg:41.33ms
step:807/1845 train_time:33371ms step_avg:41.35ms
step:808/1845 train_time:33433ms step_avg:41.38ms
step:809/1845 train_time:33495ms step_avg:41.40ms
step:810/1845 train_time:33556ms step_avg:41.43ms
step:811/1845 train_time:33618ms step_avg:41.45ms
step:812/1845 train_time:33679ms step_avg:41.48ms
step:813/1845 train_time:33741ms step_avg:41.50ms
step:814/1845 train_time:33802ms step_avg:41.53ms
step:815/1845 train_time:33865ms step_avg:41.55ms
step:816/1845 train_time:33926ms step_avg:41.58ms
step:817/1845 train_time:33988ms step_avg:41.60ms
step:818/1845 train_time:34049ms step_avg:41.63ms
step:819/1845 train_time:34112ms step_avg:41.65ms
step:820/1845 train_time:34174ms step_avg:41.68ms
step:821/1845 train_time:34236ms step_avg:41.70ms
step:822/1845 train_time:34298ms step_avg:41.72ms
step:823/1845 train_time:34360ms step_avg:41.75ms
step:824/1845 train_time:34421ms step_avg:41.77ms
step:825/1845 train_time:34483ms step_avg:41.80ms
step:826/1845 train_time:34545ms step_avg:41.82ms
step:827/1845 train_time:34606ms step_avg:41.85ms
step:828/1845 train_time:34668ms step_avg:41.87ms
step:829/1845 train_time:34730ms step_avg:41.89ms
step:830/1845 train_time:34791ms step_avg:41.92ms
step:831/1845 train_time:34853ms step_avg:41.94ms
step:832/1845 train_time:34914ms step_avg:41.96ms
step:833/1845 train_time:34977ms step_avg:41.99ms
step:834/1845 train_time:35038ms step_avg:42.01ms
step:835/1845 train_time:35100ms step_avg:42.04ms
step:836/1845 train_time:35161ms step_avg:42.06ms
step:837/1845 train_time:35224ms step_avg:42.08ms
step:838/1845 train_time:35286ms step_avg:42.11ms
step:839/1845 train_time:35347ms step_avg:42.13ms
step:840/1845 train_time:35409ms step_avg:42.15ms
step:841/1845 train_time:35471ms step_avg:42.18ms
step:842/1845 train_time:35532ms step_avg:42.20ms
step:843/1845 train_time:35594ms step_avg:42.22ms
step:844/1845 train_time:35655ms step_avg:42.25ms
step:845/1845 train_time:35717ms step_avg:42.27ms
step:846/1845 train_time:35778ms step_avg:42.29ms
step:847/1845 train_time:35840ms step_avg:42.31ms
step:848/1845 train_time:35901ms step_avg:42.34ms
step:849/1845 train_time:35963ms step_avg:42.36ms
step:850/1845 train_time:36025ms step_avg:42.38ms
step:851/1845 train_time:36087ms step_avg:42.41ms
step:852/1845 train_time:36149ms step_avg:42.43ms
step:853/1845 train_time:36211ms step_avg:42.45ms
step:854/1845 train_time:36272ms step_avg:42.47ms
step:855/1845 train_time:36335ms step_avg:42.50ms
step:856/1845 train_time:36396ms step_avg:42.52ms
step:857/1845 train_time:36459ms step_avg:42.54ms
step:858/1845 train_time:36520ms step_avg:42.56ms
step:859/1845 train_time:36582ms step_avg:42.59ms
step:860/1845 train_time:36644ms step_avg:42.61ms
step:861/1845 train_time:36706ms step_avg:42.63ms
step:862/1845 train_time:36768ms step_avg:42.65ms
step:863/1845 train_time:36830ms step_avg:42.68ms
step:864/1845 train_time:36891ms step_avg:42.70ms
step:865/1845 train_time:36953ms step_avg:42.72ms
step:866/1845 train_time:37014ms step_avg:42.74ms
step:867/1845 train_time:37077ms step_avg:42.76ms
step:868/1845 train_time:37138ms step_avg:42.79ms
step:869/1845 train_time:37200ms step_avg:42.81ms
step:870/1845 train_time:37261ms step_avg:42.83ms
step:871/1845 train_time:37323ms step_avg:42.85ms
step:872/1845 train_time:37385ms step_avg:42.87ms
step:873/1845 train_time:37447ms step_avg:42.89ms
step:874/1845 train_time:37508ms step_avg:42.92ms
step:875/1845 train_time:37571ms step_avg:42.94ms
step:876/1845 train_time:37632ms step_avg:42.96ms
step:877/1845 train_time:37694ms step_avg:42.98ms
step:878/1845 train_time:37755ms step_avg:43.00ms
step:879/1845 train_time:37817ms step_avg:43.02ms
step:880/1845 train_time:37878ms step_avg:43.04ms
step:881/1845 train_time:37940ms step_avg:43.07ms
step:882/1845 train_time:38002ms step_avg:43.09ms
step:883/1845 train_time:38064ms step_avg:43.11ms
step:884/1845 train_time:38124ms step_avg:43.13ms
step:885/1845 train_time:38187ms step_avg:43.15ms
step:886/1845 train_time:38248ms step_avg:43.17ms
step:887/1845 train_time:38310ms step_avg:43.19ms
step:888/1845 train_time:38372ms step_avg:43.21ms
step:889/1845 train_time:38434ms step_avg:43.23ms
step:890/1845 train_time:38496ms step_avg:43.25ms
step:891/1845 train_time:38559ms step_avg:43.28ms
step:892/1845 train_time:38620ms step_avg:43.30ms
step:893/1845 train_time:38682ms step_avg:43.32ms
step:894/1845 train_time:38743ms step_avg:43.34ms
step:895/1845 train_time:38805ms step_avg:43.36ms
step:896/1845 train_time:38867ms step_avg:43.38ms
step:897/1845 train_time:38930ms step_avg:43.40ms
step:898/1845 train_time:38991ms step_avg:43.42ms
step:899/1845 train_time:39053ms step_avg:43.44ms
step:900/1845 train_time:39115ms step_avg:43.46ms
step:901/1845 train_time:39177ms step_avg:43.48ms
step:902/1845 train_time:39238ms step_avg:43.50ms
step:903/1845 train_time:39301ms step_avg:43.52ms
step:904/1845 train_time:39362ms step_avg:43.54ms
step:905/1845 train_time:39424ms step_avg:43.56ms
step:906/1845 train_time:39486ms step_avg:43.58ms
step:907/1845 train_time:39548ms step_avg:43.60ms
step:908/1845 train_time:39609ms step_avg:43.62ms
step:909/1845 train_time:39672ms step_avg:43.64ms
step:910/1845 train_time:39733ms step_avg:43.66ms
step:911/1845 train_time:39797ms step_avg:43.68ms
step:912/1845 train_time:39858ms step_avg:43.70ms
step:913/1845 train_time:39920ms step_avg:43.72ms
step:914/1845 train_time:39981ms step_avg:43.74ms
step:915/1845 train_time:40043ms step_avg:43.76ms
step:916/1845 train_time:40105ms step_avg:43.78ms
step:917/1845 train_time:40167ms step_avg:43.80ms
step:918/1845 train_time:40228ms step_avg:43.82ms
step:919/1845 train_time:40291ms step_avg:43.84ms
step:920/1845 train_time:40352ms step_avg:43.86ms
step:921/1845 train_time:40414ms step_avg:43.88ms
step:922/1845 train_time:40476ms step_avg:43.90ms
step:923/1845 train_time:40539ms step_avg:43.92ms
step:924/1845 train_time:40600ms step_avg:43.94ms
step:925/1845 train_time:40662ms step_avg:43.96ms
step:926/1845 train_time:40723ms step_avg:43.98ms
step:927/1845 train_time:40785ms step_avg:44.00ms
step:928/1845 train_time:40846ms step_avg:44.01ms
step:929/1845 train_time:40908ms step_avg:44.03ms
step:930/1845 train_time:40969ms step_avg:44.05ms
step:931/1845 train_time:41032ms step_avg:44.07ms
step:932/1845 train_time:41093ms step_avg:44.09ms
step:933/1845 train_time:41156ms step_avg:44.11ms
step:934/1845 train_time:41217ms step_avg:44.13ms
step:935/1845 train_time:41279ms step_avg:44.15ms
step:936/1845 train_time:41340ms step_avg:44.17ms
step:937/1845 train_time:41402ms step_avg:44.19ms
step:938/1845 train_time:41464ms step_avg:44.21ms
step:939/1845 train_time:41526ms step_avg:44.22ms
step:940/1845 train_time:41587ms step_avg:44.24ms
step:941/1845 train_time:41649ms step_avg:44.26ms
step:942/1845 train_time:41710ms step_avg:44.28ms
step:943/1845 train_time:41773ms step_avg:44.30ms
step:944/1845 train_time:41835ms step_avg:44.32ms
step:945/1845 train_time:41897ms step_avg:44.34ms
step:946/1845 train_time:41958ms step_avg:44.35ms
step:947/1845 train_time:42020ms step_avg:44.37ms
step:948/1845 train_time:42081ms step_avg:44.39ms
step:949/1845 train_time:42143ms step_avg:44.41ms
step:950/1845 train_time:42205ms step_avg:44.43ms
step:951/1845 train_time:42267ms step_avg:44.44ms
step:952/1845 train_time:42328ms step_avg:44.46ms
step:953/1845 train_time:42390ms step_avg:44.48ms
step:954/1845 train_time:42452ms step_avg:44.50ms
step:955/1845 train_time:42515ms step_avg:44.52ms
step:956/1845 train_time:42576ms step_avg:44.54ms
step:957/1845 train_time:42639ms step_avg:44.55ms
step:958/1845 train_time:42700ms step_avg:44.57ms
step:959/1845 train_time:42762ms step_avg:44.59ms
step:960/1845 train_time:42824ms step_avg:44.61ms
step:961/1845 train_time:42886ms step_avg:44.63ms
step:962/1845 train_time:42948ms step_avg:44.64ms
step:963/1845 train_time:43010ms step_avg:44.66ms
step:964/1845 train_time:43071ms step_avg:44.68ms
step:965/1845 train_time:43133ms step_avg:44.70ms
step:966/1845 train_time:43194ms step_avg:44.71ms
step:967/1845 train_time:43256ms step_avg:44.73ms
step:968/1845 train_time:43317ms step_avg:44.75ms
step:969/1845 train_time:43380ms step_avg:44.77ms
step:970/1845 train_time:43441ms step_avg:44.78ms
step:971/1845 train_time:43503ms step_avg:44.80ms
step:972/1845 train_time:43565ms step_avg:44.82ms
step:973/1845 train_time:43626ms step_avg:44.84ms
step:974/1845 train_time:43688ms step_avg:44.85ms
step:975/1845 train_time:43751ms step_avg:44.87ms
step:976/1845 train_time:43812ms step_avg:44.89ms
step:977/1845 train_time:43874ms step_avg:44.91ms
step:978/1845 train_time:43935ms step_avg:44.92ms
step:979/1845 train_time:43998ms step_avg:44.94ms
step:980/1845 train_time:44059ms step_avg:44.96ms
step:981/1845 train_time:44122ms step_avg:44.98ms
step:982/1845 train_time:44183ms step_avg:44.99ms
step:983/1845 train_time:44246ms step_avg:45.01ms
step:984/1845 train_time:44307ms step_avg:45.03ms
step:985/1845 train_time:44369ms step_avg:45.04ms
step:986/1845 train_time:44430ms step_avg:45.06ms
step:987/1845 train_time:44492ms step_avg:45.08ms
step:988/1845 train_time:44554ms step_avg:45.10ms
step:989/1845 train_time:44617ms step_avg:45.11ms
step:990/1845 train_time:44678ms step_avg:45.13ms
step:991/1845 train_time:44741ms step_avg:45.15ms
step:992/1845 train_time:44802ms step_avg:45.16ms
step:993/1845 train_time:44864ms step_avg:45.18ms
step:994/1845 train_time:44926ms step_avg:45.20ms
step:995/1845 train_time:44988ms step_avg:45.21ms
step:996/1845 train_time:45050ms step_avg:45.23ms
step:997/1845 train_time:45112ms step_avg:45.25ms
step:998/1845 train_time:45173ms step_avg:45.26ms
step:999/1845 train_time:45235ms step_avg:45.28ms
step:1000/1845 train_time:45296ms step_avg:45.30ms
step:1000/1845 val_loss:3.7764 train_time:45360ms step_avg:45.36ms
step:1001/1845 train_time:45379ms step_avg:45.33ms
step:1002/1845 train_time:45421ms step_avg:45.33ms
step:1003/1845 train_time:45487ms step_avg:45.35ms
step:1004/1845 train_time:45552ms step_avg:45.37ms
step:1005/1845 train_time:45614ms step_avg:45.39ms
step:1006/1845 train_time:45675ms step_avg:45.40ms
step:1007/1845 train_time:45738ms step_avg:45.42ms
step:1008/1845 train_time:45799ms step_avg:45.44ms
step:1009/1845 train_time:45861ms step_avg:45.45ms
step:1010/1845 train_time:45922ms step_avg:45.47ms
step:1011/1845 train_time:45983ms step_avg:45.48ms
step:1012/1845 train_time:46044ms step_avg:45.50ms
step:1013/1845 train_time:46105ms step_avg:45.51ms
step:1014/1845 train_time:46166ms step_avg:45.53ms
step:1015/1845 train_time:46227ms step_avg:45.54ms
step:1016/1845 train_time:46289ms step_avg:45.56ms
step:1017/1845 train_time:46352ms step_avg:45.58ms
step:1018/1845 train_time:46415ms step_avg:45.59ms
step:1019/1845 train_time:46478ms step_avg:45.61ms
step:1020/1845 train_time:46540ms step_avg:45.63ms
step:1021/1845 train_time:46603ms step_avg:45.64ms
step:1022/1845 train_time:46665ms step_avg:45.66ms
step:1023/1845 train_time:46727ms step_avg:45.68ms
step:1024/1845 train_time:46789ms step_avg:45.69ms
step:1025/1845 train_time:46851ms step_avg:45.71ms
step:1026/1845 train_time:46912ms step_avg:45.72ms
step:1027/1845 train_time:46975ms step_avg:45.74ms
step:1028/1845 train_time:47035ms step_avg:45.75ms
step:1029/1845 train_time:47097ms step_avg:45.77ms
step:1030/1845 train_time:47157ms step_avg:45.78ms
step:1031/1845 train_time:47219ms step_avg:45.80ms
step:1032/1845 train_time:47280ms step_avg:45.81ms
step:1033/1845 train_time:47343ms step_avg:45.83ms
step:1034/1845 train_time:47404ms step_avg:45.85ms
step:1035/1845 train_time:47467ms step_avg:45.86ms
step:1036/1845 train_time:47530ms step_avg:45.88ms
step:1037/1845 train_time:47592ms step_avg:45.89ms
step:1038/1845 train_time:47654ms step_avg:45.91ms
step:1039/1845 train_time:47717ms step_avg:45.93ms
step:1040/1845 train_time:47779ms step_avg:45.94ms
step:1041/1845 train_time:47841ms step_avg:45.96ms
step:1042/1845 train_time:47902ms step_avg:45.97ms
step:1043/1845 train_time:47964ms step_avg:45.99ms
step:1044/1845 train_time:48025ms step_avg:46.00ms
step:1045/1845 train_time:48087ms step_avg:46.02ms
step:1046/1845 train_time:48149ms step_avg:46.03ms
step:1047/1845 train_time:48211ms step_avg:46.05ms
step:1048/1845 train_time:48273ms step_avg:46.06ms
step:1049/1845 train_time:48335ms step_avg:46.08ms
step:1050/1845 train_time:48396ms step_avg:46.09ms
step:1051/1845 train_time:48459ms step_avg:46.11ms
step:1052/1845 train_time:48520ms step_avg:46.12ms
step:1053/1845 train_time:48582ms step_avg:46.14ms
step:1054/1845 train_time:48644ms step_avg:46.15ms
step:1055/1845 train_time:48706ms step_avg:46.17ms
step:1056/1845 train_time:48768ms step_avg:46.18ms
step:1057/1845 train_time:48829ms step_avg:46.20ms
step:1058/1845 train_time:48891ms step_avg:46.21ms
step:1059/1845 train_time:48953ms step_avg:46.23ms
step:1060/1845 train_time:49014ms step_avg:46.24ms
step:1061/1845 train_time:49076ms step_avg:46.25ms
step:1062/1845 train_time:49138ms step_avg:46.27ms
step:1063/1845 train_time:49200ms step_avg:46.28ms
step:1064/1845 train_time:49261ms step_avg:46.30ms
step:1065/1845 train_time:49323ms step_avg:46.31ms
step:1066/1845 train_time:49384ms step_avg:46.33ms
step:1067/1845 train_time:49447ms step_avg:46.34ms
step:1068/1845 train_time:49508ms step_avg:46.36ms
step:1069/1845 train_time:49570ms step_avg:46.37ms
step:1070/1845 train_time:49632ms step_avg:46.38ms
step:1071/1845 train_time:49694ms step_avg:46.40ms
step:1072/1845 train_time:49754ms step_avg:46.41ms
step:1073/1845 train_time:49817ms step_avg:46.43ms
step:1074/1845 train_time:49878ms step_avg:46.44ms
step:1075/1845 train_time:49941ms step_avg:46.46ms
step:1076/1845 train_time:50002ms step_avg:46.47ms
step:1077/1845 train_time:50065ms step_avg:46.49ms
step:1078/1845 train_time:50126ms step_avg:46.50ms
step:1079/1845 train_time:50188ms step_avg:46.51ms
step:1080/1845 train_time:50250ms step_avg:46.53ms
step:1081/1845 train_time:50312ms step_avg:46.54ms
step:1082/1845 train_time:50374ms step_avg:46.56ms
step:1083/1845 train_time:50436ms step_avg:46.57ms
step:1084/1845 train_time:50497ms step_avg:46.58ms
step:1085/1845 train_time:50560ms step_avg:46.60ms
step:1086/1845 train_time:50621ms step_avg:46.61ms
step:1087/1845 train_time:50683ms step_avg:46.63ms
step:1088/1845 train_time:50745ms step_avg:46.64ms
step:1089/1845 train_time:50807ms step_avg:46.65ms
step:1090/1845 train_time:50869ms step_avg:46.67ms
step:1091/1845 train_time:50931ms step_avg:46.68ms
step:1092/1845 train_time:50992ms step_avg:46.70ms
step:1093/1845 train_time:51055ms step_avg:46.71ms
step:1094/1845 train_time:51116ms step_avg:46.72ms
step:1095/1845 train_time:51178ms step_avg:46.74ms
step:1096/1845 train_time:51240ms step_avg:46.75ms
step:1097/1845 train_time:51302ms step_avg:46.77ms
step:1098/1845 train_time:51364ms step_avg:46.78ms
step:1099/1845 train_time:51426ms step_avg:46.79ms
step:1100/1845 train_time:51487ms step_avg:46.81ms
step:1101/1845 train_time:51550ms step_avg:46.82ms
step:1102/1845 train_time:51611ms step_avg:46.83ms
step:1103/1845 train_time:51673ms step_avg:46.85ms
step:1104/1845 train_time:51735ms step_avg:46.86ms
step:1105/1845 train_time:51797ms step_avg:46.87ms
step:1106/1845 train_time:51858ms step_avg:46.89ms
step:1107/1845 train_time:51920ms step_avg:46.90ms
step:1108/1845 train_time:51982ms step_avg:46.91ms
step:1109/1845 train_time:52044ms step_avg:46.93ms
step:1110/1845 train_time:52106ms step_avg:46.94ms
step:1111/1845 train_time:52168ms step_avg:46.96ms
step:1112/1845 train_time:52230ms step_avg:46.97ms
step:1113/1845 train_time:52292ms step_avg:46.98ms
step:1114/1845 train_time:52353ms step_avg:47.00ms
step:1115/1845 train_time:52416ms step_avg:47.01ms
step:1116/1845 train_time:52478ms step_avg:47.02ms
step:1117/1845 train_time:52540ms step_avg:47.04ms
step:1118/1845 train_time:52601ms step_avg:47.05ms
step:1119/1845 train_time:52664ms step_avg:47.06ms
step:1120/1845 train_time:52724ms step_avg:47.08ms
step:1121/1845 train_time:52786ms step_avg:47.09ms
step:1122/1845 train_time:52848ms step_avg:47.10ms
step:1123/1845 train_time:52911ms step_avg:47.12ms
step:1124/1845 train_time:52972ms step_avg:47.13ms
step:1125/1845 train_time:53034ms step_avg:47.14ms
step:1126/1845 train_time:53096ms step_avg:47.15ms
step:1127/1845 train_time:53158ms step_avg:47.17ms
step:1128/1845 train_time:53219ms step_avg:47.18ms
step:1129/1845 train_time:53282ms step_avg:47.19ms
step:1130/1845 train_time:53343ms step_avg:47.21ms
step:1131/1845 train_time:53405ms step_avg:47.22ms
step:1132/1845 train_time:53466ms step_avg:47.23ms
step:1133/1845 train_time:53529ms step_avg:47.24ms
step:1134/1845 train_time:53591ms step_avg:47.26ms
step:1135/1845 train_time:53654ms step_avg:47.27ms
step:1136/1845 train_time:53714ms step_avg:47.28ms
step:1137/1845 train_time:53776ms step_avg:47.30ms
step:1138/1845 train_time:53838ms step_avg:47.31ms
step:1139/1845 train_time:53900ms step_avg:47.32ms
step:1140/1845 train_time:53962ms step_avg:47.33ms
step:1141/1845 train_time:54024ms step_avg:47.35ms
step:1142/1845 train_time:54084ms step_avg:47.36ms
step:1143/1845 train_time:54147ms step_avg:47.37ms
step:1144/1845 train_time:54209ms step_avg:47.39ms
step:1145/1845 train_time:54271ms step_avg:47.40ms
step:1146/1845 train_time:54333ms step_avg:47.41ms
step:1147/1845 train_time:54395ms step_avg:47.42ms
step:1148/1845 train_time:54456ms step_avg:47.44ms
step:1149/1845 train_time:54519ms step_avg:47.45ms
step:1150/1845 train_time:54580ms step_avg:47.46ms
step:1151/1845 train_time:54643ms step_avg:47.47ms
step:1152/1845 train_time:54704ms step_avg:47.49ms
step:1153/1845 train_time:54767ms step_avg:47.50ms
step:1154/1845 train_time:54828ms step_avg:47.51ms
step:1155/1845 train_time:54891ms step_avg:47.52ms
step:1156/1845 train_time:54952ms step_avg:47.54ms
step:1157/1845 train_time:55014ms step_avg:47.55ms
step:1158/1845 train_time:55075ms step_avg:47.56ms
step:1159/1845 train_time:55138ms step_avg:47.57ms
step:1160/1845 train_time:55199ms step_avg:47.59ms
step:1161/1845 train_time:55261ms step_avg:47.60ms
step:1162/1845 train_time:55322ms step_avg:47.61ms
step:1163/1845 train_time:55385ms step_avg:47.62ms
step:1164/1845 train_time:55445ms step_avg:47.63ms
step:1165/1845 train_time:55508ms step_avg:47.65ms
step:1166/1845 train_time:55569ms step_avg:47.66ms
step:1167/1845 train_time:55631ms step_avg:47.67ms
step:1168/1845 train_time:55693ms step_avg:47.68ms
step:1169/1845 train_time:55754ms step_avg:47.69ms
step:1170/1845 train_time:55815ms step_avg:47.71ms
step:1171/1845 train_time:55878ms step_avg:47.72ms
step:1172/1845 train_time:55939ms step_avg:47.73ms
step:1173/1845 train_time:56001ms step_avg:47.74ms
step:1174/1845 train_time:56063ms step_avg:47.75ms
step:1175/1845 train_time:56125ms step_avg:47.77ms
step:1176/1845 train_time:56186ms step_avg:47.78ms
step:1177/1845 train_time:56248ms step_avg:47.79ms
step:1178/1845 train_time:56310ms step_avg:47.80ms
step:1179/1845 train_time:56373ms step_avg:47.81ms
step:1180/1845 train_time:56434ms step_avg:47.83ms
step:1181/1845 train_time:56496ms step_avg:47.84ms
step:1182/1845 train_time:56557ms step_avg:47.85ms
step:1183/1845 train_time:56620ms step_avg:47.86ms
step:1184/1845 train_time:56681ms step_avg:47.87ms
step:1185/1845 train_time:56743ms step_avg:47.88ms
step:1186/1845 train_time:56804ms step_avg:47.90ms
step:1187/1845 train_time:56867ms step_avg:47.91ms
step:1188/1845 train_time:56929ms step_avg:47.92ms
step:1189/1845 train_time:56991ms step_avg:47.93ms
step:1190/1845 train_time:57052ms step_avg:47.94ms
step:1191/1845 train_time:57115ms step_avg:47.96ms
step:1192/1845 train_time:57176ms step_avg:47.97ms
step:1193/1845 train_time:57239ms step_avg:47.98ms
step:1194/1845 train_time:57300ms step_avg:47.99ms
step:1195/1845 train_time:57363ms step_avg:48.00ms
step:1196/1845 train_time:57424ms step_avg:48.01ms
step:1197/1845 train_time:57486ms step_avg:48.03ms
step:1198/1845 train_time:57548ms step_avg:48.04ms
step:1199/1845 train_time:57610ms step_avg:48.05ms
step:1200/1845 train_time:57672ms step_avg:48.06ms
step:1201/1845 train_time:57734ms step_avg:48.07ms
step:1202/1845 train_time:57795ms step_avg:48.08ms
step:1203/1845 train_time:57858ms step_avg:48.09ms
step:1204/1845 train_time:57919ms step_avg:48.11ms
step:1205/1845 train_time:57981ms step_avg:48.12ms
step:1206/1845 train_time:58068ms step_avg:48.15ms
step:1207/1845 train_time:58158ms step_avg:48.18ms
step:1208/1845 train_time:58246ms step_avg:48.22ms
step:1209/1845 train_time:58336ms step_avg:48.25ms
step:1210/1845 train_time:58424ms step_avg:48.28ms
step:1211/1845 train_time:58514ms step_avg:48.32ms
step:1212/1845 train_time:58601ms step_avg:48.35ms
step:1213/1845 train_time:58689ms step_avg:48.38ms
step:1214/1845 train_time:58776ms step_avg:48.42ms
step:1215/1845 train_time:58865ms step_avg:48.45ms
step:1216/1845 train_time:58953ms step_avg:48.48ms
step:1217/1845 train_time:59041ms step_avg:48.51ms
step:1218/1845 train_time:59128ms step_avg:48.55ms
step:1219/1845 train_time:59217ms step_avg:48.58ms
step:1220/1845 train_time:59305ms step_avg:48.61ms
step:1221/1845 train_time:59394ms step_avg:48.64ms
step:1222/1845 train_time:59482ms step_avg:48.68ms
step:1223/1845 train_time:59571ms step_avg:48.71ms
step:1224/1845 train_time:59658ms step_avg:48.74ms
step:1225/1845 train_time:59746ms step_avg:48.77ms
step:1226/1845 train_time:59833ms step_avg:48.80ms
step:1227/1845 train_time:59922ms step_avg:48.84ms
step:1228/1845 train_time:60009ms step_avg:48.87ms
step:1229/1845 train_time:60097ms step_avg:48.90ms
step:1230/1845 train_time:60185ms step_avg:48.93ms
step:1231/1845 train_time:60274ms step_avg:48.96ms
step:1232/1845 train_time:60362ms step_avg:49.00ms
step:1233/1845 train_time:60450ms step_avg:49.03ms
step:1234/1845 train_time:60539ms step_avg:49.06ms
step:1235/1845 train_time:60628ms step_avg:49.09ms
step:1236/1845 train_time:60715ms step_avg:49.12ms
step:1237/1845 train_time:60803ms step_avg:49.15ms
step:1238/1845 train_time:60890ms step_avg:49.18ms
step:1239/1845 train_time:60979ms step_avg:49.22ms
step:1240/1845 train_time:61066ms step_avg:49.25ms
step:1241/1845 train_time:61155ms step_avg:49.28ms
step:1242/1845 train_time:61243ms step_avg:49.31ms
step:1243/1845 train_time:61332ms step_avg:49.34ms
step:1244/1845 train_time:61420ms step_avg:49.37ms
step:1245/1845 train_time:61509ms step_avg:49.40ms
step:1246/1845 train_time:61596ms step_avg:49.44ms
step:1247/1845 train_time:61684ms step_avg:49.47ms
step:1248/1845 train_time:61772ms step_avg:49.50ms
step:1249/1845 train_time:61860ms step_avg:49.53ms
step:1250/1845 train_time:61948ms step_avg:49.56ms
step:1250/1845 val_loss:3.5389 train_time:62037ms step_avg:49.63ms
step:1251/1845 train_time:62057ms step_avg:49.61ms
step:1252/1845 train_time:62126ms step_avg:49.62ms
step:1253/1845 train_time:62217ms step_avg:49.65ms
step:1254/1845 train_time:62305ms step_avg:49.68ms
step:1255/1845 train_time:62393ms step_avg:49.72ms
step:1256/1845 train_time:62480ms step_avg:49.75ms
step:1257/1845 train_time:62567ms step_avg:49.78ms
step:1258/1845 train_time:62654ms step_avg:49.80ms
step:1259/1845 train_time:62743ms step_avg:49.84ms
step:1260/1845 train_time:62829ms step_avg:49.86ms
step:1261/1845 train_time:62917ms step_avg:49.89ms
step:1262/1845 train_time:63006ms step_avg:49.93ms
step:1263/1845 train_time:63097ms step_avg:49.96ms
step:1264/1845 train_time:63187ms step_avg:49.99ms
step:1265/1845 train_time:63277ms step_avg:50.02ms
step:1266/1845 train_time:63365ms step_avg:50.05ms
step:1267/1845 train_time:63453ms step_avg:50.08ms
step:1268/1845 train_time:63540ms step_avg:50.11ms
step:1269/1845 train_time:63628ms step_avg:50.14ms
step:1270/1845 train_time:63716ms step_avg:50.17ms
step:1271/1845 train_time:63805ms step_avg:50.20ms
step:1272/1845 train_time:63892ms step_avg:50.23ms
step:1273/1845 train_time:63980ms step_avg:50.26ms
step:1274/1845 train_time:64070ms step_avg:50.29ms
step:1275/1845 train_time:64160ms step_avg:50.32ms
step:1276/1845 train_time:64249ms step_avg:50.35ms
step:1277/1845 train_time:64338ms step_avg:50.38ms
step:1278/1845 train_time:64426ms step_avg:50.41ms
step:1279/1845 train_time:64513ms step_avg:50.44ms
step:1280/1845 train_time:64601ms step_avg:50.47ms
step:1281/1845 train_time:64689ms step_avg:50.50ms
step:1282/1845 train_time:64776ms step_avg:50.53ms
step:1283/1845 train_time:64864ms step_avg:50.56ms
step:1284/1845 train_time:64952ms step_avg:50.59ms
step:1285/1845 train_time:65042ms step_avg:50.62ms
step:1286/1845 train_time:65130ms step_avg:50.65ms
step:1287/1845 train_time:65219ms step_avg:50.68ms
step:1288/1845 train_time:65308ms step_avg:50.71ms
step:1289/1845 train_time:65398ms step_avg:50.74ms
step:1290/1845 train_time:65485ms step_avg:50.76ms
step:1291/1845 train_time:65573ms step_avg:50.79ms
step:1292/1845 train_time:65659ms step_avg:50.82ms
step:1293/1845 train_time:65748ms step_avg:50.85ms
step:1294/1845 train_time:65836ms step_avg:50.88ms
step:1295/1845 train_time:65924ms step_avg:50.91ms
step:1296/1845 train_time:66012ms step_avg:50.94ms
step:1297/1845 train_time:66102ms step_avg:50.97ms
step:1298/1845 train_time:66191ms step_avg:50.99ms
step:1299/1845 train_time:66278ms step_avg:51.02ms
step:1300/1845 train_time:66367ms step_avg:51.05ms
step:1301/1845 train_time:66455ms step_avg:51.08ms
step:1302/1845 train_time:66543ms step_avg:51.11ms
step:1303/1845 train_time:66631ms step_avg:51.14ms
step:1304/1845 train_time:66718ms step_avg:51.16ms
step:1305/1845 train_time:66805ms step_avg:51.19ms
step:1306/1845 train_time:66893ms step_avg:51.22ms
step:1307/1845 train_time:66982ms step_avg:51.25ms
step:1308/1845 train_time:67069ms step_avg:51.28ms
step:1309/1845 train_time:67158ms step_avg:51.30ms
step:1310/1845 train_time:67245ms step_avg:51.33ms
step:1311/1845 train_time:67334ms step_avg:51.36ms
step:1312/1845 train_time:67422ms step_avg:51.39ms
step:1313/1845 train_time:67510ms step_avg:51.42ms
step:1314/1845 train_time:67598ms step_avg:51.44ms
step:1315/1845 train_time:67686ms step_avg:51.47ms
step:1316/1845 train_time:67774ms step_avg:51.50ms
step:1317/1845 train_time:67862ms step_avg:51.53ms
step:1318/1845 train_time:67949ms step_avg:51.55ms
step:1319/1845 train_time:68037ms step_avg:51.58ms
step:1320/1845 train_time:68126ms step_avg:51.61ms
step:1321/1845 train_time:68214ms step_avg:51.64ms
step:1322/1845 train_time:68302ms step_avg:51.67ms
step:1323/1845 train_time:68391ms step_avg:51.69ms
step:1324/1845 train_time:68478ms step_avg:51.72ms
step:1325/1845 train_time:68567ms step_avg:51.75ms
step:1326/1845 train_time:68655ms step_avg:51.78ms
step:1327/1845 train_time:68743ms step_avg:51.80ms
step:1328/1845 train_time:68831ms step_avg:51.83ms
step:1329/1845 train_time:68919ms step_avg:51.86ms
step:1330/1845 train_time:69007ms step_avg:51.88ms
step:1331/1845 train_time:69095ms step_avg:51.91ms
step:1332/1845 train_time:69183ms step_avg:51.94ms
step:1333/1845 train_time:69271ms step_avg:51.97ms
step:1334/1845 train_time:69359ms step_avg:51.99ms
step:1335/1845 train_time:69447ms step_avg:52.02ms
step:1336/1845 train_time:69536ms step_avg:52.05ms
step:1337/1845 train_time:69624ms step_avg:52.07ms
step:1338/1845 train_time:69712ms step_avg:52.10ms
step:1339/1845 train_time:69800ms step_avg:52.13ms
step:1340/1845 train_time:69888ms step_avg:52.15ms
step:1341/1845 train_time:69976ms step_avg:52.18ms
step:1342/1845 train_time:70064ms step_avg:52.21ms
step:1343/1845 train_time:70152ms step_avg:52.24ms
step:1344/1845 train_time:70240ms step_avg:52.26ms
step:1345/1845 train_time:70329ms step_avg:52.29ms
step:1346/1845 train_time:70417ms step_avg:52.32ms
step:1347/1845 train_time:70506ms step_avg:52.34ms
step:1348/1845 train_time:70594ms step_avg:52.37ms
step:1349/1845 train_time:70682ms step_avg:52.40ms
step:1350/1845 train_time:70770ms step_avg:52.42ms
step:1351/1845 train_time:70858ms step_avg:52.45ms
step:1352/1845 train_time:70946ms step_avg:52.48ms
step:1353/1845 train_time:71034ms step_avg:52.50ms
step:1354/1845 train_time:71122ms step_avg:52.53ms
step:1355/1845 train_time:71211ms step_avg:52.55ms
step:1356/1845 train_time:71298ms step_avg:52.58ms
step:1357/1845 train_time:71387ms step_avg:52.61ms
step:1358/1845 train_time:71475ms step_avg:52.63ms
step:1359/1845 train_time:71564ms step_avg:52.66ms
step:1360/1845 train_time:71652ms step_avg:52.69ms
step:1361/1845 train_time:71740ms step_avg:52.71ms
step:1362/1845 train_time:71828ms step_avg:52.74ms
step:1363/1845 train_time:71917ms step_avg:52.76ms
step:1364/1845 train_time:72004ms step_avg:52.79ms
step:1365/1845 train_time:72093ms step_avg:52.82ms
step:1366/1845 train_time:72181ms step_avg:52.84ms
step:1367/1845 train_time:72269ms step_avg:52.87ms
step:1368/1845 train_time:72357ms step_avg:52.89ms
step:1369/1845 train_time:72445ms step_avg:52.92ms
step:1370/1845 train_time:72533ms step_avg:52.94ms
step:1371/1845 train_time:72622ms step_avg:52.97ms
step:1372/1845 train_time:72709ms step_avg:52.99ms
step:1373/1845 train_time:72798ms step_avg:53.02ms
step:1374/1845 train_time:72885ms step_avg:53.05ms
step:1375/1845 train_time:72973ms step_avg:53.07ms
step:1376/1845 train_time:73060ms step_avg:53.10ms
step:1377/1845 train_time:73149ms step_avg:53.12ms
step:1378/1845 train_time:73238ms step_avg:53.15ms
step:1379/1845 train_time:73325ms step_avg:53.17ms
step:1380/1845 train_time:73414ms step_avg:53.20ms
step:1381/1845 train_time:73502ms step_avg:53.22ms
step:1382/1845 train_time:73590ms step_avg:53.25ms
step:1383/1845 train_time:73680ms step_avg:53.28ms
step:1384/1845 train_time:73766ms step_avg:53.30ms
step:1385/1845 train_time:73855ms step_avg:53.32ms
step:1386/1845 train_time:73943ms step_avg:53.35ms
step:1387/1845 train_time:74031ms step_avg:53.37ms
step:1388/1845 train_time:74118ms step_avg:53.40ms
step:1389/1845 train_time:74207ms step_avg:53.42ms
step:1390/1845 train_time:74296ms step_avg:53.45ms
step:1391/1845 train_time:74384ms step_avg:53.48ms
step:1392/1845 train_time:74472ms step_avg:53.50ms
step:1393/1845 train_time:74561ms step_avg:53.53ms
step:1394/1845 train_time:74648ms step_avg:53.55ms
step:1395/1845 train_time:74736ms step_avg:53.57ms
step:1396/1845 train_time:74824ms step_avg:53.60ms
step:1397/1845 train_time:74912ms step_avg:53.62ms
step:1398/1845 train_time:74999ms step_avg:53.65ms
step:1399/1845 train_time:75088ms step_avg:53.67ms
step:1400/1845 train_time:75175ms step_avg:53.70ms
step:1401/1845 train_time:75264ms step_avg:53.72ms
step:1402/1845 train_time:75353ms step_avg:53.75ms
step:1403/1845 train_time:75441ms step_avg:53.77ms
step:1404/1845 train_time:75529ms step_avg:53.80ms
step:1405/1845 train_time:75617ms step_avg:53.82ms
step:1406/1845 train_time:75704ms step_avg:53.84ms
step:1407/1845 train_time:75793ms step_avg:53.87ms
step:1408/1845 train_time:75881ms step_avg:53.89ms
step:1409/1845 train_time:75970ms step_avg:53.92ms
step:1410/1845 train_time:76059ms step_avg:53.94ms
step:1411/1845 train_time:76146ms step_avg:53.97ms
step:1412/1845 train_time:76235ms step_avg:53.99ms
step:1413/1845 train_time:76323ms step_avg:54.01ms
step:1414/1845 train_time:76411ms step_avg:54.04ms
step:1415/1845 train_time:76500ms step_avg:54.06ms
step:1416/1845 train_time:76588ms step_avg:54.09ms
step:1417/1845 train_time:76676ms step_avg:54.11ms
step:1418/1845 train_time:76763ms step_avg:54.13ms
step:1419/1845 train_time:76851ms step_avg:54.16ms
step:1420/1845 train_time:76940ms step_avg:54.18ms
step:1421/1845 train_time:77029ms step_avg:54.21ms
step:1422/1845 train_time:77117ms step_avg:54.23ms
step:1423/1845 train_time:77205ms step_avg:54.26ms
step:1424/1845 train_time:77293ms step_avg:54.28ms
step:1425/1845 train_time:77382ms step_avg:54.30ms
step:1426/1845 train_time:77470ms step_avg:54.33ms
step:1427/1845 train_time:77558ms step_avg:54.35ms
step:1428/1845 train_time:77646ms step_avg:54.37ms
step:1429/1845 train_time:77734ms step_avg:54.40ms
step:1430/1845 train_time:77822ms step_avg:54.42ms
step:1431/1845 train_time:77910ms step_avg:54.44ms
step:1432/1845 train_time:77998ms step_avg:54.47ms
step:1433/1845 train_time:78086ms step_avg:54.49ms
step:1434/1845 train_time:78174ms step_avg:54.51ms
step:1435/1845 train_time:78263ms step_avg:54.54ms
step:1436/1845 train_time:78351ms step_avg:54.56ms
step:1437/1845 train_time:78439ms step_avg:54.59ms
step:1438/1845 train_time:78527ms step_avg:54.61ms
step:1439/1845 train_time:78615ms step_avg:54.63ms
step:1440/1845 train_time:78701ms step_avg:54.65ms
step:1441/1845 train_time:78790ms step_avg:54.68ms
step:1442/1845 train_time:78878ms step_avg:54.70ms
step:1443/1845 train_time:78967ms step_avg:54.72ms
step:1444/1845 train_time:79055ms step_avg:54.75ms
step:1445/1845 train_time:79142ms step_avg:54.77ms
step:1446/1845 train_time:79230ms step_avg:54.79ms
step:1447/1845 train_time:79318ms step_avg:54.82ms
step:1448/1845 train_time:79406ms step_avg:54.84ms
step:1449/1845 train_time:79495ms step_avg:54.86ms
step:1450/1845 train_time:79584ms step_avg:54.89ms
step:1451/1845 train_time:79674ms step_avg:54.91ms
step:1452/1845 train_time:79761ms step_avg:54.93ms
step:1453/1845 train_time:79849ms step_avg:54.95ms
step:1454/1845 train_time:79937ms step_avg:54.98ms
step:1455/1845 train_time:80026ms step_avg:55.00ms
step:1456/1845 train_time:80114ms step_avg:55.02ms
step:1457/1845 train_time:80202ms step_avg:55.05ms
step:1458/1845 train_time:80290ms step_avg:55.07ms
step:1459/1845 train_time:80378ms step_avg:55.09ms
step:1460/1845 train_time:80467ms step_avg:55.11ms
step:1461/1845 train_time:80556ms step_avg:55.14ms
step:1462/1845 train_time:80645ms step_avg:55.16ms
step:1463/1845 train_time:80732ms step_avg:55.18ms
step:1464/1845 train_time:80819ms step_avg:55.20ms
step:1465/1845 train_time:80907ms step_avg:55.23ms
step:1466/1845 train_time:80996ms step_avg:55.25ms
step:1467/1845 train_time:81085ms step_avg:55.27ms
step:1468/1845 train_time:81173ms step_avg:55.29ms
step:1469/1845 train_time:81260ms step_avg:55.32ms
step:1470/1845 train_time:81348ms step_avg:55.34ms
step:1471/1845 train_time:81436ms step_avg:55.36ms
step:1472/1845 train_time:81525ms step_avg:55.38ms
step:1473/1845 train_time:81613ms step_avg:55.41ms
step:1474/1845 train_time:81701ms step_avg:55.43ms
step:1475/1845 train_time:81789ms step_avg:55.45ms
step:1476/1845 train_time:81877ms step_avg:55.47ms
step:1477/1845 train_time:81965ms step_avg:55.49ms
step:1478/1845 train_time:82053ms step_avg:55.52ms
step:1479/1845 train_time:82142ms step_avg:55.54ms
step:1480/1845 train_time:82230ms step_avg:55.56ms
step:1481/1845 train_time:82320ms step_avg:55.58ms
step:1482/1845 train_time:82407ms step_avg:55.61ms
step:1483/1845 train_time:82496ms step_avg:55.63ms
step:1484/1845 train_time:82583ms step_avg:55.65ms
step:1485/1845 train_time:82673ms step_avg:55.67ms
step:1486/1845 train_time:82761ms step_avg:55.69ms
step:1487/1845 train_time:82849ms step_avg:55.72ms
step:1488/1845 train_time:82937ms step_avg:55.74ms
step:1489/1845 train_time:83025ms step_avg:55.76ms
step:1490/1845 train_time:83113ms step_avg:55.78ms
step:1491/1845 train_time:83201ms step_avg:55.80ms
step:1492/1845 train_time:83288ms step_avg:55.82ms
step:1493/1845 train_time:83378ms step_avg:55.85ms
step:1494/1845 train_time:83466ms step_avg:55.87ms
step:1495/1845 train_time:83556ms step_avg:55.89ms
step:1496/1845 train_time:83643ms step_avg:55.91ms
step:1497/1845 train_time:83732ms step_avg:55.93ms
step:1498/1845 train_time:83819ms step_avg:55.95ms
step:1499/1845 train_time:83907ms step_avg:55.98ms
step:1500/1845 train_time:83995ms step_avg:56.00ms
step:1500/1845 val_loss:3.4043 train_time:84086ms step_avg:56.06ms
step:1501/1845 train_time:84105ms step_avg:56.03ms
step:1502/1845 train_time:84178ms step_avg:56.04ms
step:1503/1845 train_time:84268ms step_avg:56.07ms
step:1504/1845 train_time:84355ms step_avg:56.09ms
step:1505/1845 train_time:84442ms step_avg:56.11ms
step:1506/1845 train_time:84529ms step_avg:56.13ms
step:1507/1845 train_time:84616ms step_avg:56.15ms
step:1508/1845 train_time:84704ms step_avg:56.17ms
step:1509/1845 train_time:84792ms step_avg:56.19ms
step:1510/1845 train_time:84878ms step_avg:56.21ms
step:1511/1845 train_time:84965ms step_avg:56.23ms
step:1512/1845 train_time:85055ms step_avg:56.25ms
step:1513/1845 train_time:85146ms step_avg:56.28ms
step:1514/1845 train_time:85235ms step_avg:56.30ms
step:1515/1845 train_time:85323ms step_avg:56.32ms
step:1516/1845 train_time:85411ms step_avg:56.34ms
step:1517/1845 train_time:85498ms step_avg:56.36ms
step:1518/1845 train_time:85585ms step_avg:56.38ms
step:1519/1845 train_time:85673ms step_avg:56.40ms
step:1520/1845 train_time:85760ms step_avg:56.42ms
step:1521/1845 train_time:85847ms step_avg:56.44ms
step:1522/1845 train_time:85934ms step_avg:56.46ms
step:1523/1845 train_time:86022ms step_avg:56.48ms
step:1524/1845 train_time:86112ms step_avg:56.50ms
step:1525/1845 train_time:86201ms step_avg:56.53ms
step:1526/1845 train_time:86290ms step_avg:56.55ms
step:1527/1845 train_time:86378ms step_avg:56.57ms
step:1528/1845 train_time:86466ms step_avg:56.59ms
step:1529/1845 train_time:86553ms step_avg:56.61ms
step:1530/1845 train_time:86641ms step_avg:56.63ms
step:1531/1845 train_time:86728ms step_avg:56.65ms
step:1532/1845 train_time:86815ms step_avg:56.67ms
step:1533/1845 train_time:86903ms step_avg:56.69ms
step:1534/1845 train_time:86992ms step_avg:56.71ms
step:1535/1845 train_time:87080ms step_avg:56.73ms
step:1536/1845 train_time:87169ms step_avg:56.75ms
step:1537/1845 train_time:87257ms step_avg:56.77ms
step:1538/1845 train_time:87346ms step_avg:56.79ms
step:1539/1845 train_time:87434ms step_avg:56.81ms
step:1540/1845 train_time:87522ms step_avg:56.83ms
step:1541/1845 train_time:87610ms step_avg:56.85ms
step:1542/1845 train_time:87697ms step_avg:56.87ms
step:1543/1845 train_time:87784ms step_avg:56.89ms
step:1544/1845 train_time:87872ms step_avg:56.91ms
step:1545/1845 train_time:87960ms step_avg:56.93ms
step:1546/1845 train_time:88048ms step_avg:56.95ms
step:1547/1845 train_time:88136ms step_avg:56.97ms
step:1548/1845 train_time:88225ms step_avg:56.99ms
step:1549/1845 train_time:88313ms step_avg:57.01ms
step:1550/1845 train_time:88401ms step_avg:57.03ms
step:1551/1845 train_time:88489ms step_avg:57.05ms
step:1552/1845 train_time:88577ms step_avg:57.07ms
step:1553/1845 train_time:88665ms step_avg:57.09ms
step:1554/1845 train_time:88752ms step_avg:57.11ms
step:1555/1845 train_time:88839ms step_avg:57.13ms
step:1556/1845 train_time:88928ms step_avg:57.15ms
step:1557/1845 train_time:89016ms step_avg:57.17ms
step:1558/1845 train_time:89103ms step_avg:57.19ms
step:1559/1845 train_time:89192ms step_avg:57.21ms
step:1560/1845 train_time:89281ms step_avg:57.23ms
step:1561/1845 train_time:89370ms step_avg:57.25ms
step:1562/1845 train_time:89458ms step_avg:57.27ms
step:1563/1845 train_time:89546ms step_avg:57.29ms
step:1564/1845 train_time:89634ms step_avg:57.31ms
step:1565/1845 train_time:89722ms step_avg:57.33ms
step:1566/1845 train_time:89809ms step_avg:57.35ms
step:1567/1845 train_time:89897ms step_avg:57.37ms
step:1568/1845 train_time:89984ms step_avg:57.39ms
step:1569/1845 train_time:90073ms step_avg:57.41ms
step:1570/1845 train_time:90161ms step_avg:57.43ms
step:1571/1845 train_time:90249ms step_avg:57.45ms
step:1572/1845 train_time:90336ms step_avg:57.47ms
step:1573/1845 train_time:90425ms step_avg:57.49ms
step:1574/1845 train_time:90513ms step_avg:57.50ms
step:1575/1845 train_time:90601ms step_avg:57.52ms
step:1576/1845 train_time:90688ms step_avg:57.54ms
step:1577/1845 train_time:90777ms step_avg:57.56ms
step:1578/1845 train_time:90865ms step_avg:57.58ms
step:1579/1845 train_time:90953ms step_avg:57.60ms
step:1580/1845 train_time:91040ms step_avg:57.62ms
step:1581/1845 train_time:91128ms step_avg:57.64ms
step:1582/1845 train_time:91216ms step_avg:57.66ms
step:1583/1845 train_time:91304ms step_avg:57.68ms
step:1584/1845 train_time:91394ms step_avg:57.70ms
step:1585/1845 train_time:91481ms step_avg:57.72ms
step:1586/1845 train_time:91569ms step_avg:57.74ms
step:1587/1845 train_time:91656ms step_avg:57.75ms
step:1588/1845 train_time:91745ms step_avg:57.77ms
step:1589/1845 train_time:91833ms step_avg:57.79ms
step:1590/1845 train_time:91921ms step_avg:57.81ms
step:1591/1845 train_time:92009ms step_avg:57.83ms
step:1592/1845 train_time:92097ms step_avg:57.85ms
step:1593/1845 train_time:92185ms step_avg:57.87ms
step:1594/1845 train_time:92273ms step_avg:57.89ms
step:1595/1845 train_time:92361ms step_avg:57.91ms
step:1596/1845 train_time:92448ms step_avg:57.92ms
step:1597/1845 train_time:92537ms step_avg:57.94ms
step:1598/1845 train_time:92625ms step_avg:57.96ms
step:1599/1845 train_time:92712ms step_avg:57.98ms
step:1600/1845 train_time:92800ms step_avg:58.00ms
step:1601/1845 train_time:92888ms step_avg:58.02ms
step:1602/1845 train_time:92975ms step_avg:58.04ms
step:1603/1845 train_time:93063ms step_avg:58.06ms
step:1604/1845 train_time:93151ms step_avg:58.07ms
step:1605/1845 train_time:93239ms step_avg:58.09ms
step:1606/1845 train_time:93328ms step_avg:58.11ms
step:1607/1845 train_time:93415ms step_avg:58.13ms
step:1608/1845 train_time:93503ms step_avg:58.15ms
step:1609/1845 train_time:93592ms step_avg:58.17ms
step:1610/1845 train_time:93679ms step_avg:58.19ms
step:1611/1845 train_time:93767ms step_avg:58.20ms
step:1612/1845 train_time:93854ms step_avg:58.22ms
step:1613/1845 train_time:93942ms step_avg:58.24ms
step:1614/1845 train_time:94030ms step_avg:58.26ms
step:1615/1845 train_time:94118ms step_avg:58.28ms
step:1616/1845 train_time:94206ms step_avg:58.30ms
step:1617/1845 train_time:94294ms step_avg:58.31ms
step:1618/1845 train_time:94382ms step_avg:58.33ms
step:1619/1845 train_time:94470ms step_avg:58.35ms
step:1620/1845 train_time:94557ms step_avg:58.37ms
step:1621/1845 train_time:94646ms step_avg:58.39ms
step:1622/1845 train_time:94733ms step_avg:58.41ms
step:1623/1845 train_time:94821ms step_avg:58.42ms
step:1624/1845 train_time:94910ms step_avg:58.44ms
step:1625/1845 train_time:94997ms step_avg:58.46ms
step:1626/1845 train_time:95085ms step_avg:58.48ms
step:1627/1845 train_time:95173ms step_avg:58.50ms
step:1628/1845 train_time:95261ms step_avg:58.51ms
step:1629/1845 train_time:95349ms step_avg:58.53ms
step:1630/1845 train_time:95436ms step_avg:58.55ms
step:1631/1845 train_time:95524ms step_avg:58.57ms
step:1632/1845 train_time:95612ms step_avg:58.59ms
step:1633/1845 train_time:95700ms step_avg:58.60ms
step:1634/1845 train_time:95788ms step_avg:58.62ms
step:1635/1845 train_time:95877ms step_avg:58.64ms
step:1636/1845 train_time:95964ms step_avg:58.66ms
step:1637/1845 train_time:96052ms step_avg:58.68ms
step:1638/1845 train_time:96140ms step_avg:58.69ms
step:1639/1845 train_time:96229ms step_avg:58.71ms
step:1640/1845 train_time:96316ms step_avg:58.73ms
step:1641/1845 train_time:96405ms step_avg:58.75ms
step:1642/1845 train_time:96493ms step_avg:58.77ms
step:1643/1845 train_time:96581ms step_avg:58.78ms
step:1644/1845 train_time:96669ms step_avg:58.80ms
step:1645/1845 train_time:96756ms step_avg:58.82ms
step:1646/1845 train_time:96844ms step_avg:58.84ms
step:1647/1845 train_time:96932ms step_avg:58.85ms
step:1648/1845 train_time:97020ms step_avg:58.87ms
step:1649/1845 train_time:97108ms step_avg:58.89ms
step:1650/1845 train_time:97195ms step_avg:58.91ms
step:1651/1845 train_time:97284ms step_avg:58.92ms
step:1652/1845 train_time:97372ms step_avg:58.94ms
step:1653/1845 train_time:97460ms step_avg:58.96ms
step:1654/1845 train_time:97548ms step_avg:58.98ms
step:1655/1845 train_time:97635ms step_avg:58.99ms
step:1656/1845 train_time:97723ms step_avg:59.01ms
step:1657/1845 train_time:97811ms step_avg:59.03ms
step:1658/1845 train_time:97899ms step_avg:59.05ms
step:1659/1845 train_time:97987ms step_avg:59.06ms
step:1660/1845 train_time:98075ms step_avg:59.08ms
step:1661/1845 train_time:98164ms step_avg:59.10ms
step:1662/1845 train_time:98251ms step_avg:59.12ms
step:1663/1845 train_time:98339ms step_avg:59.13ms
step:1664/1845 train_time:98427ms step_avg:59.15ms
step:1665/1845 train_time:98515ms step_avg:59.17ms
step:1666/1845 train_time:98603ms step_avg:59.19ms
step:1667/1845 train_time:98692ms step_avg:59.20ms
step:1668/1845 train_time:98780ms step_avg:59.22ms
step:1669/1845 train_time:98867ms step_avg:59.24ms
step:1670/1845 train_time:98955ms step_avg:59.25ms
step:1671/1845 train_time:99043ms step_avg:59.27ms
step:1672/1845 train_time:99130ms step_avg:59.29ms
step:1673/1845 train_time:99218ms step_avg:59.31ms
step:1674/1845 train_time:99307ms step_avg:59.32ms
step:1675/1845 train_time:99395ms step_avg:59.34ms
step:1676/1845 train_time:99483ms step_avg:59.36ms
step:1677/1845 train_time:99572ms step_avg:59.37ms
step:1678/1845 train_time:99659ms step_avg:59.39ms
step:1679/1845 train_time:99747ms step_avg:59.41ms
step:1680/1845 train_time:99835ms step_avg:59.43ms
step:1681/1845 train_time:99923ms step_avg:59.44ms
step:1682/1845 train_time:100010ms step_avg:59.46ms
step:1683/1845 train_time:100098ms step_avg:59.48ms
step:1684/1845 train_time:100186ms step_avg:59.49ms
step:1685/1845 train_time:100274ms step_avg:59.51ms
step:1686/1845 train_time:100364ms step_avg:59.53ms
step:1687/1845 train_time:100452ms step_avg:59.54ms
step:1688/1845 train_time:100539ms step_avg:59.56ms
step:1689/1845 train_time:100626ms step_avg:59.58ms
step:1690/1845 train_time:100714ms step_avg:59.59ms
step:1691/1845 train_time:100802ms step_avg:59.61ms
step:1692/1845 train_time:100890ms step_avg:59.63ms
step:1693/1845 train_time:100978ms step_avg:59.64ms
step:1694/1845 train_time:101067ms step_avg:59.66ms
step:1695/1845 train_time:101154ms step_avg:59.68ms
step:1696/1845 train_time:101241ms step_avg:59.69ms
step:1697/1845 train_time:101330ms step_avg:59.71ms
step:1698/1845 train_time:101417ms step_avg:59.73ms
step:1699/1845 train_time:101505ms step_avg:59.74ms
step:1700/1845 train_time:101593ms step_avg:59.76ms
step:1701/1845 train_time:101682ms step_avg:59.78ms
step:1702/1845 train_time:101769ms step_avg:59.79ms
step:1703/1845 train_time:101857ms step_avg:59.81ms
step:1704/1845 train_time:101945ms step_avg:59.83ms
step:1705/1845 train_time:102033ms step_avg:59.84ms
step:1706/1845 train_time:102121ms step_avg:59.86ms
step:1707/1845 train_time:102208ms step_avg:59.88ms
step:1708/1845 train_time:102295ms step_avg:59.89ms
step:1709/1845 train_time:102384ms step_avg:59.91ms
step:1710/1845 train_time:102472ms step_avg:59.93ms
step:1711/1845 train_time:102561ms step_avg:59.94ms
step:1712/1845 train_time:102648ms step_avg:59.96ms
step:1713/1845 train_time:102737ms step_avg:59.97ms
step:1714/1845 train_time:102825ms step_avg:59.99ms
step:1715/1845 train_time:102912ms step_avg:60.01ms
step:1716/1845 train_time:103000ms step_avg:60.02ms
step:1717/1845 train_time:103088ms step_avg:60.04ms
step:1718/1845 train_time:103176ms step_avg:60.06ms
step:1719/1845 train_time:103264ms step_avg:60.07ms
step:1720/1845 train_time:103352ms step_avg:60.09ms
step:1721/1845 train_time:103440ms step_avg:60.10ms
step:1722/1845 train_time:103527ms step_avg:60.12ms
step:1723/1845 train_time:103616ms step_avg:60.14ms
step:1724/1845 train_time:103704ms step_avg:60.15ms
step:1725/1845 train_time:103795ms step_avg:60.17ms
step:1726/1845 train_time:103881ms step_avg:60.19ms
step:1727/1845 train_time:103969ms step_avg:60.20ms
step:1728/1845 train_time:104057ms step_avg:60.22ms
step:1729/1845 train_time:104145ms step_avg:60.23ms
step:1730/1845 train_time:104233ms step_avg:60.25ms
step:1731/1845 train_time:104322ms step_avg:60.27ms
step:1732/1845 train_time:104409ms step_avg:60.28ms
step:1733/1845 train_time:104497ms step_avg:60.30ms
step:1734/1845 train_time:104585ms step_avg:60.31ms
step:1735/1845 train_time:104673ms step_avg:60.33ms
step:1736/1845 train_time:104760ms step_avg:60.35ms
step:1737/1845 train_time:104849ms step_avg:60.36ms
step:1738/1845 train_time:104936ms step_avg:60.38ms
step:1739/1845 train_time:105025ms step_avg:60.39ms
step:1740/1845 train_time:105113ms step_avg:60.41ms
step:1741/1845 train_time:105201ms step_avg:60.43ms
step:1742/1845 train_time:105290ms step_avg:60.44ms
step:1743/1845 train_time:105379ms step_avg:60.46ms
step:1744/1845 train_time:105466ms step_avg:60.47ms
step:1745/1845 train_time:105555ms step_avg:60.49ms
step:1746/1845 train_time:105642ms step_avg:60.51ms
step:1747/1845 train_time:105731ms step_avg:60.52ms
step:1748/1845 train_time:105818ms step_avg:60.54ms
step:1749/1845 train_time:105907ms step_avg:60.55ms
step:1750/1845 train_time:105994ms step_avg:60.57ms
step:1750/1845 val_loss:3.3052 train_time:106084ms step_avg:60.62ms
step:1751/1845 train_time:106110ms step_avg:60.60ms
step:1752/1845 train_time:106173ms step_avg:60.60ms
step:1753/1845 train_time:106265ms step_avg:60.62ms
step:1754/1845 train_time:106352ms step_avg:60.63ms
step:1755/1845 train_time:106440ms step_avg:60.65ms
step:1756/1845 train_time:106527ms step_avg:60.66ms
step:1757/1845 train_time:106615ms step_avg:60.68ms
step:1758/1845 train_time:106701ms step_avg:60.69ms
step:1759/1845 train_time:106789ms step_avg:60.71ms
step:1760/1845 train_time:106876ms step_avg:60.72ms
step:1761/1845 train_time:106964ms step_avg:60.74ms
step:1762/1845 train_time:107055ms step_avg:60.76ms
step:1763/1845 train_time:107146ms step_avg:60.77ms
step:1764/1845 train_time:107236ms step_avg:60.79ms
step:1765/1845 train_time:107324ms step_avg:60.81ms
step:1766/1845 train_time:107413ms step_avg:60.82ms
step:1767/1845 train_time:107501ms step_avg:60.84ms
step:1768/1845 train_time:107587ms step_avg:60.85ms
step:1769/1845 train_time:107675ms step_avg:60.87ms
step:1770/1845 train_time:107761ms step_avg:60.88ms
step:1771/1845 train_time:107849ms step_avg:60.90ms
step:1772/1845 train_time:107936ms step_avg:60.91ms
step:1773/1845 train_time:108026ms step_avg:60.93ms
step:1774/1845 train_time:108115ms step_avg:60.94ms
step:1775/1845 train_time:108204ms step_avg:60.96ms
step:1776/1845 train_time:108293ms step_avg:60.98ms
step:1777/1845 train_time:108381ms step_avg:60.99ms
step:1778/1845 train_time:108468ms step_avg:61.01ms
step:1779/1845 train_time:108556ms step_avg:61.02ms
step:1780/1845 train_time:108642ms step_avg:61.04ms
step:1781/1845 train_time:108731ms step_avg:61.05ms
step:1782/1845 train_time:108818ms step_avg:61.07ms
step:1783/1845 train_time:108907ms step_avg:61.08ms
step:1784/1845 train_time:108995ms step_avg:61.10ms
step:1785/1845 train_time:109084ms step_avg:61.11ms
step:1786/1845 train_time:109172ms step_avg:61.13ms
step:1787/1845 train_time:109262ms step_avg:61.14ms
step:1788/1845 train_time:109351ms step_avg:61.16ms
step:1789/1845 train_time:109439ms step_avg:61.17ms
step:1790/1845 train_time:109526ms step_avg:61.19ms
step:1791/1845 train_time:109614ms step_avg:61.20ms
step:1792/1845 train_time:109701ms step_avg:61.22ms
step:1793/1845 train_time:109790ms step_avg:61.23ms
step:1794/1845 train_time:109877ms step_avg:61.25ms
step:1795/1845 train_time:109966ms step_avg:61.26ms
step:1796/1845 train_time:110053ms step_avg:61.28ms
step:1797/1845 train_time:110142ms step_avg:61.29ms
step:1798/1845 train_time:110230ms step_avg:61.31ms
step:1799/1845 train_time:110319ms step_avg:61.32ms
step:1800/1845 train_time:110407ms step_avg:61.34ms
step:1801/1845 train_time:110496ms step_avg:61.35ms
step:1802/1845 train_time:110583ms step_avg:61.37ms
step:1803/1845 train_time:110672ms step_avg:61.38ms
step:1804/1845 train_time:110759ms step_avg:61.40ms
step:1805/1845 train_time:110846ms step_avg:61.41ms
step:1806/1845 train_time:110935ms step_avg:61.43ms
step:1807/1845 train_time:111024ms step_avg:61.44ms
step:1808/1845 train_time:111112ms step_avg:61.46ms
step:1809/1845 train_time:111201ms step_avg:61.47ms
step:1810/1845 train_time:111289ms step_avg:61.49ms
step:1811/1845 train_time:111378ms step_avg:61.50ms
step:1812/1845 train_time:111465ms step_avg:61.51ms
step:1813/1845 train_time:111554ms step_avg:61.53ms
step:1814/1845 train_time:111642ms step_avg:61.54ms
step:1815/1845 train_time:111730ms step_avg:61.56ms
step:1816/1845 train_time:111817ms step_avg:61.57ms
step:1817/1845 train_time:111906ms step_avg:61.59ms
step:1818/1845 train_time:111994ms step_avg:61.60ms
step:1819/1845 train_time:112084ms step_avg:61.62ms
step:1820/1845 train_time:112173ms step_avg:61.63ms
step:1821/1845 train_time:112262ms step_avg:61.65ms
step:1822/1845 train_time:112350ms step_avg:61.66ms
step:1823/1845 train_time:112440ms step_avg:61.68ms
step:1824/1845 train_time:112527ms step_avg:61.69ms
step:1825/1845 train_time:112616ms step_avg:61.71ms
step:1826/1845 train_time:112704ms step_avg:61.72ms
step:1827/1845 train_time:112792ms step_avg:61.74ms
step:1828/1845 train_time:112880ms step_avg:61.75ms
step:1829/1845 train_time:112969ms step_avg:61.77ms
step:1830/1845 train_time:113056ms step_avg:61.78ms
step:1831/1845 train_time:113147ms step_avg:61.80ms
step:1832/1845 train_time:113235ms step_avg:61.81ms
step:1833/1845 train_time:113324ms step_avg:61.82ms
step:1834/1845 train_time:113413ms step_avg:61.84ms
step:1835/1845 train_time:113502ms step_avg:61.85ms
step:1836/1845 train_time:113589ms step_avg:61.87ms
step:1837/1845 train_time:113679ms step_avg:61.88ms
step:1838/1845 train_time:113766ms step_avg:61.90ms
step:1839/1845 train_time:113855ms step_avg:61.91ms
step:1840/1845 train_time:113943ms step_avg:61.93ms
step:1841/1845 train_time:114033ms step_avg:61.94ms
step:1842/1845 train_time:114122ms step_avg:61.96ms
step:1843/1845 train_time:114211ms step_avg:61.97ms
step:1844/1845 train_time:114299ms step_avg:61.98ms
step:1845/1845 train_time:114389ms step_avg:62.00ms
step:1845/1845 val_loss:3.2788 train_time:114476ms step_avg:62.05ms
peak memory allocated: 29709 MiB reserved: 44258 MiB

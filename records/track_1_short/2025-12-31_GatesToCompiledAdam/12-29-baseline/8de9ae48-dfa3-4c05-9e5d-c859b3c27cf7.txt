import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 18:10:27 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   26C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    162238      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    162239      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    162240      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    162241      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    162242      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    162243      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    162244      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    162245      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8313 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:70ms step_avg:70.05ms
step:2/1845 train_time:92ms step_avg:45.89ms
step:3/1845 train_time:122ms step_avg:40.80ms
step:4/1845 train_time:156ms step_avg:39.12ms
step:5/1845 train_time:190ms step_avg:38.10ms
step:6/1845 train_time:268ms step_avg:44.75ms
step:7/1845 train_time:426ms step_avg:60.80ms
step:8/1845 train_time:460ms step_avg:57.46ms
step:9/1845 train_time:494ms step_avg:54.85ms
step:10/1845 train_time:528ms step_avg:52.77ms
step:11/1845 train_time:562ms step_avg:51.07ms
step:12/1845 train_time:596ms step_avg:49.67ms
step:13/1845 train_time:630ms step_avg:48.46ms
step:14/1845 train_time:664ms step_avg:47.45ms
step:15/1845 train_time:698ms step_avg:46.55ms
step:16/1845 train_time:733ms step_avg:45.78ms
step:17/1845 train_time:766ms step_avg:45.09ms
step:18/1845 train_time:801ms step_avg:44.49ms
step:19/1845 train_time:835ms step_avg:43.94ms
step:20/1845 train_time:869ms step_avg:43.46ms
step:21/1845 train_time:903ms step_avg:43.01ms
step:22/1845 train_time:937ms step_avg:42.61ms
step:23/1845 train_time:971ms step_avg:42.24ms
step:24/1845 train_time:1006ms step_avg:41.91ms
step:25/1845 train_time:1040ms step_avg:41.59ms
step:26/1845 train_time:1074ms step_avg:41.30ms
step:27/1845 train_time:1108ms step_avg:41.04ms
step:28/1845 train_time:1142ms step_avg:40.80ms
step:29/1845 train_time:1176ms step_avg:40.57ms
step:30/1845 train_time:1211ms step_avg:40.36ms
step:31/1845 train_time:1245ms step_avg:40.16ms
step:32/1845 train_time:1279ms step_avg:39.98ms
step:33/1845 train_time:1313ms step_avg:39.80ms
step:34/1845 train_time:1348ms step_avg:39.63ms
step:35/1845 train_time:1382ms step_avg:39.48ms
step:36/1845 train_time:1416ms step_avg:39.34ms
step:37/1845 train_time:1450ms step_avg:39.20ms
step:38/1845 train_time:1485ms step_avg:39.08ms
step:39/1845 train_time:1519ms step_avg:38.95ms
step:40/1845 train_time:1553ms step_avg:38.84ms
step:41/1845 train_time:1588ms step_avg:38.72ms
step:42/1845 train_time:1622ms step_avg:38.62ms
step:43/1845 train_time:1656ms step_avg:38.52ms
step:44/1845 train_time:1691ms step_avg:38.42ms
step:45/1845 train_time:1725ms step_avg:38.32ms
step:46/1845 train_time:1759ms step_avg:38.24ms
step:47/1845 train_time:1793ms step_avg:38.15ms
step:48/1845 train_time:1828ms step_avg:38.08ms
step:49/1845 train_time:1862ms step_avg:38.00ms
step:50/1845 train_time:1896ms step_avg:37.92ms
step:51/1845 train_time:1931ms step_avg:37.86ms
step:52/1845 train_time:1965ms step_avg:37.80ms
step:53/1845 train_time:2000ms step_avg:37.73ms
step:54/1845 train_time:2034ms step_avg:37.67ms
step:55/1845 train_time:2068ms step_avg:37.60ms
step:56/1845 train_time:2102ms step_avg:37.54ms
step:57/1845 train_time:2136ms step_avg:37.48ms
step:58/1845 train_time:2171ms step_avg:37.43ms
step:59/1845 train_time:2205ms step_avg:37.37ms
step:60/1845 train_time:2239ms step_avg:37.32ms
step:61/1845 train_time:2273ms step_avg:37.27ms
step:62/1845 train_time:2308ms step_avg:37.22ms
step:63/1845 train_time:2342ms step_avg:37.17ms
step:64/1845 train_time:2376ms step_avg:37.13ms
step:65/1845 train_time:2410ms step_avg:37.08ms
step:66/1845 train_time:2445ms step_avg:37.04ms
step:67/1845 train_time:2478ms step_avg:36.99ms
step:68/1845 train_time:2513ms step_avg:36.95ms
step:69/1845 train_time:2547ms step_avg:36.92ms
step:70/1845 train_time:2582ms step_avg:36.88ms
step:71/1845 train_time:2616ms step_avg:36.84ms
step:72/1845 train_time:2650ms step_avg:36.81ms
step:73/1845 train_time:2684ms step_avg:36.77ms
step:74/1845 train_time:2719ms step_avg:36.74ms
step:75/1845 train_time:2753ms step_avg:36.70ms
step:76/1845 train_time:2787ms step_avg:36.67ms
step:77/1845 train_time:2821ms step_avg:36.64ms
step:78/1845 train_time:2856ms step_avg:36.61ms
step:79/1845 train_time:2890ms step_avg:36.59ms
step:80/1845 train_time:2925ms step_avg:36.56ms
step:81/1845 train_time:2959ms step_avg:36.53ms
step:82/1845 train_time:2993ms step_avg:36.50ms
step:83/1845 train_time:3027ms step_avg:36.47ms
step:84/1845 train_time:3061ms step_avg:36.45ms
step:85/1845 train_time:3096ms step_avg:36.42ms
step:86/1845 train_time:3130ms step_avg:36.40ms
step:87/1845 train_time:3164ms step_avg:36.37ms
step:88/1845 train_time:3198ms step_avg:36.34ms
step:89/1845 train_time:3233ms step_avg:36.32ms
step:90/1845 train_time:3267ms step_avg:36.30ms
step:91/1845 train_time:3301ms step_avg:36.28ms
step:92/1845 train_time:3335ms step_avg:36.25ms
step:93/1845 train_time:3370ms step_avg:36.23ms
step:94/1845 train_time:3404ms step_avg:36.21ms
step:95/1845 train_time:3438ms step_avg:36.19ms
step:96/1845 train_time:3472ms step_avg:36.17ms
step:97/1845 train_time:3506ms step_avg:36.15ms
step:98/1845 train_time:3541ms step_avg:36.13ms
step:99/1845 train_time:3575ms step_avg:36.11ms
step:100/1845 train_time:3609ms step_avg:36.09ms
step:101/1845 train_time:3643ms step_avg:36.07ms
step:102/1845 train_time:3678ms step_avg:36.06ms
step:103/1845 train_time:3712ms step_avg:36.04ms
step:104/1845 train_time:3746ms step_avg:36.02ms
step:105/1845 train_time:3780ms step_avg:36.00ms
step:106/1845 train_time:3814ms step_avg:35.98ms
step:107/1845 train_time:3848ms step_avg:35.96ms
step:108/1845 train_time:3882ms step_avg:35.95ms
step:109/1845 train_time:3916ms step_avg:35.93ms
step:110/1845 train_time:3951ms step_avg:35.91ms
step:111/1845 train_time:3985ms step_avg:35.90ms
step:112/1845 train_time:4019ms step_avg:35.88ms
step:113/1845 train_time:4053ms step_avg:35.87ms
step:114/1845 train_time:4087ms step_avg:35.85ms
step:115/1845 train_time:4121ms step_avg:35.84ms
step:116/1845 train_time:4155ms step_avg:35.82ms
step:117/1845 train_time:4189ms step_avg:35.81ms
step:118/1845 train_time:4224ms step_avg:35.79ms
step:119/1845 train_time:4258ms step_avg:35.78ms
step:120/1845 train_time:4292ms step_avg:35.77ms
step:121/1845 train_time:4326ms step_avg:35.75ms
step:122/1845 train_time:4361ms step_avg:35.74ms
step:123/1845 train_time:4395ms step_avg:35.73ms
step:124/1845 train_time:4429ms step_avg:35.72ms
step:125/1845 train_time:4463ms step_avg:35.71ms
step:126/1845 train_time:4498ms step_avg:35.70ms
step:127/1845 train_time:4532ms step_avg:35.68ms
step:128/1845 train_time:4566ms step_avg:35.68ms
step:129/1845 train_time:4601ms step_avg:35.66ms
step:130/1845 train_time:4635ms step_avg:35.65ms
step:131/1845 train_time:4669ms step_avg:35.64ms
step:132/1845 train_time:4703ms step_avg:35.63ms
step:133/1845 train_time:4737ms step_avg:35.62ms
step:134/1845 train_time:4771ms step_avg:35.61ms
step:135/1845 train_time:4806ms step_avg:35.60ms
step:136/1845 train_time:4840ms step_avg:35.59ms
step:137/1845 train_time:4874ms step_avg:35.58ms
step:138/1845 train_time:4908ms step_avg:35.57ms
step:139/1845 train_time:4942ms step_avg:35.56ms
step:140/1845 train_time:4976ms step_avg:35.55ms
step:141/1845 train_time:5011ms step_avg:35.54ms
step:142/1845 train_time:5045ms step_avg:35.53ms
step:143/1845 train_time:5079ms step_avg:35.52ms
step:144/1845 train_time:5113ms step_avg:35.51ms
step:145/1845 train_time:5147ms step_avg:35.50ms
step:146/1845 train_time:5181ms step_avg:35.49ms
step:147/1845 train_time:5215ms step_avg:35.48ms
step:148/1845 train_time:5250ms step_avg:35.47ms
step:149/1845 train_time:5284ms step_avg:35.46ms
step:150/1845 train_time:5318ms step_avg:35.45ms
step:151/1845 train_time:5352ms step_avg:35.44ms
step:152/1845 train_time:5386ms step_avg:35.43ms
step:153/1845 train_time:5420ms step_avg:35.42ms
step:154/1845 train_time:5454ms step_avg:35.42ms
step:155/1845 train_time:5488ms step_avg:35.41ms
step:156/1845 train_time:5523ms step_avg:35.40ms
step:157/1845 train_time:5557ms step_avg:35.39ms
step:158/1845 train_time:5591ms step_avg:35.39ms
step:159/1845 train_time:5625ms step_avg:35.38ms
step:160/1845 train_time:5659ms step_avg:35.37ms
step:161/1845 train_time:5694ms step_avg:35.36ms
step:162/1845 train_time:5728ms step_avg:35.36ms
step:163/1845 train_time:5762ms step_avg:35.35ms
step:164/1845 train_time:5796ms step_avg:35.34ms
step:165/1845 train_time:5830ms step_avg:35.34ms
step:166/1845 train_time:5865ms step_avg:35.33ms
step:167/1845 train_time:5899ms step_avg:35.32ms
step:168/1845 train_time:5933ms step_avg:35.32ms
step:169/1845 train_time:5967ms step_avg:35.31ms
step:170/1845 train_time:6002ms step_avg:35.30ms
step:171/1845 train_time:6036ms step_avg:35.30ms
step:172/1845 train_time:6070ms step_avg:35.29ms
step:173/1845 train_time:6104ms step_avg:35.28ms
step:174/1845 train_time:6138ms step_avg:35.28ms
step:175/1845 train_time:6172ms step_avg:35.27ms
step:176/1845 train_time:6206ms step_avg:35.26ms
step:177/1845 train_time:6241ms step_avg:35.26ms
step:178/1845 train_time:6275ms step_avg:35.25ms
step:179/1845 train_time:6309ms step_avg:35.25ms
step:180/1845 train_time:6343ms step_avg:35.24ms
step:181/1845 train_time:6377ms step_avg:35.23ms
step:182/1845 train_time:6411ms step_avg:35.23ms
step:183/1845 train_time:6446ms step_avg:35.22ms
step:184/1845 train_time:6480ms step_avg:35.22ms
step:185/1845 train_time:6514ms step_avg:35.21ms
step:186/1845 train_time:6548ms step_avg:35.20ms
step:187/1845 train_time:6582ms step_avg:35.20ms
step:188/1845 train_time:6616ms step_avg:35.19ms
step:189/1845 train_time:6651ms step_avg:35.19ms
step:190/1845 train_time:6685ms step_avg:35.18ms
step:191/1845 train_time:6719ms step_avg:35.18ms
step:192/1845 train_time:6753ms step_avg:35.17ms
step:193/1845 train_time:6787ms step_avg:35.17ms
step:194/1845 train_time:6821ms step_avg:35.16ms
step:195/1845 train_time:6856ms step_avg:35.16ms
step:196/1845 train_time:6890ms step_avg:35.15ms
step:197/1845 train_time:6924ms step_avg:35.15ms
step:198/1845 train_time:6958ms step_avg:35.14ms
step:199/1845 train_time:6992ms step_avg:35.14ms
step:200/1845 train_time:7027ms step_avg:35.13ms
step:201/1845 train_time:7061ms step_avg:35.13ms
step:202/1845 train_time:7095ms step_avg:35.12ms
step:203/1845 train_time:7129ms step_avg:35.12ms
step:204/1845 train_time:7163ms step_avg:35.11ms
step:205/1845 train_time:7197ms step_avg:35.11ms
step:206/1845 train_time:7232ms step_avg:35.10ms
step:207/1845 train_time:7265ms step_avg:35.10ms
step:208/1845 train_time:7300ms step_avg:35.09ms
step:209/1845 train_time:7334ms step_avg:35.09ms
step:210/1845 train_time:7368ms step_avg:35.09ms
step:211/1845 train_time:7402ms step_avg:35.08ms
step:212/1845 train_time:7436ms step_avg:35.08ms
step:213/1845 train_time:7470ms step_avg:35.07ms
step:214/1845 train_time:7504ms step_avg:35.07ms
step:215/1845 train_time:7539ms step_avg:35.06ms
step:216/1845 train_time:7573ms step_avg:35.06ms
step:217/1845 train_time:7607ms step_avg:35.06ms
step:218/1845 train_time:7641ms step_avg:35.05ms
step:219/1845 train_time:7675ms step_avg:35.05ms
step:220/1845 train_time:7710ms step_avg:35.04ms
step:221/1845 train_time:7744ms step_avg:35.04ms
step:222/1845 train_time:7778ms step_avg:35.04ms
step:223/1845 train_time:7812ms step_avg:35.03ms
step:224/1845 train_time:7846ms step_avg:35.03ms
step:225/1845 train_time:7880ms step_avg:35.02ms
step:226/1845 train_time:7914ms step_avg:35.02ms
step:227/1845 train_time:7948ms step_avg:35.01ms
step:228/1845 train_time:7982ms step_avg:35.01ms
step:229/1845 train_time:8017ms step_avg:35.01ms
step:230/1845 train_time:8050ms step_avg:35.00ms
step:231/1845 train_time:8084ms step_avg:35.00ms
step:232/1845 train_time:8118ms step_avg:34.99ms
step:233/1845 train_time:8152ms step_avg:34.99ms
step:234/1845 train_time:8186ms step_avg:34.98ms
step:235/1845 train_time:8220ms step_avg:34.98ms
step:236/1845 train_time:8255ms step_avg:34.98ms
step:237/1845 train_time:8289ms step_avg:34.97ms
step:238/1845 train_time:8323ms step_avg:34.97ms
step:239/1845 train_time:8357ms step_avg:34.97ms
step:240/1845 train_time:8391ms step_avg:34.96ms
step:241/1845 train_time:8425ms step_avg:34.96ms
step:242/1845 train_time:8459ms step_avg:34.96ms
step:243/1845 train_time:8493ms step_avg:34.95ms
step:244/1845 train_time:8527ms step_avg:34.95ms
step:245/1845 train_time:8561ms step_avg:34.94ms
step:246/1845 train_time:8596ms step_avg:34.94ms
step:247/1845 train_time:8630ms step_avg:34.94ms
step:248/1845 train_time:8664ms step_avg:34.93ms
step:249/1845 train_time:8698ms step_avg:34.93ms
step:250/1845 train_time:8732ms step_avg:34.93ms
step:250/1845 val_loss:4.6139 train_time:8768ms step_avg:35.07ms
step:251/1845 train_time:8786ms step_avg:35.01ms
step:252/1845 train_time:8807ms step_avg:34.95ms
step:253/1845 train_time:8842ms step_avg:34.95ms
step:254/1845 train_time:8877ms step_avg:34.95ms
step:255/1845 train_time:8911ms step_avg:34.95ms
step:256/1845 train_time:8946ms step_avg:34.95ms
step:257/1845 train_time:8980ms step_avg:34.94ms
step:258/1845 train_time:9015ms step_avg:34.94ms
step:259/1845 train_time:9049ms step_avg:34.94ms
step:260/1845 train_time:9083ms step_avg:34.93ms
step:261/1845 train_time:9117ms step_avg:34.93ms
step:262/1845 train_time:9151ms step_avg:34.93ms
step:263/1845 train_time:9185ms step_avg:34.92ms
step:264/1845 train_time:9219ms step_avg:34.92ms
step:265/1845 train_time:9253ms step_avg:34.92ms
step:266/1845 train_time:9288ms step_avg:34.92ms
step:267/1845 train_time:9322ms step_avg:34.91ms
step:268/1845 train_time:9356ms step_avg:34.91ms
step:269/1845 train_time:9390ms step_avg:34.91ms
step:270/1845 train_time:9424ms step_avg:34.90ms
step:271/1845 train_time:9458ms step_avg:34.90ms
step:272/1845 train_time:9492ms step_avg:34.90ms
step:273/1845 train_time:9526ms step_avg:34.89ms
step:274/1845 train_time:9560ms step_avg:34.89ms
step:275/1845 train_time:9594ms step_avg:34.89ms
step:276/1845 train_time:9628ms step_avg:34.88ms
step:277/1845 train_time:9662ms step_avg:34.88ms
step:278/1845 train_time:9696ms step_avg:34.88ms
step:279/1845 train_time:9730ms step_avg:34.88ms
step:280/1845 train_time:9764ms step_avg:34.87ms
step:281/1845 train_time:9799ms step_avg:34.87ms
step:282/1845 train_time:9833ms step_avg:34.87ms
step:283/1845 train_time:9867ms step_avg:34.87ms
step:284/1845 train_time:9901ms step_avg:34.86ms
step:285/1845 train_time:9935ms step_avg:34.86ms
step:286/1845 train_time:9969ms step_avg:34.86ms
step:287/1845 train_time:10003ms step_avg:34.86ms
step:288/1845 train_time:10038ms step_avg:34.85ms
step:289/1845 train_time:10072ms step_avg:34.85ms
step:290/1845 train_time:10107ms step_avg:34.85ms
step:291/1845 train_time:10141ms step_avg:34.85ms
step:292/1845 train_time:10175ms step_avg:34.84ms
step:293/1845 train_time:10209ms step_avg:34.84ms
step:294/1845 train_time:10243ms step_avg:34.84ms
step:295/1845 train_time:10277ms step_avg:34.84ms
step:296/1845 train_time:10311ms step_avg:34.83ms
step:297/1845 train_time:10345ms step_avg:34.83ms
step:298/1845 train_time:10379ms step_avg:34.83ms
step:299/1845 train_time:10413ms step_avg:34.83ms
step:300/1845 train_time:10447ms step_avg:34.82ms
step:301/1845 train_time:10481ms step_avg:34.82ms
step:302/1845 train_time:10516ms step_avg:34.82ms
step:303/1845 train_time:10549ms step_avg:34.82ms
step:304/1845 train_time:10584ms step_avg:34.81ms
step:305/1845 train_time:10617ms step_avg:34.81ms
step:306/1845 train_time:10652ms step_avg:34.81ms
step:307/1845 train_time:10685ms step_avg:34.81ms
step:308/1845 train_time:10720ms step_avg:34.80ms
step:309/1845 train_time:10753ms step_avg:34.80ms
step:310/1845 train_time:10788ms step_avg:34.80ms
step:311/1845 train_time:10822ms step_avg:34.80ms
step:312/1845 train_time:10856ms step_avg:34.79ms
step:313/1845 train_time:10890ms step_avg:34.79ms
step:314/1845 train_time:10924ms step_avg:34.79ms
step:315/1845 train_time:10958ms step_avg:34.79ms
step:316/1845 train_time:10992ms step_avg:34.78ms
step:317/1845 train_time:11026ms step_avg:34.78ms
step:318/1845 train_time:11060ms step_avg:34.78ms
step:319/1845 train_time:11095ms step_avg:34.78ms
step:320/1845 train_time:11129ms step_avg:34.78ms
step:321/1845 train_time:11163ms step_avg:34.77ms
step:322/1845 train_time:11197ms step_avg:34.77ms
step:323/1845 train_time:11231ms step_avg:34.77ms
step:324/1845 train_time:11265ms step_avg:34.77ms
step:325/1845 train_time:11299ms step_avg:34.77ms
step:326/1845 train_time:11333ms step_avg:34.76ms
step:327/1845 train_time:11367ms step_avg:34.76ms
step:328/1845 train_time:11402ms step_avg:34.76ms
step:329/1845 train_time:11436ms step_avg:34.76ms
step:330/1845 train_time:11470ms step_avg:34.76ms
step:331/1845 train_time:11504ms step_avg:34.76ms
step:332/1845 train_time:11538ms step_avg:34.75ms
step:333/1845 train_time:11572ms step_avg:34.75ms
step:334/1845 train_time:11606ms step_avg:34.75ms
step:335/1845 train_time:11641ms step_avg:34.75ms
step:336/1845 train_time:11675ms step_avg:34.75ms
step:337/1845 train_time:11709ms step_avg:34.74ms
step:338/1845 train_time:11743ms step_avg:34.74ms
step:339/1845 train_time:11777ms step_avg:34.74ms
step:340/1845 train_time:11811ms step_avg:34.74ms
step:341/1845 train_time:11845ms step_avg:34.74ms
step:342/1845 train_time:11879ms step_avg:34.73ms
step:343/1845 train_time:11913ms step_avg:34.73ms
step:344/1845 train_time:11947ms step_avg:34.73ms
step:345/1845 train_time:11981ms step_avg:34.73ms
step:346/1845 train_time:12016ms step_avg:34.73ms
step:347/1845 train_time:12049ms step_avg:34.72ms
step:348/1845 train_time:12084ms step_avg:34.72ms
step:349/1845 train_time:12118ms step_avg:34.72ms
step:350/1845 train_time:12152ms step_avg:34.72ms
step:351/1845 train_time:12186ms step_avg:34.72ms
step:352/1845 train_time:12220ms step_avg:34.72ms
step:353/1845 train_time:12254ms step_avg:34.71ms
step:354/1845 train_time:12288ms step_avg:34.71ms
step:355/1845 train_time:12323ms step_avg:34.71ms
step:356/1845 train_time:12357ms step_avg:34.71ms
step:357/1845 train_time:12391ms step_avg:34.71ms
step:358/1845 train_time:12425ms step_avg:34.71ms
step:359/1845 train_time:12459ms step_avg:34.71ms
step:360/1845 train_time:12493ms step_avg:34.70ms
step:361/1845 train_time:12527ms step_avg:34.70ms
step:362/1845 train_time:12562ms step_avg:34.70ms
step:363/1845 train_time:12596ms step_avg:34.70ms
step:364/1845 train_time:12630ms step_avg:34.70ms
step:365/1845 train_time:12664ms step_avg:34.70ms
step:366/1845 train_time:12698ms step_avg:34.69ms
step:367/1845 train_time:12732ms step_avg:34.69ms
step:368/1845 train_time:12766ms step_avg:34.69ms
step:369/1845 train_time:12801ms step_avg:34.69ms
step:370/1845 train_time:12835ms step_avg:34.69ms
step:371/1845 train_time:12869ms step_avg:34.69ms
step:372/1845 train_time:12903ms step_avg:34.69ms
step:373/1845 train_time:12937ms step_avg:34.68ms
step:374/1845 train_time:12971ms step_avg:34.68ms
step:375/1845 train_time:13005ms step_avg:34.68ms
step:376/1845 train_time:13039ms step_avg:34.68ms
step:377/1845 train_time:13073ms step_avg:34.68ms
step:378/1845 train_time:13107ms step_avg:34.68ms
step:379/1845 train_time:13141ms step_avg:34.67ms
step:380/1845 train_time:13175ms step_avg:34.67ms
step:381/1845 train_time:13210ms step_avg:34.67ms
step:382/1845 train_time:13244ms step_avg:34.67ms
step:383/1845 train_time:13278ms step_avg:34.67ms
step:384/1845 train_time:13312ms step_avg:34.67ms
step:385/1845 train_time:13346ms step_avg:34.67ms
step:386/1845 train_time:13380ms step_avg:34.66ms
step:387/1845 train_time:13415ms step_avg:34.66ms
step:388/1845 train_time:13449ms step_avg:34.66ms
step:389/1845 train_time:13483ms step_avg:34.66ms
step:390/1845 train_time:13517ms step_avg:34.66ms
step:391/1845 train_time:13551ms step_avg:34.66ms
step:392/1845 train_time:13585ms step_avg:34.66ms
step:393/1845 train_time:13619ms step_avg:34.65ms
step:394/1845 train_time:13653ms step_avg:34.65ms
step:395/1845 train_time:13687ms step_avg:34.65ms
step:396/1845 train_time:13722ms step_avg:34.65ms
step:397/1845 train_time:13756ms step_avg:34.65ms
step:398/1845 train_time:13790ms step_avg:34.65ms
step:399/1845 train_time:13824ms step_avg:34.65ms
step:400/1845 train_time:13859ms step_avg:34.65ms
step:401/1845 train_time:13893ms step_avg:34.64ms
step:402/1845 train_time:13927ms step_avg:34.64ms
step:403/1845 train_time:13961ms step_avg:34.64ms
step:404/1845 train_time:13995ms step_avg:34.64ms
step:405/1845 train_time:14029ms step_avg:34.64ms
step:406/1845 train_time:14063ms step_avg:34.64ms
step:407/1845 train_time:14097ms step_avg:34.64ms
step:408/1845 train_time:14131ms step_avg:34.64ms
step:409/1845 train_time:14165ms step_avg:34.63ms
step:410/1845 train_time:14200ms step_avg:34.63ms
step:411/1845 train_time:14234ms step_avg:34.63ms
step:412/1845 train_time:14268ms step_avg:34.63ms
step:413/1845 train_time:14302ms step_avg:34.63ms
step:414/1845 train_time:14336ms step_avg:34.63ms
step:415/1845 train_time:14370ms step_avg:34.63ms
step:416/1845 train_time:14404ms step_avg:34.63ms
step:417/1845 train_time:14439ms step_avg:34.62ms
step:418/1845 train_time:14473ms step_avg:34.62ms
step:419/1845 train_time:14507ms step_avg:34.62ms
step:420/1845 train_time:14541ms step_avg:34.62ms
step:421/1845 train_time:14575ms step_avg:34.62ms
step:422/1845 train_time:14609ms step_avg:34.62ms
step:423/1845 train_time:14643ms step_avg:34.62ms
step:424/1845 train_time:14677ms step_avg:34.62ms
step:425/1845 train_time:14711ms step_avg:34.61ms
step:426/1845 train_time:14746ms step_avg:34.61ms
step:427/1845 train_time:14780ms step_avg:34.61ms
step:428/1845 train_time:14814ms step_avg:34.61ms
step:429/1845 train_time:14848ms step_avg:34.61ms
step:430/1845 train_time:14882ms step_avg:34.61ms
step:431/1845 train_time:14916ms step_avg:34.61ms
step:432/1845 train_time:14950ms step_avg:34.61ms
step:433/1845 train_time:14984ms step_avg:34.60ms
step:434/1845 train_time:15018ms step_avg:34.60ms
step:435/1845 train_time:15052ms step_avg:34.60ms
step:436/1845 train_time:15086ms step_avg:34.60ms
step:437/1845 train_time:15120ms step_avg:34.60ms
step:438/1845 train_time:15154ms step_avg:34.60ms
step:439/1845 train_time:15188ms step_avg:34.60ms
step:440/1845 train_time:15222ms step_avg:34.60ms
step:441/1845 train_time:15256ms step_avg:34.59ms
step:442/1845 train_time:15290ms step_avg:34.59ms
step:443/1845 train_time:15324ms step_avg:34.59ms
step:444/1845 train_time:15359ms step_avg:34.59ms
step:445/1845 train_time:15393ms step_avg:34.59ms
step:446/1845 train_time:15427ms step_avg:34.59ms
step:447/1845 train_time:15461ms step_avg:34.59ms
step:448/1845 train_time:15495ms step_avg:34.59ms
step:449/1845 train_time:15529ms step_avg:34.59ms
step:450/1845 train_time:15563ms step_avg:34.58ms
step:451/1845 train_time:15597ms step_avg:34.58ms
step:452/1845 train_time:15631ms step_avg:34.58ms
step:453/1845 train_time:15666ms step_avg:34.58ms
step:454/1845 train_time:15700ms step_avg:34.58ms
step:455/1845 train_time:15734ms step_avg:34.58ms
step:456/1845 train_time:15768ms step_avg:34.58ms
step:457/1845 train_time:15802ms step_avg:34.58ms
step:458/1845 train_time:15836ms step_avg:34.58ms
step:459/1845 train_time:15871ms step_avg:34.58ms
step:460/1845 train_time:15905ms step_avg:34.58ms
step:461/1845 train_time:15939ms step_avg:34.58ms
step:462/1845 train_time:15973ms step_avg:34.57ms
step:463/1845 train_time:16007ms step_avg:34.57ms
step:464/1845 train_time:16041ms step_avg:34.57ms
step:465/1845 train_time:16075ms step_avg:34.57ms
step:466/1845 train_time:16109ms step_avg:34.57ms
step:467/1845 train_time:16143ms step_avg:34.57ms
step:468/1845 train_time:16177ms step_avg:34.57ms
step:469/1845 train_time:16211ms step_avg:34.57ms
step:470/1845 train_time:16245ms step_avg:34.56ms
step:471/1845 train_time:16280ms step_avg:34.56ms
step:472/1845 train_time:16314ms step_avg:34.56ms
step:473/1845 train_time:16348ms step_avg:34.56ms
step:474/1845 train_time:16382ms step_avg:34.56ms
step:475/1845 train_time:16416ms step_avg:34.56ms
step:476/1845 train_time:16450ms step_avg:34.56ms
step:477/1845 train_time:16484ms step_avg:34.56ms
step:478/1845 train_time:16519ms step_avg:34.56ms
step:479/1845 train_time:16552ms step_avg:34.56ms
step:480/1845 train_time:16587ms step_avg:34.56ms
step:481/1845 train_time:16621ms step_avg:34.55ms
step:482/1845 train_time:16655ms step_avg:34.55ms
step:483/1845 train_time:16689ms step_avg:34.55ms
step:484/1845 train_time:16724ms step_avg:34.55ms
step:485/1845 train_time:16758ms step_avg:34.55ms
step:486/1845 train_time:16792ms step_avg:34.55ms
step:487/1845 train_time:16826ms step_avg:34.55ms
step:488/1845 train_time:16860ms step_avg:34.55ms
step:489/1845 train_time:16894ms step_avg:34.55ms
step:490/1845 train_time:16928ms step_avg:34.55ms
step:491/1845 train_time:16962ms step_avg:34.55ms
step:492/1845 train_time:16996ms step_avg:34.55ms
step:493/1845 train_time:17030ms step_avg:34.54ms
step:494/1845 train_time:17064ms step_avg:34.54ms
step:495/1845 train_time:17098ms step_avg:34.54ms
step:496/1845 train_time:17133ms step_avg:34.54ms
step:497/1845 train_time:17167ms step_avg:34.54ms
step:498/1845 train_time:17201ms step_avg:34.54ms
step:499/1845 train_time:17235ms step_avg:34.54ms
step:500/1845 train_time:17269ms step_avg:34.54ms
step:500/1845 val_loss:4.2894 train_time:17305ms step_avg:34.61ms
step:501/1845 train_time:17324ms step_avg:34.58ms
step:502/1845 train_time:17342ms step_avg:34.55ms
step:503/1845 train_time:17375ms step_avg:34.54ms
step:504/1845 train_time:17409ms step_avg:34.54ms
step:505/1845 train_time:17445ms step_avg:34.54ms
step:506/1845 train_time:17480ms step_avg:34.54ms
step:507/1845 train_time:17515ms step_avg:34.55ms
step:508/1845 train_time:17549ms step_avg:34.55ms
step:509/1845 train_time:17584ms step_avg:34.55ms
step:510/1845 train_time:17618ms step_avg:34.55ms
step:511/1845 train_time:17652ms step_avg:34.54ms
step:512/1845 train_time:17686ms step_avg:34.54ms
step:513/1845 train_time:17720ms step_avg:34.54ms
step:514/1845 train_time:17754ms step_avg:34.54ms
step:515/1845 train_time:17788ms step_avg:34.54ms
step:516/1845 train_time:17823ms step_avg:34.54ms
step:517/1845 train_time:17856ms step_avg:34.54ms
step:518/1845 train_time:17890ms step_avg:34.54ms
step:519/1845 train_time:17924ms step_avg:34.54ms
step:520/1845 train_time:17958ms step_avg:34.54ms
step:521/1845 train_time:17992ms step_avg:34.53ms
step:522/1845 train_time:18026ms step_avg:34.53ms
step:523/1845 train_time:18060ms step_avg:34.53ms
step:524/1845 train_time:18094ms step_avg:34.53ms
step:525/1845 train_time:18128ms step_avg:34.53ms
step:526/1845 train_time:18163ms step_avg:34.53ms
step:527/1845 train_time:18197ms step_avg:34.53ms
step:528/1845 train_time:18231ms step_avg:34.53ms
step:529/1845 train_time:18264ms step_avg:34.53ms
step:530/1845 train_time:18298ms step_avg:34.53ms
step:531/1845 train_time:18333ms step_avg:34.52ms
step:532/1845 train_time:18367ms step_avg:34.52ms
step:533/1845 train_time:18401ms step_avg:34.52ms
step:534/1845 train_time:18435ms step_avg:34.52ms
step:535/1845 train_time:18469ms step_avg:34.52ms
step:536/1845 train_time:18503ms step_avg:34.52ms
step:537/1845 train_time:18537ms step_avg:34.52ms
step:538/1845 train_time:18571ms step_avg:34.52ms
step:539/1845 train_time:18605ms step_avg:34.52ms
step:540/1845 train_time:18640ms step_avg:34.52ms
step:541/1845 train_time:18674ms step_avg:34.52ms
step:542/1845 train_time:18708ms step_avg:34.52ms
step:543/1845 train_time:18742ms step_avg:34.52ms
step:544/1845 train_time:18776ms step_avg:34.52ms
step:545/1845 train_time:18810ms step_avg:34.51ms
step:546/1845 train_time:18844ms step_avg:34.51ms
step:547/1845 train_time:18879ms step_avg:34.51ms
step:548/1845 train_time:18913ms step_avg:34.51ms
step:549/1845 train_time:18947ms step_avg:34.51ms
step:550/1845 train_time:18981ms step_avg:34.51ms
step:551/1845 train_time:19014ms step_avg:34.51ms
step:552/1845 train_time:19049ms step_avg:34.51ms
step:553/1845 train_time:19083ms step_avg:34.51ms
step:554/1845 train_time:19117ms step_avg:34.51ms
step:555/1845 train_time:19151ms step_avg:34.51ms
step:556/1845 train_time:19185ms step_avg:34.51ms
step:557/1845 train_time:19219ms step_avg:34.50ms
step:558/1845 train_time:19253ms step_avg:34.50ms
step:559/1845 train_time:19287ms step_avg:34.50ms
step:560/1845 train_time:19321ms step_avg:34.50ms
step:561/1845 train_time:19355ms step_avg:34.50ms
step:562/1845 train_time:19389ms step_avg:34.50ms
step:563/1845 train_time:19423ms step_avg:34.50ms
step:564/1845 train_time:19457ms step_avg:34.50ms
step:565/1845 train_time:19491ms step_avg:34.50ms
step:566/1845 train_time:19526ms step_avg:34.50ms
step:567/1845 train_time:19560ms step_avg:34.50ms
step:568/1845 train_time:19594ms step_avg:34.50ms
step:569/1845 train_time:19628ms step_avg:34.50ms
step:570/1845 train_time:19662ms step_avg:34.50ms
step:571/1845 train_time:19698ms step_avg:34.50ms
step:572/1845 train_time:19730ms step_avg:34.49ms
step:573/1845 train_time:19765ms step_avg:34.49ms
step:574/1845 train_time:19799ms step_avg:34.49ms
step:575/1845 train_time:19833ms step_avg:34.49ms
step:576/1845 train_time:19867ms step_avg:34.49ms
step:577/1845 train_time:19901ms step_avg:34.49ms
step:578/1845 train_time:19935ms step_avg:34.49ms
step:579/1845 train_time:19969ms step_avg:34.49ms
step:580/1845 train_time:20004ms step_avg:34.49ms
step:581/1845 train_time:20038ms step_avg:34.49ms
step:582/1845 train_time:20072ms step_avg:34.49ms
step:583/1845 train_time:20106ms step_avg:34.49ms
step:584/1845 train_time:20140ms step_avg:34.49ms
step:585/1845 train_time:20174ms step_avg:34.49ms
step:586/1845 train_time:20209ms step_avg:34.49ms
step:587/1845 train_time:20243ms step_avg:34.49ms
step:588/1845 train_time:20277ms step_avg:34.48ms
step:589/1845 train_time:20311ms step_avg:34.48ms
step:590/1845 train_time:20345ms step_avg:34.48ms
step:591/1845 train_time:20379ms step_avg:34.48ms
step:592/1845 train_time:20413ms step_avg:34.48ms
step:593/1845 train_time:20447ms step_avg:34.48ms
step:594/1845 train_time:20481ms step_avg:34.48ms
step:595/1845 train_time:20515ms step_avg:34.48ms
step:596/1845 train_time:20549ms step_avg:34.48ms
step:597/1845 train_time:20583ms step_avg:34.48ms
step:598/1845 train_time:20617ms step_avg:34.48ms
step:599/1845 train_time:20652ms step_avg:34.48ms
step:600/1845 train_time:20686ms step_avg:34.48ms
step:601/1845 train_time:20720ms step_avg:34.48ms
step:602/1845 train_time:20754ms step_avg:34.47ms
step:603/1845 train_time:20789ms step_avg:34.48ms
step:604/1845 train_time:20849ms step_avg:34.52ms
step:605/1845 train_time:20910ms step_avg:34.56ms
step:606/1845 train_time:20970ms step_avg:34.60ms
step:607/1845 train_time:21033ms step_avg:34.65ms
step:608/1845 train_time:21094ms step_avg:34.69ms
step:609/1845 train_time:21157ms step_avg:34.74ms
step:610/1845 train_time:21219ms step_avg:34.78ms
step:611/1845 train_time:21281ms step_avg:34.83ms
step:612/1845 train_time:21342ms step_avg:34.87ms
step:613/1845 train_time:21405ms step_avg:34.92ms
step:614/1845 train_time:21466ms step_avg:34.96ms
step:615/1845 train_time:21528ms step_avg:35.00ms
step:616/1845 train_time:21589ms step_avg:35.05ms
step:617/1845 train_time:21651ms step_avg:35.09ms
step:618/1845 train_time:21713ms step_avg:35.13ms
step:619/1845 train_time:21774ms step_avg:35.18ms
step:620/1845 train_time:21835ms step_avg:35.22ms
step:621/1845 train_time:21897ms step_avg:35.26ms
step:622/1845 train_time:21959ms step_avg:35.30ms
step:623/1845 train_time:22021ms step_avg:35.35ms
step:624/1845 train_time:22082ms step_avg:35.39ms
step:625/1845 train_time:22144ms step_avg:35.43ms
step:626/1845 train_time:22205ms step_avg:35.47ms
step:627/1845 train_time:22267ms step_avg:35.51ms
step:628/1845 train_time:22329ms step_avg:35.56ms
step:629/1845 train_time:22391ms step_avg:35.60ms
step:630/1845 train_time:22452ms step_avg:35.64ms
step:631/1845 train_time:22515ms step_avg:35.68ms
step:632/1845 train_time:22577ms step_avg:35.72ms
step:633/1845 train_time:22639ms step_avg:35.76ms
step:634/1845 train_time:22700ms step_avg:35.80ms
step:635/1845 train_time:22762ms step_avg:35.85ms
step:636/1845 train_time:22824ms step_avg:35.89ms
step:637/1845 train_time:22886ms step_avg:35.93ms
step:638/1845 train_time:22947ms step_avg:35.97ms
step:639/1845 train_time:23010ms step_avg:36.01ms
step:640/1845 train_time:23072ms step_avg:36.05ms
step:641/1845 train_time:23133ms step_avg:36.09ms
step:642/1845 train_time:23194ms step_avg:36.13ms
step:643/1845 train_time:23257ms step_avg:36.17ms
step:644/1845 train_time:23318ms step_avg:36.21ms
step:645/1845 train_time:23380ms step_avg:36.25ms
step:646/1845 train_time:23442ms step_avg:36.29ms
step:647/1845 train_time:23504ms step_avg:36.33ms
step:648/1845 train_time:23565ms step_avg:36.37ms
step:649/1845 train_time:23627ms step_avg:36.41ms
step:650/1845 train_time:23689ms step_avg:36.44ms
step:651/1845 train_time:23751ms step_avg:36.48ms
step:652/1845 train_time:23812ms step_avg:36.52ms
step:653/1845 train_time:23874ms step_avg:36.56ms
step:654/1845 train_time:23935ms step_avg:36.60ms
step:655/1845 train_time:23998ms step_avg:36.64ms
step:656/1845 train_time:24059ms step_avg:36.67ms
step:657/1845 train_time:24121ms step_avg:36.71ms
step:658/1845 train_time:24182ms step_avg:36.75ms
step:659/1845 train_time:24244ms step_avg:36.79ms
step:660/1845 train_time:24305ms step_avg:36.83ms
step:661/1845 train_time:24367ms step_avg:36.86ms
step:662/1845 train_time:24428ms step_avg:36.90ms
step:663/1845 train_time:24490ms step_avg:36.94ms
step:664/1845 train_time:24551ms step_avg:36.97ms
step:665/1845 train_time:24614ms step_avg:37.01ms
step:666/1845 train_time:24675ms step_avg:37.05ms
step:667/1845 train_time:24737ms step_avg:37.09ms
step:668/1845 train_time:24799ms step_avg:37.12ms
step:669/1845 train_time:24861ms step_avg:37.16ms
step:670/1845 train_time:24922ms step_avg:37.20ms
step:671/1845 train_time:24984ms step_avg:37.23ms
step:672/1845 train_time:25046ms step_avg:37.27ms
step:673/1845 train_time:25107ms step_avg:37.31ms
step:674/1845 train_time:25169ms step_avg:37.34ms
step:675/1845 train_time:25231ms step_avg:37.38ms
step:676/1845 train_time:25292ms step_avg:37.41ms
step:677/1845 train_time:25354ms step_avg:37.45ms
step:678/1845 train_time:25416ms step_avg:37.49ms
step:679/1845 train_time:25479ms step_avg:37.52ms
step:680/1845 train_time:25540ms step_avg:37.56ms
step:681/1845 train_time:25602ms step_avg:37.59ms
step:682/1845 train_time:25663ms step_avg:37.63ms
step:683/1845 train_time:25725ms step_avg:37.66ms
step:684/1845 train_time:25786ms step_avg:37.70ms
step:685/1845 train_time:25848ms step_avg:37.73ms
step:686/1845 train_time:25909ms step_avg:37.77ms
step:687/1845 train_time:25972ms step_avg:37.80ms
step:688/1845 train_time:26033ms step_avg:37.84ms
step:689/1845 train_time:26096ms step_avg:37.87ms
step:690/1845 train_time:26157ms step_avg:37.91ms
step:691/1845 train_time:26220ms step_avg:37.94ms
step:692/1845 train_time:26281ms step_avg:37.98ms
step:693/1845 train_time:26343ms step_avg:38.01ms
step:694/1845 train_time:26404ms step_avg:38.05ms
step:695/1845 train_time:26466ms step_avg:38.08ms
step:696/1845 train_time:26528ms step_avg:38.11ms
step:697/1845 train_time:26590ms step_avg:38.15ms
step:698/1845 train_time:26651ms step_avg:38.18ms
step:699/1845 train_time:26714ms step_avg:38.22ms
step:700/1845 train_time:26775ms step_avg:38.25ms
step:701/1845 train_time:26837ms step_avg:38.28ms
step:702/1845 train_time:26898ms step_avg:38.32ms
step:703/1845 train_time:26960ms step_avg:38.35ms
step:704/1845 train_time:27022ms step_avg:38.38ms
step:705/1845 train_time:27084ms step_avg:38.42ms
step:706/1845 train_time:27144ms step_avg:38.45ms
step:707/1845 train_time:27207ms step_avg:38.48ms
step:708/1845 train_time:27268ms step_avg:38.51ms
step:709/1845 train_time:27329ms step_avg:38.55ms
step:710/1845 train_time:27391ms step_avg:38.58ms
step:711/1845 train_time:27453ms step_avg:38.61ms
step:712/1845 train_time:27514ms step_avg:38.64ms
step:713/1845 train_time:27577ms step_avg:38.68ms
step:714/1845 train_time:27639ms step_avg:38.71ms
step:715/1845 train_time:27701ms step_avg:38.74ms
step:716/1845 train_time:27762ms step_avg:38.77ms
step:717/1845 train_time:27824ms step_avg:38.81ms
step:718/1845 train_time:27885ms step_avg:38.84ms
step:719/1845 train_time:27947ms step_avg:38.87ms
step:720/1845 train_time:28007ms step_avg:38.90ms
step:721/1845 train_time:28069ms step_avg:38.93ms
step:722/1845 train_time:28131ms step_avg:38.96ms
step:723/1845 train_time:28193ms step_avg:38.99ms
step:724/1845 train_time:28254ms step_avg:39.02ms
step:725/1845 train_time:28316ms step_avg:39.06ms
step:726/1845 train_time:28378ms step_avg:39.09ms
step:727/1845 train_time:28441ms step_avg:39.12ms
step:728/1845 train_time:28502ms step_avg:39.15ms
step:729/1845 train_time:28564ms step_avg:39.18ms
step:730/1845 train_time:28626ms step_avg:39.21ms
step:731/1845 train_time:28688ms step_avg:39.24ms
step:732/1845 train_time:28749ms step_avg:39.27ms
step:733/1845 train_time:28811ms step_avg:39.31ms
step:734/1845 train_time:28872ms step_avg:39.34ms
step:735/1845 train_time:28935ms step_avg:39.37ms
step:736/1845 train_time:28996ms step_avg:39.40ms
step:737/1845 train_time:29059ms step_avg:39.43ms
step:738/1845 train_time:29120ms step_avg:39.46ms
step:739/1845 train_time:29183ms step_avg:39.49ms
step:740/1845 train_time:29244ms step_avg:39.52ms
step:741/1845 train_time:29306ms step_avg:39.55ms
step:742/1845 train_time:29367ms step_avg:39.58ms
step:743/1845 train_time:29429ms step_avg:39.61ms
step:744/1845 train_time:29490ms step_avg:39.64ms
step:745/1845 train_time:29552ms step_avg:39.67ms
step:746/1845 train_time:29613ms step_avg:39.70ms
step:747/1845 train_time:29675ms step_avg:39.73ms
step:748/1845 train_time:29736ms step_avg:39.75ms
step:749/1845 train_time:29799ms step_avg:39.78ms
step:750/1845 train_time:29860ms step_avg:39.81ms
step:750/1845 val_loss:4.0205 train_time:29923ms step_avg:39.90ms
step:751/1845 train_time:29942ms step_avg:39.87ms
step:752/1845 train_time:29984ms step_avg:39.87ms
step:753/1845 train_time:30050ms step_avg:39.91ms
step:754/1845 train_time:30114ms step_avg:39.94ms
step:755/1845 train_time:30176ms step_avg:39.97ms
step:756/1845 train_time:30238ms step_avg:40.00ms
step:757/1845 train_time:30299ms step_avg:40.03ms
step:758/1845 train_time:30360ms step_avg:40.05ms
step:759/1845 train_time:30421ms step_avg:40.08ms
step:760/1845 train_time:30482ms step_avg:40.11ms
step:761/1845 train_time:30544ms step_avg:40.14ms
step:762/1845 train_time:30605ms step_avg:40.16ms
step:763/1845 train_time:30667ms step_avg:40.19ms
step:764/1845 train_time:30727ms step_avg:40.22ms
step:765/1845 train_time:30788ms step_avg:40.25ms
step:766/1845 train_time:30849ms step_avg:40.27ms
step:767/1845 train_time:30912ms step_avg:40.30ms
step:768/1845 train_time:30974ms step_avg:40.33ms
step:769/1845 train_time:31037ms step_avg:40.36ms
step:770/1845 train_time:31101ms step_avg:40.39ms
step:771/1845 train_time:31164ms step_avg:40.42ms
step:772/1845 train_time:31225ms step_avg:40.45ms
step:773/1845 train_time:31288ms step_avg:40.48ms
step:774/1845 train_time:31349ms step_avg:40.50ms
step:775/1845 train_time:31411ms step_avg:40.53ms
step:776/1845 train_time:31473ms step_avg:40.56ms
step:777/1845 train_time:31534ms step_avg:40.58ms
step:778/1845 train_time:31596ms step_avg:40.61ms
step:779/1845 train_time:31658ms step_avg:40.64ms
step:780/1845 train_time:31718ms step_avg:40.66ms
step:781/1845 train_time:31780ms step_avg:40.69ms
step:782/1845 train_time:31841ms step_avg:40.72ms
step:783/1845 train_time:31905ms step_avg:40.75ms
step:784/1845 train_time:31966ms step_avg:40.77ms
step:785/1845 train_time:32028ms step_avg:40.80ms
step:786/1845 train_time:32089ms step_avg:40.83ms
step:787/1845 train_time:32152ms step_avg:40.85ms
step:788/1845 train_time:32213ms step_avg:40.88ms
step:789/1845 train_time:32276ms step_avg:40.91ms
step:790/1845 train_time:32337ms step_avg:40.93ms
step:791/1845 train_time:32399ms step_avg:40.96ms
step:792/1845 train_time:32460ms step_avg:40.98ms
step:793/1845 train_time:32522ms step_avg:41.01ms
step:794/1845 train_time:32584ms step_avg:41.04ms
step:795/1845 train_time:32646ms step_avg:41.06ms
step:796/1845 train_time:32707ms step_avg:41.09ms
step:797/1845 train_time:32769ms step_avg:41.12ms
step:798/1845 train_time:32830ms step_avg:41.14ms
step:799/1845 train_time:32892ms step_avg:41.17ms
step:800/1845 train_time:32954ms step_avg:41.19ms
step:801/1845 train_time:33016ms step_avg:41.22ms
step:802/1845 train_time:33078ms step_avg:41.24ms
step:803/1845 train_time:33141ms step_avg:41.27ms
step:804/1845 train_time:33203ms step_avg:41.30ms
step:805/1845 train_time:33265ms step_avg:41.32ms
step:806/1845 train_time:33326ms step_avg:41.35ms
step:807/1845 train_time:33389ms step_avg:41.37ms
step:808/1845 train_time:33450ms step_avg:41.40ms
step:809/1845 train_time:33512ms step_avg:41.42ms
step:810/1845 train_time:33573ms step_avg:41.45ms
step:811/1845 train_time:33635ms step_avg:41.47ms
step:812/1845 train_time:33696ms step_avg:41.50ms
step:813/1845 train_time:33758ms step_avg:41.52ms
step:814/1845 train_time:33819ms step_avg:41.55ms
step:815/1845 train_time:33881ms step_avg:41.57ms
step:816/1845 train_time:33943ms step_avg:41.60ms
step:817/1845 train_time:34005ms step_avg:41.62ms
step:818/1845 train_time:34067ms step_avg:41.65ms
step:819/1845 train_time:34129ms step_avg:41.67ms
step:820/1845 train_time:34191ms step_avg:41.70ms
step:821/1845 train_time:34252ms step_avg:41.72ms
step:822/1845 train_time:34314ms step_avg:41.74ms
step:823/1845 train_time:34376ms step_avg:41.77ms
step:824/1845 train_time:34437ms step_avg:41.79ms
step:825/1845 train_time:34499ms step_avg:41.82ms
step:826/1845 train_time:34560ms step_avg:41.84ms
step:827/1845 train_time:34623ms step_avg:41.87ms
step:828/1845 train_time:34684ms step_avg:41.89ms
step:829/1845 train_time:34746ms step_avg:41.91ms
step:830/1845 train_time:34807ms step_avg:41.94ms
step:831/1845 train_time:34869ms step_avg:41.96ms
step:832/1845 train_time:34930ms step_avg:41.98ms
step:833/1845 train_time:34992ms step_avg:42.01ms
step:834/1845 train_time:35054ms step_avg:42.03ms
step:835/1845 train_time:35116ms step_avg:42.05ms
step:836/1845 train_time:35177ms step_avg:42.08ms
step:837/1845 train_time:35239ms step_avg:42.10ms
step:838/1845 train_time:35301ms step_avg:42.13ms
step:839/1845 train_time:35364ms step_avg:42.15ms
step:840/1845 train_time:35425ms step_avg:42.17ms
step:841/1845 train_time:35487ms step_avg:42.20ms
step:842/1845 train_time:35548ms step_avg:42.22ms
step:843/1845 train_time:35610ms step_avg:42.24ms
step:844/1845 train_time:35671ms step_avg:42.26ms
step:845/1845 train_time:35733ms step_avg:42.29ms
step:846/1845 train_time:35794ms step_avg:42.31ms
step:847/1845 train_time:35856ms step_avg:42.33ms
step:848/1845 train_time:35917ms step_avg:42.36ms
step:849/1845 train_time:35980ms step_avg:42.38ms
step:850/1845 train_time:36041ms step_avg:42.40ms
step:851/1845 train_time:36103ms step_avg:42.42ms
step:852/1845 train_time:36164ms step_avg:42.45ms
step:853/1845 train_time:36227ms step_avg:42.47ms
step:854/1845 train_time:36288ms step_avg:42.49ms
step:855/1845 train_time:36350ms step_avg:42.51ms
step:856/1845 train_time:36411ms step_avg:42.54ms
step:857/1845 train_time:36474ms step_avg:42.56ms
step:858/1845 train_time:36535ms step_avg:42.58ms
step:859/1845 train_time:36597ms step_avg:42.60ms
step:860/1845 train_time:36658ms step_avg:42.63ms
step:861/1845 train_time:36720ms step_avg:42.65ms
step:862/1845 train_time:36782ms step_avg:42.67ms
step:863/1845 train_time:36845ms step_avg:42.69ms
step:864/1845 train_time:36905ms step_avg:42.71ms
step:865/1845 train_time:36968ms step_avg:42.74ms
step:866/1845 train_time:37028ms step_avg:42.76ms
step:867/1845 train_time:37090ms step_avg:42.78ms
step:868/1845 train_time:37152ms step_avg:42.80ms
step:869/1845 train_time:37214ms step_avg:42.82ms
step:870/1845 train_time:37275ms step_avg:42.84ms
step:871/1845 train_time:37336ms step_avg:42.87ms
step:872/1845 train_time:37398ms step_avg:42.89ms
step:873/1845 train_time:37460ms step_avg:42.91ms
step:874/1845 train_time:37521ms step_avg:42.93ms
step:875/1845 train_time:37584ms step_avg:42.95ms
step:876/1845 train_time:37645ms step_avg:42.97ms
step:877/1845 train_time:37707ms step_avg:43.00ms
step:878/1845 train_time:37768ms step_avg:43.02ms
step:879/1845 train_time:37830ms step_avg:43.04ms
step:880/1845 train_time:37892ms step_avg:43.06ms
step:881/1845 train_time:37954ms step_avg:43.08ms
step:882/1845 train_time:38015ms step_avg:43.10ms
step:883/1845 train_time:38077ms step_avg:43.12ms
step:884/1845 train_time:38138ms step_avg:43.14ms
step:885/1845 train_time:38201ms step_avg:43.16ms
step:886/1845 train_time:38263ms step_avg:43.19ms
step:887/1845 train_time:38325ms step_avg:43.21ms
step:888/1845 train_time:38386ms step_avg:43.23ms
step:889/1845 train_time:38447ms step_avg:43.25ms
step:890/1845 train_time:38508ms step_avg:43.27ms
step:891/1845 train_time:38570ms step_avg:43.29ms
step:892/1845 train_time:38631ms step_avg:43.31ms
step:893/1845 train_time:38693ms step_avg:43.33ms
step:894/1845 train_time:38754ms step_avg:43.35ms
step:895/1845 train_time:38816ms step_avg:43.37ms
step:896/1845 train_time:38878ms step_avg:43.39ms
step:897/1845 train_time:38940ms step_avg:43.41ms
step:898/1845 train_time:39001ms step_avg:43.43ms
step:899/1845 train_time:39064ms step_avg:43.45ms
step:900/1845 train_time:39125ms step_avg:43.47ms
step:901/1845 train_time:39188ms step_avg:43.49ms
step:902/1845 train_time:39248ms step_avg:43.51ms
step:903/1845 train_time:39310ms step_avg:43.53ms
step:904/1845 train_time:39372ms step_avg:43.55ms
step:905/1845 train_time:39434ms step_avg:43.57ms
step:906/1845 train_time:39495ms step_avg:43.59ms
step:907/1845 train_time:39557ms step_avg:43.61ms
step:908/1845 train_time:39619ms step_avg:43.63ms
step:909/1845 train_time:39682ms step_avg:43.65ms
step:910/1845 train_time:39742ms step_avg:43.67ms
step:911/1845 train_time:39805ms step_avg:43.69ms
step:912/1845 train_time:39866ms step_avg:43.71ms
step:913/1845 train_time:39929ms step_avg:43.73ms
step:914/1845 train_time:39990ms step_avg:43.75ms
step:915/1845 train_time:40052ms step_avg:43.77ms
step:916/1845 train_time:40113ms step_avg:43.79ms
step:917/1845 train_time:40175ms step_avg:43.81ms
step:918/1845 train_time:40236ms step_avg:43.83ms
step:919/1845 train_time:40299ms step_avg:43.85ms
step:920/1845 train_time:40360ms step_avg:43.87ms
step:921/1845 train_time:40422ms step_avg:43.89ms
step:922/1845 train_time:40483ms step_avg:43.91ms
step:923/1845 train_time:40547ms step_avg:43.93ms
step:924/1845 train_time:40608ms step_avg:43.95ms
step:925/1845 train_time:40669ms step_avg:43.97ms
step:926/1845 train_time:40731ms step_avg:43.99ms
step:927/1845 train_time:40793ms step_avg:44.01ms
step:928/1845 train_time:40855ms step_avg:44.02ms
step:929/1845 train_time:40917ms step_avg:44.04ms
step:930/1845 train_time:40978ms step_avg:44.06ms
step:931/1845 train_time:41041ms step_avg:44.08ms
step:932/1845 train_time:41102ms step_avg:44.10ms
step:933/1845 train_time:41164ms step_avg:44.12ms
step:934/1845 train_time:41224ms step_avg:44.14ms
step:935/1845 train_time:41286ms step_avg:44.16ms
step:936/1845 train_time:41347ms step_avg:44.17ms
step:937/1845 train_time:41410ms step_avg:44.19ms
step:938/1845 train_time:41471ms step_avg:44.21ms
step:939/1845 train_time:41533ms step_avg:44.23ms
step:940/1845 train_time:41595ms step_avg:44.25ms
step:941/1845 train_time:41657ms step_avg:44.27ms
step:942/1845 train_time:41717ms step_avg:44.29ms
step:943/1845 train_time:41780ms step_avg:44.31ms
step:944/1845 train_time:41842ms step_avg:44.32ms
step:945/1845 train_time:41905ms step_avg:44.34ms
step:946/1845 train_time:41966ms step_avg:44.36ms
step:947/1845 train_time:42028ms step_avg:44.38ms
step:948/1845 train_time:42089ms step_avg:44.40ms
step:949/1845 train_time:42151ms step_avg:44.42ms
step:950/1845 train_time:42212ms step_avg:44.43ms
step:951/1845 train_time:42274ms step_avg:44.45ms
step:952/1845 train_time:42335ms step_avg:44.47ms
step:953/1845 train_time:42397ms step_avg:44.49ms
step:954/1845 train_time:42458ms step_avg:44.51ms
step:955/1845 train_time:42521ms step_avg:44.52ms
step:956/1845 train_time:42582ms step_avg:44.54ms
step:957/1845 train_time:42644ms step_avg:44.56ms
step:958/1845 train_time:42705ms step_avg:44.58ms
step:959/1845 train_time:42768ms step_avg:44.60ms
step:960/1845 train_time:42829ms step_avg:44.61ms
step:961/1845 train_time:42891ms step_avg:44.63ms
step:962/1845 train_time:42953ms step_avg:44.65ms
step:963/1845 train_time:43015ms step_avg:44.67ms
step:964/1845 train_time:43076ms step_avg:44.68ms
step:965/1845 train_time:43138ms step_avg:44.70ms
step:966/1845 train_time:43199ms step_avg:44.72ms
step:967/1845 train_time:43262ms step_avg:44.74ms
step:968/1845 train_time:43324ms step_avg:44.76ms
step:969/1845 train_time:43386ms step_avg:44.77ms
step:970/1845 train_time:43446ms step_avg:44.79ms
step:971/1845 train_time:43509ms step_avg:44.81ms
step:972/1845 train_time:43570ms step_avg:44.82ms
step:973/1845 train_time:43632ms step_avg:44.84ms
step:974/1845 train_time:43694ms step_avg:44.86ms
step:975/1845 train_time:43756ms step_avg:44.88ms
step:976/1845 train_time:43816ms step_avg:44.89ms
step:977/1845 train_time:43879ms step_avg:44.91ms
step:978/1845 train_time:43941ms step_avg:44.93ms
step:979/1845 train_time:44003ms step_avg:44.95ms
step:980/1845 train_time:44065ms step_avg:44.96ms
step:981/1845 train_time:44127ms step_avg:44.98ms
step:982/1845 train_time:44188ms step_avg:45.00ms
step:983/1845 train_time:44250ms step_avg:45.01ms
step:984/1845 train_time:44311ms step_avg:45.03ms
step:985/1845 train_time:44372ms step_avg:45.05ms
step:986/1845 train_time:44434ms step_avg:45.07ms
step:987/1845 train_time:44496ms step_avg:45.08ms
step:988/1845 train_time:44557ms step_avg:45.10ms
step:989/1845 train_time:44620ms step_avg:45.12ms
step:990/1845 train_time:44681ms step_avg:45.13ms
step:991/1845 train_time:44744ms step_avg:45.15ms
step:992/1845 train_time:44805ms step_avg:45.17ms
step:993/1845 train_time:44867ms step_avg:45.18ms
step:994/1845 train_time:44927ms step_avg:45.20ms
step:995/1845 train_time:44990ms step_avg:45.22ms
step:996/1845 train_time:45051ms step_avg:45.23ms
step:997/1845 train_time:45113ms step_avg:45.25ms
step:998/1845 train_time:45175ms step_avg:45.27ms
step:999/1845 train_time:45237ms step_avg:45.28ms
step:1000/1845 train_time:45298ms step_avg:45.30ms
step:1000/1845 val_loss:3.7887 train_time:45361ms step_avg:45.36ms
step:1001/1845 train_time:45381ms step_avg:45.34ms
step:1002/1845 train_time:45423ms step_avg:45.33ms
step:1003/1845 train_time:45486ms step_avg:45.35ms
step:1004/1845 train_time:45547ms step_avg:45.37ms
step:1005/1845 train_time:45609ms step_avg:45.38ms
step:1006/1845 train_time:45671ms step_avg:45.40ms
step:1007/1845 train_time:45732ms step_avg:45.41ms
step:1008/1845 train_time:45794ms step_avg:45.43ms
step:1009/1845 train_time:45855ms step_avg:45.45ms
step:1010/1845 train_time:45916ms step_avg:45.46ms
step:1011/1845 train_time:45979ms step_avg:45.48ms
step:1012/1845 train_time:46040ms step_avg:45.49ms
step:1013/1845 train_time:46102ms step_avg:45.51ms
step:1014/1845 train_time:46163ms step_avg:45.53ms
step:1015/1845 train_time:46224ms step_avg:45.54ms
step:1016/1845 train_time:46285ms step_avg:45.56ms
step:1017/1845 train_time:46348ms step_avg:45.57ms
step:1018/1845 train_time:46410ms step_avg:45.59ms
step:1019/1845 train_time:46473ms step_avg:45.61ms
step:1020/1845 train_time:46534ms step_avg:45.62ms
step:1021/1845 train_time:46597ms step_avg:45.64ms
step:1022/1845 train_time:46658ms step_avg:45.65ms
step:1023/1845 train_time:46721ms step_avg:45.67ms
step:1024/1845 train_time:46782ms step_avg:45.69ms
step:1025/1845 train_time:46844ms step_avg:45.70ms
step:1026/1845 train_time:46905ms step_avg:45.72ms
step:1027/1845 train_time:46967ms step_avg:45.73ms
step:1028/1845 train_time:47028ms step_avg:45.75ms
step:1029/1845 train_time:47089ms step_avg:45.76ms
step:1030/1845 train_time:47150ms step_avg:45.78ms
step:1031/1845 train_time:47212ms step_avg:45.79ms
step:1032/1845 train_time:47273ms step_avg:45.81ms
step:1033/1845 train_time:47335ms step_avg:45.82ms
step:1034/1845 train_time:47397ms step_avg:45.84ms
step:1035/1845 train_time:47459ms step_avg:45.85ms
step:1036/1845 train_time:47521ms step_avg:45.87ms
step:1037/1845 train_time:47583ms step_avg:45.89ms
step:1038/1845 train_time:47644ms step_avg:45.90ms
step:1039/1845 train_time:47707ms step_avg:45.92ms
step:1040/1845 train_time:47768ms step_avg:45.93ms
step:1041/1845 train_time:47830ms step_avg:45.95ms
step:1042/1845 train_time:47891ms step_avg:45.96ms
step:1043/1845 train_time:47952ms step_avg:45.98ms
step:1044/1845 train_time:48013ms step_avg:45.99ms
step:1045/1845 train_time:48075ms step_avg:46.00ms
step:1046/1845 train_time:48136ms step_avg:46.02ms
step:1047/1845 train_time:48198ms step_avg:46.03ms
step:1048/1845 train_time:48259ms step_avg:46.05ms
step:1049/1845 train_time:48322ms step_avg:46.06ms
step:1050/1845 train_time:48383ms step_avg:46.08ms
step:1051/1845 train_time:48445ms step_avg:46.09ms
step:1052/1845 train_time:48507ms step_avg:46.11ms
step:1053/1845 train_time:48569ms step_avg:46.12ms
step:1054/1845 train_time:48630ms step_avg:46.14ms
step:1055/1845 train_time:48693ms step_avg:46.15ms
step:1056/1845 train_time:48753ms step_avg:46.17ms
step:1057/1845 train_time:48815ms step_avg:46.18ms
step:1058/1845 train_time:48876ms step_avg:46.20ms
step:1059/1845 train_time:48939ms step_avg:46.21ms
step:1060/1845 train_time:49000ms step_avg:46.23ms
step:1061/1845 train_time:49063ms step_avg:46.24ms
step:1062/1845 train_time:49123ms step_avg:46.26ms
step:1063/1845 train_time:49185ms step_avg:46.27ms
step:1064/1845 train_time:49246ms step_avg:46.28ms
step:1065/1845 train_time:49308ms step_avg:46.30ms
step:1066/1845 train_time:49370ms step_avg:46.31ms
step:1067/1845 train_time:49432ms step_avg:46.33ms
step:1068/1845 train_time:49493ms step_avg:46.34ms
step:1069/1845 train_time:49555ms step_avg:46.36ms
step:1070/1845 train_time:49617ms step_avg:46.37ms
step:1071/1845 train_time:49679ms step_avg:46.39ms
step:1072/1845 train_time:49741ms step_avg:46.40ms
step:1073/1845 train_time:49803ms step_avg:46.41ms
step:1074/1845 train_time:49864ms step_avg:46.43ms
step:1075/1845 train_time:49926ms step_avg:46.44ms
step:1076/1845 train_time:49987ms step_avg:46.46ms
step:1077/1845 train_time:50049ms step_avg:46.47ms
step:1078/1845 train_time:50111ms step_avg:46.49ms
step:1079/1845 train_time:50173ms step_avg:46.50ms
step:1080/1845 train_time:50234ms step_avg:46.51ms
step:1081/1845 train_time:50296ms step_avg:46.53ms
step:1082/1845 train_time:50358ms step_avg:46.54ms
step:1083/1845 train_time:50421ms step_avg:46.56ms
step:1084/1845 train_time:50483ms step_avg:46.57ms
step:1085/1845 train_time:50545ms step_avg:46.58ms
step:1086/1845 train_time:50606ms step_avg:46.60ms
step:1087/1845 train_time:50669ms step_avg:46.61ms
step:1088/1845 train_time:50731ms step_avg:46.63ms
step:1089/1845 train_time:50792ms step_avg:46.64ms
step:1090/1845 train_time:50854ms step_avg:46.66ms
step:1091/1845 train_time:50917ms step_avg:46.67ms
step:1092/1845 train_time:50978ms step_avg:46.68ms
step:1093/1845 train_time:51040ms step_avg:46.70ms
step:1094/1845 train_time:51101ms step_avg:46.71ms
step:1095/1845 train_time:51164ms step_avg:46.72ms
step:1096/1845 train_time:51226ms step_avg:46.74ms
step:1097/1845 train_time:51288ms step_avg:46.75ms
step:1098/1845 train_time:51349ms step_avg:46.77ms
step:1099/1845 train_time:51411ms step_avg:46.78ms
step:1100/1845 train_time:51471ms step_avg:46.79ms
step:1101/1845 train_time:51534ms step_avg:46.81ms
step:1102/1845 train_time:51595ms step_avg:46.82ms
step:1103/1845 train_time:51658ms step_avg:46.83ms
step:1104/1845 train_time:51719ms step_avg:46.85ms
step:1105/1845 train_time:51782ms step_avg:46.86ms
step:1106/1845 train_time:51843ms step_avg:46.87ms
step:1107/1845 train_time:51905ms step_avg:46.89ms
step:1108/1845 train_time:51967ms step_avg:46.90ms
step:1109/1845 train_time:52029ms step_avg:46.91ms
step:1110/1845 train_time:52090ms step_avg:46.93ms
step:1111/1845 train_time:52152ms step_avg:46.94ms
step:1112/1845 train_time:52212ms step_avg:46.95ms
step:1113/1845 train_time:52274ms step_avg:46.97ms
step:1114/1845 train_time:52335ms step_avg:46.98ms
step:1115/1845 train_time:52398ms step_avg:46.99ms
step:1116/1845 train_time:52459ms step_avg:47.01ms
step:1117/1845 train_time:52522ms step_avg:47.02ms
step:1118/1845 train_time:52583ms step_avg:47.03ms
step:1119/1845 train_time:52645ms step_avg:47.05ms
step:1120/1845 train_time:52707ms step_avg:47.06ms
step:1121/1845 train_time:52769ms step_avg:47.07ms
step:1122/1845 train_time:52831ms step_avg:47.09ms
step:1123/1845 train_time:52893ms step_avg:47.10ms
step:1124/1845 train_time:52954ms step_avg:47.11ms
step:1125/1845 train_time:53017ms step_avg:47.13ms
step:1126/1845 train_time:53079ms step_avg:47.14ms
step:1127/1845 train_time:53141ms step_avg:47.15ms
step:1128/1845 train_time:53202ms step_avg:47.17ms
step:1129/1845 train_time:53265ms step_avg:47.18ms
step:1130/1845 train_time:53326ms step_avg:47.19ms
step:1131/1845 train_time:53388ms step_avg:47.20ms
step:1132/1845 train_time:53449ms step_avg:47.22ms
step:1133/1845 train_time:53511ms step_avg:47.23ms
step:1134/1845 train_time:53572ms step_avg:47.24ms
step:1135/1845 train_time:53634ms step_avg:47.25ms
step:1136/1845 train_time:53696ms step_avg:47.27ms
step:1137/1845 train_time:53759ms step_avg:47.28ms
step:1138/1845 train_time:53821ms step_avg:47.29ms
step:1139/1845 train_time:53883ms step_avg:47.31ms
step:1140/1845 train_time:53944ms step_avg:47.32ms
step:1141/1845 train_time:54006ms step_avg:47.33ms
step:1142/1845 train_time:54067ms step_avg:47.34ms
step:1143/1845 train_time:54130ms step_avg:47.36ms
step:1144/1845 train_time:54192ms step_avg:47.37ms
step:1145/1845 train_time:54253ms step_avg:47.38ms
step:1146/1845 train_time:54314ms step_avg:47.39ms
step:1147/1845 train_time:54377ms step_avg:47.41ms
step:1148/1845 train_time:54439ms step_avg:47.42ms
step:1149/1845 train_time:54501ms step_avg:47.43ms
step:1150/1845 train_time:54562ms step_avg:47.45ms
step:1151/1845 train_time:54625ms step_avg:47.46ms
step:1152/1845 train_time:54686ms step_avg:47.47ms
step:1153/1845 train_time:54748ms step_avg:47.48ms
step:1154/1845 train_time:54810ms step_avg:47.50ms
step:1155/1845 train_time:54872ms step_avg:47.51ms
step:1156/1845 train_time:54933ms step_avg:47.52ms
step:1157/1845 train_time:54995ms step_avg:47.53ms
step:1158/1845 train_time:55056ms step_avg:47.54ms
step:1159/1845 train_time:55118ms step_avg:47.56ms
step:1160/1845 train_time:55179ms step_avg:47.57ms
step:1161/1845 train_time:55241ms step_avg:47.58ms
step:1162/1845 train_time:55302ms step_avg:47.59ms
step:1163/1845 train_time:55364ms step_avg:47.60ms
step:1164/1845 train_time:55425ms step_avg:47.62ms
step:1165/1845 train_time:55487ms step_avg:47.63ms
step:1166/1845 train_time:55548ms step_avg:47.64ms
step:1167/1845 train_time:55610ms step_avg:47.65ms
step:1168/1845 train_time:55672ms step_avg:47.66ms
step:1169/1845 train_time:55733ms step_avg:47.68ms
step:1170/1845 train_time:55795ms step_avg:47.69ms
step:1171/1845 train_time:55857ms step_avg:47.70ms
step:1172/1845 train_time:55919ms step_avg:47.71ms
step:1173/1845 train_time:55982ms step_avg:47.73ms
step:1174/1845 train_time:56042ms step_avg:47.74ms
step:1175/1845 train_time:56105ms step_avg:47.75ms
step:1176/1845 train_time:56165ms step_avg:47.76ms
step:1177/1845 train_time:56228ms step_avg:47.77ms
step:1178/1845 train_time:56289ms step_avg:47.78ms
step:1179/1845 train_time:56351ms step_avg:47.80ms
step:1180/1845 train_time:56412ms step_avg:47.81ms
step:1181/1845 train_time:56474ms step_avg:47.82ms
step:1182/1845 train_time:56535ms step_avg:47.83ms
step:1183/1845 train_time:56598ms step_avg:47.84ms
step:1184/1845 train_time:56659ms step_avg:47.85ms
step:1185/1845 train_time:56721ms step_avg:47.87ms
step:1186/1845 train_time:56782ms step_avg:47.88ms
step:1187/1845 train_time:56845ms step_avg:47.89ms
step:1188/1845 train_time:56906ms step_avg:47.90ms
step:1189/1845 train_time:56968ms step_avg:47.91ms
step:1190/1845 train_time:57030ms step_avg:47.92ms
step:1191/1845 train_time:57092ms step_avg:47.94ms
step:1192/1845 train_time:57153ms step_avg:47.95ms
step:1193/1845 train_time:57215ms step_avg:47.96ms
step:1194/1845 train_time:57276ms step_avg:47.97ms
step:1195/1845 train_time:57338ms step_avg:47.98ms
step:1196/1845 train_time:57399ms step_avg:47.99ms
step:1197/1845 train_time:57461ms step_avg:48.00ms
step:1198/1845 train_time:57522ms step_avg:48.01ms
step:1199/1845 train_time:57585ms step_avg:48.03ms
step:1200/1845 train_time:57645ms step_avg:48.04ms
step:1201/1845 train_time:57707ms step_avg:48.05ms
step:1202/1845 train_time:57769ms step_avg:48.06ms
step:1203/1845 train_time:57831ms step_avg:48.07ms
step:1204/1845 train_time:57893ms step_avg:48.08ms
step:1205/1845 train_time:57955ms step_avg:48.10ms
step:1206/1845 train_time:58044ms step_avg:48.13ms
step:1207/1845 train_time:58132ms step_avg:48.16ms
step:1208/1845 train_time:58220ms step_avg:48.19ms
step:1209/1845 train_time:58308ms step_avg:48.23ms
step:1210/1845 train_time:58396ms step_avg:48.26ms
step:1211/1845 train_time:58485ms step_avg:48.30ms
step:1212/1845 train_time:58572ms step_avg:48.33ms
step:1213/1845 train_time:58660ms step_avg:48.36ms
step:1214/1845 train_time:58748ms step_avg:48.39ms
step:1215/1845 train_time:58838ms step_avg:48.43ms
step:1216/1845 train_time:58926ms step_avg:48.46ms
step:1217/1845 train_time:59015ms step_avg:48.49ms
step:1218/1845 train_time:59102ms step_avg:48.52ms
step:1219/1845 train_time:59190ms step_avg:48.56ms
step:1220/1845 train_time:59278ms step_avg:48.59ms
step:1221/1845 train_time:59365ms step_avg:48.62ms
step:1222/1845 train_time:59453ms step_avg:48.65ms
step:1223/1845 train_time:59541ms step_avg:48.68ms
step:1224/1845 train_time:59628ms step_avg:48.72ms
step:1225/1845 train_time:59716ms step_avg:48.75ms
step:1226/1845 train_time:59803ms step_avg:48.78ms
step:1227/1845 train_time:59892ms step_avg:48.81ms
step:1228/1845 train_time:59979ms step_avg:48.84ms
step:1229/1845 train_time:60067ms step_avg:48.88ms
step:1230/1845 train_time:60155ms step_avg:48.91ms
step:1231/1845 train_time:60244ms step_avg:48.94ms
step:1232/1845 train_time:60332ms step_avg:48.97ms
step:1233/1845 train_time:60420ms step_avg:49.00ms
step:1234/1845 train_time:60509ms step_avg:49.03ms
step:1235/1845 train_time:60596ms step_avg:49.07ms
step:1236/1845 train_time:60683ms step_avg:49.10ms
step:1237/1845 train_time:60771ms step_avg:49.13ms
step:1238/1845 train_time:60859ms step_avg:49.16ms
step:1239/1845 train_time:60947ms step_avg:49.19ms
step:1240/1845 train_time:61035ms step_avg:49.22ms
step:1241/1845 train_time:61125ms step_avg:49.25ms
step:1242/1845 train_time:61213ms step_avg:49.29ms
step:1243/1845 train_time:61301ms step_avg:49.32ms
step:1244/1845 train_time:61388ms step_avg:49.35ms
step:1245/1845 train_time:61477ms step_avg:49.38ms
step:1246/1845 train_time:61566ms step_avg:49.41ms
step:1247/1845 train_time:61654ms step_avg:49.44ms
step:1248/1845 train_time:61741ms step_avg:49.47ms
step:1249/1845 train_time:61830ms step_avg:49.50ms
step:1250/1845 train_time:61918ms step_avg:49.53ms
step:1250/1845 val_loss:3.5359 train_time:62008ms step_avg:49.61ms
step:1251/1845 train_time:62028ms step_avg:49.58ms
step:1252/1845 train_time:62096ms step_avg:49.60ms
step:1253/1845 train_time:62187ms step_avg:49.63ms
step:1254/1845 train_time:62274ms step_avg:49.66ms
step:1255/1845 train_time:62363ms step_avg:49.69ms
step:1256/1845 train_time:62449ms step_avg:49.72ms
step:1257/1845 train_time:62537ms step_avg:49.75ms
step:1258/1845 train_time:62624ms step_avg:49.78ms
step:1259/1845 train_time:62712ms step_avg:49.81ms
step:1260/1845 train_time:62799ms step_avg:49.84ms
step:1261/1845 train_time:62888ms step_avg:49.87ms
step:1262/1845 train_time:62976ms step_avg:49.90ms
step:1263/1845 train_time:63067ms step_avg:49.93ms
step:1264/1845 train_time:63157ms step_avg:49.97ms
step:1265/1845 train_time:63246ms step_avg:50.00ms
step:1266/1845 train_time:63334ms step_avg:50.03ms
step:1267/1845 train_time:63422ms step_avg:50.06ms
step:1268/1845 train_time:63508ms step_avg:50.09ms
step:1269/1845 train_time:63596ms step_avg:50.12ms
step:1270/1845 train_time:63683ms step_avg:50.14ms
step:1271/1845 train_time:63771ms step_avg:50.17ms
step:1272/1845 train_time:63858ms step_avg:50.20ms
step:1273/1845 train_time:63947ms step_avg:50.23ms
step:1274/1845 train_time:64036ms step_avg:50.26ms
step:1275/1845 train_time:64125ms step_avg:50.29ms
step:1276/1845 train_time:64213ms step_avg:50.32ms
step:1277/1845 train_time:64302ms step_avg:50.35ms
step:1278/1845 train_time:64389ms step_avg:50.38ms
step:1279/1845 train_time:64477ms step_avg:50.41ms
step:1280/1845 train_time:64565ms step_avg:50.44ms
step:1281/1845 train_time:64653ms step_avg:50.47ms
step:1282/1845 train_time:64740ms step_avg:50.50ms
step:1283/1845 train_time:64828ms step_avg:50.53ms
step:1284/1845 train_time:64916ms step_avg:50.56ms
step:1285/1845 train_time:65006ms step_avg:50.59ms
step:1286/1845 train_time:65094ms step_avg:50.62ms
step:1287/1845 train_time:65184ms step_avg:50.65ms
step:1288/1845 train_time:65273ms step_avg:50.68ms
step:1289/1845 train_time:65362ms step_avg:50.71ms
step:1290/1845 train_time:65451ms step_avg:50.74ms
step:1291/1845 train_time:65538ms step_avg:50.77ms
step:1292/1845 train_time:65625ms step_avg:50.79ms
step:1293/1845 train_time:65713ms step_avg:50.82ms
step:1294/1845 train_time:65800ms step_avg:50.85ms
step:1295/1845 train_time:65889ms step_avg:50.88ms
step:1296/1845 train_time:65977ms step_avg:50.91ms
step:1297/1845 train_time:66067ms step_avg:50.94ms
step:1298/1845 train_time:66155ms step_avg:50.97ms
step:1299/1845 train_time:66243ms step_avg:51.00ms
step:1300/1845 train_time:66331ms step_avg:51.02ms
step:1301/1845 train_time:66419ms step_avg:51.05ms
step:1302/1845 train_time:66507ms step_avg:51.08ms
step:1303/1845 train_time:66595ms step_avg:51.11ms
step:1304/1845 train_time:66682ms step_avg:51.14ms
step:1305/1845 train_time:66770ms step_avg:51.16ms
step:1306/1845 train_time:66857ms step_avg:51.19ms
step:1307/1845 train_time:66946ms step_avg:51.22ms
step:1308/1845 train_time:67033ms step_avg:51.25ms
step:1309/1845 train_time:67123ms step_avg:51.28ms
step:1310/1845 train_time:67210ms step_avg:51.31ms
step:1311/1845 train_time:67300ms step_avg:51.33ms
step:1312/1845 train_time:67389ms step_avg:51.36ms
step:1313/1845 train_time:67477ms step_avg:51.39ms
step:1314/1845 train_time:67563ms step_avg:51.42ms
step:1315/1845 train_time:67652ms step_avg:51.45ms
step:1316/1845 train_time:67740ms step_avg:51.47ms
step:1317/1845 train_time:67828ms step_avg:51.50ms
step:1318/1845 train_time:67915ms step_avg:51.53ms
step:1319/1845 train_time:68003ms step_avg:51.56ms
step:1320/1845 train_time:68091ms step_avg:51.58ms
step:1321/1845 train_time:68180ms step_avg:51.61ms
step:1322/1845 train_time:68268ms step_avg:51.64ms
step:1323/1845 train_time:68358ms step_avg:51.67ms
step:1324/1845 train_time:68446ms step_avg:51.70ms
step:1325/1845 train_time:68534ms step_avg:51.72ms
step:1326/1845 train_time:68621ms step_avg:51.75ms
step:1327/1845 train_time:68709ms step_avg:51.78ms
step:1328/1845 train_time:68797ms step_avg:51.81ms
step:1329/1845 train_time:68885ms step_avg:51.83ms
step:1330/1845 train_time:68973ms step_avg:51.86ms
step:1331/1845 train_time:69061ms step_avg:51.89ms
step:1332/1845 train_time:69148ms step_avg:51.91ms
step:1333/1845 train_time:69237ms step_avg:51.94ms
step:1334/1845 train_time:69325ms step_avg:51.97ms
step:1335/1845 train_time:69413ms step_avg:51.99ms
step:1336/1845 train_time:69501ms step_avg:52.02ms
step:1337/1845 train_time:69590ms step_avg:52.05ms
step:1338/1845 train_time:69678ms step_avg:52.08ms
step:1339/1845 train_time:69766ms step_avg:52.10ms
step:1340/1845 train_time:69855ms step_avg:52.13ms
step:1341/1845 train_time:69943ms step_avg:52.16ms
step:1342/1845 train_time:70030ms step_avg:52.18ms
step:1343/1845 train_time:70118ms step_avg:52.21ms
step:1344/1845 train_time:70205ms step_avg:52.24ms
step:1345/1845 train_time:70294ms step_avg:52.26ms
step:1346/1845 train_time:70382ms step_avg:52.29ms
step:1347/1845 train_time:70470ms step_avg:52.32ms
step:1348/1845 train_time:70558ms step_avg:52.34ms
step:1349/1845 train_time:70646ms step_avg:52.37ms
step:1350/1845 train_time:70734ms step_avg:52.40ms
step:1351/1845 train_time:70822ms step_avg:52.42ms
step:1352/1845 train_time:70909ms step_avg:52.45ms
step:1353/1845 train_time:70997ms step_avg:52.47ms
step:1354/1845 train_time:71085ms step_avg:52.50ms
step:1355/1845 train_time:71173ms step_avg:52.53ms
step:1356/1845 train_time:71261ms step_avg:52.55ms
step:1357/1845 train_time:71349ms step_avg:52.58ms
step:1358/1845 train_time:71436ms step_avg:52.60ms
step:1359/1845 train_time:71525ms step_avg:52.63ms
step:1360/1845 train_time:71612ms step_avg:52.66ms
step:1361/1845 train_time:71701ms step_avg:52.68ms
step:1362/1845 train_time:71789ms step_avg:52.71ms
step:1363/1845 train_time:71877ms step_avg:52.73ms
step:1364/1845 train_time:71964ms step_avg:52.76ms
step:1365/1845 train_time:72053ms step_avg:52.79ms
step:1366/1845 train_time:72140ms step_avg:52.81ms
step:1367/1845 train_time:72229ms step_avg:52.84ms
step:1368/1845 train_time:72317ms step_avg:52.86ms
step:1369/1845 train_time:72405ms step_avg:52.89ms
step:1370/1845 train_time:72492ms step_avg:52.91ms
step:1371/1845 train_time:72581ms step_avg:52.94ms
step:1372/1845 train_time:72669ms step_avg:52.97ms
step:1373/1845 train_time:72758ms step_avg:52.99ms
step:1374/1845 train_time:72847ms step_avg:53.02ms
step:1375/1845 train_time:72935ms step_avg:53.04ms
step:1376/1845 train_time:73022ms step_avg:53.07ms
step:1377/1845 train_time:73110ms step_avg:53.09ms
step:1378/1845 train_time:73199ms step_avg:53.12ms
step:1379/1845 train_time:73287ms step_avg:53.15ms
step:1380/1845 train_time:73376ms step_avg:53.17ms
step:1381/1845 train_time:73464ms step_avg:53.20ms
step:1382/1845 train_time:73551ms step_avg:53.22ms
step:1383/1845 train_time:73640ms step_avg:53.25ms
step:1384/1845 train_time:73727ms step_avg:53.27ms
step:1385/1845 train_time:73816ms step_avg:53.30ms
step:1386/1845 train_time:73902ms step_avg:53.32ms
step:1387/1845 train_time:73990ms step_avg:53.35ms
step:1388/1845 train_time:74078ms step_avg:53.37ms
step:1389/1845 train_time:74167ms step_avg:53.40ms
step:1390/1845 train_time:74255ms step_avg:53.42ms
step:1391/1845 train_time:74344ms step_avg:53.45ms
step:1392/1845 train_time:74430ms step_avg:53.47ms
step:1393/1845 train_time:74519ms step_avg:53.50ms
step:1394/1845 train_time:74606ms step_avg:53.52ms
step:1395/1845 train_time:74694ms step_avg:53.54ms
step:1396/1845 train_time:74782ms step_avg:53.57ms
step:1397/1845 train_time:74871ms step_avg:53.59ms
step:1398/1845 train_time:74959ms step_avg:53.62ms
step:1399/1845 train_time:75047ms step_avg:53.64ms
step:1400/1845 train_time:75135ms step_avg:53.67ms
step:1401/1845 train_time:75223ms step_avg:53.69ms
step:1402/1845 train_time:75311ms step_avg:53.72ms
step:1403/1845 train_time:75400ms step_avg:53.74ms
step:1404/1845 train_time:75488ms step_avg:53.77ms
step:1405/1845 train_time:75576ms step_avg:53.79ms
step:1406/1845 train_time:75663ms step_avg:53.81ms
step:1407/1845 train_time:75752ms step_avg:53.84ms
step:1408/1845 train_time:75840ms step_avg:53.86ms
step:1409/1845 train_time:75927ms step_avg:53.89ms
step:1410/1845 train_time:76015ms step_avg:53.91ms
step:1411/1845 train_time:76103ms step_avg:53.94ms
step:1412/1845 train_time:76191ms step_avg:53.96ms
step:1413/1845 train_time:76280ms step_avg:53.98ms
step:1414/1845 train_time:76368ms step_avg:54.01ms
step:1415/1845 train_time:76456ms step_avg:54.03ms
step:1416/1845 train_time:76544ms step_avg:54.06ms
step:1417/1845 train_time:76632ms step_avg:54.08ms
step:1418/1845 train_time:76720ms step_avg:54.10ms
step:1419/1845 train_time:76809ms step_avg:54.13ms
step:1420/1845 train_time:76896ms step_avg:54.15ms
step:1421/1845 train_time:76985ms step_avg:54.18ms
step:1422/1845 train_time:77072ms step_avg:54.20ms
step:1423/1845 train_time:77160ms step_avg:54.22ms
step:1424/1845 train_time:77248ms step_avg:54.25ms
step:1425/1845 train_time:77336ms step_avg:54.27ms
step:1426/1845 train_time:77423ms step_avg:54.29ms
step:1427/1845 train_time:77512ms step_avg:54.32ms
step:1428/1845 train_time:77600ms step_avg:54.34ms
step:1429/1845 train_time:77688ms step_avg:54.37ms
step:1430/1845 train_time:77776ms step_avg:54.39ms
step:1431/1845 train_time:77864ms step_avg:54.41ms
step:1432/1845 train_time:77952ms step_avg:54.44ms
step:1433/1845 train_time:78040ms step_avg:54.46ms
step:1434/1845 train_time:78128ms step_avg:54.48ms
step:1435/1845 train_time:78215ms step_avg:54.51ms
step:1436/1845 train_time:78303ms step_avg:54.53ms
step:1437/1845 train_time:78392ms step_avg:54.55ms
step:1438/1845 train_time:78479ms step_avg:54.58ms
step:1439/1845 train_time:78568ms step_avg:54.60ms
step:1440/1845 train_time:78656ms step_avg:54.62ms
step:1441/1845 train_time:78744ms step_avg:54.65ms
step:1442/1845 train_time:78832ms step_avg:54.67ms
step:1443/1845 train_time:78921ms step_avg:54.69ms
step:1444/1845 train_time:79009ms step_avg:54.72ms
step:1445/1845 train_time:79097ms step_avg:54.74ms
step:1446/1845 train_time:79185ms step_avg:54.76ms
step:1447/1845 train_time:79273ms step_avg:54.78ms
step:1448/1845 train_time:79360ms step_avg:54.81ms
step:1449/1845 train_time:79449ms step_avg:54.83ms
step:1450/1845 train_time:79536ms step_avg:54.85ms
step:1451/1845 train_time:79626ms step_avg:54.88ms
step:1452/1845 train_time:79713ms step_avg:54.90ms
step:1453/1845 train_time:79801ms step_avg:54.92ms
step:1454/1845 train_time:79888ms step_avg:54.94ms
step:1455/1845 train_time:79978ms step_avg:54.97ms
step:1456/1845 train_time:80065ms step_avg:54.99ms
step:1457/1845 train_time:80154ms step_avg:55.01ms
step:1458/1845 train_time:80241ms step_avg:55.04ms
step:1459/1845 train_time:80329ms step_avg:55.06ms
step:1460/1845 train_time:80417ms step_avg:55.08ms
step:1461/1845 train_time:80505ms step_avg:55.10ms
step:1462/1845 train_time:80592ms step_avg:55.12ms
step:1463/1845 train_time:80681ms step_avg:55.15ms
step:1464/1845 train_time:80768ms step_avg:55.17ms
step:1465/1845 train_time:80857ms step_avg:55.19ms
step:1466/1845 train_time:80944ms step_avg:55.21ms
step:1467/1845 train_time:81032ms step_avg:55.24ms
step:1468/1845 train_time:81120ms step_avg:55.26ms
step:1469/1845 train_time:81208ms step_avg:55.28ms
step:1470/1845 train_time:81295ms step_avg:55.30ms
step:1471/1845 train_time:81384ms step_avg:55.33ms
step:1472/1845 train_time:81472ms step_avg:55.35ms
step:1473/1845 train_time:81560ms step_avg:55.37ms
step:1474/1845 train_time:81648ms step_avg:55.39ms
step:1475/1845 train_time:81736ms step_avg:55.41ms
step:1476/1845 train_time:81824ms step_avg:55.44ms
step:1477/1845 train_time:81911ms step_avg:55.46ms
step:1478/1845 train_time:81999ms step_avg:55.48ms
step:1479/1845 train_time:82088ms step_avg:55.50ms
step:1480/1845 train_time:82175ms step_avg:55.52ms
step:1481/1845 train_time:82263ms step_avg:55.55ms
step:1482/1845 train_time:82351ms step_avg:55.57ms
step:1483/1845 train_time:82439ms step_avg:55.59ms
step:1484/1845 train_time:82527ms step_avg:55.61ms
step:1485/1845 train_time:82616ms step_avg:55.63ms
step:1486/1845 train_time:82704ms step_avg:55.66ms
step:1487/1845 train_time:82793ms step_avg:55.68ms
step:1488/1845 train_time:82881ms step_avg:55.70ms
step:1489/1845 train_time:82970ms step_avg:55.72ms
step:1490/1845 train_time:83060ms step_avg:55.74ms
step:1491/1845 train_time:83148ms step_avg:55.77ms
step:1492/1845 train_time:83235ms step_avg:55.79ms
step:1493/1845 train_time:83323ms step_avg:55.81ms
step:1494/1845 train_time:83410ms step_avg:55.83ms
step:1495/1845 train_time:83500ms step_avg:55.85ms
step:1496/1845 train_time:83587ms step_avg:55.87ms
step:1497/1845 train_time:83676ms step_avg:55.90ms
step:1498/1845 train_time:83763ms step_avg:55.92ms
step:1499/1845 train_time:83851ms step_avg:55.94ms
step:1500/1845 train_time:83939ms step_avg:55.96ms
step:1500/1845 val_loss:3.4030 train_time:84029ms step_avg:56.02ms
step:1501/1845 train_time:84049ms step_avg:56.00ms
step:1502/1845 train_time:84119ms step_avg:56.00ms
step:1503/1845 train_time:84211ms step_avg:56.03ms
step:1504/1845 train_time:84299ms step_avg:56.05ms
step:1505/1845 train_time:84388ms step_avg:56.07ms
step:1506/1845 train_time:84475ms step_avg:56.09ms
step:1507/1845 train_time:84562ms step_avg:56.11ms
step:1508/1845 train_time:84649ms step_avg:56.13ms
step:1509/1845 train_time:84736ms step_avg:56.15ms
step:1510/1845 train_time:84824ms step_avg:56.17ms
step:1511/1845 train_time:84911ms step_avg:56.20ms
step:1512/1845 train_time:85000ms step_avg:56.22ms
step:1513/1845 train_time:85090ms step_avg:56.24ms
step:1514/1845 train_time:85179ms step_avg:56.26ms
step:1515/1845 train_time:85270ms step_avg:56.28ms
step:1516/1845 train_time:85358ms step_avg:56.30ms
step:1517/1845 train_time:85447ms step_avg:56.33ms
step:1518/1845 train_time:85535ms step_avg:56.35ms
step:1519/1845 train_time:85622ms step_avg:56.37ms
step:1520/1845 train_time:85708ms step_avg:56.39ms
step:1521/1845 train_time:85796ms step_avg:56.41ms
step:1522/1845 train_time:85883ms step_avg:56.43ms
step:1523/1845 train_time:85971ms step_avg:56.45ms
step:1524/1845 train_time:86060ms step_avg:56.47ms
step:1525/1845 train_time:86149ms step_avg:56.49ms
step:1526/1845 train_time:86238ms step_avg:56.51ms
step:1527/1845 train_time:86328ms step_avg:56.53ms
step:1528/1845 train_time:86416ms step_avg:56.55ms
step:1529/1845 train_time:86504ms step_avg:56.58ms
step:1530/1845 train_time:86591ms step_avg:56.60ms
step:1531/1845 train_time:86680ms step_avg:56.62ms
step:1532/1845 train_time:86766ms step_avg:56.64ms
step:1533/1845 train_time:86854ms step_avg:56.66ms
step:1534/1845 train_time:86940ms step_avg:56.68ms
step:1535/1845 train_time:87029ms step_avg:56.70ms
step:1536/1845 train_time:87117ms step_avg:56.72ms
step:1537/1845 train_time:87206ms step_avg:56.74ms
step:1538/1845 train_time:87295ms step_avg:56.76ms
step:1539/1845 train_time:87383ms step_avg:56.78ms
step:1540/1845 train_time:87470ms step_avg:56.80ms
step:1541/1845 train_time:87557ms step_avg:56.82ms
step:1542/1845 train_time:87645ms step_avg:56.84ms
step:1543/1845 train_time:87732ms step_avg:56.86ms
step:1544/1845 train_time:87819ms step_avg:56.88ms
step:1545/1845 train_time:87907ms step_avg:56.90ms
step:1546/1845 train_time:87995ms step_avg:56.92ms
step:1547/1845 train_time:88085ms step_avg:56.94ms
step:1548/1845 train_time:88174ms step_avg:56.96ms
step:1549/1845 train_time:88264ms step_avg:56.98ms
step:1550/1845 train_time:88352ms step_avg:57.00ms
step:1551/1845 train_time:88440ms step_avg:57.02ms
step:1552/1845 train_time:88527ms step_avg:57.04ms
step:1553/1845 train_time:88615ms step_avg:57.06ms
step:1554/1845 train_time:88702ms step_avg:57.08ms
step:1555/1845 train_time:88790ms step_avg:57.10ms
step:1556/1845 train_time:88877ms step_avg:57.12ms
step:1557/1845 train_time:88966ms step_avg:57.14ms
step:1558/1845 train_time:89054ms step_avg:57.16ms
step:1559/1845 train_time:89144ms step_avg:57.18ms
step:1560/1845 train_time:89233ms step_avg:57.20ms
step:1561/1845 train_time:89321ms step_avg:57.22ms
step:1562/1845 train_time:89408ms step_avg:57.24ms
step:1563/1845 train_time:89497ms step_avg:57.26ms
step:1564/1845 train_time:89584ms step_avg:57.28ms
step:1565/1845 train_time:89671ms step_avg:57.30ms
step:1566/1845 train_time:89758ms step_avg:57.32ms
step:1567/1845 train_time:89847ms step_avg:57.34ms
step:1568/1845 train_time:89934ms step_avg:57.36ms
step:1569/1845 train_time:90023ms step_avg:57.38ms
step:1570/1845 train_time:90111ms step_avg:57.40ms
step:1571/1845 train_time:90200ms step_avg:57.42ms
step:1572/1845 train_time:90287ms step_avg:57.43ms
step:1573/1845 train_time:90376ms step_avg:57.45ms
step:1574/1845 train_time:90464ms step_avg:57.47ms
step:1575/1845 train_time:90551ms step_avg:57.49ms
step:1576/1845 train_time:90638ms step_avg:57.51ms
step:1577/1845 train_time:90726ms step_avg:57.53ms
step:1578/1845 train_time:90815ms step_avg:57.55ms
step:1579/1845 train_time:90904ms step_avg:57.57ms
step:1580/1845 train_time:90992ms step_avg:57.59ms
step:1581/1845 train_time:91081ms step_avg:57.61ms
step:1582/1845 train_time:91168ms step_avg:57.63ms
step:1583/1845 train_time:91257ms step_avg:57.65ms
step:1584/1845 train_time:91345ms step_avg:57.67ms
step:1585/1845 train_time:91433ms step_avg:57.69ms
step:1586/1845 train_time:91520ms step_avg:57.71ms
step:1587/1845 train_time:91609ms step_avg:57.72ms
step:1588/1845 train_time:91697ms step_avg:57.74ms
step:1589/1845 train_time:91784ms step_avg:57.76ms
step:1590/1845 train_time:91872ms step_avg:57.78ms
step:1591/1845 train_time:91960ms step_avg:57.80ms
step:1592/1845 train_time:92048ms step_avg:57.82ms
step:1593/1845 train_time:92136ms step_avg:57.84ms
step:1594/1845 train_time:92223ms step_avg:57.86ms
step:1595/1845 train_time:92312ms step_avg:57.88ms
step:1596/1845 train_time:92400ms step_avg:57.89ms
step:1597/1845 train_time:92488ms step_avg:57.91ms
step:1598/1845 train_time:92576ms step_avg:57.93ms
step:1599/1845 train_time:92663ms step_avg:57.95ms
step:1600/1845 train_time:92751ms step_avg:57.97ms
step:1601/1845 train_time:92839ms step_avg:57.99ms
step:1602/1845 train_time:92926ms step_avg:58.01ms
step:1603/1845 train_time:93016ms step_avg:58.03ms
step:1604/1845 train_time:93103ms step_avg:58.04ms
step:1605/1845 train_time:93193ms step_avg:58.06ms
step:1606/1845 train_time:93281ms step_avg:58.08ms
step:1607/1845 train_time:93369ms step_avg:58.10ms
step:1608/1845 train_time:93456ms step_avg:58.12ms
step:1609/1845 train_time:93544ms step_avg:58.14ms
step:1610/1845 train_time:93632ms step_avg:58.16ms
step:1611/1845 train_time:93720ms step_avg:58.17ms
step:1612/1845 train_time:93808ms step_avg:58.19ms
step:1613/1845 train_time:93897ms step_avg:58.21ms
step:1614/1845 train_time:93985ms step_avg:58.23ms
step:1615/1845 train_time:94073ms step_avg:58.25ms
step:1616/1845 train_time:94160ms step_avg:58.27ms
step:1617/1845 train_time:94249ms step_avg:58.29ms
step:1618/1845 train_time:94337ms step_avg:58.30ms
step:1619/1845 train_time:94426ms step_avg:58.32ms
step:1620/1845 train_time:94514ms step_avg:58.34ms
step:1621/1845 train_time:94602ms step_avg:58.36ms
step:1622/1845 train_time:94689ms step_avg:58.38ms
step:1623/1845 train_time:94777ms step_avg:58.40ms
step:1624/1845 train_time:94865ms step_avg:58.41ms
step:1625/1845 train_time:94953ms step_avg:58.43ms
step:1626/1845 train_time:95040ms step_avg:58.45ms
step:1627/1845 train_time:95130ms step_avg:58.47ms
step:1628/1845 train_time:95217ms step_avg:58.49ms
step:1629/1845 train_time:95306ms step_avg:58.51ms
step:1630/1845 train_time:95395ms step_avg:58.52ms
step:1631/1845 train_time:95483ms step_avg:58.54ms
step:1632/1845 train_time:95571ms step_avg:58.56ms
step:1633/1845 train_time:95659ms step_avg:58.58ms
step:1634/1845 train_time:95746ms step_avg:58.60ms
step:1635/1845 train_time:95834ms step_avg:58.61ms
step:1636/1845 train_time:95922ms step_avg:58.63ms
step:1637/1845 train_time:96010ms step_avg:58.65ms
step:1638/1845 train_time:96097ms step_avg:58.67ms
step:1639/1845 train_time:96186ms step_avg:58.69ms
step:1640/1845 train_time:96274ms step_avg:58.70ms
step:1641/1845 train_time:96363ms step_avg:58.72ms
step:1642/1845 train_time:96450ms step_avg:58.74ms
step:1643/1845 train_time:96538ms step_avg:58.76ms
step:1644/1845 train_time:96626ms step_avg:58.77ms
step:1645/1845 train_time:96714ms step_avg:58.79ms
step:1646/1845 train_time:96800ms step_avg:58.81ms
step:1647/1845 train_time:96889ms step_avg:58.83ms
step:1648/1845 train_time:96977ms step_avg:58.85ms
step:1649/1845 train_time:97066ms step_avg:58.86ms
step:1650/1845 train_time:97154ms step_avg:58.88ms
step:1651/1845 train_time:97241ms step_avg:58.90ms
step:1652/1845 train_time:97330ms step_avg:58.92ms
step:1653/1845 train_time:97418ms step_avg:58.93ms
step:1654/1845 train_time:97506ms step_avg:58.95ms
step:1655/1845 train_time:97594ms step_avg:58.97ms
step:1656/1845 train_time:97682ms step_avg:58.99ms
step:1657/1845 train_time:97770ms step_avg:59.00ms
step:1658/1845 train_time:97857ms step_avg:59.02ms
step:1659/1845 train_time:97946ms step_avg:59.04ms
step:1660/1845 train_time:98034ms step_avg:59.06ms
step:1661/1845 train_time:98123ms step_avg:59.07ms
step:1662/1845 train_time:98211ms step_avg:59.09ms
step:1663/1845 train_time:98299ms step_avg:59.11ms
step:1664/1845 train_time:98388ms step_avg:59.13ms
step:1665/1845 train_time:98476ms step_avg:59.14ms
step:1666/1845 train_time:98563ms step_avg:59.16ms
step:1667/1845 train_time:98651ms step_avg:59.18ms
step:1668/1845 train_time:98738ms step_avg:59.20ms
step:1669/1845 train_time:98827ms step_avg:59.21ms
step:1670/1845 train_time:98915ms step_avg:59.23ms
step:1671/1845 train_time:99003ms step_avg:59.25ms
step:1672/1845 train_time:99091ms step_avg:59.26ms
step:1673/1845 train_time:99180ms step_avg:59.28ms
step:1674/1845 train_time:99268ms step_avg:59.30ms
step:1675/1845 train_time:99357ms step_avg:59.32ms
step:1676/1845 train_time:99444ms step_avg:59.33ms
step:1677/1845 train_time:99533ms step_avg:59.35ms
step:1678/1845 train_time:99620ms step_avg:59.37ms
step:1679/1845 train_time:99709ms step_avg:59.39ms
step:1680/1845 train_time:99796ms step_avg:59.40ms
step:1681/1845 train_time:99885ms step_avg:59.42ms
step:1682/1845 train_time:99973ms step_avg:59.44ms
step:1683/1845 train_time:100061ms step_avg:59.45ms
step:1684/1845 train_time:100148ms step_avg:59.47ms
step:1685/1845 train_time:100237ms step_avg:59.49ms
step:1686/1845 train_time:100326ms step_avg:59.51ms
step:1687/1845 train_time:100414ms step_avg:59.52ms
step:1688/1845 train_time:100501ms step_avg:59.54ms
step:1689/1845 train_time:100589ms step_avg:59.56ms
step:1690/1845 train_time:100678ms step_avg:59.57ms
step:1691/1845 train_time:100766ms step_avg:59.59ms
step:1692/1845 train_time:100853ms step_avg:59.61ms
step:1693/1845 train_time:100941ms step_avg:59.62ms
step:1694/1845 train_time:101029ms step_avg:59.64ms
step:1695/1845 train_time:101118ms step_avg:59.66ms
step:1696/1845 train_time:101206ms step_avg:59.67ms
step:1697/1845 train_time:101294ms step_avg:59.69ms
step:1698/1845 train_time:101382ms step_avg:59.71ms
step:1699/1845 train_time:101470ms step_avg:59.72ms
step:1700/1845 train_time:101557ms step_avg:59.74ms
step:1701/1845 train_time:101645ms step_avg:59.76ms
step:1702/1845 train_time:101734ms step_avg:59.77ms
step:1703/1845 train_time:101823ms step_avg:59.79ms
step:1704/1845 train_time:101910ms step_avg:59.81ms
step:1705/1845 train_time:101999ms step_avg:59.82ms
step:1706/1845 train_time:102086ms step_avg:59.84ms
step:1707/1845 train_time:102174ms step_avg:59.86ms
step:1708/1845 train_time:102261ms step_avg:59.87ms
step:1709/1845 train_time:102350ms step_avg:59.89ms
step:1710/1845 train_time:102437ms step_avg:59.90ms
step:1711/1845 train_time:102525ms step_avg:59.92ms
step:1712/1845 train_time:102613ms step_avg:59.94ms
step:1713/1845 train_time:102701ms step_avg:59.95ms
step:1714/1845 train_time:102788ms step_avg:59.97ms
step:1715/1845 train_time:102876ms step_avg:59.99ms
step:1716/1845 train_time:102963ms step_avg:60.00ms
step:1717/1845 train_time:103052ms step_avg:60.02ms
step:1718/1845 train_time:103139ms step_avg:60.03ms
step:1719/1845 train_time:103228ms step_avg:60.05ms
step:1720/1845 train_time:103316ms step_avg:60.07ms
step:1721/1845 train_time:103404ms step_avg:60.08ms
step:1722/1845 train_time:103492ms step_avg:60.10ms
step:1723/1845 train_time:103580ms step_avg:60.12ms
step:1724/1845 train_time:103668ms step_avg:60.13ms
step:1725/1845 train_time:103756ms step_avg:60.15ms
step:1726/1845 train_time:103844ms step_avg:60.16ms
step:1727/1845 train_time:103932ms step_avg:60.18ms
step:1728/1845 train_time:104020ms step_avg:60.20ms
step:1729/1845 train_time:104110ms step_avg:60.21ms
step:1730/1845 train_time:104197ms step_avg:60.23ms
step:1731/1845 train_time:104287ms step_avg:60.25ms
step:1732/1845 train_time:104374ms step_avg:60.26ms
step:1733/1845 train_time:104463ms step_avg:60.28ms
step:1734/1845 train_time:104550ms step_avg:60.29ms
step:1735/1845 train_time:104638ms step_avg:60.31ms
step:1736/1845 train_time:104725ms step_avg:60.33ms
step:1737/1845 train_time:104813ms step_avg:60.34ms
step:1738/1845 train_time:104901ms step_avg:60.36ms
step:1739/1845 train_time:104990ms step_avg:60.37ms
step:1740/1845 train_time:105078ms step_avg:60.39ms
step:1741/1845 train_time:105166ms step_avg:60.41ms
step:1742/1845 train_time:105255ms step_avg:60.42ms
step:1743/1845 train_time:105345ms step_avg:60.44ms
step:1744/1845 train_time:105432ms step_avg:60.45ms
step:1745/1845 train_time:105520ms step_avg:60.47ms
step:1746/1845 train_time:105607ms step_avg:60.49ms
step:1747/1845 train_time:105696ms step_avg:60.50ms
step:1748/1845 train_time:105784ms step_avg:60.52ms
step:1749/1845 train_time:105872ms step_avg:60.53ms
step:1750/1845 train_time:105959ms step_avg:60.55ms
step:1750/1845 val_loss:3.3043 train_time:106049ms step_avg:60.60ms
step:1751/1845 train_time:106068ms step_avg:60.58ms
step:1752/1845 train_time:106140ms step_avg:60.58ms
step:1753/1845 train_time:106231ms step_avg:60.60ms
step:1754/1845 train_time:106319ms step_avg:60.61ms
step:1755/1845 train_time:106406ms step_avg:60.63ms
step:1756/1845 train_time:106492ms step_avg:60.64ms
step:1757/1845 train_time:106580ms step_avg:60.66ms
step:1758/1845 train_time:106667ms step_avg:60.68ms
step:1759/1845 train_time:106754ms step_avg:60.69ms
step:1760/1845 train_time:106841ms step_avg:60.71ms
step:1761/1845 train_time:106928ms step_avg:60.72ms
step:1762/1845 train_time:107016ms step_avg:60.74ms
step:1763/1845 train_time:107108ms step_avg:60.75ms
step:1764/1845 train_time:107198ms step_avg:60.77ms
step:1765/1845 train_time:107287ms step_avg:60.79ms
step:1766/1845 train_time:107374ms step_avg:60.80ms
step:1767/1845 train_time:107463ms step_avg:60.82ms
step:1768/1845 train_time:107550ms step_avg:60.83ms
step:1769/1845 train_time:107638ms step_avg:60.85ms
step:1770/1845 train_time:107724ms step_avg:60.86ms
step:1771/1845 train_time:107811ms step_avg:60.88ms
step:1772/1845 train_time:107898ms step_avg:60.89ms
step:1773/1845 train_time:107985ms step_avg:60.91ms
step:1774/1845 train_time:108074ms step_avg:60.92ms
step:1775/1845 train_time:108164ms step_avg:60.94ms
step:1776/1845 train_time:108252ms step_avg:60.95ms
step:1777/1845 train_time:108342ms step_avg:60.97ms
step:1778/1845 train_time:108429ms step_avg:60.98ms
step:1779/1845 train_time:108517ms step_avg:61.00ms
step:1780/1845 train_time:108604ms step_avg:61.01ms
step:1781/1845 train_time:108693ms step_avg:61.03ms
step:1782/1845 train_time:108779ms step_avg:61.04ms
step:1783/1845 train_time:108866ms step_avg:61.06ms
step:1784/1845 train_time:108954ms step_avg:61.07ms
step:1785/1845 train_time:109043ms step_avg:61.09ms
step:1786/1845 train_time:109131ms step_avg:61.10ms
step:1787/1845 train_time:109220ms step_avg:61.12ms
step:1788/1845 train_time:109308ms step_avg:61.13ms
step:1789/1845 train_time:109396ms step_avg:61.15ms
step:1790/1845 train_time:109483ms step_avg:61.16ms
step:1791/1845 train_time:109571ms step_avg:61.18ms
step:1792/1845 train_time:109658ms step_avg:61.19ms
step:1793/1845 train_time:109747ms step_avg:61.21ms
step:1794/1845 train_time:109833ms step_avg:61.22ms
step:1795/1845 train_time:109922ms step_avg:61.24ms
step:1796/1845 train_time:110009ms step_avg:61.25ms
step:1797/1845 train_time:110097ms step_avg:61.27ms
step:1798/1845 train_time:110185ms step_avg:61.28ms
step:1799/1845 train_time:110273ms step_avg:61.30ms
step:1800/1845 train_time:110361ms step_avg:61.31ms
step:1801/1845 train_time:110451ms step_avg:61.33ms
step:1802/1845 train_time:110538ms step_avg:61.34ms
step:1803/1845 train_time:110626ms step_avg:61.36ms
step:1804/1845 train_time:110713ms step_avg:61.37ms
step:1805/1845 train_time:110801ms step_avg:61.39ms
step:1806/1845 train_time:110888ms step_avg:61.40ms
step:1807/1845 train_time:110976ms step_avg:61.41ms
step:1808/1845 train_time:111064ms step_avg:61.43ms
step:1809/1845 train_time:111152ms step_avg:61.44ms
step:1810/1845 train_time:111240ms step_avg:61.46ms
step:1811/1845 train_time:111330ms step_avg:61.47ms
step:1812/1845 train_time:111419ms step_avg:61.49ms
step:1813/1845 train_time:111508ms step_avg:61.50ms
step:1814/1845 train_time:111596ms step_avg:61.52ms
step:1815/1845 train_time:111684ms step_avg:61.53ms
step:1816/1845 train_time:111771ms step_avg:61.55ms
step:1817/1845 train_time:111860ms step_avg:61.56ms
step:1818/1845 train_time:111948ms step_avg:61.58ms
step:1819/1845 train_time:112037ms step_avg:61.59ms
step:1820/1845 train_time:112126ms step_avg:61.61ms
step:1821/1845 train_time:112215ms step_avg:61.62ms
step:1822/1845 train_time:112303ms step_avg:61.64ms
step:1823/1845 train_time:112392ms step_avg:61.65ms
step:1824/1845 train_time:112480ms step_avg:61.67ms
step:1825/1845 train_time:112567ms step_avg:61.68ms
step:1826/1845 train_time:112655ms step_avg:61.69ms
step:1827/1845 train_time:112742ms step_avg:61.71ms
step:1828/1845 train_time:112830ms step_avg:61.72ms
step:1829/1845 train_time:112918ms step_avg:61.74ms
step:1830/1845 train_time:113005ms step_avg:61.75ms
step:1831/1845 train_time:113093ms step_avg:61.77ms
step:1832/1845 train_time:113181ms step_avg:61.78ms
step:1833/1845 train_time:113270ms step_avg:61.79ms
step:1834/1845 train_time:113359ms step_avg:61.81ms
step:1835/1845 train_time:113448ms step_avg:61.82ms
step:1836/1845 train_time:113535ms step_avg:61.84ms
step:1837/1845 train_time:113623ms step_avg:61.85ms
step:1838/1845 train_time:113711ms step_avg:61.87ms
step:1839/1845 train_time:113801ms step_avg:61.88ms
step:1840/1845 train_time:113889ms step_avg:61.90ms
step:1841/1845 train_time:113978ms step_avg:61.91ms
step:1842/1845 train_time:114066ms step_avg:61.92ms
step:1843/1845 train_time:114154ms step_avg:61.94ms
step:1844/1845 train_time:114242ms step_avg:61.95ms
step:1845/1845 train_time:114330ms step_avg:61.97ms
step:1845/1845 val_loss:3.2778 train_time:114419ms step_avg:62.02ms
peak memory allocated: 29405 MiB reserved: 44458 MiB

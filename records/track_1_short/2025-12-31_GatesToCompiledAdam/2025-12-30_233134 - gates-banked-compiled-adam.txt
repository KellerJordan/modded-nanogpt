# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29 - gates-banked-compiled-adam - bf16 gates - fp32 exp_avg"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            #exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device) # Testing making the optimizer state fp32.
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay (lr as weight decay schedule)
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0))
                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()  # Testing leaving these as fp32
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

if master_process:
    import wandb
    wandb.login()
    wandb.init(
        project=WANDB_PROJECT,
        name=args.run_id,
        config=args
    )
    wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        if master_process:
            # Log to wandb
            wandb.log({
                "val/loss": val_loss.item(),
                "time/training_time_ms": training_time_ms,
                "time/step_avg_ms": training_time_ms / (step + 1),
            }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process:
            final_metrics = {
                "final/val_loss": float(val_loss.item()),
                "final/train_time_ms": int(training_time_ms),
                "final/train_time": training_time_ms / 1000,
                "final/steps_total": int(step),
            }
            wandb.log(final_metrics)
            wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
if master_process:
    wandb.save(f"{args.log_dir}/{args.run_id}.txt")
    wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 07:31:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          326041      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          326042      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          326043      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          326044      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          326045      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    5   N/A  N/A          326046      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          326047      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    7   N/A  N/A          326048      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8309 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:64ms step_avg:63.61ms
step:2/1845 train_time:93ms step_avg:46.55ms
step:3/1845 train_time:120ms step_avg:40.14ms
step:4/1845 train_time:151ms step_avg:37.73ms
step:5/1845 train_time:178ms step_avg:35.69ms
step:6/1845 train_time:304ms step_avg:50.68ms
step:7/1845 train_time:336ms step_avg:48.03ms
step:8/1845 train_time:372ms step_avg:46.47ms
step:9/1845 train_time:405ms step_avg:44.99ms
step:10/1845 train_time:438ms step_avg:43.83ms
step:11/1845 train_time:472ms step_avg:42.87ms
step:12/1845 train_time:508ms step_avg:42.33ms
step:13/1845 train_time:537ms step_avg:41.29ms
step:14/1845 train_time:569ms step_avg:40.64ms
step:15/1845 train_time:600ms step_avg:40.02ms
step:16/1845 train_time:635ms step_avg:39.69ms
step:17/1845 train_time:664ms step_avg:39.07ms
step:18/1845 train_time:699ms step_avg:38.82ms
step:19/1845 train_time:730ms step_avg:38.42ms
step:20/1845 train_time:762ms step_avg:38.11ms
step:21/1845 train_time:791ms step_avg:37.69ms
step:22/1845 train_time:825ms step_avg:37.50ms
step:23/1845 train_time:856ms step_avg:37.21ms
step:24/1845 train_time:890ms step_avg:37.10ms
step:25/1845 train_time:919ms step_avg:36.77ms
step:26/1845 train_time:951ms step_avg:36.59ms
step:27/1845 train_time:983ms step_avg:36.39ms
step:28/1845 train_time:1018ms step_avg:36.36ms
step:29/1845 train_time:1048ms step_avg:36.14ms
step:30/1845 train_time:1086ms step_avg:36.21ms
step:31/1845 train_time:1116ms step_avg:35.99ms
step:32/1845 train_time:1154ms step_avg:36.06ms
step:33/1845 train_time:1184ms step_avg:35.87ms
step:34/1845 train_time:1222ms step_avg:35.94ms
step:35/1845 train_time:1253ms step_avg:35.79ms
step:36/1845 train_time:1291ms step_avg:35.87ms
step:37/1845 train_time:1321ms step_avg:35.71ms
step:38/1845 train_time:1359ms step_avg:35.78ms
step:39/1845 train_time:1389ms step_avg:35.63ms
step:40/1845 train_time:1427ms step_avg:35.68ms
step:41/1845 train_time:1457ms step_avg:35.55ms
step:42/1845 train_time:1496ms step_avg:35.61ms
step:43/1845 train_time:1527ms step_avg:35.51ms
step:44/1845 train_time:1563ms step_avg:35.53ms
step:45/1845 train_time:1593ms step_avg:35.41ms
step:46/1845 train_time:1631ms step_avg:35.47ms
step:47/1845 train_time:1661ms step_avg:35.35ms
step:48/1845 train_time:1700ms step_avg:35.41ms
step:49/1845 train_time:1729ms step_avg:35.29ms
step:50/1845 train_time:1767ms step_avg:35.35ms
step:51/1845 train_time:1797ms step_avg:35.24ms
step:52/1845 train_time:1835ms step_avg:35.30ms
step:53/1845 train_time:1867ms step_avg:35.22ms
step:54/1845 train_time:1904ms step_avg:35.25ms
step:55/1845 train_time:1934ms step_avg:35.17ms
step:56/1845 train_time:1972ms step_avg:35.21ms
step:57/1845 train_time:2002ms step_avg:35.13ms
step:58/1845 train_time:2040ms step_avg:35.17ms
step:59/1845 train_time:2071ms step_avg:35.10ms
step:60/1845 train_time:2108ms step_avg:35.13ms
step:61/1845 train_time:2138ms step_avg:35.05ms
step:62/1845 train_time:2178ms step_avg:35.12ms
step:63/1845 train_time:2209ms step_avg:35.06ms
step:64/1845 train_time:2245ms step_avg:35.08ms
step:65/1845 train_time:2275ms step_avg:35.00ms
step:66/1845 train_time:2313ms step_avg:35.05ms
step:67/1845 train_time:2344ms step_avg:34.98ms
step:68/1845 train_time:2382ms step_avg:35.03ms
step:69/1845 train_time:2412ms step_avg:34.95ms
step:70/1845 train_time:2450ms step_avg:35.00ms
step:71/1845 train_time:2480ms step_avg:34.93ms
step:72/1845 train_time:2518ms step_avg:34.97ms
step:73/1845 train_time:2548ms step_avg:34.90ms
step:74/1845 train_time:2586ms step_avg:34.94ms
step:75/1845 train_time:2616ms step_avg:34.88ms
step:76/1845 train_time:2654ms step_avg:34.92ms
step:77/1845 train_time:2684ms step_avg:34.86ms
step:78/1845 train_time:2722ms step_avg:34.90ms
step:79/1845 train_time:2753ms step_avg:34.84ms
step:80/1845 train_time:2791ms step_avg:34.88ms
step:81/1845 train_time:2821ms step_avg:34.82ms
step:82/1845 train_time:2859ms step_avg:34.86ms
step:83/1845 train_time:2888ms step_avg:34.80ms
step:84/1845 train_time:2926ms step_avg:34.84ms
step:85/1845 train_time:2957ms step_avg:34.78ms
step:86/1845 train_time:2995ms step_avg:34.83ms
step:87/1845 train_time:3025ms step_avg:34.77ms
step:88/1845 train_time:3063ms step_avg:34.81ms
step:89/1845 train_time:3093ms step_avg:34.75ms
step:90/1845 train_time:3131ms step_avg:34.79ms
step:91/1845 train_time:3161ms step_avg:34.74ms
step:92/1845 train_time:3199ms step_avg:34.77ms
step:93/1845 train_time:3229ms step_avg:34.72ms
step:94/1845 train_time:3267ms step_avg:34.76ms
step:95/1845 train_time:3297ms step_avg:34.71ms
step:96/1845 train_time:3335ms step_avg:34.74ms
step:97/1845 train_time:3366ms step_avg:34.70ms
step:98/1845 train_time:3404ms step_avg:34.74ms
step:99/1845 train_time:3434ms step_avg:34.69ms
step:100/1845 train_time:3472ms step_avg:34.72ms
step:101/1845 train_time:3502ms step_avg:34.67ms
step:102/1845 train_time:3540ms step_avg:34.71ms
step:103/1845 train_time:3570ms step_avg:34.66ms
step:104/1845 train_time:3608ms step_avg:34.69ms
step:105/1845 train_time:3638ms step_avg:34.65ms
step:106/1845 train_time:3676ms step_avg:34.68ms
step:107/1845 train_time:3707ms step_avg:34.64ms
step:108/1845 train_time:3745ms step_avg:34.67ms
step:109/1845 train_time:3775ms step_avg:34.63ms
step:110/1845 train_time:3813ms step_avg:34.66ms
step:111/1845 train_time:3843ms step_avg:34.62ms
step:112/1845 train_time:3880ms step_avg:34.65ms
step:113/1845 train_time:3910ms step_avg:34.60ms
step:114/1845 train_time:3948ms step_avg:34.64ms
step:115/1845 train_time:3978ms step_avg:34.59ms
step:116/1845 train_time:4017ms step_avg:34.63ms
step:117/1845 train_time:4047ms step_avg:34.59ms
step:118/1845 train_time:4085ms step_avg:34.62ms
step:119/1845 train_time:4115ms step_avg:34.58ms
step:120/1845 train_time:4153ms step_avg:34.61ms
step:121/1845 train_time:4183ms step_avg:34.57ms
step:122/1845 train_time:4221ms step_avg:34.60ms
step:123/1845 train_time:4251ms step_avg:34.56ms
step:124/1845 train_time:4289ms step_avg:34.59ms
step:125/1845 train_time:4319ms step_avg:34.55ms
step:126/1845 train_time:4357ms step_avg:34.58ms
step:127/1845 train_time:4387ms step_avg:34.55ms
step:128/1845 train_time:4425ms step_avg:34.57ms
step:129/1845 train_time:4455ms step_avg:34.54ms
step:130/1845 train_time:4494ms step_avg:34.57ms
step:131/1845 train_time:4523ms step_avg:34.53ms
step:132/1845 train_time:4561ms step_avg:34.55ms
step:133/1845 train_time:4591ms step_avg:34.52ms
step:134/1845 train_time:4629ms step_avg:34.54ms
step:135/1845 train_time:4659ms step_avg:34.51ms
step:136/1845 train_time:4697ms step_avg:34.54ms
step:137/1845 train_time:4727ms step_avg:34.50ms
step:138/1845 train_time:4765ms step_avg:34.53ms
step:139/1845 train_time:4795ms step_avg:34.50ms
step:140/1845 train_time:4833ms step_avg:34.52ms
step:141/1845 train_time:4863ms step_avg:34.49ms
step:142/1845 train_time:4901ms step_avg:34.52ms
step:143/1845 train_time:4931ms step_avg:34.48ms
step:144/1845 train_time:4970ms step_avg:34.51ms
step:145/1845 train_time:4999ms step_avg:34.48ms
step:146/1845 train_time:5037ms step_avg:34.50ms
step:147/1845 train_time:5068ms step_avg:34.47ms
step:148/1845 train_time:5106ms step_avg:34.50ms
step:149/1845 train_time:5135ms step_avg:34.47ms
step:150/1845 train_time:5173ms step_avg:34.49ms
step:151/1845 train_time:5203ms step_avg:34.46ms
step:152/1845 train_time:5241ms step_avg:34.48ms
step:153/1845 train_time:5271ms step_avg:34.45ms
step:154/1845 train_time:5309ms step_avg:34.48ms
step:155/1845 train_time:5339ms step_avg:34.45ms
step:156/1845 train_time:5378ms step_avg:34.47ms
step:157/1845 train_time:5408ms step_avg:34.44ms
step:158/1845 train_time:5445ms step_avg:34.46ms
step:159/1845 train_time:5475ms step_avg:34.44ms
step:160/1845 train_time:5514ms step_avg:34.46ms
step:161/1845 train_time:5543ms step_avg:34.43ms
step:162/1845 train_time:5581ms step_avg:34.45ms
step:163/1845 train_time:5611ms step_avg:34.42ms
step:164/1845 train_time:5649ms step_avg:34.45ms
step:165/1845 train_time:5679ms step_avg:34.42ms
step:166/1845 train_time:5718ms step_avg:34.44ms
step:167/1845 train_time:5747ms step_avg:34.41ms
step:168/1845 train_time:5785ms step_avg:34.43ms
step:169/1845 train_time:5815ms step_avg:34.41ms
step:170/1845 train_time:5853ms step_avg:34.43ms
step:171/1845 train_time:5886ms step_avg:34.42ms
step:172/1845 train_time:5925ms step_avg:34.45ms
step:173/1845 train_time:5957ms step_avg:34.43ms
step:174/1845 train_time:5992ms step_avg:34.44ms
step:175/1845 train_time:6024ms step_avg:34.42ms
step:176/1845 train_time:6059ms step_avg:34.43ms
step:177/1845 train_time:6090ms step_avg:34.41ms
step:178/1845 train_time:6126ms step_avg:34.41ms
step:179/1845 train_time:6156ms step_avg:34.39ms
step:180/1845 train_time:6194ms step_avg:34.41ms
step:181/1845 train_time:6225ms step_avg:34.39ms
step:182/1845 train_time:6261ms step_avg:34.40ms
step:183/1845 train_time:6292ms step_avg:34.38ms
step:184/1845 train_time:6331ms step_avg:34.41ms
step:185/1845 train_time:6365ms step_avg:34.41ms
step:186/1845 train_time:6402ms step_avg:34.42ms
step:187/1845 train_time:6435ms step_avg:34.41ms
step:188/1845 train_time:6472ms step_avg:34.42ms
step:189/1845 train_time:6505ms step_avg:34.42ms
step:190/1845 train_time:6544ms step_avg:34.44ms
step:191/1845 train_time:6577ms step_avg:34.43ms
step:192/1845 train_time:6612ms step_avg:34.44ms
step:193/1845 train_time:6644ms step_avg:34.42ms
step:194/1845 train_time:6679ms step_avg:34.43ms
step:195/1845 train_time:6711ms step_avg:34.41ms
step:196/1845 train_time:6746ms step_avg:34.42ms
step:197/1845 train_time:6777ms step_avg:34.40ms
step:198/1845 train_time:6812ms step_avg:34.40ms
step:199/1845 train_time:6843ms step_avg:34.39ms
step:200/1845 train_time:6878ms step_avg:34.39ms
step:201/1845 train_time:6909ms step_avg:34.37ms
step:202/1845 train_time:6943ms step_avg:34.37ms
step:203/1845 train_time:6974ms step_avg:34.36ms
step:204/1845 train_time:7010ms step_avg:34.36ms
step:205/1845 train_time:7040ms step_avg:34.34ms
step:206/1845 train_time:7077ms step_avg:34.36ms
step:207/1845 train_time:7108ms step_avg:34.34ms
step:208/1845 train_time:7146ms step_avg:34.35ms
step:209/1845 train_time:7176ms step_avg:34.34ms
step:210/1845 train_time:7214ms step_avg:34.35ms
step:211/1845 train_time:7245ms step_avg:34.34ms
step:212/1845 train_time:7281ms step_avg:34.34ms
step:213/1845 train_time:7313ms step_avg:34.33ms
step:214/1845 train_time:7349ms step_avg:34.34ms
step:215/1845 train_time:7380ms step_avg:34.33ms
step:216/1845 train_time:7417ms step_avg:34.34ms
step:217/1845 train_time:7449ms step_avg:34.33ms
step:218/1845 train_time:7485ms step_avg:34.33ms
step:219/1845 train_time:7518ms step_avg:34.33ms
step:220/1845 train_time:7553ms step_avg:34.33ms
step:221/1845 train_time:7584ms step_avg:34.32ms
step:222/1845 train_time:7621ms step_avg:34.33ms
step:223/1845 train_time:7652ms step_avg:34.31ms
step:224/1845 train_time:7688ms step_avg:34.32ms
step:225/1845 train_time:7719ms step_avg:34.31ms
step:226/1845 train_time:7756ms step_avg:34.32ms
step:227/1845 train_time:7787ms step_avg:34.30ms
step:228/1845 train_time:7824ms step_avg:34.32ms
step:229/1845 train_time:7855ms step_avg:34.30ms
step:230/1845 train_time:7892ms step_avg:34.31ms
step:231/1845 train_time:7923ms step_avg:34.30ms
step:232/1845 train_time:7960ms step_avg:34.31ms
step:233/1845 train_time:7991ms step_avg:34.30ms
step:234/1845 train_time:8028ms step_avg:34.31ms
step:235/1845 train_time:8060ms step_avg:34.30ms
step:236/1845 train_time:8096ms step_avg:34.30ms
step:237/1845 train_time:8127ms step_avg:34.29ms
step:238/1845 train_time:8164ms step_avg:34.30ms
step:239/1845 train_time:8195ms step_avg:34.29ms
step:240/1845 train_time:8232ms step_avg:34.30ms
step:241/1845 train_time:8263ms step_avg:34.29ms
step:242/1845 train_time:8299ms step_avg:34.29ms
step:243/1845 train_time:8332ms step_avg:34.29ms
step:244/1845 train_time:8367ms step_avg:34.29ms
step:245/1845 train_time:8398ms step_avg:34.28ms
step:246/1845 train_time:8435ms step_avg:34.29ms
step:247/1845 train_time:8466ms step_avg:34.28ms
step:248/1845 train_time:8503ms step_avg:34.29ms
step:249/1845 train_time:8535ms step_avg:34.28ms
step:250/1845 train_time:8571ms step_avg:34.28ms
step:250/1845 val_loss:4.6169 train_time:8609ms step_avg:34.43ms
step:251/1845 train_time:8635ms step_avg:34.40ms
step:252/1845 train_time:8661ms step_avg:34.37ms
step:253/1845 train_time:8685ms step_avg:34.33ms
step:254/1845 train_time:8711ms step_avg:34.30ms
step:255/1845 train_time:8739ms step_avg:34.27ms
step:256/1845 train_time:8776ms step_avg:34.28ms
step:257/1845 train_time:8808ms step_avg:34.27ms
step:258/1845 train_time:8846ms step_avg:34.29ms
step:259/1845 train_time:8877ms step_avg:34.28ms
step:260/1845 train_time:8915ms step_avg:34.29ms
step:261/1845 train_time:8946ms step_avg:34.28ms
step:262/1845 train_time:8982ms step_avg:34.28ms
step:263/1845 train_time:9015ms step_avg:34.28ms
step:264/1845 train_time:9051ms step_avg:34.28ms
step:265/1845 train_time:9084ms step_avg:34.28ms
step:266/1845 train_time:9120ms step_avg:34.29ms
step:267/1845 train_time:9153ms step_avg:34.28ms
step:268/1845 train_time:9190ms step_avg:34.29ms
step:269/1845 train_time:9222ms step_avg:34.28ms
step:270/1845 train_time:9258ms step_avg:34.29ms
step:271/1845 train_time:9290ms step_avg:34.28ms
step:272/1845 train_time:9324ms step_avg:34.28ms
step:273/1845 train_time:9356ms step_avg:34.27ms
step:274/1845 train_time:9391ms step_avg:34.27ms
step:275/1845 train_time:9422ms step_avg:34.26ms
step:276/1845 train_time:9457ms step_avg:34.26ms
step:277/1845 train_time:9489ms step_avg:34.26ms
step:278/1845 train_time:9524ms step_avg:34.26ms
step:279/1845 train_time:9556ms step_avg:34.25ms
step:280/1845 train_time:9593ms step_avg:34.26ms
step:281/1845 train_time:9625ms step_avg:34.25ms
step:282/1845 train_time:9660ms step_avg:34.26ms
step:283/1845 train_time:9692ms step_avg:34.25ms
step:284/1845 train_time:9728ms step_avg:34.25ms
step:285/1845 train_time:9760ms step_avg:34.24ms
step:286/1845 train_time:9797ms step_avg:34.25ms
step:287/1845 train_time:9829ms step_avg:34.25ms
step:288/1845 train_time:9865ms step_avg:34.25ms
step:289/1845 train_time:9897ms step_avg:34.25ms
step:290/1845 train_time:9933ms step_avg:34.25ms
step:291/1845 train_time:9965ms step_avg:34.24ms
step:292/1845 train_time:10001ms step_avg:34.25ms
step:293/1845 train_time:10032ms step_avg:34.24ms
step:294/1845 train_time:10068ms step_avg:34.25ms
step:295/1845 train_time:10100ms step_avg:34.24ms
step:296/1845 train_time:10136ms step_avg:34.24ms
step:297/1845 train_time:10169ms step_avg:34.24ms
step:298/1845 train_time:10204ms step_avg:34.24ms
step:299/1845 train_time:10235ms step_avg:34.23ms
step:300/1845 train_time:10272ms step_avg:34.24ms
step:301/1845 train_time:10304ms step_avg:34.23ms
step:302/1845 train_time:10340ms step_avg:34.24ms
step:303/1845 train_time:10372ms step_avg:34.23ms
step:304/1845 train_time:10408ms step_avg:34.24ms
step:305/1845 train_time:10439ms step_avg:34.23ms
step:306/1845 train_time:10475ms step_avg:34.23ms
step:307/1845 train_time:10508ms step_avg:34.23ms
step:308/1845 train_time:10543ms step_avg:34.23ms
step:309/1845 train_time:10575ms step_avg:34.22ms
step:310/1845 train_time:10611ms step_avg:34.23ms
step:311/1845 train_time:10645ms step_avg:34.23ms
step:312/1845 train_time:10681ms step_avg:34.23ms
step:313/1845 train_time:10713ms step_avg:34.23ms
step:314/1845 train_time:10749ms step_avg:34.23ms
step:315/1845 train_time:10780ms step_avg:34.22ms
step:316/1845 train_time:10815ms step_avg:34.23ms
step:317/1845 train_time:10846ms step_avg:34.22ms
step:318/1845 train_time:10883ms step_avg:34.22ms
step:319/1845 train_time:10914ms step_avg:34.21ms
step:320/1845 train_time:10950ms step_avg:34.22ms
step:321/1845 train_time:10982ms step_avg:34.21ms
step:322/1845 train_time:11019ms step_avg:34.22ms
step:323/1845 train_time:11050ms step_avg:34.21ms
step:324/1845 train_time:11087ms step_avg:34.22ms
step:325/1845 train_time:11118ms step_avg:34.21ms
step:326/1845 train_time:11155ms step_avg:34.22ms
step:327/1845 train_time:11187ms step_avg:34.21ms
step:328/1845 train_time:11222ms step_avg:34.21ms
step:329/1845 train_time:11252ms step_avg:34.20ms
step:330/1845 train_time:11290ms step_avg:34.21ms
step:331/1845 train_time:11320ms step_avg:34.20ms
step:332/1845 train_time:11358ms step_avg:34.21ms
step:333/1845 train_time:11389ms step_avg:34.20ms
step:334/1845 train_time:11426ms step_avg:34.21ms
step:335/1845 train_time:11456ms step_avg:34.20ms
step:336/1845 train_time:11493ms step_avg:34.21ms
step:337/1845 train_time:11525ms step_avg:34.20ms
step:338/1845 train_time:11561ms step_avg:34.21ms
step:339/1845 train_time:11591ms step_avg:34.19ms
step:340/1845 train_time:11629ms step_avg:34.20ms
step:341/1845 train_time:11660ms step_avg:34.19ms
step:342/1845 train_time:11697ms step_avg:34.20ms
step:343/1845 train_time:11728ms step_avg:34.19ms
step:344/1845 train_time:11765ms step_avg:34.20ms
step:345/1845 train_time:11795ms step_avg:34.19ms
step:346/1845 train_time:11833ms step_avg:34.20ms
step:347/1845 train_time:11864ms step_avg:34.19ms
step:348/1845 train_time:11901ms step_avg:34.20ms
step:349/1845 train_time:11931ms step_avg:34.19ms
step:350/1845 train_time:11969ms step_avg:34.20ms
step:351/1845 train_time:11999ms step_avg:34.19ms
step:352/1845 train_time:12037ms step_avg:34.20ms
step:353/1845 train_time:12067ms step_avg:34.18ms
step:354/1845 train_time:12105ms step_avg:34.20ms
step:355/1845 train_time:12135ms step_avg:34.18ms
step:356/1845 train_time:12173ms step_avg:34.19ms
step:357/1845 train_time:12203ms step_avg:34.18ms
step:358/1845 train_time:12241ms step_avg:34.19ms
step:359/1845 train_time:12273ms step_avg:34.19ms
step:360/1845 train_time:12310ms step_avg:34.19ms
step:361/1845 train_time:12339ms step_avg:34.18ms
step:362/1845 train_time:12377ms step_avg:34.19ms
step:363/1845 train_time:12408ms step_avg:34.18ms
step:364/1845 train_time:12445ms step_avg:34.19ms
step:365/1845 train_time:12475ms step_avg:34.18ms
step:366/1845 train_time:12513ms step_avg:34.19ms
step:367/1845 train_time:12543ms step_avg:34.18ms
step:368/1845 train_time:12581ms step_avg:34.19ms
step:369/1845 train_time:12613ms step_avg:34.18ms
step:370/1845 train_time:12649ms step_avg:34.19ms
step:371/1845 train_time:12679ms step_avg:34.17ms
step:372/1845 train_time:12717ms step_avg:34.19ms
step:373/1845 train_time:12747ms step_avg:34.17ms
step:374/1845 train_time:12785ms step_avg:34.18ms
step:375/1845 train_time:12815ms step_avg:34.17ms
step:376/1845 train_time:12852ms step_avg:34.18ms
step:377/1845 train_time:12882ms step_avg:34.17ms
step:378/1845 train_time:12920ms step_avg:34.18ms
step:379/1845 train_time:12950ms step_avg:34.17ms
step:380/1845 train_time:12988ms step_avg:34.18ms
step:381/1845 train_time:13020ms step_avg:34.17ms
step:382/1845 train_time:13056ms step_avg:34.18ms
step:383/1845 train_time:13086ms step_avg:34.17ms
step:384/1845 train_time:13124ms step_avg:34.18ms
step:385/1845 train_time:13154ms step_avg:34.17ms
step:386/1845 train_time:13192ms step_avg:34.18ms
step:387/1845 train_time:13221ms step_avg:34.16ms
step:388/1845 train_time:13260ms step_avg:34.18ms
step:389/1845 train_time:13290ms step_avg:34.16ms
step:390/1845 train_time:13328ms step_avg:34.17ms
step:391/1845 train_time:13357ms step_avg:34.16ms
step:392/1845 train_time:13396ms step_avg:34.17ms
step:393/1845 train_time:13425ms step_avg:34.16ms
step:394/1845 train_time:13464ms step_avg:34.17ms
step:395/1845 train_time:13494ms step_avg:34.16ms
step:396/1845 train_time:13532ms step_avg:34.17ms
step:397/1845 train_time:13562ms step_avg:34.16ms
step:398/1845 train_time:13600ms step_avg:34.17ms
step:399/1845 train_time:13630ms step_avg:34.16ms
step:400/1845 train_time:13667ms step_avg:34.17ms
step:401/1845 train_time:13697ms step_avg:34.16ms
step:402/1845 train_time:13736ms step_avg:34.17ms
step:403/1845 train_time:13767ms step_avg:34.16ms
step:404/1845 train_time:13804ms step_avg:34.17ms
step:405/1845 train_time:13834ms step_avg:34.16ms
step:406/1845 train_time:13872ms step_avg:34.17ms
step:407/1845 train_time:13903ms step_avg:34.16ms
step:408/1845 train_time:13940ms step_avg:34.17ms
step:409/1845 train_time:13971ms step_avg:34.16ms
step:410/1845 train_time:14007ms step_avg:34.16ms
step:411/1845 train_time:14038ms step_avg:34.16ms
step:412/1845 train_time:14075ms step_avg:34.16ms
step:413/1845 train_time:14105ms step_avg:34.15ms
step:414/1845 train_time:14143ms step_avg:34.16ms
step:415/1845 train_time:14173ms step_avg:34.15ms
step:416/1845 train_time:14211ms step_avg:34.16ms
step:417/1845 train_time:14241ms step_avg:34.15ms
step:418/1845 train_time:14279ms step_avg:34.16ms
step:419/1845 train_time:14309ms step_avg:34.15ms
step:420/1845 train_time:14347ms step_avg:34.16ms
step:421/1845 train_time:14378ms step_avg:34.15ms
step:422/1845 train_time:14415ms step_avg:34.16ms
step:423/1845 train_time:14446ms step_avg:34.15ms
step:424/1845 train_time:14483ms step_avg:34.16ms
step:425/1845 train_time:14512ms step_avg:34.15ms
step:426/1845 train_time:14551ms step_avg:34.16ms
step:427/1845 train_time:14580ms step_avg:34.15ms
step:428/1845 train_time:14618ms step_avg:34.16ms
step:429/1845 train_time:14648ms step_avg:34.14ms
step:430/1845 train_time:14686ms step_avg:34.15ms
step:431/1845 train_time:14717ms step_avg:34.15ms
step:432/1845 train_time:14754ms step_avg:34.15ms
step:433/1845 train_time:14784ms step_avg:34.14ms
step:434/1845 train_time:14823ms step_avg:34.15ms
step:435/1845 train_time:14854ms step_avg:34.15ms
step:436/1845 train_time:14890ms step_avg:34.15ms
step:437/1845 train_time:14920ms step_avg:34.14ms
step:438/1845 train_time:14958ms step_avg:34.15ms
step:439/1845 train_time:14989ms step_avg:34.14ms
step:440/1845 train_time:15026ms step_avg:34.15ms
step:441/1845 train_time:15056ms step_avg:34.14ms
step:442/1845 train_time:15093ms step_avg:34.15ms
step:443/1845 train_time:15123ms step_avg:34.14ms
step:444/1845 train_time:15161ms step_avg:34.15ms
step:445/1845 train_time:15191ms step_avg:34.14ms
step:446/1845 train_time:15230ms step_avg:34.15ms
step:447/1845 train_time:15260ms step_avg:34.14ms
step:448/1845 train_time:15297ms step_avg:34.14ms
step:449/1845 train_time:15326ms step_avg:34.13ms
step:450/1845 train_time:15365ms step_avg:34.14ms
step:451/1845 train_time:15394ms step_avg:34.13ms
step:452/1845 train_time:15432ms step_avg:34.14ms
step:453/1845 train_time:15463ms step_avg:34.14ms
step:454/1845 train_time:15500ms step_avg:34.14ms
step:455/1845 train_time:15532ms step_avg:34.14ms
step:456/1845 train_time:15568ms step_avg:34.14ms
step:457/1845 train_time:15599ms step_avg:34.13ms
step:458/1845 train_time:15636ms step_avg:34.14ms
step:459/1845 train_time:15665ms step_avg:34.13ms
step:460/1845 train_time:15703ms step_avg:34.14ms
step:461/1845 train_time:15735ms step_avg:34.13ms
step:462/1845 train_time:15771ms step_avg:34.14ms
step:463/1845 train_time:15803ms step_avg:34.13ms
step:464/1845 train_time:15840ms step_avg:34.14ms
step:465/1845 train_time:15871ms step_avg:34.13ms
step:466/1845 train_time:15907ms step_avg:34.14ms
step:467/1845 train_time:15938ms step_avg:34.13ms
step:468/1845 train_time:15975ms step_avg:34.13ms
step:469/1845 train_time:16005ms step_avg:34.12ms
step:470/1845 train_time:16043ms step_avg:34.13ms
step:471/1845 train_time:16075ms step_avg:34.13ms
step:472/1845 train_time:16112ms step_avg:34.14ms
step:473/1845 train_time:16144ms step_avg:34.13ms
step:474/1845 train_time:16181ms step_avg:34.14ms
step:475/1845 train_time:16215ms step_avg:34.14ms
step:476/1845 train_time:16252ms step_avg:34.14ms
step:477/1845 train_time:16281ms step_avg:34.13ms
step:478/1845 train_time:16315ms step_avg:34.13ms
step:479/1845 train_time:16347ms step_avg:34.13ms
step:480/1845 train_time:16382ms step_avg:34.13ms
step:481/1845 train_time:16411ms step_avg:34.12ms
step:482/1845 train_time:16449ms step_avg:34.13ms
step:483/1845 train_time:16479ms step_avg:34.12ms
step:484/1845 train_time:16518ms step_avg:34.13ms
step:485/1845 train_time:16548ms step_avg:34.12ms
step:486/1845 train_time:16585ms step_avg:34.13ms
step:487/1845 train_time:16615ms step_avg:34.12ms
step:488/1845 train_time:16653ms step_avg:34.13ms
step:489/1845 train_time:16683ms step_avg:34.12ms
step:490/1845 train_time:16721ms step_avg:34.12ms
step:491/1845 train_time:16752ms step_avg:34.12ms
step:492/1845 train_time:16788ms step_avg:34.12ms
step:493/1845 train_time:16818ms step_avg:34.11ms
step:494/1845 train_time:16857ms step_avg:34.12ms
step:495/1845 train_time:16886ms step_avg:34.11ms
step:496/1845 train_time:16924ms step_avg:34.12ms
step:497/1845 train_time:16954ms step_avg:34.11ms
step:498/1845 train_time:16992ms step_avg:34.12ms
step:499/1845 train_time:17022ms step_avg:34.11ms
step:500/1845 train_time:17060ms step_avg:34.12ms
step:500/1845 val_loss:4.2953 train_time:17098ms step_avg:34.20ms
step:501/1845 train_time:17121ms step_avg:34.17ms
step:502/1845 train_time:17145ms step_avg:34.15ms
step:503/1845 train_time:17166ms step_avg:34.13ms
step:504/1845 train_time:17195ms step_avg:34.12ms
step:505/1845 train_time:17228ms step_avg:34.11ms
step:506/1845 train_time:17264ms step_avg:34.12ms
step:507/1845 train_time:17296ms step_avg:34.11ms
step:508/1845 train_time:17334ms step_avg:34.12ms
step:509/1845 train_time:17365ms step_avg:34.12ms
step:510/1845 train_time:17403ms step_avg:34.12ms
step:511/1845 train_time:17435ms step_avg:34.12ms
step:512/1845 train_time:17470ms step_avg:34.12ms
step:513/1845 train_time:17502ms step_avg:34.12ms
step:514/1845 train_time:17538ms step_avg:34.12ms
step:515/1845 train_time:17574ms step_avg:34.12ms
step:516/1845 train_time:17612ms step_avg:34.13ms
step:517/1845 train_time:17645ms step_avg:34.13ms
step:518/1845 train_time:17681ms step_avg:34.13ms
step:519/1845 train_time:17714ms step_avg:34.13ms
step:520/1845 train_time:17752ms step_avg:34.14ms
step:521/1845 train_time:17785ms step_avg:34.14ms
step:522/1845 train_time:17823ms step_avg:34.14ms
step:523/1845 train_time:17857ms step_avg:34.14ms
step:524/1845 train_time:17895ms step_avg:34.15ms
step:525/1845 train_time:17928ms step_avg:34.15ms
step:526/1845 train_time:17966ms step_avg:34.16ms
step:527/1845 train_time:18000ms step_avg:34.16ms
step:528/1845 train_time:18036ms step_avg:34.16ms
step:529/1845 train_time:18068ms step_avg:34.15ms
step:530/1845 train_time:18103ms step_avg:34.16ms
step:531/1845 train_time:18135ms step_avg:34.15ms
step:532/1845 train_time:18170ms step_avg:34.15ms
step:533/1845 train_time:18202ms step_avg:34.15ms
step:534/1845 train_time:18236ms step_avg:34.15ms
step:535/1845 train_time:18267ms step_avg:34.14ms
step:536/1845 train_time:18301ms step_avg:34.14ms
step:537/1845 train_time:18333ms step_avg:34.14ms
step:538/1845 train_time:18368ms step_avg:34.14ms
step:539/1845 train_time:18400ms step_avg:34.14ms
step:540/1845 train_time:18435ms step_avg:34.14ms
step:541/1845 train_time:18466ms step_avg:34.13ms
step:542/1845 train_time:18500ms step_avg:34.13ms
step:543/1845 train_time:18532ms step_avg:34.13ms
step:544/1845 train_time:18569ms step_avg:34.13ms
step:545/1845 train_time:18602ms step_avg:34.13ms
step:546/1845 train_time:18638ms step_avg:34.14ms
step:547/1845 train_time:18670ms step_avg:34.13ms
step:548/1845 train_time:18705ms step_avg:34.13ms
step:549/1845 train_time:18737ms step_avg:34.13ms
step:550/1845 train_time:18772ms step_avg:34.13ms
step:551/1845 train_time:18804ms step_avg:34.13ms
step:552/1845 train_time:18842ms step_avg:34.13ms
step:553/1845 train_time:18871ms step_avg:34.13ms
step:554/1845 train_time:18897ms step_avg:34.11ms
step:555/1845 train_time:18924ms step_avg:34.10ms
step:556/1845 train_time:18959ms step_avg:34.10ms
step:557/1845 train_time:18992ms step_avg:34.10ms
step:558/1845 train_time:19030ms step_avg:34.10ms
step:559/1845 train_time:19060ms step_avg:34.10ms
step:560/1845 train_time:19098ms step_avg:34.10ms
step:561/1845 train_time:19128ms step_avg:34.10ms
step:562/1845 train_time:19166ms step_avg:34.10ms
step:563/1845 train_time:19196ms step_avg:34.10ms
step:564/1845 train_time:19234ms step_avg:34.10ms
step:565/1845 train_time:19266ms step_avg:34.10ms
step:566/1845 train_time:19303ms step_avg:34.10ms
step:567/1845 train_time:19336ms step_avg:34.10ms
step:568/1845 train_time:19372ms step_avg:34.11ms
step:569/1845 train_time:19405ms step_avg:34.10ms
step:570/1845 train_time:19441ms step_avg:34.11ms
step:571/1845 train_time:19473ms step_avg:34.10ms
step:572/1845 train_time:19509ms step_avg:34.11ms
step:573/1845 train_time:19542ms step_avg:34.11ms
step:574/1845 train_time:19578ms step_avg:34.11ms
step:575/1845 train_time:19610ms step_avg:34.10ms
step:576/1845 train_time:19645ms step_avg:34.11ms
step:577/1845 train_time:19677ms step_avg:34.10ms
step:578/1845 train_time:19711ms step_avg:34.10ms
step:579/1845 train_time:19744ms step_avg:34.10ms
step:580/1845 train_time:19779ms step_avg:34.10ms
step:581/1845 train_time:19810ms step_avg:34.10ms
step:582/1845 train_time:19847ms step_avg:34.10ms
step:583/1845 train_time:19879ms step_avg:34.10ms
step:584/1845 train_time:19913ms step_avg:34.10ms
step:585/1845 train_time:19945ms step_avg:34.09ms
step:586/1845 train_time:19982ms step_avg:34.10ms
step:587/1845 train_time:20014ms step_avg:34.10ms
step:588/1845 train_time:20050ms step_avg:34.10ms
step:589/1845 train_time:20082ms step_avg:34.09ms
step:590/1845 train_time:20116ms step_avg:34.10ms
step:591/1845 train_time:20148ms step_avg:34.09ms
step:592/1845 train_time:20182ms step_avg:34.09ms
step:593/1845 train_time:20214ms step_avg:34.09ms
step:594/1845 train_time:20251ms step_avg:34.09ms
step:595/1845 train_time:20283ms step_avg:34.09ms
step:596/1845 train_time:20319ms step_avg:34.09ms
step:597/1845 train_time:20352ms step_avg:34.09ms
step:598/1845 train_time:20388ms step_avg:34.09ms
step:599/1845 train_time:20420ms step_avg:34.09ms
step:600/1845 train_time:20456ms step_avg:34.09ms
step:601/1845 train_time:20487ms step_avg:34.09ms
step:602/1845 train_time:20523ms step_avg:34.09ms
step:603/1845 train_time:20556ms step_avg:34.09ms
step:604/1845 train_time:20612ms step_avg:34.13ms
step:605/1845 train_time:20671ms step_avg:34.17ms
step:606/1845 train_time:20734ms step_avg:34.21ms
step:607/1845 train_time:20794ms step_avg:34.26ms
step:608/1845 train_time:20857ms step_avg:34.30ms
step:609/1845 train_time:20917ms step_avg:34.35ms
step:610/1845 train_time:20980ms step_avg:34.39ms
step:611/1845 train_time:21040ms step_avg:34.43ms
step:612/1845 train_time:21102ms step_avg:34.48ms
step:613/1845 train_time:21163ms step_avg:34.52ms
step:614/1845 train_time:21226ms step_avg:34.57ms
step:615/1845 train_time:21286ms step_avg:34.61ms
step:616/1845 train_time:21348ms step_avg:34.66ms
step:617/1845 train_time:21408ms step_avg:34.70ms
step:618/1845 train_time:21471ms step_avg:34.74ms
step:619/1845 train_time:21531ms step_avg:34.78ms
step:620/1845 train_time:21594ms step_avg:34.83ms
step:621/1845 train_time:21653ms step_avg:34.87ms
step:622/1845 train_time:21716ms step_avg:34.91ms
step:623/1845 train_time:21776ms step_avg:34.95ms
step:624/1845 train_time:21838ms step_avg:35.00ms
step:625/1845 train_time:21899ms step_avg:35.04ms
step:626/1845 train_time:21962ms step_avg:35.08ms
step:627/1845 train_time:22022ms step_avg:35.12ms
step:628/1845 train_time:22084ms step_avg:35.17ms
step:629/1845 train_time:22144ms step_avg:35.20ms
step:630/1845 train_time:22206ms step_avg:35.25ms
step:631/1845 train_time:22267ms step_avg:35.29ms
step:632/1845 train_time:22329ms step_avg:35.33ms
step:633/1845 train_time:22389ms step_avg:35.37ms
step:634/1845 train_time:22452ms step_avg:35.41ms
step:635/1845 train_time:22511ms step_avg:35.45ms
step:636/1845 train_time:22575ms step_avg:35.49ms
step:637/1845 train_time:22633ms step_avg:35.53ms
step:638/1845 train_time:22697ms step_avg:35.57ms
step:639/1845 train_time:22757ms step_avg:35.61ms
step:640/1845 train_time:22819ms step_avg:35.66ms
step:641/1845 train_time:22879ms step_avg:35.69ms
step:642/1845 train_time:22942ms step_avg:35.74ms
step:643/1845 train_time:23002ms step_avg:35.77ms
step:644/1845 train_time:23065ms step_avg:35.82ms
step:645/1845 train_time:23125ms step_avg:35.85ms
step:646/1845 train_time:23187ms step_avg:35.89ms
step:647/1845 train_time:23247ms step_avg:35.93ms
step:648/1845 train_time:23310ms step_avg:35.97ms
step:649/1845 train_time:23371ms step_avg:36.01ms
step:650/1845 train_time:23433ms step_avg:36.05ms
step:651/1845 train_time:23493ms step_avg:36.09ms
step:652/1845 train_time:23556ms step_avg:36.13ms
step:653/1845 train_time:23615ms step_avg:36.16ms
step:654/1845 train_time:23679ms step_avg:36.21ms
step:655/1845 train_time:23739ms step_avg:36.24ms
step:656/1845 train_time:23801ms step_avg:36.28ms
step:657/1845 train_time:23860ms step_avg:36.32ms
step:658/1845 train_time:23923ms step_avg:36.36ms
step:659/1845 train_time:23983ms step_avg:36.39ms
step:660/1845 train_time:24046ms step_avg:36.43ms
step:661/1845 train_time:24106ms step_avg:36.47ms
step:662/1845 train_time:24168ms step_avg:36.51ms
step:663/1845 train_time:24228ms step_avg:36.54ms
step:664/1845 train_time:24290ms step_avg:36.58ms
step:665/1845 train_time:24351ms step_avg:36.62ms
step:666/1845 train_time:24414ms step_avg:36.66ms
step:667/1845 train_time:24474ms step_avg:36.69ms
step:668/1845 train_time:24537ms step_avg:36.73ms
step:669/1845 train_time:24596ms step_avg:36.76ms
step:670/1845 train_time:24659ms step_avg:36.80ms
step:671/1845 train_time:24719ms step_avg:36.84ms
step:672/1845 train_time:24782ms step_avg:36.88ms
step:673/1845 train_time:24841ms step_avg:36.91ms
step:674/1845 train_time:24903ms step_avg:36.95ms
step:675/1845 train_time:24963ms step_avg:36.98ms
step:676/1845 train_time:25026ms step_avg:37.02ms
step:677/1845 train_time:25085ms step_avg:37.05ms
step:678/1845 train_time:25149ms step_avg:37.09ms
step:679/1845 train_time:25208ms step_avg:37.13ms
step:680/1845 train_time:25271ms step_avg:37.16ms
step:681/1845 train_time:25330ms step_avg:37.20ms
step:682/1845 train_time:25394ms step_avg:37.23ms
step:683/1845 train_time:25454ms step_avg:37.27ms
step:684/1845 train_time:25517ms step_avg:37.31ms
step:685/1845 train_time:25577ms step_avg:37.34ms
step:686/1845 train_time:25639ms step_avg:37.38ms
step:687/1845 train_time:25700ms step_avg:37.41ms
step:688/1845 train_time:25763ms step_avg:37.45ms
step:689/1845 train_time:25822ms step_avg:37.48ms
step:690/1845 train_time:25885ms step_avg:37.51ms
step:691/1845 train_time:25944ms step_avg:37.55ms
step:692/1845 train_time:26008ms step_avg:37.58ms
step:693/1845 train_time:26067ms step_avg:37.61ms
step:694/1845 train_time:26130ms step_avg:37.65ms
step:695/1845 train_time:26190ms step_avg:37.68ms
step:696/1845 train_time:26253ms step_avg:37.72ms
step:697/1845 train_time:26313ms step_avg:37.75ms
step:698/1845 train_time:26375ms step_avg:37.79ms
step:699/1845 train_time:26435ms step_avg:37.82ms
step:700/1845 train_time:26498ms step_avg:37.85ms
step:701/1845 train_time:26558ms step_avg:37.89ms
step:702/1845 train_time:26621ms step_avg:37.92ms
step:703/1845 train_time:26680ms step_avg:37.95ms
step:704/1845 train_time:26743ms step_avg:37.99ms
step:705/1845 train_time:26803ms step_avg:38.02ms
step:706/1845 train_time:26866ms step_avg:38.05ms
step:707/1845 train_time:26926ms step_avg:38.09ms
step:708/1845 train_time:26989ms step_avg:38.12ms
step:709/1845 train_time:27048ms step_avg:38.15ms
step:710/1845 train_time:27111ms step_avg:38.18ms
step:711/1845 train_time:27171ms step_avg:38.22ms
step:712/1845 train_time:27233ms step_avg:38.25ms
step:713/1845 train_time:27294ms step_avg:38.28ms
step:714/1845 train_time:27356ms step_avg:38.31ms
step:715/1845 train_time:27416ms step_avg:38.34ms
step:716/1845 train_time:27478ms step_avg:38.38ms
step:717/1845 train_time:27538ms step_avg:38.41ms
step:718/1845 train_time:27601ms step_avg:38.44ms
step:719/1845 train_time:27661ms step_avg:38.47ms
step:720/1845 train_time:27723ms step_avg:38.50ms
step:721/1845 train_time:27783ms step_avg:38.53ms
step:722/1845 train_time:27846ms step_avg:38.57ms
step:723/1845 train_time:27906ms step_avg:38.60ms
step:724/1845 train_time:27969ms step_avg:38.63ms
step:725/1845 train_time:28028ms step_avg:38.66ms
step:726/1845 train_time:28091ms step_avg:38.69ms
step:727/1845 train_time:28151ms step_avg:38.72ms
step:728/1845 train_time:28214ms step_avg:38.75ms
step:729/1845 train_time:28274ms step_avg:38.78ms
step:730/1845 train_time:28336ms step_avg:38.82ms
step:731/1845 train_time:28397ms step_avg:38.85ms
step:732/1845 train_time:28460ms step_avg:38.88ms
step:733/1845 train_time:28519ms step_avg:38.91ms
step:734/1845 train_time:28582ms step_avg:38.94ms
step:735/1845 train_time:28642ms step_avg:38.97ms
step:736/1845 train_time:28705ms step_avg:39.00ms
step:737/1845 train_time:28765ms step_avg:39.03ms
step:738/1845 train_time:28827ms step_avg:39.06ms
step:739/1845 train_time:28887ms step_avg:39.09ms
step:740/1845 train_time:28950ms step_avg:39.12ms
step:741/1845 train_time:29010ms step_avg:39.15ms
step:742/1845 train_time:29073ms step_avg:39.18ms
step:743/1845 train_time:29133ms step_avg:39.21ms
step:744/1845 train_time:29194ms step_avg:39.24ms
step:745/1845 train_time:29254ms step_avg:39.27ms
step:746/1845 train_time:29317ms step_avg:39.30ms
step:747/1845 train_time:29378ms step_avg:39.33ms
step:748/1845 train_time:29440ms step_avg:39.36ms
step:749/1845 train_time:29501ms step_avg:39.39ms
step:750/1845 train_time:29563ms step_avg:39.42ms
step:750/1845 val_loss:4.0203 train_time:29633ms step_avg:39.51ms
step:751/1845 train_time:29658ms step_avg:39.49ms
step:752/1845 train_time:29687ms step_avg:39.48ms
step:753/1845 train_time:29746ms step_avg:39.50ms
step:754/1845 train_time:29810ms step_avg:39.54ms
step:755/1845 train_time:29870ms step_avg:39.56ms
step:756/1845 train_time:29933ms step_avg:39.59ms
step:757/1845 train_time:29992ms step_avg:39.62ms
step:758/1845 train_time:30054ms step_avg:39.65ms
step:759/1845 train_time:30113ms step_avg:39.68ms
step:760/1845 train_time:30175ms step_avg:39.70ms
step:761/1845 train_time:30235ms step_avg:39.73ms
step:762/1845 train_time:30297ms step_avg:39.76ms
step:763/1845 train_time:30357ms step_avg:39.79ms
step:764/1845 train_time:30420ms step_avg:39.82ms
step:765/1845 train_time:30479ms step_avg:39.84ms
step:766/1845 train_time:30543ms step_avg:39.87ms
step:767/1845 train_time:30605ms step_avg:39.90ms
step:768/1845 train_time:30669ms step_avg:39.93ms
step:769/1845 train_time:30730ms step_avg:39.96ms
step:770/1845 train_time:30794ms step_avg:39.99ms
step:771/1845 train_time:30854ms step_avg:40.02ms
step:772/1845 train_time:30917ms step_avg:40.05ms
step:773/1845 train_time:30977ms step_avg:40.07ms
step:774/1845 train_time:31040ms step_avg:40.10ms
step:775/1845 train_time:31099ms step_avg:40.13ms
step:776/1845 train_time:31161ms step_avg:40.16ms
step:777/1845 train_time:31221ms step_avg:40.18ms
step:778/1845 train_time:31283ms step_avg:40.21ms
step:779/1845 train_time:31343ms step_avg:40.23ms
step:780/1845 train_time:31406ms step_avg:40.26ms
step:781/1845 train_time:31466ms step_avg:40.29ms
step:782/1845 train_time:31530ms step_avg:40.32ms
step:783/1845 train_time:31590ms step_avg:40.35ms
step:784/1845 train_time:31653ms step_avg:40.37ms
step:785/1845 train_time:31713ms step_avg:40.40ms
step:786/1845 train_time:31776ms step_avg:40.43ms
step:787/1845 train_time:31837ms step_avg:40.45ms
step:788/1845 train_time:31900ms step_avg:40.48ms
step:789/1845 train_time:31960ms step_avg:40.51ms
step:790/1845 train_time:32023ms step_avg:40.54ms
step:791/1845 train_time:32083ms step_avg:40.56ms
step:792/1845 train_time:32146ms step_avg:40.59ms
step:793/1845 train_time:32205ms step_avg:40.61ms
step:794/1845 train_time:32268ms step_avg:40.64ms
step:795/1845 train_time:32327ms step_avg:40.66ms
step:796/1845 train_time:32390ms step_avg:40.69ms
step:797/1845 train_time:32450ms step_avg:40.71ms
step:798/1845 train_time:32513ms step_avg:40.74ms
step:799/1845 train_time:32573ms step_avg:40.77ms
step:800/1845 train_time:32635ms step_avg:40.79ms
step:801/1845 train_time:32696ms step_avg:40.82ms
step:802/1845 train_time:32759ms step_avg:40.85ms
step:803/1845 train_time:32819ms step_avg:40.87ms
step:804/1845 train_time:32881ms step_avg:40.90ms
step:805/1845 train_time:32942ms step_avg:40.92ms
step:806/1845 train_time:33004ms step_avg:40.95ms
step:807/1845 train_time:33064ms step_avg:40.97ms
step:808/1845 train_time:33126ms step_avg:41.00ms
step:809/1845 train_time:33187ms step_avg:41.02ms
step:810/1845 train_time:33249ms step_avg:41.05ms
step:811/1845 train_time:33310ms step_avg:41.07ms
step:812/1845 train_time:33372ms step_avg:41.10ms
step:813/1845 train_time:33432ms step_avg:41.12ms
step:814/1845 train_time:33494ms step_avg:41.15ms
step:815/1845 train_time:33554ms step_avg:41.17ms
step:816/1845 train_time:33618ms step_avg:41.20ms
step:817/1845 train_time:33678ms step_avg:41.22ms
step:818/1845 train_time:33741ms step_avg:41.25ms
step:819/1845 train_time:33801ms step_avg:41.27ms
step:820/1845 train_time:33863ms step_avg:41.30ms
step:821/1845 train_time:33924ms step_avg:41.32ms
step:822/1845 train_time:33987ms step_avg:41.35ms
step:823/1845 train_time:34046ms step_avg:41.37ms
step:824/1845 train_time:34108ms step_avg:41.39ms
step:825/1845 train_time:34168ms step_avg:41.42ms
step:826/1845 train_time:34231ms step_avg:41.44ms
step:827/1845 train_time:34291ms step_avg:41.46ms
step:828/1845 train_time:34353ms step_avg:41.49ms
step:829/1845 train_time:34413ms step_avg:41.51ms
step:830/1845 train_time:34475ms step_avg:41.54ms
step:831/1845 train_time:34535ms step_avg:41.56ms
step:832/1845 train_time:34597ms step_avg:41.58ms
step:833/1845 train_time:34657ms step_avg:41.61ms
step:834/1845 train_time:34721ms step_avg:41.63ms
step:835/1845 train_time:34781ms step_avg:41.65ms
step:836/1845 train_time:34843ms step_avg:41.68ms
step:837/1845 train_time:34904ms step_avg:41.70ms
step:838/1845 train_time:34966ms step_avg:41.73ms
step:839/1845 train_time:35027ms step_avg:41.75ms
step:840/1845 train_time:35089ms step_avg:41.77ms
step:841/1845 train_time:35150ms step_avg:41.80ms
step:842/1845 train_time:35214ms step_avg:41.82ms
step:843/1845 train_time:35272ms step_avg:41.84ms
step:844/1845 train_time:35335ms step_avg:41.87ms
step:845/1845 train_time:35395ms step_avg:41.89ms
step:846/1845 train_time:35457ms step_avg:41.91ms
step:847/1845 train_time:35518ms step_avg:41.93ms
step:848/1845 train_time:35580ms step_avg:41.96ms
step:849/1845 train_time:35639ms step_avg:41.98ms
step:850/1845 train_time:35703ms step_avg:42.00ms
step:851/1845 train_time:35763ms step_avg:42.02ms
step:852/1845 train_time:35825ms step_avg:42.05ms
step:853/1845 train_time:35885ms step_avg:42.07ms
step:854/1845 train_time:35948ms step_avg:42.09ms
step:855/1845 train_time:36008ms step_avg:42.11ms
step:856/1845 train_time:36071ms step_avg:42.14ms
step:857/1845 train_time:36131ms step_avg:42.16ms
step:858/1845 train_time:36194ms step_avg:42.18ms
step:859/1845 train_time:36254ms step_avg:42.20ms
step:860/1845 train_time:36316ms step_avg:42.23ms
step:861/1845 train_time:36376ms step_avg:42.25ms
step:862/1845 train_time:36439ms step_avg:42.27ms
step:863/1845 train_time:36499ms step_avg:42.29ms
step:864/1845 train_time:36561ms step_avg:42.32ms
step:865/1845 train_time:36621ms step_avg:42.34ms
step:866/1845 train_time:36684ms step_avg:42.36ms
step:867/1845 train_time:36744ms step_avg:42.38ms
step:868/1845 train_time:36806ms step_avg:42.40ms
step:869/1845 train_time:36866ms step_avg:42.42ms
step:870/1845 train_time:36929ms step_avg:42.45ms
step:871/1845 train_time:36989ms step_avg:42.47ms
step:872/1845 train_time:37053ms step_avg:42.49ms
step:873/1845 train_time:37113ms step_avg:42.51ms
step:874/1845 train_time:37175ms step_avg:42.53ms
step:875/1845 train_time:37235ms step_avg:42.55ms
step:876/1845 train_time:37298ms step_avg:42.58ms
step:877/1845 train_time:37358ms step_avg:42.60ms
step:878/1845 train_time:37421ms step_avg:42.62ms
step:879/1845 train_time:37481ms step_avg:42.64ms
step:880/1845 train_time:37544ms step_avg:42.66ms
step:881/1845 train_time:37604ms step_avg:42.68ms
step:882/1845 train_time:37667ms step_avg:42.71ms
step:883/1845 train_time:37727ms step_avg:42.73ms
step:884/1845 train_time:37790ms step_avg:42.75ms
step:885/1845 train_time:37850ms step_avg:42.77ms
step:886/1845 train_time:37913ms step_avg:42.79ms
step:887/1845 train_time:37972ms step_avg:42.81ms
step:888/1845 train_time:38035ms step_avg:42.83ms
step:889/1845 train_time:38095ms step_avg:42.85ms
step:890/1845 train_time:38157ms step_avg:42.87ms
step:891/1845 train_time:38218ms step_avg:42.89ms
step:892/1845 train_time:38281ms step_avg:42.92ms
step:893/1845 train_time:38341ms step_avg:42.93ms
step:894/1845 train_time:38403ms step_avg:42.96ms
step:895/1845 train_time:38463ms step_avg:42.98ms
step:896/1845 train_time:38525ms step_avg:43.00ms
step:897/1845 train_time:38586ms step_avg:43.02ms
step:898/1845 train_time:38649ms step_avg:43.04ms
step:899/1845 train_time:38709ms step_avg:43.06ms
step:900/1845 train_time:38771ms step_avg:43.08ms
step:901/1845 train_time:38832ms step_avg:43.10ms
step:902/1845 train_time:38894ms step_avg:43.12ms
step:903/1845 train_time:38954ms step_avg:43.14ms
step:904/1845 train_time:39018ms step_avg:43.16ms
step:905/1845 train_time:39077ms step_avg:43.18ms
step:906/1845 train_time:39140ms step_avg:43.20ms
step:907/1845 train_time:39200ms step_avg:43.22ms
step:908/1845 train_time:39263ms step_avg:43.24ms
step:909/1845 train_time:39324ms step_avg:43.26ms
step:910/1845 train_time:39387ms step_avg:43.28ms
step:911/1845 train_time:39446ms step_avg:43.30ms
step:912/1845 train_time:39509ms step_avg:43.32ms
step:913/1845 train_time:39569ms step_avg:43.34ms
step:914/1845 train_time:39632ms step_avg:43.36ms
step:915/1845 train_time:39692ms step_avg:43.38ms
step:916/1845 train_time:39754ms step_avg:43.40ms
step:917/1845 train_time:39814ms step_avg:43.42ms
step:918/1845 train_time:39877ms step_avg:43.44ms
step:919/1845 train_time:39937ms step_avg:43.46ms
step:920/1845 train_time:40000ms step_avg:43.48ms
step:921/1845 train_time:40059ms step_avg:43.50ms
step:922/1845 train_time:40122ms step_avg:43.52ms
step:923/1845 train_time:40181ms step_avg:43.53ms
step:924/1845 train_time:40244ms step_avg:43.55ms
step:925/1845 train_time:40305ms step_avg:43.57ms
step:926/1845 train_time:40367ms step_avg:43.59ms
step:927/1845 train_time:40428ms step_avg:43.61ms
step:928/1845 train_time:40490ms step_avg:43.63ms
step:929/1845 train_time:40550ms step_avg:43.65ms
step:930/1845 train_time:40613ms step_avg:43.67ms
step:931/1845 train_time:40672ms step_avg:43.69ms
step:932/1845 train_time:40735ms step_avg:43.71ms
step:933/1845 train_time:40795ms step_avg:43.72ms
step:934/1845 train_time:40858ms step_avg:43.74ms
step:935/1845 train_time:40918ms step_avg:43.76ms
step:936/1845 train_time:40981ms step_avg:43.78ms
step:937/1845 train_time:41041ms step_avg:43.80ms
step:938/1845 train_time:41104ms step_avg:43.82ms
step:939/1845 train_time:41163ms step_avg:43.84ms
step:940/1845 train_time:41226ms step_avg:43.86ms
step:941/1845 train_time:41286ms step_avg:43.87ms
step:942/1845 train_time:41349ms step_avg:43.90ms
step:943/1845 train_time:41410ms step_avg:43.91ms
step:944/1845 train_time:41473ms step_avg:43.93ms
step:945/1845 train_time:41533ms step_avg:43.95ms
step:946/1845 train_time:41595ms step_avg:43.97ms
step:947/1845 train_time:41656ms step_avg:43.99ms
step:948/1845 train_time:41718ms step_avg:44.01ms
step:949/1845 train_time:41778ms step_avg:44.02ms
step:950/1845 train_time:41841ms step_avg:44.04ms
step:951/1845 train_time:41901ms step_avg:44.06ms
step:952/1845 train_time:41962ms step_avg:44.08ms
step:953/1845 train_time:42022ms step_avg:44.09ms
step:954/1845 train_time:42085ms step_avg:44.11ms
step:955/1845 train_time:42145ms step_avg:44.13ms
step:956/1845 train_time:42208ms step_avg:44.15ms
step:957/1845 train_time:42268ms step_avg:44.17ms
step:958/1845 train_time:42331ms step_avg:44.19ms
step:959/1845 train_time:42391ms step_avg:44.20ms
step:960/1845 train_time:42453ms step_avg:44.22ms
step:961/1845 train_time:42513ms step_avg:44.24ms
step:962/1845 train_time:42576ms step_avg:44.26ms
step:963/1845 train_time:42636ms step_avg:44.27ms
step:964/1845 train_time:42699ms step_avg:44.29ms
step:965/1845 train_time:42759ms step_avg:44.31ms
step:966/1845 train_time:42821ms step_avg:44.33ms
step:967/1845 train_time:42882ms step_avg:44.35ms
step:968/1845 train_time:42945ms step_avg:44.36ms
step:969/1845 train_time:43005ms step_avg:44.38ms
step:970/1845 train_time:43067ms step_avg:44.40ms
step:971/1845 train_time:43127ms step_avg:44.41ms
step:972/1845 train_time:43189ms step_avg:44.43ms
step:973/1845 train_time:43249ms step_avg:44.45ms
step:974/1845 train_time:43312ms step_avg:44.47ms
step:975/1845 train_time:43372ms step_avg:44.48ms
step:976/1845 train_time:43434ms step_avg:44.50ms
step:977/1845 train_time:43493ms step_avg:44.52ms
step:978/1845 train_time:43556ms step_avg:44.54ms
step:979/1845 train_time:43616ms step_avg:44.55ms
step:980/1845 train_time:43678ms step_avg:44.57ms
step:981/1845 train_time:43738ms step_avg:44.59ms
step:982/1845 train_time:43801ms step_avg:44.60ms
step:983/1845 train_time:43861ms step_avg:44.62ms
step:984/1845 train_time:43924ms step_avg:44.64ms
step:985/1845 train_time:43984ms step_avg:44.65ms
step:986/1845 train_time:44045ms step_avg:44.67ms
step:987/1845 train_time:44106ms step_avg:44.69ms
step:988/1845 train_time:44168ms step_avg:44.70ms
step:989/1845 train_time:44228ms step_avg:44.72ms
step:990/1845 train_time:44290ms step_avg:44.74ms
step:991/1845 train_time:44350ms step_avg:44.75ms
step:992/1845 train_time:44413ms step_avg:44.77ms
step:993/1845 train_time:44472ms step_avg:44.79ms
step:994/1845 train_time:44535ms step_avg:44.80ms
step:995/1845 train_time:44595ms step_avg:44.82ms
step:996/1845 train_time:44658ms step_avg:44.84ms
step:997/1845 train_time:44718ms step_avg:44.85ms
step:998/1845 train_time:44781ms step_avg:44.87ms
step:999/1845 train_time:44841ms step_avg:44.89ms
step:1000/1845 train_time:44904ms step_avg:44.90ms
step:1000/1845 val_loss:3.7809 train_time:44973ms step_avg:44.97ms
step:1001/1845 train_time:44999ms step_avg:44.95ms
step:1002/1845 train_time:45026ms step_avg:44.94ms
step:1003/1845 train_time:45086ms step_avg:44.95ms
step:1004/1845 train_time:45149ms step_avg:44.97ms
step:1005/1845 train_time:45210ms step_avg:44.98ms
step:1006/1845 train_time:45272ms step_avg:45.00ms
step:1007/1845 train_time:45332ms step_avg:45.02ms
step:1008/1845 train_time:45394ms step_avg:45.03ms
step:1009/1845 train_time:45454ms step_avg:45.05ms
step:1010/1845 train_time:45517ms step_avg:45.07ms
step:1011/1845 train_time:45577ms step_avg:45.08ms
step:1012/1845 train_time:45640ms step_avg:45.10ms
step:1013/1845 train_time:45699ms step_avg:45.11ms
step:1014/1845 train_time:45761ms step_avg:45.13ms
step:1015/1845 train_time:45820ms step_avg:45.14ms
step:1016/1845 train_time:45883ms step_avg:45.16ms
step:1017/1845 train_time:45944ms step_avg:45.18ms
step:1018/1845 train_time:46008ms step_avg:45.19ms
step:1019/1845 train_time:46068ms step_avg:45.21ms
step:1020/1845 train_time:46132ms step_avg:45.23ms
step:1021/1845 train_time:46192ms step_avg:45.24ms
step:1022/1845 train_time:46255ms step_avg:45.26ms
step:1023/1845 train_time:46315ms step_avg:45.27ms
step:1024/1845 train_time:46377ms step_avg:45.29ms
step:1025/1845 train_time:46437ms step_avg:45.30ms
step:1026/1845 train_time:46499ms step_avg:45.32ms
step:1027/1845 train_time:46559ms step_avg:45.33ms
step:1028/1845 train_time:46620ms step_avg:45.35ms
step:1029/1845 train_time:46680ms step_avg:45.36ms
step:1030/1845 train_time:46742ms step_avg:45.38ms
step:1031/1845 train_time:46802ms step_avg:45.39ms
step:1032/1845 train_time:46865ms step_avg:45.41ms
step:1033/1845 train_time:46925ms step_avg:45.43ms
step:1034/1845 train_time:46989ms step_avg:45.44ms
step:1035/1845 train_time:47049ms step_avg:45.46ms
step:1036/1845 train_time:47112ms step_avg:45.47ms
step:1037/1845 train_time:47172ms step_avg:45.49ms
step:1038/1845 train_time:47234ms step_avg:45.50ms
step:1039/1845 train_time:47294ms step_avg:45.52ms
step:1040/1845 train_time:47357ms step_avg:45.54ms
step:1041/1845 train_time:47417ms step_avg:45.55ms
step:1042/1845 train_time:47479ms step_avg:45.57ms
step:1043/1845 train_time:47539ms step_avg:45.58ms
step:1044/1845 train_time:47601ms step_avg:45.60ms
step:1045/1845 train_time:47661ms step_avg:45.61ms
step:1046/1845 train_time:47723ms step_avg:45.62ms
step:1047/1845 train_time:47783ms step_avg:45.64ms
step:1048/1845 train_time:47846ms step_avg:45.65ms
step:1049/1845 train_time:47906ms step_avg:45.67ms
step:1050/1845 train_time:47968ms step_avg:45.68ms
step:1051/1845 train_time:48028ms step_avg:45.70ms
step:1052/1845 train_time:48091ms step_avg:45.71ms
step:1053/1845 train_time:48151ms step_avg:45.73ms
step:1054/1845 train_time:48214ms step_avg:45.74ms
step:1055/1845 train_time:48274ms step_avg:45.76ms
step:1056/1845 train_time:48337ms step_avg:45.77ms
step:1057/1845 train_time:48397ms step_avg:45.79ms
step:1058/1845 train_time:48460ms step_avg:45.80ms
step:1059/1845 train_time:48519ms step_avg:45.82ms
step:1060/1845 train_time:48582ms step_avg:45.83ms
step:1061/1845 train_time:48642ms step_avg:45.85ms
step:1062/1845 train_time:48704ms step_avg:45.86ms
step:1063/1845 train_time:48764ms step_avg:45.87ms
step:1064/1845 train_time:48826ms step_avg:45.89ms
step:1065/1845 train_time:48886ms step_avg:45.90ms
step:1066/1845 train_time:48948ms step_avg:45.92ms
step:1067/1845 train_time:49008ms step_avg:45.93ms
step:1068/1845 train_time:49071ms step_avg:45.95ms
step:1069/1845 train_time:49132ms step_avg:45.96ms
step:1070/1845 train_time:49194ms step_avg:45.98ms
step:1071/1845 train_time:49254ms step_avg:45.99ms
step:1072/1845 train_time:49317ms step_avg:46.00ms
step:1073/1845 train_time:49377ms step_avg:46.02ms
step:1074/1845 train_time:49439ms step_avg:46.03ms
step:1075/1845 train_time:49499ms step_avg:46.05ms
step:1076/1845 train_time:49562ms step_avg:46.06ms
step:1077/1845 train_time:49622ms step_avg:46.07ms
step:1078/1845 train_time:49685ms step_avg:46.09ms
step:1079/1845 train_time:49745ms step_avg:46.10ms
step:1080/1845 train_time:49807ms step_avg:46.12ms
step:1081/1845 train_time:49867ms step_avg:46.13ms
step:1082/1845 train_time:49930ms step_avg:46.15ms
step:1083/1845 train_time:49990ms step_avg:46.16ms
step:1084/1845 train_time:50052ms step_avg:46.17ms
step:1085/1845 train_time:50113ms step_avg:46.19ms
step:1086/1845 train_time:50175ms step_avg:46.20ms
step:1087/1845 train_time:50236ms step_avg:46.21ms
step:1088/1845 train_time:50299ms step_avg:46.23ms
step:1089/1845 train_time:50359ms step_avg:46.24ms
step:1090/1845 train_time:50421ms step_avg:46.26ms
step:1091/1845 train_time:50481ms step_avg:46.27ms
step:1092/1845 train_time:50544ms step_avg:46.29ms
step:1093/1845 train_time:50604ms step_avg:46.30ms
step:1094/1845 train_time:50666ms step_avg:46.31ms
step:1095/1845 train_time:50725ms step_avg:46.32ms
step:1096/1845 train_time:50788ms step_avg:46.34ms
step:1097/1845 train_time:50848ms step_avg:46.35ms
step:1098/1845 train_time:50910ms step_avg:46.37ms
step:1099/1845 train_time:50970ms step_avg:46.38ms
step:1100/1845 train_time:51032ms step_avg:46.39ms
step:1101/1845 train_time:51093ms step_avg:46.41ms
step:1102/1845 train_time:51155ms step_avg:46.42ms
step:1103/1845 train_time:51215ms step_avg:46.43ms
step:1104/1845 train_time:51277ms step_avg:46.45ms
step:1105/1845 train_time:51338ms step_avg:46.46ms
step:1106/1845 train_time:51401ms step_avg:46.47ms
step:1107/1845 train_time:51461ms step_avg:46.49ms
step:1108/1845 train_time:51523ms step_avg:46.50ms
step:1109/1845 train_time:51584ms step_avg:46.51ms
step:1110/1845 train_time:51646ms step_avg:46.53ms
step:1111/1845 train_time:51706ms step_avg:46.54ms
step:1112/1845 train_time:51768ms step_avg:46.55ms
step:1113/1845 train_time:51828ms step_avg:46.57ms
step:1114/1845 train_time:51890ms step_avg:46.58ms
step:1115/1845 train_time:51950ms step_avg:46.59ms
step:1116/1845 train_time:52013ms step_avg:46.61ms
step:1117/1845 train_time:52072ms step_avg:46.62ms
step:1118/1845 train_time:52134ms step_avg:46.63ms
step:1119/1845 train_time:52194ms step_avg:46.64ms
step:1120/1845 train_time:52257ms step_avg:46.66ms
step:1121/1845 train_time:52317ms step_avg:46.67ms
step:1122/1845 train_time:52380ms step_avg:46.68ms
step:1123/1845 train_time:52440ms step_avg:46.70ms
step:1124/1845 train_time:52502ms step_avg:46.71ms
step:1125/1845 train_time:52562ms step_avg:46.72ms
step:1126/1845 train_time:52625ms step_avg:46.74ms
step:1127/1845 train_time:52685ms step_avg:46.75ms
step:1128/1845 train_time:52747ms step_avg:46.76ms
step:1129/1845 train_time:52807ms step_avg:46.77ms
step:1130/1845 train_time:52869ms step_avg:46.79ms
step:1131/1845 train_time:52929ms step_avg:46.80ms
step:1132/1845 train_time:52992ms step_avg:46.81ms
step:1133/1845 train_time:53051ms step_avg:46.82ms
step:1134/1845 train_time:53114ms step_avg:46.84ms
step:1135/1845 train_time:53174ms step_avg:46.85ms
step:1136/1845 train_time:53238ms step_avg:46.86ms
step:1137/1845 train_time:53298ms step_avg:46.88ms
step:1138/1845 train_time:53360ms step_avg:46.89ms
step:1139/1845 train_time:53419ms step_avg:46.90ms
step:1140/1845 train_time:53482ms step_avg:46.91ms
step:1141/1845 train_time:53543ms step_avg:46.93ms
step:1142/1845 train_time:53605ms step_avg:46.94ms
step:1143/1845 train_time:53666ms step_avg:46.95ms
step:1144/1845 train_time:53728ms step_avg:46.96ms
step:1145/1845 train_time:53788ms step_avg:46.98ms
step:1146/1845 train_time:53850ms step_avg:46.99ms
step:1147/1845 train_time:53910ms step_avg:47.00ms
step:1148/1845 train_time:53973ms step_avg:47.01ms
step:1149/1845 train_time:54033ms step_avg:47.03ms
step:1150/1845 train_time:54095ms step_avg:47.04ms
step:1151/1845 train_time:54155ms step_avg:47.05ms
step:1152/1845 train_time:54217ms step_avg:47.06ms
step:1153/1845 train_time:54278ms step_avg:47.08ms
step:1154/1845 train_time:54340ms step_avg:47.09ms
step:1155/1845 train_time:54400ms step_avg:47.10ms
step:1156/1845 train_time:54463ms step_avg:47.11ms
step:1157/1845 train_time:54523ms step_avg:47.12ms
step:1158/1845 train_time:54586ms step_avg:47.14ms
step:1159/1845 train_time:54646ms step_avg:47.15ms
step:1160/1845 train_time:54708ms step_avg:47.16ms
step:1161/1845 train_time:54768ms step_avg:47.17ms
step:1162/1845 train_time:54830ms step_avg:47.19ms
step:1163/1845 train_time:54890ms step_avg:47.20ms
step:1164/1845 train_time:54952ms step_avg:47.21ms
step:1165/1845 train_time:55012ms step_avg:47.22ms
step:1166/1845 train_time:55074ms step_avg:47.23ms
step:1167/1845 train_time:55134ms step_avg:47.24ms
step:1168/1845 train_time:55197ms step_avg:47.26ms
step:1169/1845 train_time:55258ms step_avg:47.27ms
step:1170/1845 train_time:55319ms step_avg:47.28ms
step:1171/1845 train_time:55379ms step_avg:47.29ms
step:1172/1845 train_time:55442ms step_avg:47.31ms
step:1173/1845 train_time:55502ms step_avg:47.32ms
step:1174/1845 train_time:55565ms step_avg:47.33ms
step:1175/1845 train_time:55624ms step_avg:47.34ms
step:1176/1845 train_time:55687ms step_avg:47.35ms
step:1177/1845 train_time:55747ms step_avg:47.36ms
step:1178/1845 train_time:55810ms step_avg:47.38ms
step:1179/1845 train_time:55870ms step_avg:47.39ms
step:1180/1845 train_time:55932ms step_avg:47.40ms
step:1181/1845 train_time:55992ms step_avg:47.41ms
step:1182/1845 train_time:56054ms step_avg:47.42ms
step:1183/1845 train_time:56115ms step_avg:47.43ms
step:1184/1845 train_time:56177ms step_avg:47.45ms
step:1185/1845 train_time:56237ms step_avg:47.46ms
step:1186/1845 train_time:56299ms step_avg:47.47ms
step:1187/1845 train_time:56359ms step_avg:47.48ms
step:1188/1845 train_time:56422ms step_avg:47.49ms
step:1189/1845 train_time:56482ms step_avg:47.50ms
step:1190/1845 train_time:56544ms step_avg:47.52ms
step:1191/1845 train_time:56605ms step_avg:47.53ms
step:1192/1845 train_time:56668ms step_avg:47.54ms
step:1193/1845 train_time:56727ms step_avg:47.55ms
step:1194/1845 train_time:56790ms step_avg:47.56ms
step:1195/1845 train_time:56849ms step_avg:47.57ms
step:1196/1845 train_time:56912ms step_avg:47.59ms
step:1197/1845 train_time:56972ms step_avg:47.60ms
step:1198/1845 train_time:57035ms step_avg:47.61ms
step:1199/1845 train_time:57095ms step_avg:47.62ms
step:1200/1845 train_time:57158ms step_avg:47.63ms
step:1201/1845 train_time:57217ms step_avg:47.64ms
step:1202/1845 train_time:57280ms step_avg:47.65ms
step:1203/1845 train_time:57341ms step_avg:47.66ms
step:1204/1845 train_time:57403ms step_avg:47.68ms
step:1205/1845 train_time:57465ms step_avg:47.69ms
step:1206/1845 train_time:57551ms step_avg:47.72ms
step:1207/1845 train_time:57639ms step_avg:47.75ms
step:1208/1845 train_time:57726ms step_avg:47.79ms
step:1209/1845 train_time:57814ms step_avg:47.82ms
step:1210/1845 train_time:57904ms step_avg:47.85ms
step:1211/1845 train_time:57992ms step_avg:47.89ms
step:1212/1845 train_time:58082ms step_avg:47.92ms
step:1213/1845 train_time:58168ms step_avg:47.95ms
step:1214/1845 train_time:58257ms step_avg:47.99ms
step:1215/1845 train_time:58344ms step_avg:48.02ms
step:1216/1845 train_time:58433ms step_avg:48.05ms
step:1217/1845 train_time:58519ms step_avg:48.08ms
step:1218/1845 train_time:58607ms step_avg:48.12ms
step:1219/1845 train_time:58693ms step_avg:48.15ms
step:1220/1845 train_time:58783ms step_avg:48.18ms
step:1221/1845 train_time:58868ms step_avg:48.21ms
step:1222/1845 train_time:58958ms step_avg:48.25ms
step:1223/1845 train_time:59045ms step_avg:48.28ms
step:1224/1845 train_time:59134ms step_avg:48.31ms
step:1225/1845 train_time:59220ms step_avg:48.34ms
step:1226/1845 train_time:59309ms step_avg:48.38ms
step:1227/1845 train_time:59395ms step_avg:48.41ms
step:1228/1845 train_time:59485ms step_avg:48.44ms
step:1229/1845 train_time:59572ms step_avg:48.47ms
step:1230/1845 train_time:59660ms step_avg:48.50ms
step:1231/1845 train_time:59746ms step_avg:48.53ms
step:1232/1845 train_time:59835ms step_avg:48.57ms
step:1233/1845 train_time:59923ms step_avg:48.60ms
step:1234/1845 train_time:60014ms step_avg:48.63ms
step:1235/1845 train_time:60100ms step_avg:48.66ms
step:1236/1845 train_time:60188ms step_avg:48.70ms
step:1237/1845 train_time:60275ms step_avg:48.73ms
step:1238/1845 train_time:60364ms step_avg:48.76ms
step:1239/1845 train_time:60450ms step_avg:48.79ms
step:1240/1845 train_time:60540ms step_avg:48.82ms
step:1241/1845 train_time:60627ms step_avg:48.85ms
step:1242/1845 train_time:60715ms step_avg:48.88ms
step:1243/1845 train_time:60801ms step_avg:48.91ms
step:1244/1845 train_time:60889ms step_avg:48.95ms
step:1245/1845 train_time:60976ms step_avg:48.98ms
step:1246/1845 train_time:61065ms step_avg:49.01ms
step:1247/1845 train_time:61151ms step_avg:49.04ms
step:1248/1845 train_time:61240ms step_avg:49.07ms
step:1249/1845 train_time:61327ms step_avg:49.10ms
step:1250/1845 train_time:61415ms step_avg:49.13ms
step:1250/1845 val_loss:3.5344 train_time:61512ms step_avg:49.21ms
step:1251/1845 train_time:61538ms step_avg:49.19ms
step:1252/1845 train_time:61590ms step_avg:49.19ms
step:1253/1845 train_time:61680ms step_avg:49.23ms
step:1254/1845 train_time:61769ms step_avg:49.26ms
step:1255/1845 train_time:61855ms step_avg:49.29ms
step:1256/1845 train_time:61942ms step_avg:49.32ms
step:1257/1845 train_time:62028ms step_avg:49.35ms
step:1258/1845 train_time:62116ms step_avg:49.38ms
step:1259/1845 train_time:62201ms step_avg:49.40ms
step:1260/1845 train_time:62289ms step_avg:49.44ms
step:1261/1845 train_time:62375ms step_avg:49.46ms
step:1262/1845 train_time:62468ms step_avg:49.50ms
step:1263/1845 train_time:62557ms step_avg:49.53ms
step:1264/1845 train_time:62647ms step_avg:49.56ms
step:1265/1845 train_time:62733ms step_avg:49.59ms
step:1266/1845 train_time:62821ms step_avg:49.62ms
step:1267/1845 train_time:62908ms step_avg:49.65ms
step:1268/1845 train_time:62996ms step_avg:49.68ms
step:1269/1845 train_time:63081ms step_avg:49.71ms
step:1270/1845 train_time:63169ms step_avg:49.74ms
step:1271/1845 train_time:63255ms step_avg:49.77ms
step:1272/1845 train_time:63343ms step_avg:49.80ms
step:1273/1845 train_time:63430ms step_avg:49.83ms
step:1274/1845 train_time:63520ms step_avg:49.86ms
step:1275/1845 train_time:63607ms step_avg:49.89ms
step:1276/1845 train_time:63698ms step_avg:49.92ms
step:1277/1845 train_time:63785ms step_avg:49.95ms
step:1278/1845 train_time:63874ms step_avg:49.98ms
step:1279/1845 train_time:63959ms step_avg:50.01ms
step:1280/1845 train_time:64047ms step_avg:50.04ms
step:1281/1845 train_time:64132ms step_avg:50.06ms
step:1282/1845 train_time:64221ms step_avg:50.09ms
step:1283/1845 train_time:64307ms step_avg:50.12ms
step:1284/1845 train_time:64395ms step_avg:50.15ms
step:1285/1845 train_time:64481ms step_avg:50.18ms
step:1286/1845 train_time:64570ms step_avg:50.21ms
step:1287/1845 train_time:64657ms step_avg:50.24ms
step:1288/1845 train_time:64748ms step_avg:50.27ms
step:1289/1845 train_time:64834ms step_avg:50.30ms
step:1290/1845 train_time:64925ms step_avg:50.33ms
step:1291/1845 train_time:65010ms step_avg:50.36ms
step:1292/1845 train_time:65099ms step_avg:50.39ms
step:1293/1845 train_time:65185ms step_avg:50.41ms
step:1294/1845 train_time:65271ms step_avg:50.44ms
step:1295/1845 train_time:65360ms step_avg:50.47ms
step:1296/1845 train_time:65448ms step_avg:50.50ms
step:1297/1845 train_time:65535ms step_avg:50.53ms
step:1298/1845 train_time:65624ms step_avg:50.56ms
step:1299/1845 train_time:65710ms step_avg:50.59ms
step:1300/1845 train_time:65799ms step_avg:50.61ms
step:1301/1845 train_time:65885ms step_avg:50.64ms
step:1302/1845 train_time:65973ms step_avg:50.67ms
step:1303/1845 train_time:66060ms step_avg:50.70ms
step:1304/1845 train_time:66149ms step_avg:50.73ms
step:1305/1845 train_time:66234ms step_avg:50.75ms
step:1306/1845 train_time:66324ms step_avg:50.78ms
step:1307/1845 train_time:66409ms step_avg:50.81ms
step:1308/1845 train_time:66498ms step_avg:50.84ms
step:1309/1845 train_time:66584ms step_avg:50.87ms
step:1310/1845 train_time:66672ms step_avg:50.89ms
step:1311/1845 train_time:66760ms step_avg:50.92ms
step:1312/1845 train_time:66849ms step_avg:50.95ms
step:1313/1845 train_time:66935ms step_avg:50.98ms
step:1314/1845 train_time:67024ms step_avg:51.01ms
step:1315/1845 train_time:67109ms step_avg:51.03ms
step:1316/1845 train_time:67198ms step_avg:51.06ms
step:1317/1845 train_time:67284ms step_avg:51.09ms
step:1318/1845 train_time:67371ms step_avg:51.12ms
step:1319/1845 train_time:67458ms step_avg:51.14ms
step:1320/1845 train_time:67548ms step_avg:51.17ms
step:1321/1845 train_time:67634ms step_avg:51.20ms
step:1322/1845 train_time:67724ms step_avg:51.23ms
step:1323/1845 train_time:67809ms step_avg:51.25ms
step:1324/1845 train_time:67899ms step_avg:51.28ms
step:1325/1845 train_time:67986ms step_avg:51.31ms
step:1326/1845 train_time:68074ms step_avg:51.34ms
step:1327/1845 train_time:68161ms step_avg:51.36ms
step:1328/1845 train_time:68249ms step_avg:51.39ms
step:1329/1845 train_time:68335ms step_avg:51.42ms
step:1330/1845 train_time:68424ms step_avg:51.45ms
step:1331/1845 train_time:68509ms step_avg:51.47ms
step:1332/1845 train_time:68600ms step_avg:51.50ms
step:1333/1845 train_time:68687ms step_avg:51.53ms
step:1334/1845 train_time:68776ms step_avg:51.56ms
step:1335/1845 train_time:68862ms step_avg:51.58ms
step:1336/1845 train_time:68950ms step_avg:51.61ms
step:1337/1845 train_time:69036ms step_avg:51.63ms
step:1338/1845 train_time:69125ms step_avg:51.66ms
step:1339/1845 train_time:69210ms step_avg:51.69ms
step:1340/1845 train_time:69300ms step_avg:51.72ms
step:1341/1845 train_time:69387ms step_avg:51.74ms
step:1342/1845 train_time:69475ms step_avg:51.77ms
step:1343/1845 train_time:69561ms step_avg:51.80ms
step:1344/1845 train_time:69649ms step_avg:51.82ms
step:1345/1845 train_time:69736ms step_avg:51.85ms
step:1346/1845 train_time:69828ms step_avg:51.88ms
step:1347/1845 train_time:69913ms step_avg:51.90ms
step:1348/1845 train_time:70003ms step_avg:51.93ms
step:1349/1845 train_time:70089ms step_avg:51.96ms
step:1350/1845 train_time:70176ms step_avg:51.98ms
step:1351/1845 train_time:70263ms step_avg:52.01ms
step:1352/1845 train_time:70351ms step_avg:52.04ms
step:1353/1845 train_time:70438ms step_avg:52.06ms
step:1354/1845 train_time:70528ms step_avg:52.09ms
step:1355/1845 train_time:70613ms step_avg:52.11ms
step:1356/1845 train_time:70703ms step_avg:52.14ms
step:1357/1845 train_time:70789ms step_avg:52.17ms
step:1358/1845 train_time:70878ms step_avg:52.19ms
step:1359/1845 train_time:70965ms step_avg:52.22ms
step:1360/1845 train_time:71054ms step_avg:52.25ms
step:1361/1845 train_time:71140ms step_avg:52.27ms
step:1362/1845 train_time:71228ms step_avg:52.30ms
step:1363/1845 train_time:71313ms step_avg:52.32ms
step:1364/1845 train_time:71403ms step_avg:52.35ms
step:1365/1845 train_time:71489ms step_avg:52.37ms
step:1366/1845 train_time:71578ms step_avg:52.40ms
step:1367/1845 train_time:71666ms step_avg:52.43ms
step:1368/1845 train_time:71754ms step_avg:52.45ms
step:1369/1845 train_time:71840ms step_avg:52.48ms
step:1370/1845 train_time:71929ms step_avg:52.50ms
step:1371/1845 train_time:72015ms step_avg:52.53ms
step:1372/1845 train_time:72104ms step_avg:52.55ms
step:1373/1845 train_time:72190ms step_avg:52.58ms
step:1374/1845 train_time:72279ms step_avg:52.60ms
step:1375/1845 train_time:72365ms step_avg:52.63ms
step:1376/1845 train_time:72453ms step_avg:52.65ms
step:1377/1845 train_time:72539ms step_avg:52.68ms
step:1378/1845 train_time:72629ms step_avg:52.71ms
step:1379/1845 train_time:72715ms step_avg:52.73ms
step:1380/1845 train_time:72805ms step_avg:52.76ms
step:1381/1845 train_time:72891ms step_avg:52.78ms
step:1382/1845 train_time:72980ms step_avg:52.81ms
step:1383/1845 train_time:73067ms step_avg:52.83ms
step:1384/1845 train_time:73155ms step_avg:52.86ms
step:1385/1845 train_time:73242ms step_avg:52.88ms
step:1386/1845 train_time:73330ms step_avg:52.91ms
step:1387/1845 train_time:73417ms step_avg:52.93ms
step:1388/1845 train_time:73505ms step_avg:52.96ms
step:1389/1845 train_time:73591ms step_avg:52.98ms
step:1390/1845 train_time:73683ms step_avg:53.01ms
step:1391/1845 train_time:73769ms step_avg:53.03ms
step:1392/1845 train_time:73857ms step_avg:53.06ms
step:1393/1845 train_time:73942ms step_avg:53.08ms
step:1394/1845 train_time:74031ms step_avg:53.11ms
step:1395/1845 train_time:74118ms step_avg:53.13ms
step:1396/1845 train_time:74206ms step_avg:53.16ms
step:1397/1845 train_time:74292ms step_avg:53.18ms
step:1398/1845 train_time:74380ms step_avg:53.20ms
step:1399/1845 train_time:74466ms step_avg:53.23ms
step:1400/1845 train_time:74554ms step_avg:53.25ms
step:1401/1845 train_time:74641ms step_avg:53.28ms
step:1402/1845 train_time:74730ms step_avg:53.30ms
step:1403/1845 train_time:74817ms step_avg:53.33ms
step:1404/1845 train_time:74906ms step_avg:53.35ms
step:1405/1845 train_time:74992ms step_avg:53.37ms
step:1406/1845 train_time:75081ms step_avg:53.40ms
step:1407/1845 train_time:75169ms step_avg:53.43ms
step:1408/1845 train_time:75258ms step_avg:53.45ms
step:1409/1845 train_time:75344ms step_avg:53.47ms
step:1410/1845 train_time:75432ms step_avg:53.50ms
step:1411/1845 train_time:75519ms step_avg:53.52ms
step:1412/1845 train_time:75609ms step_avg:53.55ms
step:1413/1845 train_time:75694ms step_avg:53.57ms
step:1414/1845 train_time:75783ms step_avg:53.59ms
step:1415/1845 train_time:75869ms step_avg:53.62ms
step:1416/1845 train_time:75957ms step_avg:53.64ms
step:1417/1845 train_time:76044ms step_avg:53.67ms
step:1418/1845 train_time:76132ms step_avg:53.69ms
step:1419/1845 train_time:76218ms step_avg:53.71ms
step:1420/1845 train_time:76307ms step_avg:53.74ms
step:1421/1845 train_time:76393ms step_avg:53.76ms
step:1422/1845 train_time:76483ms step_avg:53.79ms
step:1423/1845 train_time:76568ms step_avg:53.81ms
step:1424/1845 train_time:76657ms step_avg:53.83ms
step:1425/1845 train_time:76743ms step_avg:53.85ms
step:1426/1845 train_time:76831ms step_avg:53.88ms
step:1427/1845 train_time:76917ms step_avg:53.90ms
step:1428/1845 train_time:77007ms step_avg:53.93ms
step:1429/1845 train_time:77092ms step_avg:53.95ms
step:1430/1845 train_time:77182ms step_avg:53.97ms
step:1431/1845 train_time:77269ms step_avg:54.00ms
step:1432/1845 train_time:77358ms step_avg:54.02ms
step:1433/1845 train_time:77445ms step_avg:54.04ms
step:1434/1845 train_time:77534ms step_avg:54.07ms
step:1435/1845 train_time:77620ms step_avg:54.09ms
step:1436/1845 train_time:77709ms step_avg:54.11ms
step:1437/1845 train_time:77795ms step_avg:54.14ms
step:1438/1845 train_time:77883ms step_avg:54.16ms
step:1439/1845 train_time:77969ms step_avg:54.18ms
step:1440/1845 train_time:78060ms step_avg:54.21ms
step:1441/1845 train_time:78146ms step_avg:54.23ms
step:1442/1845 train_time:78235ms step_avg:54.25ms
step:1443/1845 train_time:78322ms step_avg:54.28ms
step:1444/1845 train_time:78410ms step_avg:54.30ms
step:1445/1845 train_time:78497ms step_avg:54.32ms
step:1446/1845 train_time:78586ms step_avg:54.35ms
step:1447/1845 train_time:78672ms step_avg:54.37ms
step:1448/1845 train_time:78761ms step_avg:54.39ms
step:1449/1845 train_time:78847ms step_avg:54.41ms
step:1450/1845 train_time:78935ms step_avg:54.44ms
step:1451/1845 train_time:79022ms step_avg:54.46ms
step:1452/1845 train_time:79111ms step_avg:54.48ms
step:1453/1845 train_time:79198ms step_avg:54.51ms
step:1454/1845 train_time:79286ms step_avg:54.53ms
step:1455/1845 train_time:79373ms step_avg:54.55ms
step:1456/1845 train_time:79463ms step_avg:54.58ms
step:1457/1845 train_time:79550ms step_avg:54.60ms
step:1458/1845 train_time:79639ms step_avg:54.62ms
step:1459/1845 train_time:79725ms step_avg:54.64ms
step:1460/1845 train_time:79813ms step_avg:54.67ms
step:1461/1845 train_time:79900ms step_avg:54.69ms
step:1462/1845 train_time:79987ms step_avg:54.71ms
step:1463/1845 train_time:80073ms step_avg:54.73ms
step:1464/1845 train_time:80164ms step_avg:54.76ms
step:1465/1845 train_time:80250ms step_avg:54.78ms
step:1466/1845 train_time:80339ms step_avg:54.80ms
step:1467/1845 train_time:80425ms step_avg:54.82ms
step:1468/1845 train_time:80513ms step_avg:54.85ms
step:1469/1845 train_time:80600ms step_avg:54.87ms
step:1470/1845 train_time:80688ms step_avg:54.89ms
step:1471/1845 train_time:80774ms step_avg:54.91ms
step:1472/1845 train_time:80863ms step_avg:54.93ms
step:1473/1845 train_time:80950ms step_avg:54.96ms
step:1474/1845 train_time:81039ms step_avg:54.98ms
step:1475/1845 train_time:81126ms step_avg:55.00ms
step:1476/1845 train_time:81213ms step_avg:55.02ms
step:1477/1845 train_time:81300ms step_avg:55.04ms
step:1478/1845 train_time:81389ms step_avg:55.07ms
step:1479/1845 train_time:81474ms step_avg:55.09ms
step:1480/1845 train_time:81563ms step_avg:55.11ms
step:1481/1845 train_time:81650ms step_avg:55.13ms
step:1482/1845 train_time:81739ms step_avg:55.15ms
step:1483/1845 train_time:81826ms step_avg:55.18ms
step:1484/1845 train_time:81915ms step_avg:55.20ms
step:1485/1845 train_time:82001ms step_avg:55.22ms
step:1486/1845 train_time:82089ms step_avg:55.24ms
step:1487/1845 train_time:82175ms step_avg:55.26ms
step:1488/1845 train_time:82266ms step_avg:55.29ms
step:1489/1845 train_time:82352ms step_avg:55.31ms
step:1490/1845 train_time:82441ms step_avg:55.33ms
step:1491/1845 train_time:82527ms step_avg:55.35ms
step:1492/1845 train_time:82616ms step_avg:55.37ms
step:1493/1845 train_time:82703ms step_avg:55.39ms
step:1494/1845 train_time:82791ms step_avg:55.42ms
step:1495/1845 train_time:82877ms step_avg:55.44ms
step:1496/1845 train_time:82966ms step_avg:55.46ms
step:1497/1845 train_time:83052ms step_avg:55.48ms
step:1498/1845 train_time:83142ms step_avg:55.50ms
step:1499/1845 train_time:83228ms step_avg:55.52ms
step:1500/1845 train_time:83318ms step_avg:55.55ms
step:1500/1845 val_loss:3.4020 train_time:83413ms step_avg:55.61ms
step:1501/1845 train_time:83440ms step_avg:55.59ms
step:1502/1845 train_time:83492ms step_avg:55.59ms
step:1503/1845 train_time:83583ms step_avg:55.61ms
step:1504/1845 train_time:83674ms step_avg:55.63ms
step:1505/1845 train_time:83759ms step_avg:55.65ms
step:1506/1845 train_time:83846ms step_avg:55.67ms
step:1507/1845 train_time:83932ms step_avg:55.69ms
step:1508/1845 train_time:84020ms step_avg:55.72ms
step:1509/1845 train_time:84106ms step_avg:55.74ms
step:1510/1845 train_time:84194ms step_avg:55.76ms
step:1511/1845 train_time:84280ms step_avg:55.78ms
step:1512/1845 train_time:84373ms step_avg:55.80ms
step:1513/1845 train_time:84461ms step_avg:55.82ms
step:1514/1845 train_time:84552ms step_avg:55.85ms
step:1515/1845 train_time:84641ms step_avg:55.87ms
step:1516/1845 train_time:84729ms step_avg:55.89ms
step:1517/1845 train_time:84814ms step_avg:55.91ms
step:1518/1845 train_time:84902ms step_avg:55.93ms
step:1519/1845 train_time:84989ms step_avg:55.95ms
step:1520/1845 train_time:85076ms step_avg:55.97ms
step:1521/1845 train_time:85162ms step_avg:55.99ms
step:1522/1845 train_time:85250ms step_avg:56.01ms
step:1523/1845 train_time:85338ms step_avg:56.03ms
step:1524/1845 train_time:85428ms step_avg:56.06ms
step:1525/1845 train_time:85515ms step_avg:56.08ms
step:1526/1845 train_time:85605ms step_avg:56.10ms
step:1527/1845 train_time:85691ms step_avg:56.12ms
step:1528/1845 train_time:85779ms step_avg:56.14ms
step:1529/1845 train_time:85865ms step_avg:56.16ms
step:1530/1845 train_time:85952ms step_avg:56.18ms
step:1531/1845 train_time:86039ms step_avg:56.20ms
step:1532/1845 train_time:86127ms step_avg:56.22ms
step:1533/1845 train_time:86212ms step_avg:56.24ms
step:1534/1845 train_time:86303ms step_avg:56.26ms
step:1535/1845 train_time:86392ms step_avg:56.28ms
step:1536/1845 train_time:86482ms step_avg:56.30ms
step:1537/1845 train_time:86569ms step_avg:56.32ms
step:1538/1845 train_time:86658ms step_avg:56.34ms
step:1539/1845 train_time:86743ms step_avg:56.36ms
step:1540/1845 train_time:86831ms step_avg:56.38ms
step:1541/1845 train_time:86916ms step_avg:56.40ms
step:1542/1845 train_time:87006ms step_avg:56.42ms
step:1543/1845 train_time:87091ms step_avg:56.44ms
step:1544/1845 train_time:87180ms step_avg:56.46ms
step:1545/1845 train_time:87266ms step_avg:56.48ms
step:1546/1845 train_time:87354ms step_avg:56.50ms
step:1547/1845 train_time:87442ms step_avg:56.52ms
step:1548/1845 train_time:87532ms step_avg:56.54ms
step:1549/1845 train_time:87618ms step_avg:56.56ms
step:1550/1845 train_time:87707ms step_avg:56.59ms
step:1551/1845 train_time:87792ms step_avg:56.60ms
step:1552/1845 train_time:87881ms step_avg:56.62ms
step:1553/1845 train_time:87967ms step_avg:56.64ms
step:1554/1845 train_time:88055ms step_avg:56.66ms
step:1555/1845 train_time:88141ms step_avg:56.68ms
step:1556/1845 train_time:88230ms step_avg:56.70ms
step:1557/1845 train_time:88314ms step_avg:56.72ms
step:1558/1845 train_time:88407ms step_avg:56.74ms
step:1559/1845 train_time:88493ms step_avg:56.76ms
step:1560/1845 train_time:88582ms step_avg:56.78ms
step:1561/1845 train_time:88669ms step_avg:56.80ms
step:1562/1845 train_time:88758ms step_avg:56.82ms
step:1563/1845 train_time:88844ms step_avg:56.84ms
step:1564/1845 train_time:88931ms step_avg:56.86ms
step:1565/1845 train_time:89016ms step_avg:56.88ms
step:1566/1845 train_time:89106ms step_avg:56.90ms
step:1567/1845 train_time:89191ms step_avg:56.92ms
step:1568/1845 train_time:89281ms step_avg:56.94ms
step:1569/1845 train_time:89368ms step_avg:56.96ms
step:1570/1845 train_time:89456ms step_avg:56.98ms
step:1571/1845 train_time:89543ms step_avg:57.00ms
step:1572/1845 train_time:89631ms step_avg:57.02ms
step:1573/1845 train_time:89717ms step_avg:57.04ms
step:1574/1845 train_time:89806ms step_avg:57.06ms
step:1575/1845 train_time:89891ms step_avg:57.07ms
step:1576/1845 train_time:89980ms step_avg:57.09ms
step:1577/1845 train_time:90067ms step_avg:57.11ms
step:1578/1845 train_time:90155ms step_avg:57.13ms
step:1579/1845 train_time:90241ms step_avg:57.15ms
step:1580/1845 train_time:90330ms step_avg:57.17ms
step:1581/1845 train_time:90416ms step_avg:57.19ms
step:1582/1845 train_time:90506ms step_avg:57.21ms
step:1583/1845 train_time:90592ms step_avg:57.23ms
step:1584/1845 train_time:90682ms step_avg:57.25ms
step:1585/1845 train_time:90768ms step_avg:57.27ms
step:1586/1845 train_time:90856ms step_avg:57.29ms
step:1587/1845 train_time:90941ms step_avg:57.30ms
step:1588/1845 train_time:91030ms step_avg:57.32ms
step:1589/1845 train_time:91116ms step_avg:57.34ms
step:1590/1845 train_time:91206ms step_avg:57.36ms
step:1591/1845 train_time:91291ms step_avg:57.38ms
step:1592/1845 train_time:91381ms step_avg:57.40ms
step:1593/1845 train_time:91467ms step_avg:57.42ms
step:1594/1845 train_time:91555ms step_avg:57.44ms
step:1595/1845 train_time:91642ms step_avg:57.46ms
step:1596/1845 train_time:91731ms step_avg:57.48ms
step:1597/1845 train_time:91817ms step_avg:57.49ms
step:1598/1845 train_time:91906ms step_avg:57.51ms
step:1599/1845 train_time:91992ms step_avg:57.53ms
step:1600/1845 train_time:92082ms step_avg:57.55ms
step:1601/1845 train_time:92168ms step_avg:57.57ms
step:1602/1845 train_time:92256ms step_avg:57.59ms
step:1603/1845 train_time:92342ms step_avg:57.61ms
step:1604/1845 train_time:92430ms step_avg:57.62ms
step:1605/1845 train_time:92517ms step_avg:57.64ms
step:1606/1845 train_time:92607ms step_avg:57.66ms
step:1607/1845 train_time:92693ms step_avg:57.68ms
step:1608/1845 train_time:92782ms step_avg:57.70ms
step:1609/1845 train_time:92868ms step_avg:57.72ms
step:1610/1845 train_time:92957ms step_avg:57.74ms
step:1611/1845 train_time:93043ms step_avg:57.75ms
step:1612/1845 train_time:93131ms step_avg:57.77ms
step:1613/1845 train_time:93217ms step_avg:57.79ms
step:1614/1845 train_time:93307ms step_avg:57.81ms
step:1615/1845 train_time:93392ms step_avg:57.83ms
step:1616/1845 train_time:93482ms step_avg:57.85ms
step:1617/1845 train_time:93569ms step_avg:57.87ms
step:1618/1845 train_time:93658ms step_avg:57.88ms
step:1619/1845 train_time:93744ms step_avg:57.90ms
step:1620/1845 train_time:93831ms step_avg:57.92ms
step:1621/1845 train_time:93918ms step_avg:57.94ms
step:1622/1845 train_time:94008ms step_avg:57.96ms
step:1623/1845 train_time:94094ms step_avg:57.98ms
step:1624/1845 train_time:94182ms step_avg:57.99ms
step:1625/1845 train_time:94270ms step_avg:58.01ms
step:1626/1845 train_time:94358ms step_avg:58.03ms
step:1627/1845 train_time:94444ms step_avg:58.05ms
step:1628/1845 train_time:94532ms step_avg:58.07ms
step:1629/1845 train_time:94618ms step_avg:58.08ms
step:1630/1845 train_time:94707ms step_avg:58.10ms
step:1631/1845 train_time:94793ms step_avg:58.12ms
step:1632/1845 train_time:94883ms step_avg:58.14ms
step:1633/1845 train_time:94969ms step_avg:58.16ms
step:1634/1845 train_time:95058ms step_avg:58.18ms
step:1635/1845 train_time:95145ms step_avg:58.19ms
step:1636/1845 train_time:95232ms step_avg:58.21ms
step:1637/1845 train_time:95319ms step_avg:58.23ms
step:1638/1845 train_time:95409ms step_avg:58.25ms
step:1639/1845 train_time:95495ms step_avg:58.26ms
step:1640/1845 train_time:95584ms step_avg:58.28ms
step:1641/1845 train_time:95670ms step_avg:58.30ms
step:1642/1845 train_time:95759ms step_avg:58.32ms
step:1643/1845 train_time:95844ms step_avg:58.33ms
step:1644/1845 train_time:95932ms step_avg:58.35ms
step:1645/1845 train_time:96019ms step_avg:58.37ms
step:1646/1845 train_time:96108ms step_avg:58.39ms
step:1647/1845 train_time:96194ms step_avg:58.41ms
step:1648/1845 train_time:96284ms step_avg:58.42ms
step:1649/1845 train_time:96370ms step_avg:58.44ms
step:1650/1845 train_time:96459ms step_avg:58.46ms
step:1651/1845 train_time:96545ms step_avg:58.48ms
step:1652/1845 train_time:96634ms step_avg:58.49ms
step:1653/1845 train_time:96720ms step_avg:58.51ms
step:1654/1845 train_time:96810ms step_avg:58.53ms
step:1655/1845 train_time:96896ms step_avg:58.55ms
step:1656/1845 train_time:96985ms step_avg:58.57ms
step:1657/1845 train_time:97072ms step_avg:58.58ms
step:1658/1845 train_time:97161ms step_avg:58.60ms
step:1659/1845 train_time:97247ms step_avg:58.62ms
step:1660/1845 train_time:97336ms step_avg:58.64ms
step:1661/1845 train_time:97423ms step_avg:58.65ms
step:1662/1845 train_time:97511ms step_avg:58.67ms
step:1663/1845 train_time:97597ms step_avg:58.69ms
step:1664/1845 train_time:97688ms step_avg:58.71ms
step:1665/1845 train_time:97774ms step_avg:58.72ms
step:1666/1845 train_time:97863ms step_avg:58.74ms
step:1667/1845 train_time:97949ms step_avg:58.76ms
step:1668/1845 train_time:98038ms step_avg:58.78ms
step:1669/1845 train_time:98123ms step_avg:58.79ms
step:1670/1845 train_time:98211ms step_avg:58.81ms
step:1671/1845 train_time:98298ms step_avg:58.83ms
step:1672/1845 train_time:98388ms step_avg:58.84ms
step:1673/1845 train_time:98474ms step_avg:58.86ms
step:1674/1845 train_time:98563ms step_avg:58.88ms
step:1675/1845 train_time:98650ms step_avg:58.90ms
step:1676/1845 train_time:98738ms step_avg:58.91ms
step:1677/1845 train_time:98825ms step_avg:58.93ms
step:1678/1845 train_time:98913ms step_avg:58.95ms
step:1679/1845 train_time:98999ms step_avg:58.96ms
step:1680/1845 train_time:99089ms step_avg:58.98ms
step:1681/1845 train_time:99174ms step_avg:59.00ms
step:1682/1845 train_time:99263ms step_avg:59.02ms
step:1683/1845 train_time:99348ms step_avg:59.03ms
step:1684/1845 train_time:99438ms step_avg:59.05ms
step:1685/1845 train_time:99523ms step_avg:59.06ms
step:1686/1845 train_time:99613ms step_avg:59.08ms
step:1687/1845 train_time:99699ms step_avg:59.10ms
step:1688/1845 train_time:99789ms step_avg:59.12ms
step:1689/1845 train_time:99874ms step_avg:59.13ms
step:1690/1845 train_time:99965ms step_avg:59.15ms
step:1691/1845 train_time:100050ms step_avg:59.17ms
step:1692/1845 train_time:100138ms step_avg:59.18ms
step:1693/1845 train_time:100224ms step_avg:59.20ms
step:1694/1845 train_time:100312ms step_avg:59.22ms
step:1695/1845 train_time:100399ms step_avg:59.23ms
step:1696/1845 train_time:100489ms step_avg:59.25ms
step:1697/1845 train_time:100574ms step_avg:59.27ms
step:1698/1845 train_time:100664ms step_avg:59.28ms
step:1699/1845 train_time:100750ms step_avg:59.30ms
step:1700/1845 train_time:100839ms step_avg:59.32ms
step:1701/1845 train_time:100925ms step_avg:59.33ms
step:1702/1845 train_time:101013ms step_avg:59.35ms
step:1703/1845 train_time:101099ms step_avg:59.37ms
step:1704/1845 train_time:101189ms step_avg:59.38ms
step:1705/1845 train_time:101273ms step_avg:59.40ms
step:1706/1845 train_time:101364ms step_avg:59.42ms
step:1707/1845 train_time:101449ms step_avg:59.43ms
step:1708/1845 train_time:101538ms step_avg:59.45ms
step:1709/1845 train_time:101624ms step_avg:59.46ms
step:1710/1845 train_time:101713ms step_avg:59.48ms
step:1711/1845 train_time:101799ms step_avg:59.50ms
step:1712/1845 train_time:101889ms step_avg:59.51ms
step:1713/1845 train_time:101975ms step_avg:59.53ms
step:1714/1845 train_time:102064ms step_avg:59.55ms
step:1715/1845 train_time:102150ms step_avg:59.56ms
step:1716/1845 train_time:102239ms step_avg:59.58ms
step:1717/1845 train_time:102324ms step_avg:59.59ms
step:1718/1845 train_time:102412ms step_avg:59.61ms
step:1719/1845 train_time:102499ms step_avg:59.63ms
step:1720/1845 train_time:102588ms step_avg:59.64ms
step:1721/1845 train_time:102673ms step_avg:59.66ms
step:1722/1845 train_time:102763ms step_avg:59.68ms
step:1723/1845 train_time:102849ms step_avg:59.69ms
step:1724/1845 train_time:102938ms step_avg:59.71ms
step:1725/1845 train_time:103025ms step_avg:59.72ms
step:1726/1845 train_time:103113ms step_avg:59.74ms
step:1727/1845 train_time:103201ms step_avg:59.76ms
step:1728/1845 train_time:103289ms step_avg:59.77ms
step:1729/1845 train_time:103374ms step_avg:59.79ms
step:1730/1845 train_time:103464ms step_avg:59.81ms
step:1731/1845 train_time:103549ms step_avg:59.82ms
step:1732/1845 train_time:103638ms step_avg:59.84ms
step:1733/1845 train_time:103724ms step_avg:59.85ms
step:1734/1845 train_time:103813ms step_avg:59.87ms
step:1735/1845 train_time:103901ms step_avg:59.89ms
step:1736/1845 train_time:103989ms step_avg:59.90ms
step:1737/1845 train_time:104076ms step_avg:59.92ms
step:1738/1845 train_time:104165ms step_avg:59.93ms
step:1739/1845 train_time:104250ms step_avg:59.95ms
step:1740/1845 train_time:104340ms step_avg:59.97ms
step:1741/1845 train_time:104426ms step_avg:59.98ms
step:1742/1845 train_time:104514ms step_avg:60.00ms
step:1743/1845 train_time:104600ms step_avg:60.01ms
step:1744/1845 train_time:104689ms step_avg:60.03ms
step:1745/1845 train_time:104775ms step_avg:60.04ms
step:1746/1845 train_time:104864ms step_avg:60.06ms
step:1747/1845 train_time:104950ms step_avg:60.07ms
step:1748/1845 train_time:105039ms step_avg:60.09ms
step:1749/1845 train_time:105126ms step_avg:60.11ms
step:1750/1845 train_time:105214ms step_avg:60.12ms
step:1750/1845 val_loss:3.3034 train_time:105312ms step_avg:60.18ms
step:1751/1845 train_time:105339ms step_avg:60.16ms
step:1752/1845 train_time:105390ms step_avg:60.15ms
step:1753/1845 train_time:105477ms step_avg:60.17ms
step:1754/1845 train_time:105566ms step_avg:60.19ms
step:1755/1845 train_time:105652ms step_avg:60.20ms
step:1756/1845 train_time:105740ms step_avg:60.22ms
step:1757/1845 train_time:105825ms step_avg:60.23ms
step:1758/1845 train_time:105912ms step_avg:60.25ms
step:1759/1845 train_time:105999ms step_avg:60.26ms
step:1760/1845 train_time:106088ms step_avg:60.28ms
step:1761/1845 train_time:106174ms step_avg:60.29ms
step:1762/1845 train_time:106267ms step_avg:60.31ms
step:1763/1845 train_time:106355ms step_avg:60.33ms
step:1764/1845 train_time:106446ms step_avg:60.34ms
step:1765/1845 train_time:106532ms step_avg:60.36ms
step:1766/1845 train_time:106623ms step_avg:60.38ms
step:1767/1845 train_time:106708ms step_avg:60.39ms
step:1768/1845 train_time:106796ms step_avg:60.40ms
step:1769/1845 train_time:106881ms step_avg:60.42ms
step:1770/1845 train_time:106969ms step_avg:60.43ms
step:1771/1845 train_time:107054ms step_avg:60.45ms
step:1772/1845 train_time:107144ms step_avg:60.47ms
step:1773/1845 train_time:107231ms step_avg:60.48ms
step:1774/1845 train_time:107322ms step_avg:60.50ms
step:1775/1845 train_time:107408ms step_avg:60.51ms
step:1776/1845 train_time:107498ms step_avg:60.53ms
step:1777/1845 train_time:107584ms step_avg:60.54ms
step:1778/1845 train_time:107671ms step_avg:60.56ms
step:1779/1845 train_time:107758ms step_avg:60.57ms
step:1780/1845 train_time:107846ms step_avg:60.59ms
step:1781/1845 train_time:107931ms step_avg:60.60ms
step:1782/1845 train_time:108021ms step_avg:60.62ms
step:1783/1845 train_time:108107ms step_avg:60.63ms
step:1784/1845 train_time:108197ms step_avg:60.65ms
step:1785/1845 train_time:108283ms step_avg:60.66ms
step:1786/1845 train_time:108372ms step_avg:60.68ms
step:1787/1845 train_time:108459ms step_avg:60.69ms
step:1788/1845 train_time:108549ms step_avg:60.71ms
step:1789/1845 train_time:108635ms step_avg:60.72ms
step:1790/1845 train_time:108723ms step_avg:60.74ms
step:1791/1845 train_time:108809ms step_avg:60.75ms
step:1792/1845 train_time:108896ms step_avg:60.77ms
step:1793/1845 train_time:108982ms step_avg:60.78ms
step:1794/1845 train_time:109070ms step_avg:60.80ms
step:1795/1845 train_time:109157ms step_avg:60.81ms
step:1796/1845 train_time:109247ms step_avg:60.83ms
step:1797/1845 train_time:109333ms step_avg:60.84ms
step:1798/1845 train_time:109423ms step_avg:60.86ms
step:1799/1845 train_time:109509ms step_avg:60.87ms
step:1800/1845 train_time:109598ms step_avg:60.89ms
step:1801/1845 train_time:109684ms step_avg:60.90ms
step:1802/1845 train_time:109772ms step_avg:60.92ms
step:1803/1845 train_time:109858ms step_avg:60.93ms
step:1804/1845 train_time:109947ms step_avg:60.95ms
step:1805/1845 train_time:110032ms step_avg:60.96ms
step:1806/1845 train_time:110124ms step_avg:60.98ms
step:1807/1845 train_time:110210ms step_avg:60.99ms
step:1808/1845 train_time:110299ms step_avg:61.01ms
step:1809/1845 train_time:110386ms step_avg:61.02ms
step:1810/1845 train_time:110475ms step_avg:61.04ms
step:1811/1845 train_time:110562ms step_avg:61.05ms
step:1812/1845 train_time:110650ms step_avg:61.07ms
step:1813/1845 train_time:110737ms step_avg:61.08ms
step:1814/1845 train_time:110826ms step_avg:61.09ms
step:1815/1845 train_time:110913ms step_avg:61.11ms
step:1816/1845 train_time:111002ms step_avg:61.12ms
step:1817/1845 train_time:111087ms step_avg:61.14ms
step:1818/1845 train_time:111177ms step_avg:61.15ms
step:1819/1845 train_time:111264ms step_avg:61.17ms
step:1820/1845 train_time:111352ms step_avg:61.18ms
step:1821/1845 train_time:111440ms step_avg:61.20ms
step:1822/1845 train_time:111528ms step_avg:61.21ms
step:1823/1845 train_time:111615ms step_avg:61.23ms
step:1824/1845 train_time:111705ms step_avg:61.24ms
step:1825/1845 train_time:111789ms step_avg:61.25ms
step:1826/1845 train_time:111879ms step_avg:61.27ms
step:1827/1845 train_time:111966ms step_avg:61.28ms
step:1828/1845 train_time:112054ms step_avg:61.30ms
step:1829/1845 train_time:112142ms step_avg:61.31ms
step:1830/1845 train_time:112231ms step_avg:61.33ms
step:1831/1845 train_time:112316ms step_avg:61.34ms
step:1832/1845 train_time:112407ms step_avg:61.36ms
step:1833/1845 train_time:112494ms step_avg:61.37ms
step:1834/1845 train_time:112584ms step_avg:61.39ms
step:1835/1845 train_time:112669ms step_avg:61.40ms
step:1836/1845 train_time:112759ms step_avg:61.42ms
step:1837/1845 train_time:112846ms step_avg:61.43ms
step:1838/1845 train_time:112934ms step_avg:61.44ms
step:1839/1845 train_time:113020ms step_avg:61.46ms
step:1840/1845 train_time:113109ms step_avg:61.47ms
step:1841/1845 train_time:113196ms step_avg:61.49ms
step:1842/1845 train_time:113285ms step_avg:61.50ms
step:1843/1845 train_time:113372ms step_avg:61.51ms
step:1844/1845 train_time:113460ms step_avg:61.53ms
step:1845/1845 train_time:113548ms step_avg:61.54ms
step:1845/1845 val_loss:3.2780 train_time:113644ms step_avg:61.60ms
peak memory allocated: 29497 MiB reserved: 44798 MiB

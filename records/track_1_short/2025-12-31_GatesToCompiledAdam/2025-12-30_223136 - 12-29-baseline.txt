# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29-baseline"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return
            
        #Debug: print firing order (only on rank 0)
        # if dist.get_rank() == 0:
        #     if not hasattr(self, '_hook_counter'):
        #         self._hook_counter = 0
        #     if self._hook_counter > -1: # Signal to disable this printout               
        #         self._hook_counter += 1
        #         label = getattr(param, 'label', None)
        #         print(f"{self._hook_counter}: {label} shape={tuple(param.shape)}")


        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        # self._hook_counter = -1 # 0 to reset counter for next step, -1 to disable.
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

if master_process:
    import wandb
    wandb.login()
    wandb.init(
        project=WANDB_PROJECT,
        name=args.run_id,
        config=args
    )
    wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        if master_process:
            # Log to wandb
            wandb.log({
                "val/loss": val_loss.item(),
                "time/training_time_ms": training_time_ms,
                "time/step_avg_ms": training_time_ms / (step + 1),
            }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process:
            final_metrics = {
                "final/val_loss": float(val_loss.item()),
                "final/train_time_ms": int(training_time_ms),
                "final/train_time": training_time_ms / 1000,
                "final/steps_total": int(step),
            }
            wandb.log(final_metrics)
            wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
if master_process:
    wandb.save(f"{args.log_dir}/{args.run_id}.txt")
    wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 06:31:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   29C    P0            112W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          111181      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          111182      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          111183      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          111184      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          111185      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    5   N/A  N/A          111186      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          111187      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    7   N/A  N/A          111188      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8312 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:57ms step_avg:56.57ms
step:2/1845 train_time:85ms step_avg:42.67ms
step:3/1845 train_time:111ms step_avg:36.99ms
step:4/1845 train_time:140ms step_avg:34.94ms
step:5/1845 train_time:166ms step_avg:33.20ms
step:6/1845 train_time:277ms step_avg:46.17ms
step:7/1845 train_time:311ms step_avg:44.40ms
step:8/1845 train_time:349ms step_avg:43.60ms
step:9/1845 train_time:382ms step_avg:42.45ms
step:10/1845 train_time:420ms step_avg:42.01ms
step:11/1845 train_time:455ms step_avg:41.37ms
step:12/1845 train_time:494ms step_avg:41.13ms
step:13/1845 train_time:527ms step_avg:40.52ms
step:14/1845 train_time:564ms step_avg:40.32ms
step:15/1845 train_time:599ms step_avg:39.91ms
step:16/1845 train_time:639ms step_avg:39.96ms
step:17/1845 train_time:674ms step_avg:39.64ms
step:18/1845 train_time:712ms step_avg:39.53ms
step:19/1845 train_time:746ms step_avg:39.26ms
step:20/1845 train_time:784ms step_avg:39.22ms
step:21/1845 train_time:820ms step_avg:39.06ms
step:22/1845 train_time:859ms step_avg:39.06ms
step:23/1845 train_time:893ms step_avg:38.84ms
step:24/1845 train_time:932ms step_avg:38.81ms
step:25/1845 train_time:968ms step_avg:38.70ms
step:26/1845 train_time:1006ms step_avg:38.69ms
step:27/1845 train_time:1041ms step_avg:38.54ms
step:28/1845 train_time:1079ms step_avg:38.52ms
step:29/1845 train_time:1113ms step_avg:38.37ms
step:30/1845 train_time:1151ms step_avg:38.38ms
step:31/1845 train_time:1186ms step_avg:38.25ms
step:32/1845 train_time:1224ms step_avg:38.24ms
step:33/1845 train_time:1258ms step_avg:38.13ms
step:34/1845 train_time:1296ms step_avg:38.13ms
step:35/1845 train_time:1330ms step_avg:38.01ms
step:36/1845 train_time:1369ms step_avg:38.02ms
step:37/1845 train_time:1403ms step_avg:37.91ms
step:38/1845 train_time:1441ms step_avg:37.92ms
step:39/1845 train_time:1477ms step_avg:37.87ms
step:40/1845 train_time:1516ms step_avg:37.91ms
step:41/1845 train_time:1552ms step_avg:37.86ms
step:42/1845 train_time:1591ms step_avg:37.89ms
step:43/1845 train_time:1626ms step_avg:37.80ms
step:44/1845 train_time:1664ms step_avg:37.82ms
step:45/1845 train_time:1699ms step_avg:37.76ms
step:46/1845 train_time:1737ms step_avg:37.76ms
step:47/1845 train_time:1772ms step_avg:37.70ms
step:48/1845 train_time:1810ms step_avg:37.70ms
step:49/1845 train_time:1844ms step_avg:37.64ms
step:50/1845 train_time:1883ms step_avg:37.65ms
step:51/1845 train_time:1917ms step_avg:37.59ms
step:52/1845 train_time:1955ms step_avg:37.59ms
step:53/1845 train_time:1989ms step_avg:37.52ms
step:54/1845 train_time:2026ms step_avg:37.53ms
step:55/1845 train_time:2061ms step_avg:37.48ms
step:56/1845 train_time:2099ms step_avg:37.48ms
step:57/1845 train_time:2133ms step_avg:37.42ms
step:58/1845 train_time:2171ms step_avg:37.43ms
step:59/1845 train_time:2206ms step_avg:37.40ms
step:60/1845 train_time:2245ms step_avg:37.42ms
step:61/1845 train_time:2280ms step_avg:37.38ms
step:62/1845 train_time:2320ms step_avg:37.42ms
step:63/1845 train_time:2355ms step_avg:37.38ms
step:64/1845 train_time:2392ms step_avg:37.38ms
step:65/1845 train_time:2427ms step_avg:37.33ms
step:66/1845 train_time:2465ms step_avg:37.35ms
step:67/1845 train_time:2499ms step_avg:37.30ms
step:68/1845 train_time:2537ms step_avg:37.30ms
step:69/1845 train_time:2571ms step_avg:37.26ms
step:70/1845 train_time:2609ms step_avg:37.27ms
step:71/1845 train_time:2643ms step_avg:37.22ms
step:72/1845 train_time:2681ms step_avg:37.23ms
step:73/1845 train_time:2715ms step_avg:37.19ms
step:74/1845 train_time:2753ms step_avg:37.21ms
step:75/1845 train_time:2788ms step_avg:37.17ms
step:76/1845 train_time:2825ms step_avg:37.17ms
step:77/1845 train_time:2860ms step_avg:37.14ms
step:78/1845 train_time:2898ms step_avg:37.16ms
step:79/1845 train_time:2934ms step_avg:37.14ms
step:80/1845 train_time:2973ms step_avg:37.16ms
step:81/1845 train_time:3008ms step_avg:37.14ms
step:82/1845 train_time:3047ms step_avg:37.15ms
step:83/1845 train_time:3081ms step_avg:37.12ms
step:84/1845 train_time:3119ms step_avg:37.13ms
step:85/1845 train_time:3154ms step_avg:37.11ms
step:86/1845 train_time:3192ms step_avg:37.12ms
step:87/1845 train_time:3227ms step_avg:37.09ms
step:88/1845 train_time:3265ms step_avg:37.10ms
step:89/1845 train_time:3299ms step_avg:37.06ms
step:90/1845 train_time:3337ms step_avg:37.07ms
step:91/1845 train_time:3371ms step_avg:37.05ms
step:92/1845 train_time:3409ms step_avg:37.05ms
step:93/1845 train_time:3443ms step_avg:37.02ms
step:94/1845 train_time:3481ms step_avg:37.03ms
step:95/1845 train_time:3515ms step_avg:37.00ms
step:96/1845 train_time:3553ms step_avg:37.01ms
step:97/1845 train_time:3587ms step_avg:36.98ms
step:98/1845 train_time:3626ms step_avg:37.00ms
step:99/1845 train_time:3661ms step_avg:36.98ms
step:100/1845 train_time:3698ms step_avg:36.98ms
step:101/1845 train_time:3732ms step_avg:36.95ms
step:102/1845 train_time:3770ms step_avg:36.96ms
step:103/1845 train_time:3804ms step_avg:36.94ms
step:104/1845 train_time:3843ms step_avg:36.95ms
step:105/1845 train_time:3879ms step_avg:36.94ms
step:106/1845 train_time:3917ms step_avg:36.95ms
step:107/1845 train_time:3951ms step_avg:36.93ms
step:108/1845 train_time:3991ms step_avg:36.95ms
step:109/1845 train_time:4025ms step_avg:36.92ms
step:110/1845 train_time:4064ms step_avg:36.94ms
step:111/1845 train_time:4099ms step_avg:36.92ms
step:112/1845 train_time:4137ms step_avg:36.93ms
step:113/1845 train_time:4171ms step_avg:36.91ms
step:114/1845 train_time:4208ms step_avg:36.92ms
step:115/1845 train_time:4243ms step_avg:36.89ms
step:116/1845 train_time:4281ms step_avg:36.90ms
step:117/1845 train_time:4315ms step_avg:36.88ms
step:118/1845 train_time:4353ms step_avg:36.89ms
step:119/1845 train_time:4388ms step_avg:36.87ms
step:120/1845 train_time:4426ms step_avg:36.88ms
step:121/1845 train_time:4461ms step_avg:36.87ms
step:122/1845 train_time:4498ms step_avg:36.87ms
step:123/1845 train_time:4533ms step_avg:36.85ms
step:124/1845 train_time:4571ms step_avg:36.86ms
step:125/1845 train_time:4605ms step_avg:36.84ms
step:126/1845 train_time:4643ms step_avg:36.85ms
step:127/1845 train_time:4677ms step_avg:36.82ms
step:128/1845 train_time:4714ms step_avg:36.83ms
step:129/1845 train_time:4748ms step_avg:36.81ms
step:130/1845 train_time:4787ms step_avg:36.82ms
step:131/1845 train_time:4823ms step_avg:36.81ms
step:132/1845 train_time:4861ms step_avg:36.83ms
step:133/1845 train_time:4896ms step_avg:36.81ms
step:134/1845 train_time:4935ms step_avg:36.83ms
step:135/1845 train_time:4970ms step_avg:36.81ms
step:136/1845 train_time:5008ms step_avg:36.82ms
step:137/1845 train_time:5043ms step_avg:36.81ms
step:138/1845 train_time:5081ms step_avg:36.82ms
step:139/1845 train_time:5115ms step_avg:36.80ms
step:140/1845 train_time:5152ms step_avg:36.80ms
step:141/1845 train_time:5187ms step_avg:36.79ms
step:142/1845 train_time:5225ms step_avg:36.80ms
step:143/1845 train_time:5259ms step_avg:36.78ms
step:144/1845 train_time:5298ms step_avg:36.79ms
step:145/1845 train_time:5332ms step_avg:36.77ms
step:146/1845 train_time:5370ms step_avg:36.78ms
step:147/1845 train_time:5404ms step_avg:36.76ms
step:148/1845 train_time:5441ms step_avg:36.76ms
step:149/1845 train_time:5475ms step_avg:36.75ms
step:150/1845 train_time:5513ms step_avg:36.76ms
step:151/1845 train_time:5549ms step_avg:36.75ms
step:152/1845 train_time:5588ms step_avg:36.76ms
step:153/1845 train_time:5622ms step_avg:36.75ms
step:154/1845 train_time:5661ms step_avg:36.76ms
step:155/1845 train_time:5696ms step_avg:36.75ms
step:156/1845 train_time:5734ms step_avg:36.76ms
step:157/1845 train_time:5769ms step_avg:36.75ms
step:158/1845 train_time:5807ms step_avg:36.75ms
step:159/1845 train_time:5841ms step_avg:36.74ms
step:160/1845 train_time:5879ms step_avg:36.74ms
step:161/1845 train_time:5913ms step_avg:36.73ms
step:162/1845 train_time:5951ms step_avg:36.73ms
step:163/1845 train_time:5985ms step_avg:36.72ms
step:164/1845 train_time:6023ms step_avg:36.72ms
step:165/1845 train_time:6057ms step_avg:36.71ms
step:166/1845 train_time:6097ms step_avg:36.73ms
step:167/1845 train_time:6132ms step_avg:36.72ms
step:168/1845 train_time:6170ms step_avg:36.73ms
step:169/1845 train_time:6205ms step_avg:36.72ms
step:170/1845 train_time:6242ms step_avg:36.72ms
step:171/1845 train_time:6277ms step_avg:36.71ms
step:172/1845 train_time:6315ms step_avg:36.71ms
step:173/1845 train_time:6348ms step_avg:36.70ms
step:174/1845 train_time:6384ms step_avg:36.69ms
step:175/1845 train_time:6419ms step_avg:36.68ms
step:176/1845 train_time:6458ms step_avg:36.69ms
step:177/1845 train_time:6494ms step_avg:36.69ms
step:178/1845 train_time:6532ms step_avg:36.70ms
step:179/1845 train_time:6567ms step_avg:36.69ms
step:180/1845 train_time:6606ms step_avg:36.70ms
step:181/1845 train_time:6640ms step_avg:36.69ms
step:182/1845 train_time:6678ms step_avg:36.69ms
step:183/1845 train_time:6712ms step_avg:36.68ms
step:184/1845 train_time:6749ms step_avg:36.68ms
step:185/1845 train_time:6783ms step_avg:36.67ms
step:186/1845 train_time:6821ms step_avg:36.67ms
step:187/1845 train_time:6855ms step_avg:36.66ms
step:188/1845 train_time:6893ms step_avg:36.66ms
step:189/1845 train_time:6927ms step_avg:36.65ms
step:190/1845 train_time:6965ms step_avg:36.66ms
step:191/1845 train_time:6999ms step_avg:36.65ms
step:192/1845 train_time:7038ms step_avg:36.65ms
step:193/1845 train_time:7072ms step_avg:36.64ms
step:194/1845 train_time:7110ms step_avg:36.65ms
step:195/1845 train_time:7143ms step_avg:36.63ms
step:196/1845 train_time:7182ms step_avg:36.64ms
step:197/1845 train_time:7217ms step_avg:36.64ms
step:198/1845 train_time:7256ms step_avg:36.65ms
step:199/1845 train_time:7291ms step_avg:36.64ms
step:200/1845 train_time:7330ms step_avg:36.65ms
step:201/1845 train_time:7364ms step_avg:36.64ms
step:202/1845 train_time:7402ms step_avg:36.64ms
step:203/1845 train_time:7437ms step_avg:36.63ms
step:204/1845 train_time:7475ms step_avg:36.64ms
step:205/1845 train_time:7509ms step_avg:36.63ms
step:206/1845 train_time:7547ms step_avg:36.63ms
step:207/1845 train_time:7581ms step_avg:36.62ms
step:208/1845 train_time:7618ms step_avg:36.63ms
step:209/1845 train_time:7652ms step_avg:36.61ms
step:210/1845 train_time:7690ms step_avg:36.62ms
step:211/1845 train_time:7724ms step_avg:36.61ms
step:212/1845 train_time:7762ms step_avg:36.61ms
step:213/1845 train_time:7796ms step_avg:36.60ms
step:214/1845 train_time:7834ms step_avg:36.61ms
step:215/1845 train_time:7869ms step_avg:36.60ms
step:216/1845 train_time:7907ms step_avg:36.61ms
step:217/1845 train_time:7942ms step_avg:36.60ms
step:218/1845 train_time:7982ms step_avg:36.61ms
step:219/1845 train_time:8016ms step_avg:36.60ms
step:220/1845 train_time:8055ms step_avg:36.61ms
step:221/1845 train_time:8090ms step_avg:36.60ms
step:222/1845 train_time:8127ms step_avg:36.61ms
step:223/1845 train_time:8162ms step_avg:36.60ms
step:224/1845 train_time:8200ms step_avg:36.61ms
step:225/1845 train_time:8234ms step_avg:36.60ms
step:226/1845 train_time:8272ms step_avg:36.60ms
step:227/1845 train_time:8307ms step_avg:36.59ms
step:228/1845 train_time:8344ms step_avg:36.60ms
step:229/1845 train_time:8378ms step_avg:36.59ms
step:230/1845 train_time:8417ms step_avg:36.59ms
step:231/1845 train_time:8450ms step_avg:36.58ms
step:232/1845 train_time:8489ms step_avg:36.59ms
step:233/1845 train_time:8523ms step_avg:36.58ms
step:234/1845 train_time:8561ms step_avg:36.58ms
step:235/1845 train_time:8595ms step_avg:36.57ms
step:236/1845 train_time:8632ms step_avg:36.58ms
step:237/1845 train_time:8666ms step_avg:36.57ms
step:238/1845 train_time:8704ms step_avg:36.57ms
step:239/1845 train_time:8738ms step_avg:36.56ms
step:240/1845 train_time:8776ms step_avg:36.57ms
step:241/1845 train_time:8811ms step_avg:36.56ms
step:242/1845 train_time:8849ms step_avg:36.57ms
step:243/1845 train_time:8885ms step_avg:36.57ms
step:244/1845 train_time:8923ms step_avg:36.57ms
step:245/1845 train_time:8958ms step_avg:36.56ms
step:246/1845 train_time:8996ms step_avg:36.57ms
step:247/1845 train_time:9030ms step_avg:36.56ms
step:248/1845 train_time:9069ms step_avg:36.57ms
step:249/1845 train_time:9103ms step_avg:36.56ms
step:250/1845 train_time:9141ms step_avg:36.57ms
step:250/1845 val_loss:4.5977 train_time:9144ms step_avg:36.57ms
step:251/1845 train_time:9171ms step_avg:36.54ms
step:252/1845 train_time:9198ms step_avg:36.50ms
step:253/1845 train_time:9222ms step_avg:36.45ms
step:254/1845 train_time:9249ms step_avg:36.42ms
step:255/1845 train_time:9283ms step_avg:36.40ms
step:256/1845 train_time:9318ms step_avg:36.40ms
step:257/1845 train_time:9352ms step_avg:36.39ms
step:258/1845 train_time:9388ms step_avg:36.39ms
step:259/1845 train_time:9421ms step_avg:36.37ms
step:260/1845 train_time:9459ms step_avg:36.38ms
step:261/1845 train_time:9493ms step_avg:36.37ms
step:262/1845 train_time:9527ms step_avg:36.36ms
step:263/1845 train_time:9558ms step_avg:36.34ms
step:264/1845 train_time:9592ms step_avg:36.33ms
step:265/1845 train_time:9626ms step_avg:36.32ms
step:266/1845 train_time:9662ms step_avg:36.32ms
step:267/1845 train_time:9696ms step_avg:36.31ms
step:268/1845 train_time:9731ms step_avg:36.31ms
step:269/1845 train_time:9766ms step_avg:36.30ms
step:270/1845 train_time:9801ms step_avg:36.30ms
step:271/1845 train_time:9835ms step_avg:36.29ms
step:272/1845 train_time:9871ms step_avg:36.29ms
step:273/1845 train_time:9906ms step_avg:36.29ms
step:274/1845 train_time:9941ms step_avg:36.28ms
step:275/1845 train_time:9976ms step_avg:36.28ms
step:276/1845 train_time:10011ms step_avg:36.27ms
step:277/1845 train_time:10045ms step_avg:36.26ms
step:278/1845 train_time:10081ms step_avg:36.26ms
step:279/1845 train_time:10116ms step_avg:36.26ms
step:280/1845 train_time:10151ms step_avg:36.25ms
step:281/1845 train_time:10186ms step_avg:36.25ms
step:282/1845 train_time:10221ms step_avg:36.24ms
step:283/1845 train_time:10256ms step_avg:36.24ms
step:284/1845 train_time:10292ms step_avg:36.24ms
step:285/1845 train_time:10326ms step_avg:36.23ms
step:286/1845 train_time:10362ms step_avg:36.23ms
step:287/1845 train_time:10396ms step_avg:36.22ms
step:288/1845 train_time:10432ms step_avg:36.22ms
step:289/1845 train_time:10466ms step_avg:36.22ms
step:290/1845 train_time:10501ms step_avg:36.21ms
step:291/1845 train_time:10537ms step_avg:36.21ms
step:292/1845 train_time:10572ms step_avg:36.21ms
step:293/1845 train_time:10607ms step_avg:36.20ms
step:294/1845 train_time:10642ms step_avg:36.20ms
step:295/1845 train_time:10677ms step_avg:36.19ms
step:296/1845 train_time:10713ms step_avg:36.19ms
step:297/1845 train_time:10747ms step_avg:36.19ms
step:298/1845 train_time:10783ms step_avg:36.18ms
step:299/1845 train_time:10818ms step_avg:36.18ms
step:300/1845 train_time:10853ms step_avg:36.18ms
step:301/1845 train_time:10888ms step_avg:36.17ms
step:302/1845 train_time:10923ms step_avg:36.17ms
step:303/1845 train_time:10958ms step_avg:36.16ms
step:304/1845 train_time:10993ms step_avg:36.16ms
step:305/1845 train_time:11028ms step_avg:36.16ms
step:306/1845 train_time:11064ms step_avg:36.16ms
step:307/1845 train_time:11098ms step_avg:36.15ms
step:308/1845 train_time:11134ms step_avg:36.15ms
step:309/1845 train_time:11168ms step_avg:36.14ms
step:310/1845 train_time:11204ms step_avg:36.14ms
step:311/1845 train_time:11238ms step_avg:36.13ms
step:312/1845 train_time:11274ms step_avg:36.14ms
step:313/1845 train_time:11308ms step_avg:36.13ms
step:314/1845 train_time:11344ms step_avg:36.13ms
step:315/1845 train_time:11378ms step_avg:36.12ms
step:316/1845 train_time:11414ms step_avg:36.12ms
step:317/1845 train_time:11448ms step_avg:36.11ms
step:318/1845 train_time:11484ms step_avg:36.11ms
step:319/1845 train_time:11519ms step_avg:36.11ms
step:320/1845 train_time:11555ms step_avg:36.11ms
step:321/1845 train_time:11589ms step_avg:36.10ms
step:322/1845 train_time:11624ms step_avg:36.10ms
step:323/1845 train_time:11658ms step_avg:36.09ms
step:324/1845 train_time:11694ms step_avg:36.09ms
step:325/1845 train_time:11728ms step_avg:36.09ms
step:326/1845 train_time:11764ms step_avg:36.09ms
step:327/1845 train_time:11798ms step_avg:36.08ms
step:328/1845 train_time:11834ms step_avg:36.08ms
step:329/1845 train_time:11868ms step_avg:36.07ms
step:330/1845 train_time:11904ms step_avg:36.07ms
step:331/1845 train_time:11938ms step_avg:36.07ms
step:332/1845 train_time:11974ms step_avg:36.07ms
step:333/1845 train_time:12008ms step_avg:36.06ms
step:334/1845 train_time:12045ms step_avg:36.06ms
step:335/1845 train_time:12078ms step_avg:36.05ms
step:336/1845 train_time:12113ms step_avg:36.05ms
step:337/1845 train_time:12152ms step_avg:36.06ms
step:338/1845 train_time:12190ms step_avg:36.06ms
step:339/1845 train_time:12223ms step_avg:36.05ms
step:340/1845 train_time:12259ms step_avg:36.06ms
step:341/1845 train_time:12292ms step_avg:36.05ms
step:342/1845 train_time:12328ms step_avg:36.05ms
step:343/1845 train_time:12360ms step_avg:36.03ms
step:344/1845 train_time:12396ms step_avg:36.03ms
step:345/1845 train_time:12428ms step_avg:36.02ms
step:346/1845 train_time:12464ms step_avg:36.02ms
step:347/1845 train_time:12498ms step_avg:36.02ms
step:348/1845 train_time:12534ms step_avg:36.02ms
step:349/1845 train_time:12568ms step_avg:36.01ms
step:350/1845 train_time:12604ms step_avg:36.01ms
step:351/1845 train_time:12638ms step_avg:36.01ms
step:352/1845 train_time:12674ms step_avg:36.01ms
step:353/1845 train_time:12708ms step_avg:36.00ms
step:354/1845 train_time:12744ms step_avg:36.00ms
step:355/1845 train_time:12779ms step_avg:36.00ms
step:356/1845 train_time:12814ms step_avg:36.00ms
step:357/1845 train_time:12850ms step_avg:35.99ms
step:358/1845 train_time:12885ms step_avg:35.99ms
step:359/1845 train_time:12921ms step_avg:35.99ms
step:360/1845 train_time:12961ms step_avg:36.00ms
step:361/1845 train_time:12997ms step_avg:36.00ms
step:362/1845 train_time:13036ms step_avg:36.01ms
step:363/1845 train_time:13072ms step_avg:36.01ms
step:364/1845 train_time:13112ms step_avg:36.02ms
step:365/1845 train_time:13147ms step_avg:36.02ms
step:366/1845 train_time:13185ms step_avg:36.02ms
step:367/1845 train_time:13220ms step_avg:36.02ms
step:368/1845 train_time:13261ms step_avg:36.04ms
step:369/1845 train_time:13297ms step_avg:36.03ms
step:370/1845 train_time:13335ms step_avg:36.04ms
step:371/1845 train_time:13369ms step_avg:36.04ms
step:372/1845 train_time:13406ms step_avg:36.04ms
step:373/1845 train_time:13441ms step_avg:36.03ms
step:374/1845 train_time:13478ms step_avg:36.04ms
step:375/1845 train_time:13511ms step_avg:36.03ms
step:376/1845 train_time:13547ms step_avg:36.03ms
step:377/1845 train_time:13580ms step_avg:36.02ms
step:378/1845 train_time:13617ms step_avg:36.02ms
step:379/1845 train_time:13649ms step_avg:36.01ms
step:380/1845 train_time:13684ms step_avg:36.01ms
step:381/1845 train_time:13716ms step_avg:36.00ms
step:382/1845 train_time:13751ms step_avg:36.00ms
step:383/1845 train_time:13783ms step_avg:35.99ms
step:384/1845 train_time:13819ms step_avg:35.99ms
step:385/1845 train_time:13850ms step_avg:35.98ms
step:386/1845 train_time:13885ms step_avg:35.97ms
step:387/1845 train_time:13916ms step_avg:35.96ms
step:388/1845 train_time:13950ms step_avg:35.95ms
step:389/1845 train_time:13981ms step_avg:35.94ms
step:390/1845 train_time:14016ms step_avg:35.94ms
step:391/1845 train_time:14046ms step_avg:35.92ms
step:392/1845 train_time:14080ms step_avg:35.92ms
step:393/1845 train_time:14114ms step_avg:35.91ms
step:394/1845 train_time:14149ms step_avg:35.91ms
step:395/1845 train_time:14184ms step_avg:35.91ms
step:396/1845 train_time:14221ms step_avg:35.91ms
step:397/1845 train_time:14254ms step_avg:35.91ms
step:398/1845 train_time:14291ms step_avg:35.91ms
step:399/1845 train_time:14326ms step_avg:35.90ms
step:400/1845 train_time:14365ms step_avg:35.91ms
step:401/1845 train_time:14400ms step_avg:35.91ms
step:402/1845 train_time:14440ms step_avg:35.92ms
step:403/1845 train_time:14475ms step_avg:35.92ms
step:404/1845 train_time:14514ms step_avg:35.93ms
step:405/1845 train_time:14550ms step_avg:35.93ms
step:406/1845 train_time:14590ms step_avg:35.94ms
step:407/1845 train_time:14625ms step_avg:35.93ms
step:408/1845 train_time:14665ms step_avg:35.94ms
step:409/1845 train_time:14702ms step_avg:35.95ms
step:410/1845 train_time:14740ms step_avg:35.95ms
step:411/1845 train_time:14776ms step_avg:35.95ms
step:412/1845 train_time:14815ms step_avg:35.96ms
step:413/1845 train_time:14850ms step_avg:35.96ms
step:414/1845 train_time:14890ms step_avg:35.97ms
step:415/1845 train_time:14926ms step_avg:35.97ms
step:416/1845 train_time:14964ms step_avg:35.97ms
step:417/1845 train_time:14999ms step_avg:35.97ms
step:418/1845 train_time:15037ms step_avg:35.97ms
step:419/1845 train_time:15071ms step_avg:35.97ms
step:420/1845 train_time:15110ms step_avg:35.98ms
step:421/1845 train_time:15144ms step_avg:35.97ms
step:422/1845 train_time:15181ms step_avg:35.97ms
step:423/1845 train_time:15216ms step_avg:35.97ms
step:424/1845 train_time:15254ms step_avg:35.98ms
step:425/1845 train_time:15288ms step_avg:35.97ms
step:426/1845 train_time:15326ms step_avg:35.98ms
step:427/1845 train_time:15362ms step_avg:35.98ms
step:428/1845 train_time:15401ms step_avg:35.98ms
step:429/1845 train_time:15436ms step_avg:35.98ms
step:430/1845 train_time:15476ms step_avg:35.99ms
step:431/1845 train_time:15510ms step_avg:35.99ms
step:432/1845 train_time:15548ms step_avg:35.99ms
step:433/1845 train_time:15583ms step_avg:35.99ms
step:434/1845 train_time:15621ms step_avg:35.99ms
step:435/1845 train_time:15655ms step_avg:35.99ms
step:436/1845 train_time:15693ms step_avg:35.99ms
step:437/1845 train_time:15728ms step_avg:35.99ms
step:438/1845 train_time:15766ms step_avg:36.00ms
step:439/1845 train_time:15801ms step_avg:35.99ms
step:440/1845 train_time:15839ms step_avg:36.00ms
step:441/1845 train_time:15873ms step_avg:35.99ms
step:442/1845 train_time:15911ms step_avg:36.00ms
step:443/1845 train_time:15945ms step_avg:35.99ms
step:444/1845 train_time:15983ms step_avg:36.00ms
step:445/1845 train_time:16017ms step_avg:35.99ms
step:446/1845 train_time:16055ms step_avg:36.00ms
step:447/1845 train_time:16091ms step_avg:36.00ms
step:448/1845 train_time:16130ms step_avg:36.00ms
step:449/1845 train_time:16164ms step_avg:36.00ms
step:450/1845 train_time:16203ms step_avg:36.01ms
step:451/1845 train_time:16238ms step_avg:36.00ms
step:452/1845 train_time:16276ms step_avg:36.01ms
step:453/1845 train_time:16311ms step_avg:36.01ms
step:454/1845 train_time:16348ms step_avg:36.01ms
step:455/1845 train_time:16383ms step_avg:36.01ms
step:456/1845 train_time:16420ms step_avg:36.01ms
step:457/1845 train_time:16455ms step_avg:36.01ms
step:458/1845 train_time:16492ms step_avg:36.01ms
step:459/1845 train_time:16527ms step_avg:36.01ms
step:460/1845 train_time:16564ms step_avg:36.01ms
step:461/1845 train_time:16599ms step_avg:36.01ms
step:462/1845 train_time:16637ms step_avg:36.01ms
step:463/1845 train_time:16671ms step_avg:36.01ms
step:464/1845 train_time:16710ms step_avg:36.01ms
step:465/1845 train_time:16745ms step_avg:36.01ms
step:466/1845 train_time:16783ms step_avg:36.01ms
step:467/1845 train_time:16817ms step_avg:36.01ms
step:468/1845 train_time:16854ms step_avg:36.01ms
step:469/1845 train_time:16889ms step_avg:36.01ms
step:470/1845 train_time:16928ms step_avg:36.02ms
step:471/1845 train_time:16962ms step_avg:36.01ms
step:472/1845 train_time:16999ms step_avg:36.02ms
step:473/1845 train_time:17035ms step_avg:36.01ms
step:474/1845 train_time:17074ms step_avg:36.02ms
step:475/1845 train_time:17108ms step_avg:36.02ms
step:476/1845 train_time:17147ms step_avg:36.02ms
step:477/1845 train_time:17183ms step_avg:36.02ms
step:478/1845 train_time:17222ms step_avg:36.03ms
step:479/1845 train_time:17257ms step_avg:36.03ms
step:480/1845 train_time:17296ms step_avg:36.03ms
step:481/1845 train_time:17331ms step_avg:36.03ms
step:482/1845 train_time:17368ms step_avg:36.03ms
step:483/1845 train_time:17402ms step_avg:36.03ms
step:484/1845 train_time:17440ms step_avg:36.03ms
step:485/1845 train_time:17475ms step_avg:36.03ms
step:486/1845 train_time:17513ms step_avg:36.04ms
step:487/1845 train_time:17547ms step_avg:36.03ms
step:488/1845 train_time:17585ms step_avg:36.04ms
step:489/1845 train_time:17620ms step_avg:36.03ms
step:490/1845 train_time:17658ms step_avg:36.04ms
step:491/1845 train_time:17692ms step_avg:36.03ms
step:492/1845 train_time:17730ms step_avg:36.04ms
step:493/1845 train_time:17764ms step_avg:36.03ms
step:494/1845 train_time:17801ms step_avg:36.04ms
step:495/1845 train_time:17836ms step_avg:36.03ms
step:496/1845 train_time:17876ms step_avg:36.04ms
step:497/1845 train_time:17910ms step_avg:36.04ms
step:498/1845 train_time:17948ms step_avg:36.04ms
step:499/1845 train_time:17984ms step_avg:36.04ms
step:500/1845 train_time:18022ms step_avg:36.04ms
step:500/1845 val_loss:4.2835 train_time:18024ms step_avg:36.05ms
step:501/1845 train_time:18051ms step_avg:36.03ms
step:502/1845 train_time:18079ms step_avg:36.01ms
step:503/1845 train_time:18104ms step_avg:35.99ms
step:504/1845 train_time:18132ms step_avg:35.98ms
step:505/1845 train_time:18162ms step_avg:35.96ms
step:506/1845 train_time:18198ms step_avg:35.96ms
step:507/1845 train_time:18232ms step_avg:35.96ms
step:508/1845 train_time:18269ms step_avg:35.96ms
step:509/1845 train_time:18304ms step_avg:35.96ms
step:510/1845 train_time:18342ms step_avg:35.96ms
step:511/1845 train_time:18375ms step_avg:35.96ms
step:512/1845 train_time:18413ms step_avg:35.96ms
step:513/1845 train_time:18449ms step_avg:35.96ms
step:514/1845 train_time:18487ms step_avg:35.97ms
step:515/1845 train_time:18522ms step_avg:35.97ms
step:516/1845 train_time:18561ms step_avg:35.97ms
step:517/1845 train_time:18598ms step_avg:35.97ms
step:518/1845 train_time:18637ms step_avg:35.98ms
step:519/1845 train_time:18671ms step_avg:35.97ms
step:520/1845 train_time:18707ms step_avg:35.98ms
step:521/1845 train_time:18740ms step_avg:35.97ms
step:522/1845 train_time:18778ms step_avg:35.97ms
step:523/1845 train_time:18811ms step_avg:35.97ms
step:524/1845 train_time:18847ms step_avg:35.97ms
step:525/1845 train_time:18880ms step_avg:35.96ms
step:526/1845 train_time:18916ms step_avg:35.96ms
step:527/1845 train_time:18949ms step_avg:35.96ms
step:528/1845 train_time:18987ms step_avg:35.96ms
step:529/1845 train_time:19020ms step_avg:35.96ms
step:530/1845 train_time:19060ms step_avg:35.96ms
step:531/1845 train_time:19095ms step_avg:35.96ms
step:532/1845 train_time:19133ms step_avg:35.97ms
step:533/1845 train_time:19169ms step_avg:35.96ms
step:534/1845 train_time:19209ms step_avg:35.97ms
step:535/1845 train_time:19244ms step_avg:35.97ms
step:536/1845 train_time:19282ms step_avg:35.97ms
step:537/1845 train_time:19317ms step_avg:35.97ms
step:538/1845 train_time:19356ms step_avg:35.98ms
step:539/1845 train_time:19391ms step_avg:35.98ms
step:540/1845 train_time:19429ms step_avg:35.98ms
step:541/1845 train_time:19463ms step_avg:35.98ms
step:542/1845 train_time:19501ms step_avg:35.98ms
step:543/1845 train_time:19538ms step_avg:35.98ms
step:544/1845 train_time:19578ms step_avg:35.99ms
step:545/1845 train_time:19613ms step_avg:35.99ms
step:546/1845 train_time:19652ms step_avg:35.99ms
step:547/1845 train_time:19687ms step_avg:35.99ms
step:548/1845 train_time:19725ms step_avg:35.99ms
step:549/1845 train_time:19760ms step_avg:35.99ms
step:550/1845 train_time:19797ms step_avg:36.00ms
step:551/1845 train_time:19832ms step_avg:35.99ms
step:552/1845 train_time:19870ms step_avg:36.00ms
step:553/1845 train_time:19905ms step_avg:35.99ms
step:554/1845 train_time:19942ms step_avg:36.00ms
step:555/1845 train_time:19976ms step_avg:35.99ms
step:556/1845 train_time:20013ms step_avg:35.99ms
step:557/1845 train_time:20047ms step_avg:35.99ms
step:558/1845 train_time:20084ms step_avg:35.99ms
step:559/1845 train_time:20118ms step_avg:35.99ms
step:560/1845 train_time:20156ms step_avg:35.99ms
step:561/1845 train_time:20190ms step_avg:35.99ms
step:562/1845 train_time:20227ms step_avg:35.99ms
step:563/1845 train_time:20261ms step_avg:35.99ms
step:564/1845 train_time:20298ms step_avg:35.99ms
step:565/1845 train_time:20332ms step_avg:35.99ms
step:566/1845 train_time:20369ms step_avg:35.99ms
step:567/1845 train_time:20403ms step_avg:35.98ms
step:568/1845 train_time:20440ms step_avg:35.99ms
step:569/1845 train_time:20476ms step_avg:35.99ms
step:570/1845 train_time:20514ms step_avg:35.99ms
step:571/1845 train_time:20549ms step_avg:35.99ms
step:572/1845 train_time:20588ms step_avg:35.99ms
step:573/1845 train_time:20622ms step_avg:35.99ms
step:574/1845 train_time:20660ms step_avg:35.99ms
step:575/1845 train_time:20695ms step_avg:35.99ms
step:576/1845 train_time:20733ms step_avg:35.99ms
step:577/1845 train_time:20767ms step_avg:35.99ms
step:578/1845 train_time:20805ms step_avg:35.99ms
step:579/1845 train_time:20839ms step_avg:35.99ms
step:580/1845 train_time:20876ms step_avg:35.99ms
step:581/1845 train_time:20910ms step_avg:35.99ms
step:582/1845 train_time:20947ms step_avg:35.99ms
step:583/1845 train_time:20981ms step_avg:35.99ms
step:584/1845 train_time:21019ms step_avg:35.99ms
step:585/1845 train_time:21052ms step_avg:35.99ms
step:586/1845 train_time:21090ms step_avg:35.99ms
step:587/1845 train_time:21125ms step_avg:35.99ms
step:588/1845 train_time:21162ms step_avg:35.99ms
step:589/1845 train_time:21196ms step_avg:35.99ms
step:590/1845 train_time:21233ms step_avg:35.99ms
step:591/1845 train_time:21266ms step_avg:35.98ms
step:592/1845 train_time:21304ms step_avg:35.99ms
step:593/1845 train_time:21339ms step_avg:35.98ms
step:594/1845 train_time:21376ms step_avg:35.99ms
step:595/1845 train_time:21410ms step_avg:35.98ms
step:596/1845 train_time:21447ms step_avg:35.99ms
step:597/1845 train_time:21481ms step_avg:35.98ms
step:598/1845 train_time:21518ms step_avg:35.98ms
step:599/1845 train_time:21552ms step_avg:35.98ms
step:600/1845 train_time:21590ms step_avg:35.98ms
step:601/1845 train_time:21626ms step_avg:35.98ms
step:602/1845 train_time:21664ms step_avg:35.99ms
step:603/1845 train_time:21700ms step_avg:35.99ms
step:604/1845 train_time:21739ms step_avg:35.99ms
step:605/1845 train_time:21788ms step_avg:36.01ms
step:606/1845 train_time:21848ms step_avg:36.05ms
step:607/1845 train_time:21910ms step_avg:36.09ms
step:608/1845 train_time:21969ms step_avg:36.13ms
step:609/1845 train_time:22032ms step_avg:36.18ms
step:610/1845 train_time:22092ms step_avg:36.22ms
step:611/1845 train_time:22154ms step_avg:36.26ms
step:612/1845 train_time:22215ms step_avg:36.30ms
step:613/1845 train_time:22279ms step_avg:36.34ms
step:614/1845 train_time:22339ms step_avg:36.38ms
step:615/1845 train_time:22401ms step_avg:36.42ms
step:616/1845 train_time:22463ms step_avg:36.47ms
step:617/1845 train_time:22526ms step_avg:36.51ms
step:618/1845 train_time:22587ms step_avg:36.55ms
step:619/1845 train_time:22649ms step_avg:36.59ms
step:620/1845 train_time:22710ms step_avg:36.63ms
step:621/1845 train_time:22771ms step_avg:36.67ms
step:622/1845 train_time:22831ms step_avg:36.71ms
step:623/1845 train_time:22893ms step_avg:36.75ms
step:624/1845 train_time:22954ms step_avg:36.79ms
step:625/1845 train_time:23017ms step_avg:36.83ms
step:626/1845 train_time:23077ms step_avg:36.86ms
step:627/1845 train_time:23139ms step_avg:36.90ms
step:628/1845 train_time:23199ms step_avg:36.94ms
step:629/1845 train_time:23262ms step_avg:36.98ms
step:630/1845 train_time:23323ms step_avg:37.02ms
step:631/1845 train_time:23385ms step_avg:37.06ms
step:632/1845 train_time:23446ms step_avg:37.10ms
step:633/1845 train_time:23509ms step_avg:37.14ms
step:634/1845 train_time:23569ms step_avg:37.18ms
step:635/1845 train_time:23631ms step_avg:37.21ms
step:636/1845 train_time:23692ms step_avg:37.25ms
step:637/1845 train_time:23753ms step_avg:37.29ms
step:638/1845 train_time:23813ms step_avg:37.32ms
step:639/1845 train_time:23876ms step_avg:37.36ms
step:640/1845 train_time:23936ms step_avg:37.40ms
step:641/1845 train_time:23998ms step_avg:37.44ms
step:642/1845 train_time:24059ms step_avg:37.48ms
step:643/1845 train_time:24121ms step_avg:37.51ms
step:644/1845 train_time:24182ms step_avg:37.55ms
step:645/1845 train_time:24244ms step_avg:37.59ms
step:646/1845 train_time:24305ms step_avg:37.62ms
step:647/1845 train_time:24366ms step_avg:37.66ms
step:648/1845 train_time:24428ms step_avg:37.70ms
step:649/1845 train_time:24489ms step_avg:37.73ms
step:650/1845 train_time:24551ms step_avg:37.77ms
step:651/1845 train_time:24613ms step_avg:37.81ms
step:652/1845 train_time:24673ms step_avg:37.84ms
step:653/1845 train_time:24736ms step_avg:37.88ms
step:654/1845 train_time:24796ms step_avg:37.91ms
step:655/1845 train_time:24859ms step_avg:37.95ms
step:656/1845 train_time:24919ms step_avg:37.99ms
step:657/1845 train_time:24981ms step_avg:38.02ms
step:658/1845 train_time:25042ms step_avg:38.06ms
step:659/1845 train_time:25104ms step_avg:38.09ms
step:660/1845 train_time:25164ms step_avg:38.13ms
step:661/1845 train_time:25227ms step_avg:38.16ms
step:662/1845 train_time:25288ms step_avg:38.20ms
step:663/1845 train_time:25349ms step_avg:38.23ms
step:664/1845 train_time:25410ms step_avg:38.27ms
step:665/1845 train_time:25472ms step_avg:38.30ms
step:666/1845 train_time:25532ms step_avg:38.34ms
step:667/1845 train_time:25593ms step_avg:38.37ms
step:668/1845 train_time:25655ms step_avg:38.41ms
step:669/1845 train_time:25717ms step_avg:38.44ms
step:670/1845 train_time:25778ms step_avg:38.47ms
step:671/1845 train_time:25839ms step_avg:38.51ms
step:672/1845 train_time:25900ms step_avg:38.54ms
step:673/1845 train_time:25962ms step_avg:38.58ms
step:674/1845 train_time:26023ms step_avg:38.61ms
step:675/1845 train_time:26084ms step_avg:38.64ms
step:676/1845 train_time:26146ms step_avg:38.68ms
step:677/1845 train_time:26207ms step_avg:38.71ms
step:678/1845 train_time:26268ms step_avg:38.74ms
step:679/1845 train_time:26330ms step_avg:38.78ms
step:680/1845 train_time:26391ms step_avg:38.81ms
step:681/1845 train_time:26452ms step_avg:38.84ms
step:682/1845 train_time:26513ms step_avg:38.88ms
step:683/1845 train_time:26575ms step_avg:38.91ms
step:684/1845 train_time:26636ms step_avg:38.94ms
step:685/1845 train_time:26698ms step_avg:38.98ms
step:686/1845 train_time:26760ms step_avg:39.01ms
step:687/1845 train_time:26821ms step_avg:39.04ms
step:688/1845 train_time:26881ms step_avg:39.07ms
step:689/1845 train_time:26942ms step_avg:39.10ms
step:690/1845 train_time:27004ms step_avg:39.14ms
step:691/1845 train_time:27065ms step_avg:39.17ms
step:692/1845 train_time:27127ms step_avg:39.20ms
step:693/1845 train_time:27188ms step_avg:39.23ms
step:694/1845 train_time:27250ms step_avg:39.27ms
step:695/1845 train_time:27311ms step_avg:39.30ms
step:696/1845 train_time:27372ms step_avg:39.33ms
step:697/1845 train_time:27434ms step_avg:39.36ms
step:698/1845 train_time:27495ms step_avg:39.39ms
step:699/1845 train_time:27556ms step_avg:39.42ms
step:700/1845 train_time:27617ms step_avg:39.45ms
step:701/1845 train_time:27679ms step_avg:39.49ms
step:702/1845 train_time:27740ms step_avg:39.52ms
step:703/1845 train_time:27801ms step_avg:39.55ms
step:704/1845 train_time:27862ms step_avg:39.58ms
step:705/1845 train_time:27924ms step_avg:39.61ms
step:706/1845 train_time:27985ms step_avg:39.64ms
step:707/1845 train_time:28047ms step_avg:39.67ms
step:708/1845 train_time:28108ms step_avg:39.70ms
step:709/1845 train_time:28169ms step_avg:39.73ms
step:710/1845 train_time:28230ms step_avg:39.76ms
step:711/1845 train_time:28292ms step_avg:39.79ms
step:712/1845 train_time:28352ms step_avg:39.82ms
step:713/1845 train_time:28413ms step_avg:39.85ms
step:714/1845 train_time:28474ms step_avg:39.88ms
step:715/1845 train_time:28536ms step_avg:39.91ms
step:716/1845 train_time:28597ms step_avg:39.94ms
step:717/1845 train_time:28659ms step_avg:39.97ms
step:718/1845 train_time:28719ms step_avg:40.00ms
step:719/1845 train_time:28781ms step_avg:40.03ms
step:720/1845 train_time:28843ms step_avg:40.06ms
step:721/1845 train_time:28904ms step_avg:40.09ms
step:722/1845 train_time:28966ms step_avg:40.12ms
step:723/1845 train_time:29027ms step_avg:40.15ms
step:724/1845 train_time:29089ms step_avg:40.18ms
step:725/1845 train_time:29151ms step_avg:40.21ms
step:726/1845 train_time:29211ms step_avg:40.24ms
step:727/1845 train_time:29273ms step_avg:40.27ms
step:728/1845 train_time:29334ms step_avg:40.29ms
step:729/1845 train_time:29396ms step_avg:40.32ms
step:730/1845 train_time:29457ms step_avg:40.35ms
step:731/1845 train_time:29519ms step_avg:40.38ms
step:732/1845 train_time:29580ms step_avg:40.41ms
step:733/1845 train_time:29641ms step_avg:40.44ms
step:734/1845 train_time:29702ms step_avg:40.47ms
step:735/1845 train_time:29764ms step_avg:40.49ms
step:736/1845 train_time:29825ms step_avg:40.52ms
step:737/1845 train_time:29887ms step_avg:40.55ms
step:738/1845 train_time:29948ms step_avg:40.58ms
step:739/1845 train_time:30009ms step_avg:40.61ms
step:740/1845 train_time:30070ms step_avg:40.63ms
step:741/1845 train_time:30131ms step_avg:40.66ms
step:742/1845 train_time:30192ms step_avg:40.69ms
step:743/1845 train_time:30254ms step_avg:40.72ms
step:744/1845 train_time:30314ms step_avg:40.74ms
step:745/1845 train_time:30376ms step_avg:40.77ms
step:746/1845 train_time:30437ms step_avg:40.80ms
step:747/1845 train_time:30499ms step_avg:40.83ms
step:748/1845 train_time:30559ms step_avg:40.85ms
step:749/1845 train_time:30622ms step_avg:40.88ms
step:750/1845 train_time:30683ms step_avg:40.91ms
step:750/1845 val_loss:4.0166 train_time:30743ms step_avg:40.99ms
step:751/1845 train_time:30772ms step_avg:40.97ms
step:752/1845 train_time:30806ms step_avg:40.96ms
step:753/1845 train_time:30870ms step_avg:41.00ms
step:754/1845 train_time:30932ms step_avg:41.02ms
step:755/1845 train_time:30996ms step_avg:41.05ms
step:756/1845 train_time:31056ms step_avg:41.08ms
step:757/1845 train_time:31119ms step_avg:41.11ms
step:758/1845 train_time:31179ms step_avg:41.13ms
step:759/1845 train_time:31241ms step_avg:41.16ms
step:760/1845 train_time:31301ms step_avg:41.19ms
step:761/1845 train_time:31362ms step_avg:41.21ms
step:762/1845 train_time:31422ms step_avg:41.24ms
step:763/1845 train_time:31484ms step_avg:41.26ms
step:764/1845 train_time:31545ms step_avg:41.29ms
step:765/1845 train_time:31606ms step_avg:41.32ms
step:766/1845 train_time:31668ms step_avg:41.34ms
step:767/1845 train_time:31732ms step_avg:41.37ms
step:768/1845 train_time:31795ms step_avg:41.40ms
step:769/1845 train_time:31858ms step_avg:41.43ms
step:770/1845 train_time:31919ms step_avg:41.45ms
step:771/1845 train_time:31981ms step_avg:41.48ms
step:772/1845 train_time:32042ms step_avg:41.50ms
step:773/1845 train_time:32105ms step_avg:41.53ms
step:774/1845 train_time:32165ms step_avg:41.56ms
step:775/1845 train_time:32227ms step_avg:41.58ms
step:776/1845 train_time:32287ms step_avg:41.61ms
step:777/1845 train_time:32349ms step_avg:41.63ms
step:778/1845 train_time:32410ms step_avg:41.66ms
step:779/1845 train_time:32472ms step_avg:41.68ms
step:780/1845 train_time:32532ms step_avg:41.71ms
step:781/1845 train_time:32595ms step_avg:41.74ms
step:782/1845 train_time:32656ms step_avg:41.76ms
step:783/1845 train_time:32718ms step_avg:41.79ms
step:784/1845 train_time:32779ms step_avg:41.81ms
step:785/1845 train_time:32842ms step_avg:41.84ms
step:786/1845 train_time:32903ms step_avg:41.86ms
step:787/1845 train_time:32966ms step_avg:41.89ms
step:788/1845 train_time:33026ms step_avg:41.91ms
step:789/1845 train_time:33088ms step_avg:41.94ms
step:790/1845 train_time:33149ms step_avg:41.96ms
step:791/1845 train_time:33211ms step_avg:41.99ms
step:792/1845 train_time:33272ms step_avg:42.01ms
step:793/1845 train_time:33335ms step_avg:42.04ms
step:794/1845 train_time:33396ms step_avg:42.06ms
step:795/1845 train_time:33457ms step_avg:42.08ms
step:796/1845 train_time:33519ms step_avg:42.11ms
step:797/1845 train_time:33580ms step_avg:42.13ms
step:798/1845 train_time:33642ms step_avg:42.16ms
step:799/1845 train_time:33703ms step_avg:42.18ms
step:800/1845 train_time:33764ms step_avg:42.21ms
step:801/1845 train_time:33826ms step_avg:42.23ms
step:802/1845 train_time:33887ms step_avg:42.25ms
step:803/1845 train_time:33950ms step_avg:42.28ms
step:804/1845 train_time:34011ms step_avg:42.30ms
step:805/1845 train_time:34073ms step_avg:42.33ms
step:806/1845 train_time:34135ms step_avg:42.35ms
step:807/1845 train_time:34197ms step_avg:42.38ms
step:808/1845 train_time:34258ms step_avg:42.40ms
step:809/1845 train_time:34319ms step_avg:42.42ms
step:810/1845 train_time:34380ms step_avg:42.44ms
step:811/1845 train_time:34441ms step_avg:42.47ms
step:812/1845 train_time:34502ms step_avg:42.49ms
step:813/1845 train_time:34563ms step_avg:42.51ms
step:814/1845 train_time:34624ms step_avg:42.54ms
step:815/1845 train_time:34686ms step_avg:42.56ms
step:816/1845 train_time:34747ms step_avg:42.58ms
step:817/1845 train_time:34809ms step_avg:42.61ms
step:818/1845 train_time:34870ms step_avg:42.63ms
step:819/1845 train_time:34933ms step_avg:42.65ms
step:820/1845 train_time:34994ms step_avg:42.68ms
step:821/1845 train_time:35056ms step_avg:42.70ms
step:822/1845 train_time:35118ms step_avg:42.72ms
step:823/1845 train_time:35179ms step_avg:42.75ms
step:824/1845 train_time:35240ms step_avg:42.77ms
step:825/1845 train_time:35302ms step_avg:42.79ms
step:826/1845 train_time:35362ms step_avg:42.81ms
step:827/1845 train_time:35424ms step_avg:42.83ms
step:828/1845 train_time:35485ms step_avg:42.86ms
step:829/1845 train_time:35547ms step_avg:42.88ms
step:830/1845 train_time:35608ms step_avg:42.90ms
step:831/1845 train_time:35669ms step_avg:42.92ms
step:832/1845 train_time:35730ms step_avg:42.94ms
step:833/1845 train_time:35792ms step_avg:42.97ms
step:834/1845 train_time:35853ms step_avg:42.99ms
step:835/1845 train_time:35915ms step_avg:43.01ms
step:836/1845 train_time:35977ms step_avg:43.03ms
step:837/1845 train_time:36039ms step_avg:43.06ms
step:838/1845 train_time:36101ms step_avg:43.08ms
step:839/1845 train_time:36162ms step_avg:43.10ms
step:840/1845 train_time:36223ms step_avg:43.12ms
step:841/1845 train_time:36286ms step_avg:43.15ms
step:842/1845 train_time:36347ms step_avg:43.17ms
step:843/1845 train_time:36409ms step_avg:43.19ms
step:844/1845 train_time:36469ms step_avg:43.21ms
step:845/1845 train_time:36533ms step_avg:43.23ms
step:846/1845 train_time:36594ms step_avg:43.25ms
step:847/1845 train_time:36656ms step_avg:43.28ms
step:848/1845 train_time:36718ms step_avg:43.30ms
step:849/1845 train_time:36779ms step_avg:43.32ms
step:850/1845 train_time:36840ms step_avg:43.34ms
step:851/1845 train_time:36901ms step_avg:43.36ms
step:852/1845 train_time:36962ms step_avg:43.38ms
step:853/1845 train_time:37025ms step_avg:43.41ms
step:854/1845 train_time:37086ms step_avg:43.43ms
step:855/1845 train_time:37148ms step_avg:43.45ms
step:856/1845 train_time:37209ms step_avg:43.47ms
step:857/1845 train_time:37271ms step_avg:43.49ms
step:858/1845 train_time:37332ms step_avg:43.51ms
step:859/1845 train_time:37395ms step_avg:43.53ms
step:860/1845 train_time:37456ms step_avg:43.55ms
step:861/1845 train_time:37519ms step_avg:43.58ms
step:862/1845 train_time:37580ms step_avg:43.60ms
step:863/1845 train_time:37641ms step_avg:43.62ms
step:864/1845 train_time:37702ms step_avg:43.64ms
step:865/1845 train_time:37764ms step_avg:43.66ms
step:866/1845 train_time:37824ms step_avg:43.68ms
step:867/1845 train_time:37887ms step_avg:43.70ms
step:868/1845 train_time:37947ms step_avg:43.72ms
step:869/1845 train_time:38009ms step_avg:43.74ms
step:870/1845 train_time:38071ms step_avg:43.76ms
step:871/1845 train_time:38133ms step_avg:43.78ms
step:872/1845 train_time:38194ms step_avg:43.80ms
step:873/1845 train_time:38257ms step_avg:43.82ms
step:874/1845 train_time:38318ms step_avg:43.84ms
step:875/1845 train_time:38381ms step_avg:43.86ms
step:876/1845 train_time:38441ms step_avg:43.88ms
step:877/1845 train_time:38503ms step_avg:43.90ms
step:878/1845 train_time:38563ms step_avg:43.92ms
step:879/1845 train_time:38625ms step_avg:43.94ms
step:880/1845 train_time:38686ms step_avg:43.96ms
step:881/1845 train_time:38748ms step_avg:43.98ms
step:882/1845 train_time:38809ms step_avg:44.00ms
step:883/1845 train_time:38872ms step_avg:44.02ms
step:884/1845 train_time:38933ms step_avg:44.04ms
step:885/1845 train_time:38995ms step_avg:44.06ms
step:886/1845 train_time:39057ms step_avg:44.08ms
step:887/1845 train_time:39119ms step_avg:44.10ms
step:888/1845 train_time:39180ms step_avg:44.12ms
step:889/1845 train_time:39242ms step_avg:44.14ms
step:890/1845 train_time:39302ms step_avg:44.16ms
step:891/1845 train_time:39365ms step_avg:44.18ms
step:892/1845 train_time:39425ms step_avg:44.20ms
step:893/1845 train_time:39487ms step_avg:44.22ms
step:894/1845 train_time:39548ms step_avg:44.24ms
step:895/1845 train_time:39611ms step_avg:44.26ms
step:896/1845 train_time:39672ms step_avg:44.28ms
step:897/1845 train_time:39734ms step_avg:44.30ms
step:898/1845 train_time:39796ms step_avg:44.32ms
step:899/1845 train_time:39857ms step_avg:44.33ms
step:900/1845 train_time:39918ms step_avg:44.35ms
step:901/1845 train_time:39980ms step_avg:44.37ms
step:902/1845 train_time:40042ms step_avg:44.39ms
step:903/1845 train_time:40103ms step_avg:44.41ms
step:904/1845 train_time:40164ms step_avg:44.43ms
step:905/1845 train_time:40226ms step_avg:44.45ms
step:906/1845 train_time:40287ms step_avg:44.47ms
step:907/1845 train_time:40350ms step_avg:44.49ms
step:908/1845 train_time:40411ms step_avg:44.51ms
step:909/1845 train_time:40473ms step_avg:44.52ms
step:910/1845 train_time:40534ms step_avg:44.54ms
step:911/1845 train_time:40597ms step_avg:44.56ms
step:912/1845 train_time:40657ms step_avg:44.58ms
step:913/1845 train_time:40719ms step_avg:44.60ms
step:914/1845 train_time:40780ms step_avg:44.62ms
step:915/1845 train_time:40842ms step_avg:44.64ms
step:916/1845 train_time:40903ms step_avg:44.65ms
step:917/1845 train_time:40964ms step_avg:44.67ms
step:918/1845 train_time:41025ms step_avg:44.69ms
step:919/1845 train_time:41087ms step_avg:44.71ms
step:920/1845 train_time:41147ms step_avg:44.73ms
step:921/1845 train_time:41209ms step_avg:44.74ms
step:922/1845 train_time:41270ms step_avg:44.76ms
step:923/1845 train_time:41333ms step_avg:44.78ms
step:924/1845 train_time:41394ms step_avg:44.80ms
step:925/1845 train_time:41457ms step_avg:44.82ms
step:926/1845 train_time:41519ms step_avg:44.84ms
step:927/1845 train_time:41581ms step_avg:44.86ms
step:928/1845 train_time:41642ms step_avg:44.87ms
step:929/1845 train_time:41703ms step_avg:44.89ms
step:930/1845 train_time:41764ms step_avg:44.91ms
step:931/1845 train_time:41826ms step_avg:44.93ms
step:932/1845 train_time:41887ms step_avg:44.94ms
step:933/1845 train_time:41949ms step_avg:44.96ms
step:934/1845 train_time:42010ms step_avg:44.98ms
step:935/1845 train_time:42072ms step_avg:45.00ms
step:936/1845 train_time:42133ms step_avg:45.01ms
step:937/1845 train_time:42196ms step_avg:45.03ms
step:938/1845 train_time:42257ms step_avg:45.05ms
step:939/1845 train_time:42318ms step_avg:45.07ms
step:940/1845 train_time:42379ms step_avg:45.08ms
step:941/1845 train_time:42441ms step_avg:45.10ms
step:942/1845 train_time:42502ms step_avg:45.12ms
step:943/1845 train_time:42564ms step_avg:45.14ms
step:944/1845 train_time:42624ms step_avg:45.15ms
step:945/1845 train_time:42686ms step_avg:45.17ms
step:946/1845 train_time:42747ms step_avg:45.19ms
step:947/1845 train_time:42809ms step_avg:45.21ms
step:948/1845 train_time:42871ms step_avg:45.22ms
step:949/1845 train_time:42933ms step_avg:45.24ms
step:950/1845 train_time:42994ms step_avg:45.26ms
step:951/1845 train_time:43056ms step_avg:45.27ms
step:952/1845 train_time:43117ms step_avg:45.29ms
step:953/1845 train_time:43178ms step_avg:45.31ms
step:954/1845 train_time:43240ms step_avg:45.32ms
step:955/1845 train_time:43301ms step_avg:45.34ms
step:956/1845 train_time:43362ms step_avg:45.36ms
step:957/1845 train_time:43424ms step_avg:45.37ms
step:958/1845 train_time:43485ms step_avg:45.39ms
step:959/1845 train_time:43546ms step_avg:45.41ms
step:960/1845 train_time:43607ms step_avg:45.42ms
step:961/1845 train_time:43669ms step_avg:45.44ms
step:962/1845 train_time:43730ms step_avg:45.46ms
step:963/1845 train_time:43792ms step_avg:45.47ms
step:964/1845 train_time:43853ms step_avg:45.49ms
step:965/1845 train_time:43915ms step_avg:45.51ms
step:966/1845 train_time:43976ms step_avg:45.52ms
step:967/1845 train_time:44038ms step_avg:45.54ms
step:968/1845 train_time:44100ms step_avg:45.56ms
step:969/1845 train_time:44161ms step_avg:45.57ms
step:970/1845 train_time:44222ms step_avg:45.59ms
step:971/1845 train_time:44283ms step_avg:45.61ms
step:972/1845 train_time:44344ms step_avg:45.62ms
step:973/1845 train_time:44406ms step_avg:45.64ms
step:974/1845 train_time:44466ms step_avg:45.65ms
step:975/1845 train_time:44529ms step_avg:45.67ms
step:976/1845 train_time:44589ms step_avg:45.69ms
step:977/1845 train_time:44652ms step_avg:45.70ms
step:978/1845 train_time:44713ms step_avg:45.72ms
step:979/1845 train_time:44774ms step_avg:45.73ms
step:980/1845 train_time:44836ms step_avg:45.75ms
step:981/1845 train_time:44898ms step_avg:45.77ms
step:982/1845 train_time:44959ms step_avg:45.78ms
step:983/1845 train_time:45020ms step_avg:45.80ms
step:984/1845 train_time:45082ms step_avg:45.81ms
step:985/1845 train_time:45143ms step_avg:45.83ms
step:986/1845 train_time:45205ms step_avg:45.85ms
step:987/1845 train_time:45267ms step_avg:45.86ms
step:988/1845 train_time:45327ms step_avg:45.88ms
step:989/1845 train_time:45389ms step_avg:45.89ms
step:990/1845 train_time:45450ms step_avg:45.91ms
step:991/1845 train_time:45512ms step_avg:45.93ms
step:992/1845 train_time:45573ms step_avg:45.94ms
step:993/1845 train_time:45636ms step_avg:45.96ms
step:994/1845 train_time:45698ms step_avg:45.97ms
step:995/1845 train_time:45759ms step_avg:45.99ms
step:996/1845 train_time:45820ms step_avg:46.00ms
step:997/1845 train_time:45882ms step_avg:46.02ms
step:998/1845 train_time:45942ms step_avg:46.03ms
step:999/1845 train_time:46005ms step_avg:46.05ms
step:1000/1845 train_time:46066ms step_avg:46.07ms
step:1000/1845 val_loss:3.7775 train_time:46126ms step_avg:46.13ms
step:1001/1845 train_time:46154ms step_avg:46.11ms
step:1002/1845 train_time:46187ms step_avg:46.10ms
step:1003/1845 train_time:46252ms step_avg:46.11ms
step:1004/1845 train_time:46314ms step_avg:46.13ms
step:1005/1845 train_time:46375ms step_avg:46.14ms
step:1006/1845 train_time:46436ms step_avg:46.16ms
step:1007/1845 train_time:46497ms step_avg:46.17ms
step:1008/1845 train_time:46557ms step_avg:46.19ms
step:1009/1845 train_time:46618ms step_avg:46.20ms
step:1010/1845 train_time:46679ms step_avg:46.22ms
step:1011/1845 train_time:46740ms step_avg:46.23ms
step:1012/1845 train_time:46800ms step_avg:46.25ms
step:1013/1845 train_time:46862ms step_avg:46.26ms
step:1014/1845 train_time:46923ms step_avg:46.27ms
step:1015/1845 train_time:46986ms step_avg:46.29ms
step:1016/1845 train_time:47049ms step_avg:46.31ms
step:1017/1845 train_time:47112ms step_avg:46.32ms
step:1018/1845 train_time:47173ms step_avg:46.34ms
step:1019/1845 train_time:47237ms step_avg:46.36ms
step:1020/1845 train_time:47297ms step_avg:46.37ms
step:1021/1845 train_time:47362ms step_avg:46.39ms
step:1022/1845 train_time:47423ms step_avg:46.40ms
step:1023/1845 train_time:47484ms step_avg:46.42ms
step:1024/1845 train_time:47545ms step_avg:46.43ms
step:1025/1845 train_time:47607ms step_avg:46.45ms
step:1026/1845 train_time:47668ms step_avg:46.46ms
step:1027/1845 train_time:47729ms step_avg:46.47ms
step:1028/1845 train_time:47789ms step_avg:46.49ms
step:1029/1845 train_time:47850ms step_avg:46.50ms
step:1030/1845 train_time:47911ms step_avg:46.52ms
step:1031/1845 train_time:47973ms step_avg:46.53ms
step:1032/1845 train_time:48034ms step_avg:46.54ms
step:1033/1845 train_time:48096ms step_avg:46.56ms
step:1034/1845 train_time:48157ms step_avg:46.57ms
step:1035/1845 train_time:48220ms step_avg:46.59ms
step:1036/1845 train_time:48282ms step_avg:46.60ms
step:1037/1845 train_time:48344ms step_avg:46.62ms
step:1038/1845 train_time:48406ms step_avg:46.63ms
step:1039/1845 train_time:48467ms step_avg:46.65ms
step:1040/1845 train_time:48529ms step_avg:46.66ms
step:1041/1845 train_time:48590ms step_avg:46.68ms
step:1042/1845 train_time:48651ms step_avg:46.69ms
step:1043/1845 train_time:48712ms step_avg:46.70ms
step:1044/1845 train_time:48772ms step_avg:46.72ms
step:1045/1845 train_time:48834ms step_avg:46.73ms
step:1046/1845 train_time:48894ms step_avg:46.74ms
step:1047/1845 train_time:48956ms step_avg:46.76ms
step:1048/1845 train_time:49017ms step_avg:46.77ms
step:1049/1845 train_time:49079ms step_avg:46.79ms
step:1050/1845 train_time:49140ms step_avg:46.80ms
step:1051/1845 train_time:49202ms step_avg:46.81ms
step:1052/1845 train_time:49263ms step_avg:46.83ms
step:1053/1845 train_time:49326ms step_avg:46.84ms
step:1054/1845 train_time:49387ms step_avg:46.86ms
step:1055/1845 train_time:49449ms step_avg:46.87ms
step:1056/1845 train_time:49510ms step_avg:46.88ms
step:1057/1845 train_time:49571ms step_avg:46.90ms
step:1058/1845 train_time:49633ms step_avg:46.91ms
step:1059/1845 train_time:49694ms step_avg:46.93ms
step:1060/1845 train_time:49755ms step_avg:46.94ms
step:1061/1845 train_time:49816ms step_avg:46.95ms
step:1062/1845 train_time:49877ms step_avg:46.96ms
step:1063/1845 train_time:49939ms step_avg:46.98ms
step:1064/1845 train_time:49999ms step_avg:46.99ms
step:1065/1845 train_time:50061ms step_avg:47.01ms
step:1066/1845 train_time:50122ms step_avg:47.02ms
step:1067/1845 train_time:50184ms step_avg:47.03ms
step:1068/1845 train_time:50245ms step_avg:47.05ms
step:1069/1845 train_time:50307ms step_avg:47.06ms
step:1070/1845 train_time:50369ms step_avg:47.07ms
step:1071/1845 train_time:50431ms step_avg:47.09ms
step:1072/1845 train_time:50492ms step_avg:47.10ms
step:1073/1845 train_time:50554ms step_avg:47.11ms
step:1074/1845 train_time:50613ms step_avg:47.13ms
step:1075/1845 train_time:50674ms step_avg:47.14ms
step:1076/1845 train_time:50734ms step_avg:47.15ms
step:1077/1845 train_time:50796ms step_avg:47.16ms
step:1078/1845 train_time:50857ms step_avg:47.18ms
step:1079/1845 train_time:50920ms step_avg:47.19ms
step:1080/1845 train_time:50981ms step_avg:47.20ms
step:1081/1845 train_time:51044ms step_avg:47.22ms
step:1082/1845 train_time:51105ms step_avg:47.23ms
step:1083/1845 train_time:51167ms step_avg:47.25ms
step:1084/1845 train_time:51229ms step_avg:47.26ms
step:1085/1845 train_time:51291ms step_avg:47.27ms
step:1086/1845 train_time:51352ms step_avg:47.29ms
step:1087/1845 train_time:51413ms step_avg:47.30ms
step:1088/1845 train_time:51474ms step_avg:47.31ms
step:1089/1845 train_time:51536ms step_avg:47.32ms
step:1090/1845 train_time:51597ms step_avg:47.34ms
step:1091/1845 train_time:51658ms step_avg:47.35ms
step:1092/1845 train_time:51719ms step_avg:47.36ms
step:1093/1845 train_time:51781ms step_avg:47.37ms
step:1094/1845 train_time:51842ms step_avg:47.39ms
step:1095/1845 train_time:51903ms step_avg:47.40ms
step:1096/1845 train_time:51965ms step_avg:47.41ms
step:1097/1845 train_time:52027ms step_avg:47.43ms
step:1098/1845 train_time:52088ms step_avg:47.44ms
step:1099/1845 train_time:52149ms step_avg:47.45ms
step:1100/1845 train_time:52210ms step_avg:47.46ms
step:1101/1845 train_time:52272ms step_avg:47.48ms
step:1102/1845 train_time:52333ms step_avg:47.49ms
step:1103/1845 train_time:52395ms step_avg:47.50ms
step:1104/1845 train_time:52457ms step_avg:47.52ms
step:1105/1845 train_time:52518ms step_avg:47.53ms
step:1106/1845 train_time:52579ms step_avg:47.54ms
step:1107/1845 train_time:52640ms step_avg:47.55ms
step:1108/1845 train_time:52701ms step_avg:47.56ms
step:1109/1845 train_time:52763ms step_avg:47.58ms
step:1110/1845 train_time:52824ms step_avg:47.59ms
step:1111/1845 train_time:52885ms step_avg:47.60ms
step:1112/1845 train_time:52946ms step_avg:47.61ms
step:1113/1845 train_time:53008ms step_avg:47.63ms
step:1114/1845 train_time:53069ms step_avg:47.64ms
step:1115/1845 train_time:53131ms step_avg:47.65ms
step:1116/1845 train_time:53192ms step_avg:47.66ms
step:1117/1845 train_time:53253ms step_avg:47.68ms
step:1118/1845 train_time:53314ms step_avg:47.69ms
step:1119/1845 train_time:53376ms step_avg:47.70ms
step:1120/1845 train_time:53437ms step_avg:47.71ms
step:1121/1845 train_time:53499ms step_avg:47.72ms
step:1122/1845 train_time:53559ms step_avg:47.74ms
step:1123/1845 train_time:53621ms step_avg:47.75ms
step:1124/1845 train_time:53682ms step_avg:47.76ms
step:1125/1845 train_time:53743ms step_avg:47.77ms
step:1126/1845 train_time:53805ms step_avg:47.78ms
step:1127/1845 train_time:53866ms step_avg:47.80ms
step:1128/1845 train_time:53927ms step_avg:47.81ms
step:1129/1845 train_time:53989ms step_avg:47.82ms
step:1130/1845 train_time:54050ms step_avg:47.83ms
step:1131/1845 train_time:54111ms step_avg:47.84ms
step:1132/1845 train_time:54172ms step_avg:47.86ms
step:1133/1845 train_time:54233ms step_avg:47.87ms
step:1134/1845 train_time:54295ms step_avg:47.88ms
step:1135/1845 train_time:54356ms step_avg:47.89ms
step:1136/1845 train_time:54416ms step_avg:47.90ms
step:1137/1845 train_time:54480ms step_avg:47.92ms
step:1138/1845 train_time:54540ms step_avg:47.93ms
step:1139/1845 train_time:54602ms step_avg:47.94ms
step:1140/1845 train_time:54663ms step_avg:47.95ms
step:1141/1845 train_time:54725ms step_avg:47.96ms
step:1142/1845 train_time:54787ms step_avg:47.97ms
step:1143/1845 train_time:54848ms step_avg:47.99ms
step:1144/1845 train_time:54909ms step_avg:48.00ms
step:1145/1845 train_time:54971ms step_avg:48.01ms
step:1146/1845 train_time:55032ms step_avg:48.02ms
step:1147/1845 train_time:55093ms step_avg:48.03ms
step:1148/1845 train_time:55154ms step_avg:48.04ms
step:1149/1845 train_time:55216ms step_avg:48.06ms
step:1150/1845 train_time:55277ms step_avg:48.07ms
step:1151/1845 train_time:55339ms step_avg:48.08ms
step:1152/1845 train_time:55399ms step_avg:48.09ms
step:1153/1845 train_time:55462ms step_avg:48.10ms
step:1154/1845 train_time:55522ms step_avg:48.11ms
step:1155/1845 train_time:55584ms step_avg:48.12ms
step:1156/1845 train_time:55646ms step_avg:48.14ms
step:1157/1845 train_time:55707ms step_avg:48.15ms
step:1158/1845 train_time:55769ms step_avg:48.16ms
step:1159/1845 train_time:55831ms step_avg:48.17ms
step:1160/1845 train_time:55892ms step_avg:48.18ms
step:1161/1845 train_time:55953ms step_avg:48.19ms
step:1162/1845 train_time:56013ms step_avg:48.20ms
step:1163/1845 train_time:56075ms step_avg:48.22ms
step:1164/1845 train_time:56136ms step_avg:48.23ms
step:1165/1845 train_time:56198ms step_avg:48.24ms
step:1166/1845 train_time:56259ms step_avg:48.25ms
step:1167/1845 train_time:56320ms step_avg:48.26ms
step:1168/1845 train_time:56381ms step_avg:48.27ms
step:1169/1845 train_time:56443ms step_avg:48.28ms
step:1170/1845 train_time:56504ms step_avg:48.29ms
step:1171/1845 train_time:56566ms step_avg:48.31ms
step:1172/1845 train_time:56627ms step_avg:48.32ms
step:1173/1845 train_time:56689ms step_avg:48.33ms
step:1174/1845 train_time:56750ms step_avg:48.34ms
step:1175/1845 train_time:56811ms step_avg:48.35ms
step:1176/1845 train_time:56873ms step_avg:48.36ms
step:1177/1845 train_time:56934ms step_avg:48.37ms
step:1178/1845 train_time:56995ms step_avg:48.38ms
step:1179/1845 train_time:57057ms step_avg:48.39ms
step:1180/1845 train_time:57117ms step_avg:48.40ms
step:1181/1845 train_time:57179ms step_avg:48.42ms
step:1182/1845 train_time:57240ms step_avg:48.43ms
step:1183/1845 train_time:57302ms step_avg:48.44ms
step:1184/1845 train_time:57363ms step_avg:48.45ms
step:1185/1845 train_time:57425ms step_avg:48.46ms
step:1186/1845 train_time:57486ms step_avg:48.47ms
step:1187/1845 train_time:57548ms step_avg:48.48ms
step:1188/1845 train_time:57609ms step_avg:48.49ms
step:1189/1845 train_time:57670ms step_avg:48.50ms
step:1190/1845 train_time:57731ms step_avg:48.51ms
step:1191/1845 train_time:57793ms step_avg:48.52ms
step:1192/1845 train_time:57854ms step_avg:48.54ms
step:1193/1845 train_time:57916ms step_avg:48.55ms
step:1194/1845 train_time:57976ms step_avg:48.56ms
step:1195/1845 train_time:58039ms step_avg:48.57ms
step:1196/1845 train_time:58100ms step_avg:48.58ms
step:1197/1845 train_time:58161ms step_avg:48.59ms
step:1198/1845 train_time:58222ms step_avg:48.60ms
step:1199/1845 train_time:58284ms step_avg:48.61ms
step:1200/1845 train_time:58345ms step_avg:48.62ms
step:1201/1845 train_time:58406ms step_avg:48.63ms
step:1202/1845 train_time:58468ms step_avg:48.64ms
step:1203/1845 train_time:58529ms step_avg:48.65ms
step:1204/1845 train_time:58590ms step_avg:48.66ms
step:1205/1845 train_time:58653ms step_avg:48.67ms
step:1206/1845 train_time:58741ms step_avg:48.71ms
step:1207/1845 train_time:58830ms step_avg:48.74ms
step:1208/1845 train_time:58918ms step_avg:48.77ms
step:1209/1845 train_time:59006ms step_avg:48.81ms
step:1210/1845 train_time:59094ms step_avg:48.84ms
step:1211/1845 train_time:59182ms step_avg:48.87ms
step:1212/1845 train_time:59269ms step_avg:48.90ms
step:1213/1845 train_time:59359ms step_avg:48.94ms
step:1214/1845 train_time:59445ms step_avg:48.97ms
step:1215/1845 train_time:59534ms step_avg:49.00ms
step:1216/1845 train_time:59622ms step_avg:49.03ms
step:1217/1845 train_time:59709ms step_avg:49.06ms
step:1218/1845 train_time:59796ms step_avg:49.09ms
step:1219/1845 train_time:59885ms step_avg:49.13ms
step:1220/1845 train_time:59973ms step_avg:49.16ms
step:1221/1845 train_time:60061ms step_avg:49.19ms
step:1222/1845 train_time:60148ms step_avg:49.22ms
step:1223/1845 train_time:60235ms step_avg:49.25ms
step:1224/1845 train_time:60323ms step_avg:49.28ms
step:1225/1845 train_time:60412ms step_avg:49.32ms
step:1226/1845 train_time:60499ms step_avg:49.35ms
step:1227/1845 train_time:60587ms step_avg:49.38ms
step:1228/1845 train_time:60674ms step_avg:49.41ms
step:1229/1845 train_time:60763ms step_avg:49.44ms
step:1230/1845 train_time:60850ms step_avg:49.47ms
step:1231/1845 train_time:60938ms step_avg:49.50ms
step:1232/1845 train_time:61026ms step_avg:49.53ms
step:1233/1845 train_time:61114ms step_avg:49.57ms
step:1234/1845 train_time:61204ms step_avg:49.60ms
step:1235/1845 train_time:61291ms step_avg:49.63ms
step:1236/1845 train_time:61378ms step_avg:49.66ms
step:1237/1845 train_time:61467ms step_avg:49.69ms
step:1238/1845 train_time:61555ms step_avg:49.72ms
step:1239/1845 train_time:61643ms step_avg:49.75ms
step:1240/1845 train_time:61730ms step_avg:49.78ms
step:1241/1845 train_time:61820ms step_avg:49.81ms
step:1242/1845 train_time:61906ms step_avg:49.84ms
step:1243/1845 train_time:61995ms step_avg:49.88ms
step:1244/1845 train_time:62083ms step_avg:49.91ms
step:1245/1845 train_time:62170ms step_avg:49.94ms
step:1246/1845 train_time:62257ms step_avg:49.97ms
step:1247/1845 train_time:62345ms step_avg:50.00ms
step:1248/1845 train_time:62433ms step_avg:50.03ms
step:1249/1845 train_time:62522ms step_avg:50.06ms
step:1250/1845 train_time:62608ms step_avg:50.09ms
step:1250/1845 val_loss:3.5360 train_time:62696ms step_avg:50.16ms
step:1251/1845 train_time:62724ms step_avg:50.14ms
step:1252/1845 train_time:62783ms step_avg:50.15ms
step:1253/1845 train_time:62875ms step_avg:50.18ms
step:1254/1845 train_time:62963ms step_avg:50.21ms
step:1255/1845 train_time:63052ms step_avg:50.24ms
step:1256/1845 train_time:63139ms step_avg:50.27ms
step:1257/1845 train_time:63225ms step_avg:50.30ms
step:1258/1845 train_time:63312ms step_avg:50.33ms
step:1259/1845 train_time:63400ms step_avg:50.36ms
step:1260/1845 train_time:63487ms step_avg:50.39ms
step:1261/1845 train_time:63575ms step_avg:50.42ms
step:1262/1845 train_time:63667ms step_avg:50.45ms
step:1263/1845 train_time:63755ms step_avg:50.48ms
step:1264/1845 train_time:63843ms step_avg:50.51ms
step:1265/1845 train_time:63933ms step_avg:50.54ms
step:1266/1845 train_time:64019ms step_avg:50.57ms
step:1267/1845 train_time:64107ms step_avg:50.60ms
step:1268/1845 train_time:64194ms step_avg:50.63ms
step:1269/1845 train_time:64281ms step_avg:50.66ms
step:1270/1845 train_time:64368ms step_avg:50.68ms
step:1271/1845 train_time:64456ms step_avg:50.71ms
step:1272/1845 train_time:64543ms step_avg:50.74ms
step:1273/1845 train_time:64631ms step_avg:50.77ms
step:1274/1845 train_time:64721ms step_avg:50.80ms
step:1275/1845 train_time:64809ms step_avg:50.83ms
step:1276/1845 train_time:64897ms step_avg:50.86ms
step:1277/1845 train_time:64986ms step_avg:50.89ms
step:1278/1845 train_time:65073ms step_avg:50.92ms
step:1279/1845 train_time:65163ms step_avg:50.95ms
step:1280/1845 train_time:65250ms step_avg:50.98ms
step:1281/1845 train_time:65338ms step_avg:51.01ms
step:1282/1845 train_time:65425ms step_avg:51.03ms
step:1283/1845 train_time:65513ms step_avg:51.06ms
step:1284/1845 train_time:65599ms step_avg:51.09ms
step:1285/1845 train_time:65746ms step_avg:51.16ms
step:1286/1845 train_time:65784ms step_avg:51.15ms
step:1287/1845 train_time:65876ms step_avg:51.19ms
step:1288/1845 train_time:65960ms step_avg:51.21ms
step:1289/1845 train_time:66040ms step_avg:51.23ms
step:1290/1845 train_time:66131ms step_avg:51.26ms
step:1291/1845 train_time:66216ms step_avg:51.29ms
step:1292/1845 train_time:66303ms step_avg:51.32ms
step:1293/1845 train_time:66390ms step_avg:51.35ms
step:1294/1845 train_time:66480ms step_avg:51.38ms
step:1295/1845 train_time:66568ms step_avg:51.40ms
step:1296/1845 train_time:66657ms step_avg:51.43ms
step:1297/1845 train_time:66743ms step_avg:51.46ms
step:1298/1845 train_time:66831ms step_avg:51.49ms
step:1299/1845 train_time:66920ms step_avg:51.52ms
step:1300/1845 train_time:67007ms step_avg:51.54ms
step:1301/1845 train_time:67096ms step_avg:51.57ms
step:1302/1845 train_time:67183ms step_avg:51.60ms
step:1303/1845 train_time:67271ms step_avg:51.63ms
step:1304/1845 train_time:67359ms step_avg:51.66ms
step:1305/1845 train_time:67446ms step_avg:51.68ms
step:1306/1845 train_time:67534ms step_avg:51.71ms
step:1307/1845 train_time:67623ms step_avg:51.74ms
step:1308/1845 train_time:67709ms step_avg:51.77ms
step:1309/1845 train_time:67798ms step_avg:51.79ms
step:1310/1845 train_time:67885ms step_avg:51.82ms
step:1311/1845 train_time:67974ms step_avg:51.85ms
step:1312/1845 train_time:68062ms step_avg:51.88ms
step:1313/1845 train_time:68149ms step_avg:51.90ms
step:1314/1845 train_time:68236ms step_avg:51.93ms
step:1315/1845 train_time:68323ms step_avg:51.96ms
step:1316/1845 train_time:68411ms step_avg:51.98ms
step:1317/1845 train_time:68499ms step_avg:52.01ms
step:1318/1845 train_time:68586ms step_avg:52.04ms
step:1319/1845 train_time:68673ms step_avg:52.06ms
step:1320/1845 train_time:68762ms step_avg:52.09ms
step:1321/1845 train_time:68849ms step_avg:52.12ms
step:1322/1845 train_time:68937ms step_avg:52.15ms
step:1323/1845 train_time:69026ms step_avg:52.17ms
step:1324/1845 train_time:69113ms step_avg:52.20ms
step:1325/1845 train_time:69201ms step_avg:52.23ms
step:1326/1845 train_time:69288ms step_avg:52.25ms
step:1327/1845 train_time:69377ms step_avg:52.28ms
step:1328/1845 train_time:69465ms step_avg:52.31ms
step:1329/1845 train_time:69552ms step_avg:52.33ms
step:1330/1845 train_time:69639ms step_avg:52.36ms
step:1331/1845 train_time:69727ms step_avg:52.39ms
step:1332/1845 train_time:69816ms step_avg:52.41ms
step:1333/1845 train_time:69903ms step_avg:52.44ms
step:1334/1845 train_time:69992ms step_avg:52.47ms
step:1335/1845 train_time:70079ms step_avg:52.49ms
step:1336/1845 train_time:70167ms step_avg:52.52ms
step:1337/1845 train_time:70254ms step_avg:52.55ms
step:1338/1845 train_time:70342ms step_avg:52.57ms
step:1339/1845 train_time:70429ms step_avg:52.60ms
step:1340/1845 train_time:70518ms step_avg:52.63ms
step:1341/1845 train_time:70605ms step_avg:52.65ms
step:1342/1845 train_time:70691ms step_avg:52.68ms
step:1343/1845 train_time:70781ms step_avg:52.70ms
step:1344/1845 train_time:70868ms step_avg:52.73ms
step:1345/1845 train_time:70956ms step_avg:52.76ms
step:1346/1845 train_time:71044ms step_avg:52.78ms
step:1347/1845 train_time:71131ms step_avg:52.81ms
step:1348/1845 train_time:71220ms step_avg:52.83ms
step:1349/1845 train_time:71306ms step_avg:52.86ms
step:1350/1845 train_time:71394ms step_avg:52.88ms
step:1351/1845 train_time:71482ms step_avg:52.91ms
step:1352/1845 train_time:71570ms step_avg:52.94ms
step:1353/1845 train_time:71658ms step_avg:52.96ms
step:1354/1845 train_time:71745ms step_avg:52.99ms
step:1355/1845 train_time:71833ms step_avg:53.01ms
step:1356/1845 train_time:71922ms step_avg:53.04ms
step:1357/1845 train_time:72008ms step_avg:53.06ms
step:1358/1845 train_time:72096ms step_avg:53.09ms
step:1359/1845 train_time:72183ms step_avg:53.12ms
step:1360/1845 train_time:72271ms step_avg:53.14ms
step:1361/1845 train_time:72360ms step_avg:53.17ms
step:1362/1845 train_time:72447ms step_avg:53.19ms
step:1363/1845 train_time:72534ms step_avg:53.22ms
step:1364/1845 train_time:72622ms step_avg:53.24ms
step:1365/1845 train_time:72710ms step_avg:53.27ms
step:1366/1845 train_time:72797ms step_avg:53.29ms
step:1367/1845 train_time:72886ms step_avg:53.32ms
step:1368/1845 train_time:72974ms step_avg:53.34ms
step:1369/1845 train_time:73062ms step_avg:53.37ms
step:1370/1845 train_time:73150ms step_avg:53.39ms
step:1371/1845 train_time:73237ms step_avg:53.42ms
step:1372/1845 train_time:73326ms step_avg:53.44ms
step:1373/1845 train_time:73413ms step_avg:53.47ms
step:1374/1845 train_time:73500ms step_avg:53.49ms
step:1375/1845 train_time:73588ms step_avg:53.52ms
step:1376/1845 train_time:73675ms step_avg:53.54ms
step:1377/1845 train_time:73764ms step_avg:53.57ms
step:1378/1845 train_time:73852ms step_avg:53.59ms
step:1379/1845 train_time:73940ms step_avg:53.62ms
step:1380/1845 train_time:74029ms step_avg:53.64ms
step:1381/1845 train_time:74115ms step_avg:53.67ms
step:1382/1845 train_time:74203ms step_avg:53.69ms
step:1383/1845 train_time:74292ms step_avg:53.72ms
step:1384/1845 train_time:74379ms step_avg:53.74ms
step:1385/1845 train_time:74467ms step_avg:53.77ms
step:1386/1845 train_time:74554ms step_avg:53.79ms
step:1387/1845 train_time:74641ms step_avg:53.81ms
step:1388/1845 train_time:74729ms step_avg:53.84ms
step:1389/1845 train_time:74817ms step_avg:53.86ms
step:1390/1845 train_time:74905ms step_avg:53.89ms
step:1391/1845 train_time:74993ms step_avg:53.91ms
step:1392/1845 train_time:75081ms step_avg:53.94ms
step:1393/1845 train_time:75168ms step_avg:53.96ms
step:1394/1845 train_time:75256ms step_avg:53.99ms
step:1395/1845 train_time:75344ms step_avg:54.01ms
step:1396/1845 train_time:75431ms step_avg:54.03ms
step:1397/1845 train_time:75518ms step_avg:54.06ms
step:1398/1845 train_time:75605ms step_avg:54.08ms
step:1399/1845 train_time:75694ms step_avg:54.11ms
step:1400/1845 train_time:75781ms step_avg:54.13ms
step:1401/1845 train_time:75868ms step_avg:54.15ms
step:1402/1845 train_time:75956ms step_avg:54.18ms
step:1403/1845 train_time:76044ms step_avg:54.20ms
step:1404/1845 train_time:76131ms step_avg:54.22ms
step:1405/1845 train_time:76219ms step_avg:54.25ms
step:1406/1845 train_time:76306ms step_avg:54.27ms
step:1407/1845 train_time:76394ms step_avg:54.30ms
step:1408/1845 train_time:76482ms step_avg:54.32ms
step:1409/1845 train_time:76569ms step_avg:54.34ms
step:1410/1845 train_time:76657ms step_avg:54.37ms
step:1411/1845 train_time:76746ms step_avg:54.39ms
step:1412/1845 train_time:76833ms step_avg:54.41ms
step:1413/1845 train_time:76921ms step_avg:54.44ms
step:1414/1845 train_time:77009ms step_avg:54.46ms
step:1415/1845 train_time:77097ms step_avg:54.49ms
step:1416/1845 train_time:77184ms step_avg:54.51ms
step:1417/1845 train_time:77272ms step_avg:54.53ms
step:1418/1845 train_time:77359ms step_avg:54.56ms
step:1419/1845 train_time:77447ms step_avg:54.58ms
step:1420/1845 train_time:77534ms step_avg:54.60ms
step:1421/1845 train_time:77623ms step_avg:54.63ms
step:1422/1845 train_time:77710ms step_avg:54.65ms
step:1423/1845 train_time:77798ms step_avg:54.67ms
step:1424/1845 train_time:77886ms step_avg:54.70ms
step:1425/1845 train_time:77973ms step_avg:54.72ms
step:1426/1845 train_time:78061ms step_avg:54.74ms
step:1427/1845 train_time:78149ms step_avg:54.76ms
step:1428/1845 train_time:78237ms step_avg:54.79ms
step:1429/1845 train_time:78324ms step_avg:54.81ms
step:1430/1845 train_time:78413ms step_avg:54.83ms
step:1431/1845 train_time:78500ms step_avg:54.86ms
step:1432/1845 train_time:78587ms step_avg:54.88ms
step:1433/1845 train_time:78676ms step_avg:54.90ms
step:1434/1845 train_time:78765ms step_avg:54.93ms
step:1435/1845 train_time:78852ms step_avg:54.95ms
step:1436/1845 train_time:78939ms step_avg:54.97ms
step:1437/1845 train_time:79027ms step_avg:54.99ms
step:1438/1845 train_time:79115ms step_avg:55.02ms
step:1439/1845 train_time:79203ms step_avg:55.04ms
step:1440/1845 train_time:79290ms step_avg:55.06ms
step:1441/1845 train_time:79379ms step_avg:55.09ms
step:1442/1845 train_time:79467ms step_avg:55.11ms
step:1443/1845 train_time:79556ms step_avg:55.13ms
step:1444/1845 train_time:79643ms step_avg:55.15ms
step:1445/1845 train_time:79731ms step_avg:55.18ms
step:1446/1845 train_time:79819ms step_avg:55.20ms
step:1447/1845 train_time:79906ms step_avg:55.22ms
step:1448/1845 train_time:79994ms step_avg:55.24ms
step:1449/1845 train_time:80081ms step_avg:55.27ms
step:1450/1845 train_time:80168ms step_avg:55.29ms
step:1451/1845 train_time:80258ms step_avg:55.31ms
step:1452/1845 train_time:80346ms step_avg:55.33ms
step:1453/1845 train_time:80433ms step_avg:55.36ms
step:1454/1845 train_time:80521ms step_avg:55.38ms
step:1455/1845 train_time:80609ms step_avg:55.40ms
step:1456/1845 train_time:80697ms step_avg:55.42ms
step:1457/1845 train_time:80784ms step_avg:55.45ms
step:1458/1845 train_time:80871ms step_avg:55.47ms
step:1459/1845 train_time:80960ms step_avg:55.49ms
step:1460/1845 train_time:81047ms step_avg:55.51ms
step:1461/1845 train_time:81135ms step_avg:55.53ms
step:1462/1845 train_time:81222ms step_avg:55.56ms
step:1463/1845 train_time:81309ms step_avg:55.58ms
step:1464/1845 train_time:81397ms step_avg:55.60ms
step:1465/1845 train_time:81485ms step_avg:55.62ms
step:1466/1845 train_time:81571ms step_avg:55.64ms
step:1467/1845 train_time:81661ms step_avg:55.67ms
step:1468/1845 train_time:81748ms step_avg:55.69ms
step:1469/1845 train_time:81836ms step_avg:55.71ms
step:1470/1845 train_time:81924ms step_avg:55.73ms
step:1471/1845 train_time:82011ms step_avg:55.75ms
step:1472/1845 train_time:82099ms step_avg:55.77ms
step:1473/1845 train_time:82187ms step_avg:55.80ms
step:1474/1845 train_time:82274ms step_avg:55.82ms
step:1475/1845 train_time:82364ms step_avg:55.84ms
step:1476/1845 train_time:82450ms step_avg:55.86ms
step:1477/1845 train_time:82538ms step_avg:55.88ms
step:1478/1845 train_time:82627ms step_avg:55.90ms
step:1479/1845 train_time:82714ms step_avg:55.93ms
step:1480/1845 train_time:82801ms step_avg:55.95ms
step:1481/1845 train_time:82890ms step_avg:55.97ms
step:1482/1845 train_time:82978ms step_avg:55.99ms
step:1483/1845 train_time:83066ms step_avg:56.01ms
step:1484/1845 train_time:83154ms step_avg:56.03ms
step:1485/1845 train_time:83241ms step_avg:56.05ms
step:1486/1845 train_time:83329ms step_avg:56.08ms
step:1487/1845 train_time:83417ms step_avg:56.10ms
step:1488/1845 train_time:83504ms step_avg:56.12ms
step:1489/1845 train_time:83593ms step_avg:56.14ms
step:1490/1845 train_time:83680ms step_avg:56.16ms
step:1491/1845 train_time:83769ms step_avg:56.18ms
step:1492/1845 train_time:83857ms step_avg:56.20ms
step:1493/1845 train_time:83944ms step_avg:56.23ms
step:1494/1845 train_time:84032ms step_avg:56.25ms
step:1495/1845 train_time:84120ms step_avg:56.27ms
step:1496/1845 train_time:84207ms step_avg:56.29ms
step:1497/1845 train_time:84295ms step_avg:56.31ms
step:1498/1845 train_time:84383ms step_avg:56.33ms
step:1499/1845 train_time:84470ms step_avg:56.35ms
step:1500/1845 train_time:84559ms step_avg:56.37ms
step:1500/1845 val_loss:3.4037 train_time:84645ms step_avg:56.43ms
step:1501/1845 train_time:84674ms step_avg:56.41ms
step:1502/1845 train_time:84736ms step_avg:56.42ms
step:1503/1845 train_time:84826ms step_avg:56.44ms
step:1504/1845 train_time:84915ms step_avg:56.46ms
step:1505/1845 train_time:85002ms step_avg:56.48ms
step:1506/1845 train_time:85088ms step_avg:56.50ms
step:1507/1845 train_time:85175ms step_avg:56.52ms
step:1508/1845 train_time:85262ms step_avg:56.54ms
step:1509/1845 train_time:85348ms step_avg:56.56ms
step:1510/1845 train_time:85436ms step_avg:56.58ms
step:1511/1845 train_time:85523ms step_avg:56.60ms
step:1512/1845 train_time:85612ms step_avg:56.62ms
step:1513/1845 train_time:85703ms step_avg:56.64ms
step:1514/1845 train_time:85791ms step_avg:56.67ms
step:1515/1845 train_time:85881ms step_avg:56.69ms
step:1516/1845 train_time:85967ms step_avg:56.71ms
step:1517/1845 train_time:86056ms step_avg:56.73ms
step:1518/1845 train_time:86142ms step_avg:56.75ms
step:1519/1845 train_time:86229ms step_avg:56.77ms
step:1520/1845 train_time:86316ms step_avg:56.79ms
step:1521/1845 train_time:86403ms step_avg:56.81ms
step:1522/1845 train_time:86490ms step_avg:56.83ms
step:1523/1845 train_time:86579ms step_avg:56.85ms
step:1524/1845 train_time:86668ms step_avg:56.87ms
step:1525/1845 train_time:86758ms step_avg:56.89ms
step:1526/1845 train_time:86846ms step_avg:56.91ms
step:1527/1845 train_time:86935ms step_avg:56.93ms
step:1528/1845 train_time:87022ms step_avg:56.95ms
step:1529/1845 train_time:87109ms step_avg:56.97ms
step:1530/1845 train_time:87197ms step_avg:56.99ms
step:1531/1845 train_time:87283ms step_avg:57.01ms
step:1532/1845 train_time:87371ms step_avg:57.03ms
step:1533/1845 train_time:87458ms step_avg:57.05ms
step:1534/1845 train_time:87546ms step_avg:57.07ms
step:1535/1845 train_time:87634ms step_avg:57.09ms
step:1536/1845 train_time:87723ms step_avg:57.11ms
step:1537/1845 train_time:87811ms step_avg:57.13ms
step:1538/1845 train_time:87899ms step_avg:57.15ms
step:1539/1845 train_time:87987ms step_avg:57.17ms
step:1540/1845 train_time:88075ms step_avg:57.19ms
step:1541/1845 train_time:88163ms step_avg:57.21ms
step:1542/1845 train_time:88249ms step_avg:57.23ms
step:1543/1845 train_time:88336ms step_avg:57.25ms
step:1544/1845 train_time:88424ms step_avg:57.27ms
step:1545/1845 train_time:88511ms step_avg:57.29ms
step:1546/1845 train_time:88599ms step_avg:57.31ms
step:1547/1845 train_time:88688ms step_avg:57.33ms
step:1548/1845 train_time:88777ms step_avg:57.35ms
step:1549/1845 train_time:88865ms step_avg:57.37ms
step:1550/1845 train_time:88953ms step_avg:57.39ms
step:1551/1845 train_time:89040ms step_avg:57.41ms
step:1552/1845 train_time:89127ms step_avg:57.43ms
step:1553/1845 train_time:89213ms step_avg:57.45ms
step:1554/1845 train_time:89300ms step_avg:57.46ms
step:1555/1845 train_time:89389ms step_avg:57.48ms
step:1556/1845 train_time:89476ms step_avg:57.50ms
step:1557/1845 train_time:89564ms step_avg:57.52ms
step:1558/1845 train_time:89652ms step_avg:57.54ms
step:1559/1845 train_time:89740ms step_avg:57.56ms
step:1560/1845 train_time:89827ms step_avg:57.58ms
step:1561/1845 train_time:89916ms step_avg:57.60ms
step:1562/1845 train_time:90005ms step_avg:57.62ms
step:1563/1845 train_time:90091ms step_avg:57.64ms
step:1564/1845 train_time:90179ms step_avg:57.66ms
step:1565/1845 train_time:90266ms step_avg:57.68ms
step:1566/1845 train_time:90353ms step_avg:57.70ms
step:1567/1845 train_time:90441ms step_avg:57.72ms
step:1568/1845 train_time:90528ms step_avg:57.73ms
step:1569/1845 train_time:90616ms step_avg:57.75ms
step:1570/1845 train_time:90704ms step_avg:57.77ms
step:1571/1845 train_time:90792ms step_avg:57.79ms
step:1572/1845 train_time:90879ms step_avg:57.81ms
step:1573/1845 train_time:90967ms step_avg:57.83ms
step:1574/1845 train_time:91056ms step_avg:57.85ms
step:1575/1845 train_time:91143ms step_avg:57.87ms
step:1576/1845 train_time:91230ms step_avg:57.89ms
step:1577/1845 train_time:91317ms step_avg:57.91ms
step:1578/1845 train_time:91406ms step_avg:57.93ms
step:1579/1845 train_time:91492ms step_avg:57.94ms
step:1580/1845 train_time:91580ms step_avg:57.96ms
step:1581/1845 train_time:91668ms step_avg:57.98ms
step:1582/1845 train_time:91756ms step_avg:58.00ms
step:1583/1845 train_time:91843ms step_avg:58.02ms
step:1584/1845 train_time:91930ms step_avg:58.04ms
step:1585/1845 train_time:92019ms step_avg:58.06ms
step:1586/1845 train_time:92106ms step_avg:58.07ms
step:1587/1845 train_time:92194ms step_avg:58.09ms
step:1588/1845 train_time:92282ms step_avg:58.11ms
step:1589/1845 train_time:92368ms step_avg:58.13ms
step:1590/1845 train_time:92455ms step_avg:58.15ms
step:1591/1845 train_time:92543ms step_avg:58.17ms
step:1592/1845 train_time:92630ms step_avg:58.18ms
step:1593/1845 train_time:92719ms step_avg:58.20ms
step:1594/1845 train_time:92807ms step_avg:58.22ms
step:1595/1845 train_time:92895ms step_avg:58.24ms
step:1596/1845 train_time:92984ms step_avg:58.26ms
step:1597/1845 train_time:93071ms step_avg:58.28ms
step:1598/1845 train_time:93159ms step_avg:58.30ms
step:1599/1845 train_time:93246ms step_avg:58.32ms
step:1600/1845 train_time:93334ms step_avg:58.33ms
step:1601/1845 train_time:93421ms step_avg:58.35ms
step:1602/1845 train_time:93508ms step_avg:58.37ms
step:1603/1845 train_time:93596ms step_avg:58.39ms
step:1604/1845 train_time:93684ms step_avg:58.41ms
step:1605/1845 train_time:93772ms step_avg:58.43ms
step:1606/1845 train_time:93861ms step_avg:58.44ms
step:1607/1845 train_time:93948ms step_avg:58.46ms
step:1608/1845 train_time:94036ms step_avg:58.48ms
step:1609/1845 train_time:94123ms step_avg:58.50ms
step:1610/1845 train_time:94210ms step_avg:58.52ms
step:1611/1845 train_time:94298ms step_avg:58.53ms
step:1612/1845 train_time:94386ms step_avg:58.55ms
step:1613/1845 train_time:94472ms step_avg:58.57ms
step:1614/1845 train_time:94561ms step_avg:58.59ms
step:1615/1845 train_time:94648ms step_avg:58.61ms
step:1616/1845 train_time:94736ms step_avg:58.62ms
step:1617/1845 train_time:94824ms step_avg:58.64ms
step:1618/1845 train_time:94911ms step_avg:58.66ms
step:1619/1845 train_time:95000ms step_avg:58.68ms
step:1620/1845 train_time:95087ms step_avg:58.70ms
step:1621/1845 train_time:95175ms step_avg:58.71ms
step:1622/1845 train_time:95263ms step_avg:58.73ms
step:1623/1845 train_time:95350ms step_avg:58.75ms
step:1624/1845 train_time:95438ms step_avg:58.77ms
step:1625/1845 train_time:95525ms step_avg:58.78ms
step:1626/1845 train_time:95612ms step_avg:58.80ms
step:1627/1845 train_time:95701ms step_avg:58.82ms
step:1628/1845 train_time:95787ms step_avg:58.84ms
step:1629/1845 train_time:95876ms step_avg:58.86ms
step:1630/1845 train_time:95965ms step_avg:58.87ms
step:1631/1845 train_time:96052ms step_avg:58.89ms
step:1632/1845 train_time:96139ms step_avg:58.91ms
step:1633/1845 train_time:96227ms step_avg:58.93ms
step:1634/1845 train_time:96316ms step_avg:58.94ms
step:1635/1845 train_time:96403ms step_avg:58.96ms
step:1636/1845 train_time:96489ms step_avg:58.98ms
step:1637/1845 train_time:96578ms step_avg:59.00ms
step:1638/1845 train_time:96665ms step_avg:59.01ms
step:1639/1845 train_time:96753ms step_avg:59.03ms
step:1640/1845 train_time:96841ms step_avg:59.05ms
step:1641/1845 train_time:96929ms step_avg:59.07ms
step:1642/1845 train_time:97017ms step_avg:59.08ms
step:1643/1845 train_time:97105ms step_avg:59.10ms
step:1644/1845 train_time:97192ms step_avg:59.12ms
step:1645/1845 train_time:97280ms step_avg:59.14ms
step:1646/1845 train_time:97367ms step_avg:59.15ms
step:1647/1845 train_time:97454ms step_avg:59.17ms
step:1648/1845 train_time:97543ms step_avg:59.19ms
step:1649/1845 train_time:97630ms step_avg:59.21ms
step:1650/1845 train_time:97718ms step_avg:59.22ms
step:1651/1845 train_time:97804ms step_avg:59.24ms
step:1652/1845 train_time:97894ms step_avg:59.26ms
step:1653/1845 train_time:97982ms step_avg:59.28ms
step:1654/1845 train_time:98070ms step_avg:59.29ms
step:1655/1845 train_time:98159ms step_avg:59.31ms
step:1656/1845 train_time:98246ms step_avg:59.33ms
step:1657/1845 train_time:98335ms step_avg:59.35ms
step:1658/1845 train_time:98423ms step_avg:59.36ms
step:1659/1845 train_time:98509ms step_avg:59.38ms
step:1660/1845 train_time:98597ms step_avg:59.40ms
step:1661/1845 train_time:98684ms step_avg:59.41ms
step:1662/1845 train_time:98772ms step_avg:59.43ms
step:1663/1845 train_time:98860ms step_avg:59.45ms
step:1664/1845 train_time:98947ms step_avg:59.46ms
step:1665/1845 train_time:99035ms step_avg:59.48ms
step:1666/1845 train_time:99123ms step_avg:59.50ms
step:1667/1845 train_time:99211ms step_avg:59.51ms
step:1668/1845 train_time:99299ms step_avg:59.53ms
step:1669/1845 train_time:99386ms step_avg:59.55ms
step:1670/1845 train_time:99474ms step_avg:59.57ms
step:1671/1845 train_time:99562ms step_avg:59.58ms
step:1672/1845 train_time:99649ms step_avg:59.60ms
step:1673/1845 train_time:99737ms step_avg:59.62ms
step:1674/1845 train_time:99824ms step_avg:59.63ms
step:1675/1845 train_time:99912ms step_avg:59.65ms
step:1676/1845 train_time:100000ms step_avg:59.67ms
step:1677/1845 train_time:100088ms step_avg:59.68ms
step:1678/1845 train_time:100177ms step_avg:59.70ms
step:1679/1845 train_time:100264ms step_avg:59.72ms
step:1680/1845 train_time:100352ms step_avg:59.73ms
step:1681/1845 train_time:100440ms step_avg:59.75ms
step:1682/1845 train_time:100527ms step_avg:59.77ms
step:1683/1845 train_time:100613ms step_avg:59.78ms
step:1684/1845 train_time:100701ms step_avg:59.80ms
step:1685/1845 train_time:100790ms step_avg:59.82ms
step:1686/1845 train_time:100878ms step_avg:59.83ms
step:1687/1845 train_time:100965ms step_avg:59.85ms
step:1688/1845 train_time:101053ms step_avg:59.87ms
step:1689/1845 train_time:101142ms step_avg:59.88ms
step:1690/1845 train_time:101229ms step_avg:59.90ms
step:1691/1845 train_time:101317ms step_avg:59.92ms
step:1692/1845 train_time:101405ms step_avg:59.93ms
step:1693/1845 train_time:101492ms step_avg:59.95ms
step:1694/1845 train_time:101581ms step_avg:59.96ms
step:1695/1845 train_time:101668ms step_avg:59.98ms
step:1696/1845 train_time:101755ms step_avg:60.00ms
step:1697/1845 train_time:101844ms step_avg:60.01ms
step:1698/1845 train_time:101931ms step_avg:60.03ms
step:1699/1845 train_time:102020ms step_avg:60.05ms
step:1700/1845 train_time:102107ms step_avg:60.06ms
step:1701/1845 train_time:102196ms step_avg:60.08ms
step:1702/1845 train_time:102285ms step_avg:60.10ms
step:1703/1845 train_time:102372ms step_avg:60.11ms
step:1704/1845 train_time:102460ms step_avg:60.13ms
step:1705/1845 train_time:102547ms step_avg:60.15ms
step:1706/1845 train_time:102636ms step_avg:60.16ms
step:1707/1845 train_time:102724ms step_avg:60.18ms
step:1708/1845 train_time:102810ms step_avg:60.19ms
step:1709/1845 train_time:102898ms step_avg:60.21ms
step:1710/1845 train_time:102986ms step_avg:60.23ms
step:1711/1845 train_time:103073ms step_avg:60.24ms
step:1712/1845 train_time:103162ms step_avg:60.26ms
step:1713/1845 train_time:103249ms step_avg:60.27ms
step:1714/1845 train_time:103337ms step_avg:60.29ms
step:1715/1845 train_time:103424ms step_avg:60.31ms
step:1716/1845 train_time:103512ms step_avg:60.32ms
step:1717/1845 train_time:103600ms step_avg:60.34ms
step:1718/1845 train_time:103687ms step_avg:60.35ms
step:1719/1845 train_time:103775ms step_avg:60.37ms
step:1720/1845 train_time:103863ms step_avg:60.39ms
step:1721/1845 train_time:103950ms step_avg:60.40ms
step:1722/1845 train_time:104037ms step_avg:60.42ms
step:1723/1845 train_time:104126ms step_avg:60.43ms
step:1724/1845 train_time:104214ms step_avg:60.45ms
step:1725/1845 train_time:104301ms step_avg:60.46ms
step:1726/1845 train_time:104388ms step_avg:60.48ms
step:1727/1845 train_time:104476ms step_avg:60.50ms
step:1728/1845 train_time:104565ms step_avg:60.51ms
step:1729/1845 train_time:104652ms step_avg:60.53ms
step:1730/1845 train_time:104740ms step_avg:60.54ms
step:1731/1845 train_time:104828ms step_avg:60.56ms
step:1732/1845 train_time:104916ms step_avg:60.57ms
step:1733/1845 train_time:105003ms step_avg:60.59ms
step:1734/1845 train_time:105090ms step_avg:60.61ms
step:1735/1845 train_time:105179ms step_avg:60.62ms
step:1736/1845 train_time:105266ms step_avg:60.64ms
step:1737/1845 train_time:105353ms step_avg:60.65ms
step:1738/1845 train_time:105441ms step_avg:60.67ms
step:1739/1845 train_time:105529ms step_avg:60.68ms
step:1740/1845 train_time:105617ms step_avg:60.70ms
step:1741/1845 train_time:105706ms step_avg:60.72ms
step:1742/1845 train_time:105792ms step_avg:60.73ms
step:1743/1845 train_time:105881ms step_avg:60.75ms
step:1744/1845 train_time:105968ms step_avg:60.76ms
step:1745/1845 train_time:106056ms step_avg:60.78ms
step:1746/1845 train_time:106143ms step_avg:60.79ms
step:1747/1845 train_time:106231ms step_avg:60.81ms
step:1748/1845 train_time:106319ms step_avg:60.82ms
step:1749/1845 train_time:106406ms step_avg:60.84ms
step:1750/1845 train_time:106494ms step_avg:60.85ms
step:1750/1845 val_loss:3.3052 train_time:106581ms step_avg:60.90ms
step:1751/1845 train_time:106610ms step_avg:60.88ms
step:1752/1845 train_time:106671ms step_avg:60.89ms
step:1753/1845 train_time:106763ms step_avg:60.90ms
step:1754/1845 train_time:106852ms step_avg:60.92ms
step:1755/1845 train_time:106939ms step_avg:60.93ms
step:1756/1845 train_time:107026ms step_avg:60.95ms
step:1757/1845 train_time:107113ms step_avg:60.96ms
step:1758/1845 train_time:107200ms step_avg:60.98ms
step:1759/1845 train_time:107286ms step_avg:60.99ms
step:1760/1845 train_time:107374ms step_avg:61.01ms
step:1761/1845 train_time:107461ms step_avg:61.02ms
step:1762/1845 train_time:107551ms step_avg:61.04ms
step:1763/1845 train_time:107641ms step_avg:61.06ms
step:1764/1845 train_time:107730ms step_avg:61.07ms
step:1765/1845 train_time:107819ms step_avg:61.09ms
step:1766/1845 train_time:107906ms step_avg:61.10ms
step:1767/1845 train_time:107994ms step_avg:61.12ms
step:1768/1845 train_time:108081ms step_avg:61.13ms
step:1769/1845 train_time:108167ms step_avg:61.15ms
step:1770/1845 train_time:108254ms step_avg:61.16ms
step:1771/1845 train_time:108341ms step_avg:61.18ms
step:1772/1845 train_time:108428ms step_avg:61.19ms
step:1773/1845 train_time:108516ms step_avg:61.20ms
step:1774/1845 train_time:108604ms step_avg:61.22ms
step:1775/1845 train_time:108694ms step_avg:61.24ms
step:1776/1845 train_time:108783ms step_avg:61.25ms
step:1777/1845 train_time:108869ms step_avg:61.27ms
step:1778/1845 train_time:108959ms step_avg:61.28ms
step:1779/1845 train_time:109046ms step_avg:61.30ms
step:1780/1845 train_time:109133ms step_avg:61.31ms
step:1781/1845 train_time:109220ms step_avg:61.33ms
step:1782/1845 train_time:109308ms step_avg:61.34ms
step:1783/1845 train_time:109395ms step_avg:61.35ms
step:1784/1845 train_time:109482ms step_avg:61.37ms
step:1785/1845 train_time:109571ms step_avg:61.38ms
step:1786/1845 train_time:109660ms step_avg:61.40ms
step:1787/1845 train_time:109747ms step_avg:61.41ms
step:1788/1845 train_time:109836ms step_avg:61.43ms
step:1789/1845 train_time:109923ms step_avg:61.44ms
step:1790/1845 train_time:110011ms step_avg:61.46ms
step:1791/1845 train_time:110098ms step_avg:61.47ms
step:1792/1845 train_time:110185ms step_avg:61.49ms
step:1793/1845 train_time:110272ms step_avg:61.50ms
step:1794/1845 train_time:110359ms step_avg:61.52ms
step:1795/1845 train_time:110446ms step_avg:61.53ms
step:1796/1845 train_time:110534ms step_avg:61.54ms
step:1797/1845 train_time:110622ms step_avg:61.56ms
step:1798/1845 train_time:110710ms step_avg:61.57ms
step:1799/1845 train_time:110798ms step_avg:61.59ms
step:1800/1845 train_time:110885ms step_avg:61.60ms
step:1801/1845 train_time:110974ms step_avg:61.62ms
step:1802/1845 train_time:111060ms step_avg:61.63ms
step:1803/1845 train_time:111149ms step_avg:61.65ms
step:1804/1845 train_time:111236ms step_avg:61.66ms
step:1805/1845 train_time:111323ms step_avg:61.67ms
step:1806/1845 train_time:111411ms step_avg:61.69ms
step:1807/1845 train_time:111500ms step_avg:61.70ms
step:1808/1845 train_time:111589ms step_avg:61.72ms
step:1809/1845 train_time:111677ms step_avg:61.73ms
step:1810/1845 train_time:111764ms step_avg:61.75ms
step:1811/1845 train_time:111854ms step_avg:61.76ms
step:1812/1845 train_time:111942ms step_avg:61.78ms
step:1813/1845 train_time:112030ms step_avg:61.79ms
step:1814/1845 train_time:112118ms step_avg:61.81ms
step:1815/1845 train_time:112205ms step_avg:61.82ms
step:1816/1845 train_time:112293ms step_avg:61.84ms
step:1817/1845 train_time:112381ms step_avg:61.85ms
step:1818/1845 train_time:112469ms step_avg:61.86ms
step:1819/1845 train_time:112557ms step_avg:61.88ms
step:1820/1845 train_time:112644ms step_avg:61.89ms
step:1821/1845 train_time:112734ms step_avg:61.91ms
step:1822/1845 train_time:112822ms step_avg:61.92ms
step:1823/1845 train_time:112909ms step_avg:61.94ms
step:1824/1845 train_time:112998ms step_avg:61.95ms
step:1825/1845 train_time:113085ms step_avg:61.96ms
step:1826/1845 train_time:113173ms step_avg:61.98ms
step:1827/1845 train_time:113260ms step_avg:61.99ms
step:1828/1845 train_time:113347ms step_avg:62.01ms
step:1829/1845 train_time:113436ms step_avg:62.02ms
step:1830/1845 train_time:113524ms step_avg:62.03ms
step:1831/1845 train_time:113612ms step_avg:62.05ms
step:1832/1845 train_time:113700ms step_avg:62.06ms
step:1833/1845 train_time:113788ms step_avg:62.08ms
step:1834/1845 train_time:113875ms step_avg:62.09ms
step:1835/1845 train_time:113964ms step_avg:62.11ms
step:1836/1845 train_time:114051ms step_avg:62.12ms
step:1837/1845 train_time:114140ms step_avg:62.13ms
step:1838/1845 train_time:114227ms step_avg:62.15ms
step:1839/1845 train_time:114315ms step_avg:62.16ms
step:1840/1845 train_time:114402ms step_avg:62.18ms
step:1841/1845 train_time:114490ms step_avg:62.19ms
step:1842/1845 train_time:114577ms step_avg:62.20ms
step:1843/1845 train_time:114666ms step_avg:62.22ms
step:1844/1845 train_time:114754ms step_avg:62.23ms
step:1845/1845 train_time:114842ms step_avg:62.24ms
step:1845/1845 val_loss:3.2788 train_time:114927ms step_avg:62.29ms
peak memory allocated: 29405 MiB reserved: 44698 MiB

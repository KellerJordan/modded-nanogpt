# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29 - gates-banked-compiled-adam - bf16 gates - fp32 exp_avg"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            #exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device) # Testing making the optimizer state fp32.
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay (lr as weight decay schedule)
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0))
                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()  # Testing leaving these as fp32
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

if master_process:
    import wandb
    wandb.login()
    wandb.init(
        project=WANDB_PROJECT,
        name=args.run_id,
        config=args
    )
    wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        if master_process:
            # Log to wandb
            wandb.log({
                "val/loss": val_loss.item(),
                "time/training_time_ms": training_time_ms,
                "time/step_avg_ms": training_time_ms / (step + 1),
            }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process:
            final_metrics = {
                "final/val_loss": float(val_loss.item()),
                "final/train_time_ms": int(training_time_ms),
                "final/train_time": training_time_ms / 1000,
                "final/steps_total": int(step),
            }
            wandb.log(final_metrics)
            wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
if master_process:
    wandb.save(f"{args.log_dir}/{args.run_id}.txt")
    wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 07:39:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   39C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   33C    P0            114W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          354716      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          354717      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          354718      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          354719      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          354720      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    5   N/A  N/A          354721      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          354722      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    7   N/A  N/A          354723      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8326 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:56ms step_avg:55.69ms
step:2/1845 train_time:84ms step_avg:41.92ms
step:3/1845 train_time:110ms step_avg:36.75ms
step:4/1845 train_time:140ms step_avg:35.04ms
step:5/1845 train_time:167ms step_avg:33.39ms
step:6/1845 train_time:288ms step_avg:48.03ms
step:7/1845 train_time:318ms step_avg:45.43ms
step:8/1845 train_time:351ms step_avg:43.85ms
step:9/1845 train_time:380ms step_avg:42.26ms
step:10/1845 train_time:416ms step_avg:41.60ms
step:11/1845 train_time:448ms step_avg:40.76ms
step:12/1845 train_time:482ms step_avg:40.15ms
step:13/1845 train_time:513ms step_avg:39.45ms
step:14/1845 train_time:547ms step_avg:39.09ms
step:15/1845 train_time:577ms step_avg:38.45ms
step:16/1845 train_time:610ms step_avg:38.10ms
step:17/1845 train_time:641ms step_avg:37.73ms
step:18/1845 train_time:674ms step_avg:37.46ms
step:19/1845 train_time:706ms step_avg:37.15ms
step:20/1845 train_time:739ms step_avg:36.96ms
step:21/1845 train_time:769ms step_avg:36.60ms
step:22/1845 train_time:804ms step_avg:36.56ms
step:23/1845 train_time:836ms step_avg:36.34ms
step:24/1845 train_time:869ms step_avg:36.20ms
step:25/1845 train_time:901ms step_avg:36.03ms
step:26/1845 train_time:934ms step_avg:35.94ms
step:27/1845 train_time:965ms step_avg:35.75ms
step:28/1845 train_time:1003ms step_avg:35.84ms
step:29/1845 train_time:1033ms step_avg:35.63ms
step:30/1845 train_time:1072ms step_avg:35.72ms
step:31/1845 train_time:1101ms step_avg:35.53ms
step:32/1845 train_time:1139ms step_avg:35.59ms
step:33/1845 train_time:1169ms step_avg:35.42ms
step:34/1845 train_time:1208ms step_avg:35.54ms
step:35/1845 train_time:1238ms step_avg:35.38ms
step:36/1845 train_time:1275ms step_avg:35.43ms
step:37/1845 train_time:1306ms step_avg:35.29ms
step:38/1845 train_time:1344ms step_avg:35.36ms
step:39/1845 train_time:1374ms step_avg:35.22ms
step:40/1845 train_time:1412ms step_avg:35.31ms
step:41/1845 train_time:1442ms step_avg:35.17ms
step:42/1845 train_time:1480ms step_avg:35.25ms
step:43/1845 train_time:1510ms step_avg:35.12ms
step:44/1845 train_time:1548ms step_avg:35.19ms
step:45/1845 train_time:1578ms step_avg:35.07ms
step:46/1845 train_time:1617ms step_avg:35.14ms
step:47/1845 train_time:1646ms step_avg:35.03ms
step:48/1845 train_time:1685ms step_avg:35.10ms
step:49/1845 train_time:1715ms step_avg:34.99ms
step:50/1845 train_time:1753ms step_avg:35.06ms
step:51/1845 train_time:1785ms step_avg:35.00ms
step:52/1845 train_time:1821ms step_avg:35.02ms
step:53/1845 train_time:1851ms step_avg:34.92ms
step:54/1845 train_time:1889ms step_avg:34.98ms
step:55/1845 train_time:1921ms step_avg:34.92ms
step:56/1845 train_time:1957ms step_avg:34.94ms
step:57/1845 train_time:1988ms step_avg:34.87ms
step:58/1845 train_time:2025ms step_avg:34.92ms
step:59/1845 train_time:2057ms step_avg:34.86ms
step:60/1845 train_time:2093ms step_avg:34.88ms
step:61/1845 train_time:2124ms step_avg:34.82ms
step:62/1845 train_time:2160ms step_avg:34.84ms
step:63/1845 train_time:2190ms step_avg:34.77ms
step:64/1845 train_time:2229ms step_avg:34.82ms
step:65/1845 train_time:2258ms step_avg:34.75ms
step:66/1845 train_time:2297ms step_avg:34.80ms
step:67/1845 train_time:2327ms step_avg:34.73ms
step:68/1845 train_time:2365ms step_avg:34.78ms
step:69/1845 train_time:2395ms step_avg:34.71ms
step:70/1845 train_time:2434ms step_avg:34.77ms
step:71/1845 train_time:2464ms step_avg:34.70ms
step:72/1845 train_time:2502ms step_avg:34.74ms
step:73/1845 train_time:2532ms step_avg:34.68ms
step:74/1845 train_time:2570ms step_avg:34.73ms
step:75/1845 train_time:2600ms step_avg:34.66ms
step:76/1845 train_time:2638ms step_avg:34.71ms
step:77/1845 train_time:2668ms step_avg:34.65ms
step:78/1845 train_time:2706ms step_avg:34.69ms
step:79/1845 train_time:2736ms step_avg:34.63ms
step:80/1845 train_time:2775ms step_avg:34.69ms
step:81/1845 train_time:2805ms step_avg:34.63ms
step:82/1845 train_time:2844ms step_avg:34.68ms
step:83/1845 train_time:2873ms step_avg:34.61ms
step:84/1845 train_time:2911ms step_avg:34.66ms
step:85/1845 train_time:2941ms step_avg:34.60ms
step:86/1845 train_time:2979ms step_avg:34.64ms
step:87/1845 train_time:3009ms step_avg:34.59ms
step:88/1845 train_time:3047ms step_avg:34.63ms
step:89/1845 train_time:3077ms step_avg:34.57ms
step:90/1845 train_time:3115ms step_avg:34.61ms
step:91/1845 train_time:3145ms step_avg:34.56ms
step:92/1845 train_time:3183ms step_avg:34.60ms
step:93/1845 train_time:3213ms step_avg:34.55ms
step:94/1845 train_time:3251ms step_avg:34.59ms
step:95/1845 train_time:3283ms step_avg:34.56ms
step:96/1845 train_time:3320ms step_avg:34.58ms
step:97/1845 train_time:3349ms step_avg:34.53ms
step:98/1845 train_time:3387ms step_avg:34.56ms
step:99/1845 train_time:3417ms step_avg:34.52ms
step:100/1845 train_time:3455ms step_avg:34.55ms
step:101/1845 train_time:3485ms step_avg:34.50ms
step:102/1845 train_time:3523ms step_avg:34.54ms
step:103/1845 train_time:3553ms step_avg:34.49ms
step:104/1845 train_time:3590ms step_avg:34.52ms
step:105/1845 train_time:3620ms step_avg:34.48ms
step:106/1845 train_time:3659ms step_avg:34.52ms
step:107/1845 train_time:3690ms step_avg:34.49ms
step:108/1845 train_time:3728ms step_avg:34.51ms
step:109/1845 train_time:3759ms step_avg:34.49ms
step:110/1845 train_time:3795ms step_avg:34.50ms
step:111/1845 train_time:3826ms step_avg:34.47ms
step:112/1845 train_time:3863ms step_avg:34.49ms
step:113/1845 train_time:3895ms step_avg:34.47ms
step:114/1845 train_time:3931ms step_avg:34.49ms
step:115/1845 train_time:3963ms step_avg:34.46ms
step:116/1845 train_time:4000ms step_avg:34.48ms
step:117/1845 train_time:4031ms step_avg:34.46ms
step:118/1845 train_time:4067ms step_avg:34.47ms
step:119/1845 train_time:4100ms step_avg:34.45ms
step:120/1845 train_time:4135ms step_avg:34.46ms
step:121/1845 train_time:4166ms step_avg:34.43ms
step:122/1845 train_time:4204ms step_avg:34.46ms
step:123/1845 train_time:4234ms step_avg:34.42ms
step:124/1845 train_time:4272ms step_avg:34.45ms
step:125/1845 train_time:4302ms step_avg:34.41ms
step:126/1845 train_time:4340ms step_avg:34.44ms
step:127/1845 train_time:4372ms step_avg:34.43ms
step:128/1845 train_time:4407ms step_avg:34.43ms
step:129/1845 train_time:4437ms step_avg:34.40ms
step:130/1845 train_time:4475ms step_avg:34.43ms
step:131/1845 train_time:4507ms step_avg:34.41ms
step:132/1845 train_time:4543ms step_avg:34.42ms
step:133/1845 train_time:4575ms step_avg:34.40ms
step:134/1845 train_time:4611ms step_avg:34.41ms
step:135/1845 train_time:4643ms step_avg:34.39ms
step:136/1845 train_time:4680ms step_avg:34.41ms
step:137/1845 train_time:4711ms step_avg:34.39ms
step:138/1845 train_time:4747ms step_avg:34.40ms
step:139/1845 train_time:4780ms step_avg:34.39ms
step:140/1845 train_time:4815ms step_avg:34.40ms
step:141/1845 train_time:4847ms step_avg:34.38ms
step:142/1845 train_time:4883ms step_avg:34.39ms
step:143/1845 train_time:4916ms step_avg:34.38ms
step:144/1845 train_time:4952ms step_avg:34.39ms
step:145/1845 train_time:4984ms step_avg:34.37ms
step:146/1845 train_time:5019ms step_avg:34.38ms
step:147/1845 train_time:5051ms step_avg:34.36ms
step:148/1845 train_time:5087ms step_avg:34.37ms
step:149/1845 train_time:5118ms step_avg:34.35ms
step:150/1845 train_time:5155ms step_avg:34.37ms
step:151/1845 train_time:5187ms step_avg:34.35ms
step:152/1845 train_time:5223ms step_avg:34.36ms
step:153/1845 train_time:5253ms step_avg:34.33ms
step:154/1845 train_time:5292ms step_avg:34.36ms
step:155/1845 train_time:5324ms step_avg:34.35ms
step:156/1845 train_time:5361ms step_avg:34.36ms
step:157/1845 train_time:5393ms step_avg:34.35ms
step:158/1845 train_time:5428ms step_avg:34.35ms
step:159/1845 train_time:5460ms step_avg:34.34ms
step:160/1845 train_time:5495ms step_avg:34.34ms
step:161/1845 train_time:5527ms step_avg:34.33ms
step:162/1845 train_time:5563ms step_avg:34.34ms
step:163/1845 train_time:5595ms step_avg:34.33ms
step:164/1845 train_time:5631ms step_avg:34.34ms
step:165/1845 train_time:5664ms step_avg:34.33ms
step:166/1845 train_time:5700ms step_avg:34.34ms
step:167/1845 train_time:5733ms step_avg:34.33ms
step:168/1845 train_time:5770ms step_avg:34.34ms
step:169/1845 train_time:5800ms step_avg:34.32ms
step:170/1845 train_time:5837ms step_avg:34.33ms
step:171/1845 train_time:5869ms step_avg:34.32ms
step:172/1845 train_time:5904ms step_avg:34.32ms
step:173/1845 train_time:5935ms step_avg:34.31ms
step:174/1845 train_time:5970ms step_avg:34.31ms
step:175/1845 train_time:6002ms step_avg:34.30ms
step:176/1845 train_time:6039ms step_avg:34.31ms
step:177/1845 train_time:6071ms step_avg:34.30ms
step:178/1845 train_time:6106ms step_avg:34.30ms
step:179/1845 train_time:6138ms step_avg:34.29ms
step:180/1845 train_time:6174ms step_avg:34.30ms
step:181/1845 train_time:6204ms step_avg:34.28ms
step:182/1845 train_time:6243ms step_avg:34.30ms
step:183/1845 train_time:6275ms step_avg:34.29ms
step:184/1845 train_time:6310ms step_avg:34.29ms
step:185/1845 train_time:6342ms step_avg:34.28ms
step:186/1845 train_time:6378ms step_avg:34.29ms
step:187/1845 train_time:6409ms step_avg:34.27ms
step:188/1845 train_time:6446ms step_avg:34.29ms
step:189/1845 train_time:6476ms step_avg:34.27ms
step:190/1845 train_time:6514ms step_avg:34.28ms
step:191/1845 train_time:6545ms step_avg:34.27ms
step:192/1845 train_time:6582ms step_avg:34.28ms
step:193/1845 train_time:6614ms step_avg:34.27ms
step:194/1845 train_time:6650ms step_avg:34.28ms
step:195/1845 train_time:6682ms step_avg:34.27ms
step:196/1845 train_time:6719ms step_avg:34.28ms
step:197/1845 train_time:6757ms step_avg:34.30ms
step:198/1845 train_time:6794ms step_avg:34.31ms
step:199/1845 train_time:6826ms step_avg:34.30ms
step:200/1845 train_time:6862ms step_avg:34.31ms
step:201/1845 train_time:6894ms step_avg:34.30ms
step:202/1845 train_time:6929ms step_avg:34.30ms
step:203/1845 train_time:6961ms step_avg:34.29ms
step:204/1845 train_time:6996ms step_avg:34.29ms
step:205/1845 train_time:7027ms step_avg:34.28ms
step:206/1845 train_time:7062ms step_avg:34.28ms
step:207/1845 train_time:7094ms step_avg:34.27ms
step:208/1845 train_time:7129ms step_avg:34.27ms
step:209/1845 train_time:7160ms step_avg:34.26ms
step:210/1845 train_time:7195ms step_avg:34.26ms
step:211/1845 train_time:7227ms step_avg:34.25ms
step:212/1845 train_time:7262ms step_avg:34.26ms
step:213/1845 train_time:7294ms step_avg:34.25ms
step:214/1845 train_time:7330ms step_avg:34.25ms
step:215/1845 train_time:7362ms step_avg:34.24ms
step:216/1845 train_time:7398ms step_avg:34.25ms
step:217/1845 train_time:7429ms step_avg:34.24ms
step:218/1845 train_time:7466ms step_avg:34.25ms
step:219/1845 train_time:7498ms step_avg:34.24ms
step:220/1845 train_time:7533ms step_avg:34.24ms
step:221/1845 train_time:7565ms step_avg:34.23ms
step:222/1845 train_time:7601ms step_avg:34.24ms
step:223/1845 train_time:7633ms step_avg:34.23ms
step:224/1845 train_time:7669ms step_avg:34.24ms
step:225/1845 train_time:7701ms step_avg:34.23ms
step:226/1845 train_time:7737ms step_avg:34.24ms
step:227/1845 train_time:7769ms step_avg:34.23ms
step:228/1845 train_time:7806ms step_avg:34.23ms
step:229/1845 train_time:7837ms step_avg:34.22ms
step:230/1845 train_time:7874ms step_avg:34.23ms
step:231/1845 train_time:7905ms step_avg:34.22ms
step:232/1845 train_time:7942ms step_avg:34.23ms
step:233/1845 train_time:7974ms step_avg:34.22ms
step:234/1845 train_time:8009ms step_avg:34.23ms
step:235/1845 train_time:8041ms step_avg:34.22ms
step:236/1845 train_time:8078ms step_avg:34.23ms
step:237/1845 train_time:8109ms step_avg:34.22ms
step:238/1845 train_time:8145ms step_avg:34.22ms
step:239/1845 train_time:8177ms step_avg:34.22ms
step:240/1845 train_time:8213ms step_avg:34.22ms
step:241/1845 train_time:8245ms step_avg:34.21ms
step:242/1845 train_time:8281ms step_avg:34.22ms
step:243/1845 train_time:8313ms step_avg:34.21ms
step:244/1845 train_time:8349ms step_avg:34.22ms
step:245/1845 train_time:8381ms step_avg:34.21ms
step:246/1845 train_time:8417ms step_avg:34.22ms
step:247/1845 train_time:8449ms step_avg:34.20ms
step:248/1845 train_time:8485ms step_avg:34.21ms
step:249/1845 train_time:8517ms step_avg:34.21ms
step:250/1845 train_time:8553ms step_avg:34.21ms
step:250/1845 val_loss:4.6074 train_time:8590ms step_avg:34.36ms
step:251/1845 train_time:8617ms step_avg:34.33ms
step:252/1845 train_time:8644ms step_avg:34.30ms
step:253/1845 train_time:8669ms step_avg:34.26ms
step:254/1845 train_time:8695ms step_avg:34.23ms
step:255/1845 train_time:8721ms step_avg:34.20ms
step:256/1845 train_time:8758ms step_avg:34.21ms
step:257/1845 train_time:8790ms step_avg:34.20ms
step:258/1845 train_time:8826ms step_avg:34.21ms
step:259/1845 train_time:8858ms step_avg:34.20ms
step:260/1845 train_time:8896ms step_avg:34.22ms
step:261/1845 train_time:8926ms step_avg:34.20ms
step:262/1845 train_time:8964ms step_avg:34.21ms
step:263/1845 train_time:8994ms step_avg:34.20ms
step:264/1845 train_time:9032ms step_avg:34.21ms
step:265/1845 train_time:9062ms step_avg:34.20ms
step:266/1845 train_time:9099ms step_avg:34.21ms
step:267/1845 train_time:9130ms step_avg:34.19ms
step:268/1845 train_time:9168ms step_avg:34.21ms
step:269/1845 train_time:9200ms step_avg:34.20ms
step:270/1845 train_time:9235ms step_avg:34.21ms
step:271/1845 train_time:9267ms step_avg:34.19ms
step:272/1845 train_time:9303ms step_avg:34.20ms
step:273/1845 train_time:9334ms step_avg:34.19ms
step:274/1845 train_time:9371ms step_avg:34.20ms
step:275/1845 train_time:9403ms step_avg:34.19ms
step:276/1845 train_time:9439ms step_avg:34.20ms
step:277/1845 train_time:9471ms step_avg:34.19ms
step:278/1845 train_time:9506ms step_avg:34.19ms
step:279/1845 train_time:9538ms step_avg:34.19ms
step:280/1845 train_time:9574ms step_avg:34.19ms
step:281/1845 train_time:9605ms step_avg:34.18ms
step:282/1845 train_time:9642ms step_avg:34.19ms
step:283/1845 train_time:9674ms step_avg:34.18ms
step:284/1845 train_time:9710ms step_avg:34.19ms
step:285/1845 train_time:9742ms step_avg:34.18ms
step:286/1845 train_time:9779ms step_avg:34.19ms
step:287/1845 train_time:9811ms step_avg:34.18ms
step:288/1845 train_time:9847ms step_avg:34.19ms
step:289/1845 train_time:9879ms step_avg:34.18ms
step:290/1845 train_time:9915ms step_avg:34.19ms
step:291/1845 train_time:9948ms step_avg:34.19ms
step:292/1845 train_time:9983ms step_avg:34.19ms
step:293/1845 train_time:10015ms step_avg:34.18ms
step:294/1845 train_time:10052ms step_avg:34.19ms
step:295/1845 train_time:10084ms step_avg:34.18ms
step:296/1845 train_time:10120ms step_avg:34.19ms
step:297/1845 train_time:10152ms step_avg:34.18ms
step:298/1845 train_time:10187ms step_avg:34.18ms
step:299/1845 train_time:10219ms step_avg:34.18ms
step:300/1845 train_time:10255ms step_avg:34.18ms
step:301/1845 train_time:10287ms step_avg:34.18ms
step:302/1845 train_time:10323ms step_avg:34.18ms
step:303/1845 train_time:10354ms step_avg:34.17ms
step:304/1845 train_time:10390ms step_avg:34.18ms
step:305/1845 train_time:10422ms step_avg:34.17ms
step:306/1845 train_time:10458ms step_avg:34.18ms
step:307/1845 train_time:10490ms step_avg:34.17ms
step:308/1845 train_time:10526ms step_avg:34.17ms
step:309/1845 train_time:10557ms step_avg:34.17ms
step:310/1845 train_time:10594ms step_avg:34.17ms
step:311/1845 train_time:10626ms step_avg:34.17ms
step:312/1845 train_time:10662ms step_avg:34.17ms
step:313/1845 train_time:10693ms step_avg:34.16ms
step:314/1845 train_time:10731ms step_avg:34.17ms
step:315/1845 train_time:10762ms step_avg:34.16ms
step:316/1845 train_time:10798ms step_avg:34.17ms
step:317/1845 train_time:10829ms step_avg:34.16ms
step:318/1845 train_time:10866ms step_avg:34.17ms
step:319/1845 train_time:10898ms step_avg:34.16ms
step:320/1845 train_time:10934ms step_avg:34.17ms
step:321/1845 train_time:10966ms step_avg:34.16ms
step:322/1845 train_time:11002ms step_avg:34.17ms
step:323/1845 train_time:11034ms step_avg:34.16ms
step:324/1845 train_time:11070ms step_avg:34.17ms
step:325/1845 train_time:11102ms step_avg:34.16ms
step:326/1845 train_time:11138ms step_avg:34.17ms
step:327/1845 train_time:11170ms step_avg:34.16ms
step:328/1845 train_time:11206ms step_avg:34.16ms
step:329/1845 train_time:11238ms step_avg:34.16ms
step:330/1845 train_time:11273ms step_avg:34.16ms
step:331/1845 train_time:11305ms step_avg:34.15ms
step:332/1845 train_time:11341ms step_avg:34.16ms
step:333/1845 train_time:11373ms step_avg:34.15ms
step:334/1845 train_time:11409ms step_avg:34.16ms
step:335/1845 train_time:11441ms step_avg:34.15ms
step:336/1845 train_time:11476ms step_avg:34.16ms
step:337/1845 train_time:11508ms step_avg:34.15ms
step:338/1845 train_time:11544ms step_avg:34.15ms
step:339/1845 train_time:11576ms step_avg:34.15ms
step:340/1845 train_time:11612ms step_avg:34.15ms
step:341/1845 train_time:11644ms step_avg:34.15ms
step:342/1845 train_time:11680ms step_avg:34.15ms
step:343/1845 train_time:11711ms step_avg:34.14ms
step:344/1845 train_time:11748ms step_avg:34.15ms
step:345/1845 train_time:11779ms step_avg:34.14ms
step:346/1845 train_time:11815ms step_avg:34.15ms
step:347/1845 train_time:11847ms step_avg:34.14ms
step:348/1845 train_time:11883ms step_avg:34.15ms
step:349/1845 train_time:11915ms step_avg:34.14ms
step:350/1845 train_time:11952ms step_avg:34.15ms
step:351/1845 train_time:11984ms step_avg:34.14ms
step:352/1845 train_time:12019ms step_avg:34.14ms
step:353/1845 train_time:12050ms step_avg:34.14ms
step:354/1845 train_time:12087ms step_avg:34.14ms
step:355/1845 train_time:12119ms step_avg:34.14ms
step:356/1845 train_time:12155ms step_avg:34.14ms
step:357/1845 train_time:12187ms step_avg:34.14ms
step:358/1845 train_time:12223ms step_avg:34.14ms
step:359/1845 train_time:12255ms step_avg:34.14ms
step:360/1845 train_time:12290ms step_avg:34.14ms
step:361/1845 train_time:12322ms step_avg:34.13ms
step:362/1845 train_time:12358ms step_avg:34.14ms
step:363/1845 train_time:12390ms step_avg:34.13ms
step:364/1845 train_time:12425ms step_avg:34.14ms
step:365/1845 train_time:12457ms step_avg:34.13ms
step:366/1845 train_time:12493ms step_avg:34.13ms
step:367/1845 train_time:12524ms step_avg:34.13ms
step:368/1845 train_time:12561ms step_avg:34.13ms
step:369/1845 train_time:12593ms step_avg:34.13ms
step:370/1845 train_time:12628ms step_avg:34.13ms
step:371/1845 train_time:12660ms step_avg:34.12ms
step:372/1845 train_time:12696ms step_avg:34.13ms
step:373/1845 train_time:12728ms step_avg:34.12ms
step:374/1845 train_time:12764ms step_avg:34.13ms
step:375/1845 train_time:12796ms step_avg:34.12ms
step:376/1845 train_time:12831ms step_avg:34.13ms
step:377/1845 train_time:12864ms step_avg:34.12ms
step:378/1845 train_time:12900ms step_avg:34.13ms
step:379/1845 train_time:12932ms step_avg:34.12ms
step:380/1845 train_time:12968ms step_avg:34.13ms
step:381/1845 train_time:13000ms step_avg:34.12ms
step:382/1845 train_time:13035ms step_avg:34.12ms
step:383/1845 train_time:13067ms step_avg:34.12ms
step:384/1845 train_time:13103ms step_avg:34.12ms
step:385/1845 train_time:13135ms step_avg:34.12ms
step:386/1845 train_time:13172ms step_avg:34.12ms
step:387/1845 train_time:13204ms step_avg:34.12ms
step:388/1845 train_time:13239ms step_avg:34.12ms
step:389/1845 train_time:13271ms step_avg:34.12ms
step:390/1845 train_time:13307ms step_avg:34.12ms
step:391/1845 train_time:13339ms step_avg:34.11ms
step:392/1845 train_time:13375ms step_avg:34.12ms
step:393/1845 train_time:13407ms step_avg:34.11ms
step:394/1845 train_time:13443ms step_avg:34.12ms
step:395/1845 train_time:13475ms step_avg:34.11ms
step:396/1845 train_time:13510ms step_avg:34.12ms
step:397/1845 train_time:13542ms step_avg:34.11ms
step:398/1845 train_time:13578ms step_avg:34.11ms
step:399/1845 train_time:13610ms step_avg:34.11ms
step:400/1845 train_time:13646ms step_avg:34.12ms
step:401/1845 train_time:13678ms step_avg:34.11ms
step:402/1845 train_time:13714ms step_avg:34.11ms
step:403/1845 train_time:13745ms step_avg:34.11ms
step:404/1845 train_time:13781ms step_avg:34.11ms
step:405/1845 train_time:13813ms step_avg:34.11ms
step:406/1845 train_time:13849ms step_avg:34.11ms
step:407/1845 train_time:13881ms step_avg:34.11ms
step:408/1845 train_time:13916ms step_avg:34.11ms
step:409/1845 train_time:13948ms step_avg:34.10ms
step:410/1845 train_time:13984ms step_avg:34.11ms
step:411/1845 train_time:14016ms step_avg:34.10ms
step:412/1845 train_time:14053ms step_avg:34.11ms
step:413/1845 train_time:14084ms step_avg:34.10ms
step:414/1845 train_time:14120ms step_avg:34.11ms
step:415/1845 train_time:14152ms step_avg:34.10ms
step:416/1845 train_time:14188ms step_avg:34.11ms
step:417/1845 train_time:14220ms step_avg:34.10ms
step:418/1845 train_time:14257ms step_avg:34.11ms
step:419/1845 train_time:14288ms step_avg:34.10ms
step:420/1845 train_time:14327ms step_avg:34.11ms
step:421/1845 train_time:14359ms step_avg:34.11ms
step:422/1845 train_time:14395ms step_avg:34.11ms
step:423/1845 train_time:14427ms step_avg:34.11ms
step:424/1845 train_time:14463ms step_avg:34.11ms
step:425/1845 train_time:14496ms step_avg:34.11ms
step:426/1845 train_time:14534ms step_avg:34.12ms
step:427/1845 train_time:14568ms step_avg:34.12ms
step:428/1845 train_time:14605ms step_avg:34.12ms
step:429/1845 train_time:14640ms step_avg:34.13ms
step:430/1845 train_time:14680ms step_avg:34.14ms
step:431/1845 train_time:14713ms step_avg:34.14ms
step:432/1845 train_time:14749ms step_avg:34.14ms
step:433/1845 train_time:14782ms step_avg:34.14ms
step:434/1845 train_time:14817ms step_avg:34.14ms
step:435/1845 train_time:14849ms step_avg:34.14ms
step:436/1845 train_time:14885ms step_avg:34.14ms
step:437/1845 train_time:14917ms step_avg:34.14ms
step:438/1845 train_time:14952ms step_avg:34.14ms
step:439/1845 train_time:14983ms step_avg:34.13ms
step:440/1845 train_time:15018ms step_avg:34.13ms
step:441/1845 train_time:15050ms step_avg:34.13ms
step:442/1845 train_time:15085ms step_avg:34.13ms
step:443/1845 train_time:15116ms step_avg:34.12ms
step:444/1845 train_time:15151ms step_avg:34.12ms
step:445/1845 train_time:15184ms step_avg:34.12ms
step:446/1845 train_time:15219ms step_avg:34.12ms
step:447/1845 train_time:15252ms step_avg:34.12ms
step:448/1845 train_time:15287ms step_avg:34.12ms
step:449/1845 train_time:15319ms step_avg:34.12ms
step:450/1845 train_time:15354ms step_avg:34.12ms
step:451/1845 train_time:15386ms step_avg:34.11ms
step:452/1845 train_time:15420ms step_avg:34.12ms
step:453/1845 train_time:15451ms step_avg:34.11ms
step:454/1845 train_time:15487ms step_avg:34.11ms
step:455/1845 train_time:15518ms step_avg:34.11ms
step:456/1845 train_time:15552ms step_avg:34.11ms
step:457/1845 train_time:15583ms step_avg:34.10ms
step:458/1845 train_time:15617ms step_avg:34.10ms
step:459/1845 train_time:15648ms step_avg:34.09ms
step:460/1845 train_time:15682ms step_avg:34.09ms
step:461/1845 train_time:15713ms step_avg:34.08ms
step:462/1845 train_time:15748ms step_avg:34.09ms
step:463/1845 train_time:15779ms step_avg:34.08ms
step:464/1845 train_time:15816ms step_avg:34.09ms
step:465/1845 train_time:15847ms step_avg:34.08ms
step:466/1845 train_time:15883ms step_avg:34.08ms
step:467/1845 train_time:15914ms step_avg:34.08ms
step:468/1845 train_time:15951ms step_avg:34.08ms
step:469/1845 train_time:15982ms step_avg:34.08ms
step:470/1845 train_time:16019ms step_avg:34.08ms
step:471/1845 train_time:16051ms step_avg:34.08ms
step:472/1845 train_time:16087ms step_avg:34.08ms
step:473/1845 train_time:16118ms step_avg:34.08ms
step:474/1845 train_time:16154ms step_avg:34.08ms
step:475/1845 train_time:16186ms step_avg:34.08ms
step:476/1845 train_time:16222ms step_avg:34.08ms
step:477/1845 train_time:16253ms step_avg:34.07ms
step:478/1845 train_time:16290ms step_avg:34.08ms
step:479/1845 train_time:16323ms step_avg:34.08ms
step:480/1845 train_time:16358ms step_avg:34.08ms
step:481/1845 train_time:16390ms step_avg:34.07ms
step:482/1845 train_time:16426ms step_avg:34.08ms
step:483/1845 train_time:16458ms step_avg:34.07ms
step:484/1845 train_time:16494ms step_avg:34.08ms
step:485/1845 train_time:16525ms step_avg:34.07ms
step:486/1845 train_time:16564ms step_avg:34.08ms
step:487/1845 train_time:16596ms step_avg:34.08ms
step:488/1845 train_time:16635ms step_avg:34.09ms
step:489/1845 train_time:16667ms step_avg:34.08ms
step:490/1845 train_time:16702ms step_avg:34.09ms
step:491/1845 train_time:16734ms step_avg:34.08ms
step:492/1845 train_time:16771ms step_avg:34.09ms
step:493/1845 train_time:16802ms step_avg:34.08ms
step:494/1845 train_time:16838ms step_avg:34.08ms
step:495/1845 train_time:16869ms step_avg:34.08ms
step:496/1845 train_time:16907ms step_avg:34.09ms
step:497/1845 train_time:16939ms step_avg:34.08ms
step:498/1845 train_time:16976ms step_avg:34.09ms
step:499/1845 train_time:17008ms step_avg:34.08ms
step:500/1845 train_time:17044ms step_avg:34.09ms
step:500/1845 val_loss:4.2910 train_time:17082ms step_avg:34.16ms
step:501/1845 train_time:17108ms step_avg:34.15ms
step:502/1845 train_time:17136ms step_avg:34.14ms
step:503/1845 train_time:17162ms step_avg:34.12ms
step:504/1845 train_time:17191ms step_avg:34.11ms
step:505/1845 train_time:17220ms step_avg:34.10ms
step:506/1845 train_time:17253ms step_avg:34.10ms
step:507/1845 train_time:17283ms step_avg:34.09ms
step:508/1845 train_time:17320ms step_avg:34.09ms
step:509/1845 train_time:17353ms step_avg:34.09ms
step:510/1845 train_time:17390ms step_avg:34.10ms
step:511/1845 train_time:17423ms step_avg:34.10ms
step:512/1845 train_time:17460ms step_avg:34.10ms
step:513/1845 train_time:17492ms step_avg:34.10ms
step:514/1845 train_time:17529ms step_avg:34.10ms
step:515/1845 train_time:17564ms step_avg:34.10ms
step:516/1845 train_time:17601ms step_avg:34.11ms
step:517/1845 train_time:17636ms step_avg:34.11ms
step:518/1845 train_time:17674ms step_avg:34.12ms
step:519/1845 train_time:17708ms step_avg:34.12ms
step:520/1845 train_time:17746ms step_avg:34.13ms
step:521/1845 train_time:17780ms step_avg:34.13ms
step:522/1845 train_time:17819ms step_avg:34.14ms
step:523/1845 train_time:17853ms step_avg:34.14ms
step:524/1845 train_time:17891ms step_avg:34.14ms
step:525/1845 train_time:17925ms step_avg:34.14ms
step:526/1845 train_time:17963ms step_avg:34.15ms
step:527/1845 train_time:17998ms step_avg:34.15ms
step:528/1845 train_time:18036ms step_avg:34.16ms
step:529/1845 train_time:18070ms step_avg:34.16ms
step:530/1845 train_time:18108ms step_avg:34.17ms
step:531/1845 train_time:18143ms step_avg:34.17ms
step:532/1845 train_time:18181ms step_avg:34.18ms
step:533/1845 train_time:18216ms step_avg:34.18ms
step:534/1845 train_time:18254ms step_avg:34.18ms
step:535/1845 train_time:18288ms step_avg:34.18ms
step:536/1845 train_time:18326ms step_avg:34.19ms
step:537/1845 train_time:18361ms step_avg:34.19ms
step:538/1845 train_time:18397ms step_avg:34.20ms
step:539/1845 train_time:18432ms step_avg:34.20ms
step:540/1845 train_time:18470ms step_avg:34.20ms
step:541/1845 train_time:18502ms step_avg:34.20ms
step:542/1845 train_time:18541ms step_avg:34.21ms
step:543/1845 train_time:18575ms step_avg:34.21ms
step:544/1845 train_time:18610ms step_avg:34.21ms
step:545/1845 train_time:18643ms step_avg:34.21ms
step:546/1845 train_time:18678ms step_avg:34.21ms
step:547/1845 train_time:18710ms step_avg:34.20ms
step:548/1845 train_time:18746ms step_avg:34.21ms
step:549/1845 train_time:18780ms step_avg:34.21ms
step:550/1845 train_time:18815ms step_avg:34.21ms
step:551/1845 train_time:18848ms step_avg:34.21ms
step:552/1845 train_time:18883ms step_avg:34.21ms
step:553/1845 train_time:18915ms step_avg:34.20ms
step:554/1845 train_time:18950ms step_avg:34.21ms
step:555/1845 train_time:18982ms step_avg:34.20ms
step:556/1845 train_time:19018ms step_avg:34.21ms
step:557/1845 train_time:19052ms step_avg:34.20ms
step:558/1845 train_time:19087ms step_avg:34.21ms
step:559/1845 train_time:19118ms step_avg:34.20ms
step:560/1845 train_time:19153ms step_avg:34.20ms
step:561/1845 train_time:19185ms step_avg:34.20ms
step:562/1845 train_time:19219ms step_avg:34.20ms
step:563/1845 train_time:19250ms step_avg:34.19ms
step:564/1845 train_time:19286ms step_avg:34.20ms
step:565/1845 train_time:19318ms step_avg:34.19ms
step:566/1845 train_time:19353ms step_avg:34.19ms
step:567/1845 train_time:19385ms step_avg:34.19ms
step:568/1845 train_time:19420ms step_avg:34.19ms
step:569/1845 train_time:19452ms step_avg:34.19ms
step:570/1845 train_time:19486ms step_avg:34.19ms
step:571/1845 train_time:19517ms step_avg:34.18ms
step:572/1845 train_time:19552ms step_avg:34.18ms
step:573/1845 train_time:19583ms step_avg:34.18ms
step:574/1845 train_time:19617ms step_avg:34.18ms
step:575/1845 train_time:19649ms step_avg:34.17ms
step:576/1845 train_time:19683ms step_avg:34.17ms
step:577/1845 train_time:19713ms step_avg:34.17ms
step:578/1845 train_time:19748ms step_avg:34.17ms
step:579/1845 train_time:19779ms step_avg:34.16ms
step:580/1845 train_time:19813ms step_avg:34.16ms
step:581/1845 train_time:19844ms step_avg:34.15ms
step:582/1845 train_time:19878ms step_avg:34.15ms
step:583/1845 train_time:19909ms step_avg:34.15ms
step:584/1845 train_time:19944ms step_avg:34.15ms
step:585/1845 train_time:19975ms step_avg:34.15ms
step:586/1845 train_time:20010ms step_avg:34.15ms
step:587/1845 train_time:20040ms step_avg:34.14ms
step:588/1845 train_time:20075ms step_avg:34.14ms
step:589/1845 train_time:20106ms step_avg:34.14ms
step:590/1845 train_time:20141ms step_avg:34.14ms
step:591/1845 train_time:20172ms step_avg:34.13ms
step:592/1845 train_time:20207ms step_avg:34.13ms
step:593/1845 train_time:20238ms step_avg:34.13ms
step:594/1845 train_time:20275ms step_avg:34.13ms
step:595/1845 train_time:20306ms step_avg:34.13ms
step:596/1845 train_time:20343ms step_avg:34.13ms
step:597/1845 train_time:20374ms step_avg:34.13ms
step:598/1845 train_time:20411ms step_avg:34.13ms
step:599/1845 train_time:20442ms step_avg:34.13ms
step:600/1845 train_time:20479ms step_avg:34.13ms
step:601/1845 train_time:20511ms step_avg:34.13ms
step:602/1845 train_time:20547ms step_avg:34.13ms
step:603/1845 train_time:20581ms step_avg:34.13ms
step:604/1845 train_time:20637ms step_avg:34.17ms
step:605/1845 train_time:20697ms step_avg:34.21ms
step:606/1845 train_time:20760ms step_avg:34.26ms
step:607/1845 train_time:20821ms step_avg:34.30ms
step:608/1845 train_time:20883ms step_avg:34.35ms
step:609/1845 train_time:20943ms step_avg:34.39ms
step:610/1845 train_time:21006ms step_avg:34.44ms
step:611/1845 train_time:21066ms step_avg:34.48ms
step:612/1845 train_time:21128ms step_avg:34.52ms
step:613/1845 train_time:21188ms step_avg:34.56ms
step:614/1845 train_time:21251ms step_avg:34.61ms
step:615/1845 train_time:21311ms step_avg:34.65ms
step:616/1845 train_time:21374ms step_avg:34.70ms
step:617/1845 train_time:21434ms step_avg:34.74ms
step:618/1845 train_time:21497ms step_avg:34.78ms
step:619/1845 train_time:21557ms step_avg:34.82ms
step:620/1845 train_time:21619ms step_avg:34.87ms
step:621/1845 train_time:21679ms step_avg:34.91ms
step:622/1845 train_time:21742ms step_avg:34.95ms
step:623/1845 train_time:21802ms step_avg:34.99ms
step:624/1845 train_time:21864ms step_avg:35.04ms
step:625/1845 train_time:21924ms step_avg:35.08ms
step:626/1845 train_time:21986ms step_avg:35.12ms
step:627/1845 train_time:22046ms step_avg:35.16ms
step:628/1845 train_time:22109ms step_avg:35.21ms
step:629/1845 train_time:22169ms step_avg:35.24ms
step:630/1845 train_time:22231ms step_avg:35.29ms
step:631/1845 train_time:22292ms step_avg:35.33ms
step:632/1845 train_time:22355ms step_avg:35.37ms
step:633/1845 train_time:22415ms step_avg:35.41ms
step:634/1845 train_time:22478ms step_avg:35.45ms
step:635/1845 train_time:22538ms step_avg:35.49ms
step:636/1845 train_time:22600ms step_avg:35.53ms
step:637/1845 train_time:22660ms step_avg:35.57ms
step:638/1845 train_time:22722ms step_avg:35.61ms
step:639/1845 train_time:22782ms step_avg:35.65ms
step:640/1845 train_time:22844ms step_avg:35.69ms
step:641/1845 train_time:22905ms step_avg:35.73ms
step:642/1845 train_time:22967ms step_avg:35.77ms
step:643/1845 train_time:23027ms step_avg:35.81ms
step:644/1845 train_time:23090ms step_avg:35.85ms
step:645/1845 train_time:23149ms step_avg:35.89ms
step:646/1845 train_time:23212ms step_avg:35.93ms
step:647/1845 train_time:23272ms step_avg:35.97ms
step:648/1845 train_time:23335ms step_avg:36.01ms
step:649/1845 train_time:23395ms step_avg:36.05ms
step:650/1845 train_time:23457ms step_avg:36.09ms
step:651/1845 train_time:23517ms step_avg:36.12ms
step:652/1845 train_time:23580ms step_avg:36.17ms
step:653/1845 train_time:23640ms step_avg:36.20ms
step:654/1845 train_time:23703ms step_avg:36.24ms
step:655/1845 train_time:23762ms step_avg:36.28ms
step:656/1845 train_time:23824ms step_avg:36.32ms
step:657/1845 train_time:23884ms step_avg:36.35ms
step:658/1845 train_time:23946ms step_avg:36.39ms
step:659/1845 train_time:24006ms step_avg:36.43ms
step:660/1845 train_time:24070ms step_avg:36.47ms
step:661/1845 train_time:24130ms step_avg:36.51ms
step:662/1845 train_time:24192ms step_avg:36.54ms
step:663/1845 train_time:24252ms step_avg:36.58ms
step:664/1845 train_time:24315ms step_avg:36.62ms
step:665/1845 train_time:24375ms step_avg:36.65ms
step:666/1845 train_time:24438ms step_avg:36.69ms
step:667/1845 train_time:24498ms step_avg:36.73ms
step:668/1845 train_time:24561ms step_avg:36.77ms
step:669/1845 train_time:24620ms step_avg:36.80ms
step:670/1845 train_time:24683ms step_avg:36.84ms
step:671/1845 train_time:24743ms step_avg:36.87ms
step:672/1845 train_time:24805ms step_avg:36.91ms
step:673/1845 train_time:24866ms step_avg:36.95ms
step:674/1845 train_time:24928ms step_avg:36.99ms
step:675/1845 train_time:24987ms step_avg:37.02ms
step:676/1845 train_time:25050ms step_avg:37.06ms
step:677/1845 train_time:25110ms step_avg:37.09ms
step:678/1845 train_time:25173ms step_avg:37.13ms
step:679/1845 train_time:25233ms step_avg:37.16ms
step:680/1845 train_time:25295ms step_avg:37.20ms
step:681/1845 train_time:25355ms step_avg:37.23ms
step:682/1845 train_time:25418ms step_avg:37.27ms
step:683/1845 train_time:25477ms step_avg:37.30ms
step:684/1845 train_time:25540ms step_avg:37.34ms
step:685/1845 train_time:25600ms step_avg:37.37ms
step:686/1845 train_time:25663ms step_avg:37.41ms
step:687/1845 train_time:25722ms step_avg:37.44ms
step:688/1845 train_time:25785ms step_avg:37.48ms
step:689/1845 train_time:25844ms step_avg:37.51ms
step:690/1845 train_time:25907ms step_avg:37.55ms
step:691/1845 train_time:25967ms step_avg:37.58ms
step:692/1845 train_time:26030ms step_avg:37.62ms
step:693/1845 train_time:26089ms step_avg:37.65ms
step:694/1845 train_time:26153ms step_avg:37.68ms
step:695/1845 train_time:26213ms step_avg:37.72ms
step:696/1845 train_time:26275ms step_avg:37.75ms
step:697/1845 train_time:26335ms step_avg:37.78ms
step:698/1845 train_time:26397ms step_avg:37.82ms
step:699/1845 train_time:26457ms step_avg:37.85ms
step:700/1845 train_time:26520ms step_avg:37.89ms
step:701/1845 train_time:26580ms step_avg:37.92ms
step:702/1845 train_time:26642ms step_avg:37.95ms
step:703/1845 train_time:26702ms step_avg:37.98ms
step:704/1845 train_time:26765ms step_avg:38.02ms
step:705/1845 train_time:26825ms step_avg:38.05ms
step:706/1845 train_time:26887ms step_avg:38.08ms
step:707/1845 train_time:26947ms step_avg:38.11ms
step:708/1845 train_time:27009ms step_avg:38.15ms
step:709/1845 train_time:27069ms step_avg:38.18ms
step:710/1845 train_time:27132ms step_avg:38.21ms
step:711/1845 train_time:27192ms step_avg:38.24ms
step:712/1845 train_time:27255ms step_avg:38.28ms
step:713/1845 train_time:27315ms step_avg:38.31ms
step:714/1845 train_time:27378ms step_avg:38.34ms
step:715/1845 train_time:27438ms step_avg:38.37ms
step:716/1845 train_time:27500ms step_avg:38.41ms
step:717/1845 train_time:27560ms step_avg:38.44ms
step:718/1845 train_time:27623ms step_avg:38.47ms
step:719/1845 train_time:27683ms step_avg:38.50ms
step:720/1845 train_time:27744ms step_avg:38.53ms
step:721/1845 train_time:27805ms step_avg:38.56ms
step:722/1845 train_time:27868ms step_avg:38.60ms
step:723/1845 train_time:27928ms step_avg:38.63ms
step:724/1845 train_time:27991ms step_avg:38.66ms
step:725/1845 train_time:28051ms step_avg:38.69ms
step:726/1845 train_time:28113ms step_avg:38.72ms
step:727/1845 train_time:28173ms step_avg:38.75ms
step:728/1845 train_time:28236ms step_avg:38.79ms
step:729/1845 train_time:28296ms step_avg:38.81ms
step:730/1845 train_time:28358ms step_avg:38.85ms
step:731/1845 train_time:28418ms step_avg:38.88ms
step:732/1845 train_time:28481ms step_avg:38.91ms
step:733/1845 train_time:28541ms step_avg:38.94ms
step:734/1845 train_time:28603ms step_avg:38.97ms
step:735/1845 train_time:28663ms step_avg:39.00ms
step:736/1845 train_time:28726ms step_avg:39.03ms
step:737/1845 train_time:28786ms step_avg:39.06ms
step:738/1845 train_time:28849ms step_avg:39.09ms
step:739/1845 train_time:28908ms step_avg:39.12ms
step:740/1845 train_time:28971ms step_avg:39.15ms
step:741/1845 train_time:29030ms step_avg:39.18ms
step:742/1845 train_time:29093ms step_avg:39.21ms
step:743/1845 train_time:29154ms step_avg:39.24ms
step:744/1845 train_time:29217ms step_avg:39.27ms
step:745/1845 train_time:29277ms step_avg:39.30ms
step:746/1845 train_time:29339ms step_avg:39.33ms
step:747/1845 train_time:29399ms step_avg:39.36ms
step:748/1845 train_time:29462ms step_avg:39.39ms
step:749/1845 train_time:29522ms step_avg:39.41ms
step:750/1845 train_time:29584ms step_avg:39.45ms
step:750/1845 val_loss:4.0118 train_time:29654ms step_avg:39.54ms
step:751/1845 train_time:29681ms step_avg:39.52ms
step:752/1845 train_time:29708ms step_avg:39.51ms
step:753/1845 train_time:29768ms step_avg:39.53ms
step:754/1845 train_time:29831ms step_avg:39.56ms
step:755/1845 train_time:29891ms step_avg:39.59ms
step:756/1845 train_time:29953ms step_avg:39.62ms
step:757/1845 train_time:30013ms step_avg:39.65ms
step:758/1845 train_time:30075ms step_avg:39.68ms
step:759/1845 train_time:30134ms step_avg:39.70ms
step:760/1845 train_time:30197ms step_avg:39.73ms
step:761/1845 train_time:30257ms step_avg:39.76ms
step:762/1845 train_time:30319ms step_avg:39.79ms
step:763/1845 train_time:30378ms step_avg:39.81ms
step:764/1845 train_time:30440ms step_avg:39.84ms
step:765/1845 train_time:30500ms step_avg:39.87ms
step:766/1845 train_time:30562ms step_avg:39.90ms
step:767/1845 train_time:30623ms step_avg:39.93ms
step:768/1845 train_time:30687ms step_avg:39.96ms
step:769/1845 train_time:30747ms step_avg:39.98ms
step:770/1845 train_time:30810ms step_avg:40.01ms
step:771/1845 train_time:30870ms step_avg:40.04ms
step:772/1845 train_time:30932ms step_avg:40.07ms
step:773/1845 train_time:30993ms step_avg:40.09ms
step:774/1845 train_time:31054ms step_avg:40.12ms
step:775/1845 train_time:31114ms step_avg:40.15ms
step:776/1845 train_time:31177ms step_avg:40.18ms
step:777/1845 train_time:31237ms step_avg:40.20ms
step:778/1845 train_time:31300ms step_avg:40.23ms
step:779/1845 train_time:31359ms step_avg:40.26ms
step:780/1845 train_time:31421ms step_avg:40.28ms
step:781/1845 train_time:31481ms step_avg:40.31ms
step:782/1845 train_time:31543ms step_avg:40.34ms
step:783/1845 train_time:31604ms step_avg:40.36ms
step:784/1845 train_time:31667ms step_avg:40.39ms
step:785/1845 train_time:31727ms step_avg:40.42ms
step:786/1845 train_time:31791ms step_avg:40.45ms
step:787/1845 train_time:31851ms step_avg:40.47ms
step:788/1845 train_time:31913ms step_avg:40.50ms
step:789/1845 train_time:31974ms step_avg:40.52ms
step:790/1845 train_time:32036ms step_avg:40.55ms
step:791/1845 train_time:32096ms step_avg:40.58ms
step:792/1845 train_time:32158ms step_avg:40.60ms
step:793/1845 train_time:32218ms step_avg:40.63ms
step:794/1845 train_time:32280ms step_avg:40.66ms
step:795/1845 train_time:32339ms step_avg:40.68ms
step:796/1845 train_time:32402ms step_avg:40.71ms
step:797/1845 train_time:32461ms step_avg:40.73ms
step:798/1845 train_time:32524ms step_avg:40.76ms
step:799/1845 train_time:32584ms step_avg:40.78ms
step:800/1845 train_time:32646ms step_avg:40.81ms
step:801/1845 train_time:32706ms step_avg:40.83ms
step:802/1845 train_time:32769ms step_avg:40.86ms
step:803/1845 train_time:32830ms step_avg:40.88ms
step:804/1845 train_time:32892ms step_avg:40.91ms
step:805/1845 train_time:32953ms step_avg:40.94ms
step:806/1845 train_time:33015ms step_avg:40.96ms
step:807/1845 train_time:33075ms step_avg:40.99ms
step:808/1845 train_time:33137ms step_avg:41.01ms
step:809/1845 train_time:33197ms step_avg:41.03ms
step:810/1845 train_time:33259ms step_avg:41.06ms
step:811/1845 train_time:33319ms step_avg:41.08ms
step:812/1845 train_time:33381ms step_avg:41.11ms
step:813/1845 train_time:33441ms step_avg:41.13ms
step:814/1845 train_time:33503ms step_avg:41.16ms
step:815/1845 train_time:33563ms step_avg:41.18ms
step:816/1845 train_time:33625ms step_avg:41.21ms
step:817/1845 train_time:33685ms step_avg:41.23ms
step:818/1845 train_time:33748ms step_avg:41.26ms
step:819/1845 train_time:33809ms step_avg:41.28ms
step:820/1845 train_time:33872ms step_avg:41.31ms
step:821/1845 train_time:33933ms step_avg:41.33ms
step:822/1845 train_time:33995ms step_avg:41.36ms
step:823/1845 train_time:34055ms step_avg:41.38ms
step:824/1845 train_time:34117ms step_avg:41.40ms
step:825/1845 train_time:34177ms step_avg:41.43ms
step:826/1845 train_time:34239ms step_avg:41.45ms
step:827/1845 train_time:34299ms step_avg:41.47ms
step:828/1845 train_time:34362ms step_avg:41.50ms
step:829/1845 train_time:34421ms step_avg:41.52ms
step:830/1845 train_time:34484ms step_avg:41.55ms
step:831/1845 train_time:34543ms step_avg:41.57ms
step:832/1845 train_time:34606ms step_avg:41.59ms
step:833/1845 train_time:34666ms step_avg:41.62ms
step:834/1845 train_time:34729ms step_avg:41.64ms
step:835/1845 train_time:34790ms step_avg:41.66ms
step:836/1845 train_time:34852ms step_avg:41.69ms
step:837/1845 train_time:34913ms step_avg:41.71ms
step:838/1845 train_time:34976ms step_avg:41.74ms
step:839/1845 train_time:35036ms step_avg:41.76ms
step:840/1845 train_time:35098ms step_avg:41.78ms
step:841/1845 train_time:35158ms step_avg:41.80ms
step:842/1845 train_time:35220ms step_avg:41.83ms
step:843/1845 train_time:35280ms step_avg:41.85ms
step:844/1845 train_time:35342ms step_avg:41.87ms
step:845/1845 train_time:35402ms step_avg:41.90ms
step:846/1845 train_time:35465ms step_avg:41.92ms
step:847/1845 train_time:35525ms step_avg:41.94ms
step:848/1845 train_time:35588ms step_avg:41.97ms
step:849/1845 train_time:35647ms step_avg:41.99ms
step:850/1845 train_time:35710ms step_avg:42.01ms
step:851/1845 train_time:35770ms step_avg:42.03ms
step:852/1845 train_time:35833ms step_avg:42.06ms
step:853/1845 train_time:35893ms step_avg:42.08ms
step:854/1845 train_time:35955ms step_avg:42.10ms
step:855/1845 train_time:36015ms step_avg:42.12ms
step:856/1845 train_time:36077ms step_avg:42.15ms
step:857/1845 train_time:36137ms step_avg:42.17ms
step:858/1845 train_time:36200ms step_avg:42.19ms
step:859/1845 train_time:36260ms step_avg:42.21ms
step:860/1845 train_time:36322ms step_avg:42.23ms
step:861/1845 train_time:36382ms step_avg:42.26ms
step:862/1845 train_time:36444ms step_avg:42.28ms
step:863/1845 train_time:36505ms step_avg:42.30ms
step:864/1845 train_time:36567ms step_avg:42.32ms
step:865/1845 train_time:36627ms step_avg:42.34ms
step:866/1845 train_time:36689ms step_avg:42.37ms
step:867/1845 train_time:36749ms step_avg:42.39ms
step:868/1845 train_time:36812ms step_avg:42.41ms
step:869/1845 train_time:36873ms step_avg:42.43ms
step:870/1845 train_time:36935ms step_avg:42.45ms
step:871/1845 train_time:36994ms step_avg:42.47ms
step:872/1845 train_time:37057ms step_avg:42.50ms
step:873/1845 train_time:37117ms step_avg:42.52ms
step:874/1845 train_time:37179ms step_avg:42.54ms
step:875/1845 train_time:37240ms step_avg:42.56ms
step:876/1845 train_time:37302ms step_avg:42.58ms
step:877/1845 train_time:37362ms step_avg:42.60ms
step:878/1845 train_time:37424ms step_avg:42.62ms
step:879/1845 train_time:37485ms step_avg:42.64ms
step:880/1845 train_time:37547ms step_avg:42.67ms
step:881/1845 train_time:37608ms step_avg:42.69ms
step:882/1845 train_time:37670ms step_avg:42.71ms
step:883/1845 train_time:37730ms step_avg:42.73ms
step:884/1845 train_time:37792ms step_avg:42.75ms
step:885/1845 train_time:37853ms step_avg:42.77ms
step:886/1845 train_time:37915ms step_avg:42.79ms
step:887/1845 train_time:37976ms step_avg:42.81ms
step:888/1845 train_time:38038ms step_avg:42.84ms
step:889/1845 train_time:38098ms step_avg:42.86ms
step:890/1845 train_time:38160ms step_avg:42.88ms
step:891/1845 train_time:38220ms step_avg:42.90ms
step:892/1845 train_time:38283ms step_avg:42.92ms
step:893/1845 train_time:38342ms step_avg:42.94ms
step:894/1845 train_time:38405ms step_avg:42.96ms
step:895/1845 train_time:38465ms step_avg:42.98ms
step:896/1845 train_time:38528ms step_avg:43.00ms
step:897/1845 train_time:38588ms step_avg:43.02ms
step:898/1845 train_time:38651ms step_avg:43.04ms
step:899/1845 train_time:38711ms step_avg:43.06ms
step:900/1845 train_time:38773ms step_avg:43.08ms
step:901/1845 train_time:38834ms step_avg:43.10ms
step:902/1845 train_time:38895ms step_avg:43.12ms
step:903/1845 train_time:38956ms step_avg:43.14ms
step:904/1845 train_time:39018ms step_avg:43.16ms
step:905/1845 train_time:39079ms step_avg:43.18ms
step:906/1845 train_time:39141ms step_avg:43.20ms
step:907/1845 train_time:39200ms step_avg:43.22ms
step:908/1845 train_time:39263ms step_avg:43.24ms
step:909/1845 train_time:39323ms step_avg:43.26ms
step:910/1845 train_time:39386ms step_avg:43.28ms
step:911/1845 train_time:39446ms step_avg:43.30ms
step:912/1845 train_time:39509ms step_avg:43.32ms
step:913/1845 train_time:39569ms step_avg:43.34ms
step:914/1845 train_time:39632ms step_avg:43.36ms
step:915/1845 train_time:39692ms step_avg:43.38ms
step:916/1845 train_time:39753ms step_avg:43.40ms
step:917/1845 train_time:39813ms step_avg:43.42ms
step:918/1845 train_time:39876ms step_avg:43.44ms
step:919/1845 train_time:39936ms step_avg:43.46ms
step:920/1845 train_time:39997ms step_avg:43.48ms
step:921/1845 train_time:40058ms step_avg:43.49ms
step:922/1845 train_time:40121ms step_avg:43.51ms
step:923/1845 train_time:40181ms step_avg:43.53ms
step:924/1845 train_time:40243ms step_avg:43.55ms
step:925/1845 train_time:40303ms step_avg:43.57ms
step:926/1845 train_time:40366ms step_avg:43.59ms
step:927/1845 train_time:40427ms step_avg:43.61ms
step:928/1845 train_time:40489ms step_avg:43.63ms
step:929/1845 train_time:40549ms step_avg:43.65ms
step:930/1845 train_time:40612ms step_avg:43.67ms
step:931/1845 train_time:40672ms step_avg:43.69ms
step:932/1845 train_time:40734ms step_avg:43.71ms
step:933/1845 train_time:40794ms step_avg:43.72ms
step:934/1845 train_time:40856ms step_avg:43.74ms
step:935/1845 train_time:40916ms step_avg:43.76ms
step:936/1845 train_time:40978ms step_avg:43.78ms
step:937/1845 train_time:41038ms step_avg:43.80ms
step:938/1845 train_time:41101ms step_avg:43.82ms
step:939/1845 train_time:41160ms step_avg:43.83ms
step:940/1845 train_time:41223ms step_avg:43.85ms
step:941/1845 train_time:41283ms step_avg:43.87ms
step:942/1845 train_time:41346ms step_avg:43.89ms
step:943/1845 train_time:41406ms step_avg:43.91ms
step:944/1845 train_time:41469ms step_avg:43.93ms
step:945/1845 train_time:41528ms step_avg:43.95ms
step:946/1845 train_time:41591ms step_avg:43.97ms
step:947/1845 train_time:41651ms step_avg:43.98ms
step:948/1845 train_time:41714ms step_avg:44.00ms
step:949/1845 train_time:41774ms step_avg:44.02ms
step:950/1845 train_time:41835ms step_avg:44.04ms
step:951/1845 train_time:41896ms step_avg:44.05ms
step:952/1845 train_time:41958ms step_avg:44.07ms
step:953/1845 train_time:42018ms step_avg:44.09ms
step:954/1845 train_time:42081ms step_avg:44.11ms
step:955/1845 train_time:42141ms step_avg:44.13ms
step:956/1845 train_time:42204ms step_avg:44.15ms
step:957/1845 train_time:42264ms step_avg:44.16ms
step:958/1845 train_time:42327ms step_avg:44.18ms
step:959/1845 train_time:42386ms step_avg:44.20ms
step:960/1845 train_time:42449ms step_avg:44.22ms
step:961/1845 train_time:42509ms step_avg:44.23ms
step:962/1845 train_time:42572ms step_avg:44.25ms
step:963/1845 train_time:42632ms step_avg:44.27ms
step:964/1845 train_time:42693ms step_avg:44.29ms
step:965/1845 train_time:42753ms step_avg:44.30ms
step:966/1845 train_time:42815ms step_avg:44.32ms
step:967/1845 train_time:42876ms step_avg:44.34ms
step:968/1845 train_time:42938ms step_avg:44.36ms
step:969/1845 train_time:42998ms step_avg:44.37ms
step:970/1845 train_time:43061ms step_avg:44.39ms
step:971/1845 train_time:43120ms step_avg:44.41ms
step:972/1845 train_time:43184ms step_avg:44.43ms
step:973/1845 train_time:43244ms step_avg:44.44ms
step:974/1845 train_time:43307ms step_avg:44.46ms
step:975/1845 train_time:43367ms step_avg:44.48ms
step:976/1845 train_time:43430ms step_avg:44.50ms
step:977/1845 train_time:43490ms step_avg:44.51ms
step:978/1845 train_time:43553ms step_avg:44.53ms
step:979/1845 train_time:43612ms step_avg:44.55ms
step:980/1845 train_time:43675ms step_avg:44.57ms
step:981/1845 train_time:43735ms step_avg:44.58ms
step:982/1845 train_time:43797ms step_avg:44.60ms
step:983/1845 train_time:43857ms step_avg:44.62ms
step:984/1845 train_time:43919ms step_avg:44.63ms
step:985/1845 train_time:43979ms step_avg:44.65ms
step:986/1845 train_time:44042ms step_avg:44.67ms
step:987/1845 train_time:44102ms step_avg:44.68ms
step:988/1845 train_time:44165ms step_avg:44.70ms
step:989/1845 train_time:44225ms step_avg:44.72ms
step:990/1845 train_time:44287ms step_avg:44.73ms
step:991/1845 train_time:44347ms step_avg:44.75ms
step:992/1845 train_time:44410ms step_avg:44.77ms
step:993/1845 train_time:44470ms step_avg:44.78ms
step:994/1845 train_time:44532ms step_avg:44.80ms
step:995/1845 train_time:44592ms step_avg:44.82ms
step:996/1845 train_time:44655ms step_avg:44.83ms
step:997/1845 train_time:44715ms step_avg:44.85ms
step:998/1845 train_time:44777ms step_avg:44.87ms
step:999/1845 train_time:44836ms step_avg:44.88ms
step:1000/1845 train_time:44899ms step_avg:44.90ms
step:1000/1845 val_loss:3.7761 train_time:44969ms step_avg:44.97ms
step:1001/1845 train_time:44995ms step_avg:44.95ms
step:1002/1845 train_time:45023ms step_avg:44.93ms
step:1003/1845 train_time:45082ms step_avg:44.95ms
step:1004/1845 train_time:45147ms step_avg:44.97ms
step:1005/1845 train_time:45208ms step_avg:44.98ms
step:1006/1845 train_time:45271ms step_avg:45.00ms
step:1007/1845 train_time:45331ms step_avg:45.02ms
step:1008/1845 train_time:45393ms step_avg:45.03ms
step:1009/1845 train_time:45453ms step_avg:45.05ms
step:1010/1845 train_time:45515ms step_avg:45.06ms
step:1011/1845 train_time:45575ms step_avg:45.08ms
step:1012/1845 train_time:45637ms step_avg:45.10ms
step:1013/1845 train_time:45696ms step_avg:45.11ms
step:1014/1845 train_time:45758ms step_avg:45.13ms
step:1015/1845 train_time:45817ms step_avg:45.14ms
step:1016/1845 train_time:45882ms step_avg:45.16ms
step:1017/1845 train_time:45943ms step_avg:45.18ms
step:1018/1845 train_time:46005ms step_avg:45.19ms
step:1019/1845 train_time:46066ms step_avg:45.21ms
step:1020/1845 train_time:46129ms step_avg:45.22ms
step:1021/1845 train_time:46190ms step_avg:45.24ms
step:1022/1845 train_time:46253ms step_avg:45.26ms
step:1023/1845 train_time:46313ms step_avg:45.27ms
step:1024/1845 train_time:46375ms step_avg:45.29ms
step:1025/1845 train_time:46435ms step_avg:45.30ms
step:1026/1845 train_time:46497ms step_avg:45.32ms
step:1027/1845 train_time:46556ms step_avg:45.33ms
step:1028/1845 train_time:46619ms step_avg:45.35ms
step:1029/1845 train_time:46679ms step_avg:45.36ms
step:1030/1845 train_time:46741ms step_avg:45.38ms
step:1031/1845 train_time:46800ms step_avg:45.39ms
step:1032/1845 train_time:46863ms step_avg:45.41ms
step:1033/1845 train_time:46924ms step_avg:45.42ms
step:1034/1845 train_time:46987ms step_avg:45.44ms
step:1035/1845 train_time:47047ms step_avg:45.46ms
step:1036/1845 train_time:47110ms step_avg:45.47ms
step:1037/1845 train_time:47170ms step_avg:45.49ms
step:1038/1845 train_time:47233ms step_avg:45.50ms
step:1039/1845 train_time:47292ms step_avg:45.52ms
step:1040/1845 train_time:47356ms step_avg:45.53ms
step:1041/1845 train_time:47415ms step_avg:45.55ms
step:1042/1845 train_time:47478ms step_avg:45.56ms
step:1043/1845 train_time:47537ms step_avg:45.58ms
step:1044/1845 train_time:47600ms step_avg:45.59ms
step:1045/1845 train_time:47659ms step_avg:45.61ms
step:1046/1845 train_time:47722ms step_avg:45.62ms
step:1047/1845 train_time:47781ms step_avg:45.64ms
step:1048/1845 train_time:47843ms step_avg:45.65ms
step:1049/1845 train_time:47904ms step_avg:45.67ms
step:1050/1845 train_time:47966ms step_avg:45.68ms
step:1051/1845 train_time:48027ms step_avg:45.70ms
step:1052/1845 train_time:48090ms step_avg:45.71ms
step:1053/1845 train_time:48150ms step_avg:45.73ms
step:1054/1845 train_time:48212ms step_avg:45.74ms
step:1055/1845 train_time:48273ms step_avg:45.76ms
step:1056/1845 train_time:48335ms step_avg:45.77ms
step:1057/1845 train_time:48395ms step_avg:45.79ms
step:1058/1845 train_time:48458ms step_avg:45.80ms
step:1059/1845 train_time:48518ms step_avg:45.81ms
step:1060/1845 train_time:48580ms step_avg:45.83ms
step:1061/1845 train_time:48639ms step_avg:45.84ms
step:1062/1845 train_time:48701ms step_avg:45.86ms
step:1063/1845 train_time:48761ms step_avg:45.87ms
step:1064/1845 train_time:48824ms step_avg:45.89ms
step:1065/1845 train_time:48884ms step_avg:45.90ms
step:1066/1845 train_time:48946ms step_avg:45.92ms
step:1067/1845 train_time:49006ms step_avg:45.93ms
step:1068/1845 train_time:49069ms step_avg:45.94ms
step:1069/1845 train_time:49129ms step_avg:45.96ms
step:1070/1845 train_time:49192ms step_avg:45.97ms
step:1071/1845 train_time:49252ms step_avg:45.99ms
step:1072/1845 train_time:49314ms step_avg:46.00ms
step:1073/1845 train_time:49374ms step_avg:46.02ms
step:1074/1845 train_time:49437ms step_avg:46.03ms
step:1075/1845 train_time:49497ms step_avg:46.04ms
step:1076/1845 train_time:49559ms step_avg:46.06ms
step:1077/1845 train_time:49619ms step_avg:46.07ms
step:1078/1845 train_time:49681ms step_avg:46.09ms
step:1079/1845 train_time:49741ms step_avg:46.10ms
step:1080/1845 train_time:49803ms step_avg:46.11ms
step:1081/1845 train_time:49864ms step_avg:46.13ms
step:1082/1845 train_time:49926ms step_avg:46.14ms
step:1083/1845 train_time:49986ms step_avg:46.15ms
step:1084/1845 train_time:50048ms step_avg:46.17ms
step:1085/1845 train_time:50109ms step_avg:46.18ms
step:1086/1845 train_time:50172ms step_avg:46.20ms
step:1087/1845 train_time:50232ms step_avg:46.21ms
step:1088/1845 train_time:50295ms step_avg:46.23ms
step:1089/1845 train_time:50355ms step_avg:46.24ms
step:1090/1845 train_time:50418ms step_avg:46.26ms
step:1091/1845 train_time:50478ms step_avg:46.27ms
step:1092/1845 train_time:50541ms step_avg:46.28ms
step:1093/1845 train_time:50601ms step_avg:46.30ms
step:1094/1845 train_time:50663ms step_avg:46.31ms
step:1095/1845 train_time:50722ms step_avg:46.32ms
step:1096/1845 train_time:50785ms step_avg:46.34ms
step:1097/1845 train_time:50845ms step_avg:46.35ms
step:1098/1845 train_time:50907ms step_avg:46.36ms
step:1099/1845 train_time:50967ms step_avg:46.38ms
step:1100/1845 train_time:51030ms step_avg:46.39ms
step:1101/1845 train_time:51089ms step_avg:46.40ms
step:1102/1845 train_time:51152ms step_avg:46.42ms
step:1103/1845 train_time:51212ms step_avg:46.43ms
step:1104/1845 train_time:51275ms step_avg:46.44ms
step:1105/1845 train_time:51336ms step_avg:46.46ms
step:1106/1845 train_time:51398ms step_avg:46.47ms
step:1107/1845 train_time:51458ms step_avg:46.48ms
step:1108/1845 train_time:51520ms step_avg:46.50ms
step:1109/1845 train_time:51580ms step_avg:46.51ms
step:1110/1845 train_time:51642ms step_avg:46.52ms
step:1111/1845 train_time:51702ms step_avg:46.54ms
step:1112/1845 train_time:51765ms step_avg:46.55ms
step:1113/1845 train_time:51824ms step_avg:46.56ms
step:1114/1845 train_time:51887ms step_avg:46.58ms
step:1115/1845 train_time:51947ms step_avg:46.59ms
step:1116/1845 train_time:52010ms step_avg:46.60ms
step:1117/1845 train_time:52069ms step_avg:46.62ms
step:1118/1845 train_time:52133ms step_avg:46.63ms
step:1119/1845 train_time:52192ms step_avg:46.64ms
step:1120/1845 train_time:52255ms step_avg:46.66ms
step:1121/1845 train_time:52315ms step_avg:46.67ms
step:1122/1845 train_time:52378ms step_avg:46.68ms
step:1123/1845 train_time:52438ms step_avg:46.69ms
step:1124/1845 train_time:52501ms step_avg:46.71ms
step:1125/1845 train_time:52561ms step_avg:46.72ms
step:1126/1845 train_time:52624ms step_avg:46.74ms
step:1127/1845 train_time:52683ms step_avg:46.75ms
step:1128/1845 train_time:52745ms step_avg:46.76ms
step:1129/1845 train_time:52805ms step_avg:46.77ms
step:1130/1845 train_time:52867ms step_avg:46.79ms
step:1131/1845 train_time:52928ms step_avg:46.80ms
step:1132/1845 train_time:52990ms step_avg:46.81ms
step:1133/1845 train_time:53050ms step_avg:46.82ms
step:1134/1845 train_time:53113ms step_avg:46.84ms
step:1135/1845 train_time:53172ms step_avg:46.85ms
step:1136/1845 train_time:53235ms step_avg:46.86ms
step:1137/1845 train_time:53295ms step_avg:46.87ms
step:1138/1845 train_time:53358ms step_avg:46.89ms
step:1139/1845 train_time:53419ms step_avg:46.90ms
step:1140/1845 train_time:53481ms step_avg:46.91ms
step:1141/1845 train_time:53541ms step_avg:46.92ms
step:1142/1845 train_time:53609ms step_avg:46.94ms
step:1143/1845 train_time:53664ms step_avg:46.95ms
step:1144/1845 train_time:53727ms step_avg:46.96ms
step:1145/1845 train_time:53786ms step_avg:46.97ms
step:1146/1845 train_time:53849ms step_avg:46.99ms
step:1147/1845 train_time:53909ms step_avg:47.00ms
step:1148/1845 train_time:53971ms step_avg:47.01ms
step:1149/1845 train_time:54031ms step_avg:47.02ms
step:1150/1845 train_time:54093ms step_avg:47.04ms
step:1151/1845 train_time:54152ms step_avg:47.05ms
step:1152/1845 train_time:54215ms step_avg:47.06ms
step:1153/1845 train_time:54275ms step_avg:47.07ms
step:1154/1845 train_time:54338ms step_avg:47.09ms
step:1155/1845 train_time:54398ms step_avg:47.10ms
step:1156/1845 train_time:54460ms step_avg:47.11ms
step:1157/1845 train_time:54520ms step_avg:47.12ms
step:1158/1845 train_time:54583ms step_avg:47.14ms
step:1159/1845 train_time:54643ms step_avg:47.15ms
step:1160/1845 train_time:54705ms step_avg:47.16ms
step:1161/1845 train_time:54765ms step_avg:47.17ms
step:1162/1845 train_time:54828ms step_avg:47.18ms
step:1163/1845 train_time:54887ms step_avg:47.19ms
step:1164/1845 train_time:54949ms step_avg:47.21ms
step:1165/1845 train_time:55009ms step_avg:47.22ms
step:1166/1845 train_time:55072ms step_avg:47.23ms
step:1167/1845 train_time:55132ms step_avg:47.24ms
step:1168/1845 train_time:55194ms step_avg:47.26ms
step:1169/1845 train_time:55255ms step_avg:47.27ms
step:1170/1845 train_time:55318ms step_avg:47.28ms
step:1171/1845 train_time:55378ms step_avg:47.29ms
step:1172/1845 train_time:55441ms step_avg:47.30ms
step:1173/1845 train_time:55501ms step_avg:47.32ms
step:1174/1845 train_time:55563ms step_avg:47.33ms
step:1175/1845 train_time:55624ms step_avg:47.34ms
step:1176/1845 train_time:55686ms step_avg:47.35ms
step:1177/1845 train_time:55746ms step_avg:47.36ms
step:1178/1845 train_time:55808ms step_avg:47.38ms
step:1179/1845 train_time:55868ms step_avg:47.39ms
step:1180/1845 train_time:55930ms step_avg:47.40ms
step:1181/1845 train_time:55989ms step_avg:47.41ms
step:1182/1845 train_time:56052ms step_avg:47.42ms
step:1183/1845 train_time:56112ms step_avg:47.43ms
step:1184/1845 train_time:56175ms step_avg:47.44ms
step:1185/1845 train_time:56235ms step_avg:47.46ms
step:1186/1845 train_time:56297ms step_avg:47.47ms
step:1187/1845 train_time:56357ms step_avg:47.48ms
step:1188/1845 train_time:56419ms step_avg:47.49ms
step:1189/1845 train_time:56479ms step_avg:47.50ms
step:1190/1845 train_time:56542ms step_avg:47.51ms
step:1191/1845 train_time:56603ms step_avg:47.53ms
step:1192/1845 train_time:56665ms step_avg:47.54ms
step:1193/1845 train_time:56724ms step_avg:47.55ms
step:1194/1845 train_time:56787ms step_avg:47.56ms
step:1195/1845 train_time:56847ms step_avg:47.57ms
step:1196/1845 train_time:56909ms step_avg:47.58ms
step:1197/1845 train_time:56969ms step_avg:47.59ms
step:1198/1845 train_time:57031ms step_avg:47.61ms
step:1199/1845 train_time:57091ms step_avg:47.62ms
step:1200/1845 train_time:57154ms step_avg:47.63ms
step:1201/1845 train_time:57214ms step_avg:47.64ms
step:1202/1845 train_time:57277ms step_avg:47.65ms
step:1203/1845 train_time:57337ms step_avg:47.66ms
step:1204/1845 train_time:57400ms step_avg:47.67ms
step:1205/1845 train_time:57461ms step_avg:47.69ms
step:1206/1845 train_time:57548ms step_avg:47.72ms
step:1207/1845 train_time:57636ms step_avg:47.75ms
step:1208/1845 train_time:57726ms step_avg:47.79ms
step:1209/1845 train_time:57813ms step_avg:47.82ms
step:1210/1845 train_time:57901ms step_avg:47.85ms
step:1211/1845 train_time:57987ms step_avg:47.88ms
step:1212/1845 train_time:58075ms step_avg:47.92ms
step:1213/1845 train_time:58161ms step_avg:47.95ms
step:1214/1845 train_time:58251ms step_avg:47.98ms
step:1215/1845 train_time:58338ms step_avg:48.01ms
step:1216/1845 train_time:58428ms step_avg:48.05ms
step:1217/1845 train_time:58515ms step_avg:48.08ms
step:1218/1845 train_time:58603ms step_avg:48.11ms
step:1219/1845 train_time:58689ms step_avg:48.15ms
step:1220/1845 train_time:58778ms step_avg:48.18ms
step:1221/1845 train_time:58864ms step_avg:48.21ms
step:1222/1845 train_time:58953ms step_avg:48.24ms
step:1223/1845 train_time:59039ms step_avg:48.27ms
step:1224/1845 train_time:59128ms step_avg:48.31ms
step:1225/1845 train_time:59213ms step_avg:48.34ms
step:1226/1845 train_time:59302ms step_avg:48.37ms
step:1227/1845 train_time:59390ms step_avg:48.40ms
step:1228/1845 train_time:59478ms step_avg:48.43ms
step:1229/1845 train_time:59564ms step_avg:48.47ms
step:1230/1845 train_time:59653ms step_avg:48.50ms
step:1231/1845 train_time:59739ms step_avg:48.53ms
step:1232/1845 train_time:59829ms step_avg:48.56ms
step:1233/1845 train_time:59916ms step_avg:48.59ms
step:1234/1845 train_time:60005ms step_avg:48.63ms
step:1235/1845 train_time:60091ms step_avg:48.66ms
step:1236/1845 train_time:60179ms step_avg:48.69ms
step:1237/1845 train_time:60266ms step_avg:48.72ms
step:1238/1845 train_time:60355ms step_avg:48.75ms
step:1239/1845 train_time:60441ms step_avg:48.78ms
step:1240/1845 train_time:60531ms step_avg:48.82ms
step:1241/1845 train_time:60617ms step_avg:48.85ms
step:1242/1845 train_time:60705ms step_avg:48.88ms
step:1243/1845 train_time:60792ms step_avg:48.91ms
step:1244/1845 train_time:60880ms step_avg:48.94ms
step:1245/1845 train_time:60967ms step_avg:48.97ms
step:1246/1845 train_time:61056ms step_avg:49.00ms
step:1247/1845 train_time:61141ms step_avg:49.03ms
step:1248/1845 train_time:61231ms step_avg:49.06ms
step:1249/1845 train_time:61318ms step_avg:49.09ms
step:1250/1845 train_time:61408ms step_avg:49.13ms
step:1250/1845 val_loss:3.5329 train_time:61506ms step_avg:49.20ms
step:1251/1845 train_time:61537ms step_avg:49.19ms
step:1252/1845 train_time:61587ms step_avg:49.19ms
step:1253/1845 train_time:61671ms step_avg:49.22ms
step:1254/1845 train_time:61761ms step_avg:49.25ms
step:1255/1845 train_time:61847ms step_avg:49.28ms
step:1256/1845 train_time:61935ms step_avg:49.31ms
step:1257/1845 train_time:62021ms step_avg:49.34ms
step:1258/1845 train_time:62110ms step_avg:49.37ms
step:1259/1845 train_time:62195ms step_avg:49.40ms
step:1260/1845 train_time:62284ms step_avg:49.43ms
step:1261/1845 train_time:62369ms step_avg:49.46ms
step:1262/1845 train_time:62461ms step_avg:49.49ms
step:1263/1845 train_time:62549ms step_avg:49.52ms
step:1264/1845 train_time:62638ms step_avg:49.56ms
step:1265/1845 train_time:62725ms step_avg:49.59ms
step:1266/1845 train_time:62815ms step_avg:49.62ms
step:1267/1845 train_time:62900ms step_avg:49.65ms
step:1268/1845 train_time:62989ms step_avg:49.68ms
step:1269/1845 train_time:63074ms step_avg:49.70ms
step:1270/1845 train_time:63163ms step_avg:49.73ms
step:1271/1845 train_time:63249ms step_avg:49.76ms
step:1272/1845 train_time:63337ms step_avg:49.79ms
step:1273/1845 train_time:63424ms step_avg:49.82ms
step:1274/1845 train_time:63515ms step_avg:49.85ms
step:1275/1845 train_time:63600ms step_avg:49.88ms
step:1276/1845 train_time:63691ms step_avg:49.91ms
step:1277/1845 train_time:63777ms step_avg:49.94ms
step:1278/1845 train_time:63865ms step_avg:49.97ms
step:1279/1845 train_time:63951ms step_avg:50.00ms
step:1280/1845 train_time:64038ms step_avg:50.03ms
step:1281/1845 train_time:64125ms step_avg:50.06ms
step:1282/1845 train_time:64214ms step_avg:50.09ms
step:1283/1845 train_time:64299ms step_avg:50.12ms
step:1284/1845 train_time:64389ms step_avg:50.15ms
step:1285/1845 train_time:64476ms step_avg:50.18ms
step:1286/1845 train_time:64567ms step_avg:50.21ms
step:1287/1845 train_time:64653ms step_avg:50.24ms
step:1288/1845 train_time:64742ms step_avg:50.27ms
step:1289/1845 train_time:64829ms step_avg:50.29ms
step:1290/1845 train_time:64917ms step_avg:50.32ms
step:1291/1845 train_time:65003ms step_avg:50.35ms
step:1292/1845 train_time:65092ms step_avg:50.38ms
step:1293/1845 train_time:65178ms step_avg:50.41ms
step:1294/1845 train_time:65269ms step_avg:50.44ms
step:1295/1845 train_time:65355ms step_avg:50.47ms
step:1296/1845 train_time:65444ms step_avg:50.50ms
step:1297/1845 train_time:65531ms step_avg:50.52ms
step:1298/1845 train_time:65619ms step_avg:50.55ms
step:1299/1845 train_time:65706ms step_avg:50.58ms
step:1300/1845 train_time:65795ms step_avg:50.61ms
step:1301/1845 train_time:65880ms step_avg:50.64ms
step:1302/1845 train_time:65969ms step_avg:50.67ms
step:1303/1845 train_time:66056ms step_avg:50.70ms
step:1304/1845 train_time:66144ms step_avg:50.72ms
step:1305/1845 train_time:66230ms step_avg:50.75ms
step:1306/1845 train_time:66318ms step_avg:50.78ms
step:1307/1845 train_time:66406ms step_avg:50.81ms
step:1308/1845 train_time:66495ms step_avg:50.84ms
step:1309/1845 train_time:66581ms step_avg:50.86ms
step:1310/1845 train_time:66670ms step_avg:50.89ms
step:1311/1845 train_time:66756ms step_avg:50.92ms
step:1312/1845 train_time:66846ms step_avg:50.95ms
step:1313/1845 train_time:66932ms step_avg:50.98ms
step:1314/1845 train_time:67019ms step_avg:51.00ms
step:1315/1845 train_time:67106ms step_avg:51.03ms
step:1316/1845 train_time:67195ms step_avg:51.06ms
step:1317/1845 train_time:67280ms step_avg:51.09ms
step:1318/1845 train_time:67369ms step_avg:51.11ms
step:1319/1845 train_time:67456ms step_avg:51.14ms
step:1320/1845 train_time:67545ms step_avg:51.17ms
step:1321/1845 train_time:67632ms step_avg:51.20ms
step:1322/1845 train_time:67720ms step_avg:51.23ms
step:1323/1845 train_time:67807ms step_avg:51.25ms
step:1324/1845 train_time:67897ms step_avg:51.28ms
step:1325/1845 train_time:67983ms step_avg:51.31ms
step:1326/1845 train_time:68071ms step_avg:51.34ms
step:1327/1845 train_time:68157ms step_avg:51.36ms
step:1328/1845 train_time:68246ms step_avg:51.39ms
step:1329/1845 train_time:68334ms step_avg:51.42ms
step:1330/1845 train_time:68422ms step_avg:51.45ms
step:1331/1845 train_time:68509ms step_avg:51.47ms
step:1332/1845 train_time:68597ms step_avg:51.50ms
step:1333/1845 train_time:68684ms step_avg:51.53ms
step:1334/1845 train_time:68773ms step_avg:51.55ms
step:1335/1845 train_time:68859ms step_avg:51.58ms
step:1336/1845 train_time:68949ms step_avg:51.61ms
step:1337/1845 train_time:69035ms step_avg:51.63ms
step:1338/1845 train_time:69123ms step_avg:51.66ms
step:1339/1845 train_time:69208ms step_avg:51.69ms
step:1340/1845 train_time:69298ms step_avg:51.71ms
step:1341/1845 train_time:69384ms step_avg:51.74ms
step:1342/1845 train_time:69473ms step_avg:51.77ms
step:1343/1845 train_time:69559ms step_avg:51.79ms
step:1344/1845 train_time:69648ms step_avg:51.82ms
step:1345/1845 train_time:69735ms step_avg:51.85ms
step:1346/1845 train_time:69824ms step_avg:51.88ms
step:1347/1845 train_time:69910ms step_avg:51.90ms
step:1348/1845 train_time:69999ms step_avg:51.93ms
step:1349/1845 train_time:70086ms step_avg:51.95ms
step:1350/1845 train_time:70174ms step_avg:51.98ms
step:1351/1845 train_time:70260ms step_avg:52.01ms
step:1352/1845 train_time:70349ms step_avg:52.03ms
step:1353/1845 train_time:70436ms step_avg:52.06ms
step:1354/1845 train_time:70525ms step_avg:52.09ms
step:1355/1845 train_time:70611ms step_avg:52.11ms
step:1356/1845 train_time:70699ms step_avg:52.14ms
step:1357/1845 train_time:70786ms step_avg:52.16ms
step:1358/1845 train_time:70875ms step_avg:52.19ms
step:1359/1845 train_time:70961ms step_avg:52.22ms
step:1360/1845 train_time:71051ms step_avg:52.24ms
step:1361/1845 train_time:71136ms step_avg:52.27ms
step:1362/1845 train_time:71225ms step_avg:52.29ms
step:1363/1845 train_time:71311ms step_avg:52.32ms
step:1364/1845 train_time:71399ms step_avg:52.35ms
step:1365/1845 train_time:71486ms step_avg:52.37ms
step:1366/1845 train_time:71574ms step_avg:52.40ms
step:1367/1845 train_time:71661ms step_avg:52.42ms
step:1368/1845 train_time:71750ms step_avg:52.45ms
step:1369/1845 train_time:71837ms step_avg:52.47ms
step:1370/1845 train_time:71925ms step_avg:52.50ms
step:1371/1845 train_time:72011ms step_avg:52.52ms
step:1372/1845 train_time:72099ms step_avg:52.55ms
step:1373/1845 train_time:72186ms step_avg:52.58ms
step:1374/1845 train_time:72275ms step_avg:52.60ms
step:1375/1845 train_time:72361ms step_avg:52.63ms
step:1376/1845 train_time:72451ms step_avg:52.65ms
step:1377/1845 train_time:72538ms step_avg:52.68ms
step:1378/1845 train_time:72628ms step_avg:52.71ms
step:1379/1845 train_time:72713ms step_avg:52.73ms
step:1380/1845 train_time:72802ms step_avg:52.76ms
step:1381/1845 train_time:72888ms step_avg:52.78ms
step:1382/1845 train_time:72976ms step_avg:52.80ms
step:1383/1845 train_time:73064ms step_avg:52.83ms
step:1384/1845 train_time:73152ms step_avg:52.86ms
step:1385/1845 train_time:73238ms step_avg:52.88ms
step:1386/1845 train_time:73328ms step_avg:52.91ms
step:1387/1845 train_time:73414ms step_avg:52.93ms
step:1388/1845 train_time:73503ms step_avg:52.96ms
step:1389/1845 train_time:73589ms step_avg:52.98ms
step:1390/1845 train_time:73677ms step_avg:53.01ms
step:1391/1845 train_time:73764ms step_avg:53.03ms
step:1392/1845 train_time:73853ms step_avg:53.06ms
step:1393/1845 train_time:73939ms step_avg:53.08ms
step:1394/1845 train_time:74029ms step_avg:53.11ms
step:1395/1845 train_time:74116ms step_avg:53.13ms
step:1396/1845 train_time:74204ms step_avg:53.15ms
step:1397/1845 train_time:74291ms step_avg:53.18ms
step:1398/1845 train_time:74379ms step_avg:53.20ms
step:1399/1845 train_time:74466ms step_avg:53.23ms
step:1400/1845 train_time:74554ms step_avg:53.25ms
step:1401/1845 train_time:74640ms step_avg:53.28ms
step:1402/1845 train_time:74731ms step_avg:53.30ms
step:1403/1845 train_time:74817ms step_avg:53.33ms
step:1404/1845 train_time:74905ms step_avg:53.35ms
step:1405/1845 train_time:74991ms step_avg:53.37ms
step:1406/1845 train_time:75078ms step_avg:53.40ms
step:1407/1845 train_time:75166ms step_avg:53.42ms
step:1408/1845 train_time:75255ms step_avg:53.45ms
step:1409/1845 train_time:75340ms step_avg:53.47ms
step:1410/1845 train_time:75431ms step_avg:53.50ms
step:1411/1845 train_time:75518ms step_avg:53.52ms
step:1412/1845 train_time:75607ms step_avg:53.55ms
step:1413/1845 train_time:75693ms step_avg:53.57ms
step:1414/1845 train_time:75780ms step_avg:53.59ms
step:1415/1845 train_time:75867ms step_avg:53.62ms
step:1416/1845 train_time:75955ms step_avg:53.64ms
step:1417/1845 train_time:76041ms step_avg:53.66ms
step:1418/1845 train_time:76131ms step_avg:53.69ms
step:1419/1845 train_time:76218ms step_avg:53.71ms
step:1420/1845 train_time:76307ms step_avg:53.74ms
step:1421/1845 train_time:76393ms step_avg:53.76ms
step:1422/1845 train_time:76482ms step_avg:53.78ms
step:1423/1845 train_time:76569ms step_avg:53.81ms
step:1424/1845 train_time:76656ms step_avg:53.83ms
step:1425/1845 train_time:76743ms step_avg:53.85ms
step:1426/1845 train_time:76833ms step_avg:53.88ms
step:1427/1845 train_time:76918ms step_avg:53.90ms
step:1428/1845 train_time:77007ms step_avg:53.93ms
step:1429/1845 train_time:77092ms step_avg:53.95ms
step:1430/1845 train_time:77181ms step_avg:53.97ms
step:1431/1845 train_time:77269ms step_avg:54.00ms
step:1432/1845 train_time:77357ms step_avg:54.02ms
step:1433/1845 train_time:77443ms step_avg:54.04ms
step:1434/1845 train_time:77532ms step_avg:54.07ms
step:1435/1845 train_time:77618ms step_avg:54.09ms
step:1436/1845 train_time:77707ms step_avg:54.11ms
step:1437/1845 train_time:77792ms step_avg:54.14ms
step:1438/1845 train_time:77880ms step_avg:54.16ms
step:1439/1845 train_time:77967ms step_avg:54.18ms
step:1440/1845 train_time:78056ms step_avg:54.21ms
step:1441/1845 train_time:78142ms step_avg:54.23ms
step:1442/1845 train_time:78233ms step_avg:54.25ms
step:1443/1845 train_time:78318ms step_avg:54.27ms
step:1444/1845 train_time:78408ms step_avg:54.30ms
step:1445/1845 train_time:78494ms step_avg:54.32ms
step:1446/1845 train_time:78582ms step_avg:54.34ms
step:1447/1845 train_time:78670ms step_avg:54.37ms
step:1448/1845 train_time:78758ms step_avg:54.39ms
step:1449/1845 train_time:78845ms step_avg:54.41ms
step:1450/1845 train_time:78934ms step_avg:54.44ms
step:1451/1845 train_time:79019ms step_avg:54.46ms
step:1452/1845 train_time:79110ms step_avg:54.48ms
step:1453/1845 train_time:79195ms step_avg:54.50ms
step:1454/1845 train_time:79284ms step_avg:54.53ms
step:1455/1845 train_time:79371ms step_avg:54.55ms
step:1456/1845 train_time:79459ms step_avg:54.57ms
step:1457/1845 train_time:79546ms step_avg:54.60ms
step:1458/1845 train_time:79636ms step_avg:54.62ms
step:1459/1845 train_time:79723ms step_avg:54.64ms
step:1460/1845 train_time:79812ms step_avg:54.67ms
step:1461/1845 train_time:79897ms step_avg:54.69ms
step:1462/1845 train_time:79986ms step_avg:54.71ms
step:1463/1845 train_time:80073ms step_avg:54.73ms
step:1464/1845 train_time:80161ms step_avg:54.75ms
step:1465/1845 train_time:80246ms step_avg:54.78ms
step:1466/1845 train_time:80335ms step_avg:54.80ms
step:1467/1845 train_time:80421ms step_avg:54.82ms
step:1468/1845 train_time:80511ms step_avg:54.84ms
step:1469/1845 train_time:80597ms step_avg:54.86ms
step:1470/1845 train_time:80685ms step_avg:54.89ms
step:1471/1845 train_time:80772ms step_avg:54.91ms
step:1472/1845 train_time:80860ms step_avg:54.93ms
step:1473/1845 train_time:80947ms step_avg:54.95ms
step:1474/1845 train_time:81036ms step_avg:54.98ms
step:1475/1845 train_time:81122ms step_avg:55.00ms
step:1476/1845 train_time:81212ms step_avg:55.02ms
step:1477/1845 train_time:81297ms step_avg:55.04ms
step:1478/1845 train_time:81386ms step_avg:55.07ms
step:1479/1845 train_time:81473ms step_avg:55.09ms
step:1480/1845 train_time:81562ms step_avg:55.11ms
step:1481/1845 train_time:81648ms step_avg:55.13ms
step:1482/1845 train_time:81736ms step_avg:55.15ms
step:1483/1845 train_time:81824ms step_avg:55.17ms
step:1484/1845 train_time:81914ms step_avg:55.20ms
step:1485/1845 train_time:81999ms step_avg:55.22ms
step:1486/1845 train_time:82088ms step_avg:55.24ms
step:1487/1845 train_time:82174ms step_avg:55.26ms
step:1488/1845 train_time:82264ms step_avg:55.28ms
step:1489/1845 train_time:82349ms step_avg:55.31ms
step:1490/1845 train_time:82438ms step_avg:55.33ms
step:1491/1845 train_time:82524ms step_avg:55.35ms
step:1492/1845 train_time:82614ms step_avg:55.37ms
step:1493/1845 train_time:82700ms step_avg:55.39ms
step:1494/1845 train_time:82790ms step_avg:55.41ms
step:1495/1845 train_time:82876ms step_avg:55.44ms
step:1496/1845 train_time:82966ms step_avg:55.46ms
step:1497/1845 train_time:83053ms step_avg:55.48ms
step:1498/1845 train_time:83141ms step_avg:55.50ms
step:1499/1845 train_time:83228ms step_avg:55.52ms
step:1500/1845 train_time:83316ms step_avg:55.54ms
step:1500/1845 val_loss:3.4019 train_time:83412ms step_avg:55.61ms
step:1501/1845 train_time:83440ms step_avg:55.59ms
step:1502/1845 train_time:83492ms step_avg:55.59ms
step:1503/1845 train_time:83580ms step_avg:55.61ms
step:1504/1845 train_time:83668ms step_avg:55.63ms
step:1505/1845 train_time:83754ms step_avg:55.65ms
step:1506/1845 train_time:83841ms step_avg:55.67ms
step:1507/1845 train_time:83927ms step_avg:55.69ms
step:1508/1845 train_time:84016ms step_avg:55.71ms
step:1509/1845 train_time:84101ms step_avg:55.73ms
step:1510/1845 train_time:84190ms step_avg:55.75ms
step:1511/1845 train_time:84275ms step_avg:55.77ms
step:1512/1845 train_time:84365ms step_avg:55.80ms
step:1513/1845 train_time:84453ms step_avg:55.82ms
step:1514/1845 train_time:84544ms step_avg:55.84ms
step:1515/1845 train_time:84632ms step_avg:55.86ms
step:1516/1845 train_time:84720ms step_avg:55.88ms
step:1517/1845 train_time:84806ms step_avg:55.90ms
step:1518/1845 train_time:84894ms step_avg:55.92ms
step:1519/1845 train_time:84980ms step_avg:55.94ms
step:1520/1845 train_time:85068ms step_avg:55.97ms
step:1521/1845 train_time:85152ms step_avg:55.98ms
step:1522/1845 train_time:85241ms step_avg:56.01ms
step:1523/1845 train_time:85329ms step_avg:56.03ms
step:1524/1845 train_time:85419ms step_avg:56.05ms
step:1525/1845 train_time:85507ms step_avg:56.07ms
step:1526/1845 train_time:85596ms step_avg:56.09ms
step:1527/1845 train_time:85681ms step_avg:56.11ms
step:1528/1845 train_time:85771ms step_avg:56.13ms
step:1529/1845 train_time:85858ms step_avg:56.15ms
step:1530/1845 train_time:85946ms step_avg:56.17ms
step:1531/1845 train_time:86031ms step_avg:56.19ms
step:1532/1845 train_time:86119ms step_avg:56.21ms
step:1533/1845 train_time:86204ms step_avg:56.23ms
step:1534/1845 train_time:86295ms step_avg:56.25ms
step:1535/1845 train_time:86380ms step_avg:56.27ms
step:1536/1845 train_time:86471ms step_avg:56.30ms
step:1537/1845 train_time:86557ms step_avg:56.32ms
step:1538/1845 train_time:86648ms step_avg:56.34ms
step:1539/1845 train_time:86734ms step_avg:56.36ms
step:1540/1845 train_time:86822ms step_avg:56.38ms
step:1541/1845 train_time:86909ms step_avg:56.40ms
step:1542/1845 train_time:86998ms step_avg:56.42ms
step:1543/1845 train_time:87082ms step_avg:56.44ms
step:1544/1845 train_time:87173ms step_avg:56.46ms
step:1545/1845 train_time:87259ms step_avg:56.48ms
step:1546/1845 train_time:87348ms step_avg:56.50ms
step:1547/1845 train_time:87436ms step_avg:56.52ms
step:1548/1845 train_time:87524ms step_avg:56.54ms
step:1549/1845 train_time:87612ms step_avg:56.56ms
step:1550/1845 train_time:87700ms step_avg:56.58ms
step:1551/1845 train_time:87786ms step_avg:56.60ms
step:1552/1845 train_time:87876ms step_avg:56.62ms
step:1553/1845 train_time:87961ms step_avg:56.64ms
step:1554/1845 train_time:88050ms step_avg:56.66ms
step:1555/1845 train_time:88136ms step_avg:56.68ms
step:1556/1845 train_time:88226ms step_avg:56.70ms
step:1557/1845 train_time:88311ms step_avg:56.72ms
step:1558/1845 train_time:88400ms step_avg:56.74ms
step:1559/1845 train_time:88486ms step_avg:56.76ms
step:1560/1845 train_time:88576ms step_avg:56.78ms
step:1561/1845 train_time:88662ms step_avg:56.80ms
step:1562/1845 train_time:88752ms step_avg:56.82ms
step:1563/1845 train_time:88839ms step_avg:56.84ms
step:1564/1845 train_time:88928ms step_avg:56.86ms
step:1565/1845 train_time:89012ms step_avg:56.88ms
step:1566/1845 train_time:89100ms step_avg:56.90ms
step:1567/1845 train_time:89186ms step_avg:56.92ms
step:1568/1845 train_time:89276ms step_avg:56.94ms
step:1569/1845 train_time:89362ms step_avg:56.95ms
step:1570/1845 train_time:89451ms step_avg:56.98ms
step:1571/1845 train_time:89538ms step_avg:56.99ms
step:1572/1845 train_time:89627ms step_avg:57.01ms
step:1573/1845 train_time:89714ms step_avg:57.03ms
step:1574/1845 train_time:89802ms step_avg:57.05ms
step:1575/1845 train_time:89888ms step_avg:57.07ms
step:1576/1845 train_time:89977ms step_avg:57.09ms
step:1577/1845 train_time:90063ms step_avg:57.11ms
step:1578/1845 train_time:90152ms step_avg:57.13ms
step:1579/1845 train_time:90239ms step_avg:57.15ms
step:1580/1845 train_time:90328ms step_avg:57.17ms
step:1581/1845 train_time:90414ms step_avg:57.19ms
step:1582/1845 train_time:90502ms step_avg:57.21ms
step:1583/1845 train_time:90589ms step_avg:57.23ms
step:1584/1845 train_time:90679ms step_avg:57.25ms
step:1585/1845 train_time:90765ms step_avg:57.27ms
step:1586/1845 train_time:90854ms step_avg:57.28ms
step:1587/1845 train_time:90940ms step_avg:57.30ms
step:1588/1845 train_time:91029ms step_avg:57.32ms
step:1589/1845 train_time:91114ms step_avg:57.34ms
step:1590/1845 train_time:91203ms step_avg:57.36ms
step:1591/1845 train_time:91289ms step_avg:57.38ms
step:1592/1845 train_time:91379ms step_avg:57.40ms
step:1593/1845 train_time:91465ms step_avg:57.42ms
step:1594/1845 train_time:91554ms step_avg:57.44ms
step:1595/1845 train_time:91641ms step_avg:57.46ms
step:1596/1845 train_time:91731ms step_avg:57.48ms
step:1597/1845 train_time:91817ms step_avg:57.49ms
step:1598/1845 train_time:91906ms step_avg:57.51ms
step:1599/1845 train_time:91992ms step_avg:57.53ms
step:1600/1845 train_time:92081ms step_avg:57.55ms
step:1601/1845 train_time:92167ms step_avg:57.57ms
step:1602/1845 train_time:92257ms step_avg:57.59ms
step:1603/1845 train_time:92342ms step_avg:57.61ms
step:1604/1845 train_time:92431ms step_avg:57.63ms
step:1605/1845 train_time:92518ms step_avg:57.64ms
step:1606/1845 train_time:92607ms step_avg:57.66ms
step:1607/1845 train_time:92692ms step_avg:57.68ms
step:1608/1845 train_time:92780ms step_avg:57.70ms
step:1609/1845 train_time:92867ms step_avg:57.72ms
step:1610/1845 train_time:92957ms step_avg:57.74ms
step:1611/1845 train_time:93043ms step_avg:57.75ms
step:1612/1845 train_time:93132ms step_avg:57.77ms
step:1613/1845 train_time:93217ms step_avg:57.79ms
step:1614/1845 train_time:93306ms step_avg:57.81ms
step:1615/1845 train_time:93392ms step_avg:57.83ms
step:1616/1845 train_time:93480ms step_avg:57.85ms
step:1617/1845 train_time:93566ms step_avg:57.86ms
step:1618/1845 train_time:93656ms step_avg:57.88ms
step:1619/1845 train_time:93743ms step_avg:57.90ms
step:1620/1845 train_time:93833ms step_avg:57.92ms
step:1621/1845 train_time:93919ms step_avg:57.94ms
step:1622/1845 train_time:94007ms step_avg:57.96ms
step:1623/1845 train_time:94093ms step_avg:57.97ms
step:1624/1845 train_time:94181ms step_avg:57.99ms
step:1625/1845 train_time:94267ms step_avg:58.01ms
step:1626/1845 train_time:94356ms step_avg:58.03ms
step:1627/1845 train_time:94442ms step_avg:58.05ms
step:1628/1845 train_time:94531ms step_avg:58.07ms
step:1629/1845 train_time:94617ms step_avg:58.08ms
step:1630/1845 train_time:94706ms step_avg:58.10ms
step:1631/1845 train_time:94791ms step_avg:58.12ms
step:1632/1845 train_time:94880ms step_avg:58.14ms
step:1633/1845 train_time:94967ms step_avg:58.15ms
step:1634/1845 train_time:95056ms step_avg:58.17ms
step:1635/1845 train_time:95143ms step_avg:58.19ms
step:1636/1845 train_time:95231ms step_avg:58.21ms
step:1637/1845 train_time:95317ms step_avg:58.23ms
step:1638/1845 train_time:95407ms step_avg:58.25ms
step:1639/1845 train_time:95494ms step_avg:58.26ms
step:1640/1845 train_time:95582ms step_avg:58.28ms
step:1641/1845 train_time:95669ms step_avg:58.30ms
step:1642/1845 train_time:95757ms step_avg:58.32ms
step:1643/1845 train_time:95843ms step_avg:58.33ms
step:1644/1845 train_time:95932ms step_avg:58.35ms
step:1645/1845 train_time:96018ms step_avg:58.37ms
step:1646/1845 train_time:96107ms step_avg:58.39ms
step:1647/1845 train_time:96194ms step_avg:58.41ms
step:1648/1845 train_time:96282ms step_avg:58.42ms
step:1649/1845 train_time:96367ms step_avg:58.44ms
step:1650/1845 train_time:96457ms step_avg:58.46ms
step:1651/1845 train_time:96542ms step_avg:58.48ms
step:1652/1845 train_time:96633ms step_avg:58.49ms
step:1653/1845 train_time:96720ms step_avg:58.51ms
step:1654/1845 train_time:96810ms step_avg:58.53ms
step:1655/1845 train_time:96895ms step_avg:58.55ms
step:1656/1845 train_time:96983ms step_avg:58.56ms
step:1657/1845 train_time:97071ms step_avg:58.58ms
step:1658/1845 train_time:97159ms step_avg:58.60ms
step:1659/1845 train_time:97245ms step_avg:58.62ms
step:1660/1845 train_time:97336ms step_avg:58.64ms
step:1661/1845 train_time:97422ms step_avg:58.65ms
step:1662/1845 train_time:97511ms step_avg:58.67ms
step:1663/1845 train_time:97597ms step_avg:58.69ms
step:1664/1845 train_time:97686ms step_avg:58.71ms
step:1665/1845 train_time:97772ms step_avg:58.72ms
step:1666/1845 train_time:97861ms step_avg:58.74ms
step:1667/1845 train_time:97946ms step_avg:58.76ms
step:1668/1845 train_time:98037ms step_avg:58.77ms
step:1669/1845 train_time:98121ms step_avg:58.79ms
step:1670/1845 train_time:98211ms step_avg:58.81ms
step:1671/1845 train_time:98298ms step_avg:58.83ms
step:1672/1845 train_time:98387ms step_avg:58.84ms
step:1673/1845 train_time:98473ms step_avg:58.86ms
step:1674/1845 train_time:98561ms step_avg:58.88ms
step:1675/1845 train_time:98647ms step_avg:58.89ms
step:1676/1845 train_time:98736ms step_avg:58.91ms
step:1677/1845 train_time:98822ms step_avg:58.93ms
step:1678/1845 train_time:98911ms step_avg:58.95ms
step:1679/1845 train_time:98998ms step_avg:58.96ms
step:1680/1845 train_time:99086ms step_avg:58.98ms
step:1681/1845 train_time:99172ms step_avg:59.00ms
step:1682/1845 train_time:99261ms step_avg:59.01ms
step:1683/1845 train_time:99347ms step_avg:59.03ms
step:1684/1845 train_time:99437ms step_avg:59.05ms
step:1685/1845 train_time:99522ms step_avg:59.06ms
step:1686/1845 train_time:99613ms step_avg:59.08ms
step:1687/1845 train_time:99699ms step_avg:59.10ms
step:1688/1845 train_time:99788ms step_avg:59.12ms
step:1689/1845 train_time:99873ms step_avg:59.13ms
step:1690/1845 train_time:99962ms step_avg:59.15ms
step:1691/1845 train_time:100049ms step_avg:59.17ms
step:1692/1845 train_time:100138ms step_avg:59.18ms
step:1693/1845 train_time:100224ms step_avg:59.20ms
step:1694/1845 train_time:100314ms step_avg:59.22ms
step:1695/1845 train_time:100400ms step_avg:59.23ms
step:1696/1845 train_time:100489ms step_avg:59.25ms
step:1697/1845 train_time:100575ms step_avg:59.27ms
step:1698/1845 train_time:100663ms step_avg:59.28ms
step:1699/1845 train_time:100750ms step_avg:59.30ms
step:1700/1845 train_time:100839ms step_avg:59.32ms
step:1701/1845 train_time:100925ms step_avg:59.33ms
step:1702/1845 train_time:101015ms step_avg:59.35ms
step:1703/1845 train_time:101101ms step_avg:59.37ms
step:1704/1845 train_time:101190ms step_avg:59.38ms
step:1705/1845 train_time:101276ms step_avg:59.40ms
step:1706/1845 train_time:101364ms step_avg:59.42ms
step:1707/1845 train_time:101450ms step_avg:59.43ms
step:1708/1845 train_time:101540ms step_avg:59.45ms
step:1709/1845 train_time:101625ms step_avg:59.46ms
step:1710/1845 train_time:101714ms step_avg:59.48ms
step:1711/1845 train_time:101800ms step_avg:59.50ms
step:1712/1845 train_time:101890ms step_avg:59.52ms
step:1713/1845 train_time:101977ms step_avg:59.53ms
step:1714/1845 train_time:102065ms step_avg:59.55ms
step:1715/1845 train_time:102151ms step_avg:59.56ms
step:1716/1845 train_time:102240ms step_avg:59.58ms
step:1717/1845 train_time:102326ms step_avg:59.60ms
step:1718/1845 train_time:102416ms step_avg:59.61ms
step:1719/1845 train_time:102501ms step_avg:59.63ms
step:1720/1845 train_time:102590ms step_avg:59.65ms
step:1721/1845 train_time:102676ms step_avg:59.66ms
step:1722/1845 train_time:102764ms step_avg:59.68ms
step:1723/1845 train_time:102851ms step_avg:59.69ms
step:1724/1845 train_time:102940ms step_avg:59.71ms
step:1725/1845 train_time:103028ms step_avg:59.73ms
step:1726/1845 train_time:103117ms step_avg:59.74ms
step:1727/1845 train_time:103203ms step_avg:59.76ms
step:1728/1845 train_time:103292ms step_avg:59.78ms
step:1729/1845 train_time:103378ms step_avg:59.79ms
step:1730/1845 train_time:103467ms step_avg:59.81ms
step:1731/1845 train_time:103553ms step_avg:59.82ms
step:1732/1845 train_time:103641ms step_avg:59.84ms
step:1733/1845 train_time:103726ms step_avg:59.85ms
step:1734/1845 train_time:103817ms step_avg:59.87ms
step:1735/1845 train_time:103903ms step_avg:59.89ms
step:1736/1845 train_time:103993ms step_avg:59.90ms
step:1737/1845 train_time:104080ms step_avg:59.92ms
step:1738/1845 train_time:104169ms step_avg:59.94ms
step:1739/1845 train_time:104254ms step_avg:59.95ms
step:1740/1845 train_time:104343ms step_avg:59.97ms
step:1741/1845 train_time:104429ms step_avg:59.98ms
step:1742/1845 train_time:104519ms step_avg:60.00ms
step:1743/1845 train_time:104605ms step_avg:60.01ms
step:1744/1845 train_time:104693ms step_avg:60.03ms
step:1745/1845 train_time:104779ms step_avg:60.05ms
step:1746/1845 train_time:104868ms step_avg:60.06ms
step:1747/1845 train_time:104955ms step_avg:60.08ms
step:1748/1845 train_time:105043ms step_avg:60.09ms
step:1749/1845 train_time:105130ms step_avg:60.11ms
step:1750/1845 train_time:105219ms step_avg:60.13ms
step:1750/1845 val_loss:3.3034 train_time:105316ms step_avg:60.18ms
step:1751/1845 train_time:105344ms step_avg:60.16ms
step:1752/1845 train_time:105398ms step_avg:60.16ms
step:1753/1845 train_time:105485ms step_avg:60.17ms
step:1754/1845 train_time:105576ms step_avg:60.19ms
step:1755/1845 train_time:105661ms step_avg:60.21ms
step:1756/1845 train_time:105749ms step_avg:60.22ms
step:1757/1845 train_time:105835ms step_avg:60.24ms
step:1758/1845 train_time:105925ms step_avg:60.25ms
step:1759/1845 train_time:106010ms step_avg:60.27ms
step:1760/1845 train_time:106098ms step_avg:60.28ms
step:1761/1845 train_time:106184ms step_avg:60.30ms
step:1762/1845 train_time:106273ms step_avg:60.31ms
step:1763/1845 train_time:106360ms step_avg:60.33ms
step:1764/1845 train_time:106450ms step_avg:60.35ms
step:1765/1845 train_time:106537ms step_avg:60.36ms
step:1766/1845 train_time:106626ms step_avg:60.38ms
step:1767/1845 train_time:106711ms step_avg:60.39ms
step:1768/1845 train_time:106801ms step_avg:60.41ms
step:1769/1845 train_time:106888ms step_avg:60.42ms
step:1770/1845 train_time:106977ms step_avg:60.44ms
step:1771/1845 train_time:107063ms step_avg:60.45ms
step:1772/1845 train_time:107151ms step_avg:60.47ms
step:1773/1845 train_time:107237ms step_avg:60.48ms
step:1774/1845 train_time:107327ms step_avg:60.50ms
step:1775/1845 train_time:107413ms step_avg:60.51ms
step:1776/1845 train_time:107503ms step_avg:60.53ms
step:1777/1845 train_time:107590ms step_avg:60.55ms
step:1778/1845 train_time:107679ms step_avg:60.56ms
step:1779/1845 train_time:107766ms step_avg:60.58ms
step:1780/1845 train_time:107854ms step_avg:60.59ms
step:1781/1845 train_time:107941ms step_avg:60.61ms
step:1782/1845 train_time:108028ms step_avg:60.62ms
step:1783/1845 train_time:108114ms step_avg:60.64ms
step:1784/1845 train_time:108204ms step_avg:60.65ms
step:1785/1845 train_time:108290ms step_avg:60.67ms
step:1786/1845 train_time:108381ms step_avg:60.68ms
step:1787/1845 train_time:108466ms step_avg:60.70ms
step:1788/1845 train_time:108557ms step_avg:60.71ms
step:1789/1845 train_time:108644ms step_avg:60.73ms
step:1790/1845 train_time:108731ms step_avg:60.74ms
step:1791/1845 train_time:108818ms step_avg:60.76ms
step:1792/1845 train_time:108906ms step_avg:60.77ms
step:1793/1845 train_time:108991ms step_avg:60.79ms
step:1794/1845 train_time:109081ms step_avg:60.80ms
step:1795/1845 train_time:109167ms step_avg:60.82ms
step:1796/1845 train_time:109256ms step_avg:60.83ms
step:1797/1845 train_time:109342ms step_avg:60.85ms
step:1798/1845 train_time:109430ms step_avg:60.86ms
step:1799/1845 train_time:109517ms step_avg:60.88ms
step:1800/1845 train_time:109606ms step_avg:60.89ms
step:1801/1845 train_time:109692ms step_avg:60.91ms
step:1802/1845 train_time:109782ms step_avg:60.92ms
step:1803/1845 train_time:109868ms step_avg:60.94ms
step:1804/1845 train_time:109958ms step_avg:60.95ms
step:1805/1845 train_time:110044ms step_avg:60.97ms
step:1806/1845 train_time:110134ms step_avg:60.98ms
step:1807/1845 train_time:110219ms step_avg:61.00ms
step:1808/1845 train_time:110308ms step_avg:61.01ms
step:1809/1845 train_time:110394ms step_avg:61.02ms
step:1810/1845 train_time:110483ms step_avg:61.04ms
step:1811/1845 train_time:110569ms step_avg:61.05ms
step:1812/1845 train_time:110658ms step_avg:61.07ms
step:1813/1845 train_time:110745ms step_avg:61.08ms
step:1814/1845 train_time:110834ms step_avg:61.10ms
step:1815/1845 train_time:110922ms step_avg:61.11ms
step:1816/1845 train_time:111010ms step_avg:61.13ms
step:1817/1845 train_time:111096ms step_avg:61.14ms
step:1818/1845 train_time:111186ms step_avg:61.16ms
step:1819/1845 train_time:111270ms step_avg:61.17ms
step:1820/1845 train_time:111361ms step_avg:61.19ms
step:1821/1845 train_time:111447ms step_avg:61.20ms
step:1822/1845 train_time:111537ms step_avg:61.22ms
step:1823/1845 train_time:111624ms step_avg:61.23ms
step:1824/1845 train_time:111712ms step_avg:61.25ms
step:1825/1845 train_time:111798ms step_avg:61.26ms
step:1826/1845 train_time:111887ms step_avg:61.27ms
step:1827/1845 train_time:111974ms step_avg:61.29ms
step:1828/1845 train_time:112064ms step_avg:61.30ms
step:1829/1845 train_time:112151ms step_avg:61.32ms
step:1830/1845 train_time:112240ms step_avg:61.33ms
step:1831/1845 train_time:112325ms step_avg:61.35ms
step:1832/1845 train_time:112415ms step_avg:61.36ms
step:1833/1845 train_time:112502ms step_avg:61.38ms
step:1834/1845 train_time:112591ms step_avg:61.39ms
step:1835/1845 train_time:112678ms step_avg:61.40ms
step:1836/1845 train_time:112766ms step_avg:61.42ms
step:1837/1845 train_time:112852ms step_avg:61.43ms
step:1838/1845 train_time:112941ms step_avg:61.45ms
step:1839/1845 train_time:113027ms step_avg:61.46ms
step:1840/1845 train_time:113117ms step_avg:61.48ms
step:1841/1845 train_time:113204ms step_avg:61.49ms
step:1842/1845 train_time:113292ms step_avg:61.51ms
step:1843/1845 train_time:113379ms step_avg:61.52ms
step:1844/1845 train_time:113466ms step_avg:61.53ms
step:1845/1845 train_time:113554ms step_avg:61.55ms
step:1845/1845 val_loss:3.2769 train_time:113650ms step_avg:61.60ms
peak memory allocated: 29801 MiB reserved: 45518 MiB

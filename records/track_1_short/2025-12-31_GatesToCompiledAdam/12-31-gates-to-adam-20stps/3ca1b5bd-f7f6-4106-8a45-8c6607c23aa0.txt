import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1785  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 18:45:24 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            125W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    300823      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    300824      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    300825      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    300826      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    300827      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    300828      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    300829      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    300830      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 594, 595, 596, 1189, 1190, 1191, 1784, 1785, 1786] for warmup
Resetting Model
step:0/1825 val_loss:10.8301 train_time:0ms step_avg:0.03ms
step:1/1825 train_time:96ms step_avg:96.50ms
step:2/1825 train_time:117ms step_avg:58.56ms
step:3/1825 train_time:136ms step_avg:45.25ms
step:4/1825 train_time:171ms step_avg:42.75ms
step:5/1825 train_time:204ms step_avg:40.78ms
step:6/1825 train_time:285ms step_avg:47.57ms
step:7/1825 train_time:302ms step_avg:43.07ms
step:8/1825 train_time:356ms step_avg:44.44ms
step:9/1825 train_time:388ms step_avg:43.16ms
step:10/1825 train_time:424ms step_avg:42.35ms
step:11/1825 train_time:457ms step_avg:41.51ms
step:12/1825 train_time:492ms step_avg:41.00ms
step:13/1825 train_time:525ms step_avg:40.38ms
step:14/1825 train_time:560ms step_avg:40.02ms
step:15/1825 train_time:594ms step_avg:39.57ms
step:16/1825 train_time:629ms step_avg:39.31ms
step:17/1825 train_time:662ms step_avg:38.94ms
step:18/1825 train_time:697ms step_avg:38.73ms
step:19/1825 train_time:730ms step_avg:38.42ms
step:20/1825 train_time:765ms step_avg:38.26ms
step:21/1825 train_time:798ms step_avg:38.02ms
step:22/1825 train_time:834ms step_avg:37.90ms
step:23/1825 train_time:867ms step_avg:37.68ms
step:24/1825 train_time:902ms step_avg:37.58ms
step:25/1825 train_time:935ms step_avg:37.40ms
step:26/1825 train_time:970ms step_avg:37.32ms
step:27/1825 train_time:1003ms step_avg:37.16ms
step:28/1825 train_time:1039ms step_avg:37.10ms
step:29/1825 train_time:1072ms step_avg:36.96ms
step:30/1825 train_time:1107ms step_avg:36.90ms
step:31/1825 train_time:1140ms step_avg:36.78ms
step:32/1825 train_time:1175ms step_avg:36.73ms
step:33/1825 train_time:1208ms step_avg:36.62ms
step:34/1825 train_time:1244ms step_avg:36.59ms
step:35/1825 train_time:1277ms step_avg:36.49ms
step:36/1825 train_time:1313ms step_avg:36.46ms
step:37/1825 train_time:1346ms step_avg:36.37ms
step:38/1825 train_time:1381ms step_avg:36.35ms
step:39/1825 train_time:1415ms step_avg:36.27ms
step:40/1825 train_time:1450ms step_avg:36.25ms
step:41/1825 train_time:1483ms step_avg:36.17ms
step:42/1825 train_time:1518ms step_avg:36.15ms
step:43/1825 train_time:1551ms step_avg:36.08ms
step:44/1825 train_time:1587ms step_avg:36.06ms
step:45/1825 train_time:1620ms step_avg:35.99ms
step:46/1825 train_time:1655ms step_avg:35.98ms
step:47/1825 train_time:1689ms step_avg:35.93ms
step:48/1825 train_time:1724ms step_avg:35.92ms
step:49/1825 train_time:1757ms step_avg:35.86ms
step:50/1825 train_time:1792ms step_avg:35.85ms
step:51/1825 train_time:1825ms step_avg:35.79ms
step:52/1825 train_time:1860ms step_avg:35.78ms
step:53/1825 train_time:1894ms step_avg:35.73ms
step:54/1825 train_time:1929ms step_avg:35.72ms
step:55/1825 train_time:1962ms step_avg:35.67ms
step:56/1825 train_time:1997ms step_avg:35.67ms
step:57/1825 train_time:2030ms step_avg:35.62ms
step:58/1825 train_time:2066ms step_avg:35.61ms
step:59/1825 train_time:2099ms step_avg:35.57ms
step:60/1825 train_time:2134ms step_avg:35.56ms
step:61/1825 train_time:2167ms step_avg:35.52ms
step:62/1825 train_time:2202ms step_avg:35.52ms
step:63/1825 train_time:2235ms step_avg:35.48ms
step:64/1825 train_time:2270ms step_avg:35.47ms
step:65/1825 train_time:2303ms step_avg:35.44ms
step:66/1825 train_time:2339ms step_avg:35.44ms
step:67/1825 train_time:2372ms step_avg:35.40ms
step:68/1825 train_time:2407ms step_avg:35.40ms
step:69/1825 train_time:2440ms step_avg:35.36ms
step:70/1825 train_time:2475ms step_avg:35.36ms
step:71/1825 train_time:2508ms step_avg:35.33ms
step:72/1825 train_time:2544ms step_avg:35.33ms
step:73/1825 train_time:2577ms step_avg:35.30ms
step:74/1825 train_time:2612ms step_avg:35.30ms
step:75/1825 train_time:2645ms step_avg:35.27ms
step:76/1825 train_time:2680ms step_avg:35.27ms
step:77/1825 train_time:2713ms step_avg:35.24ms
step:78/1825 train_time:2749ms step_avg:35.24ms
step:79/1825 train_time:2782ms step_avg:35.21ms
step:80/1825 train_time:2817ms step_avg:35.22ms
step:81/1825 train_time:2850ms step_avg:35.19ms
step:82/1825 train_time:2886ms step_avg:35.19ms
step:83/1825 train_time:2919ms step_avg:35.17ms
step:84/1825 train_time:2954ms step_avg:35.17ms
step:85/1825 train_time:2987ms step_avg:35.15ms
step:86/1825 train_time:3023ms step_avg:35.15ms
step:87/1825 train_time:3056ms step_avg:35.12ms
step:88/1825 train_time:3091ms step_avg:35.12ms
step:89/1825 train_time:3124ms step_avg:35.10ms
step:90/1825 train_time:3159ms step_avg:35.10ms
step:91/1825 train_time:3192ms step_avg:35.08ms
step:92/1825 train_time:3227ms step_avg:35.08ms
step:93/1825 train_time:3260ms step_avg:35.06ms
step:94/1825 train_time:3296ms step_avg:35.06ms
step:95/1825 train_time:3329ms step_avg:35.04ms
step:96/1825 train_time:3364ms step_avg:35.04ms
step:97/1825 train_time:3397ms step_avg:35.02ms
step:98/1825 train_time:3432ms step_avg:35.02ms
step:99/1825 train_time:3466ms step_avg:35.01ms
step:100/1825 train_time:3501ms step_avg:35.01ms
step:101/1825 train_time:3534ms step_avg:34.99ms
step:102/1825 train_time:3569ms step_avg:34.99ms
step:103/1825 train_time:3602ms step_avg:34.97ms
step:104/1825 train_time:3637ms step_avg:34.97ms
step:105/1825 train_time:3670ms step_avg:34.96ms
step:106/1825 train_time:3705ms step_avg:34.96ms
step:107/1825 train_time:3738ms step_avg:34.94ms
step:108/1825 train_time:3774ms step_avg:34.94ms
step:109/1825 train_time:3807ms step_avg:34.92ms
step:110/1825 train_time:3842ms step_avg:34.93ms
step:111/1825 train_time:3875ms step_avg:34.91ms
step:112/1825 train_time:3911ms step_avg:34.92ms
step:113/1825 train_time:3943ms step_avg:34.90ms
step:114/1825 train_time:3979ms step_avg:34.90ms
step:115/1825 train_time:4012ms step_avg:34.88ms
step:116/1825 train_time:4047ms step_avg:34.89ms
step:117/1825 train_time:4080ms step_avg:34.87ms
step:118/1825 train_time:4115ms step_avg:34.88ms
step:119/1825 train_time:4148ms step_avg:34.86ms
step:120/1825 train_time:4184ms step_avg:34.86ms
step:121/1825 train_time:4217ms step_avg:34.85ms
step:122/1825 train_time:4252ms step_avg:34.85ms
step:123/1825 train_time:4285ms step_avg:34.84ms
step:124/1825 train_time:4320ms step_avg:34.84ms
step:125/1825 train_time:4353ms step_avg:34.83ms
step:126/1825 train_time:4389ms step_avg:34.83ms
step:127/1825 train_time:4422ms step_avg:34.82ms
step:128/1825 train_time:4457ms step_avg:34.82ms
step:129/1825 train_time:4490ms step_avg:34.81ms
step:130/1825 train_time:4526ms step_avg:34.81ms
step:131/1825 train_time:4559ms step_avg:34.80ms
step:132/1825 train_time:4594ms step_avg:34.80ms
step:133/1825 train_time:4627ms step_avg:34.79ms
step:134/1825 train_time:4662ms step_avg:34.79ms
step:135/1825 train_time:4695ms step_avg:34.78ms
step:136/1825 train_time:4731ms step_avg:34.78ms
step:137/1825 train_time:4764ms step_avg:34.77ms
step:138/1825 train_time:4799ms step_avg:34.78ms
step:139/1825 train_time:4832ms step_avg:34.76ms
step:140/1825 train_time:4867ms step_avg:34.77ms
step:141/1825 train_time:4900ms step_avg:34.75ms
step:142/1825 train_time:4936ms step_avg:34.76ms
step:143/1825 train_time:4969ms step_avg:34.75ms
step:144/1825 train_time:5004ms step_avg:34.75ms
step:145/1825 train_time:5037ms step_avg:34.74ms
step:146/1825 train_time:5072ms step_avg:34.74ms
step:147/1825 train_time:5105ms step_avg:34.73ms
step:148/1825 train_time:5141ms step_avg:34.74ms
step:149/1825 train_time:5174ms step_avg:34.72ms
step:150/1825 train_time:5209ms step_avg:34.73ms
step:151/1825 train_time:5242ms step_avg:34.71ms
step:152/1825 train_time:5277ms step_avg:34.72ms
step:153/1825 train_time:5310ms step_avg:34.71ms
step:154/1825 train_time:5345ms step_avg:34.71ms
step:155/1825 train_time:5378ms step_avg:34.70ms
step:156/1825 train_time:5413ms step_avg:34.70ms
step:157/1825 train_time:5446ms step_avg:34.69ms
step:158/1825 train_time:5482ms step_avg:34.69ms
step:159/1825 train_time:5515ms step_avg:34.68ms
step:160/1825 train_time:5550ms step_avg:34.69ms
step:161/1825 train_time:5583ms step_avg:34.68ms
step:162/1825 train_time:5618ms step_avg:34.68ms
step:163/1825 train_time:5651ms step_avg:34.67ms
step:164/1825 train_time:5686ms step_avg:34.67ms
step:165/1825 train_time:5719ms step_avg:34.66ms
step:166/1825 train_time:5755ms step_avg:34.67ms
step:167/1825 train_time:5788ms step_avg:34.66ms
step:168/1825 train_time:5823ms step_avg:34.66ms
step:169/1825 train_time:5856ms step_avg:34.65ms
step:170/1825 train_time:5891ms step_avg:34.65ms
step:171/1825 train_time:5924ms step_avg:34.64ms
step:172/1825 train_time:5959ms step_avg:34.65ms
step:173/1825 train_time:5992ms step_avg:34.64ms
step:174/1825 train_time:6028ms step_avg:34.64ms
step:175/1825 train_time:6061ms step_avg:34.63ms
step:176/1825 train_time:6096ms step_avg:34.64ms
step:177/1825 train_time:6129ms step_avg:34.63ms
step:178/1825 train_time:6164ms step_avg:34.63ms
step:179/1825 train_time:6197ms step_avg:34.62ms
step:180/1825 train_time:6232ms step_avg:34.62ms
step:181/1825 train_time:6265ms step_avg:34.61ms
step:182/1825 train_time:6300ms step_avg:34.62ms
step:183/1825 train_time:6333ms step_avg:34.61ms
step:184/1825 train_time:6369ms step_avg:34.61ms
step:185/1825 train_time:6402ms step_avg:34.60ms
step:186/1825 train_time:6437ms step_avg:34.61ms
step:187/1825 train_time:6470ms step_avg:34.60ms
step:188/1825 train_time:6505ms step_avg:34.60ms
step:189/1825 train_time:6538ms step_avg:34.59ms
step:190/1825 train_time:6574ms step_avg:34.60ms
step:191/1825 train_time:6607ms step_avg:34.59ms
step:192/1825 train_time:6642ms step_avg:34.59ms
step:193/1825 train_time:6675ms step_avg:34.58ms
step:194/1825 train_time:6710ms step_avg:34.59ms
step:195/1825 train_time:6743ms step_avg:34.58ms
step:196/1825 train_time:6778ms step_avg:34.58ms
step:197/1825 train_time:6811ms step_avg:34.57ms
step:198/1825 train_time:6846ms step_avg:34.58ms
step:199/1825 train_time:6879ms step_avg:34.57ms
step:200/1825 train_time:6915ms step_avg:34.58ms
step:201/1825 train_time:6948ms step_avg:34.57ms
step:202/1825 train_time:6984ms step_avg:34.57ms
step:203/1825 train_time:7017ms step_avg:34.56ms
step:204/1825 train_time:7052ms step_avg:34.57ms
step:205/1825 train_time:7085ms step_avg:34.56ms
step:206/1825 train_time:7120ms step_avg:34.56ms
step:207/1825 train_time:7153ms step_avg:34.56ms
step:208/1825 train_time:7188ms step_avg:34.56ms
step:209/1825 train_time:7221ms step_avg:34.55ms
step:210/1825 train_time:7256ms step_avg:34.55ms
step:211/1825 train_time:7289ms step_avg:34.55ms
step:212/1825 train_time:7324ms step_avg:34.55ms
step:213/1825 train_time:7358ms step_avg:34.54ms
step:214/1825 train_time:7393ms step_avg:34.55ms
step:215/1825 train_time:7426ms step_avg:34.54ms
step:216/1825 train_time:7461ms step_avg:34.54ms
step:217/1825 train_time:7494ms step_avg:34.53ms
step:218/1825 train_time:7529ms step_avg:34.54ms
step:219/1825 train_time:7562ms step_avg:34.53ms
step:220/1825 train_time:7597ms step_avg:34.53ms
step:221/1825 train_time:7630ms step_avg:34.53ms
step:222/1825 train_time:7665ms step_avg:34.53ms
step:223/1825 train_time:7698ms step_avg:34.52ms
step:224/1825 train_time:7733ms step_avg:34.52ms
step:225/1825 train_time:7766ms step_avg:34.52ms
step:226/1825 train_time:7802ms step_avg:34.52ms
step:227/1825 train_time:7834ms step_avg:34.51ms
step:228/1825 train_time:7870ms step_avg:34.52ms
step:229/1825 train_time:7902ms step_avg:34.51ms
step:230/1825 train_time:7938ms step_avg:34.51ms
step:231/1825 train_time:7971ms step_avg:34.50ms
step:232/1825 train_time:8006ms step_avg:34.51ms
step:233/1825 train_time:8039ms step_avg:34.50ms
step:234/1825 train_time:8074ms step_avg:34.50ms
step:235/1825 train_time:8107ms step_avg:34.50ms
step:236/1825 train_time:8142ms step_avg:34.50ms
step:237/1825 train_time:8175ms step_avg:34.49ms
step:238/1825 train_time:8210ms step_avg:34.50ms
step:239/1825 train_time:8243ms step_avg:34.49ms
step:240/1825 train_time:8278ms step_avg:34.49ms
step:241/1825 train_time:8311ms step_avg:34.49ms
step:242/1825 train_time:8346ms step_avg:34.49ms
step:243/1825 train_time:8379ms step_avg:34.48ms
step:244/1825 train_time:8415ms step_avg:34.49ms
step:245/1825 train_time:8448ms step_avg:34.48ms
step:246/1825 train_time:8483ms step_avg:34.48ms
step:247/1825 train_time:8516ms step_avg:34.48ms
step:248/1825 train_time:8551ms step_avg:34.48ms
step:249/1825 train_time:8584ms step_avg:34.47ms
step:250/1825 train_time:8619ms step_avg:34.48ms
step:250/1825 val_loss:4.6152 train_time:8661ms step_avg:34.64ms
step:251/1825 train_time:8679ms step_avg:34.58ms
step:252/1825 train_time:8697ms step_avg:34.51ms
step:253/1825 train_time:8723ms step_avg:34.48ms
step:254/1825 train_time:8758ms step_avg:34.48ms
step:255/1825 train_time:8793ms step_avg:34.48ms
step:256/1825 train_time:8829ms step_avg:34.49ms
step:257/1825 train_time:8862ms step_avg:34.48ms
step:258/1825 train_time:8898ms step_avg:34.49ms
step:259/1825 train_time:8931ms step_avg:34.48ms
step:260/1825 train_time:8966ms step_avg:34.48ms
step:261/1825 train_time:8999ms step_avg:34.48ms
step:262/1825 train_time:9034ms step_avg:34.48ms
step:263/1825 train_time:9067ms step_avg:34.48ms
step:264/1825 train_time:9102ms step_avg:34.48ms
step:265/1825 train_time:9135ms step_avg:34.47ms
step:266/1825 train_time:9171ms step_avg:34.48ms
step:267/1825 train_time:9203ms step_avg:34.47ms
step:268/1825 train_time:9239ms step_avg:34.47ms
step:269/1825 train_time:9272ms step_avg:34.47ms
step:270/1825 train_time:9307ms step_avg:34.47ms
step:271/1825 train_time:9340ms step_avg:34.46ms
step:272/1825 train_time:9375ms step_avg:34.47ms
step:273/1825 train_time:9408ms step_avg:34.46ms
step:274/1825 train_time:9443ms step_avg:34.46ms
step:275/1825 train_time:9476ms step_avg:34.46ms
step:276/1825 train_time:9511ms step_avg:34.46ms
step:277/1825 train_time:9544ms step_avg:34.45ms
step:278/1825 train_time:9579ms step_avg:34.46ms
step:279/1825 train_time:9612ms step_avg:34.45ms
step:280/1825 train_time:9647ms step_avg:34.45ms
step:281/1825 train_time:9680ms step_avg:34.45ms
step:282/1825 train_time:9715ms step_avg:34.45ms
step:283/1825 train_time:9748ms step_avg:34.45ms
step:284/1825 train_time:9783ms step_avg:34.45ms
step:285/1825 train_time:9816ms step_avg:34.44ms
step:286/1825 train_time:9852ms step_avg:34.45ms
step:287/1825 train_time:9884ms step_avg:34.44ms
step:288/1825 train_time:9920ms step_avg:34.44ms
step:289/1825 train_time:9953ms step_avg:34.44ms
step:290/1825 train_time:9988ms step_avg:34.44ms
step:291/1825 train_time:10021ms step_avg:34.44ms
step:292/1825 train_time:10056ms step_avg:34.44ms
step:293/1825 train_time:10089ms step_avg:34.43ms
step:294/1825 train_time:10124ms step_avg:34.44ms
step:295/1825 train_time:10157ms step_avg:34.43ms
step:296/1825 train_time:10193ms step_avg:34.43ms
step:297/1825 train_time:10225ms step_avg:34.43ms
step:298/1825 train_time:10261ms step_avg:34.43ms
step:299/1825 train_time:10294ms step_avg:34.43ms
step:300/1825 train_time:10329ms step_avg:34.43ms
step:301/1825 train_time:10362ms step_avg:34.42ms
step:302/1825 train_time:10397ms step_avg:34.43ms
step:303/1825 train_time:10430ms step_avg:34.42ms
step:304/1825 train_time:10465ms step_avg:34.42ms
step:305/1825 train_time:10498ms step_avg:34.42ms
step:306/1825 train_time:10533ms step_avg:34.42ms
step:307/1825 train_time:10566ms step_avg:34.42ms
step:308/1825 train_time:10602ms step_avg:34.42ms
step:309/1825 train_time:10634ms step_avg:34.42ms
step:310/1825 train_time:10670ms step_avg:34.42ms
step:311/1825 train_time:10702ms step_avg:34.41ms
step:312/1825 train_time:10737ms step_avg:34.41ms
step:313/1825 train_time:10770ms step_avg:34.41ms
step:314/1825 train_time:10805ms step_avg:34.41ms
step:315/1825 train_time:10838ms step_avg:34.41ms
step:316/1825 train_time:10874ms step_avg:34.41ms
step:317/1825 train_time:10907ms step_avg:34.41ms
step:318/1825 train_time:10942ms step_avg:34.41ms
step:319/1825 train_time:10975ms step_avg:34.40ms
step:320/1825 train_time:11010ms step_avg:34.41ms
step:321/1825 train_time:11043ms step_avg:34.40ms
step:322/1825 train_time:11078ms step_avg:34.40ms
step:323/1825 train_time:11111ms step_avg:34.40ms
step:324/1825 train_time:11146ms step_avg:34.40ms
step:325/1825 train_time:11179ms step_avg:34.40ms
step:326/1825 train_time:11214ms step_avg:34.40ms
step:327/1825 train_time:11247ms step_avg:34.39ms
step:328/1825 train_time:11282ms step_avg:34.40ms
step:329/1825 train_time:11315ms step_avg:34.39ms
step:330/1825 train_time:11350ms step_avg:34.39ms
step:331/1825 train_time:11383ms step_avg:34.39ms
step:332/1825 train_time:11418ms step_avg:34.39ms
step:333/1825 train_time:11451ms step_avg:34.39ms
step:334/1825 train_time:11486ms step_avg:34.39ms
step:335/1825 train_time:11519ms step_avg:34.39ms
step:336/1825 train_time:11554ms step_avg:34.39ms
step:337/1825 train_time:11587ms step_avg:34.38ms
step:338/1825 train_time:11622ms step_avg:34.39ms
step:339/1825 train_time:11655ms step_avg:34.38ms
step:340/1825 train_time:11691ms step_avg:34.38ms
step:341/1825 train_time:11723ms step_avg:34.38ms
step:342/1825 train_time:11758ms step_avg:34.38ms
step:343/1825 train_time:11791ms step_avg:34.38ms
step:344/1825 train_time:11827ms step_avg:34.38ms
step:345/1825 train_time:11859ms step_avg:34.38ms
step:346/1825 train_time:11895ms step_avg:34.38ms
step:347/1825 train_time:11928ms step_avg:34.37ms
step:348/1825 train_time:11963ms step_avg:34.38ms
step:349/1825 train_time:11996ms step_avg:34.37ms
step:350/1825 train_time:12031ms step_avg:34.37ms
step:351/1825 train_time:12064ms step_avg:34.37ms
step:352/1825 train_time:12099ms step_avg:34.37ms
step:353/1825 train_time:12132ms step_avg:34.37ms
step:354/1825 train_time:12167ms step_avg:34.37ms
step:355/1825 train_time:12200ms step_avg:34.37ms
step:356/1825 train_time:12235ms step_avg:34.37ms
step:357/1825 train_time:12268ms step_avg:34.36ms
step:358/1825 train_time:12303ms step_avg:34.37ms
step:359/1825 train_time:12336ms step_avg:34.36ms
step:360/1825 train_time:12371ms step_avg:34.36ms
step:361/1825 train_time:12404ms step_avg:34.36ms
step:362/1825 train_time:12439ms step_avg:34.36ms
step:363/1825 train_time:12472ms step_avg:34.36ms
step:364/1825 train_time:12507ms step_avg:34.36ms
step:365/1825 train_time:12540ms step_avg:34.36ms
step:366/1825 train_time:12575ms step_avg:34.36ms
step:367/1825 train_time:12608ms step_avg:34.35ms
step:368/1825 train_time:12643ms step_avg:34.36ms
step:369/1825 train_time:12676ms step_avg:34.35ms
step:370/1825 train_time:12711ms step_avg:34.35ms
step:371/1825 train_time:12744ms step_avg:34.35ms
step:372/1825 train_time:12779ms step_avg:34.35ms
step:373/1825 train_time:12812ms step_avg:34.35ms
step:374/1825 train_time:12847ms step_avg:34.35ms
step:375/1825 train_time:12880ms step_avg:34.35ms
step:376/1825 train_time:12916ms step_avg:34.35ms
step:377/1825 train_time:12948ms step_avg:34.35ms
step:378/1825 train_time:12984ms step_avg:34.35ms
step:379/1825 train_time:13016ms step_avg:34.34ms
step:380/1825 train_time:13052ms step_avg:34.35ms
step:381/1825 train_time:13085ms step_avg:34.34ms
step:382/1825 train_time:13120ms step_avg:34.35ms
step:383/1825 train_time:13153ms step_avg:34.34ms
step:384/1825 train_time:13188ms step_avg:34.34ms
step:385/1825 train_time:13221ms step_avg:34.34ms
step:386/1825 train_time:13257ms step_avg:34.34ms
step:387/1825 train_time:13290ms step_avg:34.34ms
step:388/1825 train_time:13325ms step_avg:34.34ms
step:389/1825 train_time:13358ms step_avg:34.34ms
step:390/1825 train_time:13393ms step_avg:34.34ms
step:391/1825 train_time:13426ms step_avg:34.34ms
step:392/1825 train_time:13461ms step_avg:34.34ms
step:393/1825 train_time:13494ms step_avg:34.34ms
step:394/1825 train_time:13530ms step_avg:34.34ms
step:395/1825 train_time:13563ms step_avg:34.34ms
step:396/1825 train_time:13598ms step_avg:34.34ms
step:397/1825 train_time:13631ms step_avg:34.34ms
step:398/1825 train_time:13666ms step_avg:34.34ms
step:399/1825 train_time:13699ms step_avg:34.33ms
step:400/1825 train_time:13735ms step_avg:34.34ms
step:401/1825 train_time:13768ms step_avg:34.33ms
step:402/1825 train_time:13803ms step_avg:34.34ms
step:403/1825 train_time:13836ms step_avg:34.33ms
step:404/1825 train_time:13871ms step_avg:34.34ms
step:405/1825 train_time:13904ms step_avg:34.33ms
step:406/1825 train_time:13939ms step_avg:34.33ms
step:407/1825 train_time:13972ms step_avg:34.33ms
step:408/1825 train_time:14007ms step_avg:34.33ms
step:409/1825 train_time:14040ms step_avg:34.33ms
step:410/1825 train_time:14076ms step_avg:34.33ms
step:411/1825 train_time:14109ms step_avg:34.33ms
step:412/1825 train_time:14144ms step_avg:34.33ms
step:413/1825 train_time:14177ms step_avg:34.33ms
step:414/1825 train_time:14212ms step_avg:34.33ms
step:415/1825 train_time:14245ms step_avg:34.32ms
step:416/1825 train_time:14280ms step_avg:34.33ms
step:417/1825 train_time:14313ms step_avg:34.32ms
step:418/1825 train_time:14348ms step_avg:34.33ms
step:419/1825 train_time:14381ms step_avg:34.32ms
step:420/1825 train_time:14416ms step_avg:34.32ms
step:421/1825 train_time:14449ms step_avg:34.32ms
step:422/1825 train_time:14484ms step_avg:34.32ms
step:423/1825 train_time:14517ms step_avg:34.32ms
step:424/1825 train_time:14552ms step_avg:34.32ms
step:425/1825 train_time:14585ms step_avg:34.32ms
step:426/1825 train_time:14621ms step_avg:34.32ms
step:427/1825 train_time:14653ms step_avg:34.32ms
step:428/1825 train_time:14689ms step_avg:34.32ms
step:429/1825 train_time:14721ms step_avg:34.32ms
step:430/1825 train_time:14757ms step_avg:34.32ms
step:431/1825 train_time:14790ms step_avg:34.31ms
step:432/1825 train_time:14825ms step_avg:34.32ms
step:433/1825 train_time:14858ms step_avg:34.31ms
step:434/1825 train_time:14893ms step_avg:34.32ms
step:435/1825 train_time:14926ms step_avg:34.31ms
step:436/1825 train_time:14961ms step_avg:34.31ms
step:437/1825 train_time:14994ms step_avg:34.31ms
step:438/1825 train_time:15029ms step_avg:34.31ms
step:439/1825 train_time:15062ms step_avg:34.31ms
step:440/1825 train_time:15097ms step_avg:34.31ms
step:441/1825 train_time:15130ms step_avg:34.31ms
step:442/1825 train_time:15165ms step_avg:34.31ms
step:443/1825 train_time:15198ms step_avg:34.31ms
step:444/1825 train_time:15233ms step_avg:34.31ms
step:445/1825 train_time:15266ms step_avg:34.31ms
step:446/1825 train_time:15301ms step_avg:34.31ms
step:447/1825 train_time:15334ms step_avg:34.30ms
step:448/1825 train_time:15369ms step_avg:34.31ms
step:449/1825 train_time:15402ms step_avg:34.30ms
step:450/1825 train_time:15437ms step_avg:34.30ms
step:451/1825 train_time:15470ms step_avg:34.30ms
step:452/1825 train_time:15505ms step_avg:34.30ms
step:453/1825 train_time:15538ms step_avg:34.30ms
step:454/1825 train_time:15573ms step_avg:34.30ms
step:455/1825 train_time:15606ms step_avg:34.30ms
step:456/1825 train_time:15641ms step_avg:34.30ms
step:457/1825 train_time:15674ms step_avg:34.30ms
step:458/1825 train_time:15710ms step_avg:34.30ms
step:459/1825 train_time:15742ms step_avg:34.30ms
step:460/1825 train_time:15777ms step_avg:34.30ms
step:461/1825 train_time:15810ms step_avg:34.30ms
step:462/1825 train_time:15845ms step_avg:34.30ms
step:463/1825 train_time:15878ms step_avg:34.29ms
step:464/1825 train_time:15914ms step_avg:34.30ms
step:465/1825 train_time:15947ms step_avg:34.29ms
step:466/1825 train_time:15982ms step_avg:34.30ms
step:467/1825 train_time:16015ms step_avg:34.29ms
step:468/1825 train_time:16050ms step_avg:34.30ms
step:469/1825 train_time:16083ms step_avg:34.29ms
step:470/1825 train_time:16118ms step_avg:34.29ms
step:471/1825 train_time:16152ms step_avg:34.29ms
step:472/1825 train_time:16187ms step_avg:34.29ms
step:473/1825 train_time:16220ms step_avg:34.29ms
step:474/1825 train_time:16255ms step_avg:34.29ms
step:475/1825 train_time:16288ms step_avg:34.29ms
step:476/1825 train_time:16323ms step_avg:34.29ms
step:477/1825 train_time:16356ms step_avg:34.29ms
step:478/1825 train_time:16391ms step_avg:34.29ms
step:479/1825 train_time:16424ms step_avg:34.29ms
step:480/1825 train_time:16459ms step_avg:34.29ms
step:481/1825 train_time:16492ms step_avg:34.29ms
step:482/1825 train_time:16527ms step_avg:34.29ms
step:483/1825 train_time:16560ms step_avg:34.29ms
step:484/1825 train_time:16595ms step_avg:34.29ms
step:485/1825 train_time:16628ms step_avg:34.28ms
step:486/1825 train_time:16663ms step_avg:34.29ms
step:487/1825 train_time:16696ms step_avg:34.28ms
step:488/1825 train_time:16731ms step_avg:34.29ms
step:489/1825 train_time:16764ms step_avg:34.28ms
step:490/1825 train_time:16799ms step_avg:34.28ms
step:491/1825 train_time:16832ms step_avg:34.28ms
step:492/1825 train_time:16867ms step_avg:34.28ms
step:493/1825 train_time:16900ms step_avg:34.28ms
step:494/1825 train_time:16936ms step_avg:34.28ms
step:495/1825 train_time:16969ms step_avg:34.28ms
step:496/1825 train_time:17004ms step_avg:34.28ms
step:497/1825 train_time:17037ms step_avg:34.28ms
step:498/1825 train_time:17072ms step_avg:34.28ms
step:499/1825 train_time:17105ms step_avg:34.28ms
step:500/1825 train_time:17140ms step_avg:34.28ms
step:500/1825 val_loss:4.2887 train_time:17182ms step_avg:34.36ms
step:501/1825 train_time:17199ms step_avg:34.33ms
step:502/1825 train_time:17217ms step_avg:34.30ms
step:503/1825 train_time:17243ms step_avg:34.28ms
step:504/1825 train_time:17278ms step_avg:34.28ms
step:505/1825 train_time:17313ms step_avg:34.28ms
step:506/1825 train_time:17350ms step_avg:34.29ms
step:507/1825 train_time:17384ms step_avg:34.29ms
step:508/1825 train_time:17419ms step_avg:34.29ms
step:509/1825 train_time:17452ms step_avg:34.29ms
step:510/1825 train_time:17488ms step_avg:34.29ms
step:511/1825 train_time:17521ms step_avg:34.29ms
step:512/1825 train_time:17556ms step_avg:34.29ms
step:513/1825 train_time:17589ms step_avg:34.29ms
step:514/1825 train_time:17624ms step_avg:34.29ms
step:515/1825 train_time:17657ms step_avg:34.28ms
step:516/1825 train_time:17692ms step_avg:34.29ms
step:517/1825 train_time:17724ms step_avg:34.28ms
step:518/1825 train_time:17760ms step_avg:34.29ms
step:519/1825 train_time:17792ms step_avg:34.28ms
step:520/1825 train_time:17828ms step_avg:34.28ms
step:521/1825 train_time:17860ms step_avg:34.28ms
step:522/1825 train_time:17896ms step_avg:34.28ms
step:523/1825 train_time:17929ms step_avg:34.28ms
step:524/1825 train_time:17964ms step_avg:34.28ms
step:525/1825 train_time:17997ms step_avg:34.28ms
step:526/1825 train_time:18032ms step_avg:34.28ms
step:527/1825 train_time:18065ms step_avg:34.28ms
step:528/1825 train_time:18100ms step_avg:34.28ms
step:529/1825 train_time:18133ms step_avg:34.28ms
step:530/1825 train_time:18168ms step_avg:34.28ms
step:531/1825 train_time:18201ms step_avg:34.28ms
step:532/1825 train_time:18236ms step_avg:34.28ms
step:533/1825 train_time:18269ms step_avg:34.28ms
step:534/1825 train_time:18304ms step_avg:34.28ms
step:535/1825 train_time:18337ms step_avg:34.27ms
step:536/1825 train_time:18372ms step_avg:34.28ms
step:537/1825 train_time:18405ms step_avg:34.27ms
step:538/1825 train_time:18440ms step_avg:34.28ms
step:539/1825 train_time:18473ms step_avg:34.27ms
step:540/1825 train_time:18509ms step_avg:34.28ms
step:541/1825 train_time:18542ms step_avg:34.27ms
step:542/1825 train_time:18577ms step_avg:34.27ms
step:543/1825 train_time:18610ms step_avg:34.27ms
step:544/1825 train_time:18645ms step_avg:34.27ms
step:545/1825 train_time:18678ms step_avg:34.27ms
step:546/1825 train_time:18713ms step_avg:34.27ms
step:547/1825 train_time:18746ms step_avg:34.27ms
step:548/1825 train_time:18781ms step_avg:34.27ms
step:549/1825 train_time:18814ms step_avg:34.27ms
step:550/1825 train_time:18850ms step_avg:34.27ms
step:551/1825 train_time:18883ms step_avg:34.27ms
step:552/1825 train_time:18918ms step_avg:34.27ms
step:553/1825 train_time:18951ms step_avg:34.27ms
step:554/1825 train_time:18986ms step_avg:34.27ms
step:555/1825 train_time:19019ms step_avg:34.27ms
step:556/1825 train_time:19054ms step_avg:34.27ms
step:557/1825 train_time:19087ms step_avg:34.27ms
step:558/1825 train_time:19122ms step_avg:34.27ms
step:559/1825 train_time:19155ms step_avg:34.27ms
step:560/1825 train_time:19190ms step_avg:34.27ms
step:561/1825 train_time:19223ms step_avg:34.27ms
step:562/1825 train_time:19258ms step_avg:34.27ms
step:563/1825 train_time:19291ms step_avg:34.26ms
step:564/1825 train_time:19326ms step_avg:34.27ms
step:565/1825 train_time:19359ms step_avg:34.26ms
step:566/1825 train_time:19395ms step_avg:34.27ms
step:567/1825 train_time:19427ms step_avg:34.26ms
step:568/1825 train_time:19463ms step_avg:34.27ms
step:569/1825 train_time:19496ms step_avg:34.26ms
step:570/1825 train_time:19531ms step_avg:34.26ms
step:571/1825 train_time:19564ms step_avg:34.26ms
step:572/1825 train_time:19599ms step_avg:34.26ms
step:573/1825 train_time:19632ms step_avg:34.26ms
step:574/1825 train_time:19667ms step_avg:34.26ms
step:575/1825 train_time:19700ms step_avg:34.26ms
step:576/1825 train_time:19735ms step_avg:34.26ms
step:577/1825 train_time:19768ms step_avg:34.26ms
step:578/1825 train_time:19804ms step_avg:34.26ms
step:579/1825 train_time:19837ms step_avg:34.26ms
step:580/1825 train_time:19872ms step_avg:34.26ms
step:581/1825 train_time:19905ms step_avg:34.26ms
step:582/1825 train_time:19940ms step_avg:34.26ms
step:583/1825 train_time:19973ms step_avg:34.26ms
step:584/1825 train_time:20009ms step_avg:34.26ms
step:585/1825 train_time:20042ms step_avg:34.26ms
step:586/1825 train_time:20077ms step_avg:34.26ms
step:587/1825 train_time:20110ms step_avg:34.26ms
step:588/1825 train_time:20145ms step_avg:34.26ms
step:589/1825 train_time:20178ms step_avg:34.26ms
step:590/1825 train_time:20213ms step_avg:34.26ms
step:591/1825 train_time:20246ms step_avg:34.26ms
step:592/1825 train_time:20281ms step_avg:34.26ms
step:593/1825 train_time:20314ms step_avg:34.26ms
step:594/1825 train_time:20349ms step_avg:34.26ms
step:595/1825 train_time:20382ms step_avg:34.26ms
step:596/1825 train_time:20419ms step_avg:34.26ms
step:597/1825 train_time:20477ms step_avg:34.30ms
step:598/1825 train_time:20539ms step_avg:34.35ms
step:599/1825 train_time:20599ms step_avg:34.39ms
step:600/1825 train_time:20662ms step_avg:34.44ms
step:601/1825 train_time:20723ms step_avg:34.48ms
step:602/1825 train_time:20786ms step_avg:34.53ms
step:603/1825 train_time:20846ms step_avg:34.57ms
step:604/1825 train_time:20909ms step_avg:34.62ms
step:605/1825 train_time:20969ms step_avg:34.66ms
step:606/1825 train_time:21032ms step_avg:34.71ms
step:607/1825 train_time:21093ms step_avg:34.75ms
step:608/1825 train_time:21155ms step_avg:34.80ms
step:609/1825 train_time:21216ms step_avg:34.84ms
step:610/1825 train_time:21279ms step_avg:34.88ms
step:611/1825 train_time:21340ms step_avg:34.93ms
step:612/1825 train_time:21402ms step_avg:34.97ms
step:613/1825 train_time:21462ms step_avg:35.01ms
step:614/1825 train_time:21525ms step_avg:35.06ms
step:615/1825 train_time:21585ms step_avg:35.10ms
step:616/1825 train_time:21648ms step_avg:35.14ms
step:617/1825 train_time:21709ms step_avg:35.18ms
step:618/1825 train_time:21772ms step_avg:35.23ms
step:619/1825 train_time:21832ms step_avg:35.27ms
step:620/1825 train_time:21896ms step_avg:35.32ms
step:621/1825 train_time:21956ms step_avg:35.36ms
step:622/1825 train_time:22018ms step_avg:35.40ms
step:623/1825 train_time:22078ms step_avg:35.44ms
step:624/1825 train_time:22142ms step_avg:35.48ms
step:625/1825 train_time:22202ms step_avg:35.52ms
step:626/1825 train_time:22265ms step_avg:35.57ms
step:627/1825 train_time:22325ms step_avg:35.61ms
step:628/1825 train_time:22388ms step_avg:35.65ms
step:629/1825 train_time:22449ms step_avg:35.69ms
step:630/1825 train_time:22512ms step_avg:35.73ms
step:631/1825 train_time:22573ms step_avg:35.77ms
step:632/1825 train_time:22635ms step_avg:35.82ms
step:633/1825 train_time:22695ms step_avg:35.85ms
step:634/1825 train_time:22758ms step_avg:35.90ms
step:635/1825 train_time:22818ms step_avg:35.93ms
step:636/1825 train_time:22882ms step_avg:35.98ms
step:637/1825 train_time:22942ms step_avg:36.02ms
step:638/1825 train_time:23005ms step_avg:36.06ms
step:639/1825 train_time:23066ms step_avg:36.10ms
step:640/1825 train_time:23129ms step_avg:36.14ms
step:641/1825 train_time:23190ms step_avg:36.18ms
step:642/1825 train_time:23253ms step_avg:36.22ms
step:643/1825 train_time:23313ms step_avg:36.26ms
step:644/1825 train_time:23376ms step_avg:36.30ms
step:645/1825 train_time:23436ms step_avg:36.33ms
step:646/1825 train_time:23498ms step_avg:36.37ms
step:647/1825 train_time:23558ms step_avg:36.41ms
step:648/1825 train_time:23621ms step_avg:36.45ms
step:649/1825 train_time:23682ms step_avg:36.49ms
step:650/1825 train_time:23745ms step_avg:36.53ms
step:651/1825 train_time:23805ms step_avg:36.57ms
step:652/1825 train_time:23868ms step_avg:36.61ms
step:653/1825 train_time:23929ms step_avg:36.64ms
step:654/1825 train_time:23992ms step_avg:36.68ms
step:655/1825 train_time:24053ms step_avg:36.72ms
step:656/1825 train_time:24116ms step_avg:36.76ms
step:657/1825 train_time:24176ms step_avg:36.80ms
step:658/1825 train_time:24239ms step_avg:36.84ms
step:659/1825 train_time:24299ms step_avg:36.87ms
step:660/1825 train_time:24361ms step_avg:36.91ms
step:661/1825 train_time:24422ms step_avg:36.95ms
step:662/1825 train_time:24485ms step_avg:36.99ms
step:663/1825 train_time:24545ms step_avg:37.02ms
step:664/1825 train_time:24608ms step_avg:37.06ms
step:665/1825 train_time:24668ms step_avg:37.09ms
step:666/1825 train_time:24731ms step_avg:37.13ms
step:667/1825 train_time:24792ms step_avg:37.17ms
step:668/1825 train_time:24855ms step_avg:37.21ms
step:669/1825 train_time:24916ms step_avg:37.24ms
step:670/1825 train_time:24978ms step_avg:37.28ms
step:671/1825 train_time:25038ms step_avg:37.31ms
step:672/1825 train_time:25101ms step_avg:37.35ms
step:673/1825 train_time:25162ms step_avg:37.39ms
step:674/1825 train_time:25224ms step_avg:37.42ms
step:675/1825 train_time:25284ms step_avg:37.46ms
step:676/1825 train_time:25348ms step_avg:37.50ms
step:677/1825 train_time:25408ms step_avg:37.53ms
step:678/1825 train_time:25471ms step_avg:37.57ms
step:679/1825 train_time:25531ms step_avg:37.60ms
step:680/1825 train_time:25594ms step_avg:37.64ms
step:681/1825 train_time:25655ms step_avg:37.67ms
step:682/1825 train_time:25717ms step_avg:37.71ms
step:683/1825 train_time:25777ms step_avg:37.74ms
step:684/1825 train_time:25840ms step_avg:37.78ms
step:685/1825 train_time:25900ms step_avg:37.81ms
step:686/1825 train_time:25962ms step_avg:37.85ms
step:687/1825 train_time:26022ms step_avg:37.88ms
step:688/1825 train_time:26087ms step_avg:37.92ms
step:689/1825 train_time:26147ms step_avg:37.95ms
step:690/1825 train_time:26210ms step_avg:37.99ms
step:691/1825 train_time:26270ms step_avg:38.02ms
step:692/1825 train_time:26332ms step_avg:38.05ms
step:693/1825 train_time:26393ms step_avg:38.08ms
step:694/1825 train_time:26455ms step_avg:38.12ms
step:695/1825 train_time:26515ms step_avg:38.15ms
step:696/1825 train_time:26577ms step_avg:38.19ms
step:697/1825 train_time:26638ms step_avg:38.22ms
step:698/1825 train_time:26701ms step_avg:38.25ms
step:699/1825 train_time:26761ms step_avg:38.28ms
step:700/1825 train_time:26824ms step_avg:38.32ms
step:701/1825 train_time:26885ms step_avg:38.35ms
step:702/1825 train_time:26948ms step_avg:38.39ms
step:703/1825 train_time:27009ms step_avg:38.42ms
step:704/1825 train_time:27072ms step_avg:38.45ms
step:705/1825 train_time:27132ms step_avg:38.48ms
step:706/1825 train_time:27194ms step_avg:38.52ms
step:707/1825 train_time:27255ms step_avg:38.55ms
step:708/1825 train_time:27317ms step_avg:38.58ms
step:709/1825 train_time:27377ms step_avg:38.61ms
step:710/1825 train_time:27441ms step_avg:38.65ms
step:711/1825 train_time:27501ms step_avg:38.68ms
step:712/1825 train_time:27564ms step_avg:38.71ms
step:713/1825 train_time:27624ms step_avg:38.74ms
step:714/1825 train_time:27686ms step_avg:38.78ms
step:715/1825 train_time:27746ms step_avg:38.81ms
step:716/1825 train_time:27809ms step_avg:38.84ms
step:717/1825 train_time:27870ms step_avg:38.87ms
step:718/1825 train_time:27933ms step_avg:38.90ms
step:719/1825 train_time:27993ms step_avg:38.93ms
step:720/1825 train_time:28056ms step_avg:38.97ms
step:721/1825 train_time:28116ms step_avg:39.00ms
step:722/1825 train_time:28178ms step_avg:39.03ms
step:723/1825 train_time:28239ms step_avg:39.06ms
step:724/1825 train_time:28301ms step_avg:39.09ms
step:725/1825 train_time:28362ms step_avg:39.12ms
step:726/1825 train_time:28425ms step_avg:39.15ms
step:727/1825 train_time:28485ms step_avg:39.18ms
step:728/1825 train_time:28548ms step_avg:39.21ms
step:729/1825 train_time:28608ms step_avg:39.24ms
step:730/1825 train_time:28671ms step_avg:39.27ms
step:731/1825 train_time:28731ms step_avg:39.30ms
step:732/1825 train_time:28794ms step_avg:39.34ms
step:733/1825 train_time:28854ms step_avg:39.36ms
step:734/1825 train_time:28917ms step_avg:39.40ms
step:735/1825 train_time:28977ms step_avg:39.42ms
step:736/1825 train_time:29040ms step_avg:39.46ms
step:737/1825 train_time:29100ms step_avg:39.48ms
step:738/1825 train_time:29163ms step_avg:39.52ms
step:739/1825 train_time:29223ms step_avg:39.54ms
step:740/1825 train_time:29287ms step_avg:39.58ms
step:741/1825 train_time:29347ms step_avg:39.60ms
step:742/1825 train_time:29410ms step_avg:39.64ms
step:743/1825 train_time:29470ms step_avg:39.66ms
step:744/1825 train_time:29533ms step_avg:39.69ms
step:745/1825 train_time:29593ms step_avg:39.72ms
step:746/1825 train_time:29656ms step_avg:39.75ms
step:747/1825 train_time:29716ms step_avg:39.78ms
step:748/1825 train_time:29778ms step_avg:39.81ms
step:749/1825 train_time:29839ms step_avg:39.84ms
step:750/1825 train_time:29902ms step_avg:39.87ms
step:750/1825 val_loss:4.0189 train_time:29973ms step_avg:39.96ms
step:751/1825 train_time:29991ms step_avg:39.93ms
step:752/1825 train_time:30029ms step_avg:39.93ms
step:753/1825 train_time:30089ms step_avg:39.96ms
step:754/1825 train_time:30153ms step_avg:39.99ms
step:755/1825 train_time:30215ms step_avg:40.02ms
step:756/1825 train_time:30279ms step_avg:40.05ms
step:757/1825 train_time:30339ms step_avg:40.08ms
step:758/1825 train_time:30402ms step_avg:40.11ms
step:759/1825 train_time:30461ms step_avg:40.13ms
step:760/1825 train_time:30524ms step_avg:40.16ms
step:761/1825 train_time:30584ms step_avg:40.19ms
step:762/1825 train_time:30646ms step_avg:40.22ms
step:763/1825 train_time:30706ms step_avg:40.24ms
step:764/1825 train_time:30768ms step_avg:40.27ms
step:765/1825 train_time:30828ms step_avg:40.30ms
step:766/1825 train_time:30891ms step_avg:40.33ms
step:767/1825 train_time:30953ms step_avg:40.36ms
step:768/1825 train_time:31016ms step_avg:40.39ms
step:769/1825 train_time:31077ms step_avg:40.41ms
step:770/1825 train_time:31141ms step_avg:40.44ms
step:771/1825 train_time:31203ms step_avg:40.47ms
step:772/1825 train_time:31265ms step_avg:40.50ms
step:773/1825 train_time:31326ms step_avg:40.52ms
step:774/1825 train_time:31389ms step_avg:40.55ms
step:775/1825 train_time:31448ms step_avg:40.58ms
step:776/1825 train_time:31511ms step_avg:40.61ms
step:777/1825 train_time:31572ms step_avg:40.63ms
step:778/1825 train_time:31635ms step_avg:40.66ms
step:779/1825 train_time:31696ms step_avg:40.69ms
step:780/1825 train_time:31759ms step_avg:40.72ms
step:781/1825 train_time:31819ms step_avg:40.74ms
step:782/1825 train_time:31882ms step_avg:40.77ms
step:783/1825 train_time:31943ms step_avg:40.80ms
step:784/1825 train_time:32007ms step_avg:40.82ms
step:785/1825 train_time:32067ms step_avg:40.85ms
step:786/1825 train_time:32130ms step_avg:40.88ms
step:787/1825 train_time:32190ms step_avg:40.90ms
step:788/1825 train_time:32253ms step_avg:40.93ms
step:789/1825 train_time:32313ms step_avg:40.95ms
step:790/1825 train_time:32377ms step_avg:40.98ms
step:791/1825 train_time:32437ms step_avg:41.01ms
step:792/1825 train_time:32500ms step_avg:41.04ms
step:793/1825 train_time:32560ms step_avg:41.06ms
step:794/1825 train_time:32623ms step_avg:41.09ms
step:795/1825 train_time:32684ms step_avg:41.11ms
step:796/1825 train_time:32747ms step_avg:41.14ms
step:797/1825 train_time:32807ms step_avg:41.16ms
step:798/1825 train_time:32869ms step_avg:41.19ms
step:799/1825 train_time:32930ms step_avg:41.21ms
step:800/1825 train_time:32992ms step_avg:41.24ms
step:801/1825 train_time:33053ms step_avg:41.26ms
step:802/1825 train_time:33117ms step_avg:41.29ms
step:803/1825 train_time:33178ms step_avg:41.32ms
step:804/1825 train_time:33241ms step_avg:41.35ms
step:805/1825 train_time:33302ms step_avg:41.37ms
step:806/1825 train_time:33366ms step_avg:41.40ms
step:807/1825 train_time:33426ms step_avg:41.42ms
step:808/1825 train_time:33488ms step_avg:41.45ms
step:809/1825 train_time:33548ms step_avg:41.47ms
step:810/1825 train_time:33611ms step_avg:41.49ms
step:811/1825 train_time:33671ms step_avg:41.52ms
step:812/1825 train_time:33734ms step_avg:41.54ms
step:813/1825 train_time:33794ms step_avg:41.57ms
step:814/1825 train_time:33856ms step_avg:41.59ms
step:815/1825 train_time:33916ms step_avg:41.61ms
step:816/1825 train_time:33980ms step_avg:41.64ms
step:817/1825 train_time:34040ms step_avg:41.66ms
step:818/1825 train_time:34102ms step_avg:41.69ms
step:819/1825 train_time:34163ms step_avg:41.71ms
step:820/1825 train_time:34226ms step_avg:41.74ms
step:821/1825 train_time:34286ms step_avg:41.76ms
step:822/1825 train_time:34349ms step_avg:41.79ms
step:823/1825 train_time:34409ms step_avg:41.81ms
step:824/1825 train_time:34471ms step_avg:41.83ms
step:825/1825 train_time:34532ms step_avg:41.86ms
step:826/1825 train_time:34594ms step_avg:41.88ms
step:827/1825 train_time:34655ms step_avg:41.90ms
step:828/1825 train_time:34718ms step_avg:41.93ms
step:829/1825 train_time:34779ms step_avg:41.95ms
step:830/1825 train_time:34842ms step_avg:41.98ms
step:831/1825 train_time:34903ms step_avg:42.00ms
step:832/1825 train_time:34965ms step_avg:42.03ms
step:833/1825 train_time:35025ms step_avg:42.05ms
step:834/1825 train_time:35089ms step_avg:42.07ms
step:835/1825 train_time:35148ms step_avg:42.09ms
step:836/1825 train_time:35211ms step_avg:42.12ms
step:837/1825 train_time:35271ms step_avg:42.14ms
step:838/1825 train_time:35335ms step_avg:42.17ms
step:839/1825 train_time:35395ms step_avg:42.19ms
step:840/1825 train_time:35458ms step_avg:42.21ms
step:841/1825 train_time:35518ms step_avg:42.23ms
step:842/1825 train_time:35581ms step_avg:42.26ms
step:843/1825 train_time:35642ms step_avg:42.28ms
step:844/1825 train_time:35705ms step_avg:42.30ms
step:845/1825 train_time:35766ms step_avg:42.33ms
step:846/1825 train_time:35829ms step_avg:42.35ms
step:847/1825 train_time:35889ms step_avg:42.37ms
step:848/1825 train_time:35951ms step_avg:42.40ms
step:849/1825 train_time:36011ms step_avg:42.42ms
step:850/1825 train_time:36074ms step_avg:42.44ms
step:851/1825 train_time:36134ms step_avg:42.46ms
step:852/1825 train_time:36198ms step_avg:42.49ms
step:853/1825 train_time:36258ms step_avg:42.51ms
step:854/1825 train_time:36321ms step_avg:42.53ms
step:855/1825 train_time:36382ms step_avg:42.55ms
step:856/1825 train_time:36445ms step_avg:42.58ms
step:857/1825 train_time:36505ms step_avg:42.60ms
step:858/1825 train_time:36568ms step_avg:42.62ms
step:859/1825 train_time:36628ms step_avg:42.64ms
step:860/1825 train_time:36690ms step_avg:42.66ms
step:861/1825 train_time:36751ms step_avg:42.68ms
step:862/1825 train_time:36814ms step_avg:42.71ms
step:863/1825 train_time:36875ms step_avg:42.73ms
step:864/1825 train_time:36937ms step_avg:42.75ms
step:865/1825 train_time:36997ms step_avg:42.77ms
step:866/1825 train_time:37059ms step_avg:42.79ms
step:867/1825 train_time:37120ms step_avg:42.81ms
step:868/1825 train_time:37182ms step_avg:42.84ms
step:869/1825 train_time:37243ms step_avg:42.86ms
step:870/1825 train_time:37305ms step_avg:42.88ms
step:871/1825 train_time:37366ms step_avg:42.90ms
step:872/1825 train_time:37428ms step_avg:42.92ms
step:873/1825 train_time:37488ms step_avg:42.94ms
step:874/1825 train_time:37550ms step_avg:42.96ms
step:875/1825 train_time:37610ms step_avg:42.98ms
step:876/1825 train_time:37673ms step_avg:43.01ms
step:877/1825 train_time:37733ms step_avg:43.03ms
step:878/1825 train_time:37797ms step_avg:43.05ms
step:879/1825 train_time:37856ms step_avg:43.07ms
step:880/1825 train_time:37920ms step_avg:43.09ms
step:881/1825 train_time:37981ms step_avg:43.11ms
step:882/1825 train_time:38043ms step_avg:43.13ms
step:883/1825 train_time:38104ms step_avg:43.15ms
step:884/1825 train_time:38167ms step_avg:43.17ms
step:885/1825 train_time:38227ms step_avg:43.19ms
step:886/1825 train_time:38290ms step_avg:43.22ms
step:887/1825 train_time:38350ms step_avg:43.24ms
step:888/1825 train_time:38413ms step_avg:43.26ms
step:889/1825 train_time:38474ms step_avg:43.28ms
step:890/1825 train_time:38537ms step_avg:43.30ms
step:891/1825 train_time:38597ms step_avg:43.32ms
step:892/1825 train_time:38660ms step_avg:43.34ms
step:893/1825 train_time:38720ms step_avg:43.36ms
step:894/1825 train_time:38783ms step_avg:43.38ms
step:895/1825 train_time:38845ms step_avg:43.40ms
step:896/1825 train_time:38908ms step_avg:43.42ms
step:897/1825 train_time:38968ms step_avg:43.44ms
step:898/1825 train_time:39030ms step_avg:43.46ms
step:899/1825 train_time:39090ms step_avg:43.48ms
step:900/1825 train_time:39154ms step_avg:43.50ms
step:901/1825 train_time:39215ms step_avg:43.52ms
step:902/1825 train_time:39279ms step_avg:43.55ms
step:903/1825 train_time:39339ms step_avg:43.56ms
step:904/1825 train_time:39402ms step_avg:43.59ms
step:905/1825 train_time:39463ms step_avg:43.61ms
step:906/1825 train_time:39526ms step_avg:43.63ms
step:907/1825 train_time:39586ms step_avg:43.65ms
step:908/1825 train_time:39649ms step_avg:43.67ms
step:909/1825 train_time:39709ms step_avg:43.68ms
step:910/1825 train_time:39772ms step_avg:43.71ms
step:911/1825 train_time:39832ms step_avg:43.72ms
step:912/1825 train_time:39896ms step_avg:43.75ms
step:913/1825 train_time:39956ms step_avg:43.76ms
step:914/1825 train_time:40019ms step_avg:43.78ms
step:915/1825 train_time:40079ms step_avg:43.80ms
step:916/1825 train_time:40143ms step_avg:43.82ms
step:917/1825 train_time:40203ms step_avg:43.84ms
step:918/1825 train_time:40266ms step_avg:43.86ms
step:919/1825 train_time:40326ms step_avg:43.88ms
step:920/1825 train_time:40389ms step_avg:43.90ms
step:921/1825 train_time:40449ms step_avg:43.92ms
step:922/1825 train_time:40512ms step_avg:43.94ms
step:923/1825 train_time:40573ms step_avg:43.96ms
step:924/1825 train_time:40636ms step_avg:43.98ms
step:925/1825 train_time:40696ms step_avg:44.00ms
step:926/1825 train_time:40759ms step_avg:44.02ms
step:927/1825 train_time:40820ms step_avg:44.03ms
step:928/1825 train_time:40882ms step_avg:44.05ms
step:929/1825 train_time:40942ms step_avg:44.07ms
step:930/1825 train_time:41005ms step_avg:44.09ms
step:931/1825 train_time:41065ms step_avg:44.11ms
step:932/1825 train_time:41128ms step_avg:44.13ms
step:933/1825 train_time:41188ms step_avg:44.15ms
step:934/1825 train_time:41251ms step_avg:44.17ms
step:935/1825 train_time:41311ms step_avg:44.18ms
step:936/1825 train_time:41374ms step_avg:44.20ms
step:937/1825 train_time:41434ms step_avg:44.22ms
step:938/1825 train_time:41499ms step_avg:44.24ms
step:939/1825 train_time:41559ms step_avg:44.26ms
step:940/1825 train_time:41622ms step_avg:44.28ms
step:941/1825 train_time:41682ms step_avg:44.30ms
step:942/1825 train_time:41745ms step_avg:44.31ms
step:943/1825 train_time:41805ms step_avg:44.33ms
step:944/1825 train_time:41867ms step_avg:44.35ms
step:945/1825 train_time:41928ms step_avg:44.37ms
step:946/1825 train_time:41990ms step_avg:44.39ms
step:947/1825 train_time:42050ms step_avg:44.40ms
step:948/1825 train_time:42114ms step_avg:44.42ms
step:949/1825 train_time:42175ms step_avg:44.44ms
step:950/1825 train_time:42238ms step_avg:44.46ms
step:951/1825 train_time:42299ms step_avg:44.48ms
step:952/1825 train_time:42362ms step_avg:44.50ms
step:953/1825 train_time:42423ms step_avg:44.52ms
step:954/1825 train_time:42487ms step_avg:44.54ms
step:955/1825 train_time:42546ms step_avg:44.55ms
step:956/1825 train_time:42609ms step_avg:44.57ms
step:957/1825 train_time:42669ms step_avg:44.59ms
step:958/1825 train_time:42732ms step_avg:44.61ms
step:959/1825 train_time:42792ms step_avg:44.62ms
step:960/1825 train_time:42855ms step_avg:44.64ms
step:961/1825 train_time:42915ms step_avg:44.66ms
step:962/1825 train_time:42979ms step_avg:44.68ms
step:963/1825 train_time:43039ms step_avg:44.69ms
step:964/1825 train_time:43103ms step_avg:44.71ms
step:965/1825 train_time:43163ms step_avg:44.73ms
step:966/1825 train_time:43226ms step_avg:44.75ms
step:967/1825 train_time:43287ms step_avg:44.76ms
step:968/1825 train_time:43350ms step_avg:44.78ms
step:969/1825 train_time:43410ms step_avg:44.80ms
step:970/1825 train_time:43473ms step_avg:44.82ms
step:971/1825 train_time:43533ms step_avg:44.83ms
step:972/1825 train_time:43597ms step_avg:44.85ms
step:973/1825 train_time:43657ms step_avg:44.87ms
step:974/1825 train_time:43720ms step_avg:44.89ms
step:975/1825 train_time:43780ms step_avg:44.90ms
step:976/1825 train_time:43842ms step_avg:44.92ms
step:977/1825 train_time:43903ms step_avg:44.94ms
step:978/1825 train_time:43965ms step_avg:44.95ms
step:979/1825 train_time:44026ms step_avg:44.97ms
step:980/1825 train_time:44089ms step_avg:44.99ms
step:981/1825 train_time:44149ms step_avg:45.00ms
step:982/1825 train_time:44211ms step_avg:45.02ms
step:983/1825 train_time:44271ms step_avg:45.04ms
step:984/1825 train_time:44335ms step_avg:45.06ms
step:985/1825 train_time:44395ms step_avg:45.07ms
step:986/1825 train_time:44459ms step_avg:45.09ms
step:987/1825 train_time:44519ms step_avg:45.10ms
step:988/1825 train_time:44582ms step_avg:45.12ms
step:989/1825 train_time:44642ms step_avg:45.14ms
step:990/1825 train_time:44706ms step_avg:45.16ms
step:991/1825 train_time:44767ms step_avg:45.17ms
step:992/1825 train_time:44829ms step_avg:45.19ms
step:993/1825 train_time:44889ms step_avg:45.21ms
step:994/1825 train_time:44952ms step_avg:45.22ms
step:995/1825 train_time:45012ms step_avg:45.24ms
step:996/1825 train_time:45076ms step_avg:45.26ms
step:997/1825 train_time:45136ms step_avg:45.27ms
step:998/1825 train_time:45200ms step_avg:45.29ms
step:999/1825 train_time:45260ms step_avg:45.31ms
step:1000/1825 train_time:45323ms step_avg:45.32ms
step:1000/1825 val_loss:3.7701 train_time:45393ms step_avg:45.39ms
step:1001/1825 train_time:45412ms step_avg:45.37ms
step:1002/1825 train_time:45448ms step_avg:45.36ms
step:1003/1825 train_time:45509ms step_avg:45.37ms
step:1004/1825 train_time:45574ms step_avg:45.39ms
step:1005/1825 train_time:45634ms step_avg:45.41ms
step:1006/1825 train_time:45697ms step_avg:45.42ms
step:1007/1825 train_time:45758ms step_avg:45.44ms
step:1008/1825 train_time:45820ms step_avg:45.46ms
step:1009/1825 train_time:45880ms step_avg:45.47ms
step:1010/1825 train_time:45942ms step_avg:45.49ms
step:1011/1825 train_time:46002ms step_avg:45.50ms
step:1012/1825 train_time:46064ms step_avg:45.52ms
step:1013/1825 train_time:46124ms step_avg:45.53ms
step:1014/1825 train_time:46186ms step_avg:45.55ms
step:1015/1825 train_time:46246ms step_avg:45.56ms
step:1016/1825 train_time:46308ms step_avg:45.58ms
step:1017/1825 train_time:46370ms step_avg:45.59ms
step:1018/1825 train_time:46434ms step_avg:45.61ms
step:1019/1825 train_time:46495ms step_avg:45.63ms
step:1020/1825 train_time:46558ms step_avg:45.64ms
step:1021/1825 train_time:46618ms step_avg:45.66ms
step:1022/1825 train_time:46681ms step_avg:45.68ms
step:1023/1825 train_time:46742ms step_avg:45.69ms
step:1024/1825 train_time:46804ms step_avg:45.71ms
step:1025/1825 train_time:46864ms step_avg:45.72ms
step:1026/1825 train_time:46927ms step_avg:45.74ms
step:1027/1825 train_time:46986ms step_avg:45.75ms
step:1028/1825 train_time:47049ms step_avg:45.77ms
step:1029/1825 train_time:47109ms step_avg:45.78ms
step:1030/1825 train_time:47170ms step_avg:45.80ms
step:1031/1825 train_time:47230ms step_avg:45.81ms
step:1032/1825 train_time:47293ms step_avg:45.83ms
step:1033/1825 train_time:47354ms step_avg:45.84ms
step:1034/1825 train_time:47417ms step_avg:45.86ms
step:1035/1825 train_time:47478ms step_avg:45.87ms
step:1036/1825 train_time:47542ms step_avg:45.89ms
step:1037/1825 train_time:47603ms step_avg:45.90ms
step:1038/1825 train_time:47666ms step_avg:45.92ms
step:1039/1825 train_time:47726ms step_avg:45.93ms
step:1040/1825 train_time:47788ms step_avg:45.95ms
step:1041/1825 train_time:47849ms step_avg:45.96ms
step:1042/1825 train_time:47911ms step_avg:45.98ms
step:1043/1825 train_time:47971ms step_avg:45.99ms
step:1044/1825 train_time:48034ms step_avg:46.01ms
step:1045/1825 train_time:48095ms step_avg:46.02ms
step:1046/1825 train_time:48158ms step_avg:46.04ms
step:1047/1825 train_time:48218ms step_avg:46.05ms
step:1048/1825 train_time:48281ms step_avg:46.07ms
step:1049/1825 train_time:48341ms step_avg:46.08ms
step:1050/1825 train_time:48403ms step_avg:46.10ms
step:1051/1825 train_time:48464ms step_avg:46.11ms
step:1052/1825 train_time:48527ms step_avg:46.13ms
step:1053/1825 train_time:48588ms step_avg:46.14ms
step:1054/1825 train_time:48650ms step_avg:46.16ms
step:1055/1825 train_time:48711ms step_avg:46.17ms
step:1056/1825 train_time:48773ms step_avg:46.19ms
step:1057/1825 train_time:48834ms step_avg:46.20ms
step:1058/1825 train_time:48896ms step_avg:46.22ms
step:1059/1825 train_time:48957ms step_avg:46.23ms
step:1060/1825 train_time:49020ms step_avg:46.25ms
step:1061/1825 train_time:49080ms step_avg:46.26ms
step:1062/1825 train_time:49143ms step_avg:46.27ms
step:1063/1825 train_time:49203ms step_avg:46.29ms
step:1064/1825 train_time:49266ms step_avg:46.30ms
step:1065/1825 train_time:49326ms step_avg:46.32ms
step:1066/1825 train_time:49389ms step_avg:46.33ms
step:1067/1825 train_time:49449ms step_avg:46.34ms
step:1068/1825 train_time:49512ms step_avg:46.36ms
step:1069/1825 train_time:49572ms step_avg:46.37ms
step:1070/1825 train_time:49635ms step_avg:46.39ms
step:1071/1825 train_time:49696ms step_avg:46.40ms
step:1072/1825 train_time:49759ms step_avg:46.42ms
step:1073/1825 train_time:49819ms step_avg:46.43ms
step:1074/1825 train_time:49882ms step_avg:46.45ms
step:1075/1825 train_time:49943ms step_avg:46.46ms
step:1076/1825 train_time:50006ms step_avg:46.47ms
step:1077/1825 train_time:50066ms step_avg:46.49ms
step:1078/1825 train_time:50129ms step_avg:46.50ms
step:1079/1825 train_time:50189ms step_avg:46.51ms
step:1080/1825 train_time:50251ms step_avg:46.53ms
step:1081/1825 train_time:50311ms step_avg:46.54ms
step:1082/1825 train_time:50374ms step_avg:46.56ms
step:1083/1825 train_time:50436ms step_avg:46.57ms
step:1084/1825 train_time:50499ms step_avg:46.59ms
step:1085/1825 train_time:50559ms step_avg:46.60ms
step:1086/1825 train_time:50622ms step_avg:46.61ms
step:1087/1825 train_time:50682ms step_avg:46.63ms
step:1088/1825 train_time:50746ms step_avg:46.64ms
step:1089/1825 train_time:50805ms step_avg:46.65ms
step:1090/1825 train_time:50868ms step_avg:46.67ms
step:1091/1825 train_time:50928ms step_avg:46.68ms
step:1092/1825 train_time:50990ms step_avg:46.69ms
step:1093/1825 train_time:51051ms step_avg:46.71ms
step:1094/1825 train_time:51114ms step_avg:46.72ms
step:1095/1825 train_time:51174ms step_avg:46.73ms
step:1096/1825 train_time:51237ms step_avg:46.75ms
step:1097/1825 train_time:51297ms step_avg:46.76ms
step:1098/1825 train_time:51361ms step_avg:46.78ms
step:1099/1825 train_time:51421ms step_avg:46.79ms
step:1100/1825 train_time:51484ms step_avg:46.80ms
step:1101/1825 train_time:51544ms step_avg:46.82ms
step:1102/1825 train_time:51606ms step_avg:46.83ms
step:1103/1825 train_time:51666ms step_avg:46.84ms
step:1104/1825 train_time:51729ms step_avg:46.86ms
step:1105/1825 train_time:51790ms step_avg:46.87ms
step:1106/1825 train_time:51852ms step_avg:46.88ms
step:1107/1825 train_time:51912ms step_avg:46.89ms
step:1108/1825 train_time:51974ms step_avg:46.91ms
step:1109/1825 train_time:52035ms step_avg:46.92ms
step:1110/1825 train_time:52098ms step_avg:46.93ms
step:1111/1825 train_time:52158ms step_avg:46.95ms
step:1112/1825 train_time:52221ms step_avg:46.96ms
step:1113/1825 train_time:52281ms step_avg:46.97ms
step:1114/1825 train_time:52344ms step_avg:46.99ms
step:1115/1825 train_time:52404ms step_avg:47.00ms
step:1116/1825 train_time:52467ms step_avg:47.01ms
step:1117/1825 train_time:52527ms step_avg:47.02ms
step:1118/1825 train_time:52590ms step_avg:47.04ms
step:1119/1825 train_time:52650ms step_avg:47.05ms
step:1120/1825 train_time:52713ms step_avg:47.07ms
step:1121/1825 train_time:52773ms step_avg:47.08ms
step:1122/1825 train_time:52836ms step_avg:47.09ms
step:1123/1825 train_time:52896ms step_avg:47.10ms
step:1124/1825 train_time:52959ms step_avg:47.12ms
step:1125/1825 train_time:53019ms step_avg:47.13ms
step:1126/1825 train_time:53082ms step_avg:47.14ms
step:1127/1825 train_time:53143ms step_avg:47.15ms
step:1128/1825 train_time:53206ms step_avg:47.17ms
step:1129/1825 train_time:53266ms step_avg:47.18ms
step:1130/1825 train_time:53329ms step_avg:47.19ms
step:1131/1825 train_time:53389ms step_avg:47.21ms
step:1132/1825 train_time:53452ms step_avg:47.22ms
step:1133/1825 train_time:53513ms step_avg:47.23ms
step:1134/1825 train_time:53575ms step_avg:47.24ms
step:1135/1825 train_time:53635ms step_avg:47.26ms
step:1136/1825 train_time:53698ms step_avg:47.27ms
step:1137/1825 train_time:53758ms step_avg:47.28ms
step:1138/1825 train_time:53822ms step_avg:47.29ms
step:1139/1825 train_time:53882ms step_avg:47.31ms
step:1140/1825 train_time:53945ms step_avg:47.32ms
step:1141/1825 train_time:54005ms step_avg:47.33ms
step:1142/1825 train_time:54068ms step_avg:47.35ms
step:1143/1825 train_time:54128ms step_avg:47.36ms
step:1144/1825 train_time:54191ms step_avg:47.37ms
step:1145/1825 train_time:54251ms step_avg:47.38ms
step:1146/1825 train_time:54314ms step_avg:47.39ms
step:1147/1825 train_time:54375ms step_avg:47.41ms
step:1148/1825 train_time:54439ms step_avg:47.42ms
step:1149/1825 train_time:54498ms step_avg:47.43ms
step:1150/1825 train_time:54562ms step_avg:47.44ms
step:1151/1825 train_time:54622ms step_avg:47.46ms
step:1152/1825 train_time:54685ms step_avg:47.47ms
step:1153/1825 train_time:54745ms step_avg:47.48ms
step:1154/1825 train_time:54808ms step_avg:47.49ms
step:1155/1825 train_time:54868ms step_avg:47.51ms
step:1156/1825 train_time:54931ms step_avg:47.52ms
step:1157/1825 train_time:54992ms step_avg:47.53ms
step:1158/1825 train_time:55053ms step_avg:47.54ms
step:1159/1825 train_time:55114ms step_avg:47.55ms
step:1160/1825 train_time:55177ms step_avg:47.57ms
step:1161/1825 train_time:55237ms step_avg:47.58ms
step:1162/1825 train_time:55301ms step_avg:47.59ms
step:1163/1825 train_time:55360ms step_avg:47.60ms
step:1164/1825 train_time:55423ms step_avg:47.61ms
step:1165/1825 train_time:55484ms step_avg:47.63ms
step:1166/1825 train_time:55547ms step_avg:47.64ms
step:1167/1825 train_time:55607ms step_avg:47.65ms
step:1168/1825 train_time:55669ms step_avg:47.66ms
step:1169/1825 train_time:55730ms step_avg:47.67ms
step:1170/1825 train_time:55792ms step_avg:47.69ms
step:1171/1825 train_time:55853ms step_avg:47.70ms
step:1172/1825 train_time:55916ms step_avg:47.71ms
step:1173/1825 train_time:55977ms step_avg:47.72ms
step:1174/1825 train_time:56041ms step_avg:47.73ms
step:1175/1825 train_time:56100ms step_avg:47.75ms
step:1176/1825 train_time:56164ms step_avg:47.76ms
step:1177/1825 train_time:56224ms step_avg:47.77ms
step:1178/1825 train_time:56287ms step_avg:47.78ms
step:1179/1825 train_time:56347ms step_avg:47.79ms
step:1180/1825 train_time:56410ms step_avg:47.80ms
step:1181/1825 train_time:56470ms step_avg:47.82ms
step:1182/1825 train_time:56533ms step_avg:47.83ms
step:1183/1825 train_time:56593ms step_avg:47.84ms
step:1184/1825 train_time:56656ms step_avg:47.85ms
step:1185/1825 train_time:56716ms step_avg:47.86ms
step:1186/1825 train_time:56779ms step_avg:47.87ms
step:1187/1825 train_time:56839ms step_avg:47.88ms
step:1188/1825 train_time:56903ms step_avg:47.90ms
step:1189/1825 train_time:56963ms step_avg:47.91ms
step:1190/1825 train_time:57026ms step_avg:47.92ms
step:1191/1825 train_time:57088ms step_avg:47.93ms
step:1192/1825 train_time:57175ms step_avg:47.97ms
step:1193/1825 train_time:57262ms step_avg:48.00ms
step:1194/1825 train_time:57350ms step_avg:48.03ms
step:1195/1825 train_time:57437ms step_avg:48.06ms
step:1196/1825 train_time:57526ms step_avg:48.10ms
step:1197/1825 train_time:57613ms step_avg:48.13ms
step:1198/1825 train_time:57703ms step_avg:48.17ms
step:1199/1825 train_time:57790ms step_avg:48.20ms
step:1200/1825 train_time:57880ms step_avg:48.23ms
step:1201/1825 train_time:57966ms step_avg:48.26ms
step:1202/1825 train_time:58057ms step_avg:48.30ms
step:1203/1825 train_time:58143ms step_avg:48.33ms
step:1204/1825 train_time:58232ms step_avg:48.37ms
step:1205/1825 train_time:58318ms step_avg:48.40ms
step:1206/1825 train_time:58406ms step_avg:48.43ms
step:1207/1825 train_time:58494ms step_avg:48.46ms
step:1208/1825 train_time:58583ms step_avg:48.50ms
step:1209/1825 train_time:58670ms step_avg:48.53ms
step:1210/1825 train_time:58761ms step_avg:48.56ms
step:1211/1825 train_time:58847ms step_avg:48.59ms
step:1212/1825 train_time:58936ms step_avg:48.63ms
step:1213/1825 train_time:59022ms step_avg:48.66ms
step:1214/1825 train_time:59112ms step_avg:48.69ms
step:1215/1825 train_time:59198ms step_avg:48.72ms
step:1216/1825 train_time:59287ms step_avg:48.76ms
step:1217/1825 train_time:59375ms step_avg:48.79ms
step:1218/1825 train_time:59463ms step_avg:48.82ms
step:1219/1825 train_time:59549ms step_avg:48.85ms
step:1220/1825 train_time:59638ms step_avg:48.88ms
step:1221/1825 train_time:59725ms step_avg:48.91ms
step:1222/1825 train_time:59814ms step_avg:48.95ms
step:1223/1825 train_time:59901ms step_avg:48.98ms
step:1224/1825 train_time:59989ms step_avg:49.01ms
step:1225/1825 train_time:60076ms step_avg:49.04ms
step:1226/1825 train_time:60165ms step_avg:49.07ms
step:1227/1825 train_time:60251ms step_avg:49.10ms
step:1228/1825 train_time:60342ms step_avg:49.14ms
step:1229/1825 train_time:60430ms step_avg:49.17ms
step:1230/1825 train_time:60519ms step_avg:49.20ms
step:1231/1825 train_time:60604ms step_avg:49.23ms
step:1232/1825 train_time:60695ms step_avg:49.27ms
step:1233/1825 train_time:60782ms step_avg:49.30ms
step:1234/1825 train_time:60870ms step_avg:49.33ms
step:1235/1825 train_time:60956ms step_avg:49.36ms
step:1236/1825 train_time:61044ms step_avg:49.39ms
step:1237/1825 train_time:61131ms step_avg:49.42ms
step:1238/1825 train_time:61222ms step_avg:49.45ms
step:1239/1825 train_time:61308ms step_avg:49.48ms
step:1240/1825 train_time:61397ms step_avg:49.51ms
step:1241/1825 train_time:61483ms step_avg:49.54ms
step:1242/1825 train_time:61572ms step_avg:49.58ms
step:1243/1825 train_time:61659ms step_avg:49.61ms
step:1244/1825 train_time:61748ms step_avg:49.64ms
step:1245/1825 train_time:61836ms step_avg:49.67ms
step:1246/1825 train_time:61924ms step_avg:49.70ms
step:1247/1825 train_time:62010ms step_avg:49.73ms
step:1248/1825 train_time:62101ms step_avg:49.76ms
step:1249/1825 train_time:62187ms step_avg:49.79ms
step:1250/1825 train_time:62276ms step_avg:49.82ms
step:1250/1825 val_loss:3.5291 train_time:62372ms step_avg:49.90ms
step:1251/1825 train_time:62390ms step_avg:49.87ms
step:1252/1825 train_time:62451ms step_avg:49.88ms
step:1253/1825 train_time:62542ms step_avg:49.91ms
step:1254/1825 train_time:62636ms step_avg:49.95ms
step:1255/1825 train_time:62723ms step_avg:49.98ms
step:1256/1825 train_time:62812ms step_avg:50.01ms
step:1257/1825 train_time:62897ms step_avg:50.04ms
step:1258/1825 train_time:62985ms step_avg:50.07ms
step:1259/1825 train_time:63070ms step_avg:50.10ms
step:1260/1825 train_time:63157ms step_avg:50.12ms
step:1261/1825 train_time:63243ms step_avg:50.15ms
step:1262/1825 train_time:63335ms step_avg:50.19ms
step:1263/1825 train_time:63423ms step_avg:50.22ms
step:1264/1825 train_time:63515ms step_avg:50.25ms
step:1265/1825 train_time:63604ms step_avg:50.28ms
step:1266/1825 train_time:63694ms step_avg:50.31ms
step:1267/1825 train_time:63779ms step_avg:50.34ms
step:1268/1825 train_time:63869ms step_avg:50.37ms
step:1269/1825 train_time:63955ms step_avg:50.40ms
step:1270/1825 train_time:64042ms step_avg:50.43ms
step:1271/1825 train_time:64128ms step_avg:50.45ms
step:1272/1825 train_time:64216ms step_avg:50.48ms
step:1273/1825 train_time:64302ms step_avg:50.51ms
step:1274/1825 train_time:64393ms step_avg:50.54ms
step:1275/1825 train_time:64479ms step_avg:50.57ms
step:1276/1825 train_time:64571ms step_avg:50.60ms
step:1277/1825 train_time:64658ms step_avg:50.63ms
step:1278/1825 train_time:64747ms step_avg:50.66ms
step:1279/1825 train_time:64834ms step_avg:50.69ms
step:1280/1825 train_time:64922ms step_avg:50.72ms
step:1281/1825 train_time:65007ms step_avg:50.75ms
step:1282/1825 train_time:65096ms step_avg:50.78ms
step:1283/1825 train_time:65182ms step_avg:50.80ms
step:1284/1825 train_time:65272ms step_avg:50.83ms
step:1285/1825 train_time:65358ms step_avg:50.86ms
step:1286/1825 train_time:65449ms step_avg:50.89ms
step:1287/1825 train_time:65537ms step_avg:50.92ms
step:1288/1825 train_time:65626ms step_avg:50.95ms
step:1289/1825 train_time:65714ms step_avg:50.98ms
step:1290/1825 train_time:65804ms step_avg:51.01ms
step:1291/1825 train_time:65889ms step_avg:51.04ms
step:1292/1825 train_time:65978ms step_avg:51.07ms
step:1293/1825 train_time:66063ms step_avg:51.09ms
step:1294/1825 train_time:66152ms step_avg:51.12ms
step:1295/1825 train_time:66238ms step_avg:51.15ms
step:1296/1825 train_time:66327ms step_avg:51.18ms
step:1297/1825 train_time:66414ms step_avg:51.21ms
step:1298/1825 train_time:66504ms step_avg:51.24ms
step:1299/1825 train_time:66592ms step_avg:51.26ms
step:1300/1825 train_time:66682ms step_avg:51.29ms
step:1301/1825 train_time:66769ms step_avg:51.32ms
step:1302/1825 train_time:66857ms step_avg:51.35ms
step:1303/1825 train_time:66943ms step_avg:51.38ms
step:1304/1825 train_time:67033ms step_avg:51.41ms
step:1305/1825 train_time:67118ms step_avg:51.43ms
step:1306/1825 train_time:67207ms step_avg:51.46ms
step:1307/1825 train_time:67294ms step_avg:51.49ms
step:1308/1825 train_time:67382ms step_avg:51.52ms
step:1309/1825 train_time:67469ms step_avg:51.54ms
step:1310/1825 train_time:67559ms step_avg:51.57ms
step:1311/1825 train_time:67646ms step_avg:51.60ms
step:1312/1825 train_time:67736ms step_avg:51.63ms
step:1313/1825 train_time:67823ms step_avg:51.65ms
step:1314/1825 train_time:67912ms step_avg:51.68ms
step:1315/1825 train_time:67998ms step_avg:51.71ms
step:1316/1825 train_time:68088ms step_avg:51.74ms
step:1317/1825 train_time:68174ms step_avg:51.76ms
step:1318/1825 train_time:68263ms step_avg:51.79ms
step:1319/1825 train_time:68349ms step_avg:51.82ms
step:1320/1825 train_time:68437ms step_avg:51.85ms
step:1321/1825 train_time:68525ms step_avg:51.87ms
step:1322/1825 train_time:68615ms step_avg:51.90ms
step:1323/1825 train_time:68703ms step_avg:51.93ms
step:1324/1825 train_time:68792ms step_avg:51.96ms
step:1325/1825 train_time:68878ms step_avg:51.98ms
step:1326/1825 train_time:68967ms step_avg:52.01ms
step:1327/1825 train_time:69053ms step_avg:52.04ms
step:1328/1825 train_time:69142ms step_avg:52.07ms
step:1329/1825 train_time:69229ms step_avg:52.09ms
step:1330/1825 train_time:69318ms step_avg:52.12ms
step:1331/1825 train_time:69404ms step_avg:52.14ms
step:1332/1825 train_time:69494ms step_avg:52.17ms
step:1333/1825 train_time:69580ms step_avg:52.20ms
step:1334/1825 train_time:69672ms step_avg:52.23ms
step:1335/1825 train_time:69758ms step_avg:52.25ms
step:1336/1825 train_time:69847ms step_avg:52.28ms
step:1337/1825 train_time:69933ms step_avg:52.31ms
step:1338/1825 train_time:70023ms step_avg:52.33ms
step:1339/1825 train_time:70109ms step_avg:52.36ms
step:1340/1825 train_time:70197ms step_avg:52.39ms
step:1341/1825 train_time:70284ms step_avg:52.41ms
step:1342/1825 train_time:70373ms step_avg:52.44ms
step:1343/1825 train_time:70460ms step_avg:52.46ms
step:1344/1825 train_time:70551ms step_avg:52.49ms
step:1345/1825 train_time:70637ms step_avg:52.52ms
step:1346/1825 train_time:70727ms step_avg:52.55ms
step:1347/1825 train_time:70814ms step_avg:52.57ms
step:1348/1825 train_time:70903ms step_avg:52.60ms
step:1349/1825 train_time:70989ms step_avg:52.62ms
step:1350/1825 train_time:71077ms step_avg:52.65ms
step:1351/1825 train_time:71164ms step_avg:52.68ms
step:1352/1825 train_time:71253ms step_avg:52.70ms
step:1353/1825 train_time:71339ms step_avg:52.73ms
step:1354/1825 train_time:71429ms step_avg:52.75ms
step:1355/1825 train_time:71516ms step_avg:52.78ms
step:1356/1825 train_time:71605ms step_avg:52.81ms
step:1357/1825 train_time:71693ms step_avg:52.83ms
step:1358/1825 train_time:71782ms step_avg:52.86ms
step:1359/1825 train_time:71869ms step_avg:52.88ms
step:1360/1825 train_time:71957ms step_avg:52.91ms
step:1361/1825 train_time:72044ms step_avg:52.93ms
step:1362/1825 train_time:72134ms step_avg:52.96ms
step:1363/1825 train_time:72220ms step_avg:52.99ms
step:1364/1825 train_time:72310ms step_avg:53.01ms
step:1365/1825 train_time:72396ms step_avg:53.04ms
step:1366/1825 train_time:72485ms step_avg:53.06ms
step:1367/1825 train_time:72572ms step_avg:53.09ms
step:1368/1825 train_time:72660ms step_avg:53.11ms
step:1369/1825 train_time:72747ms step_avg:53.14ms
step:1370/1825 train_time:72836ms step_avg:53.17ms
step:1371/1825 train_time:72923ms step_avg:53.19ms
step:1372/1825 train_time:73013ms step_avg:53.22ms
step:1373/1825 train_time:73098ms step_avg:53.24ms
step:1374/1825 train_time:73189ms step_avg:53.27ms
step:1375/1825 train_time:73275ms step_avg:53.29ms
step:1376/1825 train_time:73364ms step_avg:53.32ms
step:1377/1825 train_time:73451ms step_avg:53.34ms
step:1378/1825 train_time:73539ms step_avg:53.37ms
step:1379/1825 train_time:73627ms step_avg:53.39ms
step:1380/1825 train_time:73716ms step_avg:53.42ms
step:1381/1825 train_time:73802ms step_avg:53.44ms
step:1382/1825 train_time:73892ms step_avg:53.47ms
step:1383/1825 train_time:73978ms step_avg:53.49ms
step:1384/1825 train_time:74069ms step_avg:53.52ms
step:1385/1825 train_time:74156ms step_avg:53.54ms
step:1386/1825 train_time:74245ms step_avg:53.57ms
step:1387/1825 train_time:74331ms step_avg:53.59ms
step:1388/1825 train_time:74421ms step_avg:53.62ms
step:1389/1825 train_time:74507ms step_avg:53.64ms
step:1390/1825 train_time:74596ms step_avg:53.67ms
step:1391/1825 train_time:74682ms step_avg:53.69ms
step:1392/1825 train_time:74772ms step_avg:53.72ms
step:1393/1825 train_time:74857ms step_avg:53.74ms
step:1394/1825 train_time:74946ms step_avg:53.76ms
step:1395/1825 train_time:75033ms step_avg:53.79ms
step:1396/1825 train_time:75123ms step_avg:53.81ms
step:1397/1825 train_time:75209ms step_avg:53.84ms
step:1398/1825 train_time:75298ms step_avg:53.86ms
step:1399/1825 train_time:75386ms step_avg:53.89ms
step:1400/1825 train_time:75474ms step_avg:53.91ms
step:1401/1825 train_time:75562ms step_avg:53.93ms
step:1402/1825 train_time:75652ms step_avg:53.96ms
step:1403/1825 train_time:75737ms step_avg:53.98ms
step:1404/1825 train_time:75827ms step_avg:54.01ms
step:1405/1825 train_time:75913ms step_avg:54.03ms
step:1406/1825 train_time:76003ms step_avg:54.06ms
step:1407/1825 train_time:76089ms step_avg:54.08ms
step:1408/1825 train_time:76177ms step_avg:54.10ms
step:1409/1825 train_time:76265ms step_avg:54.13ms
step:1410/1825 train_time:76354ms step_avg:54.15ms
step:1411/1825 train_time:76440ms step_avg:54.17ms
step:1412/1825 train_time:76529ms step_avg:54.20ms
step:1413/1825 train_time:76615ms step_avg:54.22ms
step:1414/1825 train_time:76705ms step_avg:54.25ms
step:1415/1825 train_time:76791ms step_avg:54.27ms
step:1416/1825 train_time:76878ms step_avg:54.29ms
step:1417/1825 train_time:76966ms step_avg:54.32ms
step:1418/1825 train_time:77055ms step_avg:54.34ms
step:1419/1825 train_time:77141ms step_avg:54.36ms
step:1420/1825 train_time:77232ms step_avg:54.39ms
step:1421/1825 train_time:77318ms step_avg:54.41ms
step:1422/1825 train_time:77407ms step_avg:54.44ms
step:1423/1825 train_time:77494ms step_avg:54.46ms
step:1424/1825 train_time:77584ms step_avg:54.48ms
step:1425/1825 train_time:77670ms step_avg:54.51ms
step:1426/1825 train_time:77758ms step_avg:54.53ms
step:1427/1825 train_time:77845ms step_avg:54.55ms
step:1428/1825 train_time:77934ms step_avg:54.58ms
step:1429/1825 train_time:78019ms step_avg:54.60ms
step:1430/1825 train_time:78110ms step_avg:54.62ms
step:1431/1825 train_time:78196ms step_avg:54.64ms
step:1432/1825 train_time:78285ms step_avg:54.67ms
step:1433/1825 train_time:78372ms step_avg:54.69ms
step:1434/1825 train_time:78462ms step_avg:54.72ms
step:1435/1825 train_time:78549ms step_avg:54.74ms
step:1436/1825 train_time:78638ms step_avg:54.76ms
step:1437/1825 train_time:78725ms step_avg:54.78ms
step:1438/1825 train_time:78814ms step_avg:54.81ms
step:1439/1825 train_time:78901ms step_avg:54.83ms
step:1440/1825 train_time:78990ms step_avg:54.85ms
step:1441/1825 train_time:79076ms step_avg:54.88ms
step:1442/1825 train_time:79166ms step_avg:54.90ms
step:1443/1825 train_time:79253ms step_avg:54.92ms
step:1444/1825 train_time:79341ms step_avg:54.95ms
step:1445/1825 train_time:79427ms step_avg:54.97ms
step:1446/1825 train_time:79518ms step_avg:54.99ms
step:1447/1825 train_time:79606ms step_avg:55.01ms
step:1448/1825 train_time:79695ms step_avg:55.04ms
step:1449/1825 train_time:79783ms step_avg:55.06ms
step:1450/1825 train_time:79871ms step_avg:55.08ms
step:1451/1825 train_time:79957ms step_avg:55.10ms
step:1452/1825 train_time:80048ms step_avg:55.13ms
step:1453/1825 train_time:80134ms step_avg:55.15ms
step:1454/1825 train_time:80222ms step_avg:55.17ms
step:1455/1825 train_time:80309ms step_avg:55.19ms
step:1456/1825 train_time:80396ms step_avg:55.22ms
step:1457/1825 train_time:80484ms step_avg:55.24ms
step:1458/1825 train_time:80574ms step_avg:55.26ms
step:1459/1825 train_time:80662ms step_avg:55.29ms
step:1460/1825 train_time:80751ms step_avg:55.31ms
step:1461/1825 train_time:80837ms step_avg:55.33ms
step:1462/1825 train_time:80926ms step_avg:55.35ms
step:1463/1825 train_time:81012ms step_avg:55.37ms
step:1464/1825 train_time:81101ms step_avg:55.40ms
step:1465/1825 train_time:81188ms step_avg:55.42ms
step:1466/1825 train_time:81277ms step_avg:55.44ms
step:1467/1825 train_time:81363ms step_avg:55.46ms
step:1468/1825 train_time:81453ms step_avg:55.49ms
step:1469/1825 train_time:81539ms step_avg:55.51ms
step:1470/1825 train_time:81629ms step_avg:55.53ms
step:1471/1825 train_time:81715ms step_avg:55.55ms
step:1472/1825 train_time:81804ms step_avg:55.57ms
step:1473/1825 train_time:81891ms step_avg:55.59ms
step:1474/1825 train_time:81979ms step_avg:55.62ms
step:1475/1825 train_time:82067ms step_avg:55.64ms
step:1476/1825 train_time:82155ms step_avg:55.66ms
step:1477/1825 train_time:82242ms step_avg:55.68ms
step:1478/1825 train_time:82333ms step_avg:55.71ms
step:1479/1825 train_time:82419ms step_avg:55.73ms
step:1480/1825 train_time:82508ms step_avg:55.75ms
step:1481/1825 train_time:82595ms step_avg:55.77ms
step:1482/1825 train_time:82686ms step_avg:55.79ms
step:1483/1825 train_time:82772ms step_avg:55.81ms
step:1484/1825 train_time:82861ms step_avg:55.84ms
step:1485/1825 train_time:82948ms step_avg:55.86ms
step:1486/1825 train_time:83036ms step_avg:55.88ms
step:1487/1825 train_time:83122ms step_avg:55.90ms
step:1488/1825 train_time:83213ms step_avg:55.92ms
step:1489/1825 train_time:83299ms step_avg:55.94ms
step:1490/1825 train_time:83389ms step_avg:55.97ms
step:1491/1825 train_time:83474ms step_avg:55.99ms
step:1492/1825 train_time:83564ms step_avg:56.01ms
step:1493/1825 train_time:83651ms step_avg:56.03ms
step:1494/1825 train_time:83739ms step_avg:56.05ms
step:1495/1825 train_time:83826ms step_avg:56.07ms
step:1496/1825 train_time:83916ms step_avg:56.09ms
step:1497/1825 train_time:84002ms step_avg:56.11ms
step:1498/1825 train_time:84093ms step_avg:56.14ms
step:1499/1825 train_time:84179ms step_avg:56.16ms
step:1500/1825 train_time:84268ms step_avg:56.18ms
step:1500/1825 val_loss:3.3989 train_time:84365ms step_avg:56.24ms
step:1501/1825 train_time:84383ms step_avg:56.22ms
step:1502/1825 train_time:84445ms step_avg:56.22ms
step:1503/1825 train_time:84534ms step_avg:56.24ms
step:1504/1825 train_time:84628ms step_avg:56.27ms
step:1505/1825 train_time:84714ms step_avg:56.29ms
step:1506/1825 train_time:84803ms step_avg:56.31ms
step:1507/1825 train_time:84889ms step_avg:56.33ms
step:1508/1825 train_time:84977ms step_avg:56.35ms
step:1509/1825 train_time:85062ms step_avg:56.37ms
step:1510/1825 train_time:85150ms step_avg:56.39ms
step:1511/1825 train_time:85235ms step_avg:56.41ms
step:1512/1825 train_time:85325ms step_avg:56.43ms
step:1513/1825 train_time:85413ms step_avg:56.45ms
step:1514/1825 train_time:85504ms step_avg:56.48ms
step:1515/1825 train_time:85592ms step_avg:56.50ms
step:1516/1825 train_time:85682ms step_avg:56.52ms
step:1517/1825 train_time:85768ms step_avg:56.54ms
step:1518/1825 train_time:85858ms step_avg:56.56ms
step:1519/1825 train_time:85945ms step_avg:56.58ms
step:1520/1825 train_time:86034ms step_avg:56.60ms
step:1521/1825 train_time:86119ms step_avg:56.62ms
step:1522/1825 train_time:86208ms step_avg:56.64ms
step:1523/1825 train_time:86295ms step_avg:56.66ms
step:1524/1825 train_time:86385ms step_avg:56.68ms
step:1525/1825 train_time:86472ms step_avg:56.70ms
step:1526/1825 train_time:86563ms step_avg:56.73ms
step:1527/1825 train_time:86650ms step_avg:56.75ms
step:1528/1825 train_time:86740ms step_avg:56.77ms
step:1529/1825 train_time:86826ms step_avg:56.79ms
step:1530/1825 train_time:86915ms step_avg:56.81ms
step:1531/1825 train_time:87001ms step_avg:56.83ms
step:1532/1825 train_time:87089ms step_avg:56.85ms
step:1533/1825 train_time:87174ms step_avg:56.86ms
step:1534/1825 train_time:87263ms step_avg:56.89ms
step:1535/1825 train_time:87349ms step_avg:56.90ms
step:1536/1825 train_time:87439ms step_avg:56.93ms
step:1537/1825 train_time:87526ms step_avg:56.95ms
step:1538/1825 train_time:87615ms step_avg:56.97ms
step:1539/1825 train_time:87703ms step_avg:56.99ms
step:1540/1825 train_time:87791ms step_avg:57.01ms
step:1541/1825 train_time:87878ms step_avg:57.03ms
step:1542/1825 train_time:87967ms step_avg:57.05ms
step:1543/1825 train_time:88053ms step_avg:57.07ms
step:1544/1825 train_time:88142ms step_avg:57.09ms
step:1545/1825 train_time:88228ms step_avg:57.11ms
step:1546/1825 train_time:88317ms step_avg:57.13ms
step:1547/1825 train_time:88404ms step_avg:57.15ms
step:1548/1825 train_time:88492ms step_avg:57.17ms
step:1549/1825 train_time:88580ms step_avg:57.19ms
step:1550/1825 train_time:88671ms step_avg:57.21ms
step:1551/1825 train_time:88756ms step_avg:57.23ms
step:1552/1825 train_time:88847ms step_avg:57.25ms
step:1553/1825 train_time:88934ms step_avg:57.27ms
step:1554/1825 train_time:89022ms step_avg:57.29ms
step:1555/1825 train_time:89108ms step_avg:57.30ms
step:1556/1825 train_time:89197ms step_avg:57.32ms
step:1557/1825 train_time:89283ms step_avg:57.34ms
step:1558/1825 train_time:89372ms step_avg:57.36ms
step:1559/1825 train_time:89459ms step_avg:57.38ms
step:1560/1825 train_time:89549ms step_avg:57.40ms
step:1561/1825 train_time:89636ms step_avg:57.42ms
step:1562/1825 train_time:89727ms step_avg:57.44ms
step:1563/1825 train_time:89813ms step_avg:57.46ms
step:1564/1825 train_time:89902ms step_avg:57.48ms
step:1565/1825 train_time:89988ms step_avg:57.50ms
step:1566/1825 train_time:90076ms step_avg:57.52ms
step:1567/1825 train_time:90163ms step_avg:57.54ms
step:1568/1825 train_time:90252ms step_avg:57.56ms
step:1569/1825 train_time:90337ms step_avg:57.58ms
step:1570/1825 train_time:90428ms step_avg:57.60ms
step:1571/1825 train_time:90516ms step_avg:57.62ms
step:1572/1825 train_time:90608ms step_avg:57.64ms
step:1573/1825 train_time:90695ms step_avg:57.66ms
step:1574/1825 train_time:90786ms step_avg:57.68ms
step:1575/1825 train_time:90873ms step_avg:57.70ms
step:1576/1825 train_time:90962ms step_avg:57.72ms
step:1577/1825 train_time:91048ms step_avg:57.73ms
step:1578/1825 train_time:91136ms step_avg:57.75ms
step:1579/1825 train_time:91223ms step_avg:57.77ms
step:1580/1825 train_time:91311ms step_avg:57.79ms
step:1581/1825 train_time:91398ms step_avg:57.81ms
step:1582/1825 train_time:91488ms step_avg:57.83ms
step:1583/1825 train_time:91574ms step_avg:57.85ms
step:1584/1825 train_time:91665ms step_avg:57.87ms
step:1585/1825 train_time:91751ms step_avg:57.89ms
step:1586/1825 train_time:91842ms step_avg:57.91ms
step:1587/1825 train_time:91928ms step_avg:57.93ms
step:1588/1825 train_time:92017ms step_avg:57.95ms
step:1589/1825 train_time:92104ms step_avg:57.96ms
step:1590/1825 train_time:92193ms step_avg:57.98ms
step:1591/1825 train_time:92278ms step_avg:58.00ms
step:1592/1825 train_time:92368ms step_avg:58.02ms
step:1593/1825 train_time:92453ms step_avg:58.04ms
step:1594/1825 train_time:92544ms step_avg:58.06ms
step:1595/1825 train_time:92631ms step_avg:58.08ms
step:1596/1825 train_time:92721ms step_avg:58.10ms
step:1597/1825 train_time:92807ms step_avg:58.11ms
step:1598/1825 train_time:92896ms step_avg:58.13ms
step:1599/1825 train_time:92983ms step_avg:58.15ms
step:1600/1825 train_time:93072ms step_avg:58.17ms
step:1601/1825 train_time:93158ms step_avg:58.19ms
step:1602/1825 train_time:93249ms step_avg:58.21ms
step:1603/1825 train_time:93334ms step_avg:58.22ms
step:1604/1825 train_time:93424ms step_avg:58.24ms
step:1605/1825 train_time:93510ms step_avg:58.26ms
step:1606/1825 train_time:93600ms step_avg:58.28ms
step:1607/1825 train_time:93686ms step_avg:58.30ms
step:1608/1825 train_time:93775ms step_avg:58.32ms
step:1609/1825 train_time:93861ms step_avg:58.33ms
step:1610/1825 train_time:93950ms step_avg:58.35ms
step:1611/1825 train_time:94037ms step_avg:58.37ms
step:1612/1825 train_time:94127ms step_avg:58.39ms
step:1613/1825 train_time:94213ms step_avg:58.41ms
step:1614/1825 train_time:94302ms step_avg:58.43ms
step:1615/1825 train_time:94389ms step_avg:58.45ms
step:1616/1825 train_time:94478ms step_avg:58.46ms
step:1617/1825 train_time:94565ms step_avg:58.48ms
step:1618/1825 train_time:94654ms step_avg:58.50ms
step:1619/1825 train_time:94741ms step_avg:58.52ms
step:1620/1825 train_time:94830ms step_avg:58.54ms
step:1621/1825 train_time:94916ms step_avg:58.55ms
step:1622/1825 train_time:95006ms step_avg:58.57ms
step:1623/1825 train_time:95092ms step_avg:58.59ms
step:1624/1825 train_time:95182ms step_avg:58.61ms
step:1625/1825 train_time:95268ms step_avg:58.63ms
step:1626/1825 train_time:95357ms step_avg:58.65ms
step:1627/1825 train_time:95444ms step_avg:58.66ms
step:1628/1825 train_time:95533ms step_avg:58.68ms
step:1629/1825 train_time:95621ms step_avg:58.70ms
step:1630/1825 train_time:95710ms step_avg:58.72ms
step:1631/1825 train_time:95798ms step_avg:58.74ms
step:1632/1825 train_time:95888ms step_avg:58.76ms
step:1633/1825 train_time:95975ms step_avg:58.77ms
step:1634/1825 train_time:96066ms step_avg:58.79ms
step:1635/1825 train_time:96151ms step_avg:58.81ms
step:1636/1825 train_time:96241ms step_avg:58.83ms
step:1637/1825 train_time:96328ms step_avg:58.84ms
step:1638/1825 train_time:96416ms step_avg:58.86ms
step:1639/1825 train_time:96503ms step_avg:58.88ms
step:1640/1825 train_time:96591ms step_avg:58.90ms
step:1641/1825 train_time:96678ms step_avg:58.91ms
step:1642/1825 train_time:96768ms step_avg:58.93ms
step:1643/1825 train_time:96854ms step_avg:58.95ms
step:1644/1825 train_time:96944ms step_avg:58.97ms
step:1645/1825 train_time:97031ms step_avg:58.99ms
step:1646/1825 train_time:97121ms step_avg:59.00ms
step:1647/1825 train_time:97208ms step_avg:59.02ms
step:1648/1825 train_time:97297ms step_avg:59.04ms
step:1649/1825 train_time:97385ms step_avg:59.06ms
step:1650/1825 train_time:97473ms step_avg:59.07ms
step:1651/1825 train_time:97560ms step_avg:59.09ms
step:1652/1825 train_time:97649ms step_avg:59.11ms
step:1653/1825 train_time:97736ms step_avg:59.13ms
step:1654/1825 train_time:97827ms step_avg:59.15ms
step:1655/1825 train_time:97913ms step_avg:59.16ms
step:1656/1825 train_time:98004ms step_avg:59.18ms
step:1657/1825 train_time:98090ms step_avg:59.20ms
step:1658/1825 train_time:98180ms step_avg:59.22ms
step:1659/1825 train_time:98267ms step_avg:59.23ms
step:1660/1825 train_time:98356ms step_avg:59.25ms
step:1661/1825 train_time:98443ms step_avg:59.27ms
step:1662/1825 train_time:98532ms step_avg:59.29ms
step:1663/1825 train_time:98619ms step_avg:59.30ms
step:1664/1825 train_time:98709ms step_avg:59.32ms
step:1665/1825 train_time:98797ms step_avg:59.34ms
step:1666/1825 train_time:98887ms step_avg:59.36ms
step:1667/1825 train_time:98973ms step_avg:59.37ms
step:1668/1825 train_time:99062ms step_avg:59.39ms
step:1669/1825 train_time:99149ms step_avg:59.41ms
step:1670/1825 train_time:99238ms step_avg:59.42ms
step:1671/1825 train_time:99325ms step_avg:59.44ms
step:1672/1825 train_time:99413ms step_avg:59.46ms
step:1673/1825 train_time:99499ms step_avg:59.47ms
step:1674/1825 train_time:99589ms step_avg:59.49ms
step:1675/1825 train_time:99675ms step_avg:59.51ms
step:1676/1825 train_time:99765ms step_avg:59.53ms
step:1677/1825 train_time:99851ms step_avg:59.54ms
step:1678/1825 train_time:99941ms step_avg:59.56ms
step:1679/1825 train_time:100027ms step_avg:59.58ms
step:1680/1825 train_time:100116ms step_avg:59.59ms
step:1681/1825 train_time:100202ms step_avg:59.61ms
step:1682/1825 train_time:100291ms step_avg:59.63ms
step:1683/1825 train_time:100377ms step_avg:59.64ms
step:1684/1825 train_time:100468ms step_avg:59.66ms
step:1685/1825 train_time:100553ms step_avg:59.68ms
step:1686/1825 train_time:100643ms step_avg:59.69ms
step:1687/1825 train_time:100729ms step_avg:59.71ms
step:1688/1825 train_time:100818ms step_avg:59.73ms
step:1689/1825 train_time:100905ms step_avg:59.74ms
step:1690/1825 train_time:100993ms step_avg:59.76ms
step:1691/1825 train_time:101080ms step_avg:59.78ms
step:1692/1825 train_time:101169ms step_avg:59.79ms
step:1693/1825 train_time:101255ms step_avg:59.81ms
step:1694/1825 train_time:101346ms step_avg:59.83ms
step:1695/1825 train_time:101432ms step_avg:59.84ms
step:1696/1825 train_time:101522ms step_avg:59.86ms
step:1697/1825 train_time:101609ms step_avg:59.88ms
step:1698/1825 train_time:101698ms step_avg:59.89ms
step:1699/1825 train_time:101785ms step_avg:59.91ms
step:1700/1825 train_time:101873ms step_avg:59.93ms
step:1701/1825 train_time:101960ms step_avg:59.94ms
step:1702/1825 train_time:102049ms step_avg:59.96ms
step:1703/1825 train_time:102135ms step_avg:59.97ms
step:1704/1825 train_time:102227ms step_avg:59.99ms
step:1705/1825 train_time:102312ms step_avg:60.01ms
step:1706/1825 train_time:102404ms step_avg:60.03ms
step:1707/1825 train_time:102490ms step_avg:60.04ms
step:1708/1825 train_time:102579ms step_avg:60.06ms
step:1709/1825 train_time:102665ms step_avg:60.07ms
step:1710/1825 train_time:102753ms step_avg:60.09ms
step:1711/1825 train_time:102841ms step_avg:60.11ms
step:1712/1825 train_time:102931ms step_avg:60.12ms
step:1713/1825 train_time:103017ms step_avg:60.14ms
step:1714/1825 train_time:103106ms step_avg:60.16ms
step:1715/1825 train_time:103192ms step_avg:60.17ms
step:1716/1825 train_time:103281ms step_avg:60.19ms
step:1717/1825 train_time:103369ms step_avg:60.20ms
step:1718/1825 train_time:103457ms step_avg:60.22ms
step:1719/1825 train_time:103544ms step_avg:60.24ms
step:1720/1825 train_time:103633ms step_avg:60.25ms
step:1721/1825 train_time:103721ms step_avg:60.27ms
step:1722/1825 train_time:103810ms step_avg:60.28ms
step:1723/1825 train_time:103897ms step_avg:60.30ms
step:1724/1825 train_time:103987ms step_avg:60.32ms
step:1725/1825 train_time:104072ms step_avg:60.33ms
step:1726/1825 train_time:104162ms step_avg:60.35ms
step:1727/1825 train_time:104247ms step_avg:60.36ms
step:1728/1825 train_time:104338ms step_avg:60.38ms
step:1729/1825 train_time:104425ms step_avg:60.40ms
step:1730/1825 train_time:104512ms step_avg:60.41ms
step:1731/1825 train_time:104601ms step_avg:60.43ms
step:1732/1825 train_time:104690ms step_avg:60.44ms
step:1733/1825 train_time:104777ms step_avg:60.46ms
step:1734/1825 train_time:104868ms step_avg:60.48ms
step:1735/1825 train_time:104953ms step_avg:60.49ms
step:1736/1825 train_time:105043ms step_avg:60.51ms
step:1737/1825 train_time:105129ms step_avg:60.52ms
step:1738/1825 train_time:105218ms step_avg:60.54ms
step:1739/1825 train_time:105306ms step_avg:60.56ms
step:1740/1825 train_time:105394ms step_avg:60.57ms
step:1741/1825 train_time:105481ms step_avg:60.59ms
step:1742/1825 train_time:105570ms step_avg:60.60ms
step:1743/1825 train_time:105656ms step_avg:60.62ms
step:1744/1825 train_time:105747ms step_avg:60.63ms
step:1745/1825 train_time:105834ms step_avg:60.65ms
step:1746/1825 train_time:105924ms step_avg:60.67ms
step:1747/1825 train_time:106010ms step_avg:60.68ms
step:1748/1825 train_time:106098ms step_avg:60.70ms
step:1749/1825 train_time:106185ms step_avg:60.71ms
step:1750/1825 train_time:106274ms step_avg:60.73ms
step:1750/1825 val_loss:3.3015 train_time:106371ms step_avg:60.78ms
step:1751/1825 train_time:106389ms step_avg:60.76ms
step:1752/1825 train_time:106455ms step_avg:60.76ms
step:1753/1825 train_time:106544ms step_avg:60.78ms
step:1754/1825 train_time:106635ms step_avg:60.80ms
step:1755/1825 train_time:106721ms step_avg:60.81ms
step:1756/1825 train_time:106809ms step_avg:60.83ms
step:1757/1825 train_time:106895ms step_avg:60.84ms
step:1758/1825 train_time:106983ms step_avg:60.86ms
step:1759/1825 train_time:107068ms step_avg:60.87ms
step:1760/1825 train_time:107158ms step_avg:60.89ms
step:1761/1825 train_time:107243ms step_avg:60.90ms
step:1762/1825 train_time:107334ms step_avg:60.92ms
step:1763/1825 train_time:107423ms step_avg:60.93ms
step:1764/1825 train_time:107514ms step_avg:60.95ms
step:1765/1825 train_time:107602ms step_avg:60.96ms
step:1766/1825 train_time:107691ms step_avg:60.98ms
step:1767/1825 train_time:107777ms step_avg:60.99ms
step:1768/1825 train_time:107865ms step_avg:61.01ms
step:1769/1825 train_time:107951ms step_avg:61.02ms
step:1770/1825 train_time:108040ms step_avg:61.04ms
step:1771/1825 train_time:108125ms step_avg:61.05ms
step:1772/1825 train_time:108214ms step_avg:61.07ms
step:1773/1825 train_time:108302ms step_avg:61.08ms
step:1774/1825 train_time:108393ms step_avg:61.10ms
step:1775/1825 train_time:108481ms step_avg:61.12ms
step:1776/1825 train_time:108571ms step_avg:61.13ms
step:1777/1825 train_time:108658ms step_avg:61.15ms
step:1778/1825 train_time:108747ms step_avg:61.16ms
step:1779/1825 train_time:108834ms step_avg:61.18ms
step:1780/1825 train_time:108922ms step_avg:61.19ms
step:1781/1825 train_time:109008ms step_avg:61.21ms
step:1782/1825 train_time:109098ms step_avg:61.22ms
step:1783/1825 train_time:109183ms step_avg:61.24ms
step:1784/1825 train_time:109272ms step_avg:61.25ms
step:1785/1825 train_time:109360ms step_avg:61.27ms
step:1786/1825 train_time:109452ms step_avg:61.28ms
step:1787/1825 train_time:109538ms step_avg:61.30ms
step:1788/1825 train_time:109628ms step_avg:61.31ms
step:1789/1825 train_time:109716ms step_avg:61.33ms
step:1790/1825 train_time:109804ms step_avg:61.34ms
step:1791/1825 train_time:109890ms step_avg:61.36ms
step:1792/1825 train_time:109979ms step_avg:61.37ms
step:1793/1825 train_time:110065ms step_avg:61.39ms
step:1794/1825 train_time:110154ms step_avg:61.40ms
step:1795/1825 train_time:110242ms step_avg:61.42ms
step:1796/1825 train_time:110331ms step_avg:61.43ms
step:1797/1825 train_time:110419ms step_avg:61.45ms
step:1798/1825 train_time:110510ms step_avg:61.46ms
step:1799/1825 train_time:110598ms step_avg:61.48ms
step:1800/1825 train_time:110686ms step_avg:61.49ms
step:1801/1825 train_time:110774ms step_avg:61.51ms
step:1802/1825 train_time:110864ms step_avg:61.52ms
step:1803/1825 train_time:110950ms step_avg:61.54ms
step:1804/1825 train_time:111039ms step_avg:61.55ms
step:1805/1825 train_time:111124ms step_avg:61.56ms
step:1806/1825 train_time:111213ms step_avg:61.58ms
step:1807/1825 train_time:111300ms step_avg:61.59ms
step:1808/1825 train_time:111390ms step_avg:61.61ms
step:1809/1825 train_time:111479ms step_avg:61.62ms
step:1810/1825 train_time:111570ms step_avg:61.64ms
step:1811/1825 train_time:111657ms step_avg:61.66ms
step:1812/1825 train_time:111746ms step_avg:61.67ms
step:1813/1825 train_time:111832ms step_avg:61.68ms
step:1814/1825 train_time:111921ms step_avg:61.70ms
step:1815/1825 train_time:112007ms step_avg:61.71ms
step:1816/1825 train_time:112097ms step_avg:61.73ms
step:1817/1825 train_time:112182ms step_avg:61.74ms
step:1818/1825 train_time:112272ms step_avg:61.76ms
step:1819/1825 train_time:112359ms step_avg:61.77ms
step:1820/1825 train_time:112448ms step_avg:61.78ms
step:1821/1825 train_time:112536ms step_avg:61.80ms
step:1822/1825 train_time:112626ms step_avg:61.81ms
step:1823/1825 train_time:112714ms step_avg:61.83ms
step:1824/1825 train_time:112803ms step_avg:61.84ms
step:1825/1825 train_time:112889ms step_avg:61.86ms
step:1825/1825 val_loss:3.2804 train_time:112986ms step_avg:61.91ms
peak memory allocated: 29801 MiB reserved: 44398 MiB

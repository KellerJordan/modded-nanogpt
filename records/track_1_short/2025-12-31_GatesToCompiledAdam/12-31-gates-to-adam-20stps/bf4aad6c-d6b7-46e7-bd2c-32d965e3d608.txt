import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1785  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 19:22:33 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            125W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    411604      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    411605      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    411606      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    411607      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    411608      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    411609      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    411610      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    411611      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 594, 595, 596, 1189, 1190, 1191, 1784, 1785, 1786] for warmup
Resetting Model
step:0/1825 val_loss:10.8291 train_time:0ms step_avg:0.04ms
step:1/1825 train_time:76ms step_avg:75.54ms
step:2/1825 train_time:98ms step_avg:49.10ms
step:3/1825 train_time:120ms step_avg:39.97ms
step:4/1825 train_time:155ms step_avg:38.79ms
step:5/1825 train_time:188ms step_avg:37.62ms
step:6/1825 train_time:267ms step_avg:44.47ms
step:7/1825 train_time:284ms step_avg:40.62ms
step:8/1825 train_time:313ms step_avg:39.11ms
step:9/1825 train_time:346ms step_avg:38.44ms
step:10/1825 train_time:381ms step_avg:38.13ms
step:11/1825 train_time:414ms step_avg:37.67ms
step:12/1825 train_time:450ms step_avg:37.47ms
step:13/1825 train_time:483ms step_avg:37.12ms
step:14/1825 train_time:518ms step_avg:37.02ms
step:15/1825 train_time:551ms step_avg:36.76ms
step:16/1825 train_time:587ms step_avg:36.67ms
step:17/1825 train_time:620ms step_avg:36.45ms
step:18/1825 train_time:655ms step_avg:36.39ms
step:19/1825 train_time:688ms step_avg:36.22ms
step:20/1825 train_time:723ms step_avg:36.17ms
step:21/1825 train_time:757ms step_avg:36.03ms
step:22/1825 train_time:792ms step_avg:36.00ms
step:23/1825 train_time:825ms step_avg:35.87ms
step:24/1825 train_time:861ms step_avg:35.86ms
step:25/1825 train_time:894ms step_avg:35.74ms
step:26/1825 train_time:929ms step_avg:35.73ms
step:27/1825 train_time:962ms step_avg:35.63ms
step:28/1825 train_time:998ms step_avg:35.63ms
step:29/1825 train_time:1031ms step_avg:35.55ms
step:30/1825 train_time:1066ms step_avg:35.54ms
step:31/1825 train_time:1100ms step_avg:35.47ms
step:32/1825 train_time:1135ms step_avg:35.47ms
step:33/1825 train_time:1168ms step_avg:35.40ms
step:34/1825 train_time:1204ms step_avg:35.40ms
step:35/1825 train_time:1237ms step_avg:35.34ms
step:36/1825 train_time:1272ms step_avg:35.34ms
step:37/1825 train_time:1306ms step_avg:35.28ms
step:38/1825 train_time:1341ms step_avg:35.29ms
step:39/1825 train_time:1374ms step_avg:35.24ms
step:40/1825 train_time:1410ms step_avg:35.24ms
step:41/1825 train_time:1443ms step_avg:35.19ms
step:42/1825 train_time:1478ms step_avg:35.20ms
step:43/1825 train_time:1511ms step_avg:35.15ms
step:44/1825 train_time:1547ms step_avg:35.15ms
step:45/1825 train_time:1580ms step_avg:35.11ms
step:46/1825 train_time:1615ms step_avg:35.12ms
step:47/1825 train_time:1649ms step_avg:35.08ms
step:48/1825 train_time:1684ms step_avg:35.08ms
step:49/1825 train_time:1717ms step_avg:35.05ms
step:50/1825 train_time:1753ms step_avg:35.05ms
step:51/1825 train_time:1786ms step_avg:35.01ms
step:52/1825 train_time:1821ms step_avg:35.02ms
step:53/1825 train_time:1854ms step_avg:34.98ms
step:54/1825 train_time:1889ms step_avg:34.99ms
step:55/1825 train_time:1922ms step_avg:34.95ms
step:56/1825 train_time:1958ms step_avg:34.97ms
step:57/1825 train_time:1991ms step_avg:34.93ms
step:58/1825 train_time:2027ms step_avg:34.95ms
step:59/1825 train_time:2060ms step_avg:34.92ms
step:60/1825 train_time:2095ms step_avg:34.92ms
step:61/1825 train_time:2128ms step_avg:34.89ms
step:62/1825 train_time:2164ms step_avg:34.90ms
step:63/1825 train_time:2197ms step_avg:34.87ms
step:64/1825 train_time:2232ms step_avg:34.87ms
step:65/1825 train_time:2265ms step_avg:34.85ms
step:66/1825 train_time:2301ms step_avg:34.86ms
step:67/1825 train_time:2334ms step_avg:34.83ms
step:68/1825 train_time:2369ms step_avg:34.84ms
step:69/1825 train_time:2402ms step_avg:34.82ms
step:70/1825 train_time:2438ms step_avg:34.83ms
step:71/1825 train_time:2471ms step_avg:34.81ms
step:72/1825 train_time:2507ms step_avg:34.81ms
step:73/1825 train_time:2540ms step_avg:34.79ms
step:74/1825 train_time:2575ms step_avg:34.80ms
step:75/1825 train_time:2608ms step_avg:34.77ms
step:76/1825 train_time:2644ms step_avg:34.79ms
step:77/1825 train_time:2677ms step_avg:34.77ms
step:78/1825 train_time:2712ms step_avg:34.77ms
step:79/1825 train_time:2745ms step_avg:34.75ms
step:80/1825 train_time:2781ms step_avg:34.76ms
step:81/1825 train_time:2814ms step_avg:34.74ms
step:82/1825 train_time:2849ms step_avg:34.74ms
step:83/1825 train_time:2882ms step_avg:34.72ms
step:84/1825 train_time:2918ms step_avg:34.74ms
step:85/1825 train_time:2951ms step_avg:34.72ms
step:86/1825 train_time:2986ms step_avg:34.73ms
step:87/1825 train_time:3019ms step_avg:34.70ms
step:88/1825 train_time:3054ms step_avg:34.71ms
step:89/1825 train_time:3087ms step_avg:34.69ms
step:90/1825 train_time:3123ms step_avg:34.70ms
step:91/1825 train_time:3156ms step_avg:34.68ms
step:92/1825 train_time:3191ms step_avg:34.69ms
step:93/1825 train_time:3224ms step_avg:34.67ms
step:94/1825 train_time:3260ms step_avg:34.68ms
step:95/1825 train_time:3293ms step_avg:34.66ms
step:96/1825 train_time:3328ms step_avg:34.67ms
step:97/1825 train_time:3361ms step_avg:34.65ms
step:98/1825 train_time:3397ms step_avg:34.66ms
step:99/1825 train_time:3430ms step_avg:34.64ms
step:100/1825 train_time:3465ms step_avg:34.65ms
step:101/1825 train_time:3498ms step_avg:34.64ms
step:102/1825 train_time:3534ms step_avg:34.65ms
step:103/1825 train_time:3567ms step_avg:34.63ms
step:104/1825 train_time:3602ms step_avg:34.64ms
step:105/1825 train_time:3636ms step_avg:34.62ms
step:106/1825 train_time:3671ms step_avg:34.63ms
step:107/1825 train_time:3704ms step_avg:34.61ms
step:108/1825 train_time:3739ms step_avg:34.62ms
step:109/1825 train_time:3773ms step_avg:34.61ms
step:110/1825 train_time:3808ms step_avg:34.62ms
step:111/1825 train_time:3841ms step_avg:34.60ms
step:112/1825 train_time:3876ms step_avg:34.61ms
step:113/1825 train_time:3910ms step_avg:34.60ms
step:114/1825 train_time:3945ms step_avg:34.60ms
step:115/1825 train_time:3978ms step_avg:34.59ms
step:116/1825 train_time:4013ms step_avg:34.60ms
step:117/1825 train_time:4046ms step_avg:34.58ms
step:118/1825 train_time:4082ms step_avg:34.59ms
step:119/1825 train_time:4115ms step_avg:34.58ms
step:120/1825 train_time:4150ms step_avg:34.58ms
step:121/1825 train_time:4183ms step_avg:34.57ms
step:122/1825 train_time:4219ms step_avg:34.58ms
step:123/1825 train_time:4251ms step_avg:34.56ms
step:124/1825 train_time:4287ms step_avg:34.57ms
step:125/1825 train_time:4320ms step_avg:34.56ms
step:126/1825 train_time:4355ms step_avg:34.57ms
step:127/1825 train_time:4388ms step_avg:34.55ms
step:128/1825 train_time:4424ms step_avg:34.56ms
step:129/1825 train_time:4456ms step_avg:34.55ms
step:130/1825 train_time:4492ms step_avg:34.55ms
step:131/1825 train_time:4525ms step_avg:34.54ms
step:132/1825 train_time:4560ms step_avg:34.55ms
step:133/1825 train_time:4593ms step_avg:34.53ms
step:134/1825 train_time:4628ms step_avg:34.54ms
step:135/1825 train_time:4661ms step_avg:34.53ms
step:136/1825 train_time:4697ms step_avg:34.54ms
step:137/1825 train_time:4730ms step_avg:34.53ms
step:138/1825 train_time:4765ms step_avg:34.53ms
step:139/1825 train_time:4798ms step_avg:34.52ms
step:140/1825 train_time:4834ms step_avg:34.53ms
step:141/1825 train_time:4866ms step_avg:34.51ms
step:142/1825 train_time:4902ms step_avg:34.52ms
step:143/1825 train_time:4935ms step_avg:34.51ms
step:144/1825 train_time:4970ms step_avg:34.51ms
step:145/1825 train_time:5003ms step_avg:34.50ms
step:146/1825 train_time:5039ms step_avg:34.51ms
step:147/1825 train_time:5072ms step_avg:34.50ms
step:148/1825 train_time:5107ms step_avg:34.51ms
step:149/1825 train_time:5140ms step_avg:34.50ms
step:150/1825 train_time:5175ms step_avg:34.50ms
step:151/1825 train_time:5208ms step_avg:34.49ms
step:152/1825 train_time:5243ms step_avg:34.50ms
step:153/1825 train_time:5276ms step_avg:34.49ms
step:154/1825 train_time:5311ms step_avg:34.49ms
step:155/1825 train_time:5344ms step_avg:34.48ms
step:156/1825 train_time:5380ms step_avg:34.49ms
step:157/1825 train_time:5413ms step_avg:34.48ms
step:158/1825 train_time:5448ms step_avg:34.48ms
step:159/1825 train_time:5481ms step_avg:34.47ms
step:160/1825 train_time:5516ms step_avg:34.48ms
step:161/1825 train_time:5549ms step_avg:34.47ms
step:162/1825 train_time:5585ms step_avg:34.47ms
step:163/1825 train_time:5618ms step_avg:34.46ms
step:164/1825 train_time:5653ms step_avg:34.47ms
step:165/1825 train_time:5686ms step_avg:34.46ms
step:166/1825 train_time:5721ms step_avg:34.46ms
step:167/1825 train_time:5754ms step_avg:34.46ms
step:168/1825 train_time:5789ms step_avg:34.46ms
step:169/1825 train_time:5822ms step_avg:34.45ms
step:170/1825 train_time:5858ms step_avg:34.46ms
step:171/1825 train_time:5891ms step_avg:34.45ms
step:172/1825 train_time:5926ms step_avg:34.45ms
step:173/1825 train_time:5959ms step_avg:34.44ms
step:174/1825 train_time:5994ms step_avg:34.45ms
step:175/1825 train_time:6027ms step_avg:34.44ms
step:176/1825 train_time:6063ms step_avg:34.45ms
step:177/1825 train_time:6096ms step_avg:34.44ms
step:178/1825 train_time:6131ms step_avg:34.44ms
step:179/1825 train_time:6164ms step_avg:34.44ms
step:180/1825 train_time:6200ms step_avg:34.44ms
step:181/1825 train_time:6233ms step_avg:34.44ms
step:182/1825 train_time:6268ms step_avg:34.44ms
step:183/1825 train_time:6301ms step_avg:34.43ms
step:184/1825 train_time:6336ms step_avg:34.44ms
step:185/1825 train_time:6369ms step_avg:34.43ms
step:186/1825 train_time:6405ms step_avg:34.43ms
step:187/1825 train_time:6438ms step_avg:34.43ms
step:188/1825 train_time:6473ms step_avg:34.43ms
step:189/1825 train_time:6506ms step_avg:34.42ms
step:190/1825 train_time:6541ms step_avg:34.43ms
step:191/1825 train_time:6574ms step_avg:34.42ms
step:192/1825 train_time:6609ms step_avg:34.42ms
step:193/1825 train_time:6642ms step_avg:34.42ms
step:194/1825 train_time:6678ms step_avg:34.42ms
step:195/1825 train_time:6711ms step_avg:34.41ms
step:196/1825 train_time:6746ms step_avg:34.42ms
step:197/1825 train_time:6779ms step_avg:34.41ms
step:198/1825 train_time:6814ms step_avg:34.41ms
step:199/1825 train_time:6847ms step_avg:34.41ms
step:200/1825 train_time:6882ms step_avg:34.41ms
step:201/1825 train_time:6915ms step_avg:34.40ms
step:202/1825 train_time:6950ms step_avg:34.41ms
step:203/1825 train_time:6983ms step_avg:34.40ms
step:204/1825 train_time:7019ms step_avg:34.41ms
step:205/1825 train_time:7052ms step_avg:34.40ms
step:206/1825 train_time:7087ms step_avg:34.40ms
step:207/1825 train_time:7120ms step_avg:34.40ms
step:208/1825 train_time:7155ms step_avg:34.40ms
step:209/1825 train_time:7188ms step_avg:34.39ms
step:210/1825 train_time:7224ms step_avg:34.40ms
step:211/1825 train_time:7257ms step_avg:34.39ms
step:212/1825 train_time:7292ms step_avg:34.40ms
step:213/1825 train_time:7325ms step_avg:34.39ms
step:214/1825 train_time:7360ms step_avg:34.39ms
step:215/1825 train_time:7393ms step_avg:34.39ms
step:216/1825 train_time:7428ms step_avg:34.39ms
step:217/1825 train_time:7461ms step_avg:34.38ms
step:218/1825 train_time:7497ms step_avg:34.39ms
step:219/1825 train_time:7530ms step_avg:34.38ms
step:220/1825 train_time:7565ms step_avg:34.39ms
step:221/1825 train_time:7598ms step_avg:34.38ms
step:222/1825 train_time:7633ms step_avg:34.38ms
step:223/1825 train_time:7666ms step_avg:34.38ms
step:224/1825 train_time:7701ms step_avg:34.38ms
step:225/1825 train_time:7734ms step_avg:34.37ms
step:226/1825 train_time:7769ms step_avg:34.38ms
step:227/1825 train_time:7802ms step_avg:34.37ms
step:228/1825 train_time:7838ms step_avg:34.38ms
step:229/1825 train_time:7870ms step_avg:34.37ms
step:230/1825 train_time:7906ms step_avg:34.37ms
step:231/1825 train_time:7938ms step_avg:34.37ms
step:232/1825 train_time:7974ms step_avg:34.37ms
step:233/1825 train_time:8007ms step_avg:34.36ms
step:234/1825 train_time:8042ms step_avg:34.37ms
step:235/1825 train_time:8075ms step_avg:34.36ms
step:236/1825 train_time:8110ms step_avg:34.37ms
step:237/1825 train_time:8143ms step_avg:34.36ms
step:238/1825 train_time:8178ms step_avg:34.36ms
step:239/1825 train_time:8211ms step_avg:34.36ms
step:240/1825 train_time:8247ms step_avg:34.36ms
step:241/1825 train_time:8279ms step_avg:34.35ms
step:242/1825 train_time:8315ms step_avg:34.36ms
step:243/1825 train_time:8348ms step_avg:34.35ms
step:244/1825 train_time:8383ms step_avg:34.36ms
step:245/1825 train_time:8416ms step_avg:34.35ms
step:246/1825 train_time:8451ms step_avg:34.35ms
step:247/1825 train_time:8484ms step_avg:34.35ms
step:248/1825 train_time:8519ms step_avg:34.35ms
step:249/1825 train_time:8552ms step_avg:34.35ms
step:250/1825 train_time:8587ms step_avg:34.35ms
step:250/1825 val_loss:4.6194 train_time:8629ms step_avg:34.52ms
step:251/1825 train_time:8651ms step_avg:34.47ms
step:252/1825 train_time:8669ms step_avg:34.40ms
step:253/1825 train_time:8692ms step_avg:34.35ms
step:254/1825 train_time:8727ms step_avg:34.36ms
step:255/1825 train_time:8760ms step_avg:34.35ms
step:256/1825 train_time:8796ms step_avg:34.36ms
step:257/1825 train_time:8829ms step_avg:34.35ms
step:258/1825 train_time:8864ms step_avg:34.36ms
step:259/1825 train_time:8897ms step_avg:34.35ms
step:260/1825 train_time:8932ms step_avg:34.36ms
step:261/1825 train_time:8965ms step_avg:34.35ms
step:262/1825 train_time:9001ms step_avg:34.35ms
step:263/1825 train_time:9034ms step_avg:34.35ms
step:264/1825 train_time:9069ms step_avg:34.35ms
step:265/1825 train_time:9102ms step_avg:34.35ms
step:266/1825 train_time:9137ms step_avg:34.35ms
step:267/1825 train_time:9170ms step_avg:34.34ms
step:268/1825 train_time:9205ms step_avg:34.35ms
step:269/1825 train_time:9238ms step_avg:34.34ms
step:270/1825 train_time:9273ms step_avg:34.35ms
step:271/1825 train_time:9306ms step_avg:34.34ms
step:272/1825 train_time:9341ms step_avg:34.34ms
step:273/1825 train_time:9374ms step_avg:34.34ms
step:274/1825 train_time:9410ms step_avg:34.34ms
step:275/1825 train_time:9442ms step_avg:34.34ms
step:276/1825 train_time:9478ms step_avg:34.34ms
step:277/1825 train_time:9511ms step_avg:34.33ms
step:278/1825 train_time:9546ms step_avg:34.34ms
step:279/1825 train_time:9579ms step_avg:34.33ms
step:280/1825 train_time:9614ms step_avg:34.34ms
step:281/1825 train_time:9647ms step_avg:34.33ms
step:282/1825 train_time:9682ms step_avg:34.33ms
step:283/1825 train_time:9715ms step_avg:34.33ms
step:284/1825 train_time:9751ms step_avg:34.33ms
step:285/1825 train_time:9784ms step_avg:34.33ms
step:286/1825 train_time:9819ms step_avg:34.33ms
step:287/1825 train_time:9852ms step_avg:34.33ms
step:288/1825 train_time:9887ms step_avg:34.33ms
step:289/1825 train_time:9920ms step_avg:34.33ms
step:290/1825 train_time:9955ms step_avg:34.33ms
step:291/1825 train_time:9988ms step_avg:34.32ms
step:292/1825 train_time:10024ms step_avg:34.33ms
step:293/1825 train_time:10057ms step_avg:34.32ms
step:294/1825 train_time:10092ms step_avg:34.33ms
step:295/1825 train_time:10125ms step_avg:34.32ms
step:296/1825 train_time:10160ms step_avg:34.32ms
step:297/1825 train_time:10193ms step_avg:34.32ms
step:298/1825 train_time:10228ms step_avg:34.32ms
step:299/1825 train_time:10261ms step_avg:34.32ms
step:300/1825 train_time:10296ms step_avg:34.32ms
step:301/1825 train_time:10329ms step_avg:34.32ms
step:302/1825 train_time:10365ms step_avg:34.32ms
step:303/1825 train_time:10398ms step_avg:34.32ms
step:304/1825 train_time:10433ms step_avg:34.32ms
step:305/1825 train_time:10466ms step_avg:34.31ms
step:306/1825 train_time:10501ms step_avg:34.32ms
step:307/1825 train_time:10534ms step_avg:34.31ms
step:308/1825 train_time:10569ms step_avg:34.32ms
step:309/1825 train_time:10602ms step_avg:34.31ms
step:310/1825 train_time:10637ms step_avg:34.31ms
step:311/1825 train_time:10670ms step_avg:34.31ms
step:312/1825 train_time:10705ms step_avg:34.31ms
step:313/1825 train_time:10738ms step_avg:34.31ms
step:314/1825 train_time:10774ms step_avg:34.31ms
step:315/1825 train_time:10807ms step_avg:34.31ms
step:316/1825 train_time:10842ms step_avg:34.31ms
step:317/1825 train_time:10875ms step_avg:34.31ms
step:318/1825 train_time:10910ms step_avg:34.31ms
step:319/1825 train_time:10943ms step_avg:34.30ms
step:320/1825 train_time:10978ms step_avg:34.31ms
step:321/1825 train_time:11011ms step_avg:34.30ms
step:322/1825 train_time:11046ms step_avg:34.31ms
step:323/1825 train_time:11079ms step_avg:34.30ms
step:324/1825 train_time:11115ms step_avg:34.30ms
step:325/1825 train_time:11148ms step_avg:34.30ms
step:326/1825 train_time:11183ms step_avg:34.30ms
step:327/1825 train_time:11216ms step_avg:34.30ms
step:328/1825 train_time:11251ms step_avg:34.30ms
step:329/1825 train_time:11284ms step_avg:34.30ms
step:330/1825 train_time:11319ms step_avg:34.30ms
step:331/1825 train_time:11352ms step_avg:34.30ms
step:332/1825 train_time:11387ms step_avg:34.30ms
step:333/1825 train_time:11420ms step_avg:34.30ms
step:334/1825 train_time:11455ms step_avg:34.30ms
step:335/1825 train_time:11488ms step_avg:34.29ms
step:336/1825 train_time:11524ms step_avg:34.30ms
step:337/1825 train_time:11557ms step_avg:34.29ms
step:338/1825 train_time:11592ms step_avg:34.30ms
step:339/1825 train_time:11625ms step_avg:34.29ms
step:340/1825 train_time:11660ms step_avg:34.29ms
step:341/1825 train_time:11693ms step_avg:34.29ms
step:342/1825 train_time:11728ms step_avg:34.29ms
step:343/1825 train_time:11761ms step_avg:34.29ms
step:344/1825 train_time:11796ms step_avg:34.29ms
step:345/1825 train_time:11829ms step_avg:34.29ms
step:346/1825 train_time:11864ms step_avg:34.29ms
step:347/1825 train_time:11897ms step_avg:34.29ms
step:348/1825 train_time:11933ms step_avg:34.29ms
step:349/1825 train_time:11966ms step_avg:34.29ms
step:350/1825 train_time:12001ms step_avg:34.29ms
step:351/1825 train_time:12034ms step_avg:34.28ms
step:352/1825 train_time:12069ms step_avg:34.29ms
step:353/1825 train_time:12102ms step_avg:34.28ms
step:354/1825 train_time:12137ms step_avg:34.29ms
step:355/1825 train_time:12170ms step_avg:34.28ms
step:356/1825 train_time:12206ms step_avg:34.29ms
step:357/1825 train_time:12239ms step_avg:34.28ms
step:358/1825 train_time:12274ms step_avg:34.28ms
step:359/1825 train_time:12307ms step_avg:34.28ms
step:360/1825 train_time:12342ms step_avg:34.28ms
step:361/1825 train_time:12375ms step_avg:34.28ms
step:362/1825 train_time:12410ms step_avg:34.28ms
step:363/1825 train_time:12443ms step_avg:34.28ms
step:364/1825 train_time:12478ms step_avg:34.28ms
step:365/1825 train_time:12511ms step_avg:34.28ms
step:366/1825 train_time:12546ms step_avg:34.28ms
step:367/1825 train_time:12579ms step_avg:34.28ms
step:368/1825 train_time:12614ms step_avg:34.28ms
step:369/1825 train_time:12647ms step_avg:34.27ms
step:370/1825 train_time:12683ms step_avg:34.28ms
step:371/1825 train_time:12716ms step_avg:34.27ms
step:372/1825 train_time:12751ms step_avg:34.28ms
step:373/1825 train_time:12784ms step_avg:34.27ms
step:374/1825 train_time:12819ms step_avg:34.28ms
step:375/1825 train_time:12852ms step_avg:34.27ms
step:376/1825 train_time:12887ms step_avg:34.27ms
step:377/1825 train_time:12920ms step_avg:34.27ms
step:378/1825 train_time:12956ms step_avg:34.27ms
step:379/1825 train_time:12988ms step_avg:34.27ms
step:380/1825 train_time:13024ms step_avg:34.27ms
step:381/1825 train_time:13057ms step_avg:34.27ms
step:382/1825 train_time:13092ms step_avg:34.27ms
step:383/1825 train_time:13125ms step_avg:34.27ms
step:384/1825 train_time:13160ms step_avg:34.27ms
step:385/1825 train_time:13193ms step_avg:34.27ms
step:386/1825 train_time:13229ms step_avg:34.27ms
step:387/1825 train_time:13261ms step_avg:34.27ms
step:388/1825 train_time:13297ms step_avg:34.27ms
step:389/1825 train_time:13329ms step_avg:34.27ms
step:390/1825 train_time:13365ms step_avg:34.27ms
step:391/1825 train_time:13397ms step_avg:34.26ms
step:392/1825 train_time:13433ms step_avg:34.27ms
step:393/1825 train_time:13466ms step_avg:34.26ms
step:394/1825 train_time:13501ms step_avg:34.27ms
step:395/1825 train_time:13534ms step_avg:34.26ms
step:396/1825 train_time:13569ms step_avg:34.26ms
step:397/1825 train_time:13602ms step_avg:34.26ms
step:398/1825 train_time:13637ms step_avg:34.26ms
step:399/1825 train_time:13670ms step_avg:34.26ms
step:400/1825 train_time:13705ms step_avg:34.26ms
step:401/1825 train_time:13738ms step_avg:34.26ms
step:402/1825 train_time:13773ms step_avg:34.26ms
step:403/1825 train_time:13806ms step_avg:34.26ms
step:404/1825 train_time:13841ms step_avg:34.26ms
step:405/1825 train_time:13874ms step_avg:34.26ms
step:406/1825 train_time:13910ms step_avg:34.26ms
step:407/1825 train_time:13942ms step_avg:34.26ms
step:408/1825 train_time:13978ms step_avg:34.26ms
step:409/1825 train_time:14010ms step_avg:34.26ms
step:410/1825 train_time:14046ms step_avg:34.26ms
step:411/1825 train_time:14079ms step_avg:34.26ms
step:412/1825 train_time:14114ms step_avg:34.26ms
step:413/1825 train_time:14147ms step_avg:34.25ms
step:414/1825 train_time:14182ms step_avg:34.26ms
step:415/1825 train_time:14215ms step_avg:34.25ms
step:416/1825 train_time:14251ms step_avg:34.26ms
step:417/1825 train_time:14284ms step_avg:34.25ms
step:418/1825 train_time:14319ms step_avg:34.26ms
step:419/1825 train_time:14352ms step_avg:34.25ms
step:420/1825 train_time:14387ms step_avg:34.25ms
step:421/1825 train_time:14420ms step_avg:34.25ms
step:422/1825 train_time:14455ms step_avg:34.25ms
step:423/1825 train_time:14488ms step_avg:34.25ms
step:424/1825 train_time:14524ms step_avg:34.25ms
step:425/1825 train_time:14556ms step_avg:34.25ms
step:426/1825 train_time:14592ms step_avg:34.25ms
step:427/1825 train_time:14625ms step_avg:34.25ms
step:428/1825 train_time:14660ms step_avg:34.25ms
step:429/1825 train_time:14693ms step_avg:34.25ms
step:430/1825 train_time:14728ms step_avg:34.25ms
step:431/1825 train_time:14761ms step_avg:34.25ms
step:432/1825 train_time:14797ms step_avg:34.25ms
step:433/1825 train_time:14829ms step_avg:34.25ms
step:434/1825 train_time:14865ms step_avg:34.25ms
step:435/1825 train_time:14898ms step_avg:34.25ms
step:436/1825 train_time:14933ms step_avg:34.25ms
step:437/1825 train_time:14966ms step_avg:34.25ms
step:438/1825 train_time:15001ms step_avg:34.25ms
step:439/1825 train_time:15034ms step_avg:34.25ms
step:440/1825 train_time:15069ms step_avg:34.25ms
step:441/1825 train_time:15103ms step_avg:34.25ms
step:442/1825 train_time:15138ms step_avg:34.25ms
step:443/1825 train_time:15171ms step_avg:34.25ms
step:444/1825 train_time:15206ms step_avg:34.25ms
step:445/1825 train_time:15239ms step_avg:34.24ms
step:446/1825 train_time:15274ms step_avg:34.25ms
step:447/1825 train_time:15307ms step_avg:34.24ms
step:448/1825 train_time:15342ms step_avg:34.25ms
step:449/1825 train_time:15375ms step_avg:34.24ms
step:450/1825 train_time:15410ms step_avg:34.24ms
step:451/1825 train_time:15443ms step_avg:34.24ms
step:452/1825 train_time:15478ms step_avg:34.24ms
step:453/1825 train_time:15511ms step_avg:34.24ms
step:454/1825 train_time:15546ms step_avg:34.24ms
step:455/1825 train_time:15579ms step_avg:34.24ms
step:456/1825 train_time:15615ms step_avg:34.24ms
step:457/1825 train_time:15648ms step_avg:34.24ms
step:458/1825 train_time:15683ms step_avg:34.24ms
step:459/1825 train_time:15716ms step_avg:34.24ms
step:460/1825 train_time:15751ms step_avg:34.24ms
step:461/1825 train_time:15784ms step_avg:34.24ms
step:462/1825 train_time:15820ms step_avg:34.24ms
step:463/1825 train_time:15853ms step_avg:34.24ms
step:464/1825 train_time:15888ms step_avg:34.24ms
step:465/1825 train_time:15921ms step_avg:34.24ms
step:466/1825 train_time:15956ms step_avg:34.24ms
step:467/1825 train_time:15989ms step_avg:34.24ms
step:468/1825 train_time:16025ms step_avg:34.24ms
step:469/1825 train_time:16058ms step_avg:34.24ms
step:470/1825 train_time:16093ms step_avg:34.24ms
step:471/1825 train_time:16126ms step_avg:34.24ms
step:472/1825 train_time:16161ms step_avg:34.24ms
step:473/1825 train_time:16194ms step_avg:34.24ms
step:474/1825 train_time:16229ms step_avg:34.24ms
step:475/1825 train_time:16262ms step_avg:34.24ms
step:476/1825 train_time:16298ms step_avg:34.24ms
step:477/1825 train_time:16330ms step_avg:34.24ms
step:478/1825 train_time:16366ms step_avg:34.24ms
step:479/1825 train_time:16399ms step_avg:34.23ms
step:480/1825 train_time:16434ms step_avg:34.24ms
step:481/1825 train_time:16467ms step_avg:34.23ms
step:482/1825 train_time:16502ms step_avg:34.24ms
step:483/1825 train_time:16535ms step_avg:34.23ms
step:484/1825 train_time:16570ms step_avg:34.24ms
step:485/1825 train_time:16603ms step_avg:34.23ms
step:486/1825 train_time:16639ms step_avg:34.24ms
step:487/1825 train_time:16672ms step_avg:34.23ms
step:488/1825 train_time:16707ms step_avg:34.24ms
step:489/1825 train_time:16740ms step_avg:34.23ms
step:490/1825 train_time:16775ms step_avg:34.23ms
step:491/1825 train_time:16808ms step_avg:34.23ms
step:492/1825 train_time:16843ms step_avg:34.23ms
step:493/1825 train_time:16876ms step_avg:34.23ms
step:494/1825 train_time:16911ms step_avg:34.23ms
step:495/1825 train_time:16944ms step_avg:34.23ms
step:496/1825 train_time:16980ms step_avg:34.23ms
step:497/1825 train_time:17013ms step_avg:34.23ms
step:498/1825 train_time:17048ms step_avg:34.23ms
step:499/1825 train_time:17081ms step_avg:34.23ms
step:500/1825 train_time:17116ms step_avg:34.23ms
step:500/1825 val_loss:4.2851 train_time:17158ms step_avg:34.32ms
step:501/1825 train_time:17178ms step_avg:34.29ms
step:502/1825 train_time:17196ms step_avg:34.25ms
step:503/1825 train_time:17220ms step_avg:34.23ms
step:504/1825 train_time:17255ms step_avg:34.24ms
step:505/1825 train_time:17289ms step_avg:34.24ms
step:506/1825 train_time:17325ms step_avg:34.24ms
step:507/1825 train_time:17359ms step_avg:34.24ms
step:508/1825 train_time:17395ms step_avg:34.24ms
step:509/1825 train_time:17428ms step_avg:34.24ms
step:510/1825 train_time:17464ms step_avg:34.24ms
step:511/1825 train_time:17497ms step_avg:34.24ms
step:512/1825 train_time:17532ms step_avg:34.24ms
step:513/1825 train_time:17565ms step_avg:34.24ms
step:514/1825 train_time:17600ms step_avg:34.24ms
step:515/1825 train_time:17633ms step_avg:34.24ms
step:516/1825 train_time:17668ms step_avg:34.24ms
step:517/1825 train_time:17701ms step_avg:34.24ms
step:518/1825 train_time:17736ms step_avg:34.24ms
step:519/1825 train_time:17769ms step_avg:34.24ms
step:520/1825 train_time:17804ms step_avg:34.24ms
step:521/1825 train_time:17837ms step_avg:34.24ms
step:522/1825 train_time:17873ms step_avg:34.24ms
step:523/1825 train_time:17906ms step_avg:34.24ms
step:524/1825 train_time:17941ms step_avg:34.24ms
step:525/1825 train_time:17974ms step_avg:34.24ms
step:526/1825 train_time:18009ms step_avg:34.24ms
step:527/1825 train_time:18042ms step_avg:34.23ms
step:528/1825 train_time:18077ms step_avg:34.24ms
step:529/1825 train_time:18110ms step_avg:34.23ms
step:530/1825 train_time:18145ms step_avg:34.24ms
step:531/1825 train_time:18178ms step_avg:34.23ms
step:532/1825 train_time:18213ms step_avg:34.24ms
step:533/1825 train_time:18246ms step_avg:34.23ms
step:534/1825 train_time:18281ms step_avg:34.23ms
step:535/1825 train_time:18314ms step_avg:34.23ms
step:536/1825 train_time:18350ms step_avg:34.23ms
step:537/1825 train_time:18382ms step_avg:34.23ms
step:538/1825 train_time:18418ms step_avg:34.23ms
step:539/1825 train_time:18451ms step_avg:34.23ms
step:540/1825 train_time:18486ms step_avg:34.23ms
step:541/1825 train_time:18519ms step_avg:34.23ms
step:542/1825 train_time:18554ms step_avg:34.23ms
step:543/1825 train_time:18587ms step_avg:34.23ms
step:544/1825 train_time:18622ms step_avg:34.23ms
step:545/1825 train_time:18655ms step_avg:34.23ms
step:546/1825 train_time:18691ms step_avg:34.23ms
step:547/1825 train_time:18723ms step_avg:34.23ms
step:548/1825 train_time:18758ms step_avg:34.23ms
step:549/1825 train_time:18791ms step_avg:34.23ms
step:550/1825 train_time:18827ms step_avg:34.23ms
step:551/1825 train_time:18860ms step_avg:34.23ms
step:552/1825 train_time:18895ms step_avg:34.23ms
step:553/1825 train_time:18928ms step_avg:34.23ms
step:554/1825 train_time:18963ms step_avg:34.23ms
step:555/1825 train_time:18996ms step_avg:34.23ms
step:556/1825 train_time:19031ms step_avg:34.23ms
step:557/1825 train_time:19064ms step_avg:34.23ms
step:558/1825 train_time:19099ms step_avg:34.23ms
step:559/1825 train_time:19132ms step_avg:34.23ms
step:560/1825 train_time:19167ms step_avg:34.23ms
step:561/1825 train_time:19200ms step_avg:34.22ms
step:562/1825 train_time:19235ms step_avg:34.23ms
step:563/1825 train_time:19268ms step_avg:34.22ms
step:564/1825 train_time:19303ms step_avg:34.23ms
step:565/1825 train_time:19336ms step_avg:34.22ms
step:566/1825 train_time:19371ms step_avg:34.23ms
step:567/1825 train_time:19404ms step_avg:34.22ms
step:568/1825 train_time:19439ms step_avg:34.22ms
step:569/1825 train_time:19472ms step_avg:34.22ms
step:570/1825 train_time:19508ms step_avg:34.22ms
step:571/1825 train_time:19540ms step_avg:34.22ms
step:572/1825 train_time:19576ms step_avg:34.22ms
step:573/1825 train_time:19608ms step_avg:34.22ms
step:574/1825 train_time:19644ms step_avg:34.22ms
step:575/1825 train_time:19677ms step_avg:34.22ms
step:576/1825 train_time:19712ms step_avg:34.22ms
step:577/1825 train_time:19745ms step_avg:34.22ms
step:578/1825 train_time:19780ms step_avg:34.22ms
step:579/1825 train_time:19813ms step_avg:34.22ms
step:580/1825 train_time:19848ms step_avg:34.22ms
step:581/1825 train_time:19881ms step_avg:34.22ms
step:582/1825 train_time:19916ms step_avg:34.22ms
step:583/1825 train_time:19949ms step_avg:34.22ms
step:584/1825 train_time:19984ms step_avg:34.22ms
step:585/1825 train_time:20017ms step_avg:34.22ms
step:586/1825 train_time:20053ms step_avg:34.22ms
step:587/1825 train_time:20086ms step_avg:34.22ms
step:588/1825 train_time:20121ms step_avg:34.22ms
step:589/1825 train_time:20154ms step_avg:34.22ms
step:590/1825 train_time:20190ms step_avg:34.22ms
step:591/1825 train_time:20223ms step_avg:34.22ms
step:592/1825 train_time:20258ms step_avg:34.22ms
step:593/1825 train_time:20291ms step_avg:34.22ms
step:594/1825 train_time:20326ms step_avg:34.22ms
step:595/1825 train_time:20359ms step_avg:34.22ms
step:596/1825 train_time:20396ms step_avg:34.22ms
step:597/1825 train_time:20453ms step_avg:34.26ms
step:598/1825 train_time:20517ms step_avg:34.31ms
step:599/1825 train_time:20577ms step_avg:34.35ms
step:600/1825 train_time:20640ms step_avg:34.40ms
step:601/1825 train_time:20700ms step_avg:34.44ms
step:602/1825 train_time:20763ms step_avg:34.49ms
step:603/1825 train_time:20823ms step_avg:34.53ms
step:604/1825 train_time:20885ms step_avg:34.58ms
step:605/1825 train_time:20945ms step_avg:34.62ms
step:606/1825 train_time:21009ms step_avg:34.67ms
step:607/1825 train_time:21069ms step_avg:34.71ms
step:608/1825 train_time:21132ms step_avg:34.76ms
step:609/1825 train_time:21193ms step_avg:34.80ms
step:610/1825 train_time:21256ms step_avg:34.85ms
step:611/1825 train_time:21316ms step_avg:34.89ms
step:612/1825 train_time:21379ms step_avg:34.93ms
step:613/1825 train_time:21439ms step_avg:34.97ms
step:614/1825 train_time:21501ms step_avg:35.02ms
step:615/1825 train_time:21561ms step_avg:35.06ms
step:616/1825 train_time:21623ms step_avg:35.10ms
step:617/1825 train_time:21683ms step_avg:35.14ms
step:618/1825 train_time:21746ms step_avg:35.19ms
step:619/1825 train_time:21806ms step_avg:35.23ms
step:620/1825 train_time:21869ms step_avg:35.27ms
step:621/1825 train_time:21928ms step_avg:35.31ms
step:622/1825 train_time:21992ms step_avg:35.36ms
step:623/1825 train_time:22051ms step_avg:35.40ms
step:624/1825 train_time:22115ms step_avg:35.44ms
step:625/1825 train_time:22175ms step_avg:35.48ms
step:626/1825 train_time:22237ms step_avg:35.52ms
step:627/1825 train_time:22298ms step_avg:35.56ms
step:628/1825 train_time:22360ms step_avg:35.61ms
step:629/1825 train_time:22420ms step_avg:35.64ms
step:630/1825 train_time:22483ms step_avg:35.69ms
step:631/1825 train_time:22543ms step_avg:35.73ms
step:632/1825 train_time:22606ms step_avg:35.77ms
step:633/1825 train_time:22667ms step_avg:35.81ms
step:634/1825 train_time:22730ms step_avg:35.85ms
step:635/1825 train_time:22790ms step_avg:35.89ms
step:636/1825 train_time:22853ms step_avg:35.93ms
step:637/1825 train_time:22913ms step_avg:35.97ms
step:638/1825 train_time:22975ms step_avg:36.01ms
step:639/1825 train_time:23036ms step_avg:36.05ms
step:640/1825 train_time:23098ms step_avg:36.09ms
step:641/1825 train_time:23158ms step_avg:36.13ms
step:642/1825 train_time:23221ms step_avg:36.17ms
step:643/1825 train_time:23282ms step_avg:36.21ms
step:644/1825 train_time:23344ms step_avg:36.25ms
step:645/1825 train_time:23405ms step_avg:36.29ms
step:646/1825 train_time:23468ms step_avg:36.33ms
step:647/1825 train_time:23529ms step_avg:36.37ms
step:648/1825 train_time:23592ms step_avg:36.41ms
step:649/1825 train_time:23653ms step_avg:36.44ms
step:650/1825 train_time:23716ms step_avg:36.49ms
step:651/1825 train_time:23776ms step_avg:36.52ms
step:652/1825 train_time:23838ms step_avg:36.56ms
step:653/1825 train_time:23898ms step_avg:36.60ms
step:654/1825 train_time:23961ms step_avg:36.64ms
step:655/1825 train_time:24021ms step_avg:36.67ms
step:656/1825 train_time:24085ms step_avg:36.71ms
step:657/1825 train_time:24145ms step_avg:36.75ms
step:658/1825 train_time:24209ms step_avg:36.79ms
step:659/1825 train_time:24269ms step_avg:36.83ms
step:660/1825 train_time:24331ms step_avg:36.87ms
step:661/1825 train_time:24392ms step_avg:36.90ms
step:662/1825 train_time:24455ms step_avg:36.94ms
step:663/1825 train_time:24515ms step_avg:36.98ms
step:664/1825 train_time:24577ms step_avg:37.01ms
step:665/1825 train_time:24637ms step_avg:37.05ms
step:666/1825 train_time:24700ms step_avg:37.09ms
step:667/1825 train_time:24760ms step_avg:37.12ms
step:668/1825 train_time:24823ms step_avg:37.16ms
step:669/1825 train_time:24883ms step_avg:37.19ms
step:670/1825 train_time:24945ms step_avg:37.23ms
step:671/1825 train_time:25005ms step_avg:37.27ms
step:672/1825 train_time:25068ms step_avg:37.30ms
step:673/1825 train_time:25129ms step_avg:37.34ms
step:674/1825 train_time:25192ms step_avg:37.38ms
step:675/1825 train_time:25252ms step_avg:37.41ms
step:676/1825 train_time:25316ms step_avg:37.45ms
step:677/1825 train_time:25376ms step_avg:37.48ms
step:678/1825 train_time:25439ms step_avg:37.52ms
step:679/1825 train_time:25499ms step_avg:37.55ms
step:680/1825 train_time:25561ms step_avg:37.59ms
step:681/1825 train_time:25622ms step_avg:37.62ms
step:682/1825 train_time:25685ms step_avg:37.66ms
step:683/1825 train_time:25745ms step_avg:37.69ms
step:684/1825 train_time:25808ms step_avg:37.73ms
step:685/1825 train_time:25868ms step_avg:37.76ms
step:686/1825 train_time:25931ms step_avg:37.80ms
step:687/1825 train_time:25992ms step_avg:37.83ms
step:688/1825 train_time:26055ms step_avg:37.87ms
step:689/1825 train_time:26115ms step_avg:37.90ms
step:690/1825 train_time:26177ms step_avg:37.94ms
step:691/1825 train_time:26237ms step_avg:37.97ms
step:692/1825 train_time:26300ms step_avg:38.01ms
step:693/1825 train_time:26360ms step_avg:38.04ms
step:694/1825 train_time:26423ms step_avg:38.07ms
step:695/1825 train_time:26483ms step_avg:38.11ms
step:696/1825 train_time:26546ms step_avg:38.14ms
step:697/1825 train_time:26606ms step_avg:38.17ms
step:698/1825 train_time:26670ms step_avg:38.21ms
step:699/1825 train_time:26730ms step_avg:38.24ms
step:700/1825 train_time:26793ms step_avg:38.28ms
step:701/1825 train_time:26853ms step_avg:38.31ms
step:702/1825 train_time:26916ms step_avg:38.34ms
step:703/1825 train_time:26976ms step_avg:38.37ms
step:704/1825 train_time:27038ms step_avg:38.41ms
step:705/1825 train_time:27098ms step_avg:38.44ms
step:706/1825 train_time:27161ms step_avg:38.47ms
step:707/1825 train_time:27221ms step_avg:38.50ms
step:708/1825 train_time:27285ms step_avg:38.54ms
step:709/1825 train_time:27345ms step_avg:38.57ms
step:710/1825 train_time:27409ms step_avg:38.60ms
step:711/1825 train_time:27469ms step_avg:38.63ms
step:712/1825 train_time:27532ms step_avg:38.67ms
step:713/1825 train_time:27592ms step_avg:38.70ms
step:714/1825 train_time:27655ms step_avg:38.73ms
step:715/1825 train_time:27715ms step_avg:38.76ms
step:716/1825 train_time:27778ms step_avg:38.80ms
step:717/1825 train_time:27838ms step_avg:38.83ms
step:718/1825 train_time:27901ms step_avg:38.86ms
step:719/1825 train_time:27962ms step_avg:38.89ms
step:720/1825 train_time:28024ms step_avg:38.92ms
step:721/1825 train_time:28085ms step_avg:38.95ms
step:722/1825 train_time:28148ms step_avg:38.99ms
step:723/1825 train_time:28208ms step_avg:39.02ms
step:724/1825 train_time:28271ms step_avg:39.05ms
step:725/1825 train_time:28332ms step_avg:39.08ms
step:726/1825 train_time:28395ms step_avg:39.11ms
step:727/1825 train_time:28456ms step_avg:39.14ms
step:728/1825 train_time:28519ms step_avg:39.17ms
step:729/1825 train_time:28579ms step_avg:39.20ms
step:730/1825 train_time:28642ms step_avg:39.24ms
step:731/1825 train_time:28702ms step_avg:39.26ms
step:732/1825 train_time:28765ms step_avg:39.30ms
step:733/1825 train_time:28826ms step_avg:39.33ms
step:734/1825 train_time:28889ms step_avg:39.36ms
step:735/1825 train_time:28949ms step_avg:39.39ms
step:736/1825 train_time:29013ms step_avg:39.42ms
step:737/1825 train_time:29073ms step_avg:39.45ms
step:738/1825 train_time:29135ms step_avg:39.48ms
step:739/1825 train_time:29196ms step_avg:39.51ms
step:740/1825 train_time:29258ms step_avg:39.54ms
step:741/1825 train_time:29318ms step_avg:39.57ms
step:742/1825 train_time:29382ms step_avg:39.60ms
step:743/1825 train_time:29442ms step_avg:39.63ms
step:744/1825 train_time:29505ms step_avg:39.66ms
step:745/1825 train_time:29565ms step_avg:39.68ms
step:746/1825 train_time:29628ms step_avg:39.72ms
step:747/1825 train_time:29689ms step_avg:39.74ms
step:748/1825 train_time:29752ms step_avg:39.78ms
step:749/1825 train_time:29813ms step_avg:39.80ms
step:750/1825 train_time:29876ms step_avg:39.84ms
step:750/1825 val_loss:4.0102 train_time:29946ms step_avg:39.93ms
step:751/1825 train_time:29963ms step_avg:39.90ms
step:752/1825 train_time:30000ms step_avg:39.89ms
step:753/1825 train_time:30061ms step_avg:39.92ms
step:754/1825 train_time:30126ms step_avg:39.95ms
step:755/1825 train_time:30186ms step_avg:39.98ms
step:756/1825 train_time:30249ms step_avg:40.01ms
step:757/1825 train_time:30308ms step_avg:40.04ms
step:758/1825 train_time:30371ms step_avg:40.07ms
step:759/1825 train_time:30430ms step_avg:40.09ms
step:760/1825 train_time:30492ms step_avg:40.12ms
step:761/1825 train_time:30552ms step_avg:40.15ms
step:762/1825 train_time:30614ms step_avg:40.18ms
step:763/1825 train_time:30674ms step_avg:40.20ms
step:764/1825 train_time:30737ms step_avg:40.23ms
step:765/1825 train_time:30797ms step_avg:40.26ms
step:766/1825 train_time:30861ms step_avg:40.29ms
step:767/1825 train_time:30923ms step_avg:40.32ms
step:768/1825 train_time:30986ms step_avg:40.35ms
step:769/1825 train_time:31047ms step_avg:40.37ms
step:770/1825 train_time:31112ms step_avg:40.41ms
step:771/1825 train_time:31173ms step_avg:40.43ms
step:772/1825 train_time:31235ms step_avg:40.46ms
step:773/1825 train_time:31295ms step_avg:40.49ms
step:774/1825 train_time:31357ms step_avg:40.51ms
step:775/1825 train_time:31417ms step_avg:40.54ms
step:776/1825 train_time:31479ms step_avg:40.57ms
step:777/1825 train_time:31539ms step_avg:40.59ms
step:778/1825 train_time:31603ms step_avg:40.62ms
step:779/1825 train_time:31664ms step_avg:40.65ms
step:780/1825 train_time:31727ms step_avg:40.68ms
step:781/1825 train_time:31786ms step_avg:40.70ms
step:782/1825 train_time:31850ms step_avg:40.73ms
step:783/1825 train_time:31910ms step_avg:40.75ms
step:784/1825 train_time:31974ms step_avg:40.78ms
step:785/1825 train_time:32035ms step_avg:40.81ms
step:786/1825 train_time:32098ms step_avg:40.84ms
step:787/1825 train_time:32158ms step_avg:40.86ms
step:788/1825 train_time:32221ms step_avg:40.89ms
step:789/1825 train_time:32281ms step_avg:40.91ms
step:790/1825 train_time:32345ms step_avg:40.94ms
step:791/1825 train_time:32405ms step_avg:40.97ms
step:792/1825 train_time:32468ms step_avg:40.99ms
step:793/1825 train_time:32528ms step_avg:41.02ms
step:794/1825 train_time:32591ms step_avg:41.05ms
step:795/1825 train_time:32651ms step_avg:41.07ms
step:796/1825 train_time:32714ms step_avg:41.10ms
step:797/1825 train_time:32774ms step_avg:41.12ms
step:798/1825 train_time:32836ms step_avg:41.15ms
step:799/1825 train_time:32897ms step_avg:41.17ms
step:800/1825 train_time:32959ms step_avg:41.20ms
step:801/1825 train_time:33020ms step_avg:41.22ms
step:802/1825 train_time:33082ms step_avg:41.25ms
step:803/1825 train_time:33142ms step_avg:41.27ms
step:804/1825 train_time:33206ms step_avg:41.30ms
step:805/1825 train_time:33266ms step_avg:41.32ms
step:806/1825 train_time:33329ms step_avg:41.35ms
step:807/1825 train_time:33389ms step_avg:41.37ms
step:808/1825 train_time:33452ms step_avg:41.40ms
step:809/1825 train_time:33512ms step_avg:41.42ms
step:810/1825 train_time:33575ms step_avg:41.45ms
step:811/1825 train_time:33636ms step_avg:41.47ms
step:812/1825 train_time:33698ms step_avg:41.50ms
step:813/1825 train_time:33758ms step_avg:41.52ms
step:814/1825 train_time:33821ms step_avg:41.55ms
step:815/1825 train_time:33881ms step_avg:41.57ms
step:816/1825 train_time:33944ms step_avg:41.60ms
step:817/1825 train_time:34004ms step_avg:41.62ms
step:818/1825 train_time:34067ms step_avg:41.65ms
step:819/1825 train_time:34128ms step_avg:41.67ms
step:820/1825 train_time:34191ms step_avg:41.70ms
step:821/1825 train_time:34251ms step_avg:41.72ms
step:822/1825 train_time:34314ms step_avg:41.75ms
step:823/1825 train_time:34375ms step_avg:41.77ms
step:824/1825 train_time:34438ms step_avg:41.79ms
step:825/1825 train_time:34498ms step_avg:41.82ms
step:826/1825 train_time:34561ms step_avg:41.84ms
step:827/1825 train_time:34621ms step_avg:41.86ms
step:828/1825 train_time:34684ms step_avg:41.89ms
step:829/1825 train_time:34744ms step_avg:41.91ms
step:830/1825 train_time:34807ms step_avg:41.94ms
step:831/1825 train_time:34867ms step_avg:41.96ms
step:832/1825 train_time:34930ms step_avg:41.98ms
step:833/1825 train_time:34991ms step_avg:42.01ms
step:834/1825 train_time:35055ms step_avg:42.03ms
step:835/1825 train_time:35115ms step_avg:42.05ms
step:836/1825 train_time:35178ms step_avg:42.08ms
step:837/1825 train_time:35238ms step_avg:42.10ms
step:838/1825 train_time:35301ms step_avg:42.12ms
step:839/1825 train_time:35361ms step_avg:42.15ms
step:840/1825 train_time:35424ms step_avg:42.17ms
step:841/1825 train_time:35485ms step_avg:42.19ms
step:842/1825 train_time:35548ms step_avg:42.22ms
step:843/1825 train_time:35609ms step_avg:42.24ms
step:844/1825 train_time:35672ms step_avg:42.26ms
step:845/1825 train_time:35732ms step_avg:42.29ms
step:846/1825 train_time:35794ms step_avg:42.31ms
step:847/1825 train_time:35855ms step_avg:42.33ms
step:848/1825 train_time:35918ms step_avg:42.36ms
step:849/1825 train_time:35979ms step_avg:42.38ms
step:850/1825 train_time:36041ms step_avg:42.40ms
step:851/1825 train_time:36101ms step_avg:42.42ms
step:852/1825 train_time:36163ms step_avg:42.45ms
step:853/1825 train_time:36225ms step_avg:42.47ms
step:854/1825 train_time:36287ms step_avg:42.49ms
step:855/1825 train_time:36348ms step_avg:42.51ms
step:856/1825 train_time:36412ms step_avg:42.54ms
step:857/1825 train_time:36473ms step_avg:42.56ms
step:858/1825 train_time:36536ms step_avg:42.58ms
step:859/1825 train_time:36596ms step_avg:42.60ms
step:860/1825 train_time:36659ms step_avg:42.63ms
step:861/1825 train_time:36718ms step_avg:42.65ms
step:862/1825 train_time:36781ms step_avg:42.67ms
step:863/1825 train_time:36842ms step_avg:42.69ms
step:864/1825 train_time:36905ms step_avg:42.71ms
step:865/1825 train_time:36966ms step_avg:42.73ms
step:866/1825 train_time:37030ms step_avg:42.76ms
step:867/1825 train_time:37090ms step_avg:42.78ms
step:868/1825 train_time:37154ms step_avg:42.80ms
step:869/1825 train_time:37214ms step_avg:42.82ms
step:870/1825 train_time:37278ms step_avg:42.85ms
step:871/1825 train_time:37338ms step_avg:42.87ms
step:872/1825 train_time:37402ms step_avg:42.89ms
step:873/1825 train_time:37462ms step_avg:42.91ms
step:874/1825 train_time:37524ms step_avg:42.93ms
step:875/1825 train_time:37584ms step_avg:42.95ms
step:876/1825 train_time:37647ms step_avg:42.98ms
step:877/1825 train_time:37708ms step_avg:43.00ms
step:878/1825 train_time:37772ms step_avg:43.02ms
step:879/1825 train_time:37832ms step_avg:43.04ms
step:880/1825 train_time:37895ms step_avg:43.06ms
step:881/1825 train_time:37955ms step_avg:43.08ms
step:882/1825 train_time:38017ms step_avg:43.10ms
step:883/1825 train_time:38078ms step_avg:43.12ms
step:884/1825 train_time:38141ms step_avg:43.15ms
step:885/1825 train_time:38201ms step_avg:43.17ms
step:886/1825 train_time:38264ms step_avg:43.19ms
step:887/1825 train_time:38325ms step_avg:43.21ms
step:888/1825 train_time:38388ms step_avg:43.23ms
step:889/1825 train_time:38448ms step_avg:43.25ms
step:890/1825 train_time:38511ms step_avg:43.27ms
step:891/1825 train_time:38572ms step_avg:43.29ms
step:892/1825 train_time:38634ms step_avg:43.31ms
step:893/1825 train_time:38694ms step_avg:43.33ms
step:894/1825 train_time:38757ms step_avg:43.35ms
step:895/1825 train_time:38818ms step_avg:43.37ms
step:896/1825 train_time:38880ms step_avg:43.39ms
step:897/1825 train_time:38940ms step_avg:43.41ms
step:898/1825 train_time:39003ms step_avg:43.43ms
step:899/1825 train_time:39063ms step_avg:43.45ms
step:900/1825 train_time:39126ms step_avg:43.47ms
step:901/1825 train_time:39185ms step_avg:43.49ms
step:902/1825 train_time:39248ms step_avg:43.51ms
step:903/1825 train_time:39309ms step_avg:43.53ms
step:904/1825 train_time:39372ms step_avg:43.55ms
step:905/1825 train_time:39432ms step_avg:43.57ms
step:906/1825 train_time:39495ms step_avg:43.59ms
step:907/1825 train_time:39556ms step_avg:43.61ms
step:908/1825 train_time:39619ms step_avg:43.63ms
step:909/1825 train_time:39678ms step_avg:43.65ms
step:910/1825 train_time:39741ms step_avg:43.67ms
step:911/1825 train_time:39802ms step_avg:43.69ms
step:912/1825 train_time:39866ms step_avg:43.71ms
step:913/1825 train_time:39926ms step_avg:43.73ms
step:914/1825 train_time:39989ms step_avg:43.75ms
step:915/1825 train_time:40050ms step_avg:43.77ms
step:916/1825 train_time:40113ms step_avg:43.79ms
step:917/1825 train_time:40173ms step_avg:43.81ms
step:918/1825 train_time:40236ms step_avg:43.83ms
step:919/1825 train_time:40296ms step_avg:43.85ms
step:920/1825 train_time:40358ms step_avg:43.87ms
step:921/1825 train_time:40419ms step_avg:43.89ms
step:922/1825 train_time:40481ms step_avg:43.91ms
step:923/1825 train_time:40541ms step_avg:43.92ms
step:924/1825 train_time:40605ms step_avg:43.94ms
step:925/1825 train_time:40665ms step_avg:43.96ms
step:926/1825 train_time:40728ms step_avg:43.98ms
step:927/1825 train_time:40788ms step_avg:44.00ms
step:928/1825 train_time:40851ms step_avg:44.02ms
step:929/1825 train_time:40912ms step_avg:44.04ms
step:930/1825 train_time:40975ms step_avg:44.06ms
step:931/1825 train_time:41036ms step_avg:44.08ms
step:932/1825 train_time:41099ms step_avg:44.10ms
step:933/1825 train_time:41159ms step_avg:44.11ms
step:934/1825 train_time:41222ms step_avg:44.14ms
step:935/1825 train_time:41282ms step_avg:44.15ms
step:936/1825 train_time:41344ms step_avg:44.17ms
step:937/1825 train_time:41404ms step_avg:44.19ms
step:938/1825 train_time:41467ms step_avg:44.21ms
step:939/1825 train_time:41527ms step_avg:44.23ms
step:940/1825 train_time:41590ms step_avg:44.25ms
step:941/1825 train_time:41651ms step_avg:44.26ms
step:942/1825 train_time:41713ms step_avg:44.28ms
step:943/1825 train_time:41773ms step_avg:44.30ms
step:944/1825 train_time:41836ms step_avg:44.32ms
step:945/1825 train_time:41896ms step_avg:44.33ms
step:946/1825 train_time:41959ms step_avg:44.35ms
step:947/1825 train_time:42019ms step_avg:44.37ms
step:948/1825 train_time:42082ms step_avg:44.39ms
step:949/1825 train_time:42142ms step_avg:44.41ms
step:950/1825 train_time:42206ms step_avg:44.43ms
step:951/1825 train_time:42266ms step_avg:44.44ms
step:952/1825 train_time:42330ms step_avg:44.46ms
step:953/1825 train_time:42390ms step_avg:44.48ms
step:954/1825 train_time:42453ms step_avg:44.50ms
step:955/1825 train_time:42513ms step_avg:44.52ms
step:956/1825 train_time:42575ms step_avg:44.53ms
step:957/1825 train_time:42635ms step_avg:44.55ms
step:958/1825 train_time:42698ms step_avg:44.57ms
step:959/1825 train_time:42758ms step_avg:44.59ms
step:960/1825 train_time:42821ms step_avg:44.61ms
step:961/1825 train_time:42881ms step_avg:44.62ms
step:962/1825 train_time:42944ms step_avg:44.64ms
step:963/1825 train_time:43004ms step_avg:44.66ms
step:964/1825 train_time:43067ms step_avg:44.68ms
step:965/1825 train_time:43127ms step_avg:44.69ms
step:966/1825 train_time:43190ms step_avg:44.71ms
step:967/1825 train_time:43251ms step_avg:44.73ms
step:968/1825 train_time:43314ms step_avg:44.75ms
step:969/1825 train_time:43375ms step_avg:44.76ms
step:970/1825 train_time:43438ms step_avg:44.78ms
step:971/1825 train_time:43498ms step_avg:44.80ms
step:972/1825 train_time:43560ms step_avg:44.81ms
step:973/1825 train_time:43620ms step_avg:44.83ms
step:974/1825 train_time:43683ms step_avg:44.85ms
step:975/1825 train_time:43744ms step_avg:44.87ms
step:976/1825 train_time:43808ms step_avg:44.89ms
step:977/1825 train_time:43869ms step_avg:44.90ms
step:978/1825 train_time:43931ms step_avg:44.92ms
step:979/1825 train_time:43991ms step_avg:44.93ms
step:980/1825 train_time:44054ms step_avg:44.95ms
step:981/1825 train_time:44114ms step_avg:44.97ms
step:982/1825 train_time:44177ms step_avg:44.99ms
step:983/1825 train_time:44237ms step_avg:45.00ms
step:984/1825 train_time:44299ms step_avg:45.02ms
step:985/1825 train_time:44360ms step_avg:45.04ms
step:986/1825 train_time:44423ms step_avg:45.05ms
step:987/1825 train_time:44483ms step_avg:45.07ms
step:988/1825 train_time:44546ms step_avg:45.09ms
step:989/1825 train_time:44606ms step_avg:45.10ms
step:990/1825 train_time:44669ms step_avg:45.12ms
step:991/1825 train_time:44730ms step_avg:45.14ms
step:992/1825 train_time:44792ms step_avg:45.15ms
step:993/1825 train_time:44853ms step_avg:45.17ms
step:994/1825 train_time:44916ms step_avg:45.19ms
step:995/1825 train_time:44977ms step_avg:45.20ms
step:996/1825 train_time:45039ms step_avg:45.22ms
step:997/1825 train_time:45099ms step_avg:45.23ms
step:998/1825 train_time:45162ms step_avg:45.25ms
step:999/1825 train_time:45222ms step_avg:45.27ms
step:1000/1825 train_time:45284ms step_avg:45.28ms
step:1000/1825 val_loss:3.7746 train_time:45355ms step_avg:45.36ms
step:1001/1825 train_time:45377ms step_avg:45.33ms
step:1002/1825 train_time:45411ms step_avg:45.32ms
step:1003/1825 train_time:45474ms step_avg:45.34ms
step:1004/1825 train_time:45537ms step_avg:45.36ms
step:1005/1825 train_time:45600ms step_avg:45.37ms
step:1006/1825 train_time:45664ms step_avg:45.39ms
step:1007/1825 train_time:45724ms step_avg:45.41ms
step:1008/1825 train_time:45786ms step_avg:45.42ms
step:1009/1825 train_time:45846ms step_avg:45.44ms
step:1010/1825 train_time:45909ms step_avg:45.45ms
step:1011/1825 train_time:45969ms step_avg:45.47ms
step:1012/1825 train_time:46031ms step_avg:45.48ms
step:1013/1825 train_time:46091ms step_avg:45.50ms
step:1014/1825 train_time:46154ms step_avg:45.52ms
step:1015/1825 train_time:46215ms step_avg:45.53ms
step:1016/1825 train_time:46277ms step_avg:45.55ms
step:1017/1825 train_time:46337ms step_avg:45.56ms
step:1018/1825 train_time:46400ms step_avg:45.58ms
step:1019/1825 train_time:46461ms step_avg:45.59ms
step:1020/1825 train_time:46525ms step_avg:45.61ms
step:1021/1825 train_time:46586ms step_avg:45.63ms
step:1022/1825 train_time:46649ms step_avg:45.64ms
step:1023/1825 train_time:46709ms step_avg:45.66ms
step:1024/1825 train_time:46772ms step_avg:45.68ms
step:1025/1825 train_time:46832ms step_avg:45.69ms
step:1026/1825 train_time:46894ms step_avg:45.71ms
step:1027/1825 train_time:46954ms step_avg:45.72ms
step:1028/1825 train_time:47017ms step_avg:45.74ms
step:1029/1825 train_time:47077ms step_avg:45.75ms
step:1030/1825 train_time:47139ms step_avg:45.77ms
step:1031/1825 train_time:47199ms step_avg:45.78ms
step:1032/1825 train_time:47261ms step_avg:45.80ms
step:1033/1825 train_time:47321ms step_avg:45.81ms
step:1034/1825 train_time:47384ms step_avg:45.83ms
step:1035/1825 train_time:47444ms step_avg:45.84ms
step:1036/1825 train_time:47507ms step_avg:45.86ms
step:1037/1825 train_time:47568ms step_avg:45.87ms
step:1038/1825 train_time:47631ms step_avg:45.89ms
step:1039/1825 train_time:47692ms step_avg:45.90ms
step:1040/1825 train_time:47755ms step_avg:45.92ms
step:1041/1825 train_time:47815ms step_avg:45.93ms
step:1042/1825 train_time:47879ms step_avg:45.95ms
step:1043/1825 train_time:47939ms step_avg:45.96ms
step:1044/1825 train_time:48002ms step_avg:45.98ms
step:1045/1825 train_time:48063ms step_avg:45.99ms
step:1046/1825 train_time:48125ms step_avg:46.01ms
step:1047/1825 train_time:48185ms step_avg:46.02ms
step:1048/1825 train_time:48248ms step_avg:46.04ms
step:1049/1825 train_time:48308ms step_avg:46.05ms
step:1050/1825 train_time:48371ms step_avg:46.07ms
step:1051/1825 train_time:48430ms step_avg:46.08ms
step:1052/1825 train_time:48494ms step_avg:46.10ms
step:1053/1825 train_time:48555ms step_avg:46.11ms
step:1054/1825 train_time:48618ms step_avg:46.13ms
step:1055/1825 train_time:48678ms step_avg:46.14ms
step:1056/1825 train_time:48741ms step_avg:46.16ms
step:1057/1825 train_time:48801ms step_avg:46.17ms
step:1058/1825 train_time:48863ms step_avg:46.18ms
step:1059/1825 train_time:48924ms step_avg:46.20ms
step:1060/1825 train_time:48986ms step_avg:46.21ms
step:1061/1825 train_time:49046ms step_avg:46.23ms
step:1062/1825 train_time:49109ms step_avg:46.24ms
step:1063/1825 train_time:49169ms step_avg:46.26ms
step:1064/1825 train_time:49231ms step_avg:46.27ms
step:1065/1825 train_time:49292ms step_avg:46.28ms
step:1066/1825 train_time:49355ms step_avg:46.30ms
step:1067/1825 train_time:49415ms step_avg:46.31ms
step:1068/1825 train_time:49479ms step_avg:46.33ms
step:1069/1825 train_time:49539ms step_avg:46.34ms
step:1070/1825 train_time:49602ms step_avg:46.36ms
step:1071/1825 train_time:49663ms step_avg:46.37ms
step:1072/1825 train_time:49726ms step_avg:46.39ms
step:1073/1825 train_time:49786ms step_avg:46.40ms
step:1074/1825 train_time:49848ms step_avg:46.41ms
step:1075/1825 train_time:49908ms step_avg:46.43ms
step:1076/1825 train_time:49971ms step_avg:46.44ms
step:1077/1825 train_time:50030ms step_avg:46.45ms
step:1078/1825 train_time:50094ms step_avg:46.47ms
step:1079/1825 train_time:50154ms step_avg:46.48ms
step:1080/1825 train_time:50216ms step_avg:46.50ms
step:1081/1825 train_time:50277ms step_avg:46.51ms
step:1082/1825 train_time:50339ms step_avg:46.52ms
step:1083/1825 train_time:50399ms step_avg:46.54ms
step:1084/1825 train_time:50462ms step_avg:46.55ms
step:1085/1825 train_time:50523ms step_avg:46.56ms
step:1086/1825 train_time:50586ms step_avg:46.58ms
step:1087/1825 train_time:50647ms step_avg:46.59ms
step:1088/1825 train_time:50709ms step_avg:46.61ms
step:1089/1825 train_time:50769ms step_avg:46.62ms
step:1090/1825 train_time:50832ms step_avg:46.63ms
step:1091/1825 train_time:50892ms step_avg:46.65ms
step:1092/1825 train_time:50955ms step_avg:46.66ms
step:1093/1825 train_time:51016ms step_avg:46.68ms
step:1094/1825 train_time:51079ms step_avg:46.69ms
step:1095/1825 train_time:51139ms step_avg:46.70ms
step:1096/1825 train_time:51202ms step_avg:46.72ms
step:1097/1825 train_time:51263ms step_avg:46.73ms
step:1098/1825 train_time:51325ms step_avg:46.74ms
step:1099/1825 train_time:51385ms step_avg:46.76ms
step:1100/1825 train_time:51448ms step_avg:46.77ms
step:1101/1825 train_time:51509ms step_avg:46.78ms
step:1102/1825 train_time:51572ms step_avg:46.80ms
step:1103/1825 train_time:51633ms step_avg:46.81ms
step:1104/1825 train_time:51696ms step_avg:46.83ms
step:1105/1825 train_time:51756ms step_avg:46.84ms
step:1106/1825 train_time:51819ms step_avg:46.85ms
step:1107/1825 train_time:51880ms step_avg:46.87ms
step:1108/1825 train_time:51943ms step_avg:46.88ms
step:1109/1825 train_time:52004ms step_avg:46.89ms
step:1110/1825 train_time:52067ms step_avg:46.91ms
step:1111/1825 train_time:52127ms step_avg:46.92ms
step:1112/1825 train_time:52190ms step_avg:46.93ms
step:1113/1825 train_time:52251ms step_avg:46.95ms
step:1114/1825 train_time:52315ms step_avg:46.96ms
step:1115/1825 train_time:52375ms step_avg:46.97ms
step:1116/1825 train_time:52438ms step_avg:46.99ms
step:1117/1825 train_time:52498ms step_avg:47.00ms
step:1118/1825 train_time:52562ms step_avg:47.01ms
step:1119/1825 train_time:52622ms step_avg:47.03ms
step:1120/1825 train_time:52685ms step_avg:47.04ms
step:1121/1825 train_time:52746ms step_avg:47.05ms
step:1122/1825 train_time:52809ms step_avg:47.07ms
step:1123/1825 train_time:52869ms step_avg:47.08ms
step:1124/1825 train_time:52932ms step_avg:47.09ms
step:1125/1825 train_time:52992ms step_avg:47.10ms
step:1126/1825 train_time:53055ms step_avg:47.12ms
step:1127/1825 train_time:53116ms step_avg:47.13ms
step:1128/1825 train_time:53179ms step_avg:47.14ms
step:1129/1825 train_time:53240ms step_avg:47.16ms
step:1130/1825 train_time:53302ms step_avg:47.17ms
step:1131/1825 train_time:53362ms step_avg:47.18ms
step:1132/1825 train_time:53425ms step_avg:47.19ms
step:1133/1825 train_time:53484ms step_avg:47.21ms
step:1134/1825 train_time:53548ms step_avg:47.22ms
step:1135/1825 train_time:53608ms step_avg:47.23ms
step:1136/1825 train_time:53671ms step_avg:47.25ms
step:1137/1825 train_time:53731ms step_avg:47.26ms
step:1138/1825 train_time:53794ms step_avg:47.27ms
step:1139/1825 train_time:53854ms step_avg:47.28ms
step:1140/1825 train_time:53918ms step_avg:47.30ms
step:1141/1825 train_time:53979ms step_avg:47.31ms
step:1142/1825 train_time:54042ms step_avg:47.32ms
step:1143/1825 train_time:54102ms step_avg:47.33ms
step:1144/1825 train_time:54164ms step_avg:47.35ms
step:1145/1825 train_time:54224ms step_avg:47.36ms
step:1146/1825 train_time:54287ms step_avg:47.37ms
step:1147/1825 train_time:54347ms step_avg:47.38ms
step:1148/1825 train_time:54409ms step_avg:47.39ms
step:1149/1825 train_time:54469ms step_avg:47.41ms
step:1150/1825 train_time:54532ms step_avg:47.42ms
step:1151/1825 train_time:54593ms step_avg:47.43ms
step:1152/1825 train_time:54656ms step_avg:47.44ms
step:1153/1825 train_time:54716ms step_avg:47.46ms
step:1154/1825 train_time:54779ms step_avg:47.47ms
step:1155/1825 train_time:54839ms step_avg:47.48ms
step:1156/1825 train_time:54902ms step_avg:47.49ms
step:1157/1825 train_time:54963ms step_avg:47.51ms
step:1158/1825 train_time:55026ms step_avg:47.52ms
step:1159/1825 train_time:55086ms step_avg:47.53ms
step:1160/1825 train_time:55149ms step_avg:47.54ms
step:1161/1825 train_time:55209ms step_avg:47.55ms
step:1162/1825 train_time:55272ms step_avg:47.57ms
step:1163/1825 train_time:55332ms step_avg:47.58ms
step:1164/1825 train_time:55396ms step_avg:47.59ms
step:1165/1825 train_time:55455ms step_avg:47.60ms
step:1166/1825 train_time:55519ms step_avg:47.61ms
step:1167/1825 train_time:55579ms step_avg:47.63ms
step:1168/1825 train_time:55642ms step_avg:47.64ms
step:1169/1825 train_time:55702ms step_avg:47.65ms
step:1170/1825 train_time:55765ms step_avg:47.66ms
step:1171/1825 train_time:55825ms step_avg:47.67ms
step:1172/1825 train_time:55887ms step_avg:47.69ms
step:1173/1825 train_time:55948ms step_avg:47.70ms
step:1174/1825 train_time:56011ms step_avg:47.71ms
step:1175/1825 train_time:56072ms step_avg:47.72ms
step:1176/1825 train_time:56134ms step_avg:47.73ms
step:1177/1825 train_time:56194ms step_avg:47.74ms
step:1178/1825 train_time:56257ms step_avg:47.76ms
step:1179/1825 train_time:56318ms step_avg:47.77ms
step:1180/1825 train_time:56381ms step_avg:47.78ms
step:1181/1825 train_time:56442ms step_avg:47.79ms
step:1182/1825 train_time:56504ms step_avg:47.80ms
step:1183/1825 train_time:56564ms step_avg:47.81ms
step:1184/1825 train_time:56627ms step_avg:47.83ms
step:1185/1825 train_time:56686ms step_avg:47.84ms
step:1186/1825 train_time:56749ms step_avg:47.85ms
step:1187/1825 train_time:56809ms step_avg:47.86ms
step:1188/1825 train_time:56872ms step_avg:47.87ms
step:1189/1825 train_time:56932ms step_avg:47.88ms
step:1190/1825 train_time:56996ms step_avg:47.90ms
step:1191/1825 train_time:57058ms step_avg:47.91ms
step:1192/1825 train_time:57144ms step_avg:47.94ms
step:1193/1825 train_time:57232ms step_avg:47.97ms
step:1194/1825 train_time:57320ms step_avg:48.01ms
step:1195/1825 train_time:57406ms step_avg:48.04ms
step:1196/1825 train_time:57497ms step_avg:48.07ms
step:1197/1825 train_time:57582ms step_avg:48.11ms
step:1198/1825 train_time:57673ms step_avg:48.14ms
step:1199/1825 train_time:57759ms step_avg:48.17ms
step:1200/1825 train_time:57848ms step_avg:48.21ms
step:1201/1825 train_time:57937ms step_avg:48.24ms
step:1202/1825 train_time:58026ms step_avg:48.27ms
step:1203/1825 train_time:58113ms step_avg:48.31ms
step:1204/1825 train_time:58202ms step_avg:48.34ms
step:1205/1825 train_time:58288ms step_avg:48.37ms
step:1206/1825 train_time:58377ms step_avg:48.41ms
step:1207/1825 train_time:58464ms step_avg:48.44ms
step:1208/1825 train_time:58554ms step_avg:48.47ms
step:1209/1825 train_time:58640ms step_avg:48.50ms
step:1210/1825 train_time:58730ms step_avg:48.54ms
step:1211/1825 train_time:58816ms step_avg:48.57ms
step:1212/1825 train_time:58904ms step_avg:48.60ms
step:1213/1825 train_time:58991ms step_avg:48.63ms
step:1214/1825 train_time:59080ms step_avg:48.67ms
step:1215/1825 train_time:59167ms step_avg:48.70ms
step:1216/1825 train_time:59256ms step_avg:48.73ms
step:1217/1825 train_time:59341ms step_avg:48.76ms
step:1218/1825 train_time:59430ms step_avg:48.79ms
step:1219/1825 train_time:59517ms step_avg:48.82ms
step:1220/1825 train_time:59606ms step_avg:48.86ms
step:1221/1825 train_time:59695ms step_avg:48.89ms
step:1222/1825 train_time:59783ms step_avg:48.92ms
step:1223/1825 train_time:59870ms step_avg:48.95ms
step:1224/1825 train_time:59959ms step_avg:48.99ms
step:1225/1825 train_time:60044ms step_avg:49.02ms
step:1226/1825 train_time:60134ms step_avg:49.05ms
step:1227/1825 train_time:60220ms step_avg:49.08ms
step:1228/1825 train_time:60309ms step_avg:49.11ms
step:1229/1825 train_time:60395ms step_avg:49.14ms
step:1230/1825 train_time:60483ms step_avg:49.17ms
step:1231/1825 train_time:60571ms step_avg:49.21ms
step:1232/1825 train_time:60661ms step_avg:49.24ms
step:1233/1825 train_time:60748ms step_avg:49.27ms
step:1234/1825 train_time:60838ms step_avg:49.30ms
step:1235/1825 train_time:60923ms step_avg:49.33ms
step:1236/1825 train_time:61012ms step_avg:49.36ms
step:1237/1825 train_time:61099ms step_avg:49.39ms
step:1238/1825 train_time:61188ms step_avg:49.42ms
step:1239/1825 train_time:61275ms step_avg:49.46ms
step:1240/1825 train_time:61364ms step_avg:49.49ms
step:1241/1825 train_time:61452ms step_avg:49.52ms
step:1242/1825 train_time:61541ms step_avg:49.55ms
step:1243/1825 train_time:61628ms step_avg:49.58ms
step:1244/1825 train_time:61717ms step_avg:49.61ms
step:1245/1825 train_time:61803ms step_avg:49.64ms
step:1246/1825 train_time:61892ms step_avg:49.67ms
step:1247/1825 train_time:61979ms step_avg:49.70ms
step:1248/1825 train_time:62070ms step_avg:49.74ms
step:1249/1825 train_time:62158ms step_avg:49.77ms
step:1250/1825 train_time:62246ms step_avg:49.80ms
step:1250/1825 val_loss:3.5270 train_time:62343ms step_avg:49.87ms
step:1251/1825 train_time:62361ms step_avg:49.85ms
step:1252/1825 train_time:62424ms step_avg:49.86ms
step:1253/1825 train_time:62512ms step_avg:49.89ms
step:1254/1825 train_time:62601ms step_avg:49.92ms
step:1255/1825 train_time:62688ms step_avg:49.95ms
step:1256/1825 train_time:62778ms step_avg:49.98ms
step:1257/1825 train_time:62864ms step_avg:50.01ms
step:1258/1825 train_time:62953ms step_avg:50.04ms
step:1259/1825 train_time:63039ms step_avg:50.07ms
step:1260/1825 train_time:63129ms step_avg:50.10ms
step:1261/1825 train_time:63213ms step_avg:50.13ms
step:1262/1825 train_time:63304ms step_avg:50.16ms
step:1263/1825 train_time:63392ms step_avg:50.19ms
step:1264/1825 train_time:63482ms step_avg:50.22ms
step:1265/1825 train_time:63570ms step_avg:50.25ms
step:1266/1825 train_time:63659ms step_avg:50.28ms
step:1267/1825 train_time:63747ms step_avg:50.31ms
step:1268/1825 train_time:63835ms step_avg:50.34ms
step:1269/1825 train_time:63920ms step_avg:50.37ms
step:1270/1825 train_time:64011ms step_avg:50.40ms
step:1271/1825 train_time:64096ms step_avg:50.43ms
step:1272/1825 train_time:64185ms step_avg:50.46ms
step:1273/1825 train_time:64272ms step_avg:50.49ms
step:1274/1825 train_time:64361ms step_avg:50.52ms
step:1275/1825 train_time:64449ms step_avg:50.55ms
step:1276/1825 train_time:64539ms step_avg:50.58ms
step:1277/1825 train_time:64626ms step_avg:50.61ms
step:1278/1825 train_time:64714ms step_avg:50.64ms
step:1279/1825 train_time:64801ms step_avg:50.67ms
step:1280/1825 train_time:64889ms step_avg:50.69ms
step:1281/1825 train_time:64974ms step_avg:50.72ms
step:1282/1825 train_time:65063ms step_avg:50.75ms
step:1283/1825 train_time:65150ms step_avg:50.78ms
step:1284/1825 train_time:65238ms step_avg:50.81ms
step:1285/1825 train_time:65325ms step_avg:50.84ms
step:1286/1825 train_time:65416ms step_avg:50.87ms
step:1287/1825 train_time:65504ms step_avg:50.90ms
step:1288/1825 train_time:65594ms step_avg:50.93ms
step:1289/1825 train_time:65680ms step_avg:50.95ms
step:1290/1825 train_time:65770ms step_avg:50.98ms
step:1291/1825 train_time:65856ms step_avg:51.01ms
step:1292/1825 train_time:65945ms step_avg:51.04ms
step:1293/1825 train_time:66032ms step_avg:51.07ms
step:1294/1825 train_time:66120ms step_avg:51.10ms
step:1295/1825 train_time:66207ms step_avg:51.13ms
step:1296/1825 train_time:66295ms step_avg:51.15ms
step:1297/1825 train_time:66382ms step_avg:51.18ms
step:1298/1825 train_time:66474ms step_avg:51.21ms
step:1299/1825 train_time:66561ms step_avg:51.24ms
step:1300/1825 train_time:66649ms step_avg:51.27ms
step:1301/1825 train_time:66736ms step_avg:51.30ms
step:1302/1825 train_time:66826ms step_avg:51.33ms
step:1303/1825 train_time:66912ms step_avg:51.35ms
step:1304/1825 train_time:67001ms step_avg:51.38ms
step:1305/1825 train_time:67087ms step_avg:51.41ms
step:1306/1825 train_time:67176ms step_avg:51.44ms
step:1307/1825 train_time:67262ms step_avg:51.46ms
step:1308/1825 train_time:67352ms step_avg:51.49ms
step:1309/1825 train_time:67438ms step_avg:51.52ms
step:1310/1825 train_time:67528ms step_avg:51.55ms
step:1311/1825 train_time:67615ms step_avg:51.57ms
step:1312/1825 train_time:67704ms step_avg:51.60ms
step:1313/1825 train_time:67791ms step_avg:51.63ms
step:1314/1825 train_time:67879ms step_avg:51.66ms
step:1315/1825 train_time:67965ms step_avg:51.68ms
step:1316/1825 train_time:68054ms step_avg:51.71ms
step:1317/1825 train_time:68139ms step_avg:51.74ms
step:1318/1825 train_time:68230ms step_avg:51.77ms
step:1319/1825 train_time:68316ms step_avg:51.79ms
step:1320/1825 train_time:68406ms step_avg:51.82ms
step:1321/1825 train_time:68493ms step_avg:51.85ms
step:1322/1825 train_time:68581ms step_avg:51.88ms
step:1323/1825 train_time:68670ms step_avg:51.90ms
step:1324/1825 train_time:68759ms step_avg:51.93ms
step:1325/1825 train_time:68846ms step_avg:51.96ms
step:1326/1825 train_time:68935ms step_avg:51.99ms
step:1327/1825 train_time:69020ms step_avg:52.01ms
step:1328/1825 train_time:69110ms step_avg:52.04ms
step:1329/1825 train_time:69197ms step_avg:52.07ms
step:1330/1825 train_time:69286ms step_avg:52.09ms
step:1331/1825 train_time:69374ms step_avg:52.12ms
step:1332/1825 train_time:69463ms step_avg:52.15ms
step:1333/1825 train_time:69549ms step_avg:52.17ms
step:1334/1825 train_time:69639ms step_avg:52.20ms
step:1335/1825 train_time:69725ms step_avg:52.23ms
step:1336/1825 train_time:69815ms step_avg:52.26ms
step:1337/1825 train_time:69903ms step_avg:52.28ms
step:1338/1825 train_time:69993ms step_avg:52.31ms
step:1339/1825 train_time:70078ms step_avg:52.34ms
step:1340/1825 train_time:70169ms step_avg:52.36ms
step:1341/1825 train_time:70254ms step_avg:52.39ms
step:1342/1825 train_time:70344ms step_avg:52.42ms
step:1343/1825 train_time:70432ms step_avg:52.44ms
step:1344/1825 train_time:70520ms step_avg:52.47ms
step:1345/1825 train_time:70608ms step_avg:52.50ms
step:1346/1825 train_time:70698ms step_avg:52.52ms
step:1347/1825 train_time:70785ms step_avg:52.55ms
step:1348/1825 train_time:70874ms step_avg:52.58ms
step:1349/1825 train_time:70960ms step_avg:52.60ms
step:1350/1825 train_time:71050ms step_avg:52.63ms
step:1351/1825 train_time:71136ms step_avg:52.65ms
step:1352/1825 train_time:71224ms step_avg:52.68ms
step:1353/1825 train_time:71311ms step_avg:52.71ms
step:1354/1825 train_time:71400ms step_avg:52.73ms
step:1355/1825 train_time:71487ms step_avg:52.76ms
step:1356/1825 train_time:71578ms step_avg:52.79ms
step:1357/1825 train_time:71664ms step_avg:52.81ms
step:1358/1825 train_time:71754ms step_avg:52.84ms
step:1359/1825 train_time:71843ms step_avg:52.86ms
step:1360/1825 train_time:71933ms step_avg:52.89ms
step:1361/1825 train_time:72019ms step_avg:52.92ms
step:1362/1825 train_time:72108ms step_avg:52.94ms
step:1363/1825 train_time:72194ms step_avg:52.97ms
step:1364/1825 train_time:72283ms step_avg:52.99ms
step:1365/1825 train_time:72370ms step_avg:53.02ms
step:1366/1825 train_time:72458ms step_avg:53.04ms
step:1367/1825 train_time:72544ms step_avg:53.07ms
step:1368/1825 train_time:72634ms step_avg:53.10ms
step:1369/1825 train_time:72720ms step_avg:53.12ms
step:1370/1825 train_time:72811ms step_avg:53.15ms
step:1371/1825 train_time:72897ms step_avg:53.17ms
step:1372/1825 train_time:72987ms step_avg:53.20ms
step:1373/1825 train_time:73073ms step_avg:53.22ms
step:1374/1825 train_time:73162ms step_avg:53.25ms
step:1375/1825 train_time:73250ms step_avg:53.27ms
step:1376/1825 train_time:73338ms step_avg:53.30ms
step:1377/1825 train_time:73426ms step_avg:53.32ms
step:1378/1825 train_time:73514ms step_avg:53.35ms
step:1379/1825 train_time:73602ms step_avg:53.37ms
step:1380/1825 train_time:73691ms step_avg:53.40ms
step:1381/1825 train_time:73778ms step_avg:53.42ms
step:1382/1825 train_time:73867ms step_avg:53.45ms
step:1383/1825 train_time:73954ms step_avg:53.47ms
step:1384/1825 train_time:74043ms step_avg:53.50ms
step:1385/1825 train_time:74129ms step_avg:53.52ms
step:1386/1825 train_time:74218ms step_avg:53.55ms
step:1387/1825 train_time:74305ms step_avg:53.57ms
step:1388/1825 train_time:74394ms step_avg:53.60ms
step:1389/1825 train_time:74480ms step_avg:53.62ms
step:1390/1825 train_time:74569ms step_avg:53.65ms
step:1391/1825 train_time:74655ms step_avg:53.67ms
step:1392/1825 train_time:74745ms step_avg:53.70ms
step:1393/1825 train_time:74832ms step_avg:53.72ms
step:1394/1825 train_time:74922ms step_avg:53.75ms
step:1395/1825 train_time:75008ms step_avg:53.77ms
step:1396/1825 train_time:75097ms step_avg:53.79ms
step:1397/1825 train_time:75183ms step_avg:53.82ms
step:1398/1825 train_time:75273ms step_avg:53.84ms
step:1399/1825 train_time:75359ms step_avg:53.87ms
step:1400/1825 train_time:75450ms step_avg:53.89ms
step:1401/1825 train_time:75536ms step_avg:53.92ms
step:1402/1825 train_time:75626ms step_avg:53.94ms
step:1403/1825 train_time:75712ms step_avg:53.96ms
step:1404/1825 train_time:75801ms step_avg:53.99ms
step:1405/1825 train_time:75889ms step_avg:54.01ms
step:1406/1825 train_time:75978ms step_avg:54.04ms
step:1407/1825 train_time:76064ms step_avg:54.06ms
step:1408/1825 train_time:76154ms step_avg:54.09ms
step:1409/1825 train_time:76240ms step_avg:54.11ms
step:1410/1825 train_time:76331ms step_avg:54.14ms
step:1411/1825 train_time:76417ms step_avg:54.16ms
step:1412/1825 train_time:76507ms step_avg:54.18ms
step:1413/1825 train_time:76593ms step_avg:54.21ms
step:1414/1825 train_time:76683ms step_avg:54.23ms
step:1415/1825 train_time:76769ms step_avg:54.25ms
step:1416/1825 train_time:76857ms step_avg:54.28ms
step:1417/1825 train_time:76944ms step_avg:54.30ms
step:1418/1825 train_time:77034ms step_avg:54.33ms
step:1419/1825 train_time:77120ms step_avg:54.35ms
step:1420/1825 train_time:77208ms step_avg:54.37ms
step:1421/1825 train_time:77294ms step_avg:54.39ms
step:1422/1825 train_time:77384ms step_avg:54.42ms
step:1423/1825 train_time:77470ms step_avg:54.44ms
step:1424/1825 train_time:77559ms step_avg:54.47ms
step:1425/1825 train_time:77647ms step_avg:54.49ms
step:1426/1825 train_time:77736ms step_avg:54.51ms
step:1427/1825 train_time:77822ms step_avg:54.54ms
step:1428/1825 train_time:77913ms step_avg:54.56ms
step:1429/1825 train_time:77999ms step_avg:54.58ms
step:1430/1825 train_time:78088ms step_avg:54.61ms
step:1431/1825 train_time:78175ms step_avg:54.63ms
step:1432/1825 train_time:78264ms step_avg:54.65ms
step:1433/1825 train_time:78351ms step_avg:54.68ms
step:1434/1825 train_time:78439ms step_avg:54.70ms
step:1435/1825 train_time:78526ms step_avg:54.72ms
step:1436/1825 train_time:78616ms step_avg:54.75ms
step:1437/1825 train_time:78702ms step_avg:54.77ms
step:1438/1825 train_time:78793ms step_avg:54.79ms
step:1439/1825 train_time:78878ms step_avg:54.81ms
step:1440/1825 train_time:78968ms step_avg:54.84ms
step:1441/1825 train_time:79055ms step_avg:54.86ms
step:1442/1825 train_time:79144ms step_avg:54.88ms
step:1443/1825 train_time:79231ms step_avg:54.91ms
step:1444/1825 train_time:79319ms step_avg:54.93ms
step:1445/1825 train_time:79406ms step_avg:54.95ms
step:1446/1825 train_time:79496ms step_avg:54.98ms
step:1447/1825 train_time:79583ms step_avg:55.00ms
step:1448/1825 train_time:79673ms step_avg:55.02ms
step:1449/1825 train_time:79760ms step_avg:55.05ms
step:1450/1825 train_time:79851ms step_avg:55.07ms
step:1451/1825 train_time:79936ms step_avg:55.09ms
step:1452/1825 train_time:80025ms step_avg:55.11ms
step:1453/1825 train_time:80111ms step_avg:55.14ms
step:1454/1825 train_time:80200ms step_avg:55.16ms
step:1455/1825 train_time:80288ms step_avg:55.18ms
step:1456/1825 train_time:80376ms step_avg:55.20ms
step:1457/1825 train_time:80462ms step_avg:55.22ms
step:1458/1825 train_time:80552ms step_avg:55.25ms
step:1459/1825 train_time:80640ms step_avg:55.27ms
step:1460/1825 train_time:80730ms step_avg:55.29ms
step:1461/1825 train_time:80816ms step_avg:55.32ms
step:1462/1825 train_time:80906ms step_avg:55.34ms
step:1463/1825 train_time:80992ms step_avg:55.36ms
step:1464/1825 train_time:81080ms step_avg:55.38ms
step:1465/1825 train_time:81168ms step_avg:55.40ms
step:1466/1825 train_time:81257ms step_avg:55.43ms
step:1467/1825 train_time:81343ms step_avg:55.45ms
step:1468/1825 train_time:81433ms step_avg:55.47ms
step:1469/1825 train_time:81519ms step_avg:55.49ms
step:1470/1825 train_time:81609ms step_avg:55.52ms
step:1471/1825 train_time:81695ms step_avg:55.54ms
step:1472/1825 train_time:81784ms step_avg:55.56ms
step:1473/1825 train_time:81871ms step_avg:55.58ms
step:1474/1825 train_time:81959ms step_avg:55.60ms
step:1475/1825 train_time:82048ms step_avg:55.63ms
step:1476/1825 train_time:82137ms step_avg:55.65ms
step:1477/1825 train_time:82224ms step_avg:55.67ms
step:1478/1825 train_time:82313ms step_avg:55.69ms
step:1479/1825 train_time:82399ms step_avg:55.71ms
step:1480/1825 train_time:82489ms step_avg:55.74ms
step:1481/1825 train_time:82576ms step_avg:55.76ms
step:1482/1825 train_time:82665ms step_avg:55.78ms
step:1483/1825 train_time:82751ms step_avg:55.80ms
step:1484/1825 train_time:82840ms step_avg:55.82ms
step:1485/1825 train_time:82926ms step_avg:55.84ms
step:1486/1825 train_time:83015ms step_avg:55.86ms
step:1487/1825 train_time:83101ms step_avg:55.88ms
step:1488/1825 train_time:83191ms step_avg:55.91ms
step:1489/1825 train_time:83278ms step_avg:55.93ms
step:1490/1825 train_time:83367ms step_avg:55.95ms
step:1491/1825 train_time:83453ms step_avg:55.97ms
step:1492/1825 train_time:83542ms step_avg:55.99ms
step:1493/1825 train_time:83629ms step_avg:56.01ms
step:1494/1825 train_time:83718ms step_avg:56.04ms
step:1495/1825 train_time:83803ms step_avg:56.06ms
step:1496/1825 train_time:83894ms step_avg:56.08ms
step:1497/1825 train_time:83981ms step_avg:56.10ms
step:1498/1825 train_time:84072ms step_avg:56.12ms
step:1499/1825 train_time:84158ms step_avg:56.14ms
step:1500/1825 train_time:84247ms step_avg:56.16ms
step:1500/1825 val_loss:3.3965 train_time:84344ms step_avg:56.23ms
step:1501/1825 train_time:84364ms step_avg:56.21ms
step:1502/1825 train_time:84423ms step_avg:56.21ms
step:1503/1825 train_time:84513ms step_avg:56.23ms
step:1504/1825 train_time:84602ms step_avg:56.25ms
step:1505/1825 train_time:84688ms step_avg:56.27ms
step:1506/1825 train_time:84780ms step_avg:56.29ms
step:1507/1825 train_time:84865ms step_avg:56.31ms
step:1508/1825 train_time:84953ms step_avg:56.34ms
step:1509/1825 train_time:85040ms step_avg:56.35ms
step:1510/1825 train_time:85128ms step_avg:56.38ms
step:1511/1825 train_time:85214ms step_avg:56.40ms
step:1512/1825 train_time:85304ms step_avg:56.42ms
step:1513/1825 train_time:85393ms step_avg:56.44ms
step:1514/1825 train_time:85485ms step_avg:56.46ms
step:1515/1825 train_time:85571ms step_avg:56.48ms
step:1516/1825 train_time:85660ms step_avg:56.50ms
step:1517/1825 train_time:85746ms step_avg:56.52ms
step:1518/1825 train_time:85835ms step_avg:56.54ms
step:1519/1825 train_time:85921ms step_avg:56.56ms
step:1520/1825 train_time:86009ms step_avg:56.59ms
step:1521/1825 train_time:86095ms step_avg:56.60ms
step:1522/1825 train_time:86185ms step_avg:56.63ms
step:1523/1825 train_time:86272ms step_avg:56.65ms
step:1524/1825 train_time:86362ms step_avg:56.67ms
step:1525/1825 train_time:86450ms step_avg:56.69ms
step:1526/1825 train_time:86539ms step_avg:56.71ms
step:1527/1825 train_time:86626ms step_avg:56.73ms
step:1528/1825 train_time:86716ms step_avg:56.75ms
step:1529/1825 train_time:86801ms step_avg:56.77ms
step:1530/1825 train_time:86890ms step_avg:56.79ms
step:1531/1825 train_time:86976ms step_avg:56.81ms
step:1532/1825 train_time:87066ms step_avg:56.83ms
step:1533/1825 train_time:87151ms step_avg:56.85ms
step:1534/1825 train_time:87241ms step_avg:56.87ms
step:1535/1825 train_time:87328ms step_avg:56.89ms
step:1536/1825 train_time:87418ms step_avg:56.91ms
step:1537/1825 train_time:87505ms step_avg:56.93ms
step:1538/1825 train_time:87594ms step_avg:56.95ms
step:1539/1825 train_time:87681ms step_avg:56.97ms
step:1540/1825 train_time:87770ms step_avg:56.99ms
step:1541/1825 train_time:87856ms step_avg:57.01ms
step:1542/1825 train_time:87946ms step_avg:57.03ms
step:1543/1825 train_time:88031ms step_avg:57.05ms
step:1544/1825 train_time:88121ms step_avg:57.07ms
step:1545/1825 train_time:88208ms step_avg:57.09ms
step:1546/1825 train_time:88297ms step_avg:57.11ms
step:1547/1825 train_time:88384ms step_avg:57.13ms
step:1548/1825 train_time:88472ms step_avg:57.15ms
step:1549/1825 train_time:88560ms step_avg:57.17ms
step:1550/1825 train_time:88650ms step_avg:57.19ms
step:1551/1825 train_time:88736ms step_avg:57.21ms
step:1552/1825 train_time:88826ms step_avg:57.23ms
step:1553/1825 train_time:88912ms step_avg:57.25ms
step:1554/1825 train_time:89000ms step_avg:57.27ms
step:1555/1825 train_time:89086ms step_avg:57.29ms
step:1556/1825 train_time:89175ms step_avg:57.31ms
step:1557/1825 train_time:89263ms step_avg:57.33ms
step:1558/1825 train_time:89351ms step_avg:57.35ms
step:1559/1825 train_time:89438ms step_avg:57.37ms
step:1560/1825 train_time:89528ms step_avg:57.39ms
step:1561/1825 train_time:89614ms step_avg:57.41ms
step:1562/1825 train_time:89704ms step_avg:57.43ms
step:1563/1825 train_time:89790ms step_avg:57.45ms
step:1564/1825 train_time:89878ms step_avg:57.47ms
step:1565/1825 train_time:89965ms step_avg:57.49ms
step:1566/1825 train_time:90053ms step_avg:57.51ms
step:1567/1825 train_time:90140ms step_avg:57.52ms
step:1568/1825 train_time:90231ms step_avg:57.55ms
step:1569/1825 train_time:90317ms step_avg:57.56ms
step:1570/1825 train_time:90407ms step_avg:57.58ms
step:1571/1825 train_time:90494ms step_avg:57.60ms
step:1572/1825 train_time:90584ms step_avg:57.62ms
step:1573/1825 train_time:90671ms step_avg:57.64ms
step:1574/1825 train_time:90761ms step_avg:57.66ms
step:1575/1825 train_time:90847ms step_avg:57.68ms
step:1576/1825 train_time:90936ms step_avg:57.70ms
step:1577/1825 train_time:91022ms step_avg:57.72ms
step:1578/1825 train_time:91111ms step_avg:57.74ms
step:1579/1825 train_time:91197ms step_avg:57.76ms
step:1580/1825 train_time:91289ms step_avg:57.78ms
step:1581/1825 train_time:91375ms step_avg:57.80ms
step:1582/1825 train_time:91467ms step_avg:57.82ms
step:1583/1825 train_time:91552ms step_avg:57.83ms
step:1584/1825 train_time:91643ms step_avg:57.86ms
step:1585/1825 train_time:91728ms step_avg:57.87ms
step:1586/1825 train_time:91818ms step_avg:57.89ms
step:1587/1825 train_time:91905ms step_avg:57.91ms
step:1588/1825 train_time:91993ms step_avg:57.93ms
step:1589/1825 train_time:92079ms step_avg:57.95ms
step:1590/1825 train_time:92169ms step_avg:57.97ms
step:1591/1825 train_time:92254ms step_avg:57.98ms
step:1592/1825 train_time:92346ms step_avg:58.01ms
step:1593/1825 train_time:92432ms step_avg:58.02ms
step:1594/1825 train_time:92522ms step_avg:58.04ms
step:1595/1825 train_time:92608ms step_avg:58.06ms
step:1596/1825 train_time:92697ms step_avg:58.08ms
step:1597/1825 train_time:92785ms step_avg:58.10ms
step:1598/1825 train_time:92873ms step_avg:58.12ms
step:1599/1825 train_time:92959ms step_avg:58.14ms
step:1600/1825 train_time:93048ms step_avg:58.16ms
step:1601/1825 train_time:93134ms step_avg:58.17ms
step:1602/1825 train_time:93224ms step_avg:58.19ms
step:1603/1825 train_time:93311ms step_avg:58.21ms
step:1604/1825 train_time:93400ms step_avg:58.23ms
step:1605/1825 train_time:93487ms step_avg:58.25ms
step:1606/1825 train_time:93577ms step_avg:58.27ms
step:1607/1825 train_time:93664ms step_avg:58.28ms
step:1608/1825 train_time:93753ms step_avg:58.30ms
step:1609/1825 train_time:93840ms step_avg:58.32ms
step:1610/1825 train_time:93928ms step_avg:58.34ms
step:1611/1825 train_time:94015ms step_avg:58.36ms
step:1612/1825 train_time:94105ms step_avg:58.38ms
step:1613/1825 train_time:94192ms step_avg:58.40ms
step:1614/1825 train_time:94281ms step_avg:58.41ms
step:1615/1825 train_time:94369ms step_avg:58.43ms
step:1616/1825 train_time:94459ms step_avg:58.45ms
step:1617/1825 train_time:94545ms step_avg:58.47ms
step:1618/1825 train_time:94634ms step_avg:58.49ms
step:1619/1825 train_time:94720ms step_avg:58.51ms
step:1620/1825 train_time:94809ms step_avg:58.52ms
step:1621/1825 train_time:94895ms step_avg:58.54ms
step:1622/1825 train_time:94986ms step_avg:58.56ms
step:1623/1825 train_time:95072ms step_avg:58.58ms
step:1624/1825 train_time:95162ms step_avg:58.60ms
step:1625/1825 train_time:95248ms step_avg:58.61ms
step:1626/1825 train_time:95337ms step_avg:58.63ms
step:1627/1825 train_time:95423ms step_avg:58.65ms
step:1628/1825 train_time:95512ms step_avg:58.67ms
step:1629/1825 train_time:95600ms step_avg:58.69ms
step:1630/1825 train_time:95690ms step_avg:58.71ms
step:1631/1825 train_time:95776ms step_avg:58.72ms
step:1632/1825 train_time:95866ms step_avg:58.74ms
step:1633/1825 train_time:95952ms step_avg:58.76ms
step:1634/1825 train_time:96041ms step_avg:58.78ms
step:1635/1825 train_time:96127ms step_avg:58.79ms
step:1636/1825 train_time:96217ms step_avg:58.81ms
step:1637/1825 train_time:96306ms step_avg:58.83ms
step:1638/1825 train_time:96396ms step_avg:58.85ms
step:1639/1825 train_time:96482ms step_avg:58.87ms
step:1640/1825 train_time:96571ms step_avg:58.88ms
step:1641/1825 train_time:96659ms step_avg:58.90ms
step:1642/1825 train_time:96747ms step_avg:58.92ms
step:1643/1825 train_time:96834ms step_avg:58.94ms
step:1644/1825 train_time:96924ms step_avg:58.96ms
step:1645/1825 train_time:97010ms step_avg:58.97ms
step:1646/1825 train_time:97100ms step_avg:58.99ms
step:1647/1825 train_time:97187ms step_avg:59.01ms
step:1648/1825 train_time:97276ms step_avg:59.03ms
step:1649/1825 train_time:97363ms step_avg:59.04ms
step:1650/1825 train_time:97451ms step_avg:59.06ms
step:1651/1825 train_time:97539ms step_avg:59.08ms
step:1652/1825 train_time:97629ms step_avg:59.10ms
step:1653/1825 train_time:97715ms step_avg:59.11ms
step:1654/1825 train_time:97805ms step_avg:59.13ms
step:1655/1825 train_time:97891ms step_avg:59.15ms
step:1656/1825 train_time:97982ms step_avg:59.17ms
step:1657/1825 train_time:98068ms step_avg:59.18ms
step:1658/1825 train_time:98158ms step_avg:59.20ms
step:1659/1825 train_time:98246ms step_avg:59.22ms
step:1660/1825 train_time:98334ms step_avg:59.24ms
step:1661/1825 train_time:98421ms step_avg:59.25ms
step:1662/1825 train_time:98510ms step_avg:59.27ms
step:1663/1825 train_time:98597ms step_avg:59.29ms
step:1664/1825 train_time:98688ms step_avg:59.31ms
step:1665/1825 train_time:98775ms step_avg:59.32ms
step:1666/1825 train_time:98865ms step_avg:59.34ms
step:1667/1825 train_time:98951ms step_avg:59.36ms
step:1668/1825 train_time:99040ms step_avg:59.38ms
step:1669/1825 train_time:99126ms step_avg:59.39ms
step:1670/1825 train_time:99215ms step_avg:59.41ms
step:1671/1825 train_time:99303ms step_avg:59.43ms
step:1672/1825 train_time:99391ms step_avg:59.44ms
step:1673/1825 train_time:99478ms step_avg:59.46ms
step:1674/1825 train_time:99569ms step_avg:59.48ms
step:1675/1825 train_time:99655ms step_avg:59.50ms
step:1676/1825 train_time:99745ms step_avg:59.51ms
step:1677/1825 train_time:99831ms step_avg:59.53ms
step:1678/1825 train_time:99921ms step_avg:59.55ms
step:1679/1825 train_time:100007ms step_avg:59.56ms
step:1680/1825 train_time:100097ms step_avg:59.58ms
step:1681/1825 train_time:100183ms step_avg:59.60ms
step:1682/1825 train_time:100272ms step_avg:59.61ms
step:1683/1825 train_time:100358ms step_avg:59.63ms
step:1684/1825 train_time:100448ms step_avg:59.65ms
step:1685/1825 train_time:100534ms step_avg:59.66ms
step:1686/1825 train_time:100623ms step_avg:59.68ms
step:1687/1825 train_time:100710ms step_avg:59.70ms
step:1688/1825 train_time:100800ms step_avg:59.72ms
step:1689/1825 train_time:100886ms step_avg:59.73ms
step:1690/1825 train_time:100974ms step_avg:59.75ms
step:1691/1825 train_time:101061ms step_avg:59.76ms
step:1692/1825 train_time:101149ms step_avg:59.78ms
step:1693/1825 train_time:101235ms step_avg:59.80ms
step:1694/1825 train_time:101325ms step_avg:59.81ms
step:1695/1825 train_time:101412ms step_avg:59.83ms
step:1696/1825 train_time:101501ms step_avg:59.85ms
step:1697/1825 train_time:101588ms step_avg:59.86ms
step:1698/1825 train_time:101678ms step_avg:59.88ms
step:1699/1825 train_time:101765ms step_avg:59.90ms
step:1700/1825 train_time:101853ms step_avg:59.91ms
step:1701/1825 train_time:101941ms step_avg:59.93ms
step:1702/1825 train_time:102030ms step_avg:59.95ms
step:1703/1825 train_time:102117ms step_avg:59.96ms
step:1704/1825 train_time:102207ms step_avg:59.98ms
step:1705/1825 train_time:102292ms step_avg:60.00ms
step:1706/1825 train_time:102383ms step_avg:60.01ms
step:1707/1825 train_time:102469ms step_avg:60.03ms
step:1708/1825 train_time:102558ms step_avg:60.05ms
step:1709/1825 train_time:102645ms step_avg:60.06ms
step:1710/1825 train_time:102733ms step_avg:60.08ms
step:1711/1825 train_time:102821ms step_avg:60.09ms
step:1712/1825 train_time:102910ms step_avg:60.11ms
step:1713/1825 train_time:102995ms step_avg:60.13ms
step:1714/1825 train_time:103084ms step_avg:60.14ms
step:1715/1825 train_time:103171ms step_avg:60.16ms
step:1716/1825 train_time:103260ms step_avg:60.18ms
step:1717/1825 train_time:103346ms step_avg:60.19ms
step:1718/1825 train_time:103436ms step_avg:60.21ms
step:1719/1825 train_time:103523ms step_avg:60.22ms
step:1720/1825 train_time:103611ms step_avg:60.24ms
step:1721/1825 train_time:103698ms step_avg:60.25ms
step:1722/1825 train_time:103789ms step_avg:60.27ms
step:1723/1825 train_time:103875ms step_avg:60.29ms
step:1724/1825 train_time:103965ms step_avg:60.30ms
step:1725/1825 train_time:104051ms step_avg:60.32ms
step:1726/1825 train_time:104141ms step_avg:60.34ms
step:1727/1825 train_time:104228ms step_avg:60.35ms
step:1728/1825 train_time:104317ms step_avg:60.37ms
step:1729/1825 train_time:104403ms step_avg:60.38ms
step:1730/1825 train_time:104492ms step_avg:60.40ms
step:1731/1825 train_time:104580ms step_avg:60.42ms
step:1732/1825 train_time:104670ms step_avg:60.43ms
step:1733/1825 train_time:104755ms step_avg:60.45ms
step:1734/1825 train_time:104846ms step_avg:60.46ms
step:1735/1825 train_time:104932ms step_avg:60.48ms
step:1736/1825 train_time:105023ms step_avg:60.50ms
step:1737/1825 train_time:105109ms step_avg:60.51ms
step:1738/1825 train_time:105198ms step_avg:60.53ms
step:1739/1825 train_time:105285ms step_avg:60.54ms
step:1740/1825 train_time:105373ms step_avg:60.56ms
step:1741/1825 train_time:105461ms step_avg:60.58ms
step:1742/1825 train_time:105550ms step_avg:60.59ms
step:1743/1825 train_time:105637ms step_avg:60.61ms
step:1744/1825 train_time:105727ms step_avg:60.62ms
step:1745/1825 train_time:105814ms step_avg:60.64ms
step:1746/1825 train_time:105903ms step_avg:60.65ms
step:1747/1825 train_time:105990ms step_avg:60.67ms
step:1748/1825 train_time:106080ms step_avg:60.69ms
step:1749/1825 train_time:106167ms step_avg:60.70ms
step:1750/1825 train_time:106256ms step_avg:60.72ms
step:1750/1825 val_loss:3.2993 train_time:106353ms step_avg:60.77ms
step:1751/1825 train_time:106377ms step_avg:60.75ms
step:1752/1825 train_time:106436ms step_avg:60.75ms
step:1753/1825 train_time:106526ms step_avg:60.77ms
step:1754/1825 train_time:106620ms step_avg:60.79ms
step:1755/1825 train_time:106706ms step_avg:60.80ms
step:1756/1825 train_time:106794ms step_avg:60.82ms
step:1757/1825 train_time:106880ms step_avg:60.83ms
step:1758/1825 train_time:106968ms step_avg:60.85ms
step:1759/1825 train_time:107053ms step_avg:60.86ms
step:1760/1825 train_time:107143ms step_avg:60.88ms
step:1761/1825 train_time:107228ms step_avg:60.89ms
step:1762/1825 train_time:107318ms step_avg:60.91ms
step:1763/1825 train_time:107407ms step_avg:60.92ms
step:1764/1825 train_time:107497ms step_avg:60.94ms
step:1765/1825 train_time:107586ms step_avg:60.96ms
step:1766/1825 train_time:107677ms step_avg:60.97ms
step:1767/1825 train_time:107765ms step_avg:60.99ms
step:1768/1825 train_time:107852ms step_avg:61.00ms
step:1769/1825 train_time:107938ms step_avg:61.02ms
step:1770/1825 train_time:108026ms step_avg:61.03ms
step:1771/1825 train_time:108112ms step_avg:61.05ms
step:1772/1825 train_time:108201ms step_avg:61.06ms
step:1773/1825 train_time:108288ms step_avg:61.08ms
step:1774/1825 train_time:108377ms step_avg:61.09ms
step:1775/1825 train_time:108465ms step_avg:61.11ms
step:1776/1825 train_time:108557ms step_avg:61.12ms
step:1777/1825 train_time:108644ms step_avg:61.14ms
step:1778/1825 train_time:108733ms step_avg:61.15ms
step:1779/1825 train_time:108821ms step_avg:61.17ms
step:1780/1825 train_time:108909ms step_avg:61.18ms
step:1781/1825 train_time:108995ms step_avg:61.20ms
step:1782/1825 train_time:109085ms step_avg:61.21ms
step:1783/1825 train_time:109170ms step_avg:61.23ms
step:1784/1825 train_time:109259ms step_avg:61.24ms
step:1785/1825 train_time:109346ms step_avg:61.26ms
step:1786/1825 train_time:109437ms step_avg:61.27ms
step:1787/1825 train_time:109523ms step_avg:61.29ms
step:1788/1825 train_time:109613ms step_avg:61.31ms
step:1789/1825 train_time:109703ms step_avg:61.32ms
step:1790/1825 train_time:109791ms step_avg:61.34ms
step:1791/1825 train_time:109878ms step_avg:61.35ms
step:1792/1825 train_time:109966ms step_avg:61.37ms
step:1793/1825 train_time:110052ms step_avg:61.38ms
step:1794/1825 train_time:110142ms step_avg:61.39ms
step:1795/1825 train_time:110228ms step_avg:61.41ms
step:1796/1825 train_time:110318ms step_avg:61.42ms
step:1797/1825 train_time:110406ms step_avg:61.44ms
step:1798/1825 train_time:110497ms step_avg:61.46ms
step:1799/1825 train_time:110584ms step_avg:61.47ms
step:1800/1825 train_time:110673ms step_avg:61.48ms
step:1801/1825 train_time:110760ms step_avg:61.50ms
step:1802/1825 train_time:110850ms step_avg:61.51ms
step:1803/1825 train_time:110936ms step_avg:61.53ms
step:1804/1825 train_time:111025ms step_avg:61.54ms
step:1805/1825 train_time:111112ms step_avg:61.56ms
step:1806/1825 train_time:111201ms step_avg:61.57ms
step:1807/1825 train_time:111287ms step_avg:61.59ms
step:1808/1825 train_time:111377ms step_avg:61.60ms
step:1809/1825 train_time:111466ms step_avg:61.62ms
step:1810/1825 train_time:111556ms step_avg:61.63ms
step:1811/1825 train_time:111644ms step_avg:61.65ms
step:1812/1825 train_time:111733ms step_avg:61.66ms
step:1813/1825 train_time:111819ms step_avg:61.68ms
step:1814/1825 train_time:111908ms step_avg:61.69ms
step:1815/1825 train_time:111994ms step_avg:61.70ms
step:1816/1825 train_time:112084ms step_avg:61.72ms
step:1817/1825 train_time:112171ms step_avg:61.73ms
step:1818/1825 train_time:112261ms step_avg:61.75ms
step:1819/1825 train_time:112347ms step_avg:61.76ms
step:1820/1825 train_time:112438ms step_avg:61.78ms
step:1821/1825 train_time:112525ms step_avg:61.79ms
step:1822/1825 train_time:112615ms step_avg:61.81ms
step:1823/1825 train_time:112705ms step_avg:61.82ms
step:1824/1825 train_time:112793ms step_avg:61.84ms
step:1825/1825 train_time:112880ms step_avg:61.85ms
step:1825/1825 val_loss:3.2780 train_time:112977ms step_avg:61.91ms
peak memory allocated: 29497 MiB reserved: 45118 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1785  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 19:03:32 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    369074      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    369075      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    369076      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    369077      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    369078      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    369079      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    369080      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    369081      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 594, 595, 596, 1189, 1190, 1191, 1784, 1785, 1786] for warmup
Resetting Model
step:0/1825 val_loss:10.8299 train_time:0ms step_avg:0.03ms
step:1/1825 train_time:85ms step_avg:84.87ms
step:2/1825 train_time:108ms step_avg:54.07ms
step:3/1825 train_time:128ms step_avg:42.62ms
step:4/1825 train_time:161ms step_avg:40.35ms
step:5/1825 train_time:194ms step_avg:38.90ms
step:6/1825 train_time:286ms step_avg:47.68ms
step:7/1825 train_time:303ms step_avg:43.30ms
step:8/1825 train_time:460ms step_avg:57.53ms
step:9/1825 train_time:493ms step_avg:54.80ms
step:10/1825 train_time:528ms step_avg:52.85ms
step:11/1825 train_time:561ms step_avg:51.03ms
step:12/1825 train_time:597ms step_avg:49.73ms
step:13/1825 train_time:630ms step_avg:48.45ms
step:14/1825 train_time:665ms step_avg:47.51ms
step:15/1825 train_time:698ms step_avg:46.55ms
step:16/1825 train_time:734ms step_avg:45.84ms
step:17/1825 train_time:766ms step_avg:45.08ms
step:18/1825 train_time:802ms step_avg:44.54ms
step:19/1825 train_time:835ms step_avg:43.94ms
step:20/1825 train_time:870ms step_avg:43.51ms
step:21/1825 train_time:903ms step_avg:43.01ms
step:22/1825 train_time:939ms step_avg:42.67ms
step:23/1825 train_time:972ms step_avg:42.25ms
step:24/1825 train_time:1007ms step_avg:41.96ms
step:25/1825 train_time:1040ms step_avg:41.60ms
step:26/1825 train_time:1075ms step_avg:41.36ms
step:27/1825 train_time:1108ms step_avg:41.05ms
step:28/1825 train_time:1144ms step_avg:40.85ms
step:29/1825 train_time:1177ms step_avg:40.58ms
step:30/1825 train_time:1212ms step_avg:40.40ms
step:31/1825 train_time:1245ms step_avg:40.16ms
step:32/1825 train_time:1280ms step_avg:40.01ms
step:33/1825 train_time:1313ms step_avg:39.80ms
step:34/1825 train_time:1349ms step_avg:39.67ms
step:35/1825 train_time:1382ms step_avg:39.48ms
step:36/1825 train_time:1418ms step_avg:39.38ms
step:37/1825 train_time:1451ms step_avg:39.21ms
step:38/1825 train_time:1486ms step_avg:39.12ms
step:39/1825 train_time:1520ms step_avg:38.97ms
step:40/1825 train_time:1555ms step_avg:38.88ms
step:41/1825 train_time:1588ms step_avg:38.74ms
step:42/1825 train_time:1624ms step_avg:38.66ms
step:43/1825 train_time:1657ms step_avg:38.53ms
step:44/1825 train_time:1692ms step_avg:38.46ms
step:45/1825 train_time:1725ms step_avg:38.34ms
step:46/1825 train_time:1760ms step_avg:38.27ms
step:47/1825 train_time:1793ms step_avg:38.16ms
step:48/1825 train_time:1829ms step_avg:38.10ms
step:49/1825 train_time:1862ms step_avg:38.00ms
step:50/1825 train_time:1897ms step_avg:37.94ms
step:51/1825 train_time:1930ms step_avg:37.84ms
step:52/1825 train_time:1965ms step_avg:37.80ms
step:53/1825 train_time:1999ms step_avg:37.71ms
step:54/1825 train_time:2034ms step_avg:37.67ms
step:55/1825 train_time:2067ms step_avg:37.58ms
step:56/1825 train_time:2102ms step_avg:37.54ms
step:57/1825 train_time:2135ms step_avg:37.46ms
step:58/1825 train_time:2171ms step_avg:37.43ms
step:59/1825 train_time:2204ms step_avg:37.35ms
step:60/1825 train_time:2239ms step_avg:37.32ms
step:61/1825 train_time:2272ms step_avg:37.25ms
step:62/1825 train_time:2308ms step_avg:37.22ms
step:63/1825 train_time:2340ms step_avg:37.15ms
step:64/1825 train_time:2376ms step_avg:37.12ms
step:65/1825 train_time:2409ms step_avg:37.06ms
step:66/1825 train_time:2444ms step_avg:37.04ms
step:67/1825 train_time:2477ms step_avg:36.98ms
step:68/1825 train_time:2513ms step_avg:36.95ms
step:69/1825 train_time:2546ms step_avg:36.89ms
step:70/1825 train_time:2581ms step_avg:36.87ms
step:71/1825 train_time:2614ms step_avg:36.82ms
step:72/1825 train_time:2650ms step_avg:36.80ms
step:73/1825 train_time:2683ms step_avg:36.75ms
step:74/1825 train_time:2718ms step_avg:36.73ms
step:75/1825 train_time:2751ms step_avg:36.68ms
step:76/1825 train_time:2787ms step_avg:36.67ms
step:77/1825 train_time:2820ms step_avg:36.62ms
step:78/1825 train_time:2855ms step_avg:36.60ms
step:79/1825 train_time:2888ms step_avg:36.55ms
step:80/1825 train_time:2923ms step_avg:36.54ms
step:81/1825 train_time:2956ms step_avg:36.50ms
step:82/1825 train_time:2991ms step_avg:36.48ms
step:83/1825 train_time:3025ms step_avg:36.44ms
step:84/1825 train_time:3060ms step_avg:36.43ms
step:85/1825 train_time:3093ms step_avg:36.39ms
step:86/1825 train_time:3128ms step_avg:36.38ms
step:87/1825 train_time:3162ms step_avg:36.34ms
step:88/1825 train_time:3197ms step_avg:36.33ms
step:89/1825 train_time:3230ms step_avg:36.29ms
step:90/1825 train_time:3265ms step_avg:36.28ms
step:91/1825 train_time:3298ms step_avg:36.24ms
step:92/1825 train_time:3333ms step_avg:36.23ms
step:93/1825 train_time:3367ms step_avg:36.20ms
step:94/1825 train_time:3402ms step_avg:36.19ms
step:95/1825 train_time:3435ms step_avg:36.16ms
step:96/1825 train_time:3470ms step_avg:36.15ms
step:97/1825 train_time:3503ms step_avg:36.12ms
step:98/1825 train_time:3539ms step_avg:36.11ms
step:99/1825 train_time:3572ms step_avg:36.08ms
step:100/1825 train_time:3607ms step_avg:36.07ms
step:101/1825 train_time:3640ms step_avg:36.04ms
step:102/1825 train_time:3676ms step_avg:36.04ms
step:103/1825 train_time:3709ms step_avg:36.01ms
step:104/1825 train_time:3744ms step_avg:36.00ms
step:105/1825 train_time:3777ms step_avg:35.97ms
step:106/1825 train_time:3812ms step_avg:35.96ms
step:107/1825 train_time:3845ms step_avg:35.94ms
step:108/1825 train_time:3881ms step_avg:35.93ms
step:109/1825 train_time:3914ms step_avg:35.90ms
step:110/1825 train_time:3949ms step_avg:35.90ms
step:111/1825 train_time:3982ms step_avg:35.87ms
step:112/1825 train_time:4017ms step_avg:35.87ms
step:113/1825 train_time:4050ms step_avg:35.84ms
step:114/1825 train_time:4086ms step_avg:35.84ms
step:115/1825 train_time:4119ms step_avg:35.81ms
step:116/1825 train_time:4154ms step_avg:35.81ms
step:117/1825 train_time:4187ms step_avg:35.79ms
step:118/1825 train_time:4222ms step_avg:35.78ms
step:119/1825 train_time:4255ms step_avg:35.76ms
step:120/1825 train_time:4291ms step_avg:35.76ms
step:121/1825 train_time:4324ms step_avg:35.73ms
step:122/1825 train_time:4359ms step_avg:35.73ms
step:123/1825 train_time:4392ms step_avg:35.71ms
step:124/1825 train_time:4427ms step_avg:35.70ms
step:125/1825 train_time:4460ms step_avg:35.68ms
step:126/1825 train_time:4495ms step_avg:35.68ms
step:127/1825 train_time:4528ms step_avg:35.66ms
step:128/1825 train_time:4564ms step_avg:35.65ms
step:129/1825 train_time:4597ms step_avg:35.63ms
step:130/1825 train_time:4632ms step_avg:35.63ms
step:131/1825 train_time:4665ms step_avg:35.61ms
step:132/1825 train_time:4700ms step_avg:35.61ms
step:133/1825 train_time:4733ms step_avg:35.59ms
step:134/1825 train_time:4769ms step_avg:35.59ms
step:135/1825 train_time:4802ms step_avg:35.57ms
step:136/1825 train_time:4837ms step_avg:35.57ms
step:137/1825 train_time:4870ms step_avg:35.55ms
step:138/1825 train_time:4905ms step_avg:35.55ms
step:139/1825 train_time:4938ms step_avg:35.53ms
step:140/1825 train_time:4973ms step_avg:35.52ms
step:141/1825 train_time:5006ms step_avg:35.51ms
step:142/1825 train_time:5042ms step_avg:35.50ms
step:143/1825 train_time:5074ms step_avg:35.49ms
step:144/1825 train_time:5110ms step_avg:35.48ms
step:145/1825 train_time:5143ms step_avg:35.47ms
step:146/1825 train_time:5178ms step_avg:35.47ms
step:147/1825 train_time:5211ms step_avg:35.45ms
step:148/1825 train_time:5247ms step_avg:35.45ms
step:149/1825 train_time:5280ms step_avg:35.43ms
step:150/1825 train_time:5315ms step_avg:35.43ms
step:151/1825 train_time:5348ms step_avg:35.42ms
step:152/1825 train_time:5383ms step_avg:35.41ms
step:153/1825 train_time:5416ms step_avg:35.40ms
step:154/1825 train_time:5451ms step_avg:35.40ms
step:155/1825 train_time:5484ms step_avg:35.38ms
step:156/1825 train_time:5520ms step_avg:35.38ms
step:157/1825 train_time:5553ms step_avg:35.37ms
step:158/1825 train_time:5588ms step_avg:35.37ms
step:159/1825 train_time:5621ms step_avg:35.35ms
step:160/1825 train_time:5656ms step_avg:35.35ms
step:161/1825 train_time:5689ms step_avg:35.34ms
step:162/1825 train_time:5725ms step_avg:35.34ms
step:163/1825 train_time:5758ms step_avg:35.32ms
step:164/1825 train_time:5793ms step_avg:35.32ms
step:165/1825 train_time:5826ms step_avg:35.31ms
step:166/1825 train_time:5861ms step_avg:35.31ms
step:167/1825 train_time:5894ms step_avg:35.29ms
step:168/1825 train_time:5929ms step_avg:35.29ms
step:169/1825 train_time:5962ms step_avg:35.28ms
step:170/1825 train_time:5998ms step_avg:35.28ms
step:171/1825 train_time:6031ms step_avg:35.27ms
step:172/1825 train_time:6066ms step_avg:35.27ms
step:173/1825 train_time:6099ms step_avg:35.25ms
step:174/1825 train_time:6134ms step_avg:35.25ms
step:175/1825 train_time:6167ms step_avg:35.24ms
step:176/1825 train_time:6202ms step_avg:35.24ms
step:177/1825 train_time:6235ms step_avg:35.23ms
step:178/1825 train_time:6271ms step_avg:35.23ms
step:179/1825 train_time:6303ms step_avg:35.21ms
step:180/1825 train_time:6339ms step_avg:35.22ms
step:181/1825 train_time:6372ms step_avg:35.20ms
step:182/1825 train_time:6407ms step_avg:35.20ms
step:183/1825 train_time:6440ms step_avg:35.19ms
step:184/1825 train_time:6475ms step_avg:35.19ms
step:185/1825 train_time:6508ms step_avg:35.18ms
step:186/1825 train_time:6543ms step_avg:35.18ms
step:187/1825 train_time:6576ms step_avg:35.17ms
step:188/1825 train_time:6612ms step_avg:35.17ms
step:189/1825 train_time:6644ms step_avg:35.16ms
step:190/1825 train_time:6680ms step_avg:35.16ms
step:191/1825 train_time:6713ms step_avg:35.14ms
step:192/1825 train_time:6748ms step_avg:35.14ms
step:193/1825 train_time:6781ms step_avg:35.13ms
step:194/1825 train_time:6816ms step_avg:35.13ms
step:195/1825 train_time:6849ms step_avg:35.12ms
step:196/1825 train_time:6884ms step_avg:35.12ms
step:197/1825 train_time:6917ms step_avg:35.11ms
step:198/1825 train_time:6952ms step_avg:35.11ms
step:199/1825 train_time:6985ms step_avg:35.10ms
step:200/1825 train_time:7021ms step_avg:35.10ms
step:201/1825 train_time:7054ms step_avg:35.09ms
step:202/1825 train_time:7089ms step_avg:35.09ms
step:203/1825 train_time:7122ms step_avg:35.08ms
step:204/1825 train_time:7157ms step_avg:35.08ms
step:205/1825 train_time:7190ms step_avg:35.07ms
step:206/1825 train_time:7225ms step_avg:35.07ms
step:207/1825 train_time:7258ms step_avg:35.06ms
step:208/1825 train_time:7294ms step_avg:35.07ms
step:209/1825 train_time:7327ms step_avg:35.06ms
step:210/1825 train_time:7362ms step_avg:35.06ms
step:211/1825 train_time:7395ms step_avg:35.05ms
step:212/1825 train_time:7430ms step_avg:35.05ms
step:213/1825 train_time:7463ms step_avg:35.04ms
step:214/1825 train_time:7498ms step_avg:35.04ms
step:215/1825 train_time:7531ms step_avg:35.03ms
step:216/1825 train_time:7566ms step_avg:35.03ms
step:217/1825 train_time:7599ms step_avg:35.02ms
step:218/1825 train_time:7635ms step_avg:35.02ms
step:219/1825 train_time:7668ms step_avg:35.01ms
step:220/1825 train_time:7703ms step_avg:35.01ms
step:221/1825 train_time:7736ms step_avg:35.01ms
step:222/1825 train_time:7772ms step_avg:35.01ms
step:223/1825 train_time:7805ms step_avg:35.00ms
step:224/1825 train_time:7840ms step_avg:35.00ms
step:225/1825 train_time:7873ms step_avg:34.99ms
step:226/1825 train_time:7908ms step_avg:34.99ms
step:227/1825 train_time:7941ms step_avg:34.98ms
step:228/1825 train_time:7976ms step_avg:34.98ms
step:229/1825 train_time:8009ms step_avg:34.97ms
step:230/1825 train_time:8044ms step_avg:34.97ms
step:231/1825 train_time:8077ms step_avg:34.97ms
step:232/1825 train_time:8112ms step_avg:34.97ms
step:233/1825 train_time:8145ms step_avg:34.96ms
step:234/1825 train_time:8181ms step_avg:34.96ms
step:235/1825 train_time:8213ms step_avg:34.95ms
step:236/1825 train_time:8249ms step_avg:34.95ms
step:237/1825 train_time:8282ms step_avg:34.94ms
step:238/1825 train_time:8317ms step_avg:34.94ms
step:239/1825 train_time:8350ms step_avg:34.94ms
step:240/1825 train_time:8385ms step_avg:34.94ms
step:241/1825 train_time:8418ms step_avg:34.93ms
step:242/1825 train_time:8453ms step_avg:34.93ms
step:243/1825 train_time:8486ms step_avg:34.92ms
step:244/1825 train_time:8521ms step_avg:34.92ms
step:245/1825 train_time:8554ms step_avg:34.92ms
step:246/1825 train_time:8589ms step_avg:34.92ms
step:247/1825 train_time:8622ms step_avg:34.91ms
step:248/1825 train_time:8658ms step_avg:34.91ms
step:249/1825 train_time:8690ms step_avg:34.90ms
step:250/1825 train_time:8726ms step_avg:34.90ms
step:250/1825 val_loss:4.6178 train_time:8767ms step_avg:35.07ms
step:251/1825 train_time:8786ms step_avg:35.00ms
step:252/1825 train_time:8804ms step_avg:34.94ms
step:253/1825 train_time:8829ms step_avg:34.90ms
step:254/1825 train_time:8865ms step_avg:34.90ms
step:255/1825 train_time:8898ms step_avg:34.89ms
step:256/1825 train_time:8936ms step_avg:34.90ms
step:257/1825 train_time:8970ms step_avg:34.90ms
step:258/1825 train_time:9007ms step_avg:34.91ms
step:259/1825 train_time:9040ms step_avg:34.90ms
step:260/1825 train_time:9075ms step_avg:34.90ms
step:261/1825 train_time:9108ms step_avg:34.90ms
step:262/1825 train_time:9144ms step_avg:34.90ms
step:263/1825 train_time:9177ms step_avg:34.89ms
step:264/1825 train_time:9212ms step_avg:34.89ms
step:265/1825 train_time:9245ms step_avg:34.89ms
step:266/1825 train_time:9280ms step_avg:34.89ms
step:267/1825 train_time:9313ms step_avg:34.88ms
step:268/1825 train_time:9348ms step_avg:34.88ms
step:269/1825 train_time:9381ms step_avg:34.87ms
step:270/1825 train_time:9417ms step_avg:34.88ms
step:271/1825 train_time:9449ms step_avg:34.87ms
step:272/1825 train_time:9484ms step_avg:34.87ms
step:273/1825 train_time:9517ms step_avg:34.86ms
step:274/1825 train_time:9553ms step_avg:34.86ms
step:275/1825 train_time:9585ms step_avg:34.86ms
step:276/1825 train_time:9621ms step_avg:34.86ms
step:277/1825 train_time:9653ms step_avg:34.85ms
step:278/1825 train_time:9689ms step_avg:34.85ms
step:279/1825 train_time:9722ms step_avg:34.84ms
step:280/1825 train_time:9757ms step_avg:34.85ms
step:281/1825 train_time:9790ms step_avg:34.84ms
step:282/1825 train_time:9825ms step_avg:34.84ms
step:283/1825 train_time:9858ms step_avg:34.83ms
step:284/1825 train_time:9893ms step_avg:34.84ms
step:285/1825 train_time:9926ms step_avg:34.83ms
step:286/1825 train_time:9961ms step_avg:34.83ms
step:287/1825 train_time:9994ms step_avg:34.82ms
step:288/1825 train_time:10029ms step_avg:34.82ms
step:289/1825 train_time:10062ms step_avg:34.82ms
step:290/1825 train_time:10097ms step_avg:34.82ms
step:291/1825 train_time:10130ms step_avg:34.81ms
step:292/1825 train_time:10165ms step_avg:34.81ms
step:293/1825 train_time:10198ms step_avg:34.81ms
step:294/1825 train_time:10234ms step_avg:34.81ms
step:295/1825 train_time:10267ms step_avg:34.80ms
step:296/1825 train_time:10302ms step_avg:34.80ms
step:297/1825 train_time:10335ms step_avg:34.80ms
step:298/1825 train_time:10370ms step_avg:34.80ms
step:299/1825 train_time:10403ms step_avg:34.79ms
step:300/1825 train_time:10438ms step_avg:34.79ms
step:301/1825 train_time:10471ms step_avg:34.79ms
step:302/1825 train_time:10506ms step_avg:34.79ms
step:303/1825 train_time:10539ms step_avg:34.78ms
step:304/1825 train_time:10574ms step_avg:34.78ms
step:305/1825 train_time:10607ms step_avg:34.78ms
step:306/1825 train_time:10643ms step_avg:34.78ms
step:307/1825 train_time:10675ms step_avg:34.77ms
step:308/1825 train_time:10711ms step_avg:34.78ms
step:309/1825 train_time:10744ms step_avg:34.77ms
step:310/1825 train_time:10779ms step_avg:34.77ms
step:311/1825 train_time:10812ms step_avg:34.76ms
step:312/1825 train_time:10847ms step_avg:34.77ms
step:313/1825 train_time:10880ms step_avg:34.76ms
step:314/1825 train_time:10915ms step_avg:34.76ms
step:315/1825 train_time:10948ms step_avg:34.75ms
step:316/1825 train_time:10983ms step_avg:34.76ms
step:317/1825 train_time:11016ms step_avg:34.75ms
step:318/1825 train_time:11051ms step_avg:34.75ms
step:319/1825 train_time:11084ms step_avg:34.75ms
step:320/1825 train_time:11119ms step_avg:34.75ms
step:321/1825 train_time:11152ms step_avg:34.74ms
step:322/1825 train_time:11187ms step_avg:34.74ms
step:323/1825 train_time:11220ms step_avg:34.74ms
step:324/1825 train_time:11255ms step_avg:34.74ms
step:325/1825 train_time:11288ms step_avg:34.73ms
step:326/1825 train_time:11323ms step_avg:34.73ms
step:327/1825 train_time:11356ms step_avg:34.73ms
step:328/1825 train_time:11392ms step_avg:34.73ms
step:329/1825 train_time:11424ms step_avg:34.72ms
step:330/1825 train_time:11460ms step_avg:34.73ms
step:331/1825 train_time:11493ms step_avg:34.72ms
step:332/1825 train_time:11528ms step_avg:34.72ms
step:333/1825 train_time:11561ms step_avg:34.72ms
step:334/1825 train_time:11596ms step_avg:34.72ms
step:335/1825 train_time:11629ms step_avg:34.71ms
step:336/1825 train_time:11664ms step_avg:34.72ms
step:337/1825 train_time:11697ms step_avg:34.71ms
step:338/1825 train_time:11732ms step_avg:34.71ms
step:339/1825 train_time:11766ms step_avg:34.71ms
step:340/1825 train_time:11801ms step_avg:34.71ms
step:341/1825 train_time:11833ms step_avg:34.70ms
step:342/1825 train_time:11869ms step_avg:34.70ms
step:343/1825 train_time:11902ms step_avg:34.70ms
step:344/1825 train_time:11937ms step_avg:34.70ms
step:345/1825 train_time:11970ms step_avg:34.70ms
step:346/1825 train_time:12005ms step_avg:34.70ms
step:347/1825 train_time:12038ms step_avg:34.69ms
step:348/1825 train_time:12073ms step_avg:34.69ms
step:349/1825 train_time:12106ms step_avg:34.69ms
step:350/1825 train_time:12141ms step_avg:34.69ms
step:351/1825 train_time:12174ms step_avg:34.68ms
step:352/1825 train_time:12209ms step_avg:34.69ms
step:353/1825 train_time:12242ms step_avg:34.68ms
step:354/1825 train_time:12277ms step_avg:34.68ms
step:355/1825 train_time:12310ms step_avg:34.68ms
step:356/1825 train_time:12345ms step_avg:34.68ms
step:357/1825 train_time:12378ms step_avg:34.67ms
step:358/1825 train_time:12414ms step_avg:34.67ms
step:359/1825 train_time:12446ms step_avg:34.67ms
step:360/1825 train_time:12482ms step_avg:34.67ms
step:361/1825 train_time:12515ms step_avg:34.67ms
step:362/1825 train_time:12550ms step_avg:34.67ms
step:363/1825 train_time:12583ms step_avg:34.66ms
step:364/1825 train_time:12618ms step_avg:34.66ms
step:365/1825 train_time:12650ms step_avg:34.66ms
step:366/1825 train_time:12686ms step_avg:34.66ms
step:367/1825 train_time:12719ms step_avg:34.66ms
step:368/1825 train_time:12754ms step_avg:34.66ms
step:369/1825 train_time:12787ms step_avg:34.65ms
step:370/1825 train_time:12822ms step_avg:34.65ms
step:371/1825 train_time:12855ms step_avg:34.65ms
step:372/1825 train_time:12890ms step_avg:34.65ms
step:373/1825 train_time:12923ms step_avg:34.65ms
step:374/1825 train_time:12958ms step_avg:34.65ms
step:375/1825 train_time:12991ms step_avg:34.64ms
step:376/1825 train_time:13026ms step_avg:34.64ms
step:377/1825 train_time:13059ms step_avg:34.64ms
step:378/1825 train_time:13095ms step_avg:34.64ms
step:379/1825 train_time:13128ms step_avg:34.64ms
step:380/1825 train_time:13163ms step_avg:34.64ms
step:381/1825 train_time:13196ms step_avg:34.63ms
step:382/1825 train_time:13231ms step_avg:34.64ms
step:383/1825 train_time:13264ms step_avg:34.63ms
step:384/1825 train_time:13299ms step_avg:34.63ms
step:385/1825 train_time:13332ms step_avg:34.63ms
step:386/1825 train_time:13367ms step_avg:34.63ms
step:387/1825 train_time:13400ms step_avg:34.63ms
step:388/1825 train_time:13435ms step_avg:34.63ms
step:389/1825 train_time:13468ms step_avg:34.62ms
step:390/1825 train_time:13503ms step_avg:34.62ms
step:391/1825 train_time:13536ms step_avg:34.62ms
step:392/1825 train_time:13571ms step_avg:34.62ms
step:393/1825 train_time:13604ms step_avg:34.62ms
step:394/1825 train_time:13639ms step_avg:34.62ms
step:395/1825 train_time:13672ms step_avg:34.61ms
step:396/1825 train_time:13707ms step_avg:34.61ms
step:397/1825 train_time:13740ms step_avg:34.61ms
step:398/1825 train_time:13775ms step_avg:34.61ms
step:399/1825 train_time:13808ms step_avg:34.61ms
step:400/1825 train_time:13843ms step_avg:34.61ms
step:401/1825 train_time:13876ms step_avg:34.60ms
step:402/1825 train_time:13911ms step_avg:34.61ms
step:403/1825 train_time:13944ms step_avg:34.60ms
step:404/1825 train_time:13979ms step_avg:34.60ms
step:405/1825 train_time:14012ms step_avg:34.60ms
step:406/1825 train_time:14047ms step_avg:34.60ms
step:407/1825 train_time:14080ms step_avg:34.60ms
step:408/1825 train_time:14115ms step_avg:34.60ms
step:409/1825 train_time:14148ms step_avg:34.59ms
step:410/1825 train_time:14184ms step_avg:34.59ms
step:411/1825 train_time:14217ms step_avg:34.59ms
step:412/1825 train_time:14252ms step_avg:34.59ms
step:413/1825 train_time:14285ms step_avg:34.59ms
step:414/1825 train_time:14320ms step_avg:34.59ms
step:415/1825 train_time:14353ms step_avg:34.59ms
step:416/1825 train_time:14388ms step_avg:34.59ms
step:417/1825 train_time:14421ms step_avg:34.58ms
step:418/1825 train_time:14457ms step_avg:34.59ms
step:419/1825 train_time:14489ms step_avg:34.58ms
step:420/1825 train_time:14525ms step_avg:34.58ms
step:421/1825 train_time:14558ms step_avg:34.58ms
step:422/1825 train_time:14593ms step_avg:34.58ms
step:423/1825 train_time:14626ms step_avg:34.58ms
step:424/1825 train_time:14661ms step_avg:34.58ms
step:425/1825 train_time:14694ms step_avg:34.57ms
step:426/1825 train_time:14729ms step_avg:34.57ms
step:427/1825 train_time:14762ms step_avg:34.57ms
step:428/1825 train_time:14797ms step_avg:34.57ms
step:429/1825 train_time:14830ms step_avg:34.57ms
step:430/1825 train_time:14865ms step_avg:34.57ms
step:431/1825 train_time:14898ms step_avg:34.57ms
step:432/1825 train_time:14933ms step_avg:34.57ms
step:433/1825 train_time:14966ms step_avg:34.56ms
step:434/1825 train_time:15001ms step_avg:34.57ms
step:435/1825 train_time:15034ms step_avg:34.56ms
step:436/1825 train_time:15069ms step_avg:34.56ms
step:437/1825 train_time:15102ms step_avg:34.56ms
step:438/1825 train_time:15138ms step_avg:34.56ms
step:439/1825 train_time:15170ms step_avg:34.56ms
step:440/1825 train_time:15206ms step_avg:34.56ms
step:441/1825 train_time:15238ms step_avg:34.55ms
step:442/1825 train_time:15274ms step_avg:34.56ms
step:443/1825 train_time:15307ms step_avg:34.55ms
step:444/1825 train_time:15342ms step_avg:34.55ms
step:445/1825 train_time:15375ms step_avg:34.55ms
step:446/1825 train_time:15410ms step_avg:34.55ms
step:447/1825 train_time:15443ms step_avg:34.55ms
step:448/1825 train_time:15478ms step_avg:34.55ms
step:449/1825 train_time:15511ms step_avg:34.55ms
step:450/1825 train_time:15546ms step_avg:34.55ms
step:451/1825 train_time:15579ms step_avg:34.54ms
step:452/1825 train_time:15615ms step_avg:34.55ms
step:453/1825 train_time:15647ms step_avg:34.54ms
step:454/1825 train_time:15683ms step_avg:34.54ms
step:455/1825 train_time:15715ms step_avg:34.54ms
step:456/1825 train_time:15751ms step_avg:34.54ms
step:457/1825 train_time:15783ms step_avg:34.54ms
step:458/1825 train_time:15819ms step_avg:34.54ms
step:459/1825 train_time:15851ms step_avg:34.53ms
step:460/1825 train_time:15887ms step_avg:34.54ms
step:461/1825 train_time:15919ms step_avg:34.53ms
step:462/1825 train_time:15955ms step_avg:34.53ms
step:463/1825 train_time:15988ms step_avg:34.53ms
step:464/1825 train_time:16023ms step_avg:34.53ms
step:465/1825 train_time:16056ms step_avg:34.53ms
step:466/1825 train_time:16091ms step_avg:34.53ms
step:467/1825 train_time:16124ms step_avg:34.53ms
step:468/1825 train_time:16159ms step_avg:34.53ms
step:469/1825 train_time:16192ms step_avg:34.52ms
step:470/1825 train_time:16227ms step_avg:34.53ms
step:471/1825 train_time:16260ms step_avg:34.52ms
step:472/1825 train_time:16296ms step_avg:34.52ms
step:473/1825 train_time:16328ms step_avg:34.52ms
step:474/1825 train_time:16364ms step_avg:34.52ms
step:475/1825 train_time:16397ms step_avg:34.52ms
step:476/1825 train_time:16432ms step_avg:34.52ms
step:477/1825 train_time:16465ms step_avg:34.52ms
step:478/1825 train_time:16500ms step_avg:34.52ms
step:479/1825 train_time:16533ms step_avg:34.52ms
step:480/1825 train_time:16568ms step_avg:34.52ms
step:481/1825 train_time:16601ms step_avg:34.51ms
step:482/1825 train_time:16636ms step_avg:34.51ms
step:483/1825 train_time:16669ms step_avg:34.51ms
step:484/1825 train_time:16704ms step_avg:34.51ms
step:485/1825 train_time:16737ms step_avg:34.51ms
step:486/1825 train_time:16772ms step_avg:34.51ms
step:487/1825 train_time:16805ms step_avg:34.51ms
step:488/1825 train_time:16840ms step_avg:34.51ms
step:489/1825 train_time:16873ms step_avg:34.51ms
step:490/1825 train_time:16908ms step_avg:34.51ms
step:491/1825 train_time:16941ms step_avg:34.50ms
step:492/1825 train_time:16976ms step_avg:34.50ms
step:493/1825 train_time:17009ms step_avg:34.50ms
step:494/1825 train_time:17044ms step_avg:34.50ms
step:495/1825 train_time:17077ms step_avg:34.50ms
step:496/1825 train_time:17112ms step_avg:34.50ms
step:497/1825 train_time:17145ms step_avg:34.50ms
step:498/1825 train_time:17181ms step_avg:34.50ms
step:499/1825 train_time:17214ms step_avg:34.50ms
step:500/1825 train_time:17249ms step_avg:34.50ms
step:500/1825 val_loss:4.2859 train_time:17291ms step_avg:34.58ms
step:501/1825 train_time:17309ms step_avg:34.55ms
step:502/1825 train_time:17327ms step_avg:34.52ms
step:503/1825 train_time:17352ms step_avg:34.50ms
step:504/1825 train_time:17388ms step_avg:34.50ms
step:505/1825 train_time:17423ms step_avg:34.50ms
step:506/1825 train_time:17460ms step_avg:34.51ms
step:507/1825 train_time:17494ms step_avg:34.51ms
step:508/1825 train_time:17530ms step_avg:34.51ms
step:509/1825 train_time:17563ms step_avg:34.51ms
step:510/1825 train_time:17599ms step_avg:34.51ms
step:511/1825 train_time:17632ms step_avg:34.50ms
step:512/1825 train_time:17667ms step_avg:34.51ms
step:513/1825 train_time:17700ms step_avg:34.50ms
step:514/1825 train_time:17735ms step_avg:34.50ms
step:515/1825 train_time:17768ms step_avg:34.50ms
step:516/1825 train_time:17803ms step_avg:34.50ms
step:517/1825 train_time:17836ms step_avg:34.50ms
step:518/1825 train_time:17871ms step_avg:34.50ms
step:519/1825 train_time:17904ms step_avg:34.50ms
step:520/1825 train_time:17939ms step_avg:34.50ms
step:521/1825 train_time:17972ms step_avg:34.50ms
step:522/1825 train_time:18007ms step_avg:34.50ms
step:523/1825 train_time:18040ms step_avg:34.49ms
step:524/1825 train_time:18075ms step_avg:34.49ms
step:525/1825 train_time:18108ms step_avg:34.49ms
step:526/1825 train_time:18143ms step_avg:34.49ms
step:527/1825 train_time:18176ms step_avg:34.49ms
step:528/1825 train_time:18211ms step_avg:34.49ms
step:529/1825 train_time:18244ms step_avg:34.49ms
step:530/1825 train_time:18279ms step_avg:34.49ms
step:531/1825 train_time:18312ms step_avg:34.49ms
step:532/1825 train_time:18347ms step_avg:34.49ms
step:533/1825 train_time:18380ms step_avg:34.48ms
step:534/1825 train_time:18415ms step_avg:34.49ms
step:535/1825 train_time:18448ms step_avg:34.48ms
step:536/1825 train_time:18484ms step_avg:34.48ms
step:537/1825 train_time:18517ms step_avg:34.48ms
step:538/1825 train_time:18552ms step_avg:34.48ms
step:539/1825 train_time:18585ms step_avg:34.48ms
step:540/1825 train_time:18621ms step_avg:34.48ms
step:541/1825 train_time:18653ms step_avg:34.48ms
step:542/1825 train_time:18689ms step_avg:34.48ms
step:543/1825 train_time:18722ms step_avg:34.48ms
step:544/1825 train_time:18757ms step_avg:34.48ms
step:545/1825 train_time:18790ms step_avg:34.48ms
step:546/1825 train_time:18825ms step_avg:34.48ms
step:547/1825 train_time:18858ms step_avg:34.48ms
step:548/1825 train_time:18893ms step_avg:34.48ms
step:549/1825 train_time:18926ms step_avg:34.47ms
step:550/1825 train_time:18961ms step_avg:34.47ms
step:551/1825 train_time:18994ms step_avg:34.47ms
step:552/1825 train_time:19030ms step_avg:34.47ms
step:553/1825 train_time:19062ms step_avg:34.47ms
step:554/1825 train_time:19098ms step_avg:34.47ms
step:555/1825 train_time:19131ms step_avg:34.47ms
step:556/1825 train_time:19166ms step_avg:34.47ms
step:557/1825 train_time:19199ms step_avg:34.47ms
step:558/1825 train_time:19234ms step_avg:34.47ms
step:559/1825 train_time:19267ms step_avg:34.47ms
step:560/1825 train_time:19302ms step_avg:34.47ms
step:561/1825 train_time:19335ms step_avg:34.47ms
step:562/1825 train_time:19371ms step_avg:34.47ms
step:563/1825 train_time:19403ms step_avg:34.46ms
step:564/1825 train_time:19439ms step_avg:34.47ms
step:565/1825 train_time:19472ms step_avg:34.46ms
step:566/1825 train_time:19507ms step_avg:34.46ms
step:567/1825 train_time:19540ms step_avg:34.46ms
step:568/1825 train_time:19575ms step_avg:34.46ms
step:569/1825 train_time:19608ms step_avg:34.46ms
step:570/1825 train_time:19644ms step_avg:34.46ms
step:571/1825 train_time:19677ms step_avg:34.46ms
step:572/1825 train_time:19712ms step_avg:34.46ms
step:573/1825 train_time:19745ms step_avg:34.46ms
step:574/1825 train_time:19780ms step_avg:34.46ms
step:575/1825 train_time:19813ms step_avg:34.46ms
step:576/1825 train_time:19848ms step_avg:34.46ms
step:577/1825 train_time:19881ms step_avg:34.46ms
step:578/1825 train_time:19916ms step_avg:34.46ms
step:579/1825 train_time:19949ms step_avg:34.45ms
step:580/1825 train_time:19985ms step_avg:34.46ms
step:581/1825 train_time:20018ms step_avg:34.45ms
step:582/1825 train_time:20053ms step_avg:34.45ms
step:583/1825 train_time:20086ms step_avg:34.45ms
step:584/1825 train_time:20121ms step_avg:34.45ms
step:585/1825 train_time:20154ms step_avg:34.45ms
step:586/1825 train_time:20189ms step_avg:34.45ms
step:587/1825 train_time:20222ms step_avg:34.45ms
step:588/1825 train_time:20257ms step_avg:34.45ms
step:589/1825 train_time:20290ms step_avg:34.45ms
step:590/1825 train_time:20325ms step_avg:34.45ms
step:591/1825 train_time:20358ms step_avg:34.45ms
step:592/1825 train_time:20393ms step_avg:34.45ms
step:593/1825 train_time:20426ms step_avg:34.45ms
step:594/1825 train_time:20461ms step_avg:34.45ms
step:595/1825 train_time:20494ms step_avg:34.44ms
step:596/1825 train_time:20531ms step_avg:34.45ms
step:597/1825 train_time:20589ms step_avg:34.49ms
step:598/1825 train_time:20651ms step_avg:34.53ms
step:599/1825 train_time:20711ms step_avg:34.58ms
step:600/1825 train_time:20774ms step_avg:34.62ms
step:601/1825 train_time:20835ms step_avg:34.67ms
step:602/1825 train_time:20898ms step_avg:34.71ms
step:603/1825 train_time:20958ms step_avg:34.76ms
step:604/1825 train_time:21021ms step_avg:34.80ms
step:605/1825 train_time:21081ms step_avg:34.84ms
step:606/1825 train_time:21144ms step_avg:34.89ms
step:607/1825 train_time:21205ms step_avg:34.93ms
step:608/1825 train_time:21268ms step_avg:34.98ms
step:609/1825 train_time:21328ms step_avg:35.02ms
step:610/1825 train_time:21390ms step_avg:35.07ms
step:611/1825 train_time:21450ms step_avg:35.11ms
step:612/1825 train_time:21513ms step_avg:35.15ms
step:613/1825 train_time:21573ms step_avg:35.19ms
step:614/1825 train_time:21635ms step_avg:35.24ms
step:615/1825 train_time:21695ms step_avg:35.28ms
step:616/1825 train_time:21758ms step_avg:35.32ms
step:617/1825 train_time:21819ms step_avg:35.36ms
step:618/1825 train_time:21881ms step_avg:35.41ms
step:619/1825 train_time:21941ms step_avg:35.45ms
step:620/1825 train_time:22004ms step_avg:35.49ms
step:621/1825 train_time:22064ms step_avg:35.53ms
step:622/1825 train_time:22127ms step_avg:35.57ms
step:623/1825 train_time:22187ms step_avg:35.61ms
step:624/1825 train_time:22250ms step_avg:35.66ms
step:625/1825 train_time:22310ms step_avg:35.70ms
step:626/1825 train_time:22373ms step_avg:35.74ms
step:627/1825 train_time:22433ms step_avg:35.78ms
step:628/1825 train_time:22496ms step_avg:35.82ms
step:629/1825 train_time:22556ms step_avg:35.86ms
step:630/1825 train_time:22619ms step_avg:35.90ms
step:631/1825 train_time:22679ms step_avg:35.94ms
step:632/1825 train_time:22742ms step_avg:35.98ms
step:633/1825 train_time:22802ms step_avg:36.02ms
step:634/1825 train_time:22864ms step_avg:36.06ms
step:635/1825 train_time:22924ms step_avg:36.10ms
step:636/1825 train_time:22987ms step_avg:36.14ms
step:637/1825 train_time:23047ms step_avg:36.18ms
step:638/1825 train_time:23110ms step_avg:36.22ms
step:639/1825 train_time:23170ms step_avg:36.26ms
step:640/1825 train_time:23233ms step_avg:36.30ms
step:641/1825 train_time:23293ms step_avg:36.34ms
step:642/1825 train_time:23358ms step_avg:36.38ms
step:643/1825 train_time:23418ms step_avg:36.42ms
step:644/1825 train_time:23481ms step_avg:36.46ms
step:645/1825 train_time:23540ms step_avg:36.50ms
step:646/1825 train_time:23603ms step_avg:36.54ms
step:647/1825 train_time:23664ms step_avg:36.58ms
step:648/1825 train_time:23727ms step_avg:36.62ms
step:649/1825 train_time:23787ms step_avg:36.65ms
step:650/1825 train_time:23850ms step_avg:36.69ms
step:651/1825 train_time:23910ms step_avg:36.73ms
step:652/1825 train_time:23973ms step_avg:36.77ms
step:653/1825 train_time:24034ms step_avg:36.81ms
step:654/1825 train_time:24096ms step_avg:36.84ms
step:655/1825 train_time:24157ms step_avg:36.88ms
step:656/1825 train_time:24220ms step_avg:36.92ms
step:657/1825 train_time:24280ms step_avg:36.96ms
step:658/1825 train_time:24344ms step_avg:37.00ms
step:659/1825 train_time:24404ms step_avg:37.03ms
step:660/1825 train_time:24467ms step_avg:37.07ms
step:661/1825 train_time:24527ms step_avg:37.11ms
step:662/1825 train_time:24590ms step_avg:37.14ms
step:663/1825 train_time:24651ms step_avg:37.18ms
step:664/1825 train_time:24713ms step_avg:37.22ms
step:665/1825 train_time:24772ms step_avg:37.25ms
step:666/1825 train_time:24835ms step_avg:37.29ms
step:667/1825 train_time:24896ms step_avg:37.33ms
step:668/1825 train_time:24959ms step_avg:37.36ms
step:669/1825 train_time:25020ms step_avg:37.40ms
step:670/1825 train_time:25082ms step_avg:37.44ms
step:671/1825 train_time:25143ms step_avg:37.47ms
step:672/1825 train_time:25205ms step_avg:37.51ms
step:673/1825 train_time:25265ms step_avg:37.54ms
step:674/1825 train_time:25327ms step_avg:37.58ms
step:675/1825 train_time:25387ms step_avg:37.61ms
step:676/1825 train_time:25450ms step_avg:37.65ms
step:677/1825 train_time:25510ms step_avg:37.68ms
step:678/1825 train_time:25573ms step_avg:37.72ms
step:679/1825 train_time:25633ms step_avg:37.75ms
step:680/1825 train_time:25696ms step_avg:37.79ms
step:681/1825 train_time:25756ms step_avg:37.82ms
step:682/1825 train_time:25819ms step_avg:37.86ms
step:683/1825 train_time:25880ms step_avg:37.89ms
step:684/1825 train_time:25943ms step_avg:37.93ms
step:685/1825 train_time:26003ms step_avg:37.96ms
step:686/1825 train_time:26066ms step_avg:38.00ms
step:687/1825 train_time:26126ms step_avg:38.03ms
step:688/1825 train_time:26188ms step_avg:38.06ms
step:689/1825 train_time:26249ms step_avg:38.10ms
step:690/1825 train_time:26312ms step_avg:38.13ms
step:691/1825 train_time:26372ms step_avg:38.16ms
step:692/1825 train_time:26435ms step_avg:38.20ms
step:693/1825 train_time:26495ms step_avg:38.23ms
step:694/1825 train_time:26559ms step_avg:38.27ms
step:695/1825 train_time:26620ms step_avg:38.30ms
step:696/1825 train_time:26683ms step_avg:38.34ms
step:697/1825 train_time:26744ms step_avg:38.37ms
step:698/1825 train_time:26806ms step_avg:38.40ms
step:699/1825 train_time:26866ms step_avg:38.43ms
step:700/1825 train_time:26928ms step_avg:38.47ms
step:701/1825 train_time:26989ms step_avg:38.50ms
step:702/1825 train_time:27051ms step_avg:38.53ms
step:703/1825 train_time:27111ms step_avg:38.56ms
step:704/1825 train_time:27174ms step_avg:38.60ms
step:705/1825 train_time:27235ms step_avg:38.63ms
step:706/1825 train_time:27297ms step_avg:38.66ms
step:707/1825 train_time:27358ms step_avg:38.70ms
step:708/1825 train_time:27421ms step_avg:38.73ms
step:709/1825 train_time:27481ms step_avg:38.76ms
step:710/1825 train_time:27544ms step_avg:38.79ms
step:711/1825 train_time:27604ms step_avg:38.82ms
step:712/1825 train_time:27667ms step_avg:38.86ms
step:713/1825 train_time:27727ms step_avg:38.89ms
step:714/1825 train_time:27789ms step_avg:38.92ms
step:715/1825 train_time:27849ms step_avg:38.95ms
step:716/1825 train_time:27911ms step_avg:38.98ms
step:717/1825 train_time:27972ms step_avg:39.01ms
step:718/1825 train_time:28035ms step_avg:39.05ms
step:719/1825 train_time:28096ms step_avg:39.08ms
step:720/1825 train_time:28159ms step_avg:39.11ms
step:721/1825 train_time:28219ms step_avg:39.14ms
step:722/1825 train_time:28282ms step_avg:39.17ms
step:723/1825 train_time:28343ms step_avg:39.20ms
step:724/1825 train_time:28406ms step_avg:39.23ms
step:725/1825 train_time:28466ms step_avg:39.26ms
step:726/1825 train_time:28529ms step_avg:39.30ms
step:727/1825 train_time:28589ms step_avg:39.32ms
step:728/1825 train_time:28652ms step_avg:39.36ms
step:729/1825 train_time:28712ms step_avg:39.39ms
step:730/1825 train_time:28775ms step_avg:39.42ms
step:731/1825 train_time:28835ms step_avg:39.45ms
step:732/1825 train_time:28899ms step_avg:39.48ms
step:733/1825 train_time:28959ms step_avg:39.51ms
step:734/1825 train_time:29022ms step_avg:39.54ms
step:735/1825 train_time:29082ms step_avg:39.57ms
step:736/1825 train_time:29145ms step_avg:39.60ms
step:737/1825 train_time:29205ms step_avg:39.63ms
step:738/1825 train_time:29268ms step_avg:39.66ms
step:739/1825 train_time:29327ms step_avg:39.68ms
step:740/1825 train_time:29390ms step_avg:39.72ms
step:741/1825 train_time:29450ms step_avg:39.74ms
step:742/1825 train_time:29512ms step_avg:39.77ms
step:743/1825 train_time:29573ms step_avg:39.80ms
step:744/1825 train_time:29636ms step_avg:39.83ms
step:745/1825 train_time:29696ms step_avg:39.86ms
step:746/1825 train_time:29759ms step_avg:39.89ms
step:747/1825 train_time:29820ms step_avg:39.92ms
step:748/1825 train_time:29883ms step_avg:39.95ms
step:749/1825 train_time:29944ms step_avg:39.98ms
step:750/1825 train_time:30007ms step_avg:40.01ms
step:750/1825 val_loss:4.0118 train_time:30077ms step_avg:40.10ms
step:751/1825 train_time:30095ms step_avg:40.07ms
step:752/1825 train_time:30132ms step_avg:40.07ms
step:753/1825 train_time:30194ms step_avg:40.10ms
step:754/1825 train_time:30259ms step_avg:40.13ms
step:755/1825 train_time:30320ms step_avg:40.16ms
step:756/1825 train_time:30385ms step_avg:40.19ms
step:757/1825 train_time:30445ms step_avg:40.22ms
step:758/1825 train_time:30507ms step_avg:40.25ms
step:759/1825 train_time:30567ms step_avg:40.27ms
step:760/1825 train_time:30630ms step_avg:40.30ms
step:761/1825 train_time:30689ms step_avg:40.33ms
step:762/1825 train_time:30751ms step_avg:40.36ms
step:763/1825 train_time:30812ms step_avg:40.38ms
step:764/1825 train_time:30874ms step_avg:40.41ms
step:765/1825 train_time:30933ms step_avg:40.44ms
step:766/1825 train_time:30995ms step_avg:40.46ms
step:767/1825 train_time:31056ms step_avg:40.49ms
step:768/1825 train_time:31119ms step_avg:40.52ms
step:769/1825 train_time:31180ms step_avg:40.55ms
step:770/1825 train_time:31244ms step_avg:40.58ms
step:771/1825 train_time:31306ms step_avg:40.60ms
step:772/1825 train_time:31370ms step_avg:40.63ms
step:773/1825 train_time:31430ms step_avg:40.66ms
step:774/1825 train_time:31492ms step_avg:40.69ms
step:775/1825 train_time:31552ms step_avg:40.71ms
step:776/1825 train_time:31614ms step_avg:40.74ms
step:777/1825 train_time:31674ms step_avg:40.76ms
step:778/1825 train_time:31737ms step_avg:40.79ms
step:779/1825 train_time:31797ms step_avg:40.82ms
step:780/1825 train_time:31860ms step_avg:40.85ms
step:781/1825 train_time:31921ms step_avg:40.87ms
step:782/1825 train_time:31984ms step_avg:40.90ms
step:783/1825 train_time:32044ms step_avg:40.92ms
step:784/1825 train_time:32107ms step_avg:40.95ms
step:785/1825 train_time:32169ms step_avg:40.98ms
step:786/1825 train_time:32232ms step_avg:41.01ms
step:787/1825 train_time:32293ms step_avg:41.03ms
step:788/1825 train_time:32356ms step_avg:41.06ms
step:789/1825 train_time:32416ms step_avg:41.09ms
step:790/1825 train_time:32481ms step_avg:41.12ms
step:791/1825 train_time:32541ms step_avg:41.14ms
step:792/1825 train_time:32605ms step_avg:41.17ms
step:793/1825 train_time:32665ms step_avg:41.19ms
step:794/1825 train_time:32727ms step_avg:41.22ms
step:795/1825 train_time:32788ms step_avg:41.24ms
step:796/1825 train_time:32851ms step_avg:41.27ms
step:797/1825 train_time:32911ms step_avg:41.29ms
step:798/1825 train_time:32974ms step_avg:41.32ms
step:799/1825 train_time:33034ms step_avg:41.34ms
step:800/1825 train_time:33097ms step_avg:41.37ms
step:801/1825 train_time:33157ms step_avg:41.39ms
step:802/1825 train_time:33220ms step_avg:41.42ms
step:803/1825 train_time:33281ms step_avg:41.45ms
step:804/1825 train_time:33343ms step_avg:41.47ms
step:805/1825 train_time:33404ms step_avg:41.50ms
step:806/1825 train_time:33468ms step_avg:41.52ms
step:807/1825 train_time:33528ms step_avg:41.55ms
step:808/1825 train_time:33591ms step_avg:41.57ms
step:809/1825 train_time:33652ms step_avg:41.60ms
step:810/1825 train_time:33715ms step_avg:41.62ms
step:811/1825 train_time:33775ms step_avg:41.65ms
step:812/1825 train_time:33837ms step_avg:41.67ms
step:813/1825 train_time:33897ms step_avg:41.69ms
step:814/1825 train_time:33960ms step_avg:41.72ms
step:815/1825 train_time:34020ms step_avg:41.74ms
step:816/1825 train_time:34084ms step_avg:41.77ms
step:817/1825 train_time:34144ms step_avg:41.79ms
step:818/1825 train_time:34207ms step_avg:41.82ms
step:819/1825 train_time:34268ms step_avg:41.84ms
step:820/1825 train_time:34332ms step_avg:41.87ms
step:821/1825 train_time:34393ms step_avg:41.89ms
step:822/1825 train_time:34455ms step_avg:41.92ms
step:823/1825 train_time:34515ms step_avg:41.94ms
step:824/1825 train_time:34579ms step_avg:41.97ms
step:825/1825 train_time:34639ms step_avg:41.99ms
step:826/1825 train_time:34703ms step_avg:42.01ms
step:827/1825 train_time:34763ms step_avg:42.04ms
step:828/1825 train_time:34826ms step_avg:42.06ms
step:829/1825 train_time:34886ms step_avg:42.08ms
step:830/1825 train_time:34949ms step_avg:42.11ms
step:831/1825 train_time:35010ms step_avg:42.13ms
step:832/1825 train_time:35072ms step_avg:42.15ms
step:833/1825 train_time:35132ms step_avg:42.18ms
step:834/1825 train_time:35195ms step_avg:42.20ms
step:835/1825 train_time:35255ms step_avg:42.22ms
step:836/1825 train_time:35318ms step_avg:42.25ms
step:837/1825 train_time:35378ms step_avg:42.27ms
step:838/1825 train_time:35441ms step_avg:42.29ms
step:839/1825 train_time:35502ms step_avg:42.31ms
step:840/1825 train_time:35565ms step_avg:42.34ms
step:841/1825 train_time:35626ms step_avg:42.36ms
step:842/1825 train_time:35689ms step_avg:42.39ms
step:843/1825 train_time:35749ms step_avg:42.41ms
step:844/1825 train_time:35811ms step_avg:42.43ms
step:845/1825 train_time:35871ms step_avg:42.45ms
step:846/1825 train_time:35934ms step_avg:42.48ms
step:847/1825 train_time:35994ms step_avg:42.50ms
step:848/1825 train_time:36057ms step_avg:42.52ms
step:849/1825 train_time:36117ms step_avg:42.54ms
step:850/1825 train_time:36180ms step_avg:42.56ms
step:851/1825 train_time:36240ms step_avg:42.59ms
step:852/1825 train_time:36303ms step_avg:42.61ms
step:853/1825 train_time:36363ms step_avg:42.63ms
step:854/1825 train_time:36426ms step_avg:42.65ms
step:855/1825 train_time:36487ms step_avg:42.67ms
step:856/1825 train_time:36550ms step_avg:42.70ms
step:857/1825 train_time:36610ms step_avg:42.72ms
step:858/1825 train_time:36674ms step_avg:42.74ms
step:859/1825 train_time:36734ms step_avg:42.76ms
step:860/1825 train_time:36796ms step_avg:42.79ms
step:861/1825 train_time:36857ms step_avg:42.81ms
step:862/1825 train_time:36920ms step_avg:42.83ms
step:863/1825 train_time:36980ms step_avg:42.85ms
step:864/1825 train_time:37043ms step_avg:42.87ms
step:865/1825 train_time:37103ms step_avg:42.89ms
step:866/1825 train_time:37166ms step_avg:42.92ms
step:867/1825 train_time:37227ms step_avg:42.94ms
step:868/1825 train_time:37289ms step_avg:42.96ms
step:869/1825 train_time:37350ms step_avg:42.98ms
step:870/1825 train_time:37413ms step_avg:43.00ms
step:871/1825 train_time:37472ms step_avg:43.02ms
step:872/1825 train_time:37535ms step_avg:43.05ms
step:873/1825 train_time:37595ms step_avg:43.06ms
step:874/1825 train_time:37658ms step_avg:43.09ms
step:875/1825 train_time:37718ms step_avg:43.11ms
step:876/1825 train_time:37782ms step_avg:43.13ms
step:877/1825 train_time:37842ms step_avg:43.15ms
step:878/1825 train_time:37905ms step_avg:43.17ms
step:879/1825 train_time:37965ms step_avg:43.19ms
step:880/1825 train_time:38028ms step_avg:43.21ms
step:881/1825 train_time:38088ms step_avg:43.23ms
step:882/1825 train_time:38150ms step_avg:43.25ms
step:883/1825 train_time:38210ms step_avg:43.27ms
step:884/1825 train_time:38273ms step_avg:43.30ms
step:885/1825 train_time:38334ms step_avg:43.32ms
step:886/1825 train_time:38396ms step_avg:43.34ms
step:887/1825 train_time:38456ms step_avg:43.36ms
step:888/1825 train_time:38519ms step_avg:43.38ms
step:889/1825 train_time:38580ms step_avg:43.40ms
step:890/1825 train_time:38643ms step_avg:43.42ms
step:891/1825 train_time:38703ms step_avg:43.44ms
step:892/1825 train_time:38766ms step_avg:43.46ms
step:893/1825 train_time:38827ms step_avg:43.48ms
step:894/1825 train_time:38889ms step_avg:43.50ms
step:895/1825 train_time:38949ms step_avg:43.52ms
step:896/1825 train_time:39013ms step_avg:43.54ms
step:897/1825 train_time:39074ms step_avg:43.56ms
step:898/1825 train_time:39136ms step_avg:43.58ms
step:899/1825 train_time:39196ms step_avg:43.60ms
step:900/1825 train_time:39259ms step_avg:43.62ms
step:901/1825 train_time:39320ms step_avg:43.64ms
step:902/1825 train_time:39384ms step_avg:43.66ms
step:903/1825 train_time:39444ms step_avg:43.68ms
step:904/1825 train_time:39506ms step_avg:43.70ms
step:905/1825 train_time:39567ms step_avg:43.72ms
step:906/1825 train_time:39630ms step_avg:43.74ms
step:907/1825 train_time:39691ms step_avg:43.76ms
step:908/1825 train_time:39753ms step_avg:43.78ms
step:909/1825 train_time:39813ms step_avg:43.80ms
step:910/1825 train_time:39876ms step_avg:43.82ms
step:911/1825 train_time:39937ms step_avg:43.84ms
step:912/1825 train_time:40000ms step_avg:43.86ms
step:913/1825 train_time:40061ms step_avg:43.88ms
step:914/1825 train_time:40123ms step_avg:43.90ms
step:915/1825 train_time:40183ms step_avg:43.92ms
step:916/1825 train_time:40245ms step_avg:43.94ms
step:917/1825 train_time:40306ms step_avg:43.95ms
step:918/1825 train_time:40369ms step_avg:43.97ms
step:919/1825 train_time:40429ms step_avg:43.99ms
step:920/1825 train_time:40491ms step_avg:44.01ms
step:921/1825 train_time:40552ms step_avg:44.03ms
step:922/1825 train_time:40615ms step_avg:44.05ms
step:923/1825 train_time:40675ms step_avg:44.07ms
step:924/1825 train_time:40737ms step_avg:44.09ms
step:925/1825 train_time:40797ms step_avg:44.10ms
step:926/1825 train_time:40860ms step_avg:44.13ms
step:927/1825 train_time:40921ms step_avg:44.14ms
step:928/1825 train_time:40985ms step_avg:44.17ms
step:929/1825 train_time:41045ms step_avg:44.18ms
step:930/1825 train_time:41108ms step_avg:44.20ms
step:931/1825 train_time:41169ms step_avg:44.22ms
step:932/1825 train_time:41231ms step_avg:44.24ms
step:933/1825 train_time:41291ms step_avg:44.26ms
step:934/1825 train_time:41354ms step_avg:44.28ms
step:935/1825 train_time:41414ms step_avg:44.29ms
step:936/1825 train_time:41477ms step_avg:44.31ms
step:937/1825 train_time:41537ms step_avg:44.33ms
step:938/1825 train_time:41601ms step_avg:44.35ms
step:939/1825 train_time:41662ms step_avg:44.37ms
step:940/1825 train_time:41725ms step_avg:44.39ms
step:941/1825 train_time:41785ms step_avg:44.41ms
step:942/1825 train_time:41848ms step_avg:44.42ms
step:943/1825 train_time:41909ms step_avg:44.44ms
step:944/1825 train_time:41972ms step_avg:44.46ms
step:945/1825 train_time:42032ms step_avg:44.48ms
step:946/1825 train_time:42095ms step_avg:44.50ms
step:947/1825 train_time:42155ms step_avg:44.51ms
step:948/1825 train_time:42218ms step_avg:44.53ms
step:949/1825 train_time:42278ms step_avg:44.55ms
step:950/1825 train_time:42341ms step_avg:44.57ms
step:951/1825 train_time:42402ms step_avg:44.59ms
step:952/1825 train_time:42465ms step_avg:44.61ms
step:953/1825 train_time:42525ms step_avg:44.62ms
step:954/1825 train_time:42588ms step_avg:44.64ms
step:955/1825 train_time:42648ms step_avg:44.66ms
step:956/1825 train_time:42711ms step_avg:44.68ms
step:957/1825 train_time:42772ms step_avg:44.69ms
step:958/1825 train_time:42835ms step_avg:44.71ms
step:959/1825 train_time:42895ms step_avg:44.73ms
step:960/1825 train_time:42958ms step_avg:44.75ms
step:961/1825 train_time:43017ms step_avg:44.76ms
step:962/1825 train_time:43081ms step_avg:44.78ms
step:963/1825 train_time:43141ms step_avg:44.80ms
step:964/1825 train_time:43204ms step_avg:44.82ms
step:965/1825 train_time:43264ms step_avg:44.83ms
step:966/1825 train_time:43326ms step_avg:44.85ms
step:967/1825 train_time:43387ms step_avg:44.87ms
step:968/1825 train_time:43450ms step_avg:44.89ms
step:969/1825 train_time:43510ms step_avg:44.90ms
step:970/1825 train_time:43573ms step_avg:44.92ms
step:971/1825 train_time:43634ms step_avg:44.94ms
step:972/1825 train_time:43696ms step_avg:44.95ms
step:973/1825 train_time:43756ms step_avg:44.97ms
step:974/1825 train_time:43819ms step_avg:44.99ms
step:975/1825 train_time:43879ms step_avg:45.00ms
step:976/1825 train_time:43941ms step_avg:45.02ms
step:977/1825 train_time:44002ms step_avg:45.04ms
step:978/1825 train_time:44065ms step_avg:45.06ms
step:979/1825 train_time:44125ms step_avg:45.07ms
step:980/1825 train_time:44188ms step_avg:45.09ms
step:981/1825 train_time:44248ms step_avg:45.10ms
step:982/1825 train_time:44310ms step_avg:45.12ms
step:983/1825 train_time:44371ms step_avg:45.14ms
step:984/1825 train_time:44433ms step_avg:45.16ms
step:985/1825 train_time:44493ms step_avg:45.17ms
step:986/1825 train_time:44555ms step_avg:45.19ms
step:987/1825 train_time:44616ms step_avg:45.20ms
step:988/1825 train_time:44679ms step_avg:45.22ms
step:989/1825 train_time:44739ms step_avg:45.24ms
step:990/1825 train_time:44803ms step_avg:45.26ms
step:991/1825 train_time:44863ms step_avg:45.27ms
step:992/1825 train_time:44925ms step_avg:45.29ms
step:993/1825 train_time:44985ms step_avg:45.30ms
step:994/1825 train_time:45048ms step_avg:45.32ms
step:995/1825 train_time:45109ms step_avg:45.34ms
step:996/1825 train_time:45171ms step_avg:45.35ms
step:997/1825 train_time:45231ms step_avg:45.37ms
step:998/1825 train_time:45294ms step_avg:45.38ms
step:999/1825 train_time:45354ms step_avg:45.40ms
step:1000/1825 train_time:45417ms step_avg:45.42ms
step:1000/1825 val_loss:3.7669 train_time:45488ms step_avg:45.49ms
step:1001/1825 train_time:45509ms step_avg:45.46ms
step:1002/1825 train_time:45542ms step_avg:45.45ms
step:1003/1825 train_time:45602ms step_avg:45.47ms
step:1004/1825 train_time:45669ms step_avg:45.49ms
step:1005/1825 train_time:45730ms step_avg:45.50ms
step:1006/1825 train_time:45795ms step_avg:45.52ms
step:1007/1825 train_time:45855ms step_avg:45.54ms
step:1008/1825 train_time:45919ms step_avg:45.55ms
step:1009/1825 train_time:45979ms step_avg:45.57ms
step:1010/1825 train_time:46040ms step_avg:45.58ms
step:1011/1825 train_time:46100ms step_avg:45.60ms
step:1012/1825 train_time:46162ms step_avg:45.62ms
step:1013/1825 train_time:46222ms step_avg:45.63ms
step:1014/1825 train_time:46285ms step_avg:45.65ms
step:1015/1825 train_time:46345ms step_avg:45.66ms
step:1016/1825 train_time:46408ms step_avg:45.68ms
step:1017/1825 train_time:46469ms step_avg:45.69ms
step:1018/1825 train_time:46531ms step_avg:45.71ms
step:1019/1825 train_time:46592ms step_avg:45.72ms
step:1020/1825 train_time:46655ms step_avg:45.74ms
step:1021/1825 train_time:46716ms step_avg:45.76ms
step:1022/1825 train_time:46779ms step_avg:45.77ms
step:1023/1825 train_time:46840ms step_avg:45.79ms
step:1024/1825 train_time:46904ms step_avg:45.80ms
step:1025/1825 train_time:46964ms step_avg:45.82ms
step:1026/1825 train_time:47025ms step_avg:45.83ms
step:1027/1825 train_time:47085ms step_avg:45.85ms
step:1028/1825 train_time:47148ms step_avg:45.86ms
step:1029/1825 train_time:47207ms step_avg:45.88ms
step:1030/1825 train_time:47270ms step_avg:45.89ms
step:1031/1825 train_time:47329ms step_avg:45.91ms
step:1032/1825 train_time:47393ms step_avg:45.92ms
step:1033/1825 train_time:47453ms step_avg:45.94ms
step:1034/1825 train_time:47516ms step_avg:45.95ms
step:1035/1825 train_time:47575ms step_avg:45.97ms
step:1036/1825 train_time:47638ms step_avg:45.98ms
step:1037/1825 train_time:47699ms step_avg:46.00ms
step:1038/1825 train_time:47762ms step_avg:46.01ms
step:1039/1825 train_time:47823ms step_avg:46.03ms
step:1040/1825 train_time:47885ms step_avg:46.04ms
step:1041/1825 train_time:47946ms step_avg:46.06ms
step:1042/1825 train_time:48008ms step_avg:46.07ms
step:1043/1825 train_time:48068ms step_avg:46.09ms
step:1044/1825 train_time:48131ms step_avg:46.10ms
step:1045/1825 train_time:48191ms step_avg:46.12ms
step:1046/1825 train_time:48253ms step_avg:46.13ms
step:1047/1825 train_time:48313ms step_avg:46.14ms
step:1048/1825 train_time:48377ms step_avg:46.16ms
step:1049/1825 train_time:48437ms step_avg:46.17ms
step:1050/1825 train_time:48499ms step_avg:46.19ms
step:1051/1825 train_time:48560ms step_avg:46.20ms
step:1052/1825 train_time:48623ms step_avg:46.22ms
step:1053/1825 train_time:48683ms step_avg:46.23ms
step:1054/1825 train_time:48746ms step_avg:46.25ms
step:1055/1825 train_time:48806ms step_avg:46.26ms
step:1056/1825 train_time:48869ms step_avg:46.28ms
step:1057/1825 train_time:48929ms step_avg:46.29ms
step:1058/1825 train_time:48992ms step_avg:46.31ms
step:1059/1825 train_time:49052ms step_avg:46.32ms
step:1060/1825 train_time:49115ms step_avg:46.34ms
step:1061/1825 train_time:49175ms step_avg:46.35ms
step:1062/1825 train_time:49238ms step_avg:46.36ms
step:1063/1825 train_time:49298ms step_avg:46.38ms
step:1064/1825 train_time:49360ms step_avg:46.39ms
step:1065/1825 train_time:49421ms step_avg:46.40ms
step:1066/1825 train_time:49484ms step_avg:46.42ms
step:1067/1825 train_time:49544ms step_avg:46.43ms
step:1068/1825 train_time:49607ms step_avg:46.45ms
step:1069/1825 train_time:49668ms step_avg:46.46ms
step:1070/1825 train_time:49731ms step_avg:46.48ms
step:1071/1825 train_time:49791ms step_avg:46.49ms
step:1072/1825 train_time:49854ms step_avg:46.51ms
step:1073/1825 train_time:49914ms step_avg:46.52ms
step:1074/1825 train_time:49977ms step_avg:46.53ms
step:1075/1825 train_time:50037ms step_avg:46.55ms
step:1076/1825 train_time:50100ms step_avg:46.56ms
step:1077/1825 train_time:50161ms step_avg:46.57ms
step:1078/1825 train_time:50224ms step_avg:46.59ms
step:1079/1825 train_time:50284ms step_avg:46.60ms
step:1080/1825 train_time:50346ms step_avg:46.62ms
step:1081/1825 train_time:50406ms step_avg:46.63ms
step:1082/1825 train_time:50468ms step_avg:46.64ms
step:1083/1825 train_time:50529ms step_avg:46.66ms
step:1084/1825 train_time:50592ms step_avg:46.67ms
step:1085/1825 train_time:50652ms step_avg:46.68ms
step:1086/1825 train_time:50715ms step_avg:46.70ms
step:1087/1825 train_time:50776ms step_avg:46.71ms
step:1088/1825 train_time:50838ms step_avg:46.73ms
step:1089/1825 train_time:50899ms step_avg:46.74ms
step:1090/1825 train_time:50962ms step_avg:46.75ms
step:1091/1825 train_time:51022ms step_avg:46.77ms
step:1092/1825 train_time:51085ms step_avg:46.78ms
step:1093/1825 train_time:51145ms step_avg:46.79ms
step:1094/1825 train_time:51207ms step_avg:46.81ms
step:1095/1825 train_time:51267ms step_avg:46.82ms
step:1096/1825 train_time:51330ms step_avg:46.83ms
step:1097/1825 train_time:51390ms step_avg:46.85ms
step:1098/1825 train_time:51453ms step_avg:46.86ms
step:1099/1825 train_time:51513ms step_avg:46.87ms
step:1100/1825 train_time:51576ms step_avg:46.89ms
step:1101/1825 train_time:51636ms step_avg:46.90ms
step:1102/1825 train_time:51699ms step_avg:46.91ms
step:1103/1825 train_time:51759ms step_avg:46.93ms
step:1104/1825 train_time:51822ms step_avg:46.94ms
step:1105/1825 train_time:51883ms step_avg:46.95ms
step:1106/1825 train_time:51946ms step_avg:46.97ms
step:1107/1825 train_time:52007ms step_avg:46.98ms
step:1108/1825 train_time:52070ms step_avg:46.99ms
step:1109/1825 train_time:52129ms step_avg:47.01ms
step:1110/1825 train_time:52192ms step_avg:47.02ms
step:1111/1825 train_time:52252ms step_avg:47.03ms
step:1112/1825 train_time:52316ms step_avg:47.05ms
step:1113/1825 train_time:52375ms step_avg:47.06ms
step:1114/1825 train_time:52438ms step_avg:47.07ms
step:1115/1825 train_time:52498ms step_avg:47.08ms
step:1116/1825 train_time:52561ms step_avg:47.10ms
step:1117/1825 train_time:52622ms step_avg:47.11ms
step:1118/1825 train_time:52684ms step_avg:47.12ms
step:1119/1825 train_time:52744ms step_avg:47.13ms
step:1120/1825 train_time:52807ms step_avg:47.15ms
step:1121/1825 train_time:52868ms step_avg:47.16ms
step:1122/1825 train_time:52930ms step_avg:47.17ms
step:1123/1825 train_time:52991ms step_avg:47.19ms
step:1124/1825 train_time:53054ms step_avg:47.20ms
step:1125/1825 train_time:53114ms step_avg:47.21ms
step:1126/1825 train_time:53177ms step_avg:47.23ms
step:1127/1825 train_time:53237ms step_avg:47.24ms
step:1128/1825 train_time:53300ms step_avg:47.25ms
step:1129/1825 train_time:53361ms step_avg:47.26ms
step:1130/1825 train_time:53423ms step_avg:47.28ms
step:1131/1825 train_time:53483ms step_avg:47.29ms
step:1132/1825 train_time:53546ms step_avg:47.30ms
step:1133/1825 train_time:53606ms step_avg:47.31ms
step:1134/1825 train_time:53669ms step_avg:47.33ms
step:1135/1825 train_time:53728ms step_avg:47.34ms
step:1136/1825 train_time:53791ms step_avg:47.35ms
step:1137/1825 train_time:53851ms step_avg:47.36ms
step:1138/1825 train_time:53915ms step_avg:47.38ms
step:1139/1825 train_time:53974ms step_avg:47.39ms
step:1140/1825 train_time:54038ms step_avg:47.40ms
step:1141/1825 train_time:54099ms step_avg:47.41ms
step:1142/1825 train_time:54162ms step_avg:47.43ms
step:1143/1825 train_time:54222ms step_avg:47.44ms
step:1144/1825 train_time:54285ms step_avg:47.45ms
step:1145/1825 train_time:54346ms step_avg:47.46ms
step:1146/1825 train_time:54408ms step_avg:47.48ms
step:1147/1825 train_time:54468ms step_avg:47.49ms
step:1148/1825 train_time:54531ms step_avg:47.50ms
step:1149/1825 train_time:54591ms step_avg:47.51ms
step:1150/1825 train_time:54654ms step_avg:47.53ms
step:1151/1825 train_time:54714ms step_avg:47.54ms
step:1152/1825 train_time:54777ms step_avg:47.55ms
step:1153/1825 train_time:54837ms step_avg:47.56ms
step:1154/1825 train_time:54900ms step_avg:47.57ms
step:1155/1825 train_time:54960ms step_avg:47.58ms
step:1156/1825 train_time:55023ms step_avg:47.60ms
step:1157/1825 train_time:55084ms step_avg:47.61ms
step:1158/1825 train_time:55147ms step_avg:47.62ms
step:1159/1825 train_time:55207ms step_avg:47.63ms
step:1160/1825 train_time:55269ms step_avg:47.65ms
step:1161/1825 train_time:55329ms step_avg:47.66ms
step:1162/1825 train_time:55393ms step_avg:47.67ms
step:1163/1825 train_time:55452ms step_avg:47.68ms
step:1164/1825 train_time:55516ms step_avg:47.69ms
step:1165/1825 train_time:55575ms step_avg:47.70ms
step:1166/1825 train_time:55639ms step_avg:47.72ms
step:1167/1825 train_time:55699ms step_avg:47.73ms
step:1168/1825 train_time:55762ms step_avg:47.74ms
step:1169/1825 train_time:55822ms step_avg:47.75ms
step:1170/1825 train_time:55885ms step_avg:47.76ms
step:1171/1825 train_time:55944ms step_avg:47.77ms
step:1172/1825 train_time:56007ms step_avg:47.79ms
step:1173/1825 train_time:56067ms step_avg:47.80ms
step:1174/1825 train_time:56129ms step_avg:47.81ms
step:1175/1825 train_time:56189ms step_avg:47.82ms
step:1176/1825 train_time:56252ms step_avg:47.83ms
step:1177/1825 train_time:56312ms step_avg:47.84ms
step:1178/1825 train_time:56375ms step_avg:47.86ms
step:1179/1825 train_time:56435ms step_avg:47.87ms
step:1180/1825 train_time:56498ms step_avg:47.88ms
step:1181/1825 train_time:56558ms step_avg:47.89ms
step:1182/1825 train_time:56620ms step_avg:47.90ms
step:1183/1825 train_time:56680ms step_avg:47.91ms
step:1184/1825 train_time:56743ms step_avg:47.93ms
step:1185/1825 train_time:56804ms step_avg:47.94ms
step:1186/1825 train_time:56866ms step_avg:47.95ms
step:1187/1825 train_time:56926ms step_avg:47.96ms
step:1188/1825 train_time:56990ms step_avg:47.97ms
step:1189/1825 train_time:57050ms step_avg:47.98ms
step:1190/1825 train_time:57112ms step_avg:47.99ms
step:1191/1825 train_time:57174ms step_avg:48.00ms
step:1192/1825 train_time:57261ms step_avg:48.04ms
step:1193/1825 train_time:57348ms step_avg:48.07ms
step:1194/1825 train_time:57437ms step_avg:48.10ms
step:1195/1825 train_time:57522ms step_avg:48.14ms
step:1196/1825 train_time:57612ms step_avg:48.17ms
step:1197/1825 train_time:57698ms step_avg:48.20ms
step:1198/1825 train_time:57786ms step_avg:48.24ms
step:1199/1825 train_time:57874ms step_avg:48.27ms
step:1200/1825 train_time:57963ms step_avg:48.30ms
step:1201/1825 train_time:58050ms step_avg:48.33ms
step:1202/1825 train_time:58139ms step_avg:48.37ms
step:1203/1825 train_time:58225ms step_avg:48.40ms
step:1204/1825 train_time:58315ms step_avg:48.43ms
step:1205/1825 train_time:58402ms step_avg:48.47ms
step:1206/1825 train_time:58491ms step_avg:48.50ms
step:1207/1825 train_time:58579ms step_avg:48.53ms
step:1208/1825 train_time:58668ms step_avg:48.57ms
step:1209/1825 train_time:58755ms step_avg:48.60ms
step:1210/1825 train_time:58843ms step_avg:48.63ms
step:1211/1825 train_time:58930ms step_avg:48.66ms
step:1212/1825 train_time:59019ms step_avg:48.70ms
step:1213/1825 train_time:59105ms step_avg:48.73ms
step:1214/1825 train_time:59193ms step_avg:48.76ms
step:1215/1825 train_time:59281ms step_avg:48.79ms
step:1216/1825 train_time:59370ms step_avg:48.82ms
step:1217/1825 train_time:59457ms step_avg:48.86ms
step:1218/1825 train_time:59546ms step_avg:48.89ms
step:1219/1825 train_time:59633ms step_avg:48.92ms
step:1220/1825 train_time:59721ms step_avg:48.95ms
step:1221/1825 train_time:59808ms step_avg:48.98ms
step:1222/1825 train_time:59898ms step_avg:49.02ms
step:1223/1825 train_time:59984ms step_avg:49.05ms
step:1224/1825 train_time:60073ms step_avg:49.08ms
step:1225/1825 train_time:60160ms step_avg:49.11ms
step:1226/1825 train_time:60249ms step_avg:49.14ms
step:1227/1825 train_time:60335ms step_avg:49.17ms
step:1228/1825 train_time:60423ms step_avg:49.20ms
step:1229/1825 train_time:60511ms step_avg:49.24ms
step:1230/1825 train_time:60599ms step_avg:49.27ms
step:1231/1825 train_time:60685ms step_avg:49.30ms
step:1232/1825 train_time:60776ms step_avg:49.33ms
step:1233/1825 train_time:60862ms step_avg:49.36ms
step:1234/1825 train_time:60952ms step_avg:49.39ms
step:1235/1825 train_time:61039ms step_avg:49.42ms
step:1236/1825 train_time:61129ms step_avg:49.46ms
step:1237/1825 train_time:61215ms step_avg:49.49ms
step:1238/1825 train_time:61304ms step_avg:49.52ms
step:1239/1825 train_time:61390ms step_avg:49.55ms
step:1240/1825 train_time:61480ms step_avg:49.58ms
step:1241/1825 train_time:61566ms step_avg:49.61ms
step:1242/1825 train_time:61655ms step_avg:49.64ms
step:1243/1825 train_time:61741ms step_avg:49.67ms
step:1244/1825 train_time:61830ms step_avg:49.70ms
step:1245/1825 train_time:61917ms step_avg:49.73ms
step:1246/1825 train_time:62004ms step_avg:49.76ms
step:1247/1825 train_time:62092ms step_avg:49.79ms
step:1248/1825 train_time:62181ms step_avg:49.82ms
step:1249/1825 train_time:62267ms step_avg:49.85ms
step:1250/1825 train_time:62358ms step_avg:49.89ms
step:1250/1825 val_loss:3.5250 train_time:62454ms step_avg:49.96ms
step:1251/1825 train_time:62474ms step_avg:49.94ms
step:1252/1825 train_time:62533ms step_avg:49.95ms
step:1253/1825 train_time:62628ms step_avg:49.98ms
step:1254/1825 train_time:62719ms step_avg:50.02ms
step:1255/1825 train_time:62807ms step_avg:50.05ms
step:1256/1825 train_time:62896ms step_avg:50.08ms
step:1257/1825 train_time:62982ms step_avg:50.11ms
step:1258/1825 train_time:63071ms step_avg:50.14ms
step:1259/1825 train_time:63156ms step_avg:50.16ms
step:1260/1825 train_time:63245ms step_avg:50.19ms
step:1261/1825 train_time:63331ms step_avg:50.22ms
step:1262/1825 train_time:63422ms step_avg:50.26ms
step:1263/1825 train_time:63509ms step_avg:50.28ms
step:1264/1825 train_time:63599ms step_avg:50.32ms
step:1265/1825 train_time:63689ms step_avg:50.35ms
step:1266/1825 train_time:63778ms step_avg:50.38ms
step:1267/1825 train_time:63866ms step_avg:50.41ms
step:1268/1825 train_time:63955ms step_avg:50.44ms
step:1269/1825 train_time:64040ms step_avg:50.46ms
step:1270/1825 train_time:64129ms step_avg:50.50ms
step:1271/1825 train_time:64213ms step_avg:50.52ms
step:1272/1825 train_time:64302ms step_avg:50.55ms
step:1273/1825 train_time:64388ms step_avg:50.58ms
step:1274/1825 train_time:64478ms step_avg:50.61ms
step:1275/1825 train_time:64567ms step_avg:50.64ms
step:1276/1825 train_time:64658ms step_avg:50.67ms
step:1277/1825 train_time:64744ms step_avg:50.70ms
step:1278/1825 train_time:64833ms step_avg:50.73ms
step:1279/1825 train_time:64921ms step_avg:50.76ms
step:1280/1825 train_time:65010ms step_avg:50.79ms
step:1281/1825 train_time:65094ms step_avg:50.81ms
step:1282/1825 train_time:65185ms step_avg:50.85ms
step:1283/1825 train_time:65271ms step_avg:50.87ms
step:1284/1825 train_time:65359ms step_avg:50.90ms
step:1285/1825 train_time:65446ms step_avg:50.93ms
step:1286/1825 train_time:65536ms step_avg:50.96ms
step:1287/1825 train_time:65624ms step_avg:50.99ms
step:1288/1825 train_time:65713ms step_avg:51.02ms
step:1289/1825 train_time:65801ms step_avg:51.05ms
step:1290/1825 train_time:65892ms step_avg:51.08ms
step:1291/1825 train_time:65978ms step_avg:51.11ms
step:1292/1825 train_time:66068ms step_avg:51.14ms
step:1293/1825 train_time:66154ms step_avg:51.16ms
step:1294/1825 train_time:66243ms step_avg:51.19ms
step:1295/1825 train_time:66328ms step_avg:51.22ms
step:1296/1825 train_time:66417ms step_avg:51.25ms
step:1297/1825 train_time:66505ms step_avg:51.28ms
step:1298/1825 train_time:66595ms step_avg:51.31ms
step:1299/1825 train_time:66682ms step_avg:51.33ms
step:1300/1825 train_time:66773ms step_avg:51.36ms
step:1301/1825 train_time:66860ms step_avg:51.39ms
step:1302/1825 train_time:66950ms step_avg:51.42ms
step:1303/1825 train_time:67036ms step_avg:51.45ms
step:1304/1825 train_time:67125ms step_avg:51.48ms
step:1305/1825 train_time:67211ms step_avg:51.50ms
step:1306/1825 train_time:67299ms step_avg:51.53ms
step:1307/1825 train_time:67386ms step_avg:51.56ms
step:1308/1825 train_time:67475ms step_avg:51.59ms
step:1309/1825 train_time:67562ms step_avg:51.61ms
step:1310/1825 train_time:67653ms step_avg:51.64ms
step:1311/1825 train_time:67739ms step_avg:51.67ms
step:1312/1825 train_time:67828ms step_avg:51.70ms
step:1313/1825 train_time:67914ms step_avg:51.72ms
step:1314/1825 train_time:68005ms step_avg:51.75ms
step:1315/1825 train_time:68092ms step_avg:51.78ms
step:1316/1825 train_time:68181ms step_avg:51.81ms
step:1317/1825 train_time:68266ms step_avg:51.83ms
step:1318/1825 train_time:68355ms step_avg:51.86ms
step:1319/1825 train_time:68441ms step_avg:51.89ms
step:1320/1825 train_time:68532ms step_avg:51.92ms
step:1321/1825 train_time:68619ms step_avg:51.95ms
step:1322/1825 train_time:68709ms step_avg:51.97ms
step:1323/1825 train_time:68796ms step_avg:52.00ms
step:1324/1825 train_time:68885ms step_avg:52.03ms
step:1325/1825 train_time:68972ms step_avg:52.05ms
step:1326/1825 train_time:69060ms step_avg:52.08ms
step:1327/1825 train_time:69147ms step_avg:52.11ms
step:1328/1825 train_time:69236ms step_avg:52.14ms
step:1329/1825 train_time:69321ms step_avg:52.16ms
step:1330/1825 train_time:69411ms step_avg:52.19ms
step:1331/1825 train_time:69496ms step_avg:52.21ms
step:1332/1825 train_time:69588ms step_avg:52.24ms
step:1333/1825 train_time:69675ms step_avg:52.27ms
step:1334/1825 train_time:69765ms step_avg:52.30ms
step:1335/1825 train_time:69852ms step_avg:52.32ms
step:1336/1825 train_time:69941ms step_avg:52.35ms
step:1337/1825 train_time:70028ms step_avg:52.38ms
step:1338/1825 train_time:70117ms step_avg:52.40ms
step:1339/1825 train_time:70204ms step_avg:52.43ms
step:1340/1825 train_time:70292ms step_avg:52.46ms
step:1341/1825 train_time:70378ms step_avg:52.48ms
step:1342/1825 train_time:70469ms step_avg:52.51ms
step:1343/1825 train_time:70556ms step_avg:52.54ms
step:1344/1825 train_time:70645ms step_avg:52.56ms
step:1345/1825 train_time:70733ms step_avg:52.59ms
step:1346/1825 train_time:70823ms step_avg:52.62ms
step:1347/1825 train_time:70909ms step_avg:52.64ms
step:1348/1825 train_time:70998ms step_avg:52.67ms
step:1349/1825 train_time:71084ms step_avg:52.69ms
step:1350/1825 train_time:71173ms step_avg:52.72ms
step:1351/1825 train_time:71260ms step_avg:52.75ms
step:1352/1825 train_time:71349ms step_avg:52.77ms
step:1353/1825 train_time:71435ms step_avg:52.80ms
step:1354/1825 train_time:71524ms step_avg:52.82ms
step:1355/1825 train_time:71610ms step_avg:52.85ms
step:1356/1825 train_time:71699ms step_avg:52.88ms
step:1357/1825 train_time:71786ms step_avg:52.90ms
step:1358/1825 train_time:71875ms step_avg:52.93ms
step:1359/1825 train_time:71962ms step_avg:52.95ms
step:1360/1825 train_time:72052ms step_avg:52.98ms
step:1361/1825 train_time:72138ms step_avg:53.00ms
step:1362/1825 train_time:72227ms step_avg:53.03ms
step:1363/1825 train_time:72314ms step_avg:53.06ms
step:1364/1825 train_time:72404ms step_avg:53.08ms
step:1365/1825 train_time:72489ms step_avg:53.11ms
step:1366/1825 train_time:72577ms step_avg:53.13ms
step:1367/1825 train_time:72664ms step_avg:53.16ms
step:1368/1825 train_time:72755ms step_avg:53.18ms
step:1369/1825 train_time:72841ms step_avg:53.21ms
step:1370/1825 train_time:72933ms step_avg:53.24ms
step:1371/1825 train_time:73021ms step_avg:53.26ms
step:1372/1825 train_time:73112ms step_avg:53.29ms
step:1373/1825 train_time:73198ms step_avg:53.31ms
step:1374/1825 train_time:73288ms step_avg:53.34ms
step:1375/1825 train_time:73374ms step_avg:53.36ms
step:1376/1825 train_time:73464ms step_avg:53.39ms
step:1377/1825 train_time:73552ms step_avg:53.41ms
step:1378/1825 train_time:73639ms step_avg:53.44ms
step:1379/1825 train_time:73728ms step_avg:53.46ms
step:1380/1825 train_time:73816ms step_avg:53.49ms
step:1381/1825 train_time:73902ms step_avg:53.51ms
step:1382/1825 train_time:73992ms step_avg:53.54ms
step:1383/1825 train_time:74079ms step_avg:53.56ms
step:1384/1825 train_time:74170ms step_avg:53.59ms
step:1385/1825 train_time:74256ms step_avg:53.61ms
step:1386/1825 train_time:74346ms step_avg:53.64ms
step:1387/1825 train_time:74432ms step_avg:53.66ms
step:1388/1825 train_time:74522ms step_avg:53.69ms
step:1389/1825 train_time:74608ms step_avg:53.71ms
step:1390/1825 train_time:74697ms step_avg:53.74ms
step:1391/1825 train_time:74783ms step_avg:53.76ms
step:1392/1825 train_time:74871ms step_avg:53.79ms
step:1393/1825 train_time:74958ms step_avg:53.81ms
step:1394/1825 train_time:75048ms step_avg:53.84ms
step:1395/1825 train_time:75134ms step_avg:53.86ms
step:1396/1825 train_time:75224ms step_avg:53.89ms
step:1397/1825 train_time:75311ms step_avg:53.91ms
step:1398/1825 train_time:75399ms step_avg:53.93ms
step:1399/1825 train_time:75487ms step_avg:53.96ms
step:1400/1825 train_time:75575ms step_avg:53.98ms
step:1401/1825 train_time:75663ms step_avg:54.01ms
step:1402/1825 train_time:75753ms step_avg:54.03ms
step:1403/1825 train_time:75838ms step_avg:54.05ms
step:1404/1825 train_time:75928ms step_avg:54.08ms
step:1405/1825 train_time:76014ms step_avg:54.10ms
step:1406/1825 train_time:76104ms step_avg:54.13ms
step:1407/1825 train_time:76191ms step_avg:54.15ms
step:1408/1825 train_time:76279ms step_avg:54.18ms
step:1409/1825 train_time:76365ms step_avg:54.20ms
step:1410/1825 train_time:76455ms step_avg:54.22ms
step:1411/1825 train_time:76541ms step_avg:54.25ms
step:1412/1825 train_time:76630ms step_avg:54.27ms
step:1413/1825 train_time:76717ms step_avg:54.29ms
step:1414/1825 train_time:76807ms step_avg:54.32ms
step:1415/1825 train_time:76894ms step_avg:54.34ms
step:1416/1825 train_time:76983ms step_avg:54.37ms
step:1417/1825 train_time:77070ms step_avg:54.39ms
step:1418/1825 train_time:77159ms step_avg:54.41ms
step:1419/1825 train_time:77246ms step_avg:54.44ms
step:1420/1825 train_time:77334ms step_avg:54.46ms
step:1421/1825 train_time:77420ms step_avg:54.48ms
step:1422/1825 train_time:77510ms step_avg:54.51ms
step:1423/1825 train_time:77596ms step_avg:54.53ms
step:1424/1825 train_time:77686ms step_avg:54.55ms
step:1425/1825 train_time:77773ms step_avg:54.58ms
step:1426/1825 train_time:77863ms step_avg:54.60ms
step:1427/1825 train_time:77950ms step_avg:54.63ms
step:1428/1825 train_time:78039ms step_avg:54.65ms
step:1429/1825 train_time:78126ms step_avg:54.67ms
step:1430/1825 train_time:78215ms step_avg:54.70ms
step:1431/1825 train_time:78301ms step_avg:54.72ms
step:1432/1825 train_time:78392ms step_avg:54.74ms
step:1433/1825 train_time:78478ms step_avg:54.76ms
step:1434/1825 train_time:78567ms step_avg:54.79ms
step:1435/1825 train_time:78654ms step_avg:54.81ms
step:1436/1825 train_time:78743ms step_avg:54.84ms
step:1437/1825 train_time:78830ms step_avg:54.86ms
step:1438/1825 train_time:78919ms step_avg:54.88ms
step:1439/1825 train_time:79006ms step_avg:54.90ms
step:1440/1825 train_time:79094ms step_avg:54.93ms
step:1441/1825 train_time:79180ms step_avg:54.95ms
step:1442/1825 train_time:79270ms step_avg:54.97ms
step:1443/1825 train_time:79357ms step_avg:54.99ms
step:1444/1825 train_time:79446ms step_avg:55.02ms
step:1445/1825 train_time:79532ms step_avg:55.04ms
step:1446/1825 train_time:79622ms step_avg:55.06ms
step:1447/1825 train_time:79708ms step_avg:55.09ms
step:1448/1825 train_time:79796ms step_avg:55.11ms
step:1449/1825 train_time:79885ms step_avg:55.13ms
step:1450/1825 train_time:79973ms step_avg:55.15ms
step:1451/1825 train_time:80061ms step_avg:55.18ms
step:1452/1825 train_time:80150ms step_avg:55.20ms
step:1453/1825 train_time:80237ms step_avg:55.22ms
step:1454/1825 train_time:80326ms step_avg:55.24ms
step:1455/1825 train_time:80413ms step_avg:55.27ms
step:1456/1825 train_time:80502ms step_avg:55.29ms
step:1457/1825 train_time:80588ms step_avg:55.31ms
step:1458/1825 train_time:80677ms step_avg:55.33ms
step:1459/1825 train_time:80763ms step_avg:55.35ms
step:1460/1825 train_time:80852ms step_avg:55.38ms
step:1461/1825 train_time:80938ms step_avg:55.40ms
step:1462/1825 train_time:81029ms step_avg:55.42ms
step:1463/1825 train_time:81115ms step_avg:55.44ms
step:1464/1825 train_time:81204ms step_avg:55.47ms
step:1465/1825 train_time:81291ms step_avg:55.49ms
step:1466/1825 train_time:81379ms step_avg:55.51ms
step:1467/1825 train_time:81467ms step_avg:55.53ms
step:1468/1825 train_time:81556ms step_avg:55.56ms
step:1469/1825 train_time:81644ms step_avg:55.58ms
step:1470/1825 train_time:81734ms step_avg:55.60ms
step:1471/1825 train_time:81819ms step_avg:55.62ms
step:1472/1825 train_time:81909ms step_avg:55.64ms
step:1473/1825 train_time:81995ms step_avg:55.67ms
step:1474/1825 train_time:82084ms step_avg:55.69ms
step:1475/1825 train_time:82172ms step_avg:55.71ms
step:1476/1825 train_time:82261ms step_avg:55.73ms
step:1477/1825 train_time:82349ms step_avg:55.75ms
step:1478/1825 train_time:82437ms step_avg:55.78ms
step:1479/1825 train_time:82525ms step_avg:55.80ms
step:1480/1825 train_time:82614ms step_avg:55.82ms
step:1481/1825 train_time:82701ms step_avg:55.84ms
step:1482/1825 train_time:82791ms step_avg:55.86ms
step:1483/1825 train_time:82877ms step_avg:55.88ms
step:1484/1825 train_time:82968ms step_avg:55.91ms
step:1485/1825 train_time:83054ms step_avg:55.93ms
step:1486/1825 train_time:83144ms step_avg:55.95ms
step:1487/1825 train_time:83231ms step_avg:55.97ms
step:1488/1825 train_time:83319ms step_avg:55.99ms
step:1489/1825 train_time:83406ms step_avg:56.01ms
step:1490/1825 train_time:83495ms step_avg:56.04ms
step:1491/1825 train_time:83582ms step_avg:56.06ms
step:1492/1825 train_time:83673ms step_avg:56.08ms
step:1493/1825 train_time:83759ms step_avg:56.10ms
step:1494/1825 train_time:83848ms step_avg:56.12ms
step:1495/1825 train_time:83934ms step_avg:56.14ms
step:1496/1825 train_time:84024ms step_avg:56.17ms
step:1497/1825 train_time:84111ms step_avg:56.19ms
step:1498/1825 train_time:84201ms step_avg:56.21ms
step:1499/1825 train_time:84288ms step_avg:56.23ms
step:1500/1825 train_time:84376ms step_avg:56.25ms
step:1500/1825 val_loss:3.3974 train_time:84475ms step_avg:56.32ms
step:1501/1825 train_time:84495ms step_avg:56.29ms
step:1502/1825 train_time:84557ms step_avg:56.30ms
step:1503/1825 train_time:84650ms step_avg:56.32ms
step:1504/1825 train_time:84741ms step_avg:56.34ms
step:1505/1825 train_time:84827ms step_avg:56.36ms
step:1506/1825 train_time:84915ms step_avg:56.38ms
step:1507/1825 train_time:85000ms step_avg:56.40ms
step:1508/1825 train_time:85089ms step_avg:56.42ms
step:1509/1825 train_time:85174ms step_avg:56.44ms
step:1510/1825 train_time:85264ms step_avg:56.47ms
step:1511/1825 train_time:85348ms step_avg:56.48ms
step:1512/1825 train_time:85437ms step_avg:56.51ms
step:1513/1825 train_time:85526ms step_avg:56.53ms
step:1514/1825 train_time:85618ms step_avg:56.55ms
step:1515/1825 train_time:85707ms step_avg:56.57ms
step:1516/1825 train_time:85797ms step_avg:56.59ms
step:1517/1825 train_time:85883ms step_avg:56.61ms
step:1518/1825 train_time:85972ms step_avg:56.64ms
step:1519/1825 train_time:86059ms step_avg:56.65ms
step:1520/1825 train_time:86147ms step_avg:56.68ms
step:1521/1825 train_time:86233ms step_avg:56.69ms
step:1522/1825 train_time:86322ms step_avg:56.72ms
step:1523/1825 train_time:86408ms step_avg:56.74ms
step:1524/1825 train_time:86498ms step_avg:56.76ms
step:1525/1825 train_time:86587ms step_avg:56.78ms
step:1526/1825 train_time:86677ms step_avg:56.80ms
step:1527/1825 train_time:86766ms step_avg:56.82ms
step:1528/1825 train_time:86855ms step_avg:56.84ms
step:1529/1825 train_time:86942ms step_avg:56.86ms
step:1530/1825 train_time:87029ms step_avg:56.88ms
step:1531/1825 train_time:87115ms step_avg:56.90ms
step:1532/1825 train_time:87204ms step_avg:56.92ms
step:1533/1825 train_time:87289ms step_avg:56.94ms
step:1534/1825 train_time:87378ms step_avg:56.96ms
step:1535/1825 train_time:87466ms step_avg:56.98ms
step:1536/1825 train_time:87557ms step_avg:57.00ms
step:1537/1825 train_time:87644ms step_avg:57.02ms
step:1538/1825 train_time:87733ms step_avg:57.04ms
step:1539/1825 train_time:87821ms step_avg:57.06ms
step:1540/1825 train_time:87910ms step_avg:57.08ms
step:1541/1825 train_time:87996ms step_avg:57.10ms
step:1542/1825 train_time:88085ms step_avg:57.12ms
step:1543/1825 train_time:88171ms step_avg:57.14ms
step:1544/1825 train_time:88259ms step_avg:57.16ms
step:1545/1825 train_time:88346ms step_avg:57.18ms
step:1546/1825 train_time:88435ms step_avg:57.20ms
step:1547/1825 train_time:88521ms step_avg:57.22ms
step:1548/1825 train_time:88611ms step_avg:57.24ms
step:1549/1825 train_time:88699ms step_avg:57.26ms
step:1550/1825 train_time:88789ms step_avg:57.28ms
step:1551/1825 train_time:88876ms step_avg:57.30ms
step:1552/1825 train_time:88966ms step_avg:57.32ms
step:1553/1825 train_time:89052ms step_avg:57.34ms
step:1554/1825 train_time:89141ms step_avg:57.36ms
step:1555/1825 train_time:89226ms step_avg:57.38ms
step:1556/1825 train_time:89314ms step_avg:57.40ms
step:1557/1825 train_time:89402ms step_avg:57.42ms
step:1558/1825 train_time:89491ms step_avg:57.44ms
step:1559/1825 train_time:89577ms step_avg:57.46ms
step:1560/1825 train_time:89668ms step_avg:57.48ms
step:1561/1825 train_time:89755ms step_avg:57.50ms
step:1562/1825 train_time:89846ms step_avg:57.52ms
step:1563/1825 train_time:89933ms step_avg:57.54ms
step:1564/1825 train_time:90021ms step_avg:57.56ms
step:1565/1825 train_time:90107ms step_avg:57.58ms
step:1566/1825 train_time:90195ms step_avg:57.60ms
step:1567/1825 train_time:90282ms step_avg:57.61ms
step:1568/1825 train_time:90370ms step_avg:57.63ms
step:1569/1825 train_time:90456ms step_avg:57.65ms
step:1570/1825 train_time:90547ms step_avg:57.67ms
step:1571/1825 train_time:90634ms step_avg:57.69ms
step:1572/1825 train_time:90725ms step_avg:57.71ms
step:1573/1825 train_time:90811ms step_avg:57.73ms
step:1574/1825 train_time:90902ms step_avg:57.75ms
step:1575/1825 train_time:90988ms step_avg:57.77ms
step:1576/1825 train_time:91077ms step_avg:57.79ms
step:1577/1825 train_time:91164ms step_avg:57.81ms
step:1578/1825 train_time:91252ms step_avg:57.83ms
step:1579/1825 train_time:91340ms step_avg:57.85ms
step:1580/1825 train_time:91428ms step_avg:57.87ms
step:1581/1825 train_time:91515ms step_avg:57.88ms
step:1582/1825 train_time:91608ms step_avg:57.91ms
step:1583/1825 train_time:91694ms step_avg:57.92ms
step:1584/1825 train_time:91784ms step_avg:57.94ms
step:1585/1825 train_time:91871ms step_avg:57.96ms
step:1586/1825 train_time:91963ms step_avg:57.98ms
step:1587/1825 train_time:92049ms step_avg:58.00ms
step:1588/1825 train_time:92137ms step_avg:58.02ms
step:1589/1825 train_time:92223ms step_avg:58.04ms
step:1590/1825 train_time:92312ms step_avg:58.06ms
step:1591/1825 train_time:92398ms step_avg:58.08ms
step:1592/1825 train_time:92488ms step_avg:58.10ms
step:1593/1825 train_time:92575ms step_avg:58.11ms
step:1594/1825 train_time:92666ms step_avg:58.13ms
step:1595/1825 train_time:92752ms step_avg:58.15ms
step:1596/1825 train_time:92842ms step_avg:58.17ms
step:1597/1825 train_time:92928ms step_avg:58.19ms
step:1598/1825 train_time:93018ms step_avg:58.21ms
step:1599/1825 train_time:93103ms step_avg:58.23ms
step:1600/1825 train_time:93191ms step_avg:58.24ms
step:1601/1825 train_time:93278ms step_avg:58.26ms
step:1602/1825 train_time:93368ms step_avg:58.28ms
step:1603/1825 train_time:93453ms step_avg:58.30ms
step:1604/1825 train_time:93543ms step_avg:58.32ms
step:1605/1825 train_time:93630ms step_avg:58.34ms
step:1606/1825 train_time:93719ms step_avg:58.36ms
step:1607/1825 train_time:93806ms step_avg:58.37ms
step:1608/1825 train_time:93896ms step_avg:58.39ms
step:1609/1825 train_time:93983ms step_avg:58.41ms
step:1610/1825 train_time:94072ms step_avg:58.43ms
step:1611/1825 train_time:94159ms step_avg:58.45ms
step:1612/1825 train_time:94248ms step_avg:58.47ms
step:1613/1825 train_time:94336ms step_avg:58.48ms
step:1614/1825 train_time:94426ms step_avg:58.50ms
step:1615/1825 train_time:94512ms step_avg:58.52ms
step:1616/1825 train_time:94603ms step_avg:58.54ms
step:1617/1825 train_time:94689ms step_avg:58.56ms
step:1618/1825 train_time:94778ms step_avg:58.58ms
step:1619/1825 train_time:94865ms step_avg:58.59ms
step:1620/1825 train_time:94953ms step_avg:58.61ms
step:1621/1825 train_time:95042ms step_avg:58.63ms
step:1622/1825 train_time:95131ms step_avg:58.65ms
step:1623/1825 train_time:95218ms step_avg:58.67ms
step:1624/1825 train_time:95307ms step_avg:58.69ms
step:1625/1825 train_time:95394ms step_avg:58.70ms
step:1626/1825 train_time:95485ms step_avg:58.72ms
step:1627/1825 train_time:95571ms step_avg:58.74ms
step:1628/1825 train_time:95660ms step_avg:58.76ms
step:1629/1825 train_time:95748ms step_avg:58.78ms
step:1630/1825 train_time:95836ms step_avg:58.80ms
step:1631/1825 train_time:95923ms step_avg:58.81ms
step:1632/1825 train_time:96012ms step_avg:58.83ms
step:1633/1825 train_time:96100ms step_avg:58.85ms
step:1634/1825 train_time:96189ms step_avg:58.87ms
step:1635/1825 train_time:96275ms step_avg:58.88ms
step:1636/1825 train_time:96366ms step_avg:58.90ms
step:1637/1825 train_time:96452ms step_avg:58.92ms
step:1638/1825 train_time:96541ms step_avg:58.94ms
step:1639/1825 train_time:96628ms step_avg:58.96ms
step:1640/1825 train_time:96717ms step_avg:58.97ms
step:1641/1825 train_time:96804ms step_avg:58.99ms
step:1642/1825 train_time:96893ms step_avg:59.01ms
step:1643/1825 train_time:96979ms step_avg:59.03ms
step:1644/1825 train_time:97069ms step_avg:59.04ms
step:1645/1825 train_time:97155ms step_avg:59.06ms
step:1646/1825 train_time:97245ms step_avg:59.08ms
step:1647/1825 train_time:97331ms step_avg:59.10ms
step:1648/1825 train_time:97421ms step_avg:59.11ms
step:1649/1825 train_time:97507ms step_avg:59.13ms
step:1650/1825 train_time:97597ms step_avg:59.15ms
step:1651/1825 train_time:97684ms step_avg:59.17ms
step:1652/1825 train_time:97773ms step_avg:59.18ms
step:1653/1825 train_time:97861ms step_avg:59.20ms
step:1654/1825 train_time:97950ms step_avg:59.22ms
step:1655/1825 train_time:98037ms step_avg:59.24ms
step:1656/1825 train_time:98126ms step_avg:59.26ms
step:1657/1825 train_time:98212ms step_avg:59.27ms
step:1658/1825 train_time:98303ms step_avg:59.29ms
step:1659/1825 train_time:98389ms step_avg:59.31ms
step:1660/1825 train_time:98477ms step_avg:59.32ms
step:1661/1825 train_time:98565ms step_avg:59.34ms
step:1662/1825 train_time:98654ms step_avg:59.36ms
step:1663/1825 train_time:98741ms step_avg:59.38ms
step:1664/1825 train_time:98829ms step_avg:59.39ms
step:1665/1825 train_time:98917ms step_avg:59.41ms
step:1666/1825 train_time:99008ms step_avg:59.43ms
step:1667/1825 train_time:99094ms step_avg:59.44ms
step:1668/1825 train_time:99184ms step_avg:59.46ms
step:1669/1825 train_time:99270ms step_avg:59.48ms
step:1670/1825 train_time:99360ms step_avg:59.50ms
step:1671/1825 train_time:99446ms step_avg:59.51ms
step:1672/1825 train_time:99536ms step_avg:59.53ms
step:1673/1825 train_time:99623ms step_avg:59.55ms
step:1674/1825 train_time:99711ms step_avg:59.56ms
step:1675/1825 train_time:99798ms step_avg:59.58ms
step:1676/1825 train_time:99888ms step_avg:59.60ms
step:1677/1825 train_time:99975ms step_avg:59.62ms
step:1678/1825 train_time:100066ms step_avg:59.63ms
step:1679/1825 train_time:100151ms step_avg:59.65ms
step:1680/1825 train_time:100241ms step_avg:59.67ms
step:1681/1825 train_time:100327ms step_avg:59.68ms
step:1682/1825 train_time:100416ms step_avg:59.70ms
step:1683/1825 train_time:100502ms step_avg:59.72ms
step:1684/1825 train_time:100591ms step_avg:59.73ms
step:1685/1825 train_time:100678ms step_avg:59.75ms
step:1686/1825 train_time:100768ms step_avg:59.77ms
step:1687/1825 train_time:100854ms step_avg:59.78ms
step:1688/1825 train_time:100943ms step_avg:59.80ms
step:1689/1825 train_time:101030ms step_avg:59.82ms
step:1690/1825 train_time:101120ms step_avg:59.83ms
step:1691/1825 train_time:101206ms step_avg:59.85ms
step:1692/1825 train_time:101295ms step_avg:59.87ms
step:1693/1825 train_time:101382ms step_avg:59.88ms
step:1694/1825 train_time:101470ms step_avg:59.90ms
step:1695/1825 train_time:101557ms step_avg:59.92ms
step:1696/1825 train_time:101647ms step_avg:59.93ms
step:1697/1825 train_time:101735ms step_avg:59.95ms
step:1698/1825 train_time:101826ms step_avg:59.97ms
step:1699/1825 train_time:101911ms step_avg:59.98ms
step:1700/1825 train_time:102001ms step_avg:60.00ms
step:1701/1825 train_time:102088ms step_avg:60.02ms
step:1702/1825 train_time:102177ms step_avg:60.03ms
step:1703/1825 train_time:102262ms step_avg:60.05ms
step:1704/1825 train_time:102351ms step_avg:60.07ms
step:1705/1825 train_time:102439ms step_avg:60.08ms
step:1706/1825 train_time:102528ms step_avg:60.10ms
step:1707/1825 train_time:102616ms step_avg:60.11ms
step:1708/1825 train_time:102705ms step_avg:60.13ms
step:1709/1825 train_time:102791ms step_avg:60.15ms
step:1710/1825 train_time:102881ms step_avg:60.16ms
step:1711/1825 train_time:102967ms step_avg:60.18ms
step:1712/1825 train_time:103057ms step_avg:60.20ms
step:1713/1825 train_time:103143ms step_avg:60.21ms
step:1714/1825 train_time:103232ms step_avg:60.23ms
step:1715/1825 train_time:103317ms step_avg:60.24ms
step:1716/1825 train_time:103407ms step_avg:60.26ms
step:1717/1825 train_time:103493ms step_avg:60.28ms
step:1718/1825 train_time:103584ms step_avg:60.29ms
step:1719/1825 train_time:103670ms step_avg:60.31ms
step:1720/1825 train_time:103759ms step_avg:60.32ms
step:1721/1825 train_time:103846ms step_avg:60.34ms
step:1722/1825 train_time:103935ms step_avg:60.36ms
step:1723/1825 train_time:104021ms step_avg:60.37ms
step:1724/1825 train_time:104111ms step_avg:60.39ms
step:1725/1825 train_time:104198ms step_avg:60.40ms
step:1726/1825 train_time:104287ms step_avg:60.42ms
step:1727/1825 train_time:104373ms step_avg:60.44ms
step:1728/1825 train_time:104463ms step_avg:60.45ms
step:1729/1825 train_time:104550ms step_avg:60.47ms
step:1730/1825 train_time:104639ms step_avg:60.48ms
step:1731/1825 train_time:104726ms step_avg:60.50ms
step:1732/1825 train_time:104814ms step_avg:60.52ms
step:1733/1825 train_time:104901ms step_avg:60.53ms
step:1734/1825 train_time:104990ms step_avg:60.55ms
step:1735/1825 train_time:105076ms step_avg:60.56ms
step:1736/1825 train_time:105167ms step_avg:60.58ms
step:1737/1825 train_time:105255ms step_avg:60.60ms
step:1738/1825 train_time:105345ms step_avg:60.61ms
step:1739/1825 train_time:105432ms step_avg:60.63ms
step:1740/1825 train_time:105520ms step_avg:60.64ms
step:1741/1825 train_time:105607ms step_avg:60.66ms
step:1742/1825 train_time:105696ms step_avg:60.68ms
step:1743/1825 train_time:105784ms step_avg:60.69ms
step:1744/1825 train_time:105873ms step_avg:60.71ms
step:1745/1825 train_time:105960ms step_avg:60.72ms
step:1746/1825 train_time:106049ms step_avg:60.74ms
step:1747/1825 train_time:106136ms step_avg:60.75ms
step:1748/1825 train_time:106226ms step_avg:60.77ms
step:1749/1825 train_time:106312ms step_avg:60.78ms
step:1750/1825 train_time:106401ms step_avg:60.80ms
step:1750/1825 val_loss:3.3002 train_time:106497ms step_avg:60.86ms
step:1751/1825 train_time:106518ms step_avg:60.83ms
step:1752/1825 train_time:106579ms step_avg:60.83ms
step:1753/1825 train_time:106669ms step_avg:60.85ms
step:1754/1825 train_time:106765ms step_avg:60.87ms
step:1755/1825 train_time:106851ms step_avg:60.88ms
step:1756/1825 train_time:106940ms step_avg:60.90ms
step:1757/1825 train_time:107025ms step_avg:60.91ms
step:1758/1825 train_time:107114ms step_avg:60.93ms
step:1759/1825 train_time:107200ms step_avg:60.94ms
step:1760/1825 train_time:107288ms step_avg:60.96ms
step:1761/1825 train_time:107372ms step_avg:60.97ms
step:1762/1825 train_time:107462ms step_avg:60.99ms
step:1763/1825 train_time:107550ms step_avg:61.00ms
step:1764/1825 train_time:107645ms step_avg:61.02ms
step:1765/1825 train_time:107736ms step_avg:61.04ms
step:1766/1825 train_time:107824ms step_avg:61.06ms
step:1767/1825 train_time:107910ms step_avg:61.07ms
step:1768/1825 train_time:108000ms step_avg:61.09ms
step:1769/1825 train_time:108086ms step_avg:61.10ms
step:1770/1825 train_time:108175ms step_avg:61.12ms
step:1771/1825 train_time:108261ms step_avg:61.13ms
step:1772/1825 train_time:108348ms step_avg:61.14ms
step:1773/1825 train_time:108436ms step_avg:61.16ms
step:1774/1825 train_time:108524ms step_avg:61.18ms
step:1775/1825 train_time:108613ms step_avg:61.19ms
step:1776/1825 train_time:108706ms step_avg:61.21ms
step:1777/1825 train_time:108792ms step_avg:61.22ms
step:1778/1825 train_time:108882ms step_avg:61.24ms
step:1779/1825 train_time:108969ms step_avg:61.25ms
step:1780/1825 train_time:109058ms step_avg:61.27ms
step:1781/1825 train_time:109144ms step_avg:61.28ms
step:1782/1825 train_time:109233ms step_avg:61.30ms
step:1783/1825 train_time:109318ms step_avg:61.31ms
step:1784/1825 train_time:109406ms step_avg:61.33ms
step:1785/1825 train_time:109493ms step_avg:61.34ms
step:1786/1825 train_time:109586ms step_avg:61.36ms
step:1787/1825 train_time:109671ms step_avg:61.37ms
step:1788/1825 train_time:109761ms step_avg:61.39ms
step:1789/1825 train_time:109847ms step_avg:61.40ms
step:1790/1825 train_time:109937ms step_avg:61.42ms
step:1791/1825 train_time:110023ms step_avg:61.43ms
step:1792/1825 train_time:110112ms step_avg:61.45ms
step:1793/1825 train_time:110199ms step_avg:61.46ms
step:1794/1825 train_time:110287ms step_avg:61.48ms
step:1795/1825 train_time:110372ms step_avg:61.49ms
step:1796/1825 train_time:110463ms step_avg:61.50ms
step:1797/1825 train_time:110550ms step_avg:61.52ms
step:1798/1825 train_time:110642ms step_avg:61.54ms
step:1799/1825 train_time:110729ms step_avg:61.55ms
step:1800/1825 train_time:110818ms step_avg:61.57ms
step:1801/1825 train_time:110904ms step_avg:61.58ms
step:1802/1825 train_time:110996ms step_avg:61.60ms
step:1803/1825 train_time:111083ms step_avg:61.61ms
step:1804/1825 train_time:111172ms step_avg:61.63ms
step:1805/1825 train_time:111258ms step_avg:61.64ms
step:1806/1825 train_time:111346ms step_avg:61.65ms
step:1807/1825 train_time:111433ms step_avg:61.67ms
step:1808/1825 train_time:111524ms step_avg:61.68ms
step:1809/1825 train_time:111611ms step_avg:61.70ms
step:1810/1825 train_time:111702ms step_avg:61.71ms
step:1811/1825 train_time:111788ms step_avg:61.73ms
step:1812/1825 train_time:111879ms step_avg:61.74ms
step:1813/1825 train_time:111965ms step_avg:61.76ms
step:1814/1825 train_time:112055ms step_avg:61.77ms
step:1815/1825 train_time:112142ms step_avg:61.79ms
step:1816/1825 train_time:112229ms step_avg:61.80ms
step:1817/1825 train_time:112316ms step_avg:61.81ms
step:1818/1825 train_time:112405ms step_avg:61.83ms
step:1819/1825 train_time:112491ms step_avg:61.84ms
step:1820/1825 train_time:112583ms step_avg:61.86ms
step:1821/1825 train_time:112668ms step_avg:61.87ms
step:1822/1825 train_time:112759ms step_avg:61.89ms
step:1823/1825 train_time:112847ms step_avg:61.90ms
step:1824/1825 train_time:112936ms step_avg:61.92ms
step:1825/1825 train_time:113022ms step_avg:61.93ms
step:1825/1825 val_loss:3.2791 train_time:113120ms step_avg:61.98ms
peak memory allocated: 29801 MiB reserved: 44718 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1785  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 19:18:54 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    398378      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    398379      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    398380      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    398381      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    398382      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    398383      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    398384      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    398385      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 594, 595, 596, 1189, 1190, 1191, 1784, 1785, 1786] for warmup
Resetting Model
step:0/1825 val_loss:10.8296 train_time:0ms step_avg:0.03ms
step:1/1825 train_time:83ms step_avg:83.47ms
step:2/1825 train_time:105ms step_avg:52.57ms
step:3/1825 train_time:129ms step_avg:43.00ms
step:4/1825 train_time:155ms step_avg:38.87ms
step:5/1825 train_time:188ms step_avg:37.63ms
step:6/1825 train_time:293ms step_avg:48.80ms
step:7/1825 train_time:310ms step_avg:44.23ms
step:8/1825 train_time:337ms step_avg:42.09ms
step:9/1825 train_time:370ms step_avg:41.08ms
step:10/1825 train_time:405ms step_avg:40.51ms
step:11/1825 train_time:438ms step_avg:39.83ms
step:12/1825 train_time:473ms step_avg:39.46ms
step:13/1825 train_time:507ms step_avg:38.96ms
step:14/1825 train_time:542ms step_avg:38.70ms
step:15/1825 train_time:575ms step_avg:38.34ms
step:16/1825 train_time:610ms step_avg:38.15ms
step:17/1825 train_time:643ms step_avg:37.85ms
step:18/1825 train_time:679ms step_avg:37.71ms
step:19/1825 train_time:712ms step_avg:37.47ms
step:20/1825 train_time:748ms step_avg:37.40ms
step:21/1825 train_time:780ms step_avg:37.16ms
step:22/1825 train_time:816ms step_avg:37.09ms
step:23/1825 train_time:849ms step_avg:36.92ms
step:24/1825 train_time:885ms step_avg:36.86ms
step:25/1825 train_time:918ms step_avg:36.72ms
step:26/1825 train_time:953ms step_avg:36.66ms
step:27/1825 train_time:986ms step_avg:36.52ms
step:28/1825 train_time:1022ms step_avg:36.49ms
step:29/1825 train_time:1055ms step_avg:36.37ms
step:30/1825 train_time:1090ms step_avg:36.33ms
step:31/1825 train_time:1123ms step_avg:36.23ms
step:32/1825 train_time:1159ms step_avg:36.21ms
step:33/1825 train_time:1192ms step_avg:36.12ms
step:34/1825 train_time:1228ms step_avg:36.11ms
step:35/1825 train_time:1261ms step_avg:36.03ms
step:36/1825 train_time:1297ms step_avg:36.02ms
step:37/1825 train_time:1330ms step_avg:35.94ms
step:38/1825 train_time:1365ms step_avg:35.93ms
step:39/1825 train_time:1398ms step_avg:35.85ms
step:40/1825 train_time:1434ms step_avg:35.84ms
step:41/1825 train_time:1467ms step_avg:35.77ms
step:42/1825 train_time:1502ms step_avg:35.77ms
step:43/1825 train_time:1535ms step_avg:35.71ms
step:44/1825 train_time:1571ms step_avg:35.70ms
step:45/1825 train_time:1604ms step_avg:35.65ms
step:46/1825 train_time:1640ms step_avg:35.65ms
step:47/1825 train_time:1673ms step_avg:35.59ms
step:48/1825 train_time:1708ms step_avg:35.59ms
step:49/1825 train_time:1741ms step_avg:35.54ms
step:50/1825 train_time:1777ms step_avg:35.53ms
step:51/1825 train_time:1810ms step_avg:35.48ms
step:52/1825 train_time:1845ms step_avg:35.48ms
step:53/1825 train_time:1878ms step_avg:35.44ms
step:54/1825 train_time:1914ms step_avg:35.44ms
step:55/1825 train_time:1946ms step_avg:35.39ms
step:56/1825 train_time:1982ms step_avg:35.39ms
step:57/1825 train_time:2015ms step_avg:35.35ms
step:58/1825 train_time:2050ms step_avg:35.35ms
step:59/1825 train_time:2083ms step_avg:35.31ms
step:60/1825 train_time:2119ms step_avg:35.31ms
step:61/1825 train_time:2152ms step_avg:35.27ms
step:62/1825 train_time:2187ms step_avg:35.27ms
step:63/1825 train_time:2220ms step_avg:35.24ms
step:64/1825 train_time:2255ms step_avg:35.24ms
step:65/1825 train_time:2288ms step_avg:35.21ms
step:66/1825 train_time:2324ms step_avg:35.21ms
step:67/1825 train_time:2357ms step_avg:35.18ms
step:68/1825 train_time:2392ms step_avg:35.18ms
step:69/1825 train_time:2425ms step_avg:35.15ms
step:70/1825 train_time:2461ms step_avg:35.15ms
step:71/1825 train_time:2494ms step_avg:35.12ms
step:72/1825 train_time:2529ms step_avg:35.13ms
step:73/1825 train_time:2562ms step_avg:35.10ms
step:74/1825 train_time:2598ms step_avg:35.11ms
step:75/1825 train_time:2631ms step_avg:35.08ms
step:76/1825 train_time:2667ms step_avg:35.09ms
step:77/1825 train_time:2700ms step_avg:35.06ms
step:78/1825 train_time:2735ms step_avg:35.07ms
step:79/1825 train_time:2768ms step_avg:35.04ms
step:80/1825 train_time:2804ms step_avg:35.05ms
step:81/1825 train_time:2837ms step_avg:35.03ms
step:82/1825 train_time:2873ms step_avg:35.03ms
step:83/1825 train_time:2906ms step_avg:35.01ms
step:84/1825 train_time:2941ms step_avg:35.01ms
step:85/1825 train_time:2975ms step_avg:35.00ms
step:86/1825 train_time:3010ms step_avg:35.00ms
step:87/1825 train_time:3043ms step_avg:34.98ms
step:88/1825 train_time:3078ms step_avg:34.98ms
step:89/1825 train_time:3111ms step_avg:34.96ms
step:90/1825 train_time:3146ms step_avg:34.96ms
step:91/1825 train_time:3179ms step_avg:34.94ms
step:92/1825 train_time:3215ms step_avg:34.94ms
step:93/1825 train_time:3248ms step_avg:34.92ms
step:94/1825 train_time:3283ms step_avg:34.93ms
step:95/1825 train_time:3316ms step_avg:34.91ms
step:96/1825 train_time:3351ms step_avg:34.91ms
step:97/1825 train_time:3384ms step_avg:34.89ms
step:98/1825 train_time:3420ms step_avg:34.90ms
step:99/1825 train_time:3453ms step_avg:34.88ms
step:100/1825 train_time:3488ms step_avg:34.88ms
step:101/1825 train_time:3521ms step_avg:34.86ms
step:102/1825 train_time:3557ms step_avg:34.87ms
step:103/1825 train_time:3590ms step_avg:34.85ms
step:104/1825 train_time:3625ms step_avg:34.85ms
step:105/1825 train_time:3658ms step_avg:34.84ms
step:106/1825 train_time:3693ms step_avg:34.84ms
step:107/1825 train_time:3726ms step_avg:34.82ms
step:108/1825 train_time:3761ms step_avg:34.83ms
step:109/1825 train_time:3794ms step_avg:34.81ms
step:110/1825 train_time:3830ms step_avg:34.82ms
step:111/1825 train_time:3863ms step_avg:34.80ms
step:112/1825 train_time:3898ms step_avg:34.81ms
step:113/1825 train_time:3931ms step_avg:34.79ms
step:114/1825 train_time:3966ms step_avg:34.79ms
step:115/1825 train_time:3999ms step_avg:34.78ms
step:116/1825 train_time:4035ms step_avg:34.78ms
step:117/1825 train_time:4068ms step_avg:34.77ms
step:118/1825 train_time:4103ms step_avg:34.77ms
step:119/1825 train_time:4136ms step_avg:34.76ms
step:120/1825 train_time:4172ms step_avg:34.76ms
step:121/1825 train_time:4205ms step_avg:34.75ms
step:122/1825 train_time:4240ms step_avg:34.75ms
step:123/1825 train_time:4273ms step_avg:34.74ms
step:124/1825 train_time:4308ms step_avg:34.74ms
step:125/1825 train_time:4341ms step_avg:34.73ms
step:126/1825 train_time:4377ms step_avg:34.73ms
step:127/1825 train_time:4409ms step_avg:34.72ms
step:128/1825 train_time:4445ms step_avg:34.72ms
step:129/1825 train_time:4478ms step_avg:34.71ms
step:130/1825 train_time:4513ms step_avg:34.72ms
step:131/1825 train_time:4546ms step_avg:34.70ms
step:132/1825 train_time:4581ms step_avg:34.71ms
step:133/1825 train_time:4614ms step_avg:34.69ms
step:134/1825 train_time:4649ms step_avg:34.70ms
step:135/1825 train_time:4682ms step_avg:34.68ms
step:136/1825 train_time:4718ms step_avg:34.69ms
step:137/1825 train_time:4751ms step_avg:34.68ms
step:138/1825 train_time:4786ms step_avg:34.68ms
step:139/1825 train_time:4819ms step_avg:34.67ms
step:140/1825 train_time:4854ms step_avg:34.67ms
step:141/1825 train_time:4887ms step_avg:34.66ms
step:142/1825 train_time:4923ms step_avg:34.67ms
step:143/1825 train_time:4955ms step_avg:34.65ms
step:144/1825 train_time:4991ms step_avg:34.66ms
step:145/1825 train_time:5024ms step_avg:34.65ms
step:146/1825 train_time:5059ms step_avg:34.65ms
step:147/1825 train_time:5092ms step_avg:34.64ms
step:148/1825 train_time:5127ms step_avg:34.64ms
step:149/1825 train_time:5160ms step_avg:34.63ms
step:150/1825 train_time:5196ms step_avg:34.64ms
step:151/1825 train_time:5229ms step_avg:34.63ms
step:152/1825 train_time:5264ms step_avg:34.63ms
step:153/1825 train_time:5297ms step_avg:34.62ms
step:154/1825 train_time:5332ms step_avg:34.62ms
step:155/1825 train_time:5365ms step_avg:34.61ms
step:156/1825 train_time:5400ms step_avg:34.62ms
step:157/1825 train_time:5433ms step_avg:34.61ms
step:158/1825 train_time:5469ms step_avg:34.61ms
step:159/1825 train_time:5502ms step_avg:34.60ms
step:160/1825 train_time:5537ms step_avg:34.61ms
step:161/1825 train_time:5570ms step_avg:34.60ms
step:162/1825 train_time:5606ms step_avg:34.60ms
step:163/1825 train_time:5639ms step_avg:34.59ms
step:164/1825 train_time:5674ms step_avg:34.60ms
step:165/1825 train_time:5707ms step_avg:34.59ms
step:166/1825 train_time:5742ms step_avg:34.59ms
step:167/1825 train_time:5775ms step_avg:34.58ms
step:168/1825 train_time:5811ms step_avg:34.59ms
step:169/1825 train_time:5844ms step_avg:34.58ms
step:170/1825 train_time:5879ms step_avg:34.58ms
step:171/1825 train_time:5912ms step_avg:34.57ms
step:172/1825 train_time:5947ms step_avg:34.58ms
step:173/1825 train_time:5980ms step_avg:34.57ms
step:174/1825 train_time:6016ms step_avg:34.57ms
step:175/1825 train_time:6049ms step_avg:34.56ms
step:176/1825 train_time:6084ms step_avg:34.57ms
step:177/1825 train_time:6117ms step_avg:34.56ms
step:178/1825 train_time:6152ms step_avg:34.56ms
step:179/1825 train_time:6185ms step_avg:34.56ms
step:180/1825 train_time:6221ms step_avg:34.56ms
step:181/1825 train_time:6254ms step_avg:34.55ms
step:182/1825 train_time:6289ms step_avg:34.56ms
step:183/1825 train_time:6322ms step_avg:34.55ms
step:184/1825 train_time:6357ms step_avg:34.55ms
step:185/1825 train_time:6390ms step_avg:34.54ms
step:186/1825 train_time:6426ms step_avg:34.55ms
step:187/1825 train_time:6458ms step_avg:34.54ms
step:188/1825 train_time:6494ms step_avg:34.54ms
step:189/1825 train_time:6527ms step_avg:34.53ms
step:190/1825 train_time:6562ms step_avg:34.54ms
step:191/1825 train_time:6595ms step_avg:34.53ms
step:192/1825 train_time:6630ms step_avg:34.53ms
step:193/1825 train_time:6663ms step_avg:34.53ms
step:194/1825 train_time:6699ms step_avg:34.53ms
step:195/1825 train_time:6732ms step_avg:34.52ms
step:196/1825 train_time:6767ms step_avg:34.53ms
step:197/1825 train_time:6800ms step_avg:34.52ms
step:198/1825 train_time:6835ms step_avg:34.52ms
step:199/1825 train_time:6868ms step_avg:34.51ms
step:200/1825 train_time:6904ms step_avg:34.52ms
step:201/1825 train_time:6937ms step_avg:34.51ms
step:202/1825 train_time:6972ms step_avg:34.51ms
step:203/1825 train_time:7005ms step_avg:34.51ms
step:204/1825 train_time:7040ms step_avg:34.51ms
step:205/1825 train_time:7073ms step_avg:34.50ms
step:206/1825 train_time:7108ms step_avg:34.51ms
step:207/1825 train_time:7141ms step_avg:34.50ms
step:208/1825 train_time:7176ms step_avg:34.50ms
step:209/1825 train_time:7209ms step_avg:34.49ms
step:210/1825 train_time:7244ms step_avg:34.50ms
step:211/1825 train_time:7278ms step_avg:34.49ms
step:212/1825 train_time:7313ms step_avg:34.49ms
step:213/1825 train_time:7346ms step_avg:34.49ms
step:214/1825 train_time:7381ms step_avg:34.49ms
step:215/1825 train_time:7414ms step_avg:34.48ms
step:216/1825 train_time:7449ms step_avg:34.49ms
step:217/1825 train_time:7482ms step_avg:34.48ms
step:218/1825 train_time:7518ms step_avg:34.48ms
step:219/1825 train_time:7551ms step_avg:34.48ms
step:220/1825 train_time:7586ms step_avg:34.48ms
step:221/1825 train_time:7619ms step_avg:34.47ms
step:222/1825 train_time:7654ms step_avg:34.48ms
step:223/1825 train_time:7687ms step_avg:34.47ms
step:224/1825 train_time:7722ms step_avg:34.47ms
step:225/1825 train_time:7755ms step_avg:34.47ms
step:226/1825 train_time:7790ms step_avg:34.47ms
step:227/1825 train_time:7823ms step_avg:34.46ms
step:228/1825 train_time:7859ms step_avg:34.47ms
step:229/1825 train_time:7892ms step_avg:34.46ms
step:230/1825 train_time:7927ms step_avg:34.46ms
step:231/1825 train_time:7960ms step_avg:34.46ms
step:232/1825 train_time:7995ms step_avg:34.46ms
step:233/1825 train_time:8028ms step_avg:34.45ms
step:234/1825 train_time:8063ms step_avg:34.46ms
step:235/1825 train_time:8096ms step_avg:34.45ms
step:236/1825 train_time:8132ms step_avg:34.46ms
step:237/1825 train_time:8165ms step_avg:34.45ms
step:238/1825 train_time:8200ms step_avg:34.45ms
step:239/1825 train_time:8233ms step_avg:34.45ms
step:240/1825 train_time:8268ms step_avg:34.45ms
step:241/1825 train_time:8301ms step_avg:34.44ms
step:242/1825 train_time:8337ms step_avg:34.45ms
step:243/1825 train_time:8370ms step_avg:34.44ms
step:244/1825 train_time:8405ms step_avg:34.45ms
step:245/1825 train_time:8438ms step_avg:34.44ms
step:246/1825 train_time:8473ms step_avg:34.44ms
step:247/1825 train_time:8506ms step_avg:34.44ms
step:248/1825 train_time:8542ms step_avg:34.44ms
step:249/1825 train_time:8575ms step_avg:34.44ms
step:250/1825 train_time:8610ms step_avg:34.44ms
step:250/1825 val_loss:4.6253 train_time:8652ms step_avg:34.61ms
step:251/1825 train_time:8673ms step_avg:34.55ms
step:252/1825 train_time:8691ms step_avg:34.49ms
step:253/1825 train_time:8716ms step_avg:34.45ms
step:254/1825 train_time:8751ms step_avg:34.45ms
step:255/1825 train_time:8785ms step_avg:34.45ms
step:256/1825 train_time:8822ms step_avg:34.46ms
step:257/1825 train_time:8855ms step_avg:34.46ms
step:258/1825 train_time:8890ms step_avg:34.46ms
step:259/1825 train_time:8923ms step_avg:34.45ms
step:260/1825 train_time:8959ms step_avg:34.46ms
step:261/1825 train_time:8992ms step_avg:34.45ms
step:262/1825 train_time:9027ms step_avg:34.45ms
step:263/1825 train_time:9060ms step_avg:34.45ms
step:264/1825 train_time:9095ms step_avg:34.45ms
step:265/1825 train_time:9128ms step_avg:34.44ms
step:266/1825 train_time:9163ms step_avg:34.45ms
step:267/1825 train_time:9196ms step_avg:34.44ms
step:268/1825 train_time:9231ms step_avg:34.45ms
step:269/1825 train_time:9264ms step_avg:34.44ms
step:270/1825 train_time:9300ms step_avg:34.44ms
step:271/1825 train_time:9332ms step_avg:34.44ms
step:272/1825 train_time:9368ms step_avg:34.44ms
step:273/1825 train_time:9400ms step_avg:34.43ms
step:274/1825 train_time:9436ms step_avg:34.44ms
step:275/1825 train_time:9469ms step_avg:34.43ms
step:276/1825 train_time:9504ms step_avg:34.44ms
step:277/1825 train_time:9537ms step_avg:34.43ms
step:278/1825 train_time:9572ms step_avg:34.43ms
step:279/1825 train_time:9605ms step_avg:34.43ms
step:280/1825 train_time:9640ms step_avg:34.43ms
step:281/1825 train_time:9673ms step_avg:34.42ms
step:282/1825 train_time:9709ms step_avg:34.43ms
step:283/1825 train_time:9742ms step_avg:34.42ms
step:284/1825 train_time:9777ms step_avg:34.43ms
step:285/1825 train_time:9810ms step_avg:34.42ms
step:286/1825 train_time:9845ms step_avg:34.42ms
step:287/1825 train_time:9878ms step_avg:34.42ms
step:288/1825 train_time:9913ms step_avg:34.42ms
step:289/1825 train_time:9946ms step_avg:34.42ms
step:290/1825 train_time:9981ms step_avg:34.42ms
step:291/1825 train_time:10014ms step_avg:34.41ms
step:292/1825 train_time:10049ms step_avg:34.42ms
step:293/1825 train_time:10083ms step_avg:34.41ms
step:294/1825 train_time:10118ms step_avg:34.41ms
step:295/1825 train_time:10151ms step_avg:34.41ms
step:296/1825 train_time:10187ms step_avg:34.41ms
step:297/1825 train_time:10219ms step_avg:34.41ms
step:298/1825 train_time:10254ms step_avg:34.41ms
step:299/1825 train_time:10287ms step_avg:34.40ms
step:300/1825 train_time:10322ms step_avg:34.41ms
step:301/1825 train_time:10355ms step_avg:34.40ms
step:302/1825 train_time:10390ms step_avg:34.41ms
step:303/1825 train_time:10423ms step_avg:34.40ms
step:304/1825 train_time:10459ms step_avg:34.40ms
step:305/1825 train_time:10492ms step_avg:34.40ms
step:306/1825 train_time:10527ms step_avg:34.40ms
step:307/1825 train_time:10560ms step_avg:34.40ms
step:308/1825 train_time:10595ms step_avg:34.40ms
step:309/1825 train_time:10628ms step_avg:34.39ms
step:310/1825 train_time:10663ms step_avg:34.40ms
step:311/1825 train_time:10696ms step_avg:34.39ms
step:312/1825 train_time:10732ms step_avg:34.40ms
step:313/1825 train_time:10765ms step_avg:34.39ms
step:314/1825 train_time:10800ms step_avg:34.39ms
step:315/1825 train_time:10833ms step_avg:34.39ms
step:316/1825 train_time:10868ms step_avg:34.39ms
step:317/1825 train_time:10901ms step_avg:34.39ms
step:318/1825 train_time:10936ms step_avg:34.39ms
step:319/1825 train_time:10969ms step_avg:34.39ms
step:320/1825 train_time:11004ms step_avg:34.39ms
step:321/1825 train_time:11037ms step_avg:34.38ms
step:322/1825 train_time:11073ms step_avg:34.39ms
step:323/1825 train_time:11106ms step_avg:34.38ms
step:324/1825 train_time:11141ms step_avg:34.39ms
step:325/1825 train_time:11174ms step_avg:34.38ms
step:326/1825 train_time:11209ms step_avg:34.38ms
step:327/1825 train_time:11242ms step_avg:34.38ms
step:328/1825 train_time:11277ms step_avg:34.38ms
step:329/1825 train_time:11310ms step_avg:34.38ms
step:330/1825 train_time:11345ms step_avg:34.38ms
step:331/1825 train_time:11378ms step_avg:34.38ms
step:332/1825 train_time:11414ms step_avg:34.38ms
step:333/1825 train_time:11447ms step_avg:34.37ms
step:334/1825 train_time:11482ms step_avg:34.38ms
step:335/1825 train_time:11515ms step_avg:34.37ms
step:336/1825 train_time:11550ms step_avg:34.38ms
step:337/1825 train_time:11583ms step_avg:34.37ms
step:338/1825 train_time:11618ms step_avg:34.37ms
step:339/1825 train_time:11651ms step_avg:34.37ms
step:340/1825 train_time:11686ms step_avg:34.37ms
step:341/1825 train_time:11719ms step_avg:34.37ms
step:342/1825 train_time:11755ms step_avg:34.37ms
step:343/1825 train_time:11788ms step_avg:34.37ms
step:344/1825 train_time:11823ms step_avg:34.37ms
step:345/1825 train_time:11856ms step_avg:34.37ms
step:346/1825 train_time:11891ms step_avg:34.37ms
step:347/1825 train_time:11924ms step_avg:34.36ms
step:348/1825 train_time:11960ms step_avg:34.37ms
step:349/1825 train_time:11993ms step_avg:34.36ms
step:350/1825 train_time:12028ms step_avg:34.37ms
step:351/1825 train_time:12061ms step_avg:34.36ms
step:352/1825 train_time:12096ms step_avg:34.36ms
step:353/1825 train_time:12129ms step_avg:34.36ms
step:354/1825 train_time:12164ms step_avg:34.36ms
step:355/1825 train_time:12198ms step_avg:34.36ms
step:356/1825 train_time:12233ms step_avg:34.36ms
step:357/1825 train_time:12266ms step_avg:34.36ms
step:358/1825 train_time:12301ms step_avg:34.36ms
step:359/1825 train_time:12334ms step_avg:34.36ms
step:360/1825 train_time:12369ms step_avg:34.36ms
step:361/1825 train_time:12402ms step_avg:34.35ms
step:362/1825 train_time:12437ms step_avg:34.36ms
step:363/1825 train_time:12470ms step_avg:34.35ms
step:364/1825 train_time:12505ms step_avg:34.36ms
step:365/1825 train_time:12538ms step_avg:34.35ms
step:366/1825 train_time:12573ms step_avg:34.35ms
step:367/1825 train_time:12606ms step_avg:34.35ms
step:368/1825 train_time:12642ms step_avg:34.35ms
step:369/1825 train_time:12675ms step_avg:34.35ms
step:370/1825 train_time:12710ms step_avg:34.35ms
step:371/1825 train_time:12743ms step_avg:34.35ms
step:372/1825 train_time:12778ms step_avg:34.35ms
step:373/1825 train_time:12811ms step_avg:34.35ms
step:374/1825 train_time:12846ms step_avg:34.35ms
step:375/1825 train_time:12879ms step_avg:34.34ms
step:376/1825 train_time:12914ms step_avg:34.35ms
step:377/1825 train_time:12947ms step_avg:34.34ms
step:378/1825 train_time:12982ms step_avg:34.34ms
step:379/1825 train_time:13015ms step_avg:34.34ms
step:380/1825 train_time:13050ms step_avg:34.34ms
step:381/1825 train_time:13083ms step_avg:34.34ms
step:382/1825 train_time:13119ms step_avg:34.34ms
step:383/1825 train_time:13151ms step_avg:34.34ms
step:384/1825 train_time:13187ms step_avg:34.34ms
step:385/1825 train_time:13220ms step_avg:34.34ms
step:386/1825 train_time:13255ms step_avg:34.34ms
step:387/1825 train_time:13288ms step_avg:34.34ms
step:388/1825 train_time:13323ms step_avg:34.34ms
step:389/1825 train_time:13356ms step_avg:34.33ms
step:390/1825 train_time:13391ms step_avg:34.34ms
step:391/1825 train_time:13424ms step_avg:34.33ms
step:392/1825 train_time:13460ms step_avg:34.34ms
step:393/1825 train_time:13493ms step_avg:34.33ms
step:394/1825 train_time:13528ms step_avg:34.34ms
step:395/1825 train_time:13561ms step_avg:34.33ms
step:396/1825 train_time:13596ms step_avg:34.33ms
step:397/1825 train_time:13629ms step_avg:34.33ms
step:398/1825 train_time:13665ms step_avg:34.33ms
step:399/1825 train_time:13698ms step_avg:34.33ms
step:400/1825 train_time:13733ms step_avg:34.33ms
step:401/1825 train_time:13766ms step_avg:34.33ms
step:402/1825 train_time:13801ms step_avg:34.33ms
step:403/1825 train_time:13834ms step_avg:34.33ms
step:404/1825 train_time:13870ms step_avg:34.33ms
step:405/1825 train_time:13902ms step_avg:34.33ms
step:406/1825 train_time:13938ms step_avg:34.33ms
step:407/1825 train_time:13970ms step_avg:34.33ms
step:408/1825 train_time:14006ms step_avg:34.33ms
step:409/1825 train_time:14038ms step_avg:34.32ms
step:410/1825 train_time:14074ms step_avg:34.33ms
step:411/1825 train_time:14107ms step_avg:34.32ms
step:412/1825 train_time:14142ms step_avg:34.32ms
step:413/1825 train_time:14175ms step_avg:34.32ms
step:414/1825 train_time:14210ms step_avg:34.32ms
step:415/1825 train_time:14243ms step_avg:34.32ms
step:416/1825 train_time:14278ms step_avg:34.32ms
step:417/1825 train_time:14311ms step_avg:34.32ms
step:418/1825 train_time:14346ms step_avg:34.32ms
step:419/1825 train_time:14379ms step_avg:34.32ms
step:420/1825 train_time:14414ms step_avg:34.32ms
step:421/1825 train_time:14447ms step_avg:34.32ms
step:422/1825 train_time:14483ms step_avg:34.32ms
step:423/1825 train_time:14515ms step_avg:34.32ms
step:424/1825 train_time:14551ms step_avg:34.32ms
step:425/1825 train_time:14584ms step_avg:34.31ms
step:426/1825 train_time:14619ms step_avg:34.32ms
step:427/1825 train_time:14652ms step_avg:34.31ms
step:428/1825 train_time:14687ms step_avg:34.32ms
step:429/1825 train_time:14720ms step_avg:34.31ms
step:430/1825 train_time:14755ms step_avg:34.31ms
step:431/1825 train_time:14788ms step_avg:34.31ms
step:432/1825 train_time:14823ms step_avg:34.31ms
step:433/1825 train_time:14856ms step_avg:34.31ms
step:434/1825 train_time:14892ms step_avg:34.31ms
step:435/1825 train_time:14925ms step_avg:34.31ms
step:436/1825 train_time:14960ms step_avg:34.31ms
step:437/1825 train_time:14993ms step_avg:34.31ms
step:438/1825 train_time:15028ms step_avg:34.31ms
step:439/1825 train_time:15061ms step_avg:34.31ms
step:440/1825 train_time:15096ms step_avg:34.31ms
step:441/1825 train_time:15129ms step_avg:34.31ms
step:442/1825 train_time:15165ms step_avg:34.31ms
step:443/1825 train_time:15197ms step_avg:34.31ms
step:444/1825 train_time:15233ms step_avg:34.31ms
step:445/1825 train_time:15266ms step_avg:34.30ms
step:446/1825 train_time:15301ms step_avg:34.31ms
step:447/1825 train_time:15334ms step_avg:34.30ms
step:448/1825 train_time:15369ms step_avg:34.31ms
step:449/1825 train_time:15402ms step_avg:34.30ms
step:450/1825 train_time:15437ms step_avg:34.30ms
step:451/1825 train_time:15470ms step_avg:34.30ms
step:452/1825 train_time:15505ms step_avg:34.30ms
step:453/1825 train_time:15538ms step_avg:34.30ms
step:454/1825 train_time:15573ms step_avg:34.30ms
step:455/1825 train_time:15606ms step_avg:34.30ms
step:456/1825 train_time:15641ms step_avg:34.30ms
step:457/1825 train_time:15674ms step_avg:34.30ms
step:458/1825 train_time:15710ms step_avg:34.30ms
step:459/1825 train_time:15742ms step_avg:34.30ms
step:460/1825 train_time:15778ms step_avg:34.30ms
step:461/1825 train_time:15811ms step_avg:34.30ms
step:462/1825 train_time:15846ms step_avg:34.30ms
step:463/1825 train_time:15879ms step_avg:34.30ms
step:464/1825 train_time:15914ms step_avg:34.30ms
step:465/1825 train_time:15947ms step_avg:34.29ms
step:466/1825 train_time:15982ms step_avg:34.30ms
step:467/1825 train_time:16015ms step_avg:34.29ms
step:468/1825 train_time:16050ms step_avg:34.30ms
step:469/1825 train_time:16083ms step_avg:34.29ms
step:470/1825 train_time:16118ms step_avg:34.29ms
step:471/1825 train_time:16151ms step_avg:34.29ms
step:472/1825 train_time:16187ms step_avg:34.29ms
step:473/1825 train_time:16220ms step_avg:34.29ms
step:474/1825 train_time:16255ms step_avg:34.29ms
step:475/1825 train_time:16288ms step_avg:34.29ms
step:476/1825 train_time:16323ms step_avg:34.29ms
step:477/1825 train_time:16356ms step_avg:34.29ms
step:478/1825 train_time:16391ms step_avg:34.29ms
step:479/1825 train_time:16424ms step_avg:34.29ms
step:480/1825 train_time:16460ms step_avg:34.29ms
step:481/1825 train_time:16492ms step_avg:34.29ms
step:482/1825 train_time:16528ms step_avg:34.29ms
step:483/1825 train_time:16561ms step_avg:34.29ms
step:484/1825 train_time:16596ms step_avg:34.29ms
step:485/1825 train_time:16629ms step_avg:34.29ms
step:486/1825 train_time:16664ms step_avg:34.29ms
step:487/1825 train_time:16697ms step_avg:34.29ms
step:488/1825 train_time:16733ms step_avg:34.29ms
step:489/1825 train_time:16765ms step_avg:34.29ms
step:490/1825 train_time:16801ms step_avg:34.29ms
step:491/1825 train_time:16834ms step_avg:34.28ms
step:492/1825 train_time:16869ms step_avg:34.29ms
step:493/1825 train_time:16902ms step_avg:34.28ms
step:494/1825 train_time:16937ms step_avg:34.29ms
step:495/1825 train_time:16970ms step_avg:34.28ms
step:496/1825 train_time:17005ms step_avg:34.29ms
step:497/1825 train_time:17038ms step_avg:34.28ms
step:498/1825 train_time:17074ms step_avg:34.28ms
step:499/1825 train_time:17106ms step_avg:34.28ms
step:500/1825 train_time:17142ms step_avg:34.28ms
step:500/1825 val_loss:4.2940 train_time:17183ms step_avg:34.37ms
step:501/1825 train_time:17204ms step_avg:34.34ms
step:502/1825 train_time:17222ms step_avg:34.31ms
step:503/1825 train_time:17245ms step_avg:34.28ms
step:504/1825 train_time:17281ms step_avg:34.29ms
step:505/1825 train_time:17314ms step_avg:34.29ms
step:506/1825 train_time:17350ms step_avg:34.29ms
step:507/1825 train_time:17384ms step_avg:34.29ms
step:508/1825 train_time:17419ms step_avg:34.29ms
step:509/1825 train_time:17453ms step_avg:34.29ms
step:510/1825 train_time:17488ms step_avg:34.29ms
step:511/1825 train_time:17521ms step_avg:34.29ms
step:512/1825 train_time:17557ms step_avg:34.29ms
step:513/1825 train_time:17590ms step_avg:34.29ms
step:514/1825 train_time:17625ms step_avg:34.29ms
step:515/1825 train_time:17658ms step_avg:34.29ms
step:516/1825 train_time:17693ms step_avg:34.29ms
step:517/1825 train_time:17726ms step_avg:34.29ms
step:518/1825 train_time:17761ms step_avg:34.29ms
step:519/1825 train_time:17794ms step_avg:34.29ms
step:520/1825 train_time:17829ms step_avg:34.29ms
step:521/1825 train_time:17862ms step_avg:34.28ms
step:522/1825 train_time:17897ms step_avg:34.29ms
step:523/1825 train_time:17930ms step_avg:34.28ms
step:524/1825 train_time:17965ms step_avg:34.29ms
step:525/1825 train_time:17998ms step_avg:34.28ms
step:526/1825 train_time:18034ms step_avg:34.28ms
step:527/1825 train_time:18067ms step_avg:34.28ms
step:528/1825 train_time:18102ms step_avg:34.28ms
step:529/1825 train_time:18135ms step_avg:34.28ms
step:530/1825 train_time:18170ms step_avg:34.28ms
step:531/1825 train_time:18203ms step_avg:34.28ms
step:532/1825 train_time:18238ms step_avg:34.28ms
step:533/1825 train_time:18272ms step_avg:34.28ms
step:534/1825 train_time:18307ms step_avg:34.28ms
step:535/1825 train_time:18340ms step_avg:34.28ms
step:536/1825 train_time:18375ms step_avg:34.28ms
step:537/1825 train_time:18408ms step_avg:34.28ms
step:538/1825 train_time:18444ms step_avg:34.28ms
step:539/1825 train_time:18477ms step_avg:34.28ms
step:540/1825 train_time:18512ms step_avg:34.28ms
step:541/1825 train_time:18545ms step_avg:34.28ms
step:542/1825 train_time:18580ms step_avg:34.28ms
step:543/1825 train_time:18613ms step_avg:34.28ms
step:544/1825 train_time:18648ms step_avg:34.28ms
step:545/1825 train_time:18681ms step_avg:34.28ms
step:546/1825 train_time:18717ms step_avg:34.28ms
step:547/1825 train_time:18749ms step_avg:34.28ms
step:548/1825 train_time:18784ms step_avg:34.28ms
step:549/1825 train_time:18817ms step_avg:34.28ms
step:550/1825 train_time:18853ms step_avg:34.28ms
step:551/1825 train_time:18886ms step_avg:34.28ms
step:552/1825 train_time:18921ms step_avg:34.28ms
step:553/1825 train_time:18954ms step_avg:34.27ms
step:554/1825 train_time:18989ms step_avg:34.28ms
step:555/1825 train_time:19022ms step_avg:34.27ms
step:556/1825 train_time:19057ms step_avg:34.28ms
step:557/1825 train_time:19090ms step_avg:34.27ms
step:558/1825 train_time:19125ms step_avg:34.27ms
step:559/1825 train_time:19158ms step_avg:34.27ms
step:560/1825 train_time:19194ms step_avg:34.27ms
step:561/1825 train_time:19226ms step_avg:34.27ms
step:562/1825 train_time:19262ms step_avg:34.27ms
step:563/1825 train_time:19295ms step_avg:34.27ms
step:564/1825 train_time:19330ms step_avg:34.27ms
step:565/1825 train_time:19363ms step_avg:34.27ms
step:566/1825 train_time:19398ms step_avg:34.27ms
step:567/1825 train_time:19431ms step_avg:34.27ms
step:568/1825 train_time:19466ms step_avg:34.27ms
step:569/1825 train_time:19499ms step_avg:34.27ms
step:570/1825 train_time:19535ms step_avg:34.27ms
step:571/1825 train_time:19568ms step_avg:34.27ms
step:572/1825 train_time:19603ms step_avg:34.27ms
step:573/1825 train_time:19636ms step_avg:34.27ms
step:574/1825 train_time:19672ms step_avg:34.27ms
step:575/1825 train_time:19704ms step_avg:34.27ms
step:576/1825 train_time:19740ms step_avg:34.27ms
step:577/1825 train_time:19773ms step_avg:34.27ms
step:578/1825 train_time:19808ms step_avg:34.27ms
step:579/1825 train_time:19841ms step_avg:34.27ms
step:580/1825 train_time:19876ms step_avg:34.27ms
step:581/1825 train_time:19909ms step_avg:34.27ms
step:582/1825 train_time:19945ms step_avg:34.27ms
step:583/1825 train_time:19977ms step_avg:34.27ms
step:584/1825 train_time:20013ms step_avg:34.27ms
step:585/1825 train_time:20046ms step_avg:34.27ms
step:586/1825 train_time:20081ms step_avg:34.27ms
step:587/1825 train_time:20114ms step_avg:34.27ms
step:588/1825 train_time:20149ms step_avg:34.27ms
step:589/1825 train_time:20182ms step_avg:34.27ms
step:590/1825 train_time:20218ms step_avg:34.27ms
step:591/1825 train_time:20251ms step_avg:34.27ms
step:592/1825 train_time:20286ms step_avg:34.27ms
step:593/1825 train_time:20319ms step_avg:34.26ms
step:594/1825 train_time:20355ms step_avg:34.27ms
step:595/1825 train_time:20388ms step_avg:34.26ms
step:596/1825 train_time:20425ms step_avg:34.27ms
step:597/1825 train_time:20482ms step_avg:34.31ms
step:598/1825 train_time:20545ms step_avg:34.36ms
step:599/1825 train_time:20605ms step_avg:34.40ms
step:600/1825 train_time:20668ms step_avg:34.45ms
step:601/1825 train_time:20728ms step_avg:34.49ms
step:602/1825 train_time:20790ms step_avg:34.54ms
step:603/1825 train_time:20851ms step_avg:34.58ms
step:604/1825 train_time:20913ms step_avg:34.62ms
step:605/1825 train_time:20973ms step_avg:34.67ms
step:606/1825 train_time:21037ms step_avg:34.71ms
step:607/1825 train_time:21098ms step_avg:34.76ms
step:608/1825 train_time:21161ms step_avg:34.80ms
step:609/1825 train_time:21221ms step_avg:34.85ms
step:610/1825 train_time:21285ms step_avg:34.89ms
step:611/1825 train_time:21345ms step_avg:34.93ms
step:612/1825 train_time:21407ms step_avg:34.98ms
step:613/1825 train_time:21467ms step_avg:35.02ms
step:614/1825 train_time:21530ms step_avg:35.06ms
step:615/1825 train_time:21590ms step_avg:35.11ms
step:616/1825 train_time:21654ms step_avg:35.15ms
step:617/1825 train_time:21714ms step_avg:35.19ms
step:618/1825 train_time:21777ms step_avg:35.24ms
step:619/1825 train_time:21837ms step_avg:35.28ms
step:620/1825 train_time:21900ms step_avg:35.32ms
step:621/1825 train_time:21961ms step_avg:35.36ms
step:622/1825 train_time:22024ms step_avg:35.41ms
step:623/1825 train_time:22084ms step_avg:35.45ms
step:624/1825 train_time:22146ms step_avg:35.49ms
step:625/1825 train_time:22207ms step_avg:35.53ms
step:626/1825 train_time:22270ms step_avg:35.58ms
step:627/1825 train_time:22331ms step_avg:35.62ms
step:628/1825 train_time:22394ms step_avg:35.66ms
step:629/1825 train_time:22454ms step_avg:35.70ms
step:630/1825 train_time:22517ms step_avg:35.74ms
step:631/1825 train_time:22577ms step_avg:35.78ms
step:632/1825 train_time:22640ms step_avg:35.82ms
step:633/1825 train_time:22700ms step_avg:35.86ms
step:634/1825 train_time:22763ms step_avg:35.90ms
step:635/1825 train_time:22823ms step_avg:35.94ms
step:636/1825 train_time:22886ms step_avg:35.98ms
step:637/1825 train_time:22946ms step_avg:36.02ms
step:638/1825 train_time:23008ms step_avg:36.06ms
step:639/1825 train_time:23069ms step_avg:36.10ms
step:640/1825 train_time:23132ms step_avg:36.14ms
step:641/1825 train_time:23193ms step_avg:36.18ms
step:642/1825 train_time:23256ms step_avg:36.22ms
step:643/1825 train_time:23316ms step_avg:36.26ms
step:644/1825 train_time:23380ms step_avg:36.30ms
step:645/1825 train_time:23440ms step_avg:36.34ms
step:646/1825 train_time:23503ms step_avg:36.38ms
step:647/1825 train_time:23563ms step_avg:36.42ms
step:648/1825 train_time:23626ms step_avg:36.46ms
step:649/1825 train_time:23686ms step_avg:36.50ms
step:650/1825 train_time:23748ms step_avg:36.54ms
step:651/1825 train_time:23808ms step_avg:36.57ms
step:652/1825 train_time:23871ms step_avg:36.61ms
step:653/1825 train_time:23931ms step_avg:36.65ms
step:654/1825 train_time:23994ms step_avg:36.69ms
step:655/1825 train_time:24054ms step_avg:36.72ms
step:656/1825 train_time:24117ms step_avg:36.76ms
step:657/1825 train_time:24178ms step_avg:36.80ms
step:658/1825 train_time:24241ms step_avg:36.84ms
step:659/1825 train_time:24301ms step_avg:36.88ms
step:660/1825 train_time:24364ms step_avg:36.92ms
step:661/1825 train_time:24424ms step_avg:36.95ms
step:662/1825 train_time:24487ms step_avg:36.99ms
step:663/1825 train_time:24547ms step_avg:37.02ms
step:664/1825 train_time:24609ms step_avg:37.06ms
step:665/1825 train_time:24670ms step_avg:37.10ms
step:666/1825 train_time:24733ms step_avg:37.14ms
step:667/1825 train_time:24793ms step_avg:37.17ms
step:668/1825 train_time:24856ms step_avg:37.21ms
step:669/1825 train_time:24917ms step_avg:37.25ms
step:670/1825 train_time:24980ms step_avg:37.28ms
step:671/1825 train_time:25040ms step_avg:37.32ms
step:672/1825 train_time:25103ms step_avg:37.36ms
step:673/1825 train_time:25163ms step_avg:37.39ms
step:674/1825 train_time:25225ms step_avg:37.43ms
step:675/1825 train_time:25286ms step_avg:37.46ms
step:676/1825 train_time:25348ms step_avg:37.50ms
step:677/1825 train_time:25408ms step_avg:37.53ms
step:678/1825 train_time:25471ms step_avg:37.57ms
step:679/1825 train_time:25532ms step_avg:37.60ms
step:680/1825 train_time:25595ms step_avg:37.64ms
step:681/1825 train_time:25656ms step_avg:37.67ms
step:682/1825 train_time:25719ms step_avg:37.71ms
step:683/1825 train_time:25779ms step_avg:37.74ms
step:684/1825 train_time:25842ms step_avg:37.78ms
step:685/1825 train_time:25902ms step_avg:37.81ms
step:686/1825 train_time:25964ms step_avg:37.85ms
step:687/1825 train_time:26025ms step_avg:37.88ms
step:688/1825 train_time:26088ms step_avg:37.92ms
step:689/1825 train_time:26148ms step_avg:37.95ms
step:690/1825 train_time:26210ms step_avg:37.99ms
step:691/1825 train_time:26271ms step_avg:38.02ms
step:692/1825 train_time:26335ms step_avg:38.06ms
step:693/1825 train_time:26395ms step_avg:38.09ms
step:694/1825 train_time:26458ms step_avg:38.12ms
step:695/1825 train_time:26518ms step_avg:38.16ms
step:696/1825 train_time:26581ms step_avg:38.19ms
step:697/1825 train_time:26642ms step_avg:38.22ms
step:698/1825 train_time:26704ms step_avg:38.26ms
step:699/1825 train_time:26764ms step_avg:38.29ms
step:700/1825 train_time:26827ms step_avg:38.32ms
step:701/1825 train_time:26887ms step_avg:38.36ms
step:702/1825 train_time:26950ms step_avg:38.39ms
step:703/1825 train_time:27011ms step_avg:38.42ms
step:704/1825 train_time:27074ms step_avg:38.46ms
step:705/1825 train_time:27134ms step_avg:38.49ms
step:706/1825 train_time:27197ms step_avg:38.52ms
step:707/1825 train_time:27257ms step_avg:38.55ms
step:708/1825 train_time:27320ms step_avg:38.59ms
step:709/1825 train_time:27380ms step_avg:38.62ms
step:710/1825 train_time:27443ms step_avg:38.65ms
step:711/1825 train_time:27504ms step_avg:38.68ms
step:712/1825 train_time:27566ms step_avg:38.72ms
step:713/1825 train_time:27627ms step_avg:38.75ms
step:714/1825 train_time:27689ms step_avg:38.78ms
step:715/1825 train_time:27750ms step_avg:38.81ms
step:716/1825 train_time:27813ms step_avg:38.85ms
step:717/1825 train_time:27873ms step_avg:38.87ms
step:718/1825 train_time:27937ms step_avg:38.91ms
step:719/1825 train_time:27998ms step_avg:38.94ms
step:720/1825 train_time:28061ms step_avg:38.97ms
step:721/1825 train_time:28120ms step_avg:39.00ms
step:722/1825 train_time:28184ms step_avg:39.04ms
step:723/1825 train_time:28245ms step_avg:39.07ms
step:724/1825 train_time:28307ms step_avg:39.10ms
step:725/1825 train_time:28367ms step_avg:39.13ms
step:726/1825 train_time:28430ms step_avg:39.16ms
step:727/1825 train_time:28490ms step_avg:39.19ms
step:728/1825 train_time:28554ms step_avg:39.22ms
step:729/1825 train_time:28614ms step_avg:39.25ms
step:730/1825 train_time:28677ms step_avg:39.28ms
step:731/1825 train_time:28737ms step_avg:39.31ms
step:732/1825 train_time:28800ms step_avg:39.34ms
step:733/1825 train_time:28860ms step_avg:39.37ms
step:734/1825 train_time:28922ms step_avg:39.40ms
step:735/1825 train_time:28983ms step_avg:39.43ms
step:736/1825 train_time:29045ms step_avg:39.46ms
step:737/1825 train_time:29105ms step_avg:39.49ms
step:738/1825 train_time:29169ms step_avg:39.52ms
step:739/1825 train_time:29229ms step_avg:39.55ms
step:740/1825 train_time:29292ms step_avg:39.58ms
step:741/1825 train_time:29353ms step_avg:39.61ms
step:742/1825 train_time:29415ms step_avg:39.64ms
step:743/1825 train_time:29476ms step_avg:39.67ms
step:744/1825 train_time:29538ms step_avg:39.70ms
step:745/1825 train_time:29598ms step_avg:39.73ms
step:746/1825 train_time:29661ms step_avg:39.76ms
step:747/1825 train_time:29721ms step_avg:39.79ms
step:748/1825 train_time:29784ms step_avg:39.82ms
step:749/1825 train_time:29844ms step_avg:39.85ms
step:750/1825 train_time:29906ms step_avg:39.87ms
step:750/1825 val_loss:4.0172 train_time:29977ms step_avg:39.97ms
step:751/1825 train_time:29999ms step_avg:39.95ms
step:752/1825 train_time:30033ms step_avg:39.94ms
step:753/1825 train_time:30093ms step_avg:39.96ms
step:754/1825 train_time:30158ms step_avg:40.00ms
step:755/1825 train_time:30221ms step_avg:40.03ms
step:756/1825 train_time:30285ms step_avg:40.06ms
step:757/1825 train_time:30344ms step_avg:40.09ms
step:758/1825 train_time:30407ms step_avg:40.11ms
step:759/1825 train_time:30466ms step_avg:40.14ms
step:760/1825 train_time:30528ms step_avg:40.17ms
step:761/1825 train_time:30588ms step_avg:40.19ms
step:762/1825 train_time:30650ms step_avg:40.22ms
step:763/1825 train_time:30710ms step_avg:40.25ms
step:764/1825 train_time:30773ms step_avg:40.28ms
step:765/1825 train_time:30833ms step_avg:40.30ms
step:766/1825 train_time:30896ms step_avg:40.33ms
step:767/1825 train_time:30957ms step_avg:40.36ms
step:768/1825 train_time:31021ms step_avg:40.39ms
step:769/1825 train_time:31082ms step_avg:40.42ms
step:770/1825 train_time:31147ms step_avg:40.45ms
step:771/1825 train_time:31208ms step_avg:40.48ms
step:772/1825 train_time:31270ms step_avg:40.51ms
step:773/1825 train_time:31330ms step_avg:40.53ms
step:774/1825 train_time:31394ms step_avg:40.56ms
step:775/1825 train_time:31454ms step_avg:40.59ms
step:776/1825 train_time:31518ms step_avg:40.62ms
step:777/1825 train_time:31578ms step_avg:40.64ms
step:778/1825 train_time:31641ms step_avg:40.67ms
step:779/1825 train_time:31702ms step_avg:40.70ms
step:780/1825 train_time:31764ms step_avg:40.72ms
step:781/1825 train_time:31825ms step_avg:40.75ms
step:782/1825 train_time:31888ms step_avg:40.78ms
step:783/1825 train_time:31949ms step_avg:40.80ms
step:784/1825 train_time:32011ms step_avg:40.83ms
step:785/1825 train_time:32072ms step_avg:40.86ms
step:786/1825 train_time:32136ms step_avg:40.89ms
step:787/1825 train_time:32196ms step_avg:40.91ms
step:788/1825 train_time:32260ms step_avg:40.94ms
step:789/1825 train_time:32321ms step_avg:40.97ms
step:790/1825 train_time:32385ms step_avg:40.99ms
step:791/1825 train_time:32445ms step_avg:41.02ms
step:792/1825 train_time:32508ms step_avg:41.05ms
step:793/1825 train_time:32568ms step_avg:41.07ms
step:794/1825 train_time:32630ms step_avg:41.10ms
step:795/1825 train_time:32690ms step_avg:41.12ms
step:796/1825 train_time:32753ms step_avg:41.15ms
step:797/1825 train_time:32813ms step_avg:41.17ms
step:798/1825 train_time:32876ms step_avg:41.20ms
step:799/1825 train_time:32936ms step_avg:41.22ms
step:800/1825 train_time:33000ms step_avg:41.25ms
step:801/1825 train_time:33060ms step_avg:41.27ms
step:802/1825 train_time:33124ms step_avg:41.30ms
step:803/1825 train_time:33184ms step_avg:41.33ms
step:804/1825 train_time:33247ms step_avg:41.35ms
step:805/1825 train_time:33307ms step_avg:41.38ms
step:806/1825 train_time:33369ms step_avg:41.40ms
step:807/1825 train_time:33430ms step_avg:41.42ms
step:808/1825 train_time:33493ms step_avg:41.45ms
step:809/1825 train_time:33553ms step_avg:41.47ms
step:810/1825 train_time:33616ms step_avg:41.50ms
step:811/1825 train_time:33676ms step_avg:41.52ms
step:812/1825 train_time:33739ms step_avg:41.55ms
step:813/1825 train_time:33800ms step_avg:41.57ms
step:814/1825 train_time:33862ms step_avg:41.60ms
step:815/1825 train_time:33922ms step_avg:41.62ms
step:816/1825 train_time:33985ms step_avg:41.65ms
step:817/1825 train_time:34046ms step_avg:41.67ms
step:818/1825 train_time:34108ms step_avg:41.70ms
step:819/1825 train_time:34169ms step_avg:41.72ms
step:820/1825 train_time:34232ms step_avg:41.75ms
step:821/1825 train_time:34292ms step_avg:41.77ms
step:822/1825 train_time:34355ms step_avg:41.79ms
step:823/1825 train_time:34416ms step_avg:41.82ms
step:824/1825 train_time:34480ms step_avg:41.84ms
step:825/1825 train_time:34540ms step_avg:41.87ms
step:826/1825 train_time:34603ms step_avg:41.89ms
step:827/1825 train_time:34664ms step_avg:41.92ms
step:828/1825 train_time:34727ms step_avg:41.94ms
step:829/1825 train_time:34787ms step_avg:41.96ms
step:830/1825 train_time:34850ms step_avg:41.99ms
step:831/1825 train_time:34910ms step_avg:42.01ms
step:832/1825 train_time:34973ms step_avg:42.03ms
step:833/1825 train_time:35033ms step_avg:42.06ms
step:834/1825 train_time:35096ms step_avg:42.08ms
step:835/1825 train_time:35156ms step_avg:42.10ms
step:836/1825 train_time:35220ms step_avg:42.13ms
step:837/1825 train_time:35281ms step_avg:42.15ms
step:838/1825 train_time:35344ms step_avg:42.18ms
step:839/1825 train_time:35404ms step_avg:42.20ms
step:840/1825 train_time:35467ms step_avg:42.22ms
step:841/1825 train_time:35527ms step_avg:42.24ms
step:842/1825 train_time:35589ms step_avg:42.27ms
step:843/1825 train_time:35650ms step_avg:42.29ms
step:844/1825 train_time:35713ms step_avg:42.31ms
step:845/1825 train_time:35773ms step_avg:42.34ms
step:846/1825 train_time:35836ms step_avg:42.36ms
step:847/1825 train_time:35896ms step_avg:42.38ms
step:848/1825 train_time:35959ms step_avg:42.40ms
step:849/1825 train_time:36020ms step_avg:42.43ms
step:850/1825 train_time:36082ms step_avg:42.45ms
step:851/1825 train_time:36142ms step_avg:42.47ms
step:852/1825 train_time:36205ms step_avg:42.49ms
step:853/1825 train_time:36266ms step_avg:42.52ms
step:854/1825 train_time:36329ms step_avg:42.54ms
step:855/1825 train_time:36390ms step_avg:42.56ms
step:856/1825 train_time:36452ms step_avg:42.58ms
step:857/1825 train_time:36512ms step_avg:42.60ms
step:858/1825 train_time:36576ms step_avg:42.63ms
step:859/1825 train_time:36636ms step_avg:42.65ms
step:860/1825 train_time:36701ms step_avg:42.68ms
step:861/1825 train_time:36761ms step_avg:42.70ms
step:862/1825 train_time:36823ms step_avg:42.72ms
step:863/1825 train_time:36884ms step_avg:42.74ms
step:864/1825 train_time:36947ms step_avg:42.76ms
step:865/1825 train_time:37007ms step_avg:42.78ms
step:866/1825 train_time:37069ms step_avg:42.81ms
step:867/1825 train_time:37130ms step_avg:42.83ms
step:868/1825 train_time:37192ms step_avg:42.85ms
step:869/1825 train_time:37253ms step_avg:42.87ms
step:870/1825 train_time:37317ms step_avg:42.89ms
step:871/1825 train_time:37376ms step_avg:42.91ms
step:872/1825 train_time:37440ms step_avg:42.94ms
step:873/1825 train_time:37501ms step_avg:42.96ms
step:874/1825 train_time:37564ms step_avg:42.98ms
step:875/1825 train_time:37625ms step_avg:43.00ms
step:876/1825 train_time:37688ms step_avg:43.02ms
step:877/1825 train_time:37748ms step_avg:43.04ms
step:878/1825 train_time:37810ms step_avg:43.06ms
step:879/1825 train_time:37871ms step_avg:43.08ms
step:880/1825 train_time:37934ms step_avg:43.11ms
step:881/1825 train_time:37995ms step_avg:43.13ms
step:882/1825 train_time:38057ms step_avg:43.15ms
step:883/1825 train_time:38117ms step_avg:43.17ms
step:884/1825 train_time:38180ms step_avg:43.19ms
step:885/1825 train_time:38241ms step_avg:43.21ms
step:886/1825 train_time:38304ms step_avg:43.23ms
step:887/1825 train_time:38364ms step_avg:43.25ms
step:888/1825 train_time:38426ms step_avg:43.27ms
step:889/1825 train_time:38486ms step_avg:43.29ms
step:890/1825 train_time:38550ms step_avg:43.31ms
step:891/1825 train_time:38610ms step_avg:43.33ms
step:892/1825 train_time:38673ms step_avg:43.36ms
step:893/1825 train_time:38734ms step_avg:43.37ms
step:894/1825 train_time:38797ms step_avg:43.40ms
step:895/1825 train_time:38857ms step_avg:43.42ms
step:896/1825 train_time:38921ms step_avg:43.44ms
step:897/1825 train_time:38982ms step_avg:43.46ms
step:898/1825 train_time:39045ms step_avg:43.48ms
step:899/1825 train_time:39104ms step_avg:43.50ms
step:900/1825 train_time:39167ms step_avg:43.52ms
step:901/1825 train_time:39228ms step_avg:43.54ms
step:902/1825 train_time:39290ms step_avg:43.56ms
step:903/1825 train_time:39350ms step_avg:43.58ms
step:904/1825 train_time:39414ms step_avg:43.60ms
step:905/1825 train_time:39474ms step_avg:43.62ms
step:906/1825 train_time:39538ms step_avg:43.64ms
step:907/1825 train_time:39598ms step_avg:43.66ms
step:908/1825 train_time:39661ms step_avg:43.68ms
step:909/1825 train_time:39722ms step_avg:43.70ms
step:910/1825 train_time:39786ms step_avg:43.72ms
step:911/1825 train_time:39846ms step_avg:43.74ms
step:912/1825 train_time:39908ms step_avg:43.76ms
step:913/1825 train_time:39969ms step_avg:43.78ms
step:914/1825 train_time:40032ms step_avg:43.80ms
step:915/1825 train_time:40092ms step_avg:43.82ms
step:916/1825 train_time:40154ms step_avg:43.84ms
step:917/1825 train_time:40214ms step_avg:43.85ms
step:918/1825 train_time:40278ms step_avg:43.88ms
step:919/1825 train_time:40338ms step_avg:43.89ms
step:920/1825 train_time:40401ms step_avg:43.91ms
step:921/1825 train_time:40462ms step_avg:43.93ms
step:922/1825 train_time:40525ms step_avg:43.95ms
step:923/1825 train_time:40585ms step_avg:43.97ms
step:924/1825 train_time:40648ms step_avg:43.99ms
step:925/1825 train_time:40708ms step_avg:44.01ms
step:926/1825 train_time:40770ms step_avg:44.03ms
step:927/1825 train_time:40830ms step_avg:44.05ms
step:928/1825 train_time:40894ms step_avg:44.07ms
step:929/1825 train_time:40954ms step_avg:44.08ms
step:930/1825 train_time:41017ms step_avg:44.10ms
step:931/1825 train_time:41077ms step_avg:44.12ms
step:932/1825 train_time:41140ms step_avg:44.14ms
step:933/1825 train_time:41200ms step_avg:44.16ms
step:934/1825 train_time:41263ms step_avg:44.18ms
step:935/1825 train_time:41323ms step_avg:44.20ms
step:936/1825 train_time:41385ms step_avg:44.21ms
step:937/1825 train_time:41445ms step_avg:44.23ms
step:938/1825 train_time:41508ms step_avg:44.25ms
step:939/1825 train_time:41568ms step_avg:44.27ms
step:940/1825 train_time:41631ms step_avg:44.29ms
step:941/1825 train_time:41692ms step_avg:44.31ms
step:942/1825 train_time:41754ms step_avg:44.33ms
step:943/1825 train_time:41815ms step_avg:44.34ms
step:944/1825 train_time:41878ms step_avg:44.36ms
step:945/1825 train_time:41938ms step_avg:44.38ms
step:946/1825 train_time:42001ms step_avg:44.40ms
step:947/1825 train_time:42061ms step_avg:44.41ms
step:948/1825 train_time:42124ms step_avg:44.43ms
step:949/1825 train_time:42184ms step_avg:44.45ms
step:950/1825 train_time:42247ms step_avg:44.47ms
step:951/1825 train_time:42307ms step_avg:44.49ms
step:952/1825 train_time:42369ms step_avg:44.51ms
step:953/1825 train_time:42429ms step_avg:44.52ms
step:954/1825 train_time:42492ms step_avg:44.54ms
step:955/1825 train_time:42552ms step_avg:44.56ms
step:956/1825 train_time:42615ms step_avg:44.58ms
step:957/1825 train_time:42675ms step_avg:44.59ms
step:958/1825 train_time:42738ms step_avg:44.61ms
step:959/1825 train_time:42799ms step_avg:44.63ms
step:960/1825 train_time:42862ms step_avg:44.65ms
step:961/1825 train_time:42922ms step_avg:44.66ms
step:962/1825 train_time:42985ms step_avg:44.68ms
step:963/1825 train_time:43045ms step_avg:44.70ms
step:964/1825 train_time:43108ms step_avg:44.72ms
step:965/1825 train_time:43169ms step_avg:44.73ms
step:966/1825 train_time:43232ms step_avg:44.75ms
step:967/1825 train_time:43291ms step_avg:44.77ms
step:968/1825 train_time:43354ms step_avg:44.79ms
step:969/1825 train_time:43414ms step_avg:44.80ms
step:970/1825 train_time:43478ms step_avg:44.82ms
step:971/1825 train_time:43539ms step_avg:44.84ms
step:972/1825 train_time:43602ms step_avg:44.86ms
step:973/1825 train_time:43663ms step_avg:44.87ms
step:974/1825 train_time:43726ms step_avg:44.89ms
step:975/1825 train_time:43786ms step_avg:44.91ms
step:976/1825 train_time:43848ms step_avg:44.93ms
step:977/1825 train_time:43909ms step_avg:44.94ms
step:978/1825 train_time:43971ms step_avg:44.96ms
step:979/1825 train_time:44032ms step_avg:44.98ms
step:980/1825 train_time:44095ms step_avg:45.00ms
step:981/1825 train_time:44156ms step_avg:45.01ms
step:982/1825 train_time:44219ms step_avg:45.03ms
step:983/1825 train_time:44279ms step_avg:45.05ms
step:984/1825 train_time:44342ms step_avg:45.06ms
step:985/1825 train_time:44403ms step_avg:45.08ms
step:986/1825 train_time:44466ms step_avg:45.10ms
step:987/1825 train_time:44526ms step_avg:45.11ms
step:988/1825 train_time:44589ms step_avg:45.13ms
step:989/1825 train_time:44649ms step_avg:45.15ms
step:990/1825 train_time:44712ms step_avg:45.16ms
step:991/1825 train_time:44773ms step_avg:45.18ms
step:992/1825 train_time:44836ms step_avg:45.20ms
step:993/1825 train_time:44896ms step_avg:45.21ms
step:994/1825 train_time:44959ms step_avg:45.23ms
step:995/1825 train_time:45019ms step_avg:45.25ms
step:996/1825 train_time:45082ms step_avg:45.26ms
step:997/1825 train_time:45142ms step_avg:45.28ms
step:998/1825 train_time:45204ms step_avg:45.29ms
step:999/1825 train_time:45264ms step_avg:45.31ms
step:1000/1825 train_time:45327ms step_avg:45.33ms
step:1000/1825 val_loss:3.7649 train_time:45396ms step_avg:45.40ms
step:1001/1825 train_time:45417ms step_avg:45.37ms
step:1002/1825 train_time:45451ms step_avg:45.36ms
step:1003/1825 train_time:45513ms step_avg:45.38ms
step:1004/1825 train_time:45577ms step_avg:45.40ms
step:1005/1825 train_time:45639ms step_avg:45.41ms
step:1006/1825 train_time:45703ms step_avg:45.43ms
step:1007/1825 train_time:45763ms step_avg:45.45ms
step:1008/1825 train_time:45826ms step_avg:45.46ms
step:1009/1825 train_time:45886ms step_avg:45.48ms
step:1010/1825 train_time:45948ms step_avg:45.49ms
step:1011/1825 train_time:46008ms step_avg:45.51ms
step:1012/1825 train_time:46071ms step_avg:45.52ms
step:1013/1825 train_time:46130ms step_avg:45.54ms
step:1014/1825 train_time:46192ms step_avg:45.55ms
step:1015/1825 train_time:46252ms step_avg:45.57ms
step:1016/1825 train_time:46315ms step_avg:45.59ms
step:1017/1825 train_time:46376ms step_avg:45.60ms
step:1018/1825 train_time:46439ms step_avg:45.62ms
step:1019/1825 train_time:46500ms step_avg:45.63ms
step:1020/1825 train_time:46565ms step_avg:45.65ms
step:1021/1825 train_time:46626ms step_avg:45.67ms
step:1022/1825 train_time:46689ms step_avg:45.68ms
step:1023/1825 train_time:46749ms step_avg:45.70ms
step:1024/1825 train_time:46812ms step_avg:45.71ms
step:1025/1825 train_time:46872ms step_avg:45.73ms
step:1026/1825 train_time:46934ms step_avg:45.74ms
step:1027/1825 train_time:46994ms step_avg:45.76ms
step:1028/1825 train_time:47057ms step_avg:45.78ms
step:1029/1825 train_time:47117ms step_avg:45.79ms
step:1030/1825 train_time:47180ms step_avg:45.81ms
step:1031/1825 train_time:47240ms step_avg:45.82ms
step:1032/1825 train_time:47303ms step_avg:45.84ms
step:1033/1825 train_time:47364ms step_avg:45.85ms
step:1034/1825 train_time:47428ms step_avg:45.87ms
step:1035/1825 train_time:47488ms step_avg:45.88ms
step:1036/1825 train_time:47551ms step_avg:45.90ms
step:1037/1825 train_time:47612ms step_avg:45.91ms
step:1038/1825 train_time:47675ms step_avg:45.93ms
step:1039/1825 train_time:47737ms step_avg:45.95ms
step:1040/1825 train_time:47799ms step_avg:45.96ms
step:1041/1825 train_time:47860ms step_avg:45.97ms
step:1042/1825 train_time:47923ms step_avg:45.99ms
step:1043/1825 train_time:47984ms step_avg:46.01ms
step:1044/1825 train_time:48046ms step_avg:46.02ms
step:1045/1825 train_time:48107ms step_avg:46.03ms
step:1046/1825 train_time:48169ms step_avg:46.05ms
step:1047/1825 train_time:48229ms step_avg:46.06ms
step:1048/1825 train_time:48292ms step_avg:46.08ms
step:1049/1825 train_time:48352ms step_avg:46.09ms
step:1050/1825 train_time:48415ms step_avg:46.11ms
step:1051/1825 train_time:48475ms step_avg:46.12ms
step:1052/1825 train_time:48539ms step_avg:46.14ms
step:1053/1825 train_time:48599ms step_avg:46.15ms
step:1054/1825 train_time:48663ms step_avg:46.17ms
step:1055/1825 train_time:48723ms step_avg:46.18ms
step:1056/1825 train_time:48786ms step_avg:46.20ms
step:1057/1825 train_time:48846ms step_avg:46.21ms
step:1058/1825 train_time:48909ms step_avg:46.23ms
step:1059/1825 train_time:48969ms step_avg:46.24ms
step:1060/1825 train_time:49032ms step_avg:46.26ms
step:1061/1825 train_time:49092ms step_avg:46.27ms
step:1062/1825 train_time:49155ms step_avg:46.28ms
step:1063/1825 train_time:49215ms step_avg:46.30ms
step:1064/1825 train_time:49278ms step_avg:46.31ms
step:1065/1825 train_time:49338ms step_avg:46.33ms
step:1066/1825 train_time:49400ms step_avg:46.34ms
step:1067/1825 train_time:49460ms step_avg:46.35ms
step:1068/1825 train_time:49524ms step_avg:46.37ms
step:1069/1825 train_time:49584ms step_avg:46.38ms
step:1070/1825 train_time:49647ms step_avg:46.40ms
step:1071/1825 train_time:49707ms step_avg:46.41ms
step:1072/1825 train_time:49770ms step_avg:46.43ms
step:1073/1825 train_time:49830ms step_avg:46.44ms
step:1074/1825 train_time:49893ms step_avg:46.46ms
step:1075/1825 train_time:49953ms step_avg:46.47ms
step:1076/1825 train_time:50015ms step_avg:46.48ms
step:1077/1825 train_time:50075ms step_avg:46.50ms
step:1078/1825 train_time:50140ms step_avg:46.51ms
step:1079/1825 train_time:50199ms step_avg:46.52ms
step:1080/1825 train_time:50263ms step_avg:46.54ms
step:1081/1825 train_time:50323ms step_avg:46.55ms
step:1082/1825 train_time:50386ms step_avg:46.57ms
step:1083/1825 train_time:50446ms step_avg:46.58ms
step:1084/1825 train_time:50509ms step_avg:46.59ms
step:1085/1825 train_time:50569ms step_avg:46.61ms
step:1086/1825 train_time:50631ms step_avg:46.62ms
step:1087/1825 train_time:50691ms step_avg:46.63ms
step:1088/1825 train_time:50754ms step_avg:46.65ms
step:1089/1825 train_time:50814ms step_avg:46.66ms
step:1090/1825 train_time:50878ms step_avg:46.68ms
step:1091/1825 train_time:50939ms step_avg:46.69ms
step:1092/1825 train_time:51002ms step_avg:46.71ms
step:1093/1825 train_time:51063ms step_avg:46.72ms
step:1094/1825 train_time:51126ms step_avg:46.73ms
step:1095/1825 train_time:51187ms step_avg:46.75ms
step:1096/1825 train_time:51250ms step_avg:46.76ms
step:1097/1825 train_time:51310ms step_avg:46.77ms
step:1098/1825 train_time:51372ms step_avg:46.79ms
step:1099/1825 train_time:51432ms step_avg:46.80ms
step:1100/1825 train_time:51495ms step_avg:46.81ms
step:1101/1825 train_time:51555ms step_avg:46.83ms
step:1102/1825 train_time:51618ms step_avg:46.84ms
step:1103/1825 train_time:51679ms step_avg:46.85ms
step:1104/1825 train_time:51743ms step_avg:46.87ms
step:1105/1825 train_time:51804ms step_avg:46.88ms
step:1106/1825 train_time:51867ms step_avg:46.90ms
step:1107/1825 train_time:51928ms step_avg:46.91ms
step:1108/1825 train_time:51991ms step_avg:46.92ms
step:1109/1825 train_time:52051ms step_avg:46.93ms
step:1110/1825 train_time:52113ms step_avg:46.95ms
step:1111/1825 train_time:52173ms step_avg:46.96ms
step:1112/1825 train_time:52237ms step_avg:46.98ms
step:1113/1825 train_time:52297ms step_avg:46.99ms
step:1114/1825 train_time:52360ms step_avg:47.00ms
step:1115/1825 train_time:52420ms step_avg:47.01ms
step:1116/1825 train_time:52483ms step_avg:47.03ms
step:1117/1825 train_time:52543ms step_avg:47.04ms
step:1118/1825 train_time:52606ms step_avg:47.05ms
step:1119/1825 train_time:52667ms step_avg:47.07ms
step:1120/1825 train_time:52730ms step_avg:47.08ms
step:1121/1825 train_time:52790ms step_avg:47.09ms
step:1122/1825 train_time:52853ms step_avg:47.11ms
step:1123/1825 train_time:52913ms step_avg:47.12ms
step:1124/1825 train_time:52976ms step_avg:47.13ms
step:1125/1825 train_time:53037ms step_avg:47.14ms
step:1126/1825 train_time:53099ms step_avg:47.16ms
step:1127/1825 train_time:53159ms step_avg:47.17ms
step:1128/1825 train_time:53222ms step_avg:47.18ms
step:1129/1825 train_time:53282ms step_avg:47.19ms
step:1130/1825 train_time:53345ms step_avg:47.21ms
step:1131/1825 train_time:53405ms step_avg:47.22ms
step:1132/1825 train_time:53468ms step_avg:47.23ms
step:1133/1825 train_time:53529ms step_avg:47.25ms
step:1134/1825 train_time:53592ms step_avg:47.26ms
step:1135/1825 train_time:53652ms step_avg:47.27ms
step:1136/1825 train_time:53715ms step_avg:47.28ms
step:1137/1825 train_time:53776ms step_avg:47.30ms
step:1138/1825 train_time:53839ms step_avg:47.31ms
step:1139/1825 train_time:53899ms step_avg:47.32ms
step:1140/1825 train_time:53963ms step_avg:47.34ms
step:1141/1825 train_time:54024ms step_avg:47.35ms
step:1142/1825 train_time:54087ms step_avg:47.36ms
step:1143/1825 train_time:54147ms step_avg:47.37ms
step:1144/1825 train_time:54210ms step_avg:47.39ms
step:1145/1825 train_time:54270ms step_avg:47.40ms
step:1146/1825 train_time:54332ms step_avg:47.41ms
step:1147/1825 train_time:54392ms step_avg:47.42ms
step:1148/1825 train_time:54455ms step_avg:47.43ms
step:1149/1825 train_time:54515ms step_avg:47.45ms
step:1150/1825 train_time:54578ms step_avg:47.46ms
step:1151/1825 train_time:54639ms step_avg:47.47ms
step:1152/1825 train_time:54702ms step_avg:47.48ms
step:1153/1825 train_time:54763ms step_avg:47.50ms
step:1154/1825 train_time:54827ms step_avg:47.51ms
step:1155/1825 train_time:54887ms step_avg:47.52ms
step:1156/1825 train_time:54950ms step_avg:47.53ms
step:1157/1825 train_time:55010ms step_avg:47.55ms
step:1158/1825 train_time:55072ms step_avg:47.56ms
step:1159/1825 train_time:55133ms step_avg:47.57ms
step:1160/1825 train_time:55196ms step_avg:47.58ms
step:1161/1825 train_time:55256ms step_avg:47.59ms
step:1162/1825 train_time:55319ms step_avg:47.61ms
step:1163/1825 train_time:55379ms step_avg:47.62ms
step:1164/1825 train_time:55443ms step_avg:47.63ms
step:1165/1825 train_time:55503ms step_avg:47.64ms
step:1166/1825 train_time:55566ms step_avg:47.65ms
step:1167/1825 train_time:55626ms step_avg:47.67ms
step:1168/1825 train_time:55689ms step_avg:47.68ms
step:1169/1825 train_time:55749ms step_avg:47.69ms
step:1170/1825 train_time:55812ms step_avg:47.70ms
step:1171/1825 train_time:55873ms step_avg:47.71ms
step:1172/1825 train_time:55935ms step_avg:47.73ms
step:1173/1825 train_time:55995ms step_avg:47.74ms
step:1174/1825 train_time:56058ms step_avg:47.75ms
step:1175/1825 train_time:56118ms step_avg:47.76ms
step:1176/1825 train_time:56181ms step_avg:47.77ms
step:1177/1825 train_time:56241ms step_avg:47.78ms
step:1178/1825 train_time:56304ms step_avg:47.80ms
step:1179/1825 train_time:56365ms step_avg:47.81ms
step:1180/1825 train_time:56429ms step_avg:47.82ms
step:1181/1825 train_time:56489ms step_avg:47.83ms
step:1182/1825 train_time:56552ms step_avg:47.84ms
step:1183/1825 train_time:56612ms step_avg:47.85ms
step:1184/1825 train_time:56675ms step_avg:47.87ms
step:1185/1825 train_time:56736ms step_avg:47.88ms
step:1186/1825 train_time:56799ms step_avg:47.89ms
step:1187/1825 train_time:56859ms step_avg:47.90ms
step:1188/1825 train_time:56922ms step_avg:47.91ms
step:1189/1825 train_time:56983ms step_avg:47.92ms
step:1190/1825 train_time:57046ms step_avg:47.94ms
step:1191/1825 train_time:57108ms step_avg:47.95ms
step:1192/1825 train_time:57195ms step_avg:47.98ms
step:1193/1825 train_time:57282ms step_avg:48.02ms
step:1194/1825 train_time:57371ms step_avg:48.05ms
step:1195/1825 train_time:57458ms step_avg:48.08ms
step:1196/1825 train_time:57547ms step_avg:48.12ms
step:1197/1825 train_time:57634ms step_avg:48.15ms
step:1198/1825 train_time:57723ms step_avg:48.18ms
step:1199/1825 train_time:57811ms step_avg:48.22ms
step:1200/1825 train_time:57902ms step_avg:48.25ms
step:1201/1825 train_time:57988ms step_avg:48.28ms
step:1202/1825 train_time:58078ms step_avg:48.32ms
step:1203/1825 train_time:58165ms step_avg:48.35ms
step:1204/1825 train_time:58253ms step_avg:48.38ms
step:1205/1825 train_time:58339ms step_avg:48.41ms
step:1206/1825 train_time:58427ms step_avg:48.45ms
step:1207/1825 train_time:58516ms step_avg:48.48ms
step:1208/1825 train_time:58605ms step_avg:48.51ms
step:1209/1825 train_time:58692ms step_avg:48.55ms
step:1210/1825 train_time:58782ms step_avg:48.58ms
step:1211/1825 train_time:58869ms step_avg:48.61ms
step:1212/1825 train_time:58959ms step_avg:48.65ms
step:1213/1825 train_time:59045ms step_avg:48.68ms
step:1214/1825 train_time:59134ms step_avg:48.71ms
step:1215/1825 train_time:59220ms step_avg:48.74ms
step:1216/1825 train_time:59308ms step_avg:48.77ms
step:1217/1825 train_time:59395ms step_avg:48.80ms
step:1218/1825 train_time:59486ms step_avg:48.84ms
step:1219/1825 train_time:59573ms step_avg:48.87ms
step:1220/1825 train_time:59663ms step_avg:48.90ms
step:1221/1825 train_time:59749ms step_avg:48.93ms
step:1222/1825 train_time:59838ms step_avg:48.97ms
step:1223/1825 train_time:59925ms step_avg:49.00ms
step:1224/1825 train_time:60013ms step_avg:49.03ms
step:1225/1825 train_time:60099ms step_avg:49.06ms
step:1226/1825 train_time:60187ms step_avg:49.09ms
step:1227/1825 train_time:60274ms step_avg:49.12ms
step:1228/1825 train_time:60363ms step_avg:49.16ms
step:1229/1825 train_time:60450ms step_avg:49.19ms
step:1230/1825 train_time:60539ms step_avg:49.22ms
step:1231/1825 train_time:60626ms step_avg:49.25ms
step:1232/1825 train_time:60715ms step_avg:49.28ms
step:1233/1825 train_time:60803ms step_avg:49.31ms
step:1234/1825 train_time:60892ms step_avg:49.35ms
step:1235/1825 train_time:60978ms step_avg:49.38ms
step:1236/1825 train_time:61067ms step_avg:49.41ms
step:1237/1825 train_time:61154ms step_avg:49.44ms
step:1238/1825 train_time:61244ms step_avg:49.47ms
step:1239/1825 train_time:61331ms step_avg:49.50ms
step:1240/1825 train_time:61420ms step_avg:49.53ms
step:1241/1825 train_time:61506ms step_avg:49.56ms
step:1242/1825 train_time:61595ms step_avg:49.59ms
step:1243/1825 train_time:61683ms step_avg:49.62ms
step:1244/1825 train_time:61771ms step_avg:49.66ms
step:1245/1825 train_time:61858ms step_avg:49.69ms
step:1246/1825 train_time:61946ms step_avg:49.72ms
step:1247/1825 train_time:62033ms step_avg:49.75ms
step:1248/1825 train_time:62123ms step_avg:49.78ms
step:1249/1825 train_time:62209ms step_avg:49.81ms
step:1250/1825 train_time:62298ms step_avg:49.84ms
step:1250/1825 val_loss:3.5258 train_time:62396ms step_avg:49.92ms
step:1251/1825 train_time:62416ms step_avg:49.89ms
step:1252/1825 train_time:62476ms step_avg:49.90ms
step:1253/1825 train_time:62566ms step_avg:49.93ms
step:1254/1825 train_time:62659ms step_avg:49.97ms
step:1255/1825 train_time:62746ms step_avg:50.00ms
step:1256/1825 train_time:62835ms step_avg:50.03ms
step:1257/1825 train_time:62921ms step_avg:50.06ms
step:1258/1825 train_time:63008ms step_avg:50.09ms
step:1259/1825 train_time:63095ms step_avg:50.12ms
step:1260/1825 train_time:63183ms step_avg:50.15ms
step:1261/1825 train_time:63270ms step_avg:50.17ms
step:1262/1825 train_time:63361ms step_avg:50.21ms
step:1263/1825 train_time:63450ms step_avg:50.24ms
step:1264/1825 train_time:63542ms step_avg:50.27ms
step:1265/1825 train_time:63629ms step_avg:50.30ms
step:1266/1825 train_time:63718ms step_avg:50.33ms
step:1267/1825 train_time:63804ms step_avg:50.36ms
step:1268/1825 train_time:63893ms step_avg:50.39ms
step:1269/1825 train_time:63979ms step_avg:50.42ms
step:1270/1825 train_time:64067ms step_avg:50.45ms
step:1271/1825 train_time:64153ms step_avg:50.47ms
step:1272/1825 train_time:64241ms step_avg:50.50ms
step:1273/1825 train_time:64327ms step_avg:50.53ms
step:1274/1825 train_time:64418ms step_avg:50.56ms
step:1275/1825 train_time:64505ms step_avg:50.59ms
step:1276/1825 train_time:64596ms step_avg:50.62ms
step:1277/1825 train_time:64684ms step_avg:50.65ms
step:1278/1825 train_time:64773ms step_avg:50.68ms
step:1279/1825 train_time:64860ms step_avg:50.71ms
step:1280/1825 train_time:64949ms step_avg:50.74ms
step:1281/1825 train_time:65034ms step_avg:50.77ms
step:1282/1825 train_time:65123ms step_avg:50.80ms
step:1283/1825 train_time:65209ms step_avg:50.83ms
step:1284/1825 train_time:65299ms step_avg:50.86ms
step:1285/1825 train_time:65386ms step_avg:50.88ms
step:1286/1825 train_time:65478ms step_avg:50.92ms
step:1287/1825 train_time:65565ms step_avg:50.94ms
step:1288/1825 train_time:65655ms step_avg:50.97ms
step:1289/1825 train_time:65742ms step_avg:51.00ms
step:1290/1825 train_time:65832ms step_avg:51.03ms
step:1291/1825 train_time:65918ms step_avg:51.06ms
step:1292/1825 train_time:66006ms step_avg:51.09ms
step:1293/1825 train_time:66093ms step_avg:51.12ms
step:1294/1825 train_time:66181ms step_avg:51.14ms
step:1295/1825 train_time:66266ms step_avg:51.17ms
step:1296/1825 train_time:66356ms step_avg:51.20ms
step:1297/1825 train_time:66444ms step_avg:51.23ms
step:1298/1825 train_time:66533ms step_avg:51.26ms
step:1299/1825 train_time:66620ms step_avg:51.29ms
step:1300/1825 train_time:66709ms step_avg:51.31ms
step:1301/1825 train_time:66796ms step_avg:51.34ms
step:1302/1825 train_time:66885ms step_avg:51.37ms
step:1303/1825 train_time:66971ms step_avg:51.40ms
step:1304/1825 train_time:67060ms step_avg:51.43ms
step:1305/1825 train_time:67145ms step_avg:51.45ms
step:1306/1825 train_time:67235ms step_avg:51.48ms
step:1307/1825 train_time:67321ms step_avg:51.51ms
step:1308/1825 train_time:67411ms step_avg:51.54ms
step:1309/1825 train_time:67498ms step_avg:51.56ms
step:1310/1825 train_time:67586ms step_avg:51.59ms
step:1311/1825 train_time:67674ms step_avg:51.62ms
step:1312/1825 train_time:67763ms step_avg:51.65ms
step:1313/1825 train_time:67850ms step_avg:51.68ms
step:1314/1825 train_time:67940ms step_avg:51.70ms
step:1315/1825 train_time:68027ms step_avg:51.73ms
step:1316/1825 train_time:68116ms step_avg:51.76ms
step:1317/1825 train_time:68201ms step_avg:51.79ms
step:1318/1825 train_time:68291ms step_avg:51.81ms
step:1319/1825 train_time:68377ms step_avg:51.84ms
step:1320/1825 train_time:68466ms step_avg:51.87ms
step:1321/1825 train_time:68554ms step_avg:51.90ms
step:1322/1825 train_time:68643ms step_avg:51.92ms
step:1323/1825 train_time:68729ms step_avg:51.95ms
step:1324/1825 train_time:68820ms step_avg:51.98ms
step:1325/1825 train_time:68907ms step_avg:52.01ms
step:1326/1825 train_time:68997ms step_avg:52.03ms
step:1327/1825 train_time:69083ms step_avg:52.06ms
step:1328/1825 train_time:69172ms step_avg:52.09ms
step:1329/1825 train_time:69258ms step_avg:52.11ms
step:1330/1825 train_time:69346ms step_avg:52.14ms
step:1331/1825 train_time:69433ms step_avg:52.17ms
step:1332/1825 train_time:69523ms step_avg:52.19ms
step:1333/1825 train_time:69608ms step_avg:52.22ms
step:1334/1825 train_time:69699ms step_avg:52.25ms
step:1335/1825 train_time:69784ms step_avg:52.27ms
step:1336/1825 train_time:69875ms step_avg:52.30ms
step:1337/1825 train_time:69962ms step_avg:52.33ms
step:1338/1825 train_time:70051ms step_avg:52.35ms
step:1339/1825 train_time:70139ms step_avg:52.38ms
step:1340/1825 train_time:70227ms step_avg:52.41ms
step:1341/1825 train_time:70315ms step_avg:52.43ms
step:1342/1825 train_time:70403ms step_avg:52.46ms
step:1343/1825 train_time:70489ms step_avg:52.49ms
step:1344/1825 train_time:70579ms step_avg:52.51ms
step:1345/1825 train_time:70666ms step_avg:52.54ms
step:1346/1825 train_time:70755ms step_avg:52.57ms
step:1347/1825 train_time:70841ms step_avg:52.59ms
step:1348/1825 train_time:70931ms step_avg:52.62ms
step:1349/1825 train_time:71018ms step_avg:52.64ms
step:1350/1825 train_time:71107ms step_avg:52.67ms
step:1351/1825 train_time:71194ms step_avg:52.70ms
step:1352/1825 train_time:71282ms step_avg:52.72ms
step:1353/1825 train_time:71369ms step_avg:52.75ms
step:1354/1825 train_time:71459ms step_avg:52.78ms
step:1355/1825 train_time:71544ms step_avg:52.80ms
step:1356/1825 train_time:71635ms step_avg:52.83ms
step:1357/1825 train_time:71722ms step_avg:52.85ms
step:1358/1825 train_time:71811ms step_avg:52.88ms
step:1359/1825 train_time:71898ms step_avg:52.90ms
step:1360/1825 train_time:71985ms step_avg:52.93ms
step:1361/1825 train_time:72075ms step_avg:52.96ms
step:1362/1825 train_time:72164ms step_avg:52.98ms
step:1363/1825 train_time:72251ms step_avg:53.01ms
step:1364/1825 train_time:72341ms step_avg:53.04ms
step:1365/1825 train_time:72427ms step_avg:53.06ms
step:1366/1825 train_time:72516ms step_avg:53.09ms
step:1367/1825 train_time:72603ms step_avg:53.11ms
step:1368/1825 train_time:72692ms step_avg:53.14ms
step:1369/1825 train_time:72779ms step_avg:53.16ms
step:1370/1825 train_time:72868ms step_avg:53.19ms
step:1371/1825 train_time:72955ms step_avg:53.21ms
step:1372/1825 train_time:73044ms step_avg:53.24ms
step:1373/1825 train_time:73130ms step_avg:53.26ms
step:1374/1825 train_time:73220ms step_avg:53.29ms
step:1375/1825 train_time:73307ms step_avg:53.31ms
step:1376/1825 train_time:73396ms step_avg:53.34ms
step:1377/1825 train_time:73482ms step_avg:53.36ms
step:1378/1825 train_time:73571ms step_avg:53.39ms
step:1379/1825 train_time:73659ms step_avg:53.41ms
step:1380/1825 train_time:73747ms step_avg:53.44ms
step:1381/1825 train_time:73834ms step_avg:53.46ms
step:1382/1825 train_time:73923ms step_avg:53.49ms
step:1383/1825 train_time:74010ms step_avg:53.51ms
step:1384/1825 train_time:74100ms step_avg:53.54ms
step:1385/1825 train_time:74186ms step_avg:53.56ms
step:1386/1825 train_time:74276ms step_avg:53.59ms
step:1387/1825 train_time:74361ms step_avg:53.61ms
step:1388/1825 train_time:74452ms step_avg:53.64ms
step:1389/1825 train_time:74539ms step_avg:53.66ms
step:1390/1825 train_time:74627ms step_avg:53.69ms
step:1391/1825 train_time:74715ms step_avg:53.71ms
step:1392/1825 train_time:74804ms step_avg:53.74ms
step:1393/1825 train_time:74890ms step_avg:53.76ms
step:1394/1825 train_time:74980ms step_avg:53.79ms
step:1395/1825 train_time:75067ms step_avg:53.81ms
step:1396/1825 train_time:75157ms step_avg:53.84ms
step:1397/1825 train_time:75244ms step_avg:53.86ms
step:1398/1825 train_time:75332ms step_avg:53.89ms
step:1399/1825 train_time:75418ms step_avg:53.91ms
step:1400/1825 train_time:75506ms step_avg:53.93ms
step:1401/1825 train_time:75594ms step_avg:53.96ms
step:1402/1825 train_time:75683ms step_avg:53.98ms
step:1403/1825 train_time:75770ms step_avg:54.01ms
step:1404/1825 train_time:75860ms step_avg:54.03ms
step:1405/1825 train_time:75946ms step_avg:54.05ms
step:1406/1825 train_time:76035ms step_avg:54.08ms
step:1407/1825 train_time:76122ms step_avg:54.10ms
step:1408/1825 train_time:76211ms step_avg:54.13ms
step:1409/1825 train_time:76297ms step_avg:54.15ms
step:1410/1825 train_time:76386ms step_avg:54.17ms
step:1411/1825 train_time:76473ms step_avg:54.20ms
step:1412/1825 train_time:76561ms step_avg:54.22ms
step:1413/1825 train_time:76649ms step_avg:54.25ms
step:1414/1825 train_time:76739ms step_avg:54.27ms
step:1415/1825 train_time:76826ms step_avg:54.29ms
step:1416/1825 train_time:76916ms step_avg:54.32ms
step:1417/1825 train_time:77003ms step_avg:54.34ms
step:1418/1825 train_time:77093ms step_avg:54.37ms
step:1419/1825 train_time:77179ms step_avg:54.39ms
step:1420/1825 train_time:77267ms step_avg:54.41ms
step:1421/1825 train_time:77354ms step_avg:54.44ms
step:1422/1825 train_time:77443ms step_avg:54.46ms
step:1423/1825 train_time:77529ms step_avg:54.48ms
step:1424/1825 train_time:77620ms step_avg:54.51ms
step:1425/1825 train_time:77706ms step_avg:54.53ms
step:1426/1825 train_time:77796ms step_avg:54.56ms
step:1427/1825 train_time:77882ms step_avg:54.58ms
step:1428/1825 train_time:77972ms step_avg:54.60ms
step:1429/1825 train_time:78059ms step_avg:54.62ms
step:1430/1825 train_time:78147ms step_avg:54.65ms
step:1431/1825 train_time:78234ms step_avg:54.67ms
step:1432/1825 train_time:78324ms step_avg:54.70ms
step:1433/1825 train_time:78410ms step_avg:54.72ms
step:1434/1825 train_time:78500ms step_avg:54.74ms
step:1435/1825 train_time:78587ms step_avg:54.76ms
step:1436/1825 train_time:78677ms step_avg:54.79ms
step:1437/1825 train_time:78763ms step_avg:54.81ms
step:1438/1825 train_time:78852ms step_avg:54.83ms
step:1439/1825 train_time:78939ms step_avg:54.86ms
step:1440/1825 train_time:79027ms step_avg:54.88ms
step:1441/1825 train_time:79115ms step_avg:54.90ms
step:1442/1825 train_time:79204ms step_avg:54.93ms
step:1443/1825 train_time:79290ms step_avg:54.95ms
step:1444/1825 train_time:79379ms step_avg:54.97ms
step:1445/1825 train_time:79464ms step_avg:54.99ms
step:1446/1825 train_time:79556ms step_avg:55.02ms
step:1447/1825 train_time:79643ms step_avg:55.04ms
step:1448/1825 train_time:79731ms step_avg:55.06ms
step:1449/1825 train_time:79820ms step_avg:55.09ms
step:1450/1825 train_time:79910ms step_avg:55.11ms
step:1451/1825 train_time:79996ms step_avg:55.13ms
step:1452/1825 train_time:80084ms step_avg:55.15ms
step:1453/1825 train_time:80170ms step_avg:55.18ms
step:1454/1825 train_time:80260ms step_avg:55.20ms
step:1455/1825 train_time:80346ms step_avg:55.22ms
step:1456/1825 train_time:80435ms step_avg:55.24ms
step:1457/1825 train_time:80522ms step_avg:55.27ms
step:1458/1825 train_time:80613ms step_avg:55.29ms
step:1459/1825 train_time:80700ms step_avg:55.31ms
step:1460/1825 train_time:80788ms step_avg:55.33ms
step:1461/1825 train_time:80876ms step_avg:55.36ms
step:1462/1825 train_time:80964ms step_avg:55.38ms
step:1463/1825 train_time:81051ms step_avg:55.40ms
step:1464/1825 train_time:81140ms step_avg:55.42ms
step:1465/1825 train_time:81227ms step_avg:55.44ms
step:1466/1825 train_time:81316ms step_avg:55.47ms
step:1467/1825 train_time:81402ms step_avg:55.49ms
step:1468/1825 train_time:81491ms step_avg:55.51ms
step:1469/1825 train_time:81578ms step_avg:55.53ms
step:1470/1825 train_time:81668ms step_avg:55.56ms
step:1471/1825 train_time:81754ms step_avg:55.58ms
step:1472/1825 train_time:81844ms step_avg:55.60ms
step:1473/1825 train_time:81931ms step_avg:55.62ms
step:1474/1825 train_time:82020ms step_avg:55.64ms
step:1475/1825 train_time:82107ms step_avg:55.67ms
step:1476/1825 train_time:82199ms step_avg:55.69ms
step:1477/1825 train_time:82286ms step_avg:55.71ms
step:1478/1825 train_time:82377ms step_avg:55.74ms
step:1479/1825 train_time:82463ms step_avg:55.76ms
step:1480/1825 train_time:82553ms step_avg:55.78ms
step:1481/1825 train_time:82640ms step_avg:55.80ms
step:1482/1825 train_time:82728ms step_avg:55.82ms
step:1483/1825 train_time:82814ms step_avg:55.84ms
step:1484/1825 train_time:82903ms step_avg:55.86ms
step:1485/1825 train_time:82990ms step_avg:55.89ms
step:1486/1825 train_time:83081ms step_avg:55.91ms
step:1487/1825 train_time:83167ms step_avg:55.93ms
step:1488/1825 train_time:83257ms step_avg:55.95ms
step:1489/1825 train_time:83344ms step_avg:55.97ms
step:1490/1825 train_time:83434ms step_avg:56.00ms
step:1491/1825 train_time:83521ms step_avg:56.02ms
step:1492/1825 train_time:83610ms step_avg:56.04ms
step:1493/1825 train_time:83697ms step_avg:56.06ms
step:1494/1825 train_time:83786ms step_avg:56.08ms
step:1495/1825 train_time:83872ms step_avg:56.10ms
step:1496/1825 train_time:83962ms step_avg:56.12ms
step:1497/1825 train_time:84048ms step_avg:56.14ms
step:1498/1825 train_time:84138ms step_avg:56.17ms
step:1499/1825 train_time:84225ms step_avg:56.19ms
step:1500/1825 train_time:84314ms step_avg:56.21ms
step:1500/1825 val_loss:3.3965 train_time:84411ms step_avg:56.27ms
step:1501/1825 train_time:84429ms step_avg:56.25ms
step:1502/1825 train_time:84491ms step_avg:56.25ms
step:1503/1825 train_time:84581ms step_avg:56.27ms
step:1504/1825 train_time:84670ms step_avg:56.30ms
step:1505/1825 train_time:84756ms step_avg:56.32ms
step:1506/1825 train_time:84845ms step_avg:56.34ms
step:1507/1825 train_time:84930ms step_avg:56.36ms
step:1508/1825 train_time:85019ms step_avg:56.38ms
step:1509/1825 train_time:85105ms step_avg:56.40ms
step:1510/1825 train_time:85192ms step_avg:56.42ms
step:1511/1825 train_time:85278ms step_avg:56.44ms
step:1512/1825 train_time:85369ms step_avg:56.46ms
step:1513/1825 train_time:85457ms step_avg:56.48ms
step:1514/1825 train_time:85549ms step_avg:56.51ms
step:1515/1825 train_time:85638ms step_avg:56.53ms
step:1516/1825 train_time:85728ms step_avg:56.55ms
step:1517/1825 train_time:85815ms step_avg:56.57ms
step:1518/1825 train_time:85903ms step_avg:56.59ms
step:1519/1825 train_time:85989ms step_avg:56.61ms
step:1520/1825 train_time:86077ms step_avg:56.63ms
step:1521/1825 train_time:86162ms step_avg:56.65ms
step:1522/1825 train_time:86252ms step_avg:56.67ms
step:1523/1825 train_time:86338ms step_avg:56.69ms
step:1524/1825 train_time:86428ms step_avg:56.71ms
step:1525/1825 train_time:86516ms step_avg:56.73ms
step:1526/1825 train_time:86605ms step_avg:56.75ms
step:1527/1825 train_time:86692ms step_avg:56.77ms
step:1528/1825 train_time:86780ms step_avg:56.79ms
step:1529/1825 train_time:86866ms step_avg:56.81ms
step:1530/1825 train_time:86955ms step_avg:56.83ms
step:1531/1825 train_time:87041ms step_avg:56.85ms
step:1532/1825 train_time:87130ms step_avg:56.87ms
step:1533/1825 train_time:87215ms step_avg:56.89ms
step:1534/1825 train_time:87304ms step_avg:56.91ms
step:1535/1825 train_time:87390ms step_avg:56.93ms
step:1536/1825 train_time:87480ms step_avg:56.95ms
step:1537/1825 train_time:87569ms step_avg:56.97ms
step:1538/1825 train_time:87658ms step_avg:57.00ms
step:1539/1825 train_time:87745ms step_avg:57.01ms
step:1540/1825 train_time:87835ms step_avg:57.04ms
step:1541/1825 train_time:87921ms step_avg:57.05ms
step:1542/1825 train_time:88010ms step_avg:57.08ms
step:1543/1825 train_time:88095ms step_avg:57.09ms
step:1544/1825 train_time:88184ms step_avg:57.11ms
step:1545/1825 train_time:88271ms step_avg:57.13ms
step:1546/1825 train_time:88359ms step_avg:57.15ms
step:1547/1825 train_time:88446ms step_avg:57.17ms
step:1548/1825 train_time:88536ms step_avg:57.19ms
step:1549/1825 train_time:88623ms step_avg:57.21ms
step:1550/1825 train_time:88713ms step_avg:57.23ms
step:1551/1825 train_time:88799ms step_avg:57.25ms
step:1552/1825 train_time:88889ms step_avg:57.27ms
step:1553/1825 train_time:88975ms step_avg:57.29ms
step:1554/1825 train_time:89064ms step_avg:57.31ms
step:1555/1825 train_time:89149ms step_avg:57.33ms
step:1556/1825 train_time:89238ms step_avg:57.35ms
step:1557/1825 train_time:89324ms step_avg:57.37ms
step:1558/1825 train_time:89414ms step_avg:57.39ms
step:1559/1825 train_time:89500ms step_avg:57.41ms
step:1560/1825 train_time:89590ms step_avg:57.43ms
step:1561/1825 train_time:89677ms step_avg:57.45ms
step:1562/1825 train_time:89766ms step_avg:57.47ms
step:1563/1825 train_time:89852ms step_avg:57.49ms
step:1564/1825 train_time:89941ms step_avg:57.51ms
step:1565/1825 train_time:90027ms step_avg:57.53ms
step:1566/1825 train_time:90115ms step_avg:57.54ms
step:1567/1825 train_time:90201ms step_avg:57.56ms
step:1568/1825 train_time:90290ms step_avg:57.58ms
step:1569/1825 train_time:90377ms step_avg:57.60ms
step:1570/1825 train_time:90467ms step_avg:57.62ms
step:1571/1825 train_time:90554ms step_avg:57.64ms
step:1572/1825 train_time:90643ms step_avg:57.66ms
step:1573/1825 train_time:90730ms step_avg:57.68ms
step:1574/1825 train_time:90819ms step_avg:57.70ms
step:1575/1825 train_time:90905ms step_avg:57.72ms
step:1576/1825 train_time:90994ms step_avg:57.74ms
step:1577/1825 train_time:91080ms step_avg:57.76ms
step:1578/1825 train_time:91168ms step_avg:57.77ms
step:1579/1825 train_time:91256ms step_avg:57.79ms
step:1580/1825 train_time:91345ms step_avg:57.81ms
step:1581/1825 train_time:91432ms step_avg:57.83ms
step:1582/1825 train_time:91520ms step_avg:57.85ms
step:1583/1825 train_time:91606ms step_avg:57.87ms
step:1584/1825 train_time:91696ms step_avg:57.89ms
step:1585/1825 train_time:91782ms step_avg:57.91ms
step:1586/1825 train_time:91871ms step_avg:57.93ms
step:1587/1825 train_time:91958ms step_avg:57.94ms
step:1588/1825 train_time:92047ms step_avg:57.96ms
step:1589/1825 train_time:92133ms step_avg:57.98ms
step:1590/1825 train_time:92221ms step_avg:58.00ms
step:1591/1825 train_time:92307ms step_avg:58.02ms
step:1592/1825 train_time:92396ms step_avg:58.04ms
step:1593/1825 train_time:92482ms step_avg:58.06ms
step:1594/1825 train_time:92575ms step_avg:58.08ms
step:1595/1825 train_time:92660ms step_avg:58.09ms
step:1596/1825 train_time:92750ms step_avg:58.11ms
step:1597/1825 train_time:92836ms step_avg:58.13ms
step:1598/1825 train_time:92925ms step_avg:58.15ms
step:1599/1825 train_time:93011ms step_avg:58.17ms
step:1600/1825 train_time:93099ms step_avg:58.19ms
step:1601/1825 train_time:93185ms step_avg:58.20ms
step:1602/1825 train_time:93275ms step_avg:58.22ms
step:1603/1825 train_time:93361ms step_avg:58.24ms
step:1604/1825 train_time:93451ms step_avg:58.26ms
step:1605/1825 train_time:93538ms step_avg:58.28ms
step:1606/1825 train_time:93627ms step_avg:58.30ms
step:1607/1825 train_time:93713ms step_avg:58.32ms
step:1608/1825 train_time:93802ms step_avg:58.33ms
step:1609/1825 train_time:93888ms step_avg:58.35ms
step:1610/1825 train_time:93977ms step_avg:58.37ms
step:1611/1825 train_time:94062ms step_avg:58.39ms
step:1612/1825 train_time:94152ms step_avg:58.41ms
step:1613/1825 train_time:94238ms step_avg:58.42ms
step:1614/1825 train_time:94327ms step_avg:58.44ms
step:1615/1825 train_time:94415ms step_avg:58.46ms
step:1616/1825 train_time:94504ms step_avg:58.48ms
step:1617/1825 train_time:94591ms step_avg:58.50ms
step:1618/1825 train_time:94679ms step_avg:58.52ms
step:1619/1825 train_time:94766ms step_avg:58.53ms
step:1620/1825 train_time:94855ms step_avg:58.55ms
step:1621/1825 train_time:94942ms step_avg:58.57ms
step:1622/1825 train_time:95031ms step_avg:58.59ms
step:1623/1825 train_time:95118ms step_avg:58.61ms
step:1624/1825 train_time:95207ms step_avg:58.62ms
step:1625/1825 train_time:95293ms step_avg:58.64ms
step:1626/1825 train_time:95382ms step_avg:58.66ms
step:1627/1825 train_time:95469ms step_avg:58.68ms
step:1628/1825 train_time:95559ms step_avg:58.70ms
step:1629/1825 train_time:95645ms step_avg:58.71ms
step:1630/1825 train_time:95734ms step_avg:58.73ms
step:1631/1825 train_time:95820ms step_avg:58.75ms
step:1632/1825 train_time:95910ms step_avg:58.77ms
step:1633/1825 train_time:95996ms step_avg:58.79ms
step:1634/1825 train_time:96084ms step_avg:58.80ms
step:1635/1825 train_time:96170ms step_avg:58.82ms
step:1636/1825 train_time:96259ms step_avg:58.84ms
step:1637/1825 train_time:96346ms step_avg:58.86ms
step:1638/1825 train_time:96435ms step_avg:58.87ms
step:1639/1825 train_time:96521ms step_avg:58.89ms
step:1640/1825 train_time:96610ms step_avg:58.91ms
step:1641/1825 train_time:96697ms step_avg:58.93ms
step:1642/1825 train_time:96786ms step_avg:58.94ms
step:1643/1825 train_time:96873ms step_avg:58.96ms
step:1644/1825 train_time:96961ms step_avg:58.98ms
step:1645/1825 train_time:97047ms step_avg:59.00ms
step:1646/1825 train_time:97137ms step_avg:59.01ms
step:1647/1825 train_time:97223ms step_avg:59.03ms
step:1648/1825 train_time:97313ms step_avg:59.05ms
step:1649/1825 train_time:97399ms step_avg:59.07ms
step:1650/1825 train_time:97488ms step_avg:59.08ms
step:1651/1825 train_time:97576ms step_avg:59.10ms
step:1652/1825 train_time:97665ms step_avg:59.12ms
step:1653/1825 train_time:97751ms step_avg:59.14ms
step:1654/1825 train_time:97840ms step_avg:59.15ms
step:1655/1825 train_time:97927ms step_avg:59.17ms
step:1656/1825 train_time:98017ms step_avg:59.19ms
step:1657/1825 train_time:98103ms step_avg:59.21ms
step:1658/1825 train_time:98192ms step_avg:59.22ms
step:1659/1825 train_time:98279ms step_avg:59.24ms
step:1660/1825 train_time:98367ms step_avg:59.26ms
step:1661/1825 train_time:98455ms step_avg:59.27ms
step:1662/1825 train_time:98544ms step_avg:59.29ms
step:1663/1825 train_time:98630ms step_avg:59.31ms
step:1664/1825 train_time:98719ms step_avg:59.33ms
step:1665/1825 train_time:98806ms step_avg:59.34ms
step:1666/1825 train_time:98895ms step_avg:59.36ms
step:1667/1825 train_time:98981ms step_avg:59.38ms
step:1668/1825 train_time:99070ms step_avg:59.39ms
step:1669/1825 train_time:99157ms step_avg:59.41ms
step:1670/1825 train_time:99246ms step_avg:59.43ms
step:1671/1825 train_time:99333ms step_avg:59.45ms
step:1672/1825 train_time:99421ms step_avg:59.46ms
step:1673/1825 train_time:99508ms step_avg:59.48ms
step:1674/1825 train_time:99597ms step_avg:59.50ms
step:1675/1825 train_time:99683ms step_avg:59.51ms
step:1676/1825 train_time:99773ms step_avg:59.53ms
step:1677/1825 train_time:99859ms step_avg:59.55ms
step:1678/1825 train_time:99949ms step_avg:59.56ms
step:1679/1825 train_time:100036ms step_avg:59.58ms
step:1680/1825 train_time:100126ms step_avg:59.60ms
step:1681/1825 train_time:100212ms step_avg:59.61ms
step:1682/1825 train_time:100300ms step_avg:59.63ms
step:1683/1825 train_time:100387ms step_avg:59.65ms
step:1684/1825 train_time:100476ms step_avg:59.67ms
step:1685/1825 train_time:100561ms step_avg:59.68ms
step:1686/1825 train_time:100650ms step_avg:59.70ms
step:1687/1825 train_time:100737ms step_avg:59.71ms
step:1688/1825 train_time:100827ms step_avg:59.73ms
step:1689/1825 train_time:100913ms step_avg:59.75ms
step:1690/1825 train_time:101002ms step_avg:59.76ms
step:1691/1825 train_time:101088ms step_avg:59.78ms
step:1692/1825 train_time:101177ms step_avg:59.80ms
step:1693/1825 train_time:101262ms step_avg:59.81ms
step:1694/1825 train_time:101353ms step_avg:59.83ms
step:1695/1825 train_time:101439ms step_avg:59.85ms
step:1696/1825 train_time:101529ms step_avg:59.86ms
step:1697/1825 train_time:101614ms step_avg:59.88ms
step:1698/1825 train_time:101705ms step_avg:59.90ms
step:1699/1825 train_time:101791ms step_avg:59.91ms
step:1700/1825 train_time:101880ms step_avg:59.93ms
step:1701/1825 train_time:101966ms step_avg:59.94ms
step:1702/1825 train_time:102056ms step_avg:59.96ms
step:1703/1825 train_time:102142ms step_avg:59.98ms
step:1704/1825 train_time:102232ms step_avg:60.00ms
step:1705/1825 train_time:102317ms step_avg:60.01ms
step:1706/1825 train_time:102407ms step_avg:60.03ms
step:1707/1825 train_time:102493ms step_avg:60.04ms
step:1708/1825 train_time:102582ms step_avg:60.06ms
step:1709/1825 train_time:102669ms step_avg:60.08ms
step:1710/1825 train_time:102757ms step_avg:60.09ms
step:1711/1825 train_time:102844ms step_avg:60.11ms
step:1712/1825 train_time:102934ms step_avg:60.13ms
step:1713/1825 train_time:103021ms step_avg:60.14ms
step:1714/1825 train_time:103110ms step_avg:60.16ms
step:1715/1825 train_time:103197ms step_avg:60.17ms
step:1716/1825 train_time:103285ms step_avg:60.19ms
step:1717/1825 train_time:103372ms step_avg:60.20ms
step:1718/1825 train_time:103460ms step_avg:60.22ms
step:1719/1825 train_time:103547ms step_avg:60.24ms
step:1720/1825 train_time:103637ms step_avg:60.25ms
step:1721/1825 train_time:103723ms step_avg:60.27ms
step:1722/1825 train_time:103813ms step_avg:60.29ms
step:1723/1825 train_time:103899ms step_avg:60.30ms
step:1724/1825 train_time:103988ms step_avg:60.32ms
step:1725/1825 train_time:104075ms step_avg:60.33ms
step:1726/1825 train_time:104164ms step_avg:60.35ms
step:1727/1825 train_time:104251ms step_avg:60.37ms
step:1728/1825 train_time:104341ms step_avg:60.38ms
step:1729/1825 train_time:104427ms step_avg:60.40ms
step:1730/1825 train_time:104515ms step_avg:60.41ms
step:1731/1825 train_time:104602ms step_avg:60.43ms
step:1732/1825 train_time:104692ms step_avg:60.45ms
step:1733/1825 train_time:104777ms step_avg:60.46ms
step:1734/1825 train_time:104867ms step_avg:60.48ms
step:1735/1825 train_time:104953ms step_avg:60.49ms
step:1736/1825 train_time:105041ms step_avg:60.51ms
step:1737/1825 train_time:105128ms step_avg:60.52ms
step:1738/1825 train_time:105217ms step_avg:60.54ms
step:1739/1825 train_time:105304ms step_avg:60.55ms
step:1740/1825 train_time:105394ms step_avg:60.57ms
step:1741/1825 train_time:105479ms step_avg:60.59ms
step:1742/1825 train_time:105568ms step_avg:60.60ms
step:1743/1825 train_time:105654ms step_avg:60.62ms
step:1744/1825 train_time:105744ms step_avg:60.63ms
step:1745/1825 train_time:105831ms step_avg:60.65ms
step:1746/1825 train_time:105919ms step_avg:60.66ms
step:1747/1825 train_time:106005ms step_avg:60.68ms
step:1748/1825 train_time:106095ms step_avg:60.69ms
step:1749/1825 train_time:106181ms step_avg:60.71ms
step:1750/1825 train_time:106270ms step_avg:60.73ms
step:1750/1825 val_loss:3.2991 train_time:106367ms step_avg:60.78ms
step:1751/1825 train_time:106385ms step_avg:60.76ms
step:1752/1825 train_time:106447ms step_avg:60.76ms
step:1753/1825 train_time:106537ms step_avg:60.77ms
step:1754/1825 train_time:106629ms step_avg:60.79ms
step:1755/1825 train_time:106716ms step_avg:60.81ms
step:1756/1825 train_time:106804ms step_avg:60.82ms
step:1757/1825 train_time:106890ms step_avg:60.84ms
step:1758/1825 train_time:106979ms step_avg:60.85ms
step:1759/1825 train_time:107064ms step_avg:60.87ms
step:1760/1825 train_time:107152ms step_avg:60.88ms
step:1761/1825 train_time:107236ms step_avg:60.90ms
step:1762/1825 train_time:107328ms step_avg:60.91ms
step:1763/1825 train_time:107416ms step_avg:60.93ms
step:1764/1825 train_time:107507ms step_avg:60.95ms
step:1765/1825 train_time:107595ms step_avg:60.96ms
step:1766/1825 train_time:107685ms step_avg:60.98ms
step:1767/1825 train_time:107771ms step_avg:60.99ms
step:1768/1825 train_time:107859ms step_avg:61.01ms
step:1769/1825 train_time:107944ms step_avg:61.02ms
step:1770/1825 train_time:108033ms step_avg:61.04ms
step:1771/1825 train_time:108119ms step_avg:61.05ms
step:1772/1825 train_time:108207ms step_avg:61.06ms
step:1773/1825 train_time:108294ms step_avg:61.08ms
step:1774/1825 train_time:108383ms step_avg:61.10ms
step:1775/1825 train_time:108471ms step_avg:61.11ms
step:1776/1825 train_time:108560ms step_avg:61.13ms
step:1777/1825 train_time:108648ms step_avg:61.14ms
step:1778/1825 train_time:108738ms step_avg:61.16ms
step:1779/1825 train_time:108824ms step_avg:61.17ms
step:1780/1825 train_time:108913ms step_avg:61.19ms
step:1781/1825 train_time:108999ms step_avg:61.20ms
step:1782/1825 train_time:109088ms step_avg:61.22ms
step:1783/1825 train_time:109175ms step_avg:61.23ms
step:1784/1825 train_time:109263ms step_avg:61.25ms
step:1785/1825 train_time:109350ms step_avg:61.26ms
step:1786/1825 train_time:109441ms step_avg:61.28ms
step:1787/1825 train_time:109527ms step_avg:61.29ms
step:1788/1825 train_time:109618ms step_avg:61.31ms
step:1789/1825 train_time:109704ms step_avg:61.32ms
step:1790/1825 train_time:109794ms step_avg:61.34ms
step:1791/1825 train_time:109880ms step_avg:61.35ms
step:1792/1825 train_time:109969ms step_avg:61.37ms
step:1793/1825 train_time:110055ms step_avg:61.38ms
step:1794/1825 train_time:110143ms step_avg:61.40ms
step:1795/1825 train_time:110230ms step_avg:61.41ms
step:1796/1825 train_time:110319ms step_avg:61.42ms
step:1797/1825 train_time:110406ms step_avg:61.44ms
step:1798/1825 train_time:110496ms step_avg:61.45ms
step:1799/1825 train_time:110584ms step_avg:61.47ms
step:1800/1825 train_time:110674ms step_avg:61.49ms
step:1801/1825 train_time:110760ms step_avg:61.50ms
step:1802/1825 train_time:110850ms step_avg:61.52ms
step:1803/1825 train_time:110936ms step_avg:61.53ms
step:1804/1825 train_time:111025ms step_avg:61.54ms
step:1805/1825 train_time:111112ms step_avg:61.56ms
step:1806/1825 train_time:111200ms step_avg:61.57ms
step:1807/1825 train_time:111287ms step_avg:61.59ms
step:1808/1825 train_time:111377ms step_avg:61.60ms
step:1809/1825 train_time:111463ms step_avg:61.62ms
step:1810/1825 train_time:111552ms step_avg:61.63ms
step:1811/1825 train_time:111639ms step_avg:61.65ms
step:1812/1825 train_time:111729ms step_avg:61.66ms
step:1813/1825 train_time:111815ms step_avg:61.67ms
step:1814/1825 train_time:111905ms step_avg:61.69ms
step:1815/1825 train_time:111991ms step_avg:61.70ms
step:1816/1825 train_time:112079ms step_avg:61.72ms
step:1817/1825 train_time:112166ms step_avg:61.73ms
step:1818/1825 train_time:112256ms step_avg:61.75ms
step:1819/1825 train_time:112342ms step_avg:61.76ms
step:1820/1825 train_time:112432ms step_avg:61.78ms
step:1821/1825 train_time:112519ms step_avg:61.79ms
step:1822/1825 train_time:112609ms step_avg:61.81ms
step:1823/1825 train_time:112697ms step_avg:61.82ms
step:1824/1825 train_time:112788ms step_avg:61.84ms
step:1825/1825 train_time:112875ms step_avg:61.85ms
step:1825/1825 val_loss:3.2780 train_time:112971ms step_avg:61.90ms
peak memory allocated: 30125 MiB reserved: 45518 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1785  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 18:59:54 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    355713      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    355714      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    355715      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    355716      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    355717      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    355718      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    355719      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    355720      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 594, 595, 596, 1189, 1190, 1191, 1784, 1785, 1786] for warmup
Resetting Model
step:0/1825 val_loss:10.8321 train_time:0ms step_avg:0.03ms
step:1/1825 train_time:74ms step_avg:74.28ms
step:2/1825 train_time:95ms step_avg:47.39ms
step:3/1825 train_time:114ms step_avg:37.90ms
step:4/1825 train_time:149ms step_avg:37.23ms
step:5/1825 train_time:182ms step_avg:36.39ms
step:6/1825 train_time:276ms step_avg:45.99ms
step:7/1825 train_time:292ms step_avg:41.71ms
step:8/1825 train_time:320ms step_avg:40.02ms
step:9/1825 train_time:353ms step_avg:39.23ms
step:10/1825 train_time:388ms step_avg:38.83ms
step:11/1825 train_time:421ms step_avg:38.29ms
step:12/1825 train_time:457ms step_avg:38.06ms
step:13/1825 train_time:490ms step_avg:37.67ms
step:14/1825 train_time:525ms step_avg:37.51ms
step:15/1825 train_time:558ms step_avg:37.22ms
step:16/1825 train_time:594ms step_avg:37.10ms
step:17/1825 train_time:627ms step_avg:36.86ms
step:18/1825 train_time:662ms step_avg:36.78ms
step:19/1825 train_time:695ms step_avg:36.58ms
step:20/1825 train_time:730ms step_avg:36.52ms
step:21/1825 train_time:764ms step_avg:36.36ms
step:22/1825 train_time:799ms step_avg:36.32ms
step:23/1825 train_time:832ms step_avg:36.18ms
step:24/1825 train_time:867ms step_avg:36.15ms
step:25/1825 train_time:901ms step_avg:36.02ms
step:26/1825 train_time:936ms step_avg:36.00ms
step:27/1825 train_time:969ms step_avg:35.89ms
step:28/1825 train_time:1004ms step_avg:35.87ms
step:29/1825 train_time:1037ms step_avg:35.77ms
step:30/1825 train_time:1073ms step_avg:35.76ms
step:31/1825 train_time:1106ms step_avg:35.67ms
step:32/1825 train_time:1141ms step_avg:35.67ms
step:33/1825 train_time:1175ms step_avg:35.60ms
step:34/1825 train_time:1210ms step_avg:35.60ms
step:35/1825 train_time:1243ms step_avg:35.53ms
step:36/1825 train_time:1279ms step_avg:35.53ms
step:37/1825 train_time:1312ms step_avg:35.46ms
step:38/1825 train_time:1348ms step_avg:35.47ms
step:39/1825 train_time:1381ms step_avg:35.42ms
step:40/1825 train_time:1417ms step_avg:35.42ms
step:41/1825 train_time:1450ms step_avg:35.36ms
step:42/1825 train_time:1485ms step_avg:35.36ms
step:43/1825 train_time:1518ms step_avg:35.31ms
step:44/1825 train_time:1554ms step_avg:35.31ms
step:45/1825 train_time:1587ms step_avg:35.26ms
step:46/1825 train_time:1622ms step_avg:35.27ms
step:47/1825 train_time:1656ms step_avg:35.22ms
step:48/1825 train_time:1691ms step_avg:35.23ms
step:49/1825 train_time:1724ms step_avg:35.18ms
step:50/1825 train_time:1759ms step_avg:35.19ms
step:51/1825 train_time:1792ms step_avg:35.15ms
step:52/1825 train_time:1828ms step_avg:35.15ms
step:53/1825 train_time:1861ms step_avg:35.12ms
step:54/1825 train_time:1897ms step_avg:35.12ms
step:55/1825 train_time:1930ms step_avg:35.09ms
step:56/1825 train_time:1965ms step_avg:35.09ms
step:57/1825 train_time:1998ms step_avg:35.06ms
step:58/1825 train_time:2034ms step_avg:35.07ms
step:59/1825 train_time:2067ms step_avg:35.03ms
step:60/1825 train_time:2102ms step_avg:35.04ms
step:61/1825 train_time:2135ms step_avg:35.01ms
step:62/1825 train_time:2171ms step_avg:35.02ms
step:63/1825 train_time:2204ms step_avg:34.99ms
step:64/1825 train_time:2240ms step_avg:34.99ms
step:65/1825 train_time:2272ms step_avg:34.96ms
step:66/1825 train_time:2308ms step_avg:34.97ms
step:67/1825 train_time:2341ms step_avg:34.94ms
step:68/1825 train_time:2376ms step_avg:34.94ms
step:69/1825 train_time:2409ms step_avg:34.91ms
step:70/1825 train_time:2445ms step_avg:34.92ms
step:71/1825 train_time:2478ms step_avg:34.90ms
step:72/1825 train_time:2513ms step_avg:34.91ms
step:73/1825 train_time:2546ms step_avg:34.88ms
step:74/1825 train_time:2582ms step_avg:34.89ms
step:75/1825 train_time:2615ms step_avg:34.87ms
step:76/1825 train_time:2650ms step_avg:34.87ms
step:77/1825 train_time:2683ms step_avg:34.85ms
step:78/1825 train_time:2719ms step_avg:34.86ms
step:79/1825 train_time:2752ms step_avg:34.83ms
step:80/1825 train_time:2787ms step_avg:34.84ms
step:81/1825 train_time:2820ms step_avg:34.82ms
step:82/1825 train_time:2855ms step_avg:34.82ms
step:83/1825 train_time:2888ms step_avg:34.80ms
step:84/1825 train_time:2924ms step_avg:34.81ms
step:85/1825 train_time:2957ms step_avg:34.79ms
step:86/1825 train_time:2993ms step_avg:34.80ms
step:87/1825 train_time:3026ms step_avg:34.78ms
step:88/1825 train_time:3061ms step_avg:34.79ms
step:89/1825 train_time:3094ms step_avg:34.77ms
step:90/1825 train_time:3129ms step_avg:34.77ms
step:91/1825 train_time:3163ms step_avg:34.75ms
step:92/1825 train_time:3198ms step_avg:34.76ms
step:93/1825 train_time:3231ms step_avg:34.74ms
step:94/1825 train_time:3267ms step_avg:34.75ms
step:95/1825 train_time:3300ms step_avg:34.73ms
step:96/1825 train_time:3335ms step_avg:34.74ms
step:97/1825 train_time:3368ms step_avg:34.72ms
step:98/1825 train_time:3404ms step_avg:34.73ms
step:99/1825 train_time:3437ms step_avg:34.72ms
step:100/1825 train_time:3472ms step_avg:34.72ms
step:101/1825 train_time:3505ms step_avg:34.71ms
step:102/1825 train_time:3541ms step_avg:34.72ms
step:103/1825 train_time:3574ms step_avg:34.70ms
step:104/1825 train_time:3610ms step_avg:34.71ms
step:105/1825 train_time:3643ms step_avg:34.69ms
step:106/1825 train_time:3678ms step_avg:34.70ms
step:107/1825 train_time:3711ms step_avg:34.68ms
step:108/1825 train_time:3746ms step_avg:34.69ms
step:109/1825 train_time:3780ms step_avg:34.67ms
step:110/1825 train_time:3815ms step_avg:34.68ms
step:111/1825 train_time:3848ms step_avg:34.67ms
step:112/1825 train_time:3883ms step_avg:34.67ms
step:113/1825 train_time:3917ms step_avg:34.66ms
step:114/1825 train_time:3952ms step_avg:34.67ms
step:115/1825 train_time:3985ms step_avg:34.65ms
step:116/1825 train_time:4020ms step_avg:34.66ms
step:117/1825 train_time:4054ms step_avg:34.65ms
step:118/1825 train_time:4089ms step_avg:34.65ms
step:119/1825 train_time:4122ms step_avg:34.64ms
step:120/1825 train_time:4158ms step_avg:34.65ms
step:121/1825 train_time:4191ms step_avg:34.64ms
step:122/1825 train_time:4227ms step_avg:34.65ms
step:123/1825 train_time:4260ms step_avg:34.63ms
step:124/1825 train_time:4295ms step_avg:34.64ms
step:125/1825 train_time:4328ms step_avg:34.63ms
step:126/1825 train_time:4363ms step_avg:34.63ms
step:127/1825 train_time:4396ms step_avg:34.62ms
step:128/1825 train_time:4432ms step_avg:34.62ms
step:129/1825 train_time:4465ms step_avg:34.61ms
step:130/1825 train_time:4500ms step_avg:34.62ms
step:131/1825 train_time:4533ms step_avg:34.60ms
step:132/1825 train_time:4568ms step_avg:34.61ms
step:133/1825 train_time:4601ms step_avg:34.60ms
step:134/1825 train_time:4637ms step_avg:34.60ms
step:135/1825 train_time:4670ms step_avg:34.59ms
step:136/1825 train_time:4705ms step_avg:34.59ms
step:137/1825 train_time:4738ms step_avg:34.58ms
step:138/1825 train_time:4773ms step_avg:34.59ms
step:139/1825 train_time:4806ms step_avg:34.57ms
step:140/1825 train_time:4841ms step_avg:34.58ms
step:141/1825 train_time:4874ms step_avg:34.57ms
step:142/1825 train_time:4909ms step_avg:34.57ms
step:143/1825 train_time:4943ms step_avg:34.56ms
step:144/1825 train_time:4978ms step_avg:34.57ms
step:145/1825 train_time:5011ms step_avg:34.56ms
step:146/1825 train_time:5046ms step_avg:34.56ms
step:147/1825 train_time:5079ms step_avg:34.55ms
step:148/1825 train_time:5115ms step_avg:34.56ms
step:149/1825 train_time:5148ms step_avg:34.55ms
step:150/1825 train_time:5183ms step_avg:34.55ms
step:151/1825 train_time:5216ms step_avg:34.54ms
step:152/1825 train_time:5251ms step_avg:34.55ms
step:153/1825 train_time:5284ms step_avg:34.54ms
step:154/1825 train_time:5320ms step_avg:34.54ms
step:155/1825 train_time:5353ms step_avg:34.53ms
step:156/1825 train_time:5388ms step_avg:34.54ms
step:157/1825 train_time:5421ms step_avg:34.53ms
step:158/1825 train_time:5456ms step_avg:34.53ms
step:159/1825 train_time:5489ms step_avg:34.52ms
step:160/1825 train_time:5524ms step_avg:34.53ms
step:161/1825 train_time:5557ms step_avg:34.52ms
step:162/1825 train_time:5593ms step_avg:34.52ms
step:163/1825 train_time:5626ms step_avg:34.51ms
step:164/1825 train_time:5661ms step_avg:34.52ms
step:165/1825 train_time:5694ms step_avg:34.51ms
step:166/1825 train_time:5729ms step_avg:34.51ms
step:167/1825 train_time:5762ms step_avg:34.51ms
step:168/1825 train_time:5798ms step_avg:34.51ms
step:169/1825 train_time:5831ms step_avg:34.50ms
step:170/1825 train_time:5866ms step_avg:34.51ms
step:171/1825 train_time:5899ms step_avg:34.50ms
step:172/1825 train_time:5934ms step_avg:34.50ms
step:173/1825 train_time:5967ms step_avg:34.49ms
step:174/1825 train_time:6002ms step_avg:34.50ms
step:175/1825 train_time:6035ms step_avg:34.49ms
step:176/1825 train_time:6071ms step_avg:34.49ms
step:177/1825 train_time:6104ms step_avg:34.48ms
step:178/1825 train_time:6139ms step_avg:34.49ms
step:179/1825 train_time:6172ms step_avg:34.48ms
step:180/1825 train_time:6207ms step_avg:34.48ms
step:181/1825 train_time:6240ms step_avg:34.47ms
step:182/1825 train_time:6275ms step_avg:34.48ms
step:183/1825 train_time:6308ms step_avg:34.47ms
step:184/1825 train_time:6343ms step_avg:34.48ms
step:185/1825 train_time:6376ms step_avg:34.47ms
step:186/1825 train_time:6412ms step_avg:34.47ms
step:187/1825 train_time:6445ms step_avg:34.46ms
step:188/1825 train_time:6480ms step_avg:34.47ms
step:189/1825 train_time:6513ms step_avg:34.46ms
step:190/1825 train_time:6548ms step_avg:34.46ms
step:191/1825 train_time:6581ms step_avg:34.46ms
step:192/1825 train_time:6616ms step_avg:34.46ms
step:193/1825 train_time:6649ms step_avg:34.45ms
step:194/1825 train_time:6685ms step_avg:34.46ms
step:195/1825 train_time:6718ms step_avg:34.45ms
step:196/1825 train_time:6753ms step_avg:34.45ms
step:197/1825 train_time:6786ms step_avg:34.45ms
step:198/1825 train_time:6821ms step_avg:34.45ms
step:199/1825 train_time:6854ms step_avg:34.44ms
step:200/1825 train_time:6889ms step_avg:34.45ms
step:201/1825 train_time:6922ms step_avg:34.44ms
step:202/1825 train_time:6958ms step_avg:34.44ms
step:203/1825 train_time:6991ms step_avg:34.44ms
step:204/1825 train_time:7026ms step_avg:34.44ms
step:205/1825 train_time:7059ms step_avg:34.43ms
step:206/1825 train_time:7094ms step_avg:34.44ms
step:207/1825 train_time:7127ms step_avg:34.43ms
step:208/1825 train_time:7162ms step_avg:34.43ms
step:209/1825 train_time:7195ms step_avg:34.43ms
step:210/1825 train_time:7231ms step_avg:34.43ms
step:211/1825 train_time:7264ms step_avg:34.43ms
step:212/1825 train_time:7299ms step_avg:34.43ms
step:213/1825 train_time:7332ms step_avg:34.42ms
step:214/1825 train_time:7367ms step_avg:34.43ms
step:215/1825 train_time:7400ms step_avg:34.42ms
step:216/1825 train_time:7435ms step_avg:34.42ms
step:217/1825 train_time:7468ms step_avg:34.42ms
step:218/1825 train_time:7504ms step_avg:34.42ms
step:219/1825 train_time:7537ms step_avg:34.41ms
step:220/1825 train_time:7572ms step_avg:34.42ms
step:221/1825 train_time:7605ms step_avg:34.41ms
step:222/1825 train_time:7641ms step_avg:34.42ms
step:223/1825 train_time:7674ms step_avg:34.41ms
step:224/1825 train_time:7709ms step_avg:34.41ms
step:225/1825 train_time:7742ms step_avg:34.41ms
step:226/1825 train_time:7777ms step_avg:34.41ms
step:227/1825 train_time:7810ms step_avg:34.41ms
step:228/1825 train_time:7845ms step_avg:34.41ms
step:229/1825 train_time:7878ms step_avg:34.40ms
step:230/1825 train_time:7914ms step_avg:34.41ms
step:231/1825 train_time:7947ms step_avg:34.40ms
step:232/1825 train_time:7982ms step_avg:34.41ms
step:233/1825 train_time:8015ms step_avg:34.40ms
step:234/1825 train_time:8051ms step_avg:34.40ms
step:235/1825 train_time:8083ms step_avg:34.40ms
step:236/1825 train_time:8119ms step_avg:34.40ms
step:237/1825 train_time:8152ms step_avg:34.40ms
step:238/1825 train_time:8187ms step_avg:34.40ms
step:239/1825 train_time:8220ms step_avg:34.39ms
step:240/1825 train_time:8255ms step_avg:34.40ms
step:241/1825 train_time:8288ms step_avg:34.39ms
step:242/1825 train_time:8324ms step_avg:34.40ms
step:243/1825 train_time:8357ms step_avg:34.39ms
step:244/1825 train_time:8392ms step_avg:34.39ms
step:245/1825 train_time:8425ms step_avg:34.39ms
step:246/1825 train_time:8460ms step_avg:34.39ms
step:247/1825 train_time:8493ms step_avg:34.39ms
step:248/1825 train_time:8528ms step_avg:34.39ms
step:249/1825 train_time:8561ms step_avg:34.38ms
step:250/1825 train_time:8597ms step_avg:34.39ms
step:250/1825 val_loss:4.6140 train_time:8638ms step_avg:34.55ms
step:251/1825 train_time:8656ms step_avg:34.49ms
step:252/1825 train_time:8674ms step_avg:34.42ms
step:253/1825 train_time:8700ms step_avg:34.39ms
step:254/1825 train_time:8737ms step_avg:34.40ms
step:255/1825 train_time:8770ms step_avg:34.39ms
step:256/1825 train_time:8808ms step_avg:34.41ms
step:257/1825 train_time:8842ms step_avg:34.40ms
step:258/1825 train_time:8878ms step_avg:34.41ms
step:259/1825 train_time:8911ms step_avg:34.40ms
step:260/1825 train_time:8946ms step_avg:34.41ms
step:261/1825 train_time:8979ms step_avg:34.40ms
step:262/1825 train_time:9014ms step_avg:34.41ms
step:263/1825 train_time:9047ms step_avg:34.40ms
step:264/1825 train_time:9082ms step_avg:34.40ms
step:265/1825 train_time:9115ms step_avg:34.40ms
step:266/1825 train_time:9151ms step_avg:34.40ms
step:267/1825 train_time:9184ms step_avg:34.40ms
step:268/1825 train_time:9219ms step_avg:34.40ms
step:269/1825 train_time:9252ms step_avg:34.39ms
step:270/1825 train_time:9287ms step_avg:34.40ms
step:271/1825 train_time:9320ms step_avg:34.39ms
step:272/1825 train_time:9355ms step_avg:34.39ms
step:273/1825 train_time:9388ms step_avg:34.39ms
step:274/1825 train_time:9423ms step_avg:34.39ms
step:275/1825 train_time:9456ms step_avg:34.39ms
step:276/1825 train_time:9492ms step_avg:34.39ms
step:277/1825 train_time:9524ms step_avg:34.38ms
step:278/1825 train_time:9560ms step_avg:34.39ms
step:279/1825 train_time:9593ms step_avg:34.38ms
step:280/1825 train_time:9628ms step_avg:34.39ms
step:281/1825 train_time:9661ms step_avg:34.38ms
step:282/1825 train_time:9696ms step_avg:34.38ms
step:283/1825 train_time:9729ms step_avg:34.38ms
step:284/1825 train_time:9764ms step_avg:34.38ms
step:285/1825 train_time:9797ms step_avg:34.38ms
step:286/1825 train_time:9833ms step_avg:34.38ms
step:287/1825 train_time:9866ms step_avg:34.38ms
step:288/1825 train_time:9901ms step_avg:34.38ms
step:289/1825 train_time:9934ms step_avg:34.37ms
step:290/1825 train_time:9969ms step_avg:34.38ms
step:291/1825 train_time:10002ms step_avg:34.37ms
step:292/1825 train_time:10038ms step_avg:34.38ms
step:293/1825 train_time:10071ms step_avg:34.37ms
step:294/1825 train_time:10107ms step_avg:34.38ms
step:295/1825 train_time:10140ms step_avg:34.37ms
step:296/1825 train_time:10175ms step_avg:34.37ms
step:297/1825 train_time:10208ms step_avg:34.37ms
step:298/1825 train_time:10243ms step_avg:34.37ms
step:299/1825 train_time:10276ms step_avg:34.37ms
step:300/1825 train_time:10311ms step_avg:34.37ms
step:301/1825 train_time:10344ms step_avg:34.37ms
step:302/1825 train_time:10379ms step_avg:34.37ms
step:303/1825 train_time:10412ms step_avg:34.36ms
step:304/1825 train_time:10448ms step_avg:34.37ms
step:305/1825 train_time:10480ms step_avg:34.36ms
step:306/1825 train_time:10516ms step_avg:34.37ms
step:307/1825 train_time:10549ms step_avg:34.36ms
step:308/1825 train_time:10584ms step_avg:34.36ms
step:309/1825 train_time:10617ms step_avg:34.36ms
step:310/1825 train_time:10652ms step_avg:34.36ms
step:311/1825 train_time:10685ms step_avg:34.36ms
step:312/1825 train_time:10721ms step_avg:34.36ms
step:313/1825 train_time:10753ms step_avg:34.36ms
step:314/1825 train_time:10789ms step_avg:34.36ms
step:315/1825 train_time:10821ms step_avg:34.35ms
step:316/1825 train_time:10857ms step_avg:34.36ms
step:317/1825 train_time:10890ms step_avg:34.35ms
step:318/1825 train_time:10925ms step_avg:34.36ms
step:319/1825 train_time:10958ms step_avg:34.35ms
step:320/1825 train_time:10993ms step_avg:34.35ms
step:321/1825 train_time:11026ms step_avg:34.35ms
step:322/1825 train_time:11061ms step_avg:34.35ms
step:323/1825 train_time:11094ms step_avg:34.35ms
step:324/1825 train_time:11129ms step_avg:34.35ms
step:325/1825 train_time:11162ms step_avg:34.35ms
step:326/1825 train_time:11198ms step_avg:34.35ms
step:327/1825 train_time:11231ms step_avg:34.34ms
step:328/1825 train_time:11266ms step_avg:34.35ms
step:329/1825 train_time:11299ms step_avg:34.34ms
step:330/1825 train_time:11334ms step_avg:34.35ms
step:331/1825 train_time:11367ms step_avg:34.34ms
step:332/1825 train_time:11402ms step_avg:34.34ms
step:333/1825 train_time:11435ms step_avg:34.34ms
step:334/1825 train_time:11471ms step_avg:34.34ms
step:335/1825 train_time:11504ms step_avg:34.34ms
step:336/1825 train_time:11539ms step_avg:34.34ms
step:337/1825 train_time:11572ms step_avg:34.34ms
step:338/1825 train_time:11607ms step_avg:34.34ms
step:339/1825 train_time:11640ms step_avg:34.34ms
step:340/1825 train_time:11675ms step_avg:34.34ms
step:341/1825 train_time:11708ms step_avg:34.33ms
step:342/1825 train_time:11744ms step_avg:34.34ms
step:343/1825 train_time:11776ms step_avg:34.33ms
step:344/1825 train_time:11812ms step_avg:34.34ms
step:345/1825 train_time:11845ms step_avg:34.33ms
step:346/1825 train_time:11880ms step_avg:34.34ms
step:347/1825 train_time:11913ms step_avg:34.33ms
step:348/1825 train_time:11949ms step_avg:34.33ms
step:349/1825 train_time:11981ms step_avg:34.33ms
step:350/1825 train_time:12017ms step_avg:34.33ms
step:351/1825 train_time:12050ms step_avg:34.33ms
step:352/1825 train_time:12085ms step_avg:34.33ms
step:353/1825 train_time:12118ms step_avg:34.33ms
step:354/1825 train_time:12153ms step_avg:34.33ms
step:355/1825 train_time:12186ms step_avg:34.33ms
step:356/1825 train_time:12221ms step_avg:34.33ms
step:357/1825 train_time:12254ms step_avg:34.33ms
step:358/1825 train_time:12289ms step_avg:34.33ms
step:359/1825 train_time:12322ms step_avg:34.32ms
step:360/1825 train_time:12358ms step_avg:34.33ms
step:361/1825 train_time:12391ms step_avg:34.32ms
step:362/1825 train_time:12426ms step_avg:34.33ms
step:363/1825 train_time:12459ms step_avg:34.32ms
step:364/1825 train_time:12494ms step_avg:34.33ms
step:365/1825 train_time:12527ms step_avg:34.32ms
step:366/1825 train_time:12563ms step_avg:34.32ms
step:367/1825 train_time:12596ms step_avg:34.32ms
step:368/1825 train_time:12631ms step_avg:34.32ms
step:369/1825 train_time:12664ms step_avg:34.32ms
step:370/1825 train_time:12699ms step_avg:34.32ms
step:371/1825 train_time:12732ms step_avg:34.32ms
step:372/1825 train_time:12767ms step_avg:34.32ms
step:373/1825 train_time:12800ms step_avg:34.32ms
step:374/1825 train_time:12836ms step_avg:34.32ms
step:375/1825 train_time:12869ms step_avg:34.32ms
step:376/1825 train_time:12904ms step_avg:34.32ms
step:377/1825 train_time:12937ms step_avg:34.32ms
step:378/1825 train_time:12972ms step_avg:34.32ms
step:379/1825 train_time:13005ms step_avg:34.31ms
step:380/1825 train_time:13040ms step_avg:34.32ms
step:381/1825 train_time:13073ms step_avg:34.31ms
step:382/1825 train_time:13109ms step_avg:34.32ms
step:383/1825 train_time:13141ms step_avg:34.31ms
step:384/1825 train_time:13177ms step_avg:34.31ms
step:385/1825 train_time:13210ms step_avg:34.31ms
step:386/1825 train_time:13245ms step_avg:34.31ms
step:387/1825 train_time:13278ms step_avg:34.31ms
step:388/1825 train_time:13313ms step_avg:34.31ms
step:389/1825 train_time:13346ms step_avg:34.31ms
step:390/1825 train_time:13381ms step_avg:34.31ms
step:391/1825 train_time:13414ms step_avg:34.31ms
step:392/1825 train_time:13449ms step_avg:34.31ms
step:393/1825 train_time:13482ms step_avg:34.31ms
step:394/1825 train_time:13517ms step_avg:34.31ms
step:395/1825 train_time:13550ms step_avg:34.30ms
step:396/1825 train_time:13585ms step_avg:34.31ms
step:397/1825 train_time:13619ms step_avg:34.30ms
step:398/1825 train_time:13654ms step_avg:34.31ms
step:399/1825 train_time:13687ms step_avg:34.30ms
step:400/1825 train_time:13722ms step_avg:34.30ms
step:401/1825 train_time:13755ms step_avg:34.30ms
step:402/1825 train_time:13790ms step_avg:34.30ms
step:403/1825 train_time:13823ms step_avg:34.30ms
step:404/1825 train_time:13858ms step_avg:34.30ms
step:405/1825 train_time:13891ms step_avg:34.30ms
step:406/1825 train_time:13927ms step_avg:34.30ms
step:407/1825 train_time:13960ms step_avg:34.30ms
step:408/1825 train_time:13995ms step_avg:34.30ms
step:409/1825 train_time:14028ms step_avg:34.30ms
step:410/1825 train_time:14063ms step_avg:34.30ms
step:411/1825 train_time:14096ms step_avg:34.30ms
step:412/1825 train_time:14131ms step_avg:34.30ms
step:413/1825 train_time:14164ms step_avg:34.30ms
step:414/1825 train_time:14200ms step_avg:34.30ms
step:415/1825 train_time:14232ms step_avg:34.30ms
step:416/1825 train_time:14268ms step_avg:34.30ms
step:417/1825 train_time:14301ms step_avg:34.29ms
step:418/1825 train_time:14336ms step_avg:34.30ms
step:419/1825 train_time:14369ms step_avg:34.29ms
step:420/1825 train_time:14404ms step_avg:34.30ms
step:421/1825 train_time:14437ms step_avg:34.29ms
step:422/1825 train_time:14472ms step_avg:34.30ms
step:423/1825 train_time:14505ms step_avg:34.29ms
step:424/1825 train_time:14541ms step_avg:34.29ms
step:425/1825 train_time:14574ms step_avg:34.29ms
step:426/1825 train_time:14609ms step_avg:34.29ms
step:427/1825 train_time:14642ms step_avg:34.29ms
step:428/1825 train_time:14678ms step_avg:34.29ms
step:429/1825 train_time:14711ms step_avg:34.29ms
step:430/1825 train_time:14746ms step_avg:34.29ms
step:431/1825 train_time:14779ms step_avg:34.29ms
step:432/1825 train_time:14814ms step_avg:34.29ms
step:433/1825 train_time:14847ms step_avg:34.29ms
step:434/1825 train_time:14882ms step_avg:34.29ms
step:435/1825 train_time:14915ms step_avg:34.29ms
step:436/1825 train_time:14950ms step_avg:34.29ms
step:437/1825 train_time:14983ms step_avg:34.29ms
step:438/1825 train_time:15019ms step_avg:34.29ms
step:439/1825 train_time:15051ms step_avg:34.29ms
step:440/1825 train_time:15087ms step_avg:34.29ms
step:441/1825 train_time:15119ms step_avg:34.28ms
step:442/1825 train_time:15155ms step_avg:34.29ms
step:443/1825 train_time:15188ms step_avg:34.28ms
step:444/1825 train_time:15223ms step_avg:34.29ms
step:445/1825 train_time:15256ms step_avg:34.28ms
step:446/1825 train_time:15291ms step_avg:34.28ms
step:447/1825 train_time:15324ms step_avg:34.28ms
step:448/1825 train_time:15359ms step_avg:34.28ms
step:449/1825 train_time:15392ms step_avg:34.28ms
step:450/1825 train_time:15427ms step_avg:34.28ms
step:451/1825 train_time:15460ms step_avg:34.28ms
step:452/1825 train_time:15495ms step_avg:34.28ms
step:453/1825 train_time:15528ms step_avg:34.28ms
step:454/1825 train_time:15563ms step_avg:34.28ms
step:455/1825 train_time:15596ms step_avg:34.28ms
step:456/1825 train_time:15632ms step_avg:34.28ms
step:457/1825 train_time:15664ms step_avg:34.28ms
step:458/1825 train_time:15700ms step_avg:34.28ms
step:459/1825 train_time:15733ms step_avg:34.28ms
step:460/1825 train_time:15768ms step_avg:34.28ms
step:461/1825 train_time:15801ms step_avg:34.27ms
step:462/1825 train_time:15836ms step_avg:34.28ms
step:463/1825 train_time:15869ms step_avg:34.27ms
step:464/1825 train_time:15904ms step_avg:34.28ms
step:465/1825 train_time:15937ms step_avg:34.27ms
step:466/1825 train_time:15973ms step_avg:34.28ms
step:467/1825 train_time:16005ms step_avg:34.27ms
step:468/1825 train_time:16041ms step_avg:34.28ms
step:469/1825 train_time:16074ms step_avg:34.27ms
step:470/1825 train_time:16109ms step_avg:34.27ms
step:471/1825 train_time:16142ms step_avg:34.27ms
step:472/1825 train_time:16177ms step_avg:34.27ms
step:473/1825 train_time:16210ms step_avg:34.27ms
step:474/1825 train_time:16245ms step_avg:34.27ms
step:475/1825 train_time:16278ms step_avg:34.27ms
step:476/1825 train_time:16314ms step_avg:34.27ms
step:477/1825 train_time:16346ms step_avg:34.27ms
step:478/1825 train_time:16382ms step_avg:34.27ms
step:479/1825 train_time:16414ms step_avg:34.27ms
step:480/1825 train_time:16449ms step_avg:34.27ms
step:481/1825 train_time:16482ms step_avg:34.27ms
step:482/1825 train_time:16517ms step_avg:34.27ms
step:483/1825 train_time:16550ms step_avg:34.27ms
step:484/1825 train_time:16586ms step_avg:34.27ms
step:485/1825 train_time:16619ms step_avg:34.27ms
step:486/1825 train_time:16654ms step_avg:34.27ms
step:487/1825 train_time:16687ms step_avg:34.26ms
step:488/1825 train_time:16722ms step_avg:34.27ms
step:489/1825 train_time:16755ms step_avg:34.26ms
step:490/1825 train_time:16790ms step_avg:34.27ms
step:491/1825 train_time:16823ms step_avg:34.26ms
step:492/1825 train_time:16858ms step_avg:34.26ms
step:493/1825 train_time:16891ms step_avg:34.26ms
step:494/1825 train_time:16926ms step_avg:34.26ms
step:495/1825 train_time:16959ms step_avg:34.26ms
step:496/1825 train_time:16994ms step_avg:34.26ms
step:497/1825 train_time:17027ms step_avg:34.26ms
step:498/1825 train_time:17063ms step_avg:34.26ms
step:499/1825 train_time:17096ms step_avg:34.26ms
step:500/1825 train_time:17131ms step_avg:34.26ms
step:500/1825 val_loss:4.2897 train_time:17173ms step_avg:34.35ms
step:501/1825 train_time:17190ms step_avg:34.31ms
step:502/1825 train_time:17207ms step_avg:34.28ms
step:503/1825 train_time:17235ms step_avg:34.26ms
step:504/1825 train_time:17270ms step_avg:34.27ms
step:505/1825 train_time:17305ms step_avg:34.27ms
step:506/1825 train_time:17342ms step_avg:34.27ms
step:507/1825 train_time:17376ms step_avg:34.27ms
step:508/1825 train_time:17412ms step_avg:34.28ms
step:509/1825 train_time:17445ms step_avg:34.27ms
step:510/1825 train_time:17481ms step_avg:34.28ms
step:511/1825 train_time:17514ms step_avg:34.27ms
step:512/1825 train_time:17549ms step_avg:34.27ms
step:513/1825 train_time:17582ms step_avg:34.27ms
step:514/1825 train_time:17617ms step_avg:34.27ms
step:515/1825 train_time:17650ms step_avg:34.27ms
step:516/1825 train_time:17685ms step_avg:34.27ms
step:517/1825 train_time:17718ms step_avg:34.27ms
step:518/1825 train_time:17753ms step_avg:34.27ms
step:519/1825 train_time:17786ms step_avg:34.27ms
step:520/1825 train_time:17821ms step_avg:34.27ms
step:521/1825 train_time:17854ms step_avg:34.27ms
step:522/1825 train_time:17889ms step_avg:34.27ms
step:523/1825 train_time:17922ms step_avg:34.27ms
step:524/1825 train_time:17957ms step_avg:34.27ms
step:525/1825 train_time:17990ms step_avg:34.27ms
step:526/1825 train_time:18026ms step_avg:34.27ms
step:527/1825 train_time:18058ms step_avg:34.27ms
step:528/1825 train_time:18094ms step_avg:34.27ms
step:529/1825 train_time:18127ms step_avg:34.27ms
step:530/1825 train_time:18162ms step_avg:34.27ms
step:531/1825 train_time:18195ms step_avg:34.27ms
step:532/1825 train_time:18230ms step_avg:34.27ms
step:533/1825 train_time:18263ms step_avg:34.26ms
step:534/1825 train_time:18298ms step_avg:34.27ms
step:535/1825 train_time:18331ms step_avg:34.26ms
step:536/1825 train_time:18367ms step_avg:34.27ms
step:537/1825 train_time:18400ms step_avg:34.26ms
step:538/1825 train_time:18435ms step_avg:34.27ms
step:539/1825 train_time:18468ms step_avg:34.26ms
step:540/1825 train_time:18503ms step_avg:34.27ms
step:541/1825 train_time:18536ms step_avg:34.26ms
step:542/1825 train_time:18572ms step_avg:34.27ms
step:543/1825 train_time:18605ms step_avg:34.26ms
step:544/1825 train_time:18640ms step_avg:34.27ms
step:545/1825 train_time:18673ms step_avg:34.26ms
step:546/1825 train_time:18708ms step_avg:34.26ms
step:547/1825 train_time:18741ms step_avg:34.26ms
step:548/1825 train_time:18777ms step_avg:34.26ms
step:549/1825 train_time:18810ms step_avg:34.26ms
step:550/1825 train_time:18845ms step_avg:34.26ms
step:551/1825 train_time:18878ms step_avg:34.26ms
step:552/1825 train_time:18913ms step_avg:34.26ms
step:553/1825 train_time:18946ms step_avg:34.26ms
step:554/1825 train_time:18982ms step_avg:34.26ms
step:555/1825 train_time:19014ms step_avg:34.26ms
step:556/1825 train_time:19050ms step_avg:34.26ms
step:557/1825 train_time:19083ms step_avg:34.26ms
step:558/1825 train_time:19118ms step_avg:34.26ms
step:559/1825 train_time:19151ms step_avg:34.26ms
step:560/1825 train_time:19186ms step_avg:34.26ms
step:561/1825 train_time:19219ms step_avg:34.26ms
step:562/1825 train_time:19254ms step_avg:34.26ms
step:563/1825 train_time:19287ms step_avg:34.26ms
step:564/1825 train_time:19322ms step_avg:34.26ms
step:565/1825 train_time:19355ms step_avg:34.26ms
step:566/1825 train_time:19390ms step_avg:34.26ms
step:567/1825 train_time:19423ms step_avg:34.26ms
step:568/1825 train_time:19459ms step_avg:34.26ms
step:569/1825 train_time:19492ms step_avg:34.26ms
step:570/1825 train_time:19527ms step_avg:34.26ms
step:571/1825 train_time:19560ms step_avg:34.26ms
step:572/1825 train_time:19596ms step_avg:34.26ms
step:573/1825 train_time:19629ms step_avg:34.26ms
step:574/1825 train_time:19664ms step_avg:34.26ms
step:575/1825 train_time:19698ms step_avg:34.26ms
step:576/1825 train_time:19733ms step_avg:34.26ms
step:577/1825 train_time:19766ms step_avg:34.26ms
step:578/1825 train_time:19801ms step_avg:34.26ms
step:579/1825 train_time:19834ms step_avg:34.26ms
step:580/1825 train_time:19869ms step_avg:34.26ms
step:581/1825 train_time:19902ms step_avg:34.26ms
step:582/1825 train_time:19939ms step_avg:34.26ms
step:583/1825 train_time:19970ms step_avg:34.25ms
step:584/1825 train_time:20006ms step_avg:34.26ms
step:585/1825 train_time:20039ms step_avg:34.25ms
step:586/1825 train_time:20074ms step_avg:34.26ms
step:587/1825 train_time:20107ms step_avg:34.25ms
step:588/1825 train_time:20142ms step_avg:34.26ms
step:589/1825 train_time:20175ms step_avg:34.25ms
step:590/1825 train_time:20210ms step_avg:34.25ms
step:591/1825 train_time:20243ms step_avg:34.25ms
step:592/1825 train_time:20278ms step_avg:34.25ms
step:593/1825 train_time:20311ms step_avg:34.25ms
step:594/1825 train_time:20346ms step_avg:34.25ms
step:595/1825 train_time:20379ms step_avg:34.25ms
step:596/1825 train_time:20416ms step_avg:34.26ms
step:597/1825 train_time:20474ms step_avg:34.29ms
step:598/1825 train_time:20536ms step_avg:34.34ms
step:599/1825 train_time:20596ms step_avg:34.38ms
step:600/1825 train_time:20659ms step_avg:34.43ms
step:601/1825 train_time:20719ms step_avg:34.47ms
step:602/1825 train_time:20782ms step_avg:34.52ms
step:603/1825 train_time:20843ms step_avg:34.56ms
step:604/1825 train_time:20905ms step_avg:34.61ms
step:605/1825 train_time:20965ms step_avg:34.65ms
step:606/1825 train_time:21028ms step_avg:34.70ms
step:607/1825 train_time:21089ms step_avg:34.74ms
step:608/1825 train_time:21151ms step_avg:34.79ms
step:609/1825 train_time:21212ms step_avg:34.83ms
step:610/1825 train_time:21276ms step_avg:34.88ms
step:611/1825 train_time:21336ms step_avg:34.92ms
step:612/1825 train_time:21399ms step_avg:34.97ms
step:613/1825 train_time:21459ms step_avg:35.01ms
step:614/1825 train_time:21522ms step_avg:35.05ms
step:615/1825 train_time:21582ms step_avg:35.09ms
step:616/1825 train_time:21645ms step_avg:35.14ms
step:617/1825 train_time:21705ms step_avg:35.18ms
step:618/1825 train_time:21767ms step_avg:35.22ms
step:619/1825 train_time:21827ms step_avg:35.26ms
step:620/1825 train_time:21890ms step_avg:35.31ms
step:621/1825 train_time:21950ms step_avg:35.35ms
step:622/1825 train_time:22014ms step_avg:35.39ms
step:623/1825 train_time:22073ms step_avg:35.43ms
step:624/1825 train_time:22137ms step_avg:35.48ms
step:625/1825 train_time:22198ms step_avg:35.52ms
step:626/1825 train_time:22261ms step_avg:35.56ms
step:627/1825 train_time:22321ms step_avg:35.60ms
step:628/1825 train_time:22384ms step_avg:35.64ms
step:629/1825 train_time:22444ms step_avg:35.68ms
step:630/1825 train_time:22506ms step_avg:35.72ms
step:631/1825 train_time:22566ms step_avg:35.76ms
step:632/1825 train_time:22629ms step_avg:35.81ms
step:633/1825 train_time:22690ms step_avg:35.85ms
step:634/1825 train_time:22753ms step_avg:35.89ms
step:635/1825 train_time:22812ms step_avg:35.92ms
step:636/1825 train_time:22876ms step_avg:35.97ms
step:637/1825 train_time:22936ms step_avg:36.01ms
step:638/1825 train_time:22999ms step_avg:36.05ms
step:639/1825 train_time:23060ms step_avg:36.09ms
step:640/1825 train_time:23124ms step_avg:36.13ms
step:641/1825 train_time:23184ms step_avg:36.17ms
step:642/1825 train_time:23246ms step_avg:36.21ms
step:643/1825 train_time:23307ms step_avg:36.25ms
step:644/1825 train_time:23370ms step_avg:36.29ms
step:645/1825 train_time:23431ms step_avg:36.33ms
step:646/1825 train_time:23494ms step_avg:36.37ms
step:647/1825 train_time:23554ms step_avg:36.40ms
step:648/1825 train_time:23617ms step_avg:36.45ms
step:649/1825 train_time:23678ms step_avg:36.48ms
step:650/1825 train_time:23742ms step_avg:36.53ms
step:651/1825 train_time:23802ms step_avg:36.56ms
step:652/1825 train_time:23864ms step_avg:36.60ms
step:653/1825 train_time:23925ms step_avg:36.64ms
step:654/1825 train_time:23987ms step_avg:36.68ms
step:655/1825 train_time:24047ms step_avg:36.71ms
step:656/1825 train_time:24110ms step_avg:36.75ms
step:657/1825 train_time:24171ms step_avg:36.79ms
step:658/1825 train_time:24234ms step_avg:36.83ms
step:659/1825 train_time:24294ms step_avg:36.87ms
step:660/1825 train_time:24357ms step_avg:36.90ms
step:661/1825 train_time:24418ms step_avg:36.94ms
step:662/1825 train_time:24481ms step_avg:36.98ms
step:663/1825 train_time:24541ms step_avg:37.01ms
step:664/1825 train_time:24604ms step_avg:37.05ms
step:665/1825 train_time:24664ms step_avg:37.09ms
step:666/1825 train_time:24726ms step_avg:37.13ms
step:667/1825 train_time:24787ms step_avg:37.16ms
step:668/1825 train_time:24850ms step_avg:37.20ms
step:669/1825 train_time:24910ms step_avg:37.23ms
step:670/1825 train_time:24973ms step_avg:37.27ms
step:671/1825 train_time:25033ms step_avg:37.31ms
step:672/1825 train_time:25096ms step_avg:37.35ms
step:673/1825 train_time:25157ms step_avg:37.38ms
step:674/1825 train_time:25220ms step_avg:37.42ms
step:675/1825 train_time:25280ms step_avg:37.45ms
step:676/1825 train_time:25343ms step_avg:37.49ms
step:677/1825 train_time:25403ms step_avg:37.52ms
step:678/1825 train_time:25465ms step_avg:37.56ms
step:679/1825 train_time:25525ms step_avg:37.59ms
step:680/1825 train_time:25588ms step_avg:37.63ms
step:681/1825 train_time:25648ms step_avg:37.66ms
step:682/1825 train_time:25711ms step_avg:37.70ms
step:683/1825 train_time:25772ms step_avg:37.73ms
step:684/1825 train_time:25836ms step_avg:37.77ms
step:685/1825 train_time:25895ms step_avg:37.80ms
step:686/1825 train_time:25958ms step_avg:37.84ms
step:687/1825 train_time:26019ms step_avg:37.87ms
step:688/1825 train_time:26081ms step_avg:37.91ms
step:689/1825 train_time:26142ms step_avg:37.94ms
step:690/1825 train_time:26205ms step_avg:37.98ms
step:691/1825 train_time:26265ms step_avg:38.01ms
step:692/1825 train_time:26327ms step_avg:38.05ms
step:693/1825 train_time:26387ms step_avg:38.08ms
step:694/1825 train_time:26451ms step_avg:38.11ms
step:695/1825 train_time:26511ms step_avg:38.15ms
step:696/1825 train_time:26574ms step_avg:38.18ms
step:697/1825 train_time:26635ms step_avg:38.21ms
step:698/1825 train_time:26697ms step_avg:38.25ms
step:699/1825 train_time:26758ms step_avg:38.28ms
step:700/1825 train_time:26821ms step_avg:38.32ms
step:701/1825 train_time:26881ms step_avg:38.35ms
step:702/1825 train_time:26943ms step_avg:38.38ms
step:703/1825 train_time:27003ms step_avg:38.41ms
step:704/1825 train_time:27066ms step_avg:38.45ms
step:705/1825 train_time:27126ms step_avg:38.48ms
step:706/1825 train_time:27189ms step_avg:38.51ms
step:707/1825 train_time:27249ms step_avg:38.54ms
step:708/1825 train_time:27313ms step_avg:38.58ms
step:709/1825 train_time:27373ms step_avg:38.61ms
step:710/1825 train_time:27436ms step_avg:38.64ms
step:711/1825 train_time:27496ms step_avg:38.67ms
step:712/1825 train_time:27559ms step_avg:38.71ms
step:713/1825 train_time:27619ms step_avg:38.74ms
step:714/1825 train_time:27681ms step_avg:38.77ms
step:715/1825 train_time:27742ms step_avg:38.80ms
step:716/1825 train_time:27805ms step_avg:38.83ms
step:717/1825 train_time:27864ms step_avg:38.86ms
step:718/1825 train_time:27927ms step_avg:38.90ms
step:719/1825 train_time:27988ms step_avg:38.93ms
step:720/1825 train_time:28051ms step_avg:38.96ms
step:721/1825 train_time:28112ms step_avg:38.99ms
step:722/1825 train_time:28176ms step_avg:39.02ms
step:723/1825 train_time:28236ms step_avg:39.05ms
step:724/1825 train_time:28299ms step_avg:39.09ms
step:725/1825 train_time:28360ms step_avg:39.12ms
step:726/1825 train_time:28422ms step_avg:39.15ms
step:727/1825 train_time:28483ms step_avg:39.18ms
step:728/1825 train_time:28546ms step_avg:39.21ms
step:729/1825 train_time:28607ms step_avg:39.24ms
step:730/1825 train_time:28669ms step_avg:39.27ms
step:731/1825 train_time:28729ms step_avg:39.30ms
step:732/1825 train_time:28792ms step_avg:39.33ms
step:733/1825 train_time:28852ms step_avg:39.36ms
step:734/1825 train_time:28915ms step_avg:39.39ms
step:735/1825 train_time:28975ms step_avg:39.42ms
step:736/1825 train_time:29039ms step_avg:39.45ms
step:737/1825 train_time:29099ms step_avg:39.48ms
step:738/1825 train_time:29163ms step_avg:39.52ms
step:739/1825 train_time:29222ms step_avg:39.54ms
step:740/1825 train_time:29286ms step_avg:39.58ms
step:741/1825 train_time:29345ms step_avg:39.60ms
step:742/1825 train_time:29408ms step_avg:39.63ms
step:743/1825 train_time:29468ms step_avg:39.66ms
step:744/1825 train_time:29532ms step_avg:39.69ms
step:745/1825 train_time:29591ms step_avg:39.72ms
step:746/1825 train_time:29655ms step_avg:39.75ms
step:747/1825 train_time:29715ms step_avg:39.78ms
step:748/1825 train_time:29778ms step_avg:39.81ms
step:749/1825 train_time:29838ms step_avg:39.84ms
step:750/1825 train_time:29901ms step_avg:39.87ms
step:750/1825 val_loss:4.0246 train_time:29970ms step_avg:39.96ms
step:751/1825 train_time:29988ms step_avg:39.93ms
step:752/1825 train_time:30025ms step_avg:39.93ms
step:753/1825 train_time:30087ms step_avg:39.96ms
step:754/1825 train_time:30151ms step_avg:39.99ms
step:755/1825 train_time:30211ms step_avg:40.01ms
step:756/1825 train_time:30275ms step_avg:40.05ms
step:757/1825 train_time:30335ms step_avg:40.07ms
step:758/1825 train_time:30399ms step_avg:40.10ms
step:759/1825 train_time:30458ms step_avg:40.13ms
step:760/1825 train_time:30521ms step_avg:40.16ms
step:761/1825 train_time:30581ms step_avg:40.19ms
step:762/1825 train_time:30643ms step_avg:40.21ms
step:763/1825 train_time:30703ms step_avg:40.24ms
step:764/1825 train_time:30765ms step_avg:40.27ms
step:765/1825 train_time:30825ms step_avg:40.29ms
step:766/1825 train_time:30888ms step_avg:40.32ms
step:767/1825 train_time:30948ms step_avg:40.35ms
step:768/1825 train_time:31011ms step_avg:40.38ms
step:769/1825 train_time:31072ms step_avg:40.41ms
step:770/1825 train_time:31137ms step_avg:40.44ms
step:771/1825 train_time:31197ms step_avg:40.46ms
step:772/1825 train_time:31260ms step_avg:40.49ms
step:773/1825 train_time:31320ms step_avg:40.52ms
step:774/1825 train_time:31383ms step_avg:40.55ms
step:775/1825 train_time:31443ms step_avg:40.57ms
step:776/1825 train_time:31505ms step_avg:40.60ms
step:777/1825 train_time:31565ms step_avg:40.62ms
step:778/1825 train_time:31628ms step_avg:40.65ms
step:779/1825 train_time:31688ms step_avg:40.68ms
step:780/1825 train_time:31751ms step_avg:40.71ms
step:781/1825 train_time:31811ms step_avg:40.73ms
step:782/1825 train_time:31874ms step_avg:40.76ms
step:783/1825 train_time:31934ms step_avg:40.78ms
step:784/1825 train_time:31998ms step_avg:40.81ms
step:785/1825 train_time:32058ms step_avg:40.84ms
step:786/1825 train_time:32121ms step_avg:40.87ms
step:787/1825 train_time:32181ms step_avg:40.89ms
step:788/1825 train_time:32244ms step_avg:40.92ms
step:789/1825 train_time:32305ms step_avg:40.94ms
step:790/1825 train_time:32367ms step_avg:40.97ms
step:791/1825 train_time:32427ms step_avg:41.00ms
step:792/1825 train_time:32490ms step_avg:41.02ms
step:793/1825 train_time:32550ms step_avg:41.05ms
step:794/1825 train_time:32612ms step_avg:41.07ms
step:795/1825 train_time:32672ms step_avg:41.10ms
step:796/1825 train_time:32736ms step_avg:41.13ms
step:797/1825 train_time:32795ms step_avg:41.15ms
step:798/1825 train_time:32859ms step_avg:41.18ms
step:799/1825 train_time:32920ms step_avg:41.20ms
step:800/1825 train_time:32983ms step_avg:41.23ms
step:801/1825 train_time:33043ms step_avg:41.25ms
step:802/1825 train_time:33106ms step_avg:41.28ms
step:803/1825 train_time:33167ms step_avg:41.30ms
step:804/1825 train_time:33229ms step_avg:41.33ms
step:805/1825 train_time:33290ms step_avg:41.35ms
step:806/1825 train_time:33353ms step_avg:41.38ms
step:807/1825 train_time:33413ms step_avg:41.40ms
step:808/1825 train_time:33477ms step_avg:41.43ms
step:809/1825 train_time:33537ms step_avg:41.45ms
step:810/1825 train_time:33600ms step_avg:41.48ms
step:811/1825 train_time:33661ms step_avg:41.51ms
step:812/1825 train_time:33723ms step_avg:41.53ms
step:813/1825 train_time:33784ms step_avg:41.55ms
step:814/1825 train_time:33846ms step_avg:41.58ms
step:815/1825 train_time:33906ms step_avg:41.60ms
step:816/1825 train_time:33969ms step_avg:41.63ms
step:817/1825 train_time:34028ms step_avg:41.65ms
step:818/1825 train_time:34091ms step_avg:41.68ms
step:819/1825 train_time:34152ms step_avg:41.70ms
step:820/1825 train_time:34215ms step_avg:41.73ms
step:821/1825 train_time:34276ms step_avg:41.75ms
step:822/1825 train_time:34339ms step_avg:41.77ms
step:823/1825 train_time:34400ms step_avg:41.80ms
step:824/1825 train_time:34463ms step_avg:41.82ms
step:825/1825 train_time:34524ms step_avg:41.85ms
step:826/1825 train_time:34586ms step_avg:41.87ms
step:827/1825 train_time:34646ms step_avg:41.89ms
step:828/1825 train_time:34708ms step_avg:41.92ms
step:829/1825 train_time:34769ms step_avg:41.94ms
step:830/1825 train_time:34832ms step_avg:41.97ms
step:831/1825 train_time:34893ms step_avg:41.99ms
step:832/1825 train_time:34955ms step_avg:42.01ms
step:833/1825 train_time:35015ms step_avg:42.04ms
step:834/1825 train_time:35080ms step_avg:42.06ms
step:835/1825 train_time:35140ms step_avg:42.08ms
step:836/1825 train_time:35202ms step_avg:42.11ms
step:837/1825 train_time:35262ms step_avg:42.13ms
step:838/1825 train_time:35325ms step_avg:42.15ms
step:839/1825 train_time:35386ms step_avg:42.18ms
step:840/1825 train_time:35448ms step_avg:42.20ms
step:841/1825 train_time:35508ms step_avg:42.22ms
step:842/1825 train_time:35571ms step_avg:42.25ms
step:843/1825 train_time:35631ms step_avg:42.27ms
step:844/1825 train_time:35694ms step_avg:42.29ms
step:845/1825 train_time:35754ms step_avg:42.31ms
step:846/1825 train_time:35818ms step_avg:42.34ms
step:847/1825 train_time:35878ms step_avg:42.36ms
step:848/1825 train_time:35941ms step_avg:42.38ms
step:849/1825 train_time:36001ms step_avg:42.40ms
step:850/1825 train_time:36064ms step_avg:42.43ms
step:851/1825 train_time:36124ms step_avg:42.45ms
step:852/1825 train_time:36186ms step_avg:42.47ms
step:853/1825 train_time:36246ms step_avg:42.49ms
step:854/1825 train_time:36308ms step_avg:42.52ms
step:855/1825 train_time:36369ms step_avg:42.54ms
step:856/1825 train_time:36432ms step_avg:42.56ms
step:857/1825 train_time:36492ms step_avg:42.58ms
step:858/1825 train_time:36556ms step_avg:42.61ms
step:859/1825 train_time:36616ms step_avg:42.63ms
step:860/1825 train_time:36679ms step_avg:42.65ms
step:861/1825 train_time:36740ms step_avg:42.67ms
step:862/1825 train_time:36802ms step_avg:42.69ms
step:863/1825 train_time:36862ms step_avg:42.71ms
step:864/1825 train_time:36925ms step_avg:42.74ms
step:865/1825 train_time:36986ms step_avg:42.76ms
step:866/1825 train_time:37049ms step_avg:42.78ms
step:867/1825 train_time:37108ms step_avg:42.80ms
step:868/1825 train_time:37172ms step_avg:42.82ms
step:869/1825 train_time:37232ms step_avg:42.84ms
step:870/1825 train_time:37296ms step_avg:42.87ms
step:871/1825 train_time:37356ms step_avg:42.89ms
step:872/1825 train_time:37420ms step_avg:42.91ms
step:873/1825 train_time:37481ms step_avg:42.93ms
step:874/1825 train_time:37544ms step_avg:42.96ms
step:875/1825 train_time:37605ms step_avg:42.98ms
step:876/1825 train_time:37668ms step_avg:43.00ms
step:877/1825 train_time:37728ms step_avg:43.02ms
step:878/1825 train_time:37790ms step_avg:43.04ms
step:879/1825 train_time:37851ms step_avg:43.06ms
step:880/1825 train_time:37914ms step_avg:43.08ms
step:881/1825 train_time:37974ms step_avg:43.10ms
step:882/1825 train_time:38037ms step_avg:43.13ms
step:883/1825 train_time:38097ms step_avg:43.15ms
step:884/1825 train_time:38160ms step_avg:43.17ms
step:885/1825 train_time:38221ms step_avg:43.19ms
step:886/1825 train_time:38284ms step_avg:43.21ms
step:887/1825 train_time:38344ms step_avg:43.23ms
step:888/1825 train_time:38407ms step_avg:43.25ms
step:889/1825 train_time:38467ms step_avg:43.27ms
step:890/1825 train_time:38529ms step_avg:43.29ms
step:891/1825 train_time:38589ms step_avg:43.31ms
step:892/1825 train_time:38654ms step_avg:43.33ms
step:893/1825 train_time:38715ms step_avg:43.35ms
step:894/1825 train_time:38779ms step_avg:43.38ms
step:895/1825 train_time:38840ms step_avg:43.40ms
step:896/1825 train_time:38902ms step_avg:43.42ms
step:897/1825 train_time:38962ms step_avg:43.44ms
step:898/1825 train_time:39026ms step_avg:43.46ms
step:899/1825 train_time:39086ms step_avg:43.48ms
step:900/1825 train_time:39148ms step_avg:43.50ms
step:901/1825 train_time:39208ms step_avg:43.52ms
step:902/1825 train_time:39271ms step_avg:43.54ms
step:903/1825 train_time:39331ms step_avg:43.56ms
step:904/1825 train_time:39395ms step_avg:43.58ms
step:905/1825 train_time:39455ms step_avg:43.60ms
step:906/1825 train_time:39518ms step_avg:43.62ms
step:907/1825 train_time:39579ms step_avg:43.64ms
step:908/1825 train_time:39642ms step_avg:43.66ms
step:909/1825 train_time:39703ms step_avg:43.68ms
step:910/1825 train_time:39766ms step_avg:43.70ms
step:911/1825 train_time:39826ms step_avg:43.72ms
step:912/1825 train_time:39889ms step_avg:43.74ms
step:913/1825 train_time:39949ms step_avg:43.76ms
step:914/1825 train_time:40012ms step_avg:43.78ms
step:915/1825 train_time:40072ms step_avg:43.79ms
step:916/1825 train_time:40134ms step_avg:43.81ms
step:917/1825 train_time:40194ms step_avg:43.83ms
step:918/1825 train_time:40258ms step_avg:43.85ms
step:919/1825 train_time:40318ms step_avg:43.87ms
step:920/1825 train_time:40381ms step_avg:43.89ms
step:921/1825 train_time:40441ms step_avg:43.91ms
step:922/1825 train_time:40504ms step_avg:43.93ms
step:923/1825 train_time:40564ms step_avg:43.95ms
step:924/1825 train_time:40626ms step_avg:43.97ms
step:925/1825 train_time:40686ms step_avg:43.98ms
step:926/1825 train_time:40748ms step_avg:44.00ms
step:927/1825 train_time:40809ms step_avg:44.02ms
step:928/1825 train_time:40872ms step_avg:44.04ms
step:929/1825 train_time:40933ms step_avg:44.06ms
step:930/1825 train_time:40996ms step_avg:44.08ms
step:931/1825 train_time:41057ms step_avg:44.10ms
step:932/1825 train_time:41119ms step_avg:44.12ms
step:933/1825 train_time:41180ms step_avg:44.14ms
step:934/1825 train_time:41242ms step_avg:44.16ms
step:935/1825 train_time:41302ms step_avg:44.17ms
step:936/1825 train_time:41365ms step_avg:44.19ms
step:937/1825 train_time:41425ms step_avg:44.21ms
step:938/1825 train_time:41488ms step_avg:44.23ms
step:939/1825 train_time:41548ms step_avg:44.25ms
step:940/1825 train_time:41611ms step_avg:44.27ms
step:941/1825 train_time:41671ms step_avg:44.28ms
step:942/1825 train_time:41734ms step_avg:44.30ms
step:943/1825 train_time:41794ms step_avg:44.32ms
step:944/1825 train_time:41858ms step_avg:44.34ms
step:945/1825 train_time:41918ms step_avg:44.36ms
step:946/1825 train_time:41981ms step_avg:44.38ms
step:947/1825 train_time:42041ms step_avg:44.39ms
step:948/1825 train_time:42104ms step_avg:44.41ms
step:949/1825 train_time:42164ms step_avg:44.43ms
step:950/1825 train_time:42226ms step_avg:44.45ms
step:951/1825 train_time:42286ms step_avg:44.46ms
step:952/1825 train_time:42349ms step_avg:44.48ms
step:953/1825 train_time:42409ms step_avg:44.50ms
step:954/1825 train_time:42473ms step_avg:44.52ms
step:955/1825 train_time:42533ms step_avg:44.54ms
step:956/1825 train_time:42596ms step_avg:44.56ms
step:957/1825 train_time:42656ms step_avg:44.57ms
step:958/1825 train_time:42719ms step_avg:44.59ms
step:959/1825 train_time:42780ms step_avg:44.61ms
step:960/1825 train_time:42843ms step_avg:44.63ms
step:961/1825 train_time:42904ms step_avg:44.65ms
step:962/1825 train_time:42967ms step_avg:44.66ms
step:963/1825 train_time:43027ms step_avg:44.68ms
step:964/1825 train_time:43089ms step_avg:44.70ms
step:965/1825 train_time:43149ms step_avg:44.71ms
step:966/1825 train_time:43212ms step_avg:44.73ms
step:967/1825 train_time:43273ms step_avg:44.75ms
step:968/1825 train_time:43336ms step_avg:44.77ms
step:969/1825 train_time:43396ms step_avg:44.78ms
step:970/1825 train_time:43459ms step_avg:44.80ms
step:971/1825 train_time:43519ms step_avg:44.82ms
step:972/1825 train_time:43582ms step_avg:44.84ms
step:973/1825 train_time:43643ms step_avg:44.85ms
step:974/1825 train_time:43706ms step_avg:44.87ms
step:975/1825 train_time:43766ms step_avg:44.89ms
step:976/1825 train_time:43828ms step_avg:44.91ms
step:977/1825 train_time:43889ms step_avg:44.92ms
step:978/1825 train_time:43951ms step_avg:44.94ms
step:979/1825 train_time:44012ms step_avg:44.96ms
step:980/1825 train_time:44075ms step_avg:44.97ms
step:981/1825 train_time:44135ms step_avg:44.99ms
step:982/1825 train_time:44198ms step_avg:45.01ms
step:983/1825 train_time:44258ms step_avg:45.02ms
step:984/1825 train_time:44321ms step_avg:45.04ms
step:985/1825 train_time:44381ms step_avg:45.06ms
step:986/1825 train_time:44443ms step_avg:45.07ms
step:987/1825 train_time:44504ms step_avg:45.09ms
step:988/1825 train_time:44567ms step_avg:45.11ms
step:989/1825 train_time:44627ms step_avg:45.12ms
step:990/1825 train_time:44690ms step_avg:45.14ms
step:991/1825 train_time:44750ms step_avg:45.16ms
step:992/1825 train_time:44813ms step_avg:45.17ms
step:993/1825 train_time:44873ms step_avg:45.19ms
step:994/1825 train_time:44936ms step_avg:45.21ms
step:995/1825 train_time:44996ms step_avg:45.22ms
step:996/1825 train_time:45059ms step_avg:45.24ms
step:997/1825 train_time:45120ms step_avg:45.26ms
step:998/1825 train_time:45183ms step_avg:45.27ms
step:999/1825 train_time:45243ms step_avg:45.29ms
step:1000/1825 train_time:45306ms step_avg:45.31ms
step:1000/1825 val_loss:3.7659 train_time:45376ms step_avg:45.38ms
step:1001/1825 train_time:45393ms step_avg:45.35ms
step:1002/1825 train_time:45430ms step_avg:45.34ms
step:1003/1825 train_time:45493ms step_avg:45.36ms
step:1004/1825 train_time:45560ms step_avg:45.38ms
step:1005/1825 train_time:45624ms step_avg:45.40ms
step:1006/1825 train_time:45687ms step_avg:45.41ms
step:1007/1825 train_time:45747ms step_avg:45.43ms
step:1008/1825 train_time:45809ms step_avg:45.45ms
step:1009/1825 train_time:45869ms step_avg:45.46ms
step:1010/1825 train_time:45931ms step_avg:45.48ms
step:1011/1825 train_time:45991ms step_avg:45.49ms
step:1012/1825 train_time:46053ms step_avg:45.51ms
step:1013/1825 train_time:46113ms step_avg:45.52ms
step:1014/1825 train_time:46176ms step_avg:45.54ms
step:1015/1825 train_time:46236ms step_avg:45.55ms
step:1016/1825 train_time:46298ms step_avg:45.57ms
step:1017/1825 train_time:46358ms step_avg:45.58ms
step:1018/1825 train_time:46423ms step_avg:45.60ms
step:1019/1825 train_time:46484ms step_avg:45.62ms
step:1020/1825 train_time:46548ms step_avg:45.64ms
step:1021/1825 train_time:46608ms step_avg:45.65ms
step:1022/1825 train_time:46671ms step_avg:45.67ms
step:1023/1825 train_time:46732ms step_avg:45.68ms
step:1024/1825 train_time:46796ms step_avg:45.70ms
step:1025/1825 train_time:46855ms step_avg:45.71ms
step:1026/1825 train_time:46919ms step_avg:45.73ms
step:1027/1825 train_time:46979ms step_avg:45.74ms
step:1028/1825 train_time:47042ms step_avg:45.76ms
step:1029/1825 train_time:47102ms step_avg:45.77ms
step:1030/1825 train_time:47164ms step_avg:45.79ms
step:1031/1825 train_time:47224ms step_avg:45.80ms
step:1032/1825 train_time:47288ms step_avg:45.82ms
step:1033/1825 train_time:47348ms step_avg:45.84ms
step:1034/1825 train_time:47411ms step_avg:45.85ms
step:1035/1825 train_time:47472ms step_avg:45.87ms
step:1036/1825 train_time:47534ms step_avg:45.88ms
step:1037/1825 train_time:47595ms step_avg:45.90ms
step:1038/1825 train_time:47659ms step_avg:45.91ms
step:1039/1825 train_time:47719ms step_avg:45.93ms
step:1040/1825 train_time:47782ms step_avg:45.94ms
step:1041/1825 train_time:47843ms step_avg:45.96ms
step:1042/1825 train_time:47905ms step_avg:45.97ms
step:1043/1825 train_time:47965ms step_avg:45.99ms
step:1044/1825 train_time:48028ms step_avg:46.00ms
step:1045/1825 train_time:48087ms step_avg:46.02ms
step:1046/1825 train_time:48150ms step_avg:46.03ms
step:1047/1825 train_time:48210ms step_avg:46.05ms
step:1048/1825 train_time:48273ms step_avg:46.06ms
step:1049/1825 train_time:48333ms step_avg:46.08ms
step:1050/1825 train_time:48396ms step_avg:46.09ms
step:1051/1825 train_time:48457ms step_avg:46.11ms
step:1052/1825 train_time:48521ms step_avg:46.12ms
step:1053/1825 train_time:48581ms step_avg:46.14ms
step:1054/1825 train_time:48644ms step_avg:46.15ms
step:1055/1825 train_time:48704ms step_avg:46.16ms
step:1056/1825 train_time:48767ms step_avg:46.18ms
step:1057/1825 train_time:48827ms step_avg:46.19ms
step:1058/1825 train_time:48890ms step_avg:46.21ms
step:1059/1825 train_time:48950ms step_avg:46.22ms
step:1060/1825 train_time:49013ms step_avg:46.24ms
step:1061/1825 train_time:49073ms step_avg:46.25ms
step:1062/1825 train_time:49135ms step_avg:46.27ms
step:1063/1825 train_time:49196ms step_avg:46.28ms
step:1064/1825 train_time:49259ms step_avg:46.30ms
step:1065/1825 train_time:49319ms step_avg:46.31ms
step:1066/1825 train_time:49381ms step_avg:46.32ms
step:1067/1825 train_time:49442ms step_avg:46.34ms
step:1068/1825 train_time:49505ms step_avg:46.35ms
step:1069/1825 train_time:49565ms step_avg:46.37ms
step:1070/1825 train_time:49628ms step_avg:46.38ms
step:1071/1825 train_time:49688ms step_avg:46.39ms
step:1072/1825 train_time:49750ms step_avg:46.41ms
step:1073/1825 train_time:49811ms step_avg:46.42ms
step:1074/1825 train_time:49874ms step_avg:46.44ms
step:1075/1825 train_time:49935ms step_avg:46.45ms
step:1076/1825 train_time:49998ms step_avg:46.47ms
step:1077/1825 train_time:50057ms step_avg:46.48ms
step:1078/1825 train_time:50121ms step_avg:46.49ms
step:1079/1825 train_time:50181ms step_avg:46.51ms
step:1080/1825 train_time:50244ms step_avg:46.52ms
step:1081/1825 train_time:50304ms step_avg:46.53ms
step:1082/1825 train_time:50366ms step_avg:46.55ms
step:1083/1825 train_time:50427ms step_avg:46.56ms
step:1084/1825 train_time:50489ms step_avg:46.58ms
step:1085/1825 train_time:50549ms step_avg:46.59ms
step:1086/1825 train_time:50612ms step_avg:46.60ms
step:1087/1825 train_time:50672ms step_avg:46.62ms
step:1088/1825 train_time:50736ms step_avg:46.63ms
step:1089/1825 train_time:50796ms step_avg:46.64ms
step:1090/1825 train_time:50860ms step_avg:46.66ms
step:1091/1825 train_time:50920ms step_avg:46.67ms
step:1092/1825 train_time:50983ms step_avg:46.69ms
step:1093/1825 train_time:51043ms step_avg:46.70ms
step:1094/1825 train_time:51106ms step_avg:46.71ms
step:1095/1825 train_time:51167ms step_avg:46.73ms
step:1096/1825 train_time:51230ms step_avg:46.74ms
step:1097/1825 train_time:51289ms step_avg:46.75ms
step:1098/1825 train_time:51352ms step_avg:46.77ms
step:1099/1825 train_time:51412ms step_avg:46.78ms
step:1100/1825 train_time:51476ms step_avg:46.80ms
step:1101/1825 train_time:51537ms step_avg:46.81ms
step:1102/1825 train_time:51601ms step_avg:46.82ms
step:1103/1825 train_time:51661ms step_avg:46.84ms
step:1104/1825 train_time:51724ms step_avg:46.85ms
step:1105/1825 train_time:51784ms step_avg:46.86ms
step:1106/1825 train_time:51848ms step_avg:46.88ms
step:1107/1825 train_time:51908ms step_avg:46.89ms
step:1108/1825 train_time:51971ms step_avg:46.90ms
step:1109/1825 train_time:52030ms step_avg:46.92ms
step:1110/1825 train_time:52094ms step_avg:46.93ms
step:1111/1825 train_time:52155ms step_avg:46.94ms
step:1112/1825 train_time:52218ms step_avg:46.96ms
step:1113/1825 train_time:52278ms step_avg:46.97ms
step:1114/1825 train_time:52341ms step_avg:46.98ms
step:1115/1825 train_time:52402ms step_avg:47.00ms
step:1116/1825 train_time:52465ms step_avg:47.01ms
step:1117/1825 train_time:52526ms step_avg:47.02ms
step:1118/1825 train_time:52588ms step_avg:47.04ms
step:1119/1825 train_time:52648ms step_avg:47.05ms
step:1120/1825 train_time:52711ms step_avg:47.06ms
step:1121/1825 train_time:52770ms step_avg:47.07ms
step:1122/1825 train_time:52834ms step_avg:47.09ms
step:1123/1825 train_time:52895ms step_avg:47.10ms
step:1124/1825 train_time:52958ms step_avg:47.12ms
step:1125/1825 train_time:53018ms step_avg:47.13ms
step:1126/1825 train_time:53081ms step_avg:47.14ms
step:1127/1825 train_time:53141ms step_avg:47.15ms
step:1128/1825 train_time:53204ms step_avg:47.17ms
step:1129/1825 train_time:53265ms step_avg:47.18ms
step:1130/1825 train_time:53327ms step_avg:47.19ms
step:1131/1825 train_time:53386ms step_avg:47.20ms
step:1132/1825 train_time:53449ms step_avg:47.22ms
step:1133/1825 train_time:53510ms step_avg:47.23ms
step:1134/1825 train_time:53572ms step_avg:47.24ms
step:1135/1825 train_time:53632ms step_avg:47.25ms
step:1136/1825 train_time:53696ms step_avg:47.27ms
step:1137/1825 train_time:53756ms step_avg:47.28ms
step:1138/1825 train_time:53820ms step_avg:47.29ms
step:1139/1825 train_time:53880ms step_avg:47.30ms
step:1140/1825 train_time:53943ms step_avg:47.32ms
step:1141/1825 train_time:54003ms step_avg:47.33ms
step:1142/1825 train_time:54066ms step_avg:47.34ms
step:1143/1825 train_time:54126ms step_avg:47.35ms
step:1144/1825 train_time:54189ms step_avg:47.37ms
step:1145/1825 train_time:54248ms step_avg:47.38ms
step:1146/1825 train_time:54311ms step_avg:47.39ms
step:1147/1825 train_time:54371ms step_avg:47.40ms
step:1148/1825 train_time:54434ms step_avg:47.42ms
step:1149/1825 train_time:54494ms step_avg:47.43ms
step:1150/1825 train_time:54558ms step_avg:47.44ms
step:1151/1825 train_time:54617ms step_avg:47.45ms
step:1152/1825 train_time:54680ms step_avg:47.47ms
step:1153/1825 train_time:54742ms step_avg:47.48ms
step:1154/1825 train_time:54805ms step_avg:47.49ms
step:1155/1825 train_time:54865ms step_avg:47.50ms
step:1156/1825 train_time:54928ms step_avg:47.52ms
step:1157/1825 train_time:54988ms step_avg:47.53ms
step:1158/1825 train_time:55050ms step_avg:47.54ms
step:1159/1825 train_time:55111ms step_avg:47.55ms
step:1160/1825 train_time:55174ms step_avg:47.56ms
step:1161/1825 train_time:55234ms step_avg:47.57ms
step:1162/1825 train_time:55298ms step_avg:47.59ms
step:1163/1825 train_time:55357ms step_avg:47.60ms
step:1164/1825 train_time:55421ms step_avg:47.61ms
step:1165/1825 train_time:55482ms step_avg:47.62ms
step:1166/1825 train_time:55545ms step_avg:47.64ms
step:1167/1825 train_time:55605ms step_avg:47.65ms
step:1168/1825 train_time:55667ms step_avg:47.66ms
step:1169/1825 train_time:55727ms step_avg:47.67ms
step:1170/1825 train_time:55790ms step_avg:47.68ms
step:1171/1825 train_time:55851ms step_avg:47.69ms
step:1172/1825 train_time:55913ms step_avg:47.71ms
step:1173/1825 train_time:55974ms step_avg:47.72ms
step:1174/1825 train_time:56037ms step_avg:47.73ms
step:1175/1825 train_time:56097ms step_avg:47.74ms
step:1176/1825 train_time:56160ms step_avg:47.76ms
step:1177/1825 train_time:56221ms step_avg:47.77ms
step:1178/1825 train_time:56284ms step_avg:47.78ms
step:1179/1825 train_time:56344ms step_avg:47.79ms
step:1180/1825 train_time:56406ms step_avg:47.80ms
step:1181/1825 train_time:56466ms step_avg:47.81ms
step:1182/1825 train_time:56528ms step_avg:47.82ms
step:1183/1825 train_time:56588ms step_avg:47.83ms
step:1184/1825 train_time:56651ms step_avg:47.85ms
step:1185/1825 train_time:56712ms step_avg:47.86ms
step:1186/1825 train_time:56774ms step_avg:47.87ms
step:1187/1825 train_time:56834ms step_avg:47.88ms
step:1188/1825 train_time:56898ms step_avg:47.89ms
step:1189/1825 train_time:56957ms step_avg:47.90ms
step:1190/1825 train_time:57021ms step_avg:47.92ms
step:1191/1825 train_time:57083ms step_avg:47.93ms
step:1192/1825 train_time:57170ms step_avg:47.96ms
step:1193/1825 train_time:57257ms step_avg:47.99ms
step:1194/1825 train_time:57345ms step_avg:48.03ms
step:1195/1825 train_time:57432ms step_avg:48.06ms
step:1196/1825 train_time:57521ms step_avg:48.09ms
step:1197/1825 train_time:57607ms step_avg:48.13ms
step:1198/1825 train_time:57697ms step_avg:48.16ms
step:1199/1825 train_time:57784ms step_avg:48.19ms
step:1200/1825 train_time:57873ms step_avg:48.23ms
step:1201/1825 train_time:57961ms step_avg:48.26ms
step:1202/1825 train_time:58050ms step_avg:48.29ms
step:1203/1825 train_time:58137ms step_avg:48.33ms
step:1204/1825 train_time:58225ms step_avg:48.36ms
step:1205/1825 train_time:58313ms step_avg:48.39ms
step:1206/1825 train_time:58401ms step_avg:48.43ms
step:1207/1825 train_time:58487ms step_avg:48.46ms
step:1208/1825 train_time:58578ms step_avg:48.49ms
step:1209/1825 train_time:58664ms step_avg:48.52ms
step:1210/1825 train_time:58754ms step_avg:48.56ms
step:1211/1825 train_time:58842ms step_avg:48.59ms
step:1212/1825 train_time:58931ms step_avg:48.62ms
step:1213/1825 train_time:59018ms step_avg:48.65ms
step:1214/1825 train_time:59106ms step_avg:48.69ms
step:1215/1825 train_time:59193ms step_avg:48.72ms
step:1216/1825 train_time:59283ms step_avg:48.75ms
step:1217/1825 train_time:59369ms step_avg:48.78ms
step:1218/1825 train_time:59459ms step_avg:48.82ms
step:1219/1825 train_time:59545ms step_avg:48.85ms
step:1220/1825 train_time:59634ms step_avg:48.88ms
step:1221/1825 train_time:59720ms step_avg:48.91ms
step:1222/1825 train_time:59812ms step_avg:48.95ms
step:1223/1825 train_time:59899ms step_avg:48.98ms
step:1224/1825 train_time:59987ms step_avg:49.01ms
step:1225/1825 train_time:60073ms step_avg:49.04ms
step:1226/1825 train_time:60162ms step_avg:49.07ms
step:1227/1825 train_time:60250ms step_avg:49.10ms
step:1228/1825 train_time:60340ms step_avg:49.14ms
step:1229/1825 train_time:60426ms step_avg:49.17ms
step:1230/1825 train_time:60515ms step_avg:49.20ms
step:1231/1825 train_time:60600ms step_avg:49.23ms
step:1232/1825 train_time:60689ms step_avg:49.26ms
step:1233/1825 train_time:60776ms step_avg:49.29ms
step:1234/1825 train_time:60865ms step_avg:49.32ms
step:1235/1825 train_time:60953ms step_avg:49.35ms
step:1236/1825 train_time:61043ms step_avg:49.39ms
step:1237/1825 train_time:61128ms step_avg:49.42ms
step:1238/1825 train_time:61219ms step_avg:49.45ms
step:1239/1825 train_time:61307ms step_avg:49.48ms
step:1240/1825 train_time:61398ms step_avg:49.51ms
step:1241/1825 train_time:61484ms step_avg:49.54ms
step:1242/1825 train_time:61573ms step_avg:49.58ms
step:1243/1825 train_time:61660ms step_avg:49.61ms
step:1244/1825 train_time:61749ms step_avg:49.64ms
step:1245/1825 train_time:61835ms step_avg:49.67ms
step:1246/1825 train_time:61923ms step_avg:49.70ms
step:1247/1825 train_time:62011ms step_avg:49.73ms
step:1248/1825 train_time:62101ms step_avg:49.76ms
step:1249/1825 train_time:62189ms step_avg:49.79ms
step:1250/1825 train_time:62280ms step_avg:49.82ms
step:1250/1825 val_loss:3.5249 train_time:62377ms step_avg:49.90ms
step:1251/1825 train_time:62395ms step_avg:49.88ms
step:1252/1825 train_time:62457ms step_avg:49.89ms
step:1253/1825 train_time:62549ms step_avg:49.92ms
step:1254/1825 train_time:62638ms step_avg:49.95ms
step:1255/1825 train_time:62725ms step_avg:49.98ms
step:1256/1825 train_time:62814ms step_avg:50.01ms
step:1257/1825 train_time:62900ms step_avg:50.04ms
step:1258/1825 train_time:62990ms step_avg:50.07ms
step:1259/1825 train_time:63075ms step_avg:50.10ms
step:1260/1825 train_time:63163ms step_avg:50.13ms
step:1261/1825 train_time:63251ms step_avg:50.16ms
step:1262/1825 train_time:63340ms step_avg:50.19ms
step:1263/1825 train_time:63428ms step_avg:50.22ms
step:1264/1825 train_time:63519ms step_avg:50.25ms
step:1265/1825 train_time:63607ms step_avg:50.28ms
step:1266/1825 train_time:63697ms step_avg:50.31ms
step:1267/1825 train_time:63783ms step_avg:50.34ms
step:1268/1825 train_time:63873ms step_avg:50.37ms
step:1269/1825 train_time:63958ms step_avg:50.40ms
step:1270/1825 train_time:64047ms step_avg:50.43ms
step:1271/1825 train_time:64133ms step_avg:50.46ms
step:1272/1825 train_time:64222ms step_avg:50.49ms
step:1273/1825 train_time:64309ms step_avg:50.52ms
step:1274/1825 train_time:64398ms step_avg:50.55ms
step:1275/1825 train_time:64486ms step_avg:50.58ms
step:1276/1825 train_time:64576ms step_avg:50.61ms
step:1277/1825 train_time:64663ms step_avg:50.64ms
step:1278/1825 train_time:64752ms step_avg:50.67ms
step:1279/1825 train_time:64839ms step_avg:50.70ms
step:1280/1825 train_time:64929ms step_avg:50.73ms
step:1281/1825 train_time:65014ms step_avg:50.75ms
step:1282/1825 train_time:65103ms step_avg:50.78ms
step:1283/1825 train_time:65189ms step_avg:50.81ms
step:1284/1825 train_time:65278ms step_avg:50.84ms
step:1285/1825 train_time:65365ms step_avg:50.87ms
step:1286/1825 train_time:65455ms step_avg:50.90ms
step:1287/1825 train_time:65542ms step_avg:50.93ms
step:1288/1825 train_time:65633ms step_avg:50.96ms
step:1289/1825 train_time:65719ms step_avg:50.98ms
step:1290/1825 train_time:65810ms step_avg:51.02ms
step:1291/1825 train_time:65896ms step_avg:51.04ms
step:1292/1825 train_time:65985ms step_avg:51.07ms
step:1293/1825 train_time:66071ms step_avg:51.10ms
step:1294/1825 train_time:66158ms step_avg:51.13ms
step:1295/1825 train_time:66245ms step_avg:51.15ms
step:1296/1825 train_time:66334ms step_avg:51.18ms
step:1297/1825 train_time:66421ms step_avg:51.21ms
step:1298/1825 train_time:66511ms step_avg:51.24ms
step:1299/1825 train_time:66597ms step_avg:51.27ms
step:1300/1825 train_time:66688ms step_avg:51.30ms
step:1301/1825 train_time:66775ms step_avg:51.33ms
step:1302/1825 train_time:66864ms step_avg:51.35ms
step:1303/1825 train_time:66950ms step_avg:51.38ms
step:1304/1825 train_time:67038ms step_avg:51.41ms
step:1305/1825 train_time:67125ms step_avg:51.44ms
step:1306/1825 train_time:67214ms step_avg:51.47ms
step:1307/1825 train_time:67300ms step_avg:51.49ms
step:1308/1825 train_time:67390ms step_avg:51.52ms
step:1309/1825 train_time:67477ms step_avg:51.55ms
step:1310/1825 train_time:67567ms step_avg:51.58ms
step:1311/1825 train_time:67654ms step_avg:51.60ms
step:1312/1825 train_time:67743ms step_avg:51.63ms
step:1313/1825 train_time:67829ms step_avg:51.66ms
step:1314/1825 train_time:67918ms step_avg:51.69ms
step:1315/1825 train_time:68004ms step_avg:51.71ms
step:1316/1825 train_time:68094ms step_avg:51.74ms
step:1317/1825 train_time:68179ms step_avg:51.77ms
step:1318/1825 train_time:68270ms step_avg:51.80ms
step:1319/1825 train_time:68357ms step_avg:51.82ms
step:1320/1825 train_time:68447ms step_avg:51.85ms
step:1321/1825 train_time:68534ms step_avg:51.88ms
step:1322/1825 train_time:68623ms step_avg:51.91ms
step:1323/1825 train_time:68709ms step_avg:51.93ms
step:1324/1825 train_time:68798ms step_avg:51.96ms
step:1325/1825 train_time:68885ms step_avg:51.99ms
step:1326/1825 train_time:68974ms step_avg:52.02ms
step:1327/1825 train_time:69062ms step_avg:52.04ms
step:1328/1825 train_time:69152ms step_avg:52.07ms
step:1329/1825 train_time:69237ms step_avg:52.10ms
step:1330/1825 train_time:69327ms step_avg:52.13ms
step:1331/1825 train_time:69413ms step_avg:52.15ms
step:1332/1825 train_time:69503ms step_avg:52.18ms
step:1333/1825 train_time:69589ms step_avg:52.21ms
step:1334/1825 train_time:69680ms step_avg:52.23ms
step:1335/1825 train_time:69766ms step_avg:52.26ms
step:1336/1825 train_time:69855ms step_avg:52.29ms
step:1337/1825 train_time:69943ms step_avg:52.31ms
step:1338/1825 train_time:70032ms step_avg:52.34ms
step:1339/1825 train_time:70119ms step_avg:52.37ms
step:1340/1825 train_time:70208ms step_avg:52.39ms
step:1341/1825 train_time:70294ms step_avg:52.42ms
step:1342/1825 train_time:70384ms step_avg:52.45ms
step:1343/1825 train_time:70471ms step_avg:52.47ms
step:1344/1825 train_time:70559ms step_avg:52.50ms
step:1345/1825 train_time:70647ms step_avg:52.53ms
step:1346/1825 train_time:70736ms step_avg:52.55ms
step:1347/1825 train_time:70822ms step_avg:52.58ms
step:1348/1825 train_time:70914ms step_avg:52.61ms
step:1349/1825 train_time:70999ms step_avg:52.63ms
step:1350/1825 train_time:71089ms step_avg:52.66ms
step:1351/1825 train_time:71175ms step_avg:52.68ms
step:1352/1825 train_time:71262ms step_avg:52.71ms
step:1353/1825 train_time:71350ms step_avg:52.73ms
step:1354/1825 train_time:71438ms step_avg:52.76ms
step:1355/1825 train_time:71525ms step_avg:52.79ms
step:1356/1825 train_time:71616ms step_avg:52.81ms
step:1357/1825 train_time:71703ms step_avg:52.84ms
step:1358/1825 train_time:71792ms step_avg:52.87ms
step:1359/1825 train_time:71880ms step_avg:52.89ms
step:1360/1825 train_time:71969ms step_avg:52.92ms
step:1361/1825 train_time:72056ms step_avg:52.94ms
step:1362/1825 train_time:72145ms step_avg:52.97ms
step:1363/1825 train_time:72232ms step_avg:52.99ms
step:1364/1825 train_time:72321ms step_avg:53.02ms
step:1365/1825 train_time:72407ms step_avg:53.05ms
step:1366/1825 train_time:72496ms step_avg:53.07ms
step:1367/1825 train_time:72582ms step_avg:53.10ms
step:1368/1825 train_time:72672ms step_avg:53.12ms
step:1369/1825 train_time:72758ms step_avg:53.15ms
step:1370/1825 train_time:72849ms step_avg:53.17ms
step:1371/1825 train_time:72935ms step_avg:53.20ms
step:1372/1825 train_time:73025ms step_avg:53.23ms
step:1373/1825 train_time:73112ms step_avg:53.25ms
step:1374/1825 train_time:73200ms step_avg:53.28ms
step:1375/1825 train_time:73287ms step_avg:53.30ms
step:1376/1825 train_time:73376ms step_avg:53.33ms
step:1377/1825 train_time:73463ms step_avg:53.35ms
step:1378/1825 train_time:73552ms step_avg:53.38ms
step:1379/1825 train_time:73639ms step_avg:53.40ms
step:1380/1825 train_time:73729ms step_avg:53.43ms
step:1381/1825 train_time:73815ms step_avg:53.45ms
step:1382/1825 train_time:73904ms step_avg:53.48ms
step:1383/1825 train_time:73990ms step_avg:53.50ms
step:1384/1825 train_time:74080ms step_avg:53.53ms
step:1385/1825 train_time:74168ms step_avg:53.55ms
step:1386/1825 train_time:74256ms step_avg:53.58ms
step:1387/1825 train_time:74344ms step_avg:53.60ms
step:1388/1825 train_time:74433ms step_avg:53.63ms
step:1389/1825 train_time:74518ms step_avg:53.65ms
step:1390/1825 train_time:74608ms step_avg:53.68ms
step:1391/1825 train_time:74695ms step_avg:53.70ms
step:1392/1825 train_time:74783ms step_avg:53.72ms
step:1393/1825 train_time:74870ms step_avg:53.75ms
step:1394/1825 train_time:74958ms step_avg:53.77ms
step:1395/1825 train_time:75046ms step_avg:53.80ms
step:1396/1825 train_time:75135ms step_avg:53.82ms
step:1397/1825 train_time:75222ms step_avg:53.85ms
step:1398/1825 train_time:75313ms step_avg:53.87ms
step:1399/1825 train_time:75399ms step_avg:53.90ms
step:1400/1825 train_time:75489ms step_avg:53.92ms
step:1401/1825 train_time:75576ms step_avg:53.94ms
step:1402/1825 train_time:75666ms step_avg:53.97ms
step:1403/1825 train_time:75752ms step_avg:53.99ms
step:1404/1825 train_time:75841ms step_avg:54.02ms
step:1405/1825 train_time:75928ms step_avg:54.04ms
step:1406/1825 train_time:76017ms step_avg:54.07ms
step:1407/1825 train_time:76102ms step_avg:54.09ms
step:1408/1825 train_time:76192ms step_avg:54.11ms
step:1409/1825 train_time:76279ms step_avg:54.14ms
step:1410/1825 train_time:76369ms step_avg:54.16ms
step:1411/1825 train_time:76455ms step_avg:54.19ms
step:1412/1825 train_time:76544ms step_avg:54.21ms
step:1413/1825 train_time:76632ms step_avg:54.23ms
step:1414/1825 train_time:76720ms step_avg:54.26ms
step:1415/1825 train_time:76807ms step_avg:54.28ms
step:1416/1825 train_time:76897ms step_avg:54.31ms
step:1417/1825 train_time:76983ms step_avg:54.33ms
step:1418/1825 train_time:77073ms step_avg:54.35ms
step:1419/1825 train_time:77158ms step_avg:54.38ms
step:1420/1825 train_time:77248ms step_avg:54.40ms
step:1421/1825 train_time:77334ms step_avg:54.42ms
step:1422/1825 train_time:77423ms step_avg:54.45ms
step:1423/1825 train_time:77509ms step_avg:54.47ms
step:1424/1825 train_time:77598ms step_avg:54.49ms
step:1425/1825 train_time:77685ms step_avg:54.52ms
step:1426/1825 train_time:77775ms step_avg:54.54ms
step:1427/1825 train_time:77860ms step_avg:54.56ms
step:1428/1825 train_time:77951ms step_avg:54.59ms
step:1429/1825 train_time:78036ms step_avg:54.61ms
step:1430/1825 train_time:78126ms step_avg:54.63ms
step:1431/1825 train_time:78212ms step_avg:54.66ms
step:1432/1825 train_time:78300ms step_avg:54.68ms
step:1433/1825 train_time:78388ms step_avg:54.70ms
step:1434/1825 train_time:78478ms step_avg:54.73ms
step:1435/1825 train_time:78565ms step_avg:54.75ms
step:1436/1825 train_time:78654ms step_avg:54.77ms
step:1437/1825 train_time:78740ms step_avg:54.79ms
step:1438/1825 train_time:78830ms step_avg:54.82ms
step:1439/1825 train_time:78916ms step_avg:54.84ms
step:1440/1825 train_time:79006ms step_avg:54.87ms
step:1441/1825 train_time:79093ms step_avg:54.89ms
step:1442/1825 train_time:79182ms step_avg:54.91ms
step:1443/1825 train_time:79269ms step_avg:54.93ms
step:1444/1825 train_time:79358ms step_avg:54.96ms
step:1445/1825 train_time:79445ms step_avg:54.98ms
step:1446/1825 train_time:79534ms step_avg:55.00ms
step:1447/1825 train_time:79620ms step_avg:55.02ms
step:1448/1825 train_time:79711ms step_avg:55.05ms
step:1449/1825 train_time:79797ms step_avg:55.07ms
step:1450/1825 train_time:79888ms step_avg:55.10ms
step:1451/1825 train_time:79974ms step_avg:55.12ms
step:1452/1825 train_time:80063ms step_avg:55.14ms
step:1453/1825 train_time:80149ms step_avg:55.16ms
step:1454/1825 train_time:80238ms step_avg:55.18ms
step:1455/1825 train_time:80324ms step_avg:55.21ms
step:1456/1825 train_time:80414ms step_avg:55.23ms
step:1457/1825 train_time:80499ms step_avg:55.25ms
step:1458/1825 train_time:80590ms step_avg:55.27ms
step:1459/1825 train_time:80676ms step_avg:55.30ms
step:1460/1825 train_time:80765ms step_avg:55.32ms
step:1461/1825 train_time:80852ms step_avg:55.34ms
step:1462/1825 train_time:80941ms step_avg:55.36ms
step:1463/1825 train_time:81027ms step_avg:55.38ms
step:1464/1825 train_time:81116ms step_avg:55.41ms
step:1465/1825 train_time:81203ms step_avg:55.43ms
step:1466/1825 train_time:81294ms step_avg:55.45ms
step:1467/1825 train_time:81380ms step_avg:55.47ms
step:1468/1825 train_time:81470ms step_avg:55.50ms
step:1469/1825 train_time:81556ms step_avg:55.52ms
step:1470/1825 train_time:81645ms step_avg:55.54ms
step:1471/1825 train_time:81731ms step_avg:55.56ms
step:1472/1825 train_time:81819ms step_avg:55.58ms
step:1473/1825 train_time:81906ms step_avg:55.60ms
step:1474/1825 train_time:81994ms step_avg:55.63ms
step:1475/1825 train_time:82082ms step_avg:55.65ms
step:1476/1825 train_time:82172ms step_avg:55.67ms
step:1477/1825 train_time:82259ms step_avg:55.69ms
step:1478/1825 train_time:82348ms step_avg:55.72ms
step:1479/1825 train_time:82433ms step_avg:55.74ms
step:1480/1825 train_time:82524ms step_avg:55.76ms
step:1481/1825 train_time:82611ms step_avg:55.78ms
step:1482/1825 train_time:82699ms step_avg:55.80ms
step:1483/1825 train_time:82786ms step_avg:55.82ms
step:1484/1825 train_time:82876ms step_avg:55.85ms
step:1485/1825 train_time:82962ms step_avg:55.87ms
step:1486/1825 train_time:83052ms step_avg:55.89ms
step:1487/1825 train_time:83138ms step_avg:55.91ms
step:1488/1825 train_time:83228ms step_avg:55.93ms
step:1489/1825 train_time:83315ms step_avg:55.95ms
step:1490/1825 train_time:83403ms step_avg:55.98ms
step:1491/1825 train_time:83490ms step_avg:56.00ms
step:1492/1825 train_time:83579ms step_avg:56.02ms
step:1493/1825 train_time:83666ms step_avg:56.04ms
step:1494/1825 train_time:83754ms step_avg:56.06ms
step:1495/1825 train_time:83841ms step_avg:56.08ms
step:1496/1825 train_time:83931ms step_avg:56.10ms
step:1497/1825 train_time:84017ms step_avg:56.12ms
step:1498/1825 train_time:84108ms step_avg:56.15ms
step:1499/1825 train_time:84194ms step_avg:56.17ms
step:1500/1825 train_time:84283ms step_avg:56.19ms
step:1500/1825 val_loss:3.3954 train_time:84382ms step_avg:56.25ms
step:1501/1825 train_time:84400ms step_avg:56.23ms
step:1502/1825 train_time:84463ms step_avg:56.23ms
step:1503/1825 train_time:84553ms step_avg:56.26ms
step:1504/1825 train_time:84646ms step_avg:56.28ms
step:1505/1825 train_time:84731ms step_avg:56.30ms
step:1506/1825 train_time:84820ms step_avg:56.32ms
step:1507/1825 train_time:84905ms step_avg:56.34ms
step:1508/1825 train_time:84993ms step_avg:56.36ms
step:1509/1825 train_time:85079ms step_avg:56.38ms
step:1510/1825 train_time:85167ms step_avg:56.40ms
step:1511/1825 train_time:85253ms step_avg:56.42ms
step:1512/1825 train_time:85343ms step_avg:56.44ms
step:1513/1825 train_time:85433ms step_avg:56.47ms
step:1514/1825 train_time:85525ms step_avg:56.49ms
step:1515/1825 train_time:85612ms step_avg:56.51ms
step:1516/1825 train_time:85702ms step_avg:56.53ms
step:1517/1825 train_time:85787ms step_avg:56.55ms
step:1518/1825 train_time:85877ms step_avg:56.57ms
step:1519/1825 train_time:85962ms step_avg:56.59ms
step:1520/1825 train_time:86051ms step_avg:56.61ms
step:1521/1825 train_time:86137ms step_avg:56.63ms
step:1522/1825 train_time:86225ms step_avg:56.65ms
step:1523/1825 train_time:86311ms step_avg:56.67ms
step:1524/1825 train_time:86402ms step_avg:56.69ms
step:1525/1825 train_time:86489ms step_avg:56.71ms
step:1526/1825 train_time:86579ms step_avg:56.74ms
step:1527/1825 train_time:86666ms step_avg:56.76ms
step:1528/1825 train_time:86755ms step_avg:56.78ms
step:1529/1825 train_time:86841ms step_avg:56.80ms
step:1530/1825 train_time:86929ms step_avg:56.82ms
step:1531/1825 train_time:87016ms step_avg:56.84ms
step:1532/1825 train_time:87104ms step_avg:56.86ms
step:1533/1825 train_time:87190ms step_avg:56.88ms
step:1534/1825 train_time:87279ms step_avg:56.90ms
step:1535/1825 train_time:87366ms step_avg:56.92ms
step:1536/1825 train_time:87456ms step_avg:56.94ms
step:1537/1825 train_time:87544ms step_avg:56.96ms
step:1538/1825 train_time:87634ms step_avg:56.98ms
step:1539/1825 train_time:87720ms step_avg:57.00ms
step:1540/1825 train_time:87809ms step_avg:57.02ms
step:1541/1825 train_time:87896ms step_avg:57.04ms
step:1542/1825 train_time:87984ms step_avg:57.06ms
step:1543/1825 train_time:88071ms step_avg:57.08ms
step:1544/1825 train_time:88159ms step_avg:57.10ms
step:1545/1825 train_time:88246ms step_avg:57.12ms
step:1546/1825 train_time:88335ms step_avg:57.14ms
step:1547/1825 train_time:88422ms step_avg:57.16ms
step:1548/1825 train_time:88511ms step_avg:57.18ms
step:1549/1825 train_time:88598ms step_avg:57.20ms
step:1550/1825 train_time:88687ms step_avg:57.22ms
step:1551/1825 train_time:88774ms step_avg:57.24ms
step:1552/1825 train_time:88864ms step_avg:57.26ms
step:1553/1825 train_time:88949ms step_avg:57.28ms
step:1554/1825 train_time:89038ms step_avg:57.30ms
step:1555/1825 train_time:89123ms step_avg:57.31ms
step:1556/1825 train_time:89213ms step_avg:57.33ms
step:1557/1825 train_time:89300ms step_avg:57.35ms
step:1558/1825 train_time:89388ms step_avg:57.37ms
step:1559/1825 train_time:89474ms step_avg:57.39ms
step:1560/1825 train_time:89564ms step_avg:57.41ms
step:1561/1825 train_time:89651ms step_avg:57.43ms
step:1562/1825 train_time:89741ms step_avg:57.45ms
step:1563/1825 train_time:89827ms step_avg:57.47ms
step:1564/1825 train_time:89916ms step_avg:57.49ms
step:1565/1825 train_time:90003ms step_avg:57.51ms
step:1566/1825 train_time:90093ms step_avg:57.53ms
step:1567/1825 train_time:90178ms step_avg:57.55ms
step:1568/1825 train_time:90267ms step_avg:57.57ms
step:1569/1825 train_time:90352ms step_avg:57.59ms
step:1570/1825 train_time:90442ms step_avg:57.61ms
step:1571/1825 train_time:90528ms step_avg:57.62ms
step:1572/1825 train_time:90618ms step_avg:57.64ms
step:1573/1825 train_time:90705ms step_avg:57.66ms
step:1574/1825 train_time:90796ms step_avg:57.68ms
step:1575/1825 train_time:90882ms step_avg:57.70ms
step:1576/1825 train_time:90970ms step_avg:57.72ms
step:1577/1825 train_time:91057ms step_avg:57.74ms
step:1578/1825 train_time:91145ms step_avg:57.76ms
step:1579/1825 train_time:91231ms step_avg:57.78ms
step:1580/1825 train_time:91321ms step_avg:57.80ms
step:1581/1825 train_time:91407ms step_avg:57.82ms
step:1582/1825 train_time:91497ms step_avg:57.84ms
step:1583/1825 train_time:91582ms step_avg:57.85ms
step:1584/1825 train_time:91673ms step_avg:57.87ms
step:1585/1825 train_time:91759ms step_avg:57.89ms
step:1586/1825 train_time:91848ms step_avg:57.91ms
step:1587/1825 train_time:91935ms step_avg:57.93ms
step:1588/1825 train_time:92025ms step_avg:57.95ms
step:1589/1825 train_time:92111ms step_avg:57.97ms
step:1590/1825 train_time:92200ms step_avg:57.99ms
step:1591/1825 train_time:92286ms step_avg:58.00ms
step:1592/1825 train_time:92376ms step_avg:58.03ms
step:1593/1825 train_time:92462ms step_avg:58.04ms
step:1594/1825 train_time:92551ms step_avg:58.06ms
step:1595/1825 train_time:92638ms step_avg:58.08ms
step:1596/1825 train_time:92727ms step_avg:58.10ms
step:1597/1825 train_time:92813ms step_avg:58.12ms
step:1598/1825 train_time:92903ms step_avg:58.14ms
step:1599/1825 train_time:92989ms step_avg:58.15ms
step:1600/1825 train_time:93079ms step_avg:58.17ms
step:1601/1825 train_time:93164ms step_avg:58.19ms
step:1602/1825 train_time:93253ms step_avg:58.21ms
step:1603/1825 train_time:93341ms step_avg:58.23ms
step:1604/1825 train_time:93429ms step_avg:58.25ms
step:1605/1825 train_time:93516ms step_avg:58.27ms
step:1606/1825 train_time:93605ms step_avg:58.28ms
step:1607/1825 train_time:93691ms step_avg:58.30ms
step:1608/1825 train_time:93783ms step_avg:58.32ms
step:1609/1825 train_time:93868ms step_avg:58.34ms
step:1610/1825 train_time:93958ms step_avg:58.36ms
step:1611/1825 train_time:94044ms step_avg:58.38ms
step:1612/1825 train_time:94133ms step_avg:58.39ms
step:1613/1825 train_time:94219ms step_avg:58.41ms
step:1614/1825 train_time:94308ms step_avg:58.43ms
step:1615/1825 train_time:94395ms step_avg:58.45ms
step:1616/1825 train_time:94484ms step_avg:58.47ms
step:1617/1825 train_time:94571ms step_avg:58.49ms
step:1618/1825 train_time:94661ms step_avg:58.50ms
step:1619/1825 train_time:94747ms step_avg:58.52ms
step:1620/1825 train_time:94836ms step_avg:58.54ms
step:1621/1825 train_time:94923ms step_avg:58.56ms
step:1622/1825 train_time:95012ms step_avg:58.58ms
step:1623/1825 train_time:95099ms step_avg:58.59ms
step:1624/1825 train_time:95186ms step_avg:58.61ms
step:1625/1825 train_time:95273ms step_avg:58.63ms
step:1626/1825 train_time:95363ms step_avg:58.65ms
step:1627/1825 train_time:95449ms step_avg:58.67ms
step:1628/1825 train_time:95539ms step_avg:58.68ms
step:1629/1825 train_time:95626ms step_avg:58.70ms
step:1630/1825 train_time:95715ms step_avg:58.72ms
step:1631/1825 train_time:95803ms step_avg:58.74ms
step:1632/1825 train_time:95893ms step_avg:58.76ms
step:1633/1825 train_time:95979ms step_avg:58.77ms
step:1634/1825 train_time:96067ms step_avg:58.79ms
step:1635/1825 train_time:96153ms step_avg:58.81ms
step:1636/1825 train_time:96243ms step_avg:58.83ms
step:1637/1825 train_time:96328ms step_avg:58.84ms
step:1638/1825 train_time:96418ms step_avg:58.86ms
step:1639/1825 train_time:96505ms step_avg:58.88ms
step:1640/1825 train_time:96594ms step_avg:58.90ms
step:1641/1825 train_time:96680ms step_avg:58.92ms
step:1642/1825 train_time:96768ms step_avg:58.93ms
step:1643/1825 train_time:96856ms step_avg:58.95ms
step:1644/1825 train_time:96946ms step_avg:58.97ms
step:1645/1825 train_time:97032ms step_avg:58.99ms
step:1646/1825 train_time:97122ms step_avg:59.00ms
step:1647/1825 train_time:97209ms step_avg:59.02ms
step:1648/1825 train_time:97297ms step_avg:59.04ms
step:1649/1825 train_time:97384ms step_avg:59.06ms
step:1650/1825 train_time:97474ms step_avg:59.08ms
step:1651/1825 train_time:97560ms step_avg:59.09ms
step:1652/1825 train_time:97648ms step_avg:59.11ms
step:1653/1825 train_time:97736ms step_avg:59.13ms
step:1654/1825 train_time:97824ms step_avg:59.14ms
step:1655/1825 train_time:97912ms step_avg:59.16ms
step:1656/1825 train_time:98003ms step_avg:59.18ms
step:1657/1825 train_time:98088ms step_avg:59.20ms
step:1658/1825 train_time:98179ms step_avg:59.22ms
step:1659/1825 train_time:98264ms step_avg:59.23ms
step:1660/1825 train_time:98353ms step_avg:59.25ms
step:1661/1825 train_time:98440ms step_avg:59.27ms
step:1662/1825 train_time:98529ms step_avg:59.28ms
step:1663/1825 train_time:98616ms step_avg:59.30ms
step:1664/1825 train_time:98706ms step_avg:59.32ms
step:1665/1825 train_time:98793ms step_avg:59.34ms
step:1666/1825 train_time:98882ms step_avg:59.35ms
step:1667/1825 train_time:98969ms step_avg:59.37ms
step:1668/1825 train_time:99059ms step_avg:59.39ms
step:1669/1825 train_time:99145ms step_avg:59.40ms
step:1670/1825 train_time:99235ms step_avg:59.42ms
step:1671/1825 train_time:99322ms step_avg:59.44ms
step:1672/1825 train_time:99411ms step_avg:59.46ms
step:1673/1825 train_time:99498ms step_avg:59.47ms
step:1674/1825 train_time:99586ms step_avg:59.49ms
step:1675/1825 train_time:99672ms step_avg:59.51ms
step:1676/1825 train_time:99763ms step_avg:59.52ms
step:1677/1825 train_time:99848ms step_avg:59.54ms
step:1678/1825 train_time:99939ms step_avg:59.56ms
step:1679/1825 train_time:100024ms step_avg:59.57ms
step:1680/1825 train_time:100114ms step_avg:59.59ms
step:1681/1825 train_time:100200ms step_avg:59.61ms
step:1682/1825 train_time:100288ms step_avg:59.62ms
step:1683/1825 train_time:100375ms step_avg:59.64ms
step:1684/1825 train_time:100464ms step_avg:59.66ms
step:1685/1825 train_time:100551ms step_avg:59.67ms
step:1686/1825 train_time:100640ms step_avg:59.69ms
step:1687/1825 train_time:100726ms step_avg:59.71ms
step:1688/1825 train_time:100816ms step_avg:59.73ms
step:1689/1825 train_time:100903ms step_avg:59.74ms
step:1690/1825 train_time:100992ms step_avg:59.76ms
step:1691/1825 train_time:101078ms step_avg:59.77ms
step:1692/1825 train_time:101167ms step_avg:59.79ms
step:1693/1825 train_time:101254ms step_avg:59.81ms
step:1694/1825 train_time:101343ms step_avg:59.82ms
step:1695/1825 train_time:101429ms step_avg:59.84ms
step:1696/1825 train_time:101520ms step_avg:59.86ms
step:1697/1825 train_time:101606ms step_avg:59.87ms
step:1698/1825 train_time:101696ms step_avg:59.89ms
step:1699/1825 train_time:101782ms step_avg:59.91ms
step:1700/1825 train_time:101872ms step_avg:59.92ms
step:1701/1825 train_time:101958ms step_avg:59.94ms
step:1702/1825 train_time:102046ms step_avg:59.96ms
step:1703/1825 train_time:102132ms step_avg:59.97ms
step:1704/1825 train_time:102222ms step_avg:59.99ms
step:1705/1825 train_time:102308ms step_avg:60.00ms
step:1706/1825 train_time:102399ms step_avg:60.02ms
step:1707/1825 train_time:102485ms step_avg:60.04ms
step:1708/1825 train_time:102573ms step_avg:60.05ms
step:1709/1825 train_time:102659ms step_avg:60.07ms
step:1710/1825 train_time:102747ms step_avg:60.09ms
step:1711/1825 train_time:102836ms step_avg:60.10ms
step:1712/1825 train_time:102925ms step_avg:60.12ms
step:1713/1825 train_time:103011ms step_avg:60.13ms
step:1714/1825 train_time:103101ms step_avg:60.15ms
step:1715/1825 train_time:103187ms step_avg:60.17ms
step:1716/1825 train_time:103277ms step_avg:60.18ms
step:1717/1825 train_time:103363ms step_avg:60.20ms
step:1718/1825 train_time:103452ms step_avg:60.22ms
step:1719/1825 train_time:103539ms step_avg:60.23ms
step:1720/1825 train_time:103628ms step_avg:60.25ms
step:1721/1825 train_time:103715ms step_avg:60.26ms
step:1722/1825 train_time:103805ms step_avg:60.28ms
step:1723/1825 train_time:103892ms step_avg:60.30ms
step:1724/1825 train_time:103981ms step_avg:60.31ms
step:1725/1825 train_time:104067ms step_avg:60.33ms
step:1726/1825 train_time:104156ms step_avg:60.35ms
step:1727/1825 train_time:104243ms step_avg:60.36ms
step:1728/1825 train_time:104333ms step_avg:60.38ms
step:1729/1825 train_time:104419ms step_avg:60.39ms
step:1730/1825 train_time:104507ms step_avg:60.41ms
step:1731/1825 train_time:104593ms step_avg:60.42ms
step:1732/1825 train_time:104682ms step_avg:60.44ms
step:1733/1825 train_time:104768ms step_avg:60.45ms
step:1734/1825 train_time:104859ms step_avg:60.47ms
step:1735/1825 train_time:104945ms step_avg:60.49ms
step:1736/1825 train_time:105035ms step_avg:60.50ms
step:1737/1825 train_time:105121ms step_avg:60.52ms
step:1738/1825 train_time:105209ms step_avg:60.53ms
step:1739/1825 train_time:105296ms step_avg:60.55ms
step:1740/1825 train_time:105385ms step_avg:60.57ms
step:1741/1825 train_time:105472ms step_avg:60.58ms
step:1742/1825 train_time:105561ms step_avg:60.60ms
step:1743/1825 train_time:105647ms step_avg:60.61ms
step:1744/1825 train_time:105737ms step_avg:60.63ms
step:1745/1825 train_time:105823ms step_avg:60.64ms
step:1746/1825 train_time:105912ms step_avg:60.66ms
step:1747/1825 train_time:105999ms step_avg:60.67ms
step:1748/1825 train_time:106087ms step_avg:60.69ms
step:1749/1825 train_time:106175ms step_avg:60.71ms
step:1750/1825 train_time:106264ms step_avg:60.72ms
step:1750/1825 val_loss:3.2986 train_time:106361ms step_avg:60.78ms
step:1751/1825 train_time:106379ms step_avg:60.75ms
step:1752/1825 train_time:106440ms step_avg:60.75ms
step:1753/1825 train_time:106531ms step_avg:60.77ms
step:1754/1825 train_time:106621ms step_avg:60.79ms
step:1755/1825 train_time:106707ms step_avg:60.80ms
step:1756/1825 train_time:106796ms step_avg:60.82ms
step:1757/1825 train_time:106882ms step_avg:60.83ms
step:1758/1825 train_time:106970ms step_avg:60.85ms
step:1759/1825 train_time:107055ms step_avg:60.86ms
step:1760/1825 train_time:107143ms step_avg:60.88ms
step:1761/1825 train_time:107229ms step_avg:60.89ms
step:1762/1825 train_time:107319ms step_avg:60.91ms
step:1763/1825 train_time:107408ms step_avg:60.92ms
step:1764/1825 train_time:107499ms step_avg:60.94ms
step:1765/1825 train_time:107586ms step_avg:60.96ms
step:1766/1825 train_time:107676ms step_avg:60.97ms
step:1767/1825 train_time:107762ms step_avg:60.99ms
step:1768/1825 train_time:107852ms step_avg:61.00ms
step:1769/1825 train_time:107938ms step_avg:61.02ms
step:1770/1825 train_time:108026ms step_avg:61.03ms
step:1771/1825 train_time:108112ms step_avg:61.05ms
step:1772/1825 train_time:108199ms step_avg:61.06ms
step:1773/1825 train_time:108286ms step_avg:61.08ms
step:1774/1825 train_time:108377ms step_avg:61.09ms
step:1775/1825 train_time:108464ms step_avg:61.11ms
step:1776/1825 train_time:108556ms step_avg:61.12ms
step:1777/1825 train_time:108643ms step_avg:61.14ms
step:1778/1825 train_time:108732ms step_avg:61.15ms
step:1779/1825 train_time:108818ms step_avg:61.17ms
step:1780/1825 train_time:108907ms step_avg:61.18ms
step:1781/1825 train_time:108992ms step_avg:61.20ms
step:1782/1825 train_time:109080ms step_avg:61.21ms
step:1783/1825 train_time:109167ms step_avg:61.23ms
step:1784/1825 train_time:109256ms step_avg:61.24ms
step:1785/1825 train_time:109342ms step_avg:61.26ms
step:1786/1825 train_time:109436ms step_avg:61.27ms
step:1787/1825 train_time:109520ms step_avg:61.29ms
step:1788/1825 train_time:109612ms step_avg:61.30ms
step:1789/1825 train_time:109699ms step_avg:61.32ms
step:1790/1825 train_time:109788ms step_avg:61.33ms
step:1791/1825 train_time:109876ms step_avg:61.35ms
step:1792/1825 train_time:109965ms step_avg:61.36ms
step:1793/1825 train_time:110050ms step_avg:61.38ms
step:1794/1825 train_time:110139ms step_avg:61.39ms
step:1795/1825 train_time:110225ms step_avg:61.41ms
step:1796/1825 train_time:110315ms step_avg:61.42ms
step:1797/1825 train_time:110404ms step_avg:61.44ms
step:1798/1825 train_time:110496ms step_avg:61.46ms
step:1799/1825 train_time:110583ms step_avg:61.47ms
step:1800/1825 train_time:110673ms step_avg:61.48ms
step:1801/1825 train_time:110759ms step_avg:61.50ms
step:1802/1825 train_time:110851ms step_avg:61.52ms
step:1803/1825 train_time:110938ms step_avg:61.53ms
step:1804/1825 train_time:111029ms step_avg:61.55ms
step:1805/1825 train_time:111115ms step_avg:61.56ms
step:1806/1825 train_time:111204ms step_avg:61.57ms
step:1807/1825 train_time:111291ms step_avg:61.59ms
step:1808/1825 train_time:111380ms step_avg:61.60ms
step:1809/1825 train_time:111468ms step_avg:61.62ms
step:1810/1825 train_time:111557ms step_avg:61.63ms
step:1811/1825 train_time:111643ms step_avg:61.65ms
step:1812/1825 train_time:111734ms step_avg:61.66ms
step:1813/1825 train_time:111819ms step_avg:61.68ms
step:1814/1825 train_time:111909ms step_avg:61.69ms
step:1815/1825 train_time:111997ms step_avg:61.71ms
step:1816/1825 train_time:112085ms step_avg:61.72ms
step:1817/1825 train_time:112171ms step_avg:61.73ms
step:1818/1825 train_time:112260ms step_avg:61.75ms
step:1819/1825 train_time:112346ms step_avg:61.76ms
step:1820/1825 train_time:112437ms step_avg:61.78ms
step:1821/1825 train_time:112523ms step_avg:61.79ms
step:1822/1825 train_time:112614ms step_avg:61.81ms
step:1823/1825 train_time:112701ms step_avg:61.82ms
step:1824/1825 train_time:112790ms step_avg:61.84ms
step:1825/1825 train_time:112877ms step_avg:61.85ms
step:1825/1825 val_loss:3.2774 train_time:112973ms step_avg:61.90ms
peak memory allocated: 29497 MiB reserved: 44938 MiB

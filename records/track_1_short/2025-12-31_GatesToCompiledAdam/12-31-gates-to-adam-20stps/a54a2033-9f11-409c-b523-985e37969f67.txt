import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1785  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan  1 18:56:17 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    341859      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    1   N/A  N/A    341860      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    2   N/A  N/A    341861      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    3   N/A  N/A    341862      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    4   N/A  N/A    341863      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    5   N/A  N/A    341864      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    6   N/A  N/A    341865      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
|    7   N/A  N/A    341866      C   ...mamba/envs/speedrun2/bin/python3.12          0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 594, 595, 596, 1189, 1190, 1191, 1784, 1785, 1786] for warmup
Resetting Model
step:0/1825 val_loss:10.8298 train_time:0ms step_avg:0.03ms
step:1/1825 train_time:83ms step_avg:82.83ms
step:2/1825 train_time:103ms step_avg:51.38ms
step:3/1825 train_time:125ms step_avg:41.83ms
step:4/1825 train_time:161ms step_avg:40.15ms
step:5/1825 train_time:193ms step_avg:38.69ms
step:6/1825 train_time:282ms step_avg:46.93ms
step:7/1825 train_time:297ms step_avg:42.49ms
step:8/1825 train_time:327ms step_avg:40.83ms
step:9/1825 train_time:359ms step_avg:39.94ms
step:10/1825 train_time:395ms step_avg:39.45ms
step:11/1825 train_time:427ms step_avg:38.85ms
step:12/1825 train_time:463ms step_avg:38.56ms
step:13/1825 train_time:496ms step_avg:38.13ms
step:14/1825 train_time:531ms step_avg:37.94ms
step:15/1825 train_time:564ms step_avg:37.61ms
step:16/1825 train_time:600ms step_avg:37.47ms
step:17/1825 train_time:632ms step_avg:37.21ms
step:18/1825 train_time:668ms step_avg:37.11ms
step:19/1825 train_time:701ms step_avg:36.89ms
step:20/1825 train_time:736ms step_avg:36.80ms
step:21/1825 train_time:769ms step_avg:36.62ms
step:22/1825 train_time:805ms step_avg:36.57ms
step:23/1825 train_time:838ms step_avg:36.41ms
step:24/1825 train_time:873ms step_avg:36.37ms
step:25/1825 train_time:906ms step_avg:36.24ms
step:26/1825 train_time:941ms step_avg:36.21ms
step:27/1825 train_time:974ms step_avg:36.09ms
step:28/1825 train_time:1010ms step_avg:36.05ms
step:29/1825 train_time:1042ms step_avg:35.95ms
step:30/1825 train_time:1078ms step_avg:35.93ms
step:31/1825 train_time:1111ms step_avg:35.84ms
step:32/1825 train_time:1146ms step_avg:35.82ms
step:33/1825 train_time:1179ms step_avg:35.74ms
step:34/1825 train_time:1215ms step_avg:35.73ms
step:35/1825 train_time:1248ms step_avg:35.65ms
step:36/1825 train_time:1284ms step_avg:35.66ms
step:37/1825 train_time:1317ms step_avg:35.59ms
step:38/1825 train_time:1352ms step_avg:35.59ms
step:39/1825 train_time:1386ms step_avg:35.53ms
step:40/1825 train_time:1421ms step_avg:35.53ms
step:41/1825 train_time:1454ms step_avg:35.47ms
step:42/1825 train_time:1490ms step_avg:35.47ms
step:43/1825 train_time:1523ms step_avg:35.42ms
step:44/1825 train_time:1558ms step_avg:35.41ms
step:45/1825 train_time:1591ms step_avg:35.36ms
step:46/1825 train_time:1627ms step_avg:35.36ms
step:47/1825 train_time:1659ms step_avg:35.31ms
step:48/1825 train_time:1695ms step_avg:35.31ms
step:49/1825 train_time:1728ms step_avg:35.26ms
step:50/1825 train_time:1763ms step_avg:35.26ms
step:51/1825 train_time:1796ms step_avg:35.22ms
step:52/1825 train_time:1831ms step_avg:35.22ms
step:53/1825 train_time:1865ms step_avg:35.18ms
step:54/1825 train_time:1900ms step_avg:35.19ms
step:55/1825 train_time:1933ms step_avg:35.14ms
step:56/1825 train_time:1969ms step_avg:35.15ms
step:57/1825 train_time:2002ms step_avg:35.11ms
step:58/1825 train_time:2037ms step_avg:35.12ms
step:59/1825 train_time:2070ms step_avg:35.08ms
step:60/1825 train_time:2105ms step_avg:35.08ms
step:61/1825 train_time:2138ms step_avg:35.05ms
step:62/1825 train_time:2173ms step_avg:35.05ms
step:63/1825 train_time:2206ms step_avg:35.02ms
step:64/1825 train_time:2242ms step_avg:35.03ms
step:65/1825 train_time:2275ms step_avg:35.00ms
step:66/1825 train_time:2310ms step_avg:35.01ms
step:67/1825 train_time:2343ms step_avg:34.98ms
step:68/1825 train_time:2379ms step_avg:34.98ms
step:69/1825 train_time:2412ms step_avg:34.95ms
step:70/1825 train_time:2448ms step_avg:34.97ms
step:71/1825 train_time:2481ms step_avg:34.94ms
step:72/1825 train_time:2516ms step_avg:34.95ms
step:73/1825 train_time:2549ms step_avg:34.92ms
step:74/1825 train_time:2585ms step_avg:34.93ms
step:75/1825 train_time:2618ms step_avg:34.90ms
step:76/1825 train_time:2653ms step_avg:34.91ms
step:77/1825 train_time:2686ms step_avg:34.88ms
step:78/1825 train_time:2722ms step_avg:34.89ms
step:79/1825 train_time:2754ms step_avg:34.87ms
step:80/1825 train_time:2790ms step_avg:34.87ms
step:81/1825 train_time:2823ms step_avg:34.85ms
step:82/1825 train_time:2858ms step_avg:34.85ms
step:83/1825 train_time:2891ms step_avg:34.83ms
step:84/1825 train_time:2926ms step_avg:34.84ms
step:85/1825 train_time:2959ms step_avg:34.82ms
step:86/1825 train_time:2995ms step_avg:34.82ms
step:87/1825 train_time:3028ms step_avg:34.80ms
step:88/1825 train_time:3063ms step_avg:34.81ms
step:89/1825 train_time:3096ms step_avg:34.78ms
step:90/1825 train_time:3131ms step_avg:34.79ms
step:91/1825 train_time:3164ms step_avg:34.77ms
step:92/1825 train_time:3200ms step_avg:34.78ms
step:93/1825 train_time:3233ms step_avg:34.76ms
step:94/1825 train_time:3268ms step_avg:34.77ms
step:95/1825 train_time:3301ms step_avg:34.75ms
step:96/1825 train_time:3336ms step_avg:34.75ms
step:97/1825 train_time:3369ms step_avg:34.73ms
step:98/1825 train_time:3404ms step_avg:34.74ms
step:99/1825 train_time:3437ms step_avg:34.72ms
step:100/1825 train_time:3473ms step_avg:34.73ms
step:101/1825 train_time:3507ms step_avg:34.72ms
step:102/1825 train_time:3542ms step_avg:34.72ms
step:103/1825 train_time:3575ms step_avg:34.71ms
step:104/1825 train_time:3610ms step_avg:34.71ms
step:105/1825 train_time:3643ms step_avg:34.69ms
step:106/1825 train_time:3678ms step_avg:34.70ms
step:107/1825 train_time:3711ms step_avg:34.68ms
step:108/1825 train_time:3746ms step_avg:34.69ms
step:109/1825 train_time:3779ms step_avg:34.67ms
step:110/1825 train_time:3815ms step_avg:34.68ms
step:111/1825 train_time:3847ms step_avg:34.66ms
step:112/1825 train_time:3883ms step_avg:34.67ms
step:113/1825 train_time:3916ms step_avg:34.65ms
step:114/1825 train_time:3951ms step_avg:34.66ms
step:115/1825 train_time:3984ms step_avg:34.64ms
step:116/1825 train_time:4019ms step_avg:34.65ms
step:117/1825 train_time:4052ms step_avg:34.63ms
step:118/1825 train_time:4088ms step_avg:34.64ms
step:119/1825 train_time:4121ms step_avg:34.63ms
step:120/1825 train_time:4156ms step_avg:34.63ms
step:121/1825 train_time:4189ms step_avg:34.62ms
step:122/1825 train_time:4224ms step_avg:34.62ms
step:123/1825 train_time:4257ms step_avg:34.61ms
step:124/1825 train_time:4292ms step_avg:34.62ms
step:125/1825 train_time:4325ms step_avg:34.60ms
step:126/1825 train_time:4361ms step_avg:34.61ms
step:127/1825 train_time:4394ms step_avg:34.59ms
step:128/1825 train_time:4429ms step_avg:34.60ms
step:129/1825 train_time:4462ms step_avg:34.59ms
step:130/1825 train_time:4497ms step_avg:34.60ms
step:131/1825 train_time:4530ms step_avg:34.58ms
step:132/1825 train_time:4566ms step_avg:34.59ms
step:133/1825 train_time:4599ms step_avg:34.58ms
step:134/1825 train_time:4634ms step_avg:34.58ms
step:135/1825 train_time:4667ms step_avg:34.57ms
step:136/1825 train_time:4702ms step_avg:34.57ms
step:137/1825 train_time:4735ms step_avg:34.56ms
step:138/1825 train_time:4771ms step_avg:34.57ms
step:139/1825 train_time:4804ms step_avg:34.56ms
step:140/1825 train_time:4839ms step_avg:34.56ms
step:141/1825 train_time:4872ms step_avg:34.55ms
step:142/1825 train_time:4907ms step_avg:34.56ms
step:143/1825 train_time:4940ms step_avg:34.55ms
step:144/1825 train_time:4975ms step_avg:34.55ms
step:145/1825 train_time:5009ms step_avg:34.54ms
step:146/1825 train_time:5044ms step_avg:34.55ms
step:147/1825 train_time:5077ms step_avg:34.54ms
step:148/1825 train_time:5112ms step_avg:34.54ms
step:149/1825 train_time:5146ms step_avg:34.53ms
step:150/1825 train_time:5181ms step_avg:34.54ms
step:151/1825 train_time:5214ms step_avg:34.53ms
step:152/1825 train_time:5249ms step_avg:34.54ms
step:153/1825 train_time:5282ms step_avg:34.52ms
step:154/1825 train_time:5318ms step_avg:34.53ms
step:155/1825 train_time:5351ms step_avg:34.52ms
step:156/1825 train_time:5386ms step_avg:34.52ms
step:157/1825 train_time:5419ms step_avg:34.51ms
step:158/1825 train_time:5454ms step_avg:34.52ms
step:159/1825 train_time:5487ms step_avg:34.51ms
step:160/1825 train_time:5522ms step_avg:34.51ms
step:161/1825 train_time:5555ms step_avg:34.50ms
step:162/1825 train_time:5590ms step_avg:34.51ms
step:163/1825 train_time:5623ms step_avg:34.50ms
step:164/1825 train_time:5658ms step_avg:34.50ms
step:165/1825 train_time:5691ms step_avg:34.49ms
step:166/1825 train_time:5726ms step_avg:34.50ms
step:167/1825 train_time:5759ms step_avg:34.49ms
step:168/1825 train_time:5794ms step_avg:34.49ms
step:169/1825 train_time:5827ms step_avg:34.48ms
step:170/1825 train_time:5863ms step_avg:34.49ms
step:171/1825 train_time:5896ms step_avg:34.48ms
step:172/1825 train_time:5931ms step_avg:34.48ms
step:173/1825 train_time:5964ms step_avg:34.48ms
step:174/1825 train_time:6000ms step_avg:34.48ms
step:175/1825 train_time:6033ms step_avg:34.47ms
step:176/1825 train_time:6068ms step_avg:34.48ms
step:177/1825 train_time:6101ms step_avg:34.47ms
step:178/1825 train_time:6136ms step_avg:34.47ms
step:179/1825 train_time:6169ms step_avg:34.47ms
step:180/1825 train_time:6205ms step_avg:34.47ms
step:181/1825 train_time:6238ms step_avg:34.46ms
step:182/1825 train_time:6273ms step_avg:34.47ms
step:183/1825 train_time:6306ms step_avg:34.46ms
step:184/1825 train_time:6341ms step_avg:34.46ms
step:185/1825 train_time:6374ms step_avg:34.45ms
step:186/1825 train_time:6409ms step_avg:34.46ms
step:187/1825 train_time:6442ms step_avg:34.45ms
step:188/1825 train_time:6478ms step_avg:34.46ms
step:189/1825 train_time:6511ms step_avg:34.45ms
step:190/1825 train_time:6546ms step_avg:34.45ms
step:191/1825 train_time:6579ms step_avg:34.45ms
step:192/1825 train_time:6614ms step_avg:34.45ms
step:193/1825 train_time:6647ms step_avg:34.44ms
step:194/1825 train_time:6683ms step_avg:34.45ms
step:195/1825 train_time:6716ms step_avg:34.44ms
step:196/1825 train_time:6751ms step_avg:34.44ms
step:197/1825 train_time:6784ms step_avg:34.43ms
step:198/1825 train_time:6819ms step_avg:34.44ms
step:199/1825 train_time:6852ms step_avg:34.43ms
step:200/1825 train_time:6887ms step_avg:34.44ms
step:201/1825 train_time:6920ms step_avg:34.43ms
step:202/1825 train_time:6956ms step_avg:34.43ms
step:203/1825 train_time:6988ms step_avg:34.43ms
step:204/1825 train_time:7024ms step_avg:34.43ms
step:205/1825 train_time:7057ms step_avg:34.42ms
step:206/1825 train_time:7092ms step_avg:34.43ms
step:207/1825 train_time:7125ms step_avg:34.42ms
step:208/1825 train_time:7160ms step_avg:34.42ms
step:209/1825 train_time:7193ms step_avg:34.42ms
step:210/1825 train_time:7229ms step_avg:34.42ms
step:211/1825 train_time:7261ms step_avg:34.41ms
step:212/1825 train_time:7296ms step_avg:34.42ms
step:213/1825 train_time:7329ms step_avg:34.41ms
step:214/1825 train_time:7364ms step_avg:34.41ms
step:215/1825 train_time:7398ms step_avg:34.41ms
step:216/1825 train_time:7433ms step_avg:34.41ms
step:217/1825 train_time:7466ms step_avg:34.40ms
step:218/1825 train_time:7501ms step_avg:34.41ms
step:219/1825 train_time:7534ms step_avg:34.40ms
step:220/1825 train_time:7569ms step_avg:34.40ms
step:221/1825 train_time:7602ms step_avg:34.40ms
step:222/1825 train_time:7637ms step_avg:34.40ms
step:223/1825 train_time:7670ms step_avg:34.40ms
step:224/1825 train_time:7705ms step_avg:34.40ms
step:225/1825 train_time:7738ms step_avg:34.39ms
step:226/1825 train_time:7773ms step_avg:34.40ms
step:227/1825 train_time:7806ms step_avg:34.39ms
step:228/1825 train_time:7842ms step_avg:34.39ms
step:229/1825 train_time:7875ms step_avg:34.39ms
step:230/1825 train_time:7910ms step_avg:34.39ms
step:231/1825 train_time:7943ms step_avg:34.38ms
step:232/1825 train_time:7978ms step_avg:34.39ms
step:233/1825 train_time:8011ms step_avg:34.38ms
step:234/1825 train_time:8046ms step_avg:34.39ms
step:235/1825 train_time:8079ms step_avg:34.38ms
step:236/1825 train_time:8114ms step_avg:34.38ms
step:237/1825 train_time:8147ms step_avg:34.38ms
step:238/1825 train_time:8182ms step_avg:34.38ms
step:239/1825 train_time:8215ms step_avg:34.37ms
step:240/1825 train_time:8250ms step_avg:34.38ms
step:241/1825 train_time:8283ms step_avg:34.37ms
step:242/1825 train_time:8318ms step_avg:34.37ms
step:243/1825 train_time:8351ms step_avg:34.37ms
step:244/1825 train_time:8387ms step_avg:34.37ms
step:245/1825 train_time:8419ms step_avg:34.37ms
step:246/1825 train_time:8455ms step_avg:34.37ms
step:247/1825 train_time:8488ms step_avg:34.36ms
step:248/1825 train_time:8523ms step_avg:34.37ms
step:249/1825 train_time:8556ms step_avg:34.36ms
step:250/1825 train_time:8591ms step_avg:34.37ms
step:250/1825 val_loss:4.6101 train_time:8633ms step_avg:34.53ms
step:251/1825 train_time:8660ms step_avg:34.50ms
step:252/1825 train_time:8679ms step_avg:34.44ms
step:253/1825 train_time:8696ms step_avg:34.37ms
step:254/1825 train_time:8732ms step_avg:34.38ms
step:255/1825 train_time:8765ms step_avg:34.37ms
step:256/1825 train_time:8801ms step_avg:34.38ms
step:257/1825 train_time:8835ms step_avg:34.38ms
step:258/1825 train_time:8871ms step_avg:34.38ms
step:259/1825 train_time:8904ms step_avg:34.38ms
step:260/1825 train_time:8939ms step_avg:34.38ms
step:261/1825 train_time:8973ms step_avg:34.38ms
step:262/1825 train_time:9008ms step_avg:34.38ms
step:263/1825 train_time:9041ms step_avg:34.38ms
step:264/1825 train_time:9076ms step_avg:34.38ms
step:265/1825 train_time:9109ms step_avg:34.37ms
step:266/1825 train_time:9144ms step_avg:34.38ms
step:267/1825 train_time:9177ms step_avg:34.37ms
step:268/1825 train_time:9213ms step_avg:34.38ms
step:269/1825 train_time:9245ms step_avg:34.37ms
step:270/1825 train_time:9280ms step_avg:34.37ms
step:271/1825 train_time:9313ms step_avg:34.37ms
step:272/1825 train_time:9349ms step_avg:34.37ms
step:273/1825 train_time:9381ms step_avg:34.36ms
step:274/1825 train_time:9417ms step_avg:34.37ms
step:275/1825 train_time:9450ms step_avg:34.36ms
step:276/1825 train_time:9485ms step_avg:34.36ms
step:277/1825 train_time:9518ms step_avg:34.36ms
step:278/1825 train_time:9553ms step_avg:34.36ms
step:279/1825 train_time:9586ms step_avg:34.36ms
step:280/1825 train_time:9621ms step_avg:34.36ms
step:281/1825 train_time:9654ms step_avg:34.36ms
step:282/1825 train_time:9689ms step_avg:34.36ms
step:283/1825 train_time:9722ms step_avg:34.35ms
step:284/1825 train_time:9757ms step_avg:34.36ms
step:285/1825 train_time:9790ms step_avg:34.35ms
step:286/1825 train_time:9826ms step_avg:34.36ms
step:287/1825 train_time:9859ms step_avg:34.35ms
step:288/1825 train_time:9895ms step_avg:34.36ms
step:289/1825 train_time:9928ms step_avg:34.35ms
step:290/1825 train_time:9963ms step_avg:34.36ms
step:291/1825 train_time:9996ms step_avg:34.35ms
step:292/1825 train_time:10031ms step_avg:34.35ms
step:293/1825 train_time:10064ms step_avg:34.35ms
step:294/1825 train_time:10099ms step_avg:34.35ms
step:295/1825 train_time:10132ms step_avg:34.35ms
step:296/1825 train_time:10167ms step_avg:34.35ms
step:297/1825 train_time:10200ms step_avg:34.34ms
step:298/1825 train_time:10235ms step_avg:34.35ms
step:299/1825 train_time:10268ms step_avg:34.34ms
step:300/1825 train_time:10303ms step_avg:34.34ms
step:301/1825 train_time:10336ms step_avg:34.34ms
step:302/1825 train_time:10372ms step_avg:34.34ms
step:303/1825 train_time:10404ms step_avg:34.34ms
step:304/1825 train_time:10439ms step_avg:34.34ms
step:305/1825 train_time:10472ms step_avg:34.34ms
step:306/1825 train_time:10508ms step_avg:34.34ms
step:307/1825 train_time:10540ms step_avg:34.33ms
step:308/1825 train_time:10576ms step_avg:34.34ms
step:309/1825 train_time:10609ms step_avg:34.33ms
step:310/1825 train_time:10644ms step_avg:34.33ms
step:311/1825 train_time:10677ms step_avg:34.33ms
step:312/1825 train_time:10712ms step_avg:34.33ms
step:313/1825 train_time:10745ms step_avg:34.33ms
step:314/1825 train_time:10780ms step_avg:34.33ms
step:315/1825 train_time:10813ms step_avg:34.33ms
step:316/1825 train_time:10849ms step_avg:34.33ms
step:317/1825 train_time:10882ms step_avg:34.33ms
step:318/1825 train_time:10917ms step_avg:34.33ms
step:319/1825 train_time:10950ms step_avg:34.33ms
step:320/1825 train_time:10985ms step_avg:34.33ms
step:321/1825 train_time:11018ms step_avg:34.32ms
step:322/1825 train_time:11053ms step_avg:34.33ms
step:323/1825 train_time:11087ms step_avg:34.32ms
step:324/1825 train_time:11121ms step_avg:34.33ms
step:325/1825 train_time:11154ms step_avg:34.32ms
step:326/1825 train_time:11190ms step_avg:34.32ms
step:327/1825 train_time:11223ms step_avg:34.32ms
step:328/1825 train_time:11258ms step_avg:34.32ms
step:329/1825 train_time:11291ms step_avg:34.32ms
step:330/1825 train_time:11326ms step_avg:34.32ms
step:331/1825 train_time:11359ms step_avg:34.32ms
step:332/1825 train_time:11394ms step_avg:34.32ms
step:333/1825 train_time:11427ms step_avg:34.32ms
step:334/1825 train_time:11462ms step_avg:34.32ms
step:335/1825 train_time:11495ms step_avg:34.31ms
step:336/1825 train_time:11531ms step_avg:34.32ms
step:337/1825 train_time:11563ms step_avg:34.31ms
step:338/1825 train_time:11598ms step_avg:34.32ms
step:339/1825 train_time:11631ms step_avg:34.31ms
step:340/1825 train_time:11667ms step_avg:34.31ms
step:341/1825 train_time:11700ms step_avg:34.31ms
step:342/1825 train_time:11735ms step_avg:34.31ms
step:343/1825 train_time:11768ms step_avg:34.31ms
step:344/1825 train_time:11803ms step_avg:34.31ms
step:345/1825 train_time:11836ms step_avg:34.31ms
step:346/1825 train_time:11871ms step_avg:34.31ms
step:347/1825 train_time:11904ms step_avg:34.31ms
step:348/1825 train_time:11939ms step_avg:34.31ms
step:349/1825 train_time:11972ms step_avg:34.30ms
step:350/1825 train_time:12007ms step_avg:34.31ms
step:351/1825 train_time:12040ms step_avg:34.30ms
step:352/1825 train_time:12075ms step_avg:34.30ms
step:353/1825 train_time:12108ms step_avg:34.30ms
step:354/1825 train_time:12143ms step_avg:34.30ms
step:355/1825 train_time:12176ms step_avg:34.30ms
step:356/1825 train_time:12212ms step_avg:34.30ms
step:357/1825 train_time:12244ms step_avg:34.30ms
step:358/1825 train_time:12280ms step_avg:34.30ms
step:359/1825 train_time:12312ms step_avg:34.30ms
step:360/1825 train_time:12348ms step_avg:34.30ms
step:361/1825 train_time:12380ms step_avg:34.29ms
step:362/1825 train_time:12416ms step_avg:34.30ms
step:363/1825 train_time:12449ms step_avg:34.29ms
step:364/1825 train_time:12484ms step_avg:34.30ms
step:365/1825 train_time:12516ms step_avg:34.29ms
step:366/1825 train_time:12551ms step_avg:34.29ms
step:367/1825 train_time:12584ms step_avg:34.29ms
step:368/1825 train_time:12620ms step_avg:34.29ms
step:369/1825 train_time:12652ms step_avg:34.29ms
step:370/1825 train_time:12688ms step_avg:34.29ms
step:371/1825 train_time:12721ms step_avg:34.29ms
step:372/1825 train_time:12756ms step_avg:34.29ms
step:373/1825 train_time:12789ms step_avg:34.29ms
step:374/1825 train_time:12824ms step_avg:34.29ms
step:375/1825 train_time:12857ms step_avg:34.28ms
step:376/1825 train_time:12892ms step_avg:34.29ms
step:377/1825 train_time:12925ms step_avg:34.28ms
step:378/1825 train_time:12960ms step_avg:34.29ms
step:379/1825 train_time:12993ms step_avg:34.28ms
step:380/1825 train_time:13028ms step_avg:34.29ms
step:381/1825 train_time:13061ms step_avg:34.28ms
step:382/1825 train_time:13096ms step_avg:34.28ms
step:383/1825 train_time:13129ms step_avg:34.28ms
step:384/1825 train_time:13164ms step_avg:34.28ms
step:385/1825 train_time:13197ms step_avg:34.28ms
step:386/1825 train_time:13232ms step_avg:34.28ms
step:387/1825 train_time:13265ms step_avg:34.28ms
step:388/1825 train_time:13300ms step_avg:34.28ms
step:389/1825 train_time:13333ms step_avg:34.28ms
step:390/1825 train_time:13369ms step_avg:34.28ms
step:391/1825 train_time:13402ms step_avg:34.28ms
step:392/1825 train_time:13437ms step_avg:34.28ms
step:393/1825 train_time:13470ms step_avg:34.27ms
step:394/1825 train_time:13505ms step_avg:34.28ms
step:395/1825 train_time:13538ms step_avg:34.27ms
step:396/1825 train_time:13573ms step_avg:34.28ms
step:397/1825 train_time:13606ms step_avg:34.27ms
step:398/1825 train_time:13641ms step_avg:34.27ms
step:399/1825 train_time:13674ms step_avg:34.27ms
step:400/1825 train_time:13709ms step_avg:34.27ms
step:401/1825 train_time:13742ms step_avg:34.27ms
step:402/1825 train_time:13777ms step_avg:34.27ms
step:403/1825 train_time:13810ms step_avg:34.27ms
step:404/1825 train_time:13846ms step_avg:34.27ms
step:405/1825 train_time:13879ms step_avg:34.27ms
step:406/1825 train_time:13914ms step_avg:34.27ms
step:407/1825 train_time:13946ms step_avg:34.27ms
step:408/1825 train_time:13982ms step_avg:34.27ms
step:409/1825 train_time:14015ms step_avg:34.27ms
step:410/1825 train_time:14050ms step_avg:34.27ms
step:411/1825 train_time:14083ms step_avg:34.27ms
step:412/1825 train_time:14118ms step_avg:34.27ms
step:413/1825 train_time:14151ms step_avg:34.26ms
step:414/1825 train_time:14186ms step_avg:34.27ms
step:415/1825 train_time:14219ms step_avg:34.26ms
step:416/1825 train_time:14254ms step_avg:34.27ms
step:417/1825 train_time:14287ms step_avg:34.26ms
step:418/1825 train_time:14322ms step_avg:34.26ms
step:419/1825 train_time:14355ms step_avg:34.26ms
step:420/1825 train_time:14390ms step_avg:34.26ms
step:421/1825 train_time:14423ms step_avg:34.26ms
step:422/1825 train_time:14458ms step_avg:34.26ms
step:423/1825 train_time:14491ms step_avg:34.26ms
step:424/1825 train_time:14527ms step_avg:34.26ms
step:425/1825 train_time:14560ms step_avg:34.26ms
step:426/1825 train_time:14595ms step_avg:34.26ms
step:427/1825 train_time:14628ms step_avg:34.26ms
step:428/1825 train_time:14663ms step_avg:34.26ms
step:429/1825 train_time:14696ms step_avg:34.26ms
step:430/1825 train_time:14731ms step_avg:34.26ms
step:431/1825 train_time:14764ms step_avg:34.25ms
step:432/1825 train_time:14799ms step_avg:34.26ms
step:433/1825 train_time:14832ms step_avg:34.25ms
step:434/1825 train_time:14867ms step_avg:34.26ms
step:435/1825 train_time:14900ms step_avg:34.25ms
step:436/1825 train_time:14935ms step_avg:34.26ms
step:437/1825 train_time:14968ms step_avg:34.25ms
step:438/1825 train_time:15003ms step_avg:34.25ms
step:439/1825 train_time:15036ms step_avg:34.25ms
step:440/1825 train_time:15071ms step_avg:34.25ms
step:441/1825 train_time:15104ms step_avg:34.25ms
step:442/1825 train_time:15139ms step_avg:34.25ms
step:443/1825 train_time:15172ms step_avg:34.25ms
step:444/1825 train_time:15207ms step_avg:34.25ms
step:445/1825 train_time:15240ms step_avg:34.25ms
step:446/1825 train_time:15276ms step_avg:34.25ms
step:447/1825 train_time:15309ms step_avg:34.25ms
step:448/1825 train_time:15344ms step_avg:34.25ms
step:449/1825 train_time:15377ms step_avg:34.25ms
step:450/1825 train_time:15412ms step_avg:34.25ms
step:451/1825 train_time:15444ms step_avg:34.24ms
step:452/1825 train_time:15480ms step_avg:34.25ms
step:453/1825 train_time:15513ms step_avg:34.24ms
step:454/1825 train_time:15548ms step_avg:34.25ms
step:455/1825 train_time:15581ms step_avg:34.24ms
step:456/1825 train_time:15616ms step_avg:34.25ms
step:457/1825 train_time:15649ms step_avg:34.24ms
step:458/1825 train_time:15684ms step_avg:34.24ms
step:459/1825 train_time:15717ms step_avg:34.24ms
step:460/1825 train_time:15752ms step_avg:34.24ms
step:461/1825 train_time:15785ms step_avg:34.24ms
step:462/1825 train_time:15820ms step_avg:34.24ms
step:463/1825 train_time:15853ms step_avg:34.24ms
step:464/1825 train_time:15889ms step_avg:34.24ms
step:465/1825 train_time:15921ms step_avg:34.24ms
step:466/1825 train_time:15957ms step_avg:34.24ms
step:467/1825 train_time:15990ms step_avg:34.24ms
step:468/1825 train_time:16025ms step_avg:34.24ms
step:469/1825 train_time:16058ms step_avg:34.24ms
step:470/1825 train_time:16093ms step_avg:34.24ms
step:471/1825 train_time:16126ms step_avg:34.24ms
step:472/1825 train_time:16161ms step_avg:34.24ms
step:473/1825 train_time:16194ms step_avg:34.24ms
step:474/1825 train_time:16229ms step_avg:34.24ms
step:475/1825 train_time:16262ms step_avg:34.24ms
step:476/1825 train_time:16297ms step_avg:34.24ms
step:477/1825 train_time:16330ms step_avg:34.23ms
step:478/1825 train_time:16365ms step_avg:34.24ms
step:479/1825 train_time:16398ms step_avg:34.23ms
step:480/1825 train_time:16433ms step_avg:34.24ms
step:481/1825 train_time:16466ms step_avg:34.23ms
step:482/1825 train_time:16501ms step_avg:34.23ms
step:483/1825 train_time:16534ms step_avg:34.23ms
step:484/1825 train_time:16569ms step_avg:34.23ms
step:485/1825 train_time:16602ms step_avg:34.23ms
step:486/1825 train_time:16637ms step_avg:34.23ms
step:487/1825 train_time:16670ms step_avg:34.23ms
step:488/1825 train_time:16706ms step_avg:34.23ms
step:489/1825 train_time:16739ms step_avg:34.23ms
step:490/1825 train_time:16774ms step_avg:34.23ms
step:491/1825 train_time:16807ms step_avg:34.23ms
step:492/1825 train_time:16842ms step_avg:34.23ms
step:493/1825 train_time:16875ms step_avg:34.23ms
step:494/1825 train_time:16910ms step_avg:34.23ms
step:495/1825 train_time:16943ms step_avg:34.23ms
step:496/1825 train_time:16978ms step_avg:34.23ms
step:497/1825 train_time:17011ms step_avg:34.23ms
step:498/1825 train_time:17046ms step_avg:34.23ms
step:499/1825 train_time:17079ms step_avg:34.23ms
step:500/1825 train_time:17114ms step_avg:34.23ms
step:500/1825 val_loss:4.2930 train_time:17156ms step_avg:34.31ms
step:501/1825 train_time:17177ms step_avg:34.29ms
step:502/1825 train_time:17195ms step_avg:34.25ms
step:503/1825 train_time:17218ms step_avg:34.23ms
step:504/1825 train_time:17253ms step_avg:34.23ms
step:505/1825 train_time:17286ms step_avg:34.23ms
step:506/1825 train_time:17324ms step_avg:34.24ms
step:507/1825 train_time:17359ms step_avg:34.24ms
step:508/1825 train_time:17395ms step_avg:34.24ms
step:509/1825 train_time:17428ms step_avg:34.24ms
step:510/1825 train_time:17464ms step_avg:34.24ms
step:511/1825 train_time:17497ms step_avg:34.24ms
step:512/1825 train_time:17532ms step_avg:34.24ms
step:513/1825 train_time:17565ms step_avg:34.24ms
step:514/1825 train_time:17600ms step_avg:34.24ms
step:515/1825 train_time:17634ms step_avg:34.24ms
step:516/1825 train_time:17669ms step_avg:34.24ms
step:517/1825 train_time:17701ms step_avg:34.24ms
step:518/1825 train_time:17737ms step_avg:34.24ms
step:519/1825 train_time:17770ms step_avg:34.24ms
step:520/1825 train_time:17805ms step_avg:34.24ms
step:521/1825 train_time:17838ms step_avg:34.24ms
step:522/1825 train_time:17873ms step_avg:34.24ms
step:523/1825 train_time:17905ms step_avg:34.24ms
step:524/1825 train_time:17941ms step_avg:34.24ms
step:525/1825 train_time:17973ms step_avg:34.24ms
step:526/1825 train_time:18009ms step_avg:34.24ms
step:527/1825 train_time:18041ms step_avg:34.23ms
step:528/1825 train_time:18077ms step_avg:34.24ms
step:529/1825 train_time:18110ms step_avg:34.23ms
step:530/1825 train_time:18145ms step_avg:34.24ms
step:531/1825 train_time:18178ms step_avg:34.23ms
step:532/1825 train_time:18213ms step_avg:34.23ms
step:533/1825 train_time:18246ms step_avg:34.23ms
step:534/1825 train_time:18281ms step_avg:34.23ms
step:535/1825 train_time:18314ms step_avg:34.23ms
step:536/1825 train_time:18349ms step_avg:34.23ms
step:537/1825 train_time:18382ms step_avg:34.23ms
step:538/1825 train_time:18417ms step_avg:34.23ms
step:539/1825 train_time:18450ms step_avg:34.23ms
step:540/1825 train_time:18485ms step_avg:34.23ms
step:541/1825 train_time:18518ms step_avg:34.23ms
step:542/1825 train_time:18554ms step_avg:34.23ms
step:543/1825 train_time:18587ms step_avg:34.23ms
step:544/1825 train_time:18622ms step_avg:34.23ms
step:545/1825 train_time:18655ms step_avg:34.23ms
step:546/1825 train_time:18690ms step_avg:34.23ms
step:547/1825 train_time:18723ms step_avg:34.23ms
step:548/1825 train_time:18758ms step_avg:34.23ms
step:549/1825 train_time:18791ms step_avg:34.23ms
step:550/1825 train_time:18826ms step_avg:34.23ms
step:551/1825 train_time:18859ms step_avg:34.23ms
step:552/1825 train_time:18894ms step_avg:34.23ms
step:553/1825 train_time:18927ms step_avg:34.23ms
step:554/1825 train_time:18962ms step_avg:34.23ms
step:555/1825 train_time:18995ms step_avg:34.23ms
step:556/1825 train_time:19030ms step_avg:34.23ms
step:557/1825 train_time:19063ms step_avg:34.22ms
step:558/1825 train_time:19099ms step_avg:34.23ms
step:559/1825 train_time:19132ms step_avg:34.22ms
step:560/1825 train_time:19167ms step_avg:34.23ms
step:561/1825 train_time:19199ms step_avg:34.22ms
step:562/1825 train_time:19235ms step_avg:34.23ms
step:563/1825 train_time:19267ms step_avg:34.22ms
step:564/1825 train_time:19303ms step_avg:34.22ms
step:565/1825 train_time:19336ms step_avg:34.22ms
step:566/1825 train_time:19371ms step_avg:34.22ms
step:567/1825 train_time:19403ms step_avg:34.22ms
step:568/1825 train_time:19439ms step_avg:34.22ms
step:569/1825 train_time:19472ms step_avg:34.22ms
step:570/1825 train_time:19507ms step_avg:34.22ms
step:571/1825 train_time:19540ms step_avg:34.22ms
step:572/1825 train_time:19575ms step_avg:34.22ms
step:573/1825 train_time:19608ms step_avg:34.22ms
step:574/1825 train_time:19643ms step_avg:34.22ms
step:575/1825 train_time:19676ms step_avg:34.22ms
step:576/1825 train_time:19711ms step_avg:34.22ms
step:577/1825 train_time:19744ms step_avg:34.22ms
step:578/1825 train_time:19779ms step_avg:34.22ms
step:579/1825 train_time:19812ms step_avg:34.22ms
step:580/1825 train_time:19848ms step_avg:34.22ms
step:581/1825 train_time:19881ms step_avg:34.22ms
step:582/1825 train_time:19916ms step_avg:34.22ms
step:583/1825 train_time:19949ms step_avg:34.22ms
step:584/1825 train_time:19984ms step_avg:34.22ms
step:585/1825 train_time:20016ms step_avg:34.22ms
step:586/1825 train_time:20052ms step_avg:34.22ms
step:587/1825 train_time:20085ms step_avg:34.22ms
step:588/1825 train_time:20120ms step_avg:34.22ms
step:589/1825 train_time:20153ms step_avg:34.22ms
step:590/1825 train_time:20188ms step_avg:34.22ms
step:591/1825 train_time:20221ms step_avg:34.21ms
step:592/1825 train_time:20256ms step_avg:34.22ms
step:593/1825 train_time:20289ms step_avg:34.21ms
step:594/1825 train_time:20324ms step_avg:34.22ms
step:595/1825 train_time:20357ms step_avg:34.21ms
step:596/1825 train_time:20394ms step_avg:34.22ms
step:597/1825 train_time:20452ms step_avg:34.26ms
step:598/1825 train_time:20514ms step_avg:34.30ms
step:599/1825 train_time:20574ms step_avg:34.35ms
step:600/1825 train_time:20636ms step_avg:34.39ms
step:601/1825 train_time:20697ms step_avg:34.44ms
step:602/1825 train_time:20759ms step_avg:34.48ms
step:603/1825 train_time:20820ms step_avg:34.53ms
step:604/1825 train_time:20883ms step_avg:34.57ms
step:605/1825 train_time:20942ms step_avg:34.62ms
step:606/1825 train_time:21004ms step_avg:34.66ms
step:607/1825 train_time:21065ms step_avg:34.70ms
step:608/1825 train_time:21128ms step_avg:34.75ms
step:609/1825 train_time:21188ms step_avg:34.79ms
step:610/1825 train_time:21251ms step_avg:34.84ms
step:611/1825 train_time:21311ms step_avg:34.88ms
step:612/1825 train_time:21374ms step_avg:34.92ms
step:613/1825 train_time:21434ms step_avg:34.97ms
step:614/1825 train_time:21497ms step_avg:35.01ms
step:615/1825 train_time:21556ms step_avg:35.05ms
step:616/1825 train_time:21619ms step_avg:35.10ms
step:617/1825 train_time:21679ms step_avg:35.14ms
step:618/1825 train_time:21742ms step_avg:35.18ms
step:619/1825 train_time:21802ms step_avg:35.22ms
step:620/1825 train_time:21865ms step_avg:35.27ms
step:621/1825 train_time:21925ms step_avg:35.31ms
step:622/1825 train_time:21987ms step_avg:35.35ms
step:623/1825 train_time:22048ms step_avg:35.39ms
step:624/1825 train_time:22111ms step_avg:35.43ms
step:625/1825 train_time:22171ms step_avg:35.47ms
step:626/1825 train_time:22233ms step_avg:35.52ms
step:627/1825 train_time:22294ms step_avg:35.56ms
step:628/1825 train_time:22357ms step_avg:35.60ms
step:629/1825 train_time:22418ms step_avg:35.64ms
step:630/1825 train_time:22480ms step_avg:35.68ms
step:631/1825 train_time:22540ms step_avg:35.72ms
step:632/1825 train_time:22603ms step_avg:35.76ms
step:633/1825 train_time:22663ms step_avg:35.80ms
step:634/1825 train_time:22725ms step_avg:35.84ms
step:635/1825 train_time:22785ms step_avg:35.88ms
step:636/1825 train_time:22849ms step_avg:35.93ms
step:637/1825 train_time:22909ms step_avg:35.96ms
step:638/1825 train_time:22973ms step_avg:36.01ms
step:639/1825 train_time:23033ms step_avg:36.05ms
step:640/1825 train_time:23096ms step_avg:36.09ms
step:641/1825 train_time:23156ms step_avg:36.13ms
step:642/1825 train_time:23219ms step_avg:36.17ms
step:643/1825 train_time:23279ms step_avg:36.20ms
step:644/1825 train_time:23341ms step_avg:36.24ms
step:645/1825 train_time:23402ms step_avg:36.28ms
step:646/1825 train_time:23464ms step_avg:36.32ms
step:647/1825 train_time:23524ms step_avg:36.36ms
step:648/1825 train_time:23587ms step_avg:36.40ms
step:649/1825 train_time:23648ms step_avg:36.44ms
step:650/1825 train_time:23711ms step_avg:36.48ms
step:651/1825 train_time:23771ms step_avg:36.51ms
step:652/1825 train_time:23834ms step_avg:36.56ms
step:653/1825 train_time:23895ms step_avg:36.59ms
step:654/1825 train_time:23958ms step_avg:36.63ms
step:655/1825 train_time:24018ms step_avg:36.67ms
step:656/1825 train_time:24080ms step_avg:36.71ms
step:657/1825 train_time:24141ms step_avg:36.74ms
step:658/1825 train_time:24203ms step_avg:36.78ms
step:659/1825 train_time:24263ms step_avg:36.82ms
step:660/1825 train_time:24326ms step_avg:36.86ms
step:661/1825 train_time:24387ms step_avg:36.89ms
step:662/1825 train_time:24450ms step_avg:36.93ms
step:663/1825 train_time:24510ms step_avg:36.97ms
step:664/1825 train_time:24573ms step_avg:37.01ms
step:665/1825 train_time:24634ms step_avg:37.04ms
step:666/1825 train_time:24696ms step_avg:37.08ms
step:667/1825 train_time:24756ms step_avg:37.12ms
step:668/1825 train_time:24819ms step_avg:37.15ms
step:669/1825 train_time:24879ms step_avg:37.19ms
step:670/1825 train_time:24941ms step_avg:37.23ms
step:671/1825 train_time:25001ms step_avg:37.26ms
step:672/1825 train_time:25064ms step_avg:37.30ms
step:673/1825 train_time:25124ms step_avg:37.33ms
step:674/1825 train_time:25187ms step_avg:37.37ms
step:675/1825 train_time:25247ms step_avg:37.40ms
step:676/1825 train_time:25310ms step_avg:37.44ms
step:677/1825 train_time:25372ms step_avg:37.48ms
step:678/1825 train_time:25434ms step_avg:37.51ms
step:679/1825 train_time:25494ms step_avg:37.55ms
step:680/1825 train_time:25557ms step_avg:37.58ms
step:681/1825 train_time:25618ms step_avg:37.62ms
step:682/1825 train_time:25680ms step_avg:37.65ms
step:683/1825 train_time:25740ms step_avg:37.69ms
step:684/1825 train_time:25803ms step_avg:37.72ms
step:685/1825 train_time:25863ms step_avg:37.76ms
step:686/1825 train_time:25926ms step_avg:37.79ms
step:687/1825 train_time:25987ms step_avg:37.83ms
step:688/1825 train_time:26050ms step_avg:37.86ms
step:689/1825 train_time:26110ms step_avg:37.90ms
step:690/1825 train_time:26173ms step_avg:37.93ms
step:691/1825 train_time:26233ms step_avg:37.96ms
step:692/1825 train_time:26297ms step_avg:38.00ms
step:693/1825 train_time:26357ms step_avg:38.03ms
step:694/1825 train_time:26419ms step_avg:38.07ms
step:695/1825 train_time:26480ms step_avg:38.10ms
step:696/1825 train_time:26542ms step_avg:38.14ms
step:697/1825 train_time:26602ms step_avg:38.17ms
step:698/1825 train_time:26665ms step_avg:38.20ms
step:699/1825 train_time:26724ms step_avg:38.23ms
step:700/1825 train_time:26787ms step_avg:38.27ms
step:701/1825 train_time:26847ms step_avg:38.30ms
step:702/1825 train_time:26911ms step_avg:38.33ms
step:703/1825 train_time:26971ms step_avg:38.37ms
step:704/1825 train_time:27034ms step_avg:38.40ms
step:705/1825 train_time:27095ms step_avg:38.43ms
step:706/1825 train_time:27158ms step_avg:38.47ms
step:707/1825 train_time:27218ms step_avg:38.50ms
step:708/1825 train_time:27281ms step_avg:38.53ms
step:709/1825 train_time:27341ms step_avg:38.56ms
step:710/1825 train_time:27403ms step_avg:38.60ms
step:711/1825 train_time:27463ms step_avg:38.63ms
step:712/1825 train_time:27526ms step_avg:38.66ms
step:713/1825 train_time:27586ms step_avg:38.69ms
step:714/1825 train_time:27649ms step_avg:38.72ms
step:715/1825 train_time:27709ms step_avg:38.75ms
step:716/1825 train_time:27772ms step_avg:38.79ms
step:717/1825 train_time:27832ms step_avg:38.82ms
step:718/1825 train_time:27895ms step_avg:38.85ms
step:719/1825 train_time:27956ms step_avg:38.88ms
step:720/1825 train_time:28019ms step_avg:38.91ms
step:721/1825 train_time:28080ms step_avg:38.95ms
step:722/1825 train_time:28142ms step_avg:38.98ms
step:723/1825 train_time:28202ms step_avg:39.01ms
step:724/1825 train_time:28264ms step_avg:39.04ms
step:725/1825 train_time:28325ms step_avg:39.07ms
step:726/1825 train_time:28388ms step_avg:39.10ms
step:727/1825 train_time:28449ms step_avg:39.13ms
step:728/1825 train_time:28512ms step_avg:39.17ms
step:729/1825 train_time:28572ms step_avg:39.19ms
step:730/1825 train_time:28635ms step_avg:39.23ms
step:731/1825 train_time:28695ms step_avg:39.25ms
step:732/1825 train_time:28758ms step_avg:39.29ms
step:733/1825 train_time:28818ms step_avg:39.32ms
step:734/1825 train_time:28880ms step_avg:39.35ms
step:735/1825 train_time:28940ms step_avg:39.37ms
step:736/1825 train_time:29003ms step_avg:39.41ms
step:737/1825 train_time:29063ms step_avg:39.43ms
step:738/1825 train_time:29126ms step_avg:39.47ms
step:739/1825 train_time:29187ms step_avg:39.50ms
step:740/1825 train_time:29251ms step_avg:39.53ms
step:741/1825 train_time:29311ms step_avg:39.56ms
step:742/1825 train_time:29374ms step_avg:39.59ms
step:743/1825 train_time:29435ms step_avg:39.62ms
step:744/1825 train_time:29497ms step_avg:39.65ms
step:745/1825 train_time:29558ms step_avg:39.67ms
step:746/1825 train_time:29620ms step_avg:39.71ms
step:747/1825 train_time:29680ms step_avg:39.73ms
step:748/1825 train_time:29743ms step_avg:39.76ms
step:749/1825 train_time:29803ms step_avg:39.79ms
step:750/1825 train_time:29865ms step_avg:39.82ms
step:750/1825 val_loss:4.0261 train_time:29937ms step_avg:39.92ms
step:751/1825 train_time:29961ms step_avg:39.89ms
step:752/1825 train_time:29992ms step_avg:39.88ms
step:753/1825 train_time:30056ms step_avg:39.92ms
step:754/1825 train_time:30121ms step_avg:39.95ms
step:755/1825 train_time:30185ms step_avg:39.98ms
step:756/1825 train_time:30248ms step_avg:40.01ms
step:757/1825 train_time:30307ms step_avg:40.04ms
step:758/1825 train_time:30369ms step_avg:40.06ms
step:759/1825 train_time:30429ms step_avg:40.09ms
step:760/1825 train_time:30491ms step_avg:40.12ms
step:761/1825 train_time:30551ms step_avg:40.15ms
step:762/1825 train_time:30613ms step_avg:40.17ms
step:763/1825 train_time:30673ms step_avg:40.20ms
step:764/1825 train_time:30735ms step_avg:40.23ms
step:765/1825 train_time:30795ms step_avg:40.25ms
step:766/1825 train_time:30858ms step_avg:40.28ms
step:767/1825 train_time:30918ms step_avg:40.31ms
step:768/1825 train_time:30980ms step_avg:40.34ms
step:769/1825 train_time:31042ms step_avg:40.37ms
step:770/1825 train_time:31106ms step_avg:40.40ms
step:771/1825 train_time:31166ms step_avg:40.42ms
step:772/1825 train_time:31230ms step_avg:40.45ms
step:773/1825 train_time:31289ms step_avg:40.48ms
step:774/1825 train_time:31352ms step_avg:40.51ms
step:775/1825 train_time:31412ms step_avg:40.53ms
step:776/1825 train_time:31474ms step_avg:40.56ms
step:777/1825 train_time:31533ms step_avg:40.58ms
step:778/1825 train_time:31596ms step_avg:40.61ms
step:779/1825 train_time:31656ms step_avg:40.64ms
step:780/1825 train_time:31718ms step_avg:40.66ms
step:781/1825 train_time:31779ms step_avg:40.69ms
step:782/1825 train_time:31841ms step_avg:40.72ms
step:783/1825 train_time:31902ms step_avg:40.74ms
step:784/1825 train_time:31964ms step_avg:40.77ms
step:785/1825 train_time:32025ms step_avg:40.80ms
step:786/1825 train_time:32089ms step_avg:40.83ms
step:787/1825 train_time:32150ms step_avg:40.85ms
step:788/1825 train_time:32212ms step_avg:40.88ms
step:789/1825 train_time:32272ms step_avg:40.90ms
step:790/1825 train_time:32336ms step_avg:40.93ms
step:791/1825 train_time:32396ms step_avg:40.96ms
step:792/1825 train_time:32458ms step_avg:40.98ms
step:793/1825 train_time:32519ms step_avg:41.01ms
step:794/1825 train_time:32583ms step_avg:41.04ms
step:795/1825 train_time:32642ms step_avg:41.06ms
step:796/1825 train_time:32705ms step_avg:41.09ms
step:797/1825 train_time:32764ms step_avg:41.11ms
step:798/1825 train_time:32827ms step_avg:41.14ms
step:799/1825 train_time:32887ms step_avg:41.16ms
step:800/1825 train_time:32950ms step_avg:41.19ms
step:801/1825 train_time:33011ms step_avg:41.21ms
step:802/1825 train_time:33074ms step_avg:41.24ms
step:803/1825 train_time:33135ms step_avg:41.26ms
step:804/1825 train_time:33198ms step_avg:41.29ms
step:805/1825 train_time:33257ms step_avg:41.31ms
step:806/1825 train_time:33321ms step_avg:41.34ms
step:807/1825 train_time:33381ms step_avg:41.36ms
step:808/1825 train_time:33444ms step_avg:41.39ms
step:809/1825 train_time:33505ms step_avg:41.42ms
step:810/1825 train_time:33568ms step_avg:41.44ms
step:811/1825 train_time:33628ms step_avg:41.47ms
step:812/1825 train_time:33691ms step_avg:41.49ms
step:813/1825 train_time:33751ms step_avg:41.51ms
step:814/1825 train_time:33814ms step_avg:41.54ms
step:815/1825 train_time:33873ms step_avg:41.56ms
step:816/1825 train_time:33936ms step_avg:41.59ms
step:817/1825 train_time:33996ms step_avg:41.61ms
step:818/1825 train_time:34058ms step_avg:41.64ms
step:819/1825 train_time:34119ms step_avg:41.66ms
step:820/1825 train_time:34181ms step_avg:41.68ms
step:821/1825 train_time:34241ms step_avg:41.71ms
step:822/1825 train_time:34305ms step_avg:41.73ms
step:823/1825 train_time:34365ms step_avg:41.76ms
step:824/1825 train_time:34428ms step_avg:41.78ms
step:825/1825 train_time:34488ms step_avg:41.80ms
step:826/1825 train_time:34551ms step_avg:41.83ms
step:827/1825 train_time:34611ms step_avg:41.85ms
step:828/1825 train_time:34674ms step_avg:41.88ms
step:829/1825 train_time:34734ms step_avg:41.90ms
step:830/1825 train_time:34797ms step_avg:41.92ms
step:831/1825 train_time:34857ms step_avg:41.95ms
step:832/1825 train_time:34919ms step_avg:41.97ms
step:833/1825 train_time:34979ms step_avg:41.99ms
step:834/1825 train_time:35042ms step_avg:42.02ms
step:835/1825 train_time:35103ms step_avg:42.04ms
step:836/1825 train_time:35165ms step_avg:42.06ms
step:837/1825 train_time:35225ms step_avg:42.08ms
step:838/1825 train_time:35288ms step_avg:42.11ms
step:839/1825 train_time:35348ms step_avg:42.13ms
step:840/1825 train_time:35411ms step_avg:42.16ms
step:841/1825 train_time:35471ms step_avg:42.18ms
step:842/1825 train_time:35534ms step_avg:42.20ms
step:843/1825 train_time:35594ms step_avg:42.22ms
step:844/1825 train_time:35657ms step_avg:42.25ms
step:845/1825 train_time:35717ms step_avg:42.27ms
step:846/1825 train_time:35779ms step_avg:42.29ms
step:847/1825 train_time:35839ms step_avg:42.31ms
step:848/1825 train_time:35903ms step_avg:42.34ms
step:849/1825 train_time:35963ms step_avg:42.36ms
step:850/1825 train_time:36026ms step_avg:42.38ms
step:851/1825 train_time:36087ms step_avg:42.40ms
step:852/1825 train_time:36149ms step_avg:42.43ms
step:853/1825 train_time:36210ms step_avg:42.45ms
step:854/1825 train_time:36273ms step_avg:42.47ms
step:855/1825 train_time:36334ms step_avg:42.50ms
step:856/1825 train_time:36397ms step_avg:42.52ms
step:857/1825 train_time:36457ms step_avg:42.54ms
step:858/1825 train_time:36520ms step_avg:42.56ms
step:859/1825 train_time:36580ms step_avg:42.58ms
step:860/1825 train_time:36643ms step_avg:42.61ms
step:861/1825 train_time:36703ms step_avg:42.63ms
step:862/1825 train_time:36767ms step_avg:42.65ms
step:863/1825 train_time:36826ms step_avg:42.67ms
step:864/1825 train_time:36889ms step_avg:42.70ms
step:865/1825 train_time:36950ms step_avg:42.72ms
step:866/1825 train_time:37014ms step_avg:42.74ms
step:867/1825 train_time:37075ms step_avg:42.76ms
step:868/1825 train_time:37137ms step_avg:42.78ms
step:869/1825 train_time:37197ms step_avg:42.80ms
step:870/1825 train_time:37260ms step_avg:42.83ms
step:871/1825 train_time:37319ms step_avg:42.85ms
step:872/1825 train_time:37383ms step_avg:42.87ms
step:873/1825 train_time:37444ms step_avg:42.89ms
step:874/1825 train_time:37509ms step_avg:42.92ms
step:875/1825 train_time:37568ms step_avg:42.94ms
step:876/1825 train_time:37631ms step_avg:42.96ms
step:877/1825 train_time:37691ms step_avg:42.98ms
step:878/1825 train_time:37753ms step_avg:43.00ms
step:879/1825 train_time:37813ms step_avg:43.02ms
step:880/1825 train_time:37876ms step_avg:43.04ms
step:881/1825 train_time:37936ms step_avg:43.06ms
step:882/1825 train_time:37998ms step_avg:43.08ms
step:883/1825 train_time:38058ms step_avg:43.10ms
step:884/1825 train_time:38121ms step_avg:43.12ms
step:885/1825 train_time:38181ms step_avg:43.14ms
step:886/1825 train_time:38244ms step_avg:43.17ms
step:887/1825 train_time:38304ms step_avg:43.18ms
step:888/1825 train_time:38368ms step_avg:43.21ms
step:889/1825 train_time:38428ms step_avg:43.23ms
step:890/1825 train_time:38491ms step_avg:43.25ms
step:891/1825 train_time:38552ms step_avg:43.27ms
step:892/1825 train_time:38615ms step_avg:43.29ms
step:893/1825 train_time:38675ms step_avg:43.31ms
step:894/1825 train_time:38738ms step_avg:43.33ms
step:895/1825 train_time:38798ms step_avg:43.35ms
step:896/1825 train_time:38860ms step_avg:43.37ms
step:897/1825 train_time:38920ms step_avg:43.39ms
step:898/1825 train_time:38984ms step_avg:43.41ms
step:899/1825 train_time:39044ms step_avg:43.43ms
step:900/1825 train_time:39107ms step_avg:43.45ms
step:901/1825 train_time:39168ms step_avg:43.47ms
step:902/1825 train_time:39231ms step_avg:43.49ms
step:903/1825 train_time:39291ms step_avg:43.51ms
step:904/1825 train_time:39354ms step_avg:43.53ms
step:905/1825 train_time:39414ms step_avg:43.55ms
step:906/1825 train_time:39477ms step_avg:43.57ms
step:907/1825 train_time:39537ms step_avg:43.59ms
step:908/1825 train_time:39599ms step_avg:43.61ms
step:909/1825 train_time:39659ms step_avg:43.63ms
step:910/1825 train_time:39723ms step_avg:43.65ms
step:911/1825 train_time:39784ms step_avg:43.67ms
step:912/1825 train_time:39847ms step_avg:43.69ms
step:913/1825 train_time:39907ms step_avg:43.71ms
step:914/1825 train_time:39969ms step_avg:43.73ms
step:915/1825 train_time:40030ms step_avg:43.75ms
step:916/1825 train_time:40092ms step_avg:43.77ms
step:917/1825 train_time:40152ms step_avg:43.79ms
step:918/1825 train_time:40215ms step_avg:43.81ms
step:919/1825 train_time:40275ms step_avg:43.82ms
step:920/1825 train_time:40337ms step_avg:43.84ms
step:921/1825 train_time:40398ms step_avg:43.86ms
step:922/1825 train_time:40460ms step_avg:43.88ms
step:923/1825 train_time:40520ms step_avg:43.90ms
step:924/1825 train_time:40583ms step_avg:43.92ms
step:925/1825 train_time:40643ms step_avg:43.94ms
step:926/1825 train_time:40707ms step_avg:43.96ms
step:927/1825 train_time:40767ms step_avg:43.98ms
step:928/1825 train_time:40829ms step_avg:44.00ms
step:929/1825 train_time:40889ms step_avg:44.01ms
step:930/1825 train_time:40953ms step_avg:44.04ms
step:931/1825 train_time:41013ms step_avg:44.05ms
step:932/1825 train_time:41075ms step_avg:44.07ms
step:933/1825 train_time:41135ms step_avg:44.09ms
step:934/1825 train_time:41198ms step_avg:44.11ms
step:935/1825 train_time:41258ms step_avg:44.13ms
step:936/1825 train_time:41321ms step_avg:44.15ms
step:937/1825 train_time:41382ms step_avg:44.16ms
step:938/1825 train_time:41444ms step_avg:44.18ms
step:939/1825 train_time:41504ms step_avg:44.20ms
step:940/1825 train_time:41567ms step_avg:44.22ms
step:941/1825 train_time:41628ms step_avg:44.24ms
step:942/1825 train_time:41691ms step_avg:44.26ms
step:943/1825 train_time:41751ms step_avg:44.27ms
step:944/1825 train_time:41813ms step_avg:44.29ms
step:945/1825 train_time:41873ms step_avg:44.31ms
step:946/1825 train_time:41936ms step_avg:44.33ms
step:947/1825 train_time:41997ms step_avg:44.35ms
step:948/1825 train_time:42059ms step_avg:44.37ms
step:949/1825 train_time:42119ms step_avg:44.38ms
step:950/1825 train_time:42183ms step_avg:44.40ms
step:951/1825 train_time:42243ms step_avg:44.42ms
step:952/1825 train_time:42306ms step_avg:44.44ms
step:953/1825 train_time:42366ms step_avg:44.46ms
step:954/1825 train_time:42429ms step_avg:44.47ms
step:955/1825 train_time:42489ms step_avg:44.49ms
step:956/1825 train_time:42552ms step_avg:44.51ms
step:957/1825 train_time:42612ms step_avg:44.53ms
step:958/1825 train_time:42675ms step_avg:44.55ms
step:959/1825 train_time:42735ms step_avg:44.56ms
step:960/1825 train_time:42798ms step_avg:44.58ms
step:961/1825 train_time:42857ms step_avg:44.60ms
step:962/1825 train_time:42920ms step_avg:44.62ms
step:963/1825 train_time:42980ms step_avg:44.63ms
step:964/1825 train_time:43043ms step_avg:44.65ms
step:965/1825 train_time:43104ms step_avg:44.67ms
step:966/1825 train_time:43166ms step_avg:44.69ms
step:967/1825 train_time:43225ms step_avg:44.70ms
step:968/1825 train_time:43289ms step_avg:44.72ms
step:969/1825 train_time:43349ms step_avg:44.74ms
step:970/1825 train_time:43412ms step_avg:44.75ms
step:971/1825 train_time:43472ms step_avg:44.77ms
step:972/1825 train_time:43536ms step_avg:44.79ms
step:973/1825 train_time:43596ms step_avg:44.81ms
step:974/1825 train_time:43658ms step_avg:44.82ms
step:975/1825 train_time:43718ms step_avg:44.84ms
step:976/1825 train_time:43781ms step_avg:44.86ms
step:977/1825 train_time:43841ms step_avg:44.87ms
step:978/1825 train_time:43904ms step_avg:44.89ms
step:979/1825 train_time:43964ms step_avg:44.91ms
step:980/1825 train_time:44027ms step_avg:44.93ms
step:981/1825 train_time:44088ms step_avg:44.94ms
step:982/1825 train_time:44151ms step_avg:44.96ms
step:983/1825 train_time:44211ms step_avg:44.98ms
step:984/1825 train_time:44273ms step_avg:44.99ms
step:985/1825 train_time:44333ms step_avg:45.01ms
step:986/1825 train_time:44396ms step_avg:45.03ms
step:987/1825 train_time:44456ms step_avg:45.04ms
step:988/1825 train_time:44518ms step_avg:45.06ms
step:989/1825 train_time:44578ms step_avg:45.07ms
step:990/1825 train_time:44642ms step_avg:45.09ms
step:991/1825 train_time:44702ms step_avg:45.11ms
step:992/1825 train_time:44765ms step_avg:45.13ms
step:993/1825 train_time:44825ms step_avg:45.14ms
step:994/1825 train_time:44888ms step_avg:45.16ms
step:995/1825 train_time:44948ms step_avg:45.17ms
step:996/1825 train_time:45011ms step_avg:45.19ms
step:997/1825 train_time:45071ms step_avg:45.21ms
step:998/1825 train_time:45134ms step_avg:45.22ms
step:999/1825 train_time:45194ms step_avg:45.24ms
step:1000/1825 train_time:45256ms step_avg:45.26ms
step:1000/1825 val_loss:3.7743 train_time:45326ms step_avg:45.33ms
step:1001/1825 train_time:45344ms step_avg:45.30ms
step:1002/1825 train_time:45379ms step_avg:45.29ms
step:1003/1825 train_time:45442ms step_avg:45.31ms
step:1004/1825 train_time:45508ms step_avg:45.33ms
step:1005/1825 train_time:45570ms step_avg:45.34ms
step:1006/1825 train_time:45633ms step_avg:45.36ms
step:1007/1825 train_time:45694ms step_avg:45.38ms
step:1008/1825 train_time:45758ms step_avg:45.39ms
step:1009/1825 train_time:45818ms step_avg:45.41ms
step:1010/1825 train_time:45881ms step_avg:45.43ms
step:1011/1825 train_time:45940ms step_avg:45.44ms
step:1012/1825 train_time:46002ms step_avg:45.46ms
step:1013/1825 train_time:46062ms step_avg:45.47ms
step:1014/1825 train_time:46124ms step_avg:45.49ms
step:1015/1825 train_time:46183ms step_avg:45.50ms
step:1016/1825 train_time:46245ms step_avg:45.52ms
step:1017/1825 train_time:46306ms step_avg:45.53ms
step:1018/1825 train_time:46370ms step_avg:45.55ms
step:1019/1825 train_time:46430ms step_avg:45.56ms
step:1020/1825 train_time:46494ms step_avg:45.58ms
step:1021/1825 train_time:46554ms step_avg:45.60ms
step:1022/1825 train_time:46618ms step_avg:45.61ms
step:1023/1825 train_time:46678ms step_avg:45.63ms
step:1024/1825 train_time:46742ms step_avg:45.65ms
step:1025/1825 train_time:46802ms step_avg:45.66ms
step:1026/1825 train_time:46864ms step_avg:45.68ms
step:1027/1825 train_time:46924ms step_avg:45.69ms
step:1028/1825 train_time:46987ms step_avg:45.71ms
step:1029/1825 train_time:47046ms step_avg:45.72ms
step:1030/1825 train_time:47108ms step_avg:45.74ms
step:1031/1825 train_time:47168ms step_avg:45.75ms
step:1032/1825 train_time:47230ms step_avg:45.77ms
step:1033/1825 train_time:47289ms step_avg:45.78ms
step:1034/1825 train_time:47352ms step_avg:45.80ms
step:1035/1825 train_time:47412ms step_avg:45.81ms
step:1036/1825 train_time:47475ms step_avg:45.83ms
step:1037/1825 train_time:47537ms step_avg:45.84ms
step:1038/1825 train_time:47600ms step_avg:45.86ms
step:1039/1825 train_time:47660ms step_avg:45.87ms
step:1040/1825 train_time:47723ms step_avg:45.89ms
step:1041/1825 train_time:47783ms step_avg:45.90ms
step:1042/1825 train_time:47846ms step_avg:45.92ms
step:1043/1825 train_time:47906ms step_avg:45.93ms
step:1044/1825 train_time:47968ms step_avg:45.95ms
step:1045/1825 train_time:48028ms step_avg:45.96ms
step:1046/1825 train_time:48090ms step_avg:45.98ms
step:1047/1825 train_time:48150ms step_avg:45.99ms
step:1048/1825 train_time:48214ms step_avg:46.01ms
step:1049/1825 train_time:48274ms step_avg:46.02ms
step:1050/1825 train_time:48337ms step_avg:46.04ms
step:1051/1825 train_time:48397ms step_avg:46.05ms
step:1052/1825 train_time:48461ms step_avg:46.07ms
step:1053/1825 train_time:48520ms step_avg:46.08ms
step:1054/1825 train_time:48584ms step_avg:46.09ms
step:1055/1825 train_time:48644ms step_avg:46.11ms
step:1056/1825 train_time:48708ms step_avg:46.12ms
step:1057/1825 train_time:48768ms step_avg:46.14ms
step:1058/1825 train_time:48830ms step_avg:46.15ms
step:1059/1825 train_time:48891ms step_avg:46.17ms
step:1060/1825 train_time:48955ms step_avg:46.18ms
step:1061/1825 train_time:49015ms step_avg:46.20ms
step:1062/1825 train_time:49078ms step_avg:46.21ms
step:1063/1825 train_time:49138ms step_avg:46.23ms
step:1064/1825 train_time:49200ms step_avg:46.24ms
step:1065/1825 train_time:49261ms step_avg:46.25ms
step:1066/1825 train_time:49324ms step_avg:46.27ms
step:1067/1825 train_time:49384ms step_avg:46.28ms
step:1068/1825 train_time:49447ms step_avg:46.30ms
step:1069/1825 train_time:49508ms step_avg:46.31ms
step:1070/1825 train_time:49570ms step_avg:46.33ms
step:1071/1825 train_time:49631ms step_avg:46.34ms
step:1072/1825 train_time:49693ms step_avg:46.36ms
step:1073/1825 train_time:49754ms step_avg:46.37ms
step:1074/1825 train_time:49819ms step_avg:46.39ms
step:1075/1825 train_time:49880ms step_avg:46.40ms
step:1076/1825 train_time:49943ms step_avg:46.42ms
step:1077/1825 train_time:50003ms step_avg:46.43ms
step:1078/1825 train_time:50066ms step_avg:46.44ms
step:1079/1825 train_time:50126ms step_avg:46.46ms
step:1080/1825 train_time:50188ms step_avg:46.47ms
step:1081/1825 train_time:50248ms step_avg:46.48ms
step:1082/1825 train_time:50310ms step_avg:46.50ms
step:1083/1825 train_time:50371ms step_avg:46.51ms
step:1084/1825 train_time:50434ms step_avg:46.53ms
step:1085/1825 train_time:50494ms step_avg:46.54ms
step:1086/1825 train_time:50558ms step_avg:46.55ms
step:1087/1825 train_time:50618ms step_avg:46.57ms
step:1088/1825 train_time:50682ms step_avg:46.58ms
step:1089/1825 train_time:50742ms step_avg:46.60ms
step:1090/1825 train_time:50805ms step_avg:46.61ms
step:1091/1825 train_time:50865ms step_avg:46.62ms
step:1092/1825 train_time:50928ms step_avg:46.64ms
step:1093/1825 train_time:50988ms step_avg:46.65ms
step:1094/1825 train_time:51050ms step_avg:46.66ms
step:1095/1825 train_time:51110ms step_avg:46.68ms
step:1096/1825 train_time:51172ms step_avg:46.69ms
step:1097/1825 train_time:51233ms step_avg:46.70ms
step:1098/1825 train_time:51295ms step_avg:46.72ms
step:1099/1825 train_time:51355ms step_avg:46.73ms
step:1100/1825 train_time:51419ms step_avg:46.74ms
step:1101/1825 train_time:51478ms step_avg:46.76ms
step:1102/1825 train_time:51542ms step_avg:46.77ms
step:1103/1825 train_time:51603ms step_avg:46.78ms
step:1104/1825 train_time:51666ms step_avg:46.80ms
step:1105/1825 train_time:51727ms step_avg:46.81ms
step:1106/1825 train_time:51789ms step_avg:46.83ms
step:1107/1825 train_time:51849ms step_avg:46.84ms
step:1108/1825 train_time:51912ms step_avg:46.85ms
step:1109/1825 train_time:51972ms step_avg:46.86ms
step:1110/1825 train_time:52035ms step_avg:46.88ms
step:1111/1825 train_time:52095ms step_avg:46.89ms
step:1112/1825 train_time:52158ms step_avg:46.90ms
step:1113/1825 train_time:52218ms step_avg:46.92ms
step:1114/1825 train_time:52282ms step_avg:46.93ms
step:1115/1825 train_time:52342ms step_avg:46.94ms
step:1116/1825 train_time:52405ms step_avg:46.96ms
step:1117/1825 train_time:52465ms step_avg:46.97ms
step:1118/1825 train_time:52527ms step_avg:46.98ms
step:1119/1825 train_time:52588ms step_avg:47.00ms
step:1120/1825 train_time:52651ms step_avg:47.01ms
step:1121/1825 train_time:52711ms step_avg:47.02ms
step:1122/1825 train_time:52774ms step_avg:47.04ms
step:1123/1825 train_time:52835ms step_avg:47.05ms
step:1124/1825 train_time:52897ms step_avg:47.06ms
step:1125/1825 train_time:52958ms step_avg:47.07ms
step:1126/1825 train_time:53020ms step_avg:47.09ms
step:1127/1825 train_time:53082ms step_avg:47.10ms
step:1128/1825 train_time:53145ms step_avg:47.11ms
step:1129/1825 train_time:53205ms step_avg:47.13ms
step:1130/1825 train_time:53267ms step_avg:47.14ms
step:1131/1825 train_time:53327ms step_avg:47.15ms
step:1132/1825 train_time:53389ms step_avg:47.16ms
step:1133/1825 train_time:53449ms step_avg:47.17ms
step:1134/1825 train_time:53512ms step_avg:47.19ms
step:1135/1825 train_time:53572ms step_avg:47.20ms
step:1136/1825 train_time:53635ms step_avg:47.21ms
step:1137/1825 train_time:53696ms step_avg:47.23ms
step:1138/1825 train_time:53760ms step_avg:47.24ms
step:1139/1825 train_time:53821ms step_avg:47.25ms
step:1140/1825 train_time:53883ms step_avg:47.27ms
step:1141/1825 train_time:53944ms step_avg:47.28ms
step:1142/1825 train_time:54006ms step_avg:47.29ms
step:1143/1825 train_time:54065ms step_avg:47.30ms
step:1144/1825 train_time:54128ms step_avg:47.32ms
step:1145/1825 train_time:54189ms step_avg:47.33ms
step:1146/1825 train_time:54251ms step_avg:47.34ms
step:1147/1825 train_time:54311ms step_avg:47.35ms
step:1148/1825 train_time:54374ms step_avg:47.36ms
step:1149/1825 train_time:54434ms step_avg:47.37ms
step:1150/1825 train_time:54497ms step_avg:47.39ms
step:1151/1825 train_time:54557ms step_avg:47.40ms
step:1152/1825 train_time:54620ms step_avg:47.41ms
step:1153/1825 train_time:54680ms step_avg:47.42ms
step:1154/1825 train_time:54743ms step_avg:47.44ms
step:1155/1825 train_time:54803ms step_avg:47.45ms
step:1156/1825 train_time:54866ms step_avg:47.46ms
step:1157/1825 train_time:54926ms step_avg:47.47ms
step:1158/1825 train_time:54988ms step_avg:47.49ms
step:1159/1825 train_time:55048ms step_avg:47.50ms
step:1160/1825 train_time:55111ms step_avg:47.51ms
step:1161/1825 train_time:55171ms step_avg:47.52ms
step:1162/1825 train_time:55234ms step_avg:47.53ms
step:1163/1825 train_time:55294ms step_avg:47.54ms
step:1164/1825 train_time:55358ms step_avg:47.56ms
step:1165/1825 train_time:55418ms step_avg:47.57ms
step:1166/1825 train_time:55481ms step_avg:47.58ms
step:1167/1825 train_time:55542ms step_avg:47.59ms
step:1168/1825 train_time:55605ms step_avg:47.61ms
step:1169/1825 train_time:55665ms step_avg:47.62ms
step:1170/1825 train_time:55727ms step_avg:47.63ms
step:1171/1825 train_time:55787ms step_avg:47.64ms
step:1172/1825 train_time:55850ms step_avg:47.65ms
step:1173/1825 train_time:55910ms step_avg:47.66ms
step:1174/1825 train_time:55973ms step_avg:47.68ms
step:1175/1825 train_time:56033ms step_avg:47.69ms
step:1176/1825 train_time:56096ms step_avg:47.70ms
step:1177/1825 train_time:56156ms step_avg:47.71ms
step:1178/1825 train_time:56220ms step_avg:47.72ms
step:1179/1825 train_time:56280ms step_avg:47.74ms
step:1180/1825 train_time:56343ms step_avg:47.75ms
step:1181/1825 train_time:56404ms step_avg:47.76ms
step:1182/1825 train_time:56466ms step_avg:47.77ms
step:1183/1825 train_time:56526ms step_avg:47.78ms
step:1184/1825 train_time:56589ms step_avg:47.79ms
step:1185/1825 train_time:56649ms step_avg:47.80ms
step:1186/1825 train_time:56712ms step_avg:47.82ms
step:1187/1825 train_time:56771ms step_avg:47.83ms
step:1188/1825 train_time:56835ms step_avg:47.84ms
step:1189/1825 train_time:56895ms step_avg:47.85ms
step:1190/1825 train_time:56959ms step_avg:47.86ms
step:1191/1825 train_time:57020ms step_avg:47.88ms
step:1192/1825 train_time:57107ms step_avg:47.91ms
step:1193/1825 train_time:57195ms step_avg:47.94ms
step:1194/1825 train_time:57283ms step_avg:47.98ms
step:1195/1825 train_time:57370ms step_avg:48.01ms
step:1196/1825 train_time:57460ms step_avg:48.04ms
step:1197/1825 train_time:57546ms step_avg:48.08ms
step:1198/1825 train_time:57638ms step_avg:48.11ms
step:1199/1825 train_time:57725ms step_avg:48.14ms
step:1200/1825 train_time:57814ms step_avg:48.18ms
step:1201/1825 train_time:57902ms step_avg:48.21ms
step:1202/1825 train_time:57991ms step_avg:48.25ms
step:1203/1825 train_time:58078ms step_avg:48.28ms
step:1204/1825 train_time:58166ms step_avg:48.31ms
step:1205/1825 train_time:58253ms step_avg:48.34ms
step:1206/1825 train_time:58341ms step_avg:48.38ms
step:1207/1825 train_time:58429ms step_avg:48.41ms
step:1208/1825 train_time:58519ms step_avg:48.44ms
step:1209/1825 train_time:58606ms step_avg:48.47ms
step:1210/1825 train_time:58696ms step_avg:48.51ms
step:1211/1825 train_time:58783ms step_avg:48.54ms
step:1212/1825 train_time:58872ms step_avg:48.57ms
step:1213/1825 train_time:58958ms step_avg:48.61ms
step:1214/1825 train_time:59046ms step_avg:48.64ms
step:1215/1825 train_time:59134ms step_avg:48.67ms
step:1216/1825 train_time:59223ms step_avg:48.70ms
step:1217/1825 train_time:59309ms step_avg:48.73ms
step:1218/1825 train_time:59400ms step_avg:48.77ms
step:1219/1825 train_time:59487ms step_avg:48.80ms
step:1220/1825 train_time:59575ms step_avg:48.83ms
step:1221/1825 train_time:59662ms step_avg:48.86ms
step:1222/1825 train_time:59753ms step_avg:48.90ms
step:1223/1825 train_time:59840ms step_avg:48.93ms
step:1224/1825 train_time:59928ms step_avg:48.96ms
step:1225/1825 train_time:60014ms step_avg:48.99ms
step:1226/1825 train_time:60103ms step_avg:49.02ms
step:1227/1825 train_time:60188ms step_avg:49.05ms
step:1228/1825 train_time:60278ms step_avg:49.09ms
step:1229/1825 train_time:60365ms step_avg:49.12ms
step:1230/1825 train_time:60454ms step_avg:49.15ms
step:1231/1825 train_time:60540ms step_avg:49.18ms
step:1232/1825 train_time:60630ms step_avg:49.21ms
step:1233/1825 train_time:60716ms step_avg:49.24ms
step:1234/1825 train_time:60805ms step_avg:49.27ms
step:1235/1825 train_time:60894ms step_avg:49.31ms
step:1236/1825 train_time:60982ms step_avg:49.34ms
step:1237/1825 train_time:61068ms step_avg:49.37ms
step:1238/1825 train_time:61158ms step_avg:49.40ms
step:1239/1825 train_time:61244ms step_avg:49.43ms
step:1240/1825 train_time:61333ms step_avg:49.46ms
step:1241/1825 train_time:61420ms step_avg:49.49ms
step:1242/1825 train_time:61509ms step_avg:49.52ms
step:1243/1825 train_time:61597ms step_avg:49.55ms
step:1244/1825 train_time:61685ms step_avg:49.59ms
step:1245/1825 train_time:61771ms step_avg:49.62ms
step:1246/1825 train_time:61861ms step_avg:49.65ms
step:1247/1825 train_time:61947ms step_avg:49.68ms
step:1248/1825 train_time:62037ms step_avg:49.71ms
step:1249/1825 train_time:62123ms step_avg:49.74ms
step:1250/1825 train_time:62213ms step_avg:49.77ms
step:1250/1825 val_loss:3.5330 train_time:62309ms step_avg:49.85ms
step:1251/1825 train_time:62331ms step_avg:49.83ms
step:1252/1825 train_time:62388ms step_avg:49.83ms
step:1253/1825 train_time:62481ms step_avg:49.87ms
step:1254/1825 train_time:62572ms step_avg:49.90ms
step:1255/1825 train_time:62662ms step_avg:49.93ms
step:1256/1825 train_time:62749ms step_avg:49.96ms
step:1257/1825 train_time:62834ms step_avg:49.99ms
step:1258/1825 train_time:62923ms step_avg:50.02ms
step:1259/1825 train_time:63007ms step_avg:50.05ms
step:1260/1825 train_time:63096ms step_avg:50.08ms
step:1261/1825 train_time:63182ms step_avg:50.10ms
step:1262/1825 train_time:63271ms step_avg:50.14ms
step:1263/1825 train_time:63359ms step_avg:50.17ms
step:1264/1825 train_time:63450ms step_avg:50.20ms
step:1265/1825 train_time:63539ms step_avg:50.23ms
step:1266/1825 train_time:63629ms step_avg:50.26ms
step:1267/1825 train_time:63718ms step_avg:50.29ms
step:1268/1825 train_time:63806ms step_avg:50.32ms
step:1269/1825 train_time:63892ms step_avg:50.35ms
step:1270/1825 train_time:63980ms step_avg:50.38ms
step:1271/1825 train_time:64066ms step_avg:50.41ms
step:1272/1825 train_time:64155ms step_avg:50.44ms
step:1273/1825 train_time:64241ms step_avg:50.46ms
step:1274/1825 train_time:64330ms step_avg:50.49ms
step:1275/1825 train_time:64418ms step_avg:50.52ms
step:1276/1825 train_time:64509ms step_avg:50.56ms
step:1277/1825 train_time:64597ms step_avg:50.58ms
step:1278/1825 train_time:64686ms step_avg:50.62ms
step:1279/1825 train_time:64773ms step_avg:50.64ms
step:1280/1825 train_time:64861ms step_avg:50.67ms
step:1281/1825 train_time:64947ms step_avg:50.70ms
step:1282/1825 train_time:65038ms step_avg:50.73ms
step:1283/1825 train_time:65124ms step_avg:50.76ms
step:1284/1825 train_time:65213ms step_avg:50.79ms
step:1285/1825 train_time:65300ms step_avg:50.82ms
step:1286/1825 train_time:65389ms step_avg:50.85ms
step:1287/1825 train_time:65477ms step_avg:50.88ms
step:1288/1825 train_time:65567ms step_avg:50.91ms
step:1289/1825 train_time:65654ms step_avg:50.93ms
step:1290/1825 train_time:65743ms step_avg:50.96ms
step:1291/1825 train_time:65828ms step_avg:50.99ms
step:1292/1825 train_time:65917ms step_avg:51.02ms
step:1293/1825 train_time:66004ms step_avg:51.05ms
step:1294/1825 train_time:66092ms step_avg:51.08ms
step:1295/1825 train_time:66178ms step_avg:51.10ms
step:1296/1825 train_time:66266ms step_avg:51.13ms
step:1297/1825 train_time:66353ms step_avg:51.16ms
step:1298/1825 train_time:66444ms step_avg:51.19ms
step:1299/1825 train_time:66531ms step_avg:51.22ms
step:1300/1825 train_time:66621ms step_avg:51.25ms
step:1301/1825 train_time:66707ms step_avg:51.27ms
step:1302/1825 train_time:66797ms step_avg:51.30ms
step:1303/1825 train_time:66883ms step_avg:51.33ms
step:1304/1825 train_time:66972ms step_avg:51.36ms
step:1305/1825 train_time:67058ms step_avg:51.39ms
step:1306/1825 train_time:67146ms step_avg:51.41ms
step:1307/1825 train_time:67232ms step_avg:51.44ms
step:1308/1825 train_time:67322ms step_avg:51.47ms
step:1309/1825 train_time:67409ms step_avg:51.50ms
step:1310/1825 train_time:67498ms step_avg:51.53ms
step:1311/1825 train_time:67585ms step_avg:51.55ms
step:1312/1825 train_time:67674ms step_avg:51.58ms
step:1313/1825 train_time:67760ms step_avg:51.61ms
step:1314/1825 train_time:67849ms step_avg:51.64ms
step:1315/1825 train_time:67934ms step_avg:51.66ms
step:1316/1825 train_time:68025ms step_avg:51.69ms
step:1317/1825 train_time:68109ms step_avg:51.72ms
step:1318/1825 train_time:68199ms step_avg:51.74ms
step:1319/1825 train_time:68286ms step_avg:51.77ms
step:1320/1825 train_time:68375ms step_avg:51.80ms
step:1321/1825 train_time:68462ms step_avg:51.83ms
step:1322/1825 train_time:68551ms step_avg:51.85ms
step:1323/1825 train_time:68638ms step_avg:51.88ms
step:1324/1825 train_time:68728ms step_avg:51.91ms
step:1325/1825 train_time:68814ms step_avg:51.94ms
step:1326/1825 train_time:68903ms step_avg:51.96ms
step:1327/1825 train_time:68990ms step_avg:51.99ms
step:1328/1825 train_time:69079ms step_avg:52.02ms
step:1329/1825 train_time:69165ms step_avg:52.04ms
step:1330/1825 train_time:69255ms step_avg:52.07ms
step:1331/1825 train_time:69341ms step_avg:52.10ms
step:1332/1825 train_time:69429ms step_avg:52.12ms
step:1333/1825 train_time:69516ms step_avg:52.15ms
step:1334/1825 train_time:69606ms step_avg:52.18ms
step:1335/1825 train_time:69692ms step_avg:52.20ms
step:1336/1825 train_time:69783ms step_avg:52.23ms
step:1337/1825 train_time:69870ms step_avg:52.26ms
step:1338/1825 train_time:69960ms step_avg:52.29ms
step:1339/1825 train_time:70046ms step_avg:52.31ms
step:1340/1825 train_time:70136ms step_avg:52.34ms
step:1341/1825 train_time:70223ms step_avg:52.37ms
step:1342/1825 train_time:70312ms step_avg:52.39ms
step:1343/1825 train_time:70400ms step_avg:52.42ms
step:1344/1825 train_time:70488ms step_avg:52.45ms
step:1345/1825 train_time:70574ms step_avg:52.47ms
step:1346/1825 train_time:70663ms step_avg:52.50ms
step:1347/1825 train_time:70750ms step_avg:52.52ms
step:1348/1825 train_time:70840ms step_avg:52.55ms
step:1349/1825 train_time:70926ms step_avg:52.58ms
step:1350/1825 train_time:71015ms step_avg:52.60ms
step:1351/1825 train_time:71102ms step_avg:52.63ms
step:1352/1825 train_time:71189ms step_avg:52.65ms
step:1353/1825 train_time:71277ms step_avg:52.68ms
step:1354/1825 train_time:71366ms step_avg:52.71ms
step:1355/1825 train_time:71453ms step_avg:52.73ms
step:1356/1825 train_time:71542ms step_avg:52.76ms
step:1357/1825 train_time:71628ms step_avg:52.78ms
step:1358/1825 train_time:71719ms step_avg:52.81ms
step:1359/1825 train_time:71805ms step_avg:52.84ms
step:1360/1825 train_time:71893ms step_avg:52.86ms
step:1361/1825 train_time:71980ms step_avg:52.89ms
step:1362/1825 train_time:72069ms step_avg:52.91ms
step:1363/1825 train_time:72156ms step_avg:52.94ms
step:1364/1825 train_time:72245ms step_avg:52.97ms
step:1365/1825 train_time:72331ms step_avg:52.99ms
step:1366/1825 train_time:72421ms step_avg:53.02ms
step:1367/1825 train_time:72508ms step_avg:53.04ms
step:1368/1825 train_time:72597ms step_avg:53.07ms
step:1369/1825 train_time:72683ms step_avg:53.09ms
step:1370/1825 train_time:72772ms step_avg:53.12ms
step:1371/1825 train_time:72858ms step_avg:53.14ms
step:1372/1825 train_time:72947ms step_avg:53.17ms
step:1373/1825 train_time:73032ms step_avg:53.19ms
step:1374/1825 train_time:73122ms step_avg:53.22ms
step:1375/1825 train_time:73208ms step_avg:53.24ms
step:1376/1825 train_time:73298ms step_avg:53.27ms
step:1377/1825 train_time:73384ms step_avg:53.29ms
step:1378/1825 train_time:73473ms step_avg:53.32ms
step:1379/1825 train_time:73561ms step_avg:53.34ms
step:1380/1825 train_time:73649ms step_avg:53.37ms
step:1381/1825 train_time:73736ms step_avg:53.39ms
step:1382/1825 train_time:73825ms step_avg:53.42ms
step:1383/1825 train_time:73911ms step_avg:53.44ms
step:1384/1825 train_time:74002ms step_avg:53.47ms
step:1385/1825 train_time:74089ms step_avg:53.49ms
step:1386/1825 train_time:74179ms step_avg:53.52ms
step:1387/1825 train_time:74266ms step_avg:53.54ms
step:1388/1825 train_time:74354ms step_avg:53.57ms
step:1389/1825 train_time:74441ms step_avg:53.59ms
step:1390/1825 train_time:74529ms step_avg:53.62ms
step:1391/1825 train_time:74617ms step_avg:53.64ms
step:1392/1825 train_time:74705ms step_avg:53.67ms
step:1393/1825 train_time:74791ms step_avg:53.69ms
step:1394/1825 train_time:74882ms step_avg:53.72ms
step:1395/1825 train_time:74968ms step_avg:53.74ms
step:1396/1825 train_time:75057ms step_avg:53.77ms
step:1397/1825 train_time:75144ms step_avg:53.79ms
step:1398/1825 train_time:75232ms step_avg:53.81ms
step:1399/1825 train_time:75319ms step_avg:53.84ms
step:1400/1825 train_time:75407ms step_avg:53.86ms
step:1401/1825 train_time:75495ms step_avg:53.89ms
step:1402/1825 train_time:75585ms step_avg:53.91ms
step:1403/1825 train_time:75670ms step_avg:53.93ms
step:1404/1825 train_time:75760ms step_avg:53.96ms
step:1405/1825 train_time:75846ms step_avg:53.98ms
step:1406/1825 train_time:75935ms step_avg:54.01ms
step:1407/1825 train_time:76021ms step_avg:54.03ms
step:1408/1825 train_time:76109ms step_avg:54.05ms
step:1409/1825 train_time:76196ms step_avg:54.08ms
step:1410/1825 train_time:76286ms step_avg:54.10ms
step:1411/1825 train_time:76372ms step_avg:54.13ms
step:1412/1825 train_time:76463ms step_avg:54.15ms
step:1413/1825 train_time:76549ms step_avg:54.18ms
step:1414/1825 train_time:76638ms step_avg:54.20ms
step:1415/1825 train_time:76724ms step_avg:54.22ms
step:1416/1825 train_time:76814ms step_avg:54.25ms
step:1417/1825 train_time:76901ms step_avg:54.27ms
step:1418/1825 train_time:76989ms step_avg:54.29ms
step:1419/1825 train_time:77076ms step_avg:54.32ms
step:1420/1825 train_time:77165ms step_avg:54.34ms
step:1421/1825 train_time:77252ms step_avg:54.36ms
step:1422/1825 train_time:77341ms step_avg:54.39ms
step:1423/1825 train_time:77427ms step_avg:54.41ms
step:1424/1825 train_time:77519ms step_avg:54.44ms
step:1425/1825 train_time:77606ms step_avg:54.46ms
step:1426/1825 train_time:77694ms step_avg:54.48ms
step:1427/1825 train_time:77781ms step_avg:54.51ms
step:1428/1825 train_time:77869ms step_avg:54.53ms
step:1429/1825 train_time:77957ms step_avg:54.55ms
step:1430/1825 train_time:78046ms step_avg:54.58ms
step:1431/1825 train_time:78132ms step_avg:54.60ms
step:1432/1825 train_time:78223ms step_avg:54.63ms
step:1433/1825 train_time:78309ms step_avg:54.65ms
step:1434/1825 train_time:78398ms step_avg:54.67ms
step:1435/1825 train_time:78485ms step_avg:54.69ms
step:1436/1825 train_time:78574ms step_avg:54.72ms
step:1437/1825 train_time:78660ms step_avg:54.74ms
step:1438/1825 train_time:78749ms step_avg:54.76ms
step:1439/1825 train_time:78837ms step_avg:54.79ms
step:1440/1825 train_time:78925ms step_avg:54.81ms
step:1441/1825 train_time:79011ms step_avg:54.83ms
step:1442/1825 train_time:79101ms step_avg:54.86ms
step:1443/1825 train_time:79187ms step_avg:54.88ms
step:1444/1825 train_time:79277ms step_avg:54.90ms
step:1445/1825 train_time:79363ms step_avg:54.92ms
step:1446/1825 train_time:79453ms step_avg:54.95ms
step:1447/1825 train_time:79538ms step_avg:54.97ms
step:1448/1825 train_time:79627ms step_avg:54.99ms
step:1449/1825 train_time:79715ms step_avg:55.01ms
step:1450/1825 train_time:79805ms step_avg:55.04ms
step:1451/1825 train_time:79891ms step_avg:55.06ms
step:1452/1825 train_time:79980ms step_avg:55.08ms
step:1453/1825 train_time:80066ms step_avg:55.10ms
step:1454/1825 train_time:80155ms step_avg:55.13ms
step:1455/1825 train_time:80242ms step_avg:55.15ms
step:1456/1825 train_time:80329ms step_avg:55.17ms
step:1457/1825 train_time:80417ms step_avg:55.19ms
step:1458/1825 train_time:80506ms step_avg:55.22ms
step:1459/1825 train_time:80592ms step_avg:55.24ms
step:1460/1825 train_time:80682ms step_avg:55.26ms
step:1461/1825 train_time:80768ms step_avg:55.28ms
step:1462/1825 train_time:80858ms step_avg:55.31ms
step:1463/1825 train_time:80943ms step_avg:55.33ms
step:1464/1825 train_time:81033ms step_avg:55.35ms
step:1465/1825 train_time:81120ms step_avg:55.37ms
step:1466/1825 train_time:81209ms step_avg:55.40ms
step:1467/1825 train_time:81296ms step_avg:55.42ms
step:1468/1825 train_time:81386ms step_avg:55.44ms
step:1469/1825 train_time:81473ms step_avg:55.46ms
step:1470/1825 train_time:81563ms step_avg:55.48ms
step:1471/1825 train_time:81648ms step_avg:55.51ms
step:1472/1825 train_time:81739ms step_avg:55.53ms
step:1473/1825 train_time:81824ms step_avg:55.55ms
step:1474/1825 train_time:81914ms step_avg:55.57ms
step:1475/1825 train_time:82001ms step_avg:55.59ms
step:1476/1825 train_time:82090ms step_avg:55.62ms
step:1477/1825 train_time:82177ms step_avg:55.64ms
step:1478/1825 train_time:82266ms step_avg:55.66ms
step:1479/1825 train_time:82353ms step_avg:55.68ms
step:1480/1825 train_time:82443ms step_avg:55.70ms
step:1481/1825 train_time:82530ms step_avg:55.73ms
step:1482/1825 train_time:82621ms step_avg:55.75ms
step:1483/1825 train_time:82706ms step_avg:55.77ms
step:1484/1825 train_time:82797ms step_avg:55.79ms
step:1485/1825 train_time:82883ms step_avg:55.81ms
step:1486/1825 train_time:82972ms step_avg:55.84ms
step:1487/1825 train_time:83058ms step_avg:55.86ms
step:1488/1825 train_time:83147ms step_avg:55.88ms
step:1489/1825 train_time:83234ms step_avg:55.90ms
step:1490/1825 train_time:83325ms step_avg:55.92ms
step:1491/1825 train_time:83411ms step_avg:55.94ms
step:1492/1825 train_time:83500ms step_avg:55.97ms
step:1493/1825 train_time:83587ms step_avg:55.99ms
step:1494/1825 train_time:83677ms step_avg:56.01ms
step:1495/1825 train_time:83763ms step_avg:56.03ms
step:1496/1825 train_time:83851ms step_avg:56.05ms
step:1497/1825 train_time:83937ms step_avg:56.07ms
step:1498/1825 train_time:84026ms step_avg:56.09ms
step:1499/1825 train_time:84112ms step_avg:56.11ms
step:1500/1825 train_time:84202ms step_avg:56.13ms
step:1500/1825 val_loss:3.3995 train_time:84299ms step_avg:56.20ms
step:1501/1825 train_time:84318ms step_avg:56.17ms
step:1502/1825 train_time:84379ms step_avg:56.18ms
step:1503/1825 train_time:84470ms step_avg:56.20ms
step:1504/1825 train_time:84568ms step_avg:56.23ms
step:1505/1825 train_time:84654ms step_avg:56.25ms
step:1506/1825 train_time:84744ms step_avg:56.27ms
step:1507/1825 train_time:84829ms step_avg:56.29ms
step:1508/1825 train_time:84918ms step_avg:56.31ms
step:1509/1825 train_time:85003ms step_avg:56.33ms
step:1510/1825 train_time:85090ms step_avg:56.35ms
step:1511/1825 train_time:85176ms step_avg:56.37ms
step:1512/1825 train_time:85266ms step_avg:56.39ms
step:1513/1825 train_time:85354ms step_avg:56.41ms
step:1514/1825 train_time:85446ms step_avg:56.44ms
step:1515/1825 train_time:85533ms step_avg:56.46ms
step:1516/1825 train_time:85624ms step_avg:56.48ms
step:1517/1825 train_time:85710ms step_avg:56.50ms
step:1518/1825 train_time:85800ms step_avg:56.52ms
step:1519/1825 train_time:85886ms step_avg:56.54ms
step:1520/1825 train_time:85973ms step_avg:56.56ms
step:1521/1825 train_time:86058ms step_avg:56.58ms
step:1522/1825 train_time:86147ms step_avg:56.60ms
step:1523/1825 train_time:86233ms step_avg:56.62ms
step:1524/1825 train_time:86323ms step_avg:56.64ms
step:1525/1825 train_time:86412ms step_avg:56.66ms
step:1526/1825 train_time:86503ms step_avg:56.69ms
step:1527/1825 train_time:86589ms step_avg:56.71ms
step:1528/1825 train_time:86678ms step_avg:56.73ms
step:1529/1825 train_time:86765ms step_avg:56.75ms
step:1530/1825 train_time:86852ms step_avg:56.77ms
step:1531/1825 train_time:86938ms step_avg:56.79ms
step:1532/1825 train_time:87027ms step_avg:56.81ms
step:1533/1825 train_time:87112ms step_avg:56.82ms
step:1534/1825 train_time:87201ms step_avg:56.85ms
step:1535/1825 train_time:87289ms step_avg:56.87ms
step:1536/1825 train_time:87378ms step_avg:56.89ms
step:1537/1825 train_time:87465ms step_avg:56.91ms
step:1538/1825 train_time:87556ms step_avg:56.93ms
step:1539/1825 train_time:87643ms step_avg:56.95ms
step:1540/1825 train_time:87733ms step_avg:56.97ms
step:1541/1825 train_time:87819ms step_avg:56.99ms
step:1542/1825 train_time:87907ms step_avg:57.01ms
step:1543/1825 train_time:87992ms step_avg:57.03ms
step:1544/1825 train_time:88081ms step_avg:57.05ms
step:1545/1825 train_time:88168ms step_avg:57.07ms
step:1546/1825 train_time:88256ms step_avg:57.09ms
step:1547/1825 train_time:88344ms step_avg:57.11ms
step:1548/1825 train_time:88433ms step_avg:57.13ms
step:1549/1825 train_time:88522ms step_avg:57.15ms
step:1550/1825 train_time:88612ms step_avg:57.17ms
step:1551/1825 train_time:88700ms step_avg:57.19ms
step:1552/1825 train_time:88790ms step_avg:57.21ms
step:1553/1825 train_time:88876ms step_avg:57.23ms
step:1554/1825 train_time:88964ms step_avg:57.25ms
step:1555/1825 train_time:89049ms step_avg:57.27ms
step:1556/1825 train_time:89139ms step_avg:57.29ms
step:1557/1825 train_time:89226ms step_avg:57.31ms
step:1558/1825 train_time:89315ms step_avg:57.33ms
step:1559/1825 train_time:89400ms step_avg:57.34ms
step:1560/1825 train_time:89490ms step_avg:57.37ms
step:1561/1825 train_time:89577ms step_avg:57.38ms
step:1562/1825 train_time:89668ms step_avg:57.41ms
step:1563/1825 train_time:89754ms step_avg:57.42ms
step:1564/1825 train_time:89844ms step_avg:57.44ms
step:1565/1825 train_time:89930ms step_avg:57.46ms
step:1566/1825 train_time:90020ms step_avg:57.48ms
step:1567/1825 train_time:90106ms step_avg:57.50ms
step:1568/1825 train_time:90194ms step_avg:57.52ms
step:1569/1825 train_time:90281ms step_avg:57.54ms
step:1570/1825 train_time:90370ms step_avg:57.56ms
step:1571/1825 train_time:90457ms step_avg:57.58ms
step:1572/1825 train_time:90549ms step_avg:57.60ms
step:1573/1825 train_time:90636ms step_avg:57.62ms
step:1574/1825 train_time:90726ms step_avg:57.64ms
step:1575/1825 train_time:90811ms step_avg:57.66ms
step:1576/1825 train_time:90900ms step_avg:57.68ms
step:1577/1825 train_time:90986ms step_avg:57.70ms
step:1578/1825 train_time:91074ms step_avg:57.71ms
step:1579/1825 train_time:91162ms step_avg:57.73ms
step:1580/1825 train_time:91251ms step_avg:57.75ms
step:1581/1825 train_time:91337ms step_avg:57.77ms
step:1582/1825 train_time:91428ms step_avg:57.79ms
step:1583/1825 train_time:91513ms step_avg:57.81ms
step:1584/1825 train_time:91606ms step_avg:57.83ms
step:1585/1825 train_time:91692ms step_avg:57.85ms
step:1586/1825 train_time:91782ms step_avg:57.87ms
step:1587/1825 train_time:91869ms step_avg:57.89ms
step:1588/1825 train_time:91958ms step_avg:57.91ms
step:1589/1825 train_time:92044ms step_avg:57.93ms
step:1590/1825 train_time:92133ms step_avg:57.95ms
step:1591/1825 train_time:92219ms step_avg:57.96ms
step:1592/1825 train_time:92309ms step_avg:57.98ms
step:1593/1825 train_time:92395ms step_avg:58.00ms
step:1594/1825 train_time:92486ms step_avg:58.02ms
step:1595/1825 train_time:92572ms step_avg:58.04ms
step:1596/1825 train_time:92662ms step_avg:58.06ms
step:1597/1825 train_time:92748ms step_avg:58.08ms
step:1598/1825 train_time:92836ms step_avg:58.10ms
step:1599/1825 train_time:92924ms step_avg:58.11ms
step:1600/1825 train_time:93012ms step_avg:58.13ms
step:1601/1825 train_time:93098ms step_avg:58.15ms
step:1602/1825 train_time:93188ms step_avg:58.17ms
step:1603/1825 train_time:93276ms step_avg:58.19ms
step:1604/1825 train_time:93365ms step_avg:58.21ms
step:1605/1825 train_time:93451ms step_avg:58.23ms
step:1606/1825 train_time:93541ms step_avg:58.24ms
step:1607/1825 train_time:93628ms step_avg:58.26ms
step:1608/1825 train_time:93718ms step_avg:58.28ms
step:1609/1825 train_time:93803ms step_avg:58.30ms
step:1610/1825 train_time:93892ms step_avg:58.32ms
step:1611/1825 train_time:93978ms step_avg:58.34ms
step:1612/1825 train_time:94069ms step_avg:58.36ms
step:1613/1825 train_time:94155ms step_avg:58.37ms
step:1614/1825 train_time:94244ms step_avg:58.39ms
step:1615/1825 train_time:94331ms step_avg:58.41ms
step:1616/1825 train_time:94422ms step_avg:58.43ms
step:1617/1825 train_time:94508ms step_avg:58.45ms
step:1618/1825 train_time:94597ms step_avg:58.47ms
step:1619/1825 train_time:94684ms step_avg:58.48ms
step:1620/1825 train_time:94772ms step_avg:58.50ms
step:1621/1825 train_time:94859ms step_avg:58.52ms
step:1622/1825 train_time:94948ms step_avg:58.54ms
step:1623/1825 train_time:95034ms step_avg:58.55ms
step:1624/1825 train_time:95123ms step_avg:58.57ms
step:1625/1825 train_time:95209ms step_avg:58.59ms
step:1626/1825 train_time:95298ms step_avg:58.61ms
step:1627/1825 train_time:95385ms step_avg:58.63ms
step:1628/1825 train_time:95474ms step_avg:58.64ms
step:1629/1825 train_time:95561ms step_avg:58.66ms
step:1630/1825 train_time:95650ms step_avg:58.68ms
step:1631/1825 train_time:95736ms step_avg:58.70ms
step:1632/1825 train_time:95827ms step_avg:58.72ms
step:1633/1825 train_time:95913ms step_avg:58.73ms
step:1634/1825 train_time:96003ms step_avg:58.75ms
step:1635/1825 train_time:96089ms step_avg:58.77ms
step:1636/1825 train_time:96178ms step_avg:58.79ms
step:1637/1825 train_time:96264ms step_avg:58.80ms
step:1638/1825 train_time:96352ms step_avg:58.82ms
step:1639/1825 train_time:96438ms step_avg:58.84ms
step:1640/1825 train_time:96529ms step_avg:58.86ms
step:1641/1825 train_time:96615ms step_avg:58.88ms
step:1642/1825 train_time:96705ms step_avg:58.89ms
step:1643/1825 train_time:96790ms step_avg:58.91ms
step:1644/1825 train_time:96879ms step_avg:58.93ms
step:1645/1825 train_time:96967ms step_avg:58.95ms
step:1646/1825 train_time:97056ms step_avg:58.96ms
step:1647/1825 train_time:97143ms step_avg:58.98ms
step:1648/1825 train_time:97232ms step_avg:59.00ms
step:1649/1825 train_time:97318ms step_avg:59.02ms
step:1650/1825 train_time:97408ms step_avg:59.03ms
step:1651/1825 train_time:97495ms step_avg:59.05ms
step:1652/1825 train_time:97585ms step_avg:59.07ms
step:1653/1825 train_time:97671ms step_avg:59.09ms
step:1654/1825 train_time:97760ms step_avg:59.11ms
step:1655/1825 train_time:97846ms step_avg:59.12ms
step:1656/1825 train_time:97935ms step_avg:59.14ms
step:1657/1825 train_time:98023ms step_avg:59.16ms
step:1658/1825 train_time:98112ms step_avg:59.17ms
step:1659/1825 train_time:98200ms step_avg:59.19ms
step:1660/1825 train_time:98289ms step_avg:59.21ms
step:1661/1825 train_time:98375ms step_avg:59.23ms
step:1662/1825 train_time:98466ms step_avg:59.25ms
step:1663/1825 train_time:98552ms step_avg:59.26ms
step:1664/1825 train_time:98642ms step_avg:59.28ms
step:1665/1825 train_time:98729ms step_avg:59.30ms
step:1666/1825 train_time:98818ms step_avg:59.31ms
step:1667/1825 train_time:98905ms step_avg:59.33ms
step:1668/1825 train_time:98993ms step_avg:59.35ms
step:1669/1825 train_time:99080ms step_avg:59.37ms
step:1670/1825 train_time:99169ms step_avg:59.38ms
step:1671/1825 train_time:99256ms step_avg:59.40ms
step:1672/1825 train_time:99345ms step_avg:59.42ms
step:1673/1825 train_time:99432ms step_avg:59.43ms
step:1674/1825 train_time:99520ms step_avg:59.45ms
step:1675/1825 train_time:99609ms step_avg:59.47ms
step:1676/1825 train_time:99698ms step_avg:59.49ms
step:1677/1825 train_time:99785ms step_avg:59.50ms
step:1678/1825 train_time:99874ms step_avg:59.52ms
step:1679/1825 train_time:99961ms step_avg:59.54ms
step:1680/1825 train_time:100051ms step_avg:59.55ms
step:1681/1825 train_time:100137ms step_avg:59.57ms
step:1682/1825 train_time:100226ms step_avg:59.59ms
step:1683/1825 train_time:100311ms step_avg:59.60ms
step:1684/1825 train_time:100402ms step_avg:59.62ms
step:1685/1825 train_time:100488ms step_avg:59.64ms
step:1686/1825 train_time:100577ms step_avg:59.65ms
step:1687/1825 train_time:100663ms step_avg:59.67ms
step:1688/1825 train_time:100752ms step_avg:59.69ms
step:1689/1825 train_time:100839ms step_avg:59.70ms
step:1690/1825 train_time:100928ms step_avg:59.72ms
step:1691/1825 train_time:101015ms step_avg:59.74ms
step:1692/1825 train_time:101106ms step_avg:59.76ms
step:1693/1825 train_time:101191ms step_avg:59.77ms
step:1694/1825 train_time:101281ms step_avg:59.79ms
step:1695/1825 train_time:101369ms step_avg:59.80ms
step:1696/1825 train_time:101458ms step_avg:59.82ms
step:1697/1825 train_time:101543ms step_avg:59.84ms
step:1698/1825 train_time:101632ms step_avg:59.85ms
step:1699/1825 train_time:101719ms step_avg:59.87ms
step:1700/1825 train_time:101810ms step_avg:59.89ms
step:1701/1825 train_time:101896ms step_avg:59.90ms
step:1702/1825 train_time:101986ms step_avg:59.92ms
step:1703/1825 train_time:102072ms step_avg:59.94ms
step:1704/1825 train_time:102161ms step_avg:59.95ms
step:1705/1825 train_time:102248ms step_avg:59.97ms
step:1706/1825 train_time:102339ms step_avg:59.99ms
step:1707/1825 train_time:102426ms step_avg:60.00ms
step:1708/1825 train_time:102514ms step_avg:60.02ms
step:1709/1825 train_time:102601ms step_avg:60.04ms
step:1710/1825 train_time:102690ms step_avg:60.05ms
step:1711/1825 train_time:102777ms step_avg:60.07ms
step:1712/1825 train_time:102867ms step_avg:60.09ms
step:1713/1825 train_time:102953ms step_avg:60.10ms
step:1714/1825 train_time:103043ms step_avg:60.12ms
step:1715/1825 train_time:103129ms step_avg:60.13ms
step:1716/1825 train_time:103218ms step_avg:60.15ms
step:1717/1825 train_time:103305ms step_avg:60.17ms
step:1718/1825 train_time:103393ms step_avg:60.18ms
step:1719/1825 train_time:103480ms step_avg:60.20ms
step:1720/1825 train_time:103569ms step_avg:60.21ms
step:1721/1825 train_time:103656ms step_avg:60.23ms
step:1722/1825 train_time:103746ms step_avg:60.25ms
step:1723/1825 train_time:103832ms step_avg:60.26ms
step:1724/1825 train_time:103923ms step_avg:60.28ms
step:1725/1825 train_time:104009ms step_avg:60.30ms
step:1726/1825 train_time:104098ms step_avg:60.31ms
step:1727/1825 train_time:104184ms step_avg:60.33ms
step:1728/1825 train_time:104274ms step_avg:60.34ms
step:1729/1825 train_time:104361ms step_avg:60.36ms
step:1730/1825 train_time:104449ms step_avg:60.38ms
step:1731/1825 train_time:104536ms step_avg:60.39ms
step:1732/1825 train_time:104625ms step_avg:60.41ms
step:1733/1825 train_time:104712ms step_avg:60.42ms
step:1734/1825 train_time:104800ms step_avg:60.44ms
step:1735/1825 train_time:104887ms step_avg:60.45ms
step:1736/1825 train_time:104976ms step_avg:60.47ms
step:1737/1825 train_time:105063ms step_avg:60.49ms
step:1738/1825 train_time:105152ms step_avg:60.50ms
step:1739/1825 train_time:105239ms step_avg:60.52ms
step:1740/1825 train_time:105328ms step_avg:60.53ms
step:1741/1825 train_time:105415ms step_avg:60.55ms
step:1742/1825 train_time:105504ms step_avg:60.56ms
step:1743/1825 train_time:105590ms step_avg:60.58ms
step:1744/1825 train_time:105680ms step_avg:60.60ms
step:1745/1825 train_time:105767ms step_avg:60.61ms
step:1746/1825 train_time:105855ms step_avg:60.63ms
step:1747/1825 train_time:105942ms step_avg:60.64ms
step:1748/1825 train_time:106031ms step_avg:60.66ms
step:1749/1825 train_time:106118ms step_avg:60.67ms
step:1750/1825 train_time:106208ms step_avg:60.69ms
step:1750/1825 val_loss:3.3017 train_time:106306ms step_avg:60.75ms
step:1751/1825 train_time:106324ms step_avg:60.72ms
step:1752/1825 train_time:106385ms step_avg:60.72ms
step:1753/1825 train_time:106477ms step_avg:60.74ms
step:1754/1825 train_time:106568ms step_avg:60.76ms
step:1755/1825 train_time:106654ms step_avg:60.77ms
step:1756/1825 train_time:106741ms step_avg:60.79ms
step:1757/1825 train_time:106827ms step_avg:60.80ms
step:1758/1825 train_time:106914ms step_avg:60.82ms
step:1759/1825 train_time:107000ms step_avg:60.83ms
step:1760/1825 train_time:107088ms step_avg:60.85ms
step:1761/1825 train_time:107175ms step_avg:60.86ms
step:1762/1825 train_time:107267ms step_avg:60.88ms
step:1763/1825 train_time:107354ms step_avg:60.89ms
step:1764/1825 train_time:107445ms step_avg:60.91ms
step:1765/1825 train_time:107533ms step_avg:60.93ms
step:1766/1825 train_time:107622ms step_avg:60.94ms
step:1767/1825 train_time:107708ms step_avg:60.96ms
step:1768/1825 train_time:107796ms step_avg:60.97ms
step:1769/1825 train_time:107883ms step_avg:60.99ms
step:1770/1825 train_time:107972ms step_avg:61.00ms
step:1771/1825 train_time:108057ms step_avg:61.01ms
step:1772/1825 train_time:108148ms step_avg:61.03ms
step:1773/1825 train_time:108235ms step_avg:61.05ms
step:1774/1825 train_time:108324ms step_avg:61.06ms
step:1775/1825 train_time:108412ms step_avg:61.08ms
step:1776/1825 train_time:108504ms step_avg:61.09ms
step:1777/1825 train_time:108590ms step_avg:61.11ms
step:1778/1825 train_time:108680ms step_avg:61.12ms
step:1779/1825 train_time:108766ms step_avg:61.14ms
step:1780/1825 train_time:108854ms step_avg:61.15ms
step:1781/1825 train_time:108942ms step_avg:61.17ms
step:1782/1825 train_time:109031ms step_avg:61.18ms
step:1783/1825 train_time:109117ms step_avg:61.20ms
step:1784/1825 train_time:109206ms step_avg:61.21ms
step:1785/1825 train_time:109292ms step_avg:61.23ms
step:1786/1825 train_time:109384ms step_avg:61.25ms
step:1787/1825 train_time:109470ms step_avg:61.26ms
step:1788/1825 train_time:109560ms step_avg:61.27ms
step:1789/1825 train_time:109647ms step_avg:61.29ms
step:1790/1825 train_time:109737ms step_avg:61.31ms
step:1791/1825 train_time:109824ms step_avg:61.32ms
step:1792/1825 train_time:109912ms step_avg:61.33ms
step:1793/1825 train_time:109998ms step_avg:61.35ms
step:1794/1825 train_time:110086ms step_avg:61.36ms
step:1795/1825 train_time:110172ms step_avg:61.38ms
step:1796/1825 train_time:110261ms step_avg:61.39ms
step:1797/1825 train_time:110349ms step_avg:61.41ms
step:1798/1825 train_time:110439ms step_avg:61.42ms
step:1799/1825 train_time:110527ms step_avg:61.44ms
step:1800/1825 train_time:110615ms step_avg:61.45ms
step:1801/1825 train_time:110703ms step_avg:61.47ms
step:1802/1825 train_time:110792ms step_avg:61.48ms
step:1803/1825 train_time:110879ms step_avg:61.50ms
step:1804/1825 train_time:110969ms step_avg:61.51ms
step:1805/1825 train_time:111055ms step_avg:61.53ms
step:1806/1825 train_time:111144ms step_avg:61.54ms
step:1807/1825 train_time:111231ms step_avg:61.56ms
step:1808/1825 train_time:111321ms step_avg:61.57ms
step:1809/1825 train_time:111409ms step_avg:61.59ms
step:1810/1825 train_time:111498ms step_avg:61.60ms
step:1811/1825 train_time:111585ms step_avg:61.61ms
step:1812/1825 train_time:111673ms step_avg:61.63ms
step:1813/1825 train_time:111760ms step_avg:61.64ms
step:1814/1825 train_time:111850ms step_avg:61.66ms
step:1815/1825 train_time:111936ms step_avg:61.67ms
step:1816/1825 train_time:112025ms step_avg:61.69ms
step:1817/1825 train_time:112111ms step_avg:61.70ms
step:1818/1825 train_time:112201ms step_avg:61.72ms
step:1819/1825 train_time:112287ms step_avg:61.73ms
step:1820/1825 train_time:112376ms step_avg:61.74ms
step:1821/1825 train_time:112465ms step_avg:61.76ms
step:1822/1825 train_time:112555ms step_avg:61.78ms
step:1823/1825 train_time:112643ms step_avg:61.79ms
step:1824/1825 train_time:112731ms step_avg:61.80ms
step:1825/1825 train_time:112817ms step_avg:61.82ms
step:1825/1825 val_loss:3.2802 train_time:112915ms step_avg:61.87ms
peak memory allocated: 29497 MiB reserved: 45438 MiB

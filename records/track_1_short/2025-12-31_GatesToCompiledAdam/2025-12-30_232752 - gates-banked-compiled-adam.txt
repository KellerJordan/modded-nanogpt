# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29 - gates-banked-compiled-adam - bf16 gates - fp32 exp_avg"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            #exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device) # Testing making the optimizer state fp32.
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay (lr as weight decay schedule)
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0))
                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()  # Testing leaving these as fp32
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

if master_process:
    import wandb
    wandb.login()
    wandb.init(
        project=WANDB_PROJECT,
        name=args.run_id,
        config=args
    )
    wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        if master_process:
            # Log to wandb
            wandb.log({
                "val/loss": val_loss.item(),
                "time/training_time_ms": training_time_ms,
                "time/step_avg_ms": training_time_ms / (step + 1),
            }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process:
            final_metrics = {
                "final/val_loss": float(val_loss.item()),
                "final/train_time_ms": int(training_time_ms),
                "final/train_time": training_time_ms / 1000,
                "final/steps_total": int(step),
            }
            wandb.log(final_metrics)
            wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
if master_process:
    wandb.save(f"{args.log_dir}/{args.run_id}.txt")
    wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 07:28:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   30C    P0            112W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          307957      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          307958      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          307959      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          307960      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          307961      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    5   N/A  N/A          307962      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          307963      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    7   N/A  N/A          307964      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8297 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:58ms step_avg:58.22ms
step:2/1845 train_time:88ms step_avg:44.05ms
step:3/1845 train_time:114ms step_avg:37.87ms
step:4/1845 train_time:142ms step_avg:35.47ms
step:5/1845 train_time:168ms step_avg:33.55ms
step:6/1845 train_time:289ms step_avg:48.13ms
step:7/1845 train_time:320ms step_avg:45.70ms
step:8/1845 train_time:354ms step_avg:44.23ms
step:9/1845 train_time:386ms step_avg:42.85ms
step:10/1845 train_time:420ms step_avg:42.02ms
step:11/1845 train_time:451ms step_avg:41.03ms
step:12/1845 train_time:486ms step_avg:40.51ms
step:13/1845 train_time:517ms step_avg:39.77ms
step:14/1845 train_time:553ms step_avg:39.50ms
step:15/1845 train_time:585ms step_avg:38.98ms
step:16/1845 train_time:620ms step_avg:38.75ms
step:17/1845 train_time:657ms step_avg:38.67ms
step:18/1845 train_time:696ms step_avg:38.64ms
step:19/1845 train_time:730ms step_avg:38.40ms
step:20/1845 train_time:768ms step_avg:38.38ms
step:21/1845 train_time:802ms step_avg:38.20ms
step:22/1845 train_time:840ms step_avg:38.19ms
step:23/1845 train_time:874ms step_avg:38.02ms
step:24/1845 train_time:913ms step_avg:38.03ms
step:25/1845 train_time:946ms step_avg:37.86ms
step:26/1845 train_time:983ms step_avg:37.82ms
step:27/1845 train_time:1017ms step_avg:37.66ms
step:28/1845 train_time:1053ms step_avg:37.60ms
step:29/1845 train_time:1089ms step_avg:37.54ms
step:30/1845 train_time:1126ms step_avg:37.54ms
step:31/1845 train_time:1160ms step_avg:37.43ms
step:32/1845 train_time:1198ms step_avg:37.44ms
step:33/1845 train_time:1231ms step_avg:37.31ms
step:34/1845 train_time:1269ms step_avg:37.33ms
step:35/1845 train_time:1303ms step_avg:37.22ms
step:36/1845 train_time:1341ms step_avg:37.25ms
step:37/1845 train_time:1375ms step_avg:37.15ms
step:38/1845 train_time:1410ms step_avg:37.10ms
step:39/1845 train_time:1441ms step_avg:36.95ms
step:40/1845 train_time:1476ms step_avg:36.90ms
step:41/1845 train_time:1508ms step_avg:36.78ms
step:42/1845 train_time:1543ms step_avg:36.73ms
step:43/1845 train_time:1574ms step_avg:36.60ms
step:44/1845 train_time:1608ms step_avg:36.55ms
step:45/1845 train_time:1639ms step_avg:36.42ms
step:46/1845 train_time:1673ms step_avg:36.37ms
step:47/1845 train_time:1703ms step_avg:36.24ms
step:48/1845 train_time:1736ms step_avg:36.17ms
step:49/1845 train_time:1766ms step_avg:36.05ms
step:50/1845 train_time:1800ms step_avg:36.00ms
step:51/1845 train_time:1830ms step_avg:35.89ms
step:52/1845 train_time:1863ms step_avg:35.83ms
step:53/1845 train_time:1893ms step_avg:35.72ms
step:54/1845 train_time:1926ms step_avg:35.68ms
step:55/1845 train_time:1956ms step_avg:35.57ms
step:56/1845 train_time:1989ms step_avg:35.53ms
step:57/1845 train_time:2019ms step_avg:35.42ms
step:58/1845 train_time:2055ms step_avg:35.43ms
step:59/1845 train_time:2086ms step_avg:35.36ms
step:60/1845 train_time:2124ms step_avg:35.40ms
step:61/1845 train_time:2156ms step_avg:35.35ms
step:62/1845 train_time:2193ms step_avg:35.37ms
step:63/1845 train_time:2225ms step_avg:35.31ms
step:64/1845 train_time:2260ms step_avg:35.32ms
step:65/1845 train_time:2292ms step_avg:35.26ms
step:66/1845 train_time:2331ms step_avg:35.31ms
step:67/1845 train_time:2365ms step_avg:35.30ms
step:68/1845 train_time:2403ms step_avg:35.34ms
step:69/1845 train_time:2435ms step_avg:35.29ms
step:70/1845 train_time:2472ms step_avg:35.32ms
step:71/1845 train_time:2507ms step_avg:35.32ms
step:72/1845 train_time:2545ms step_avg:35.35ms
step:73/1845 train_time:2577ms step_avg:35.30ms
step:74/1845 train_time:2613ms step_avg:35.31ms
step:75/1845 train_time:2644ms step_avg:35.26ms
step:76/1845 train_time:2680ms step_avg:35.26ms
step:77/1845 train_time:2712ms step_avg:35.22ms
step:78/1845 train_time:2747ms step_avg:35.22ms
step:79/1845 train_time:2779ms step_avg:35.18ms
step:80/1845 train_time:2814ms step_avg:35.18ms
step:81/1845 train_time:2846ms step_avg:35.14ms
step:82/1845 train_time:2882ms step_avg:35.14ms
step:83/1845 train_time:2915ms step_avg:35.12ms
step:84/1845 train_time:2950ms step_avg:35.12ms
step:85/1845 train_time:2982ms step_avg:35.08ms
step:86/1845 train_time:3016ms step_avg:35.07ms
step:87/1845 train_time:3047ms step_avg:35.02ms
step:88/1845 train_time:3082ms step_avg:35.02ms
step:89/1845 train_time:3114ms step_avg:34.99ms
step:90/1845 train_time:3149ms step_avg:34.99ms
step:91/1845 train_time:3181ms step_avg:34.96ms
step:92/1845 train_time:3217ms step_avg:34.97ms
step:93/1845 train_time:3251ms step_avg:34.96ms
step:94/1845 train_time:3287ms step_avg:34.96ms
step:95/1845 train_time:3319ms step_avg:34.93ms
step:96/1845 train_time:3353ms step_avg:34.93ms
step:97/1845 train_time:3385ms step_avg:34.90ms
step:98/1845 train_time:3421ms step_avg:34.91ms
step:99/1845 train_time:3453ms step_avg:34.88ms
step:100/1845 train_time:3488ms step_avg:34.88ms
step:101/1845 train_time:3520ms step_avg:34.85ms
step:102/1845 train_time:3555ms step_avg:34.86ms
step:103/1845 train_time:3587ms step_avg:34.83ms
step:104/1845 train_time:3624ms step_avg:34.84ms
step:105/1845 train_time:3655ms step_avg:34.81ms
step:106/1845 train_time:3691ms step_avg:34.82ms
step:107/1845 train_time:3723ms step_avg:34.80ms
step:108/1845 train_time:3759ms step_avg:34.81ms
step:109/1845 train_time:3791ms step_avg:34.78ms
step:110/1845 train_time:3829ms step_avg:34.81ms
step:111/1845 train_time:3863ms step_avg:34.81ms
step:112/1845 train_time:3902ms step_avg:34.84ms
step:113/1845 train_time:3936ms step_avg:34.83ms
step:114/1845 train_time:3971ms step_avg:34.83ms
step:115/1845 train_time:4002ms step_avg:34.80ms
step:116/1845 train_time:4036ms step_avg:34.79ms
step:117/1845 train_time:4068ms step_avg:34.77ms
step:118/1845 train_time:4103ms step_avg:34.78ms
step:119/1845 train_time:4135ms step_avg:34.75ms
step:120/1845 train_time:4169ms step_avg:34.74ms
step:121/1845 train_time:4201ms step_avg:34.72ms
step:122/1845 train_time:4235ms step_avg:34.72ms
step:123/1845 train_time:4266ms step_avg:34.68ms
step:124/1845 train_time:4304ms step_avg:34.71ms
step:125/1845 train_time:4335ms step_avg:34.68ms
step:126/1845 train_time:4371ms step_avg:34.69ms
step:127/1845 train_time:4402ms step_avg:34.66ms
step:128/1845 train_time:4439ms step_avg:34.68ms
step:129/1845 train_time:4470ms step_avg:34.65ms
step:130/1845 train_time:4508ms step_avg:34.67ms
step:131/1845 train_time:4539ms step_avg:34.65ms
step:132/1845 train_time:4575ms step_avg:34.66ms
step:133/1845 train_time:4607ms step_avg:34.64ms
step:134/1845 train_time:4643ms step_avg:34.65ms
step:135/1845 train_time:4674ms step_avg:34.62ms
step:136/1845 train_time:4711ms step_avg:34.64ms
step:137/1845 train_time:4742ms step_avg:34.61ms
step:138/1845 train_time:4779ms step_avg:34.63ms
step:139/1845 train_time:4810ms step_avg:34.60ms
step:140/1845 train_time:4847ms step_avg:34.62ms
step:141/1845 train_time:4878ms step_avg:34.60ms
step:142/1845 train_time:4915ms step_avg:34.61ms
step:143/1845 train_time:4946ms step_avg:34.59ms
step:144/1845 train_time:4983ms step_avg:34.60ms
step:145/1845 train_time:5014ms step_avg:34.58ms
step:146/1845 train_time:5051ms step_avg:34.60ms
step:147/1845 train_time:5082ms step_avg:34.57ms
step:148/1845 train_time:5119ms step_avg:34.59ms
step:149/1845 train_time:5150ms step_avg:34.56ms
step:150/1845 train_time:5187ms step_avg:34.58ms
step:151/1845 train_time:5218ms step_avg:34.56ms
step:152/1845 train_time:5255ms step_avg:34.57ms
step:153/1845 train_time:5285ms step_avg:34.55ms
step:154/1845 train_time:5323ms step_avg:34.56ms
step:155/1845 train_time:5354ms step_avg:34.54ms
step:156/1845 train_time:5391ms step_avg:34.56ms
step:157/1845 train_time:5422ms step_avg:34.53ms
step:158/1845 train_time:5458ms step_avg:34.55ms
step:159/1845 train_time:5489ms step_avg:34.52ms
step:160/1845 train_time:5527ms step_avg:34.54ms
step:161/1845 train_time:5558ms step_avg:34.52ms
step:162/1845 train_time:5595ms step_avg:34.53ms
step:163/1845 train_time:5626ms step_avg:34.52ms
step:164/1845 train_time:5662ms step_avg:34.52ms
step:165/1845 train_time:5693ms step_avg:34.50ms
step:166/1845 train_time:5730ms step_avg:34.52ms
step:167/1845 train_time:5761ms step_avg:34.50ms
step:168/1845 train_time:5798ms step_avg:34.51ms
step:169/1845 train_time:5829ms step_avg:34.49ms
step:170/1845 train_time:5866ms step_avg:34.51ms
step:171/1845 train_time:5897ms step_avg:34.49ms
step:172/1845 train_time:5934ms step_avg:34.50ms
step:173/1845 train_time:5965ms step_avg:34.48ms
step:174/1845 train_time:6002ms step_avg:34.49ms
step:175/1845 train_time:6033ms step_avg:34.47ms
step:176/1845 train_time:6070ms step_avg:34.49ms
step:177/1845 train_time:6101ms step_avg:34.47ms
step:178/1845 train_time:6138ms step_avg:34.48ms
step:179/1845 train_time:6169ms step_avg:34.46ms
step:180/1845 train_time:6206ms step_avg:34.48ms
step:181/1845 train_time:6237ms step_avg:34.46ms
step:182/1845 train_time:6273ms step_avg:34.47ms
step:183/1845 train_time:6304ms step_avg:34.45ms
step:184/1845 train_time:6341ms step_avg:34.46ms
step:185/1845 train_time:6372ms step_avg:34.44ms
step:186/1845 train_time:6410ms step_avg:34.46ms
step:187/1845 train_time:6441ms step_avg:34.44ms
step:188/1845 train_time:6478ms step_avg:34.46ms
step:189/1845 train_time:6509ms step_avg:34.44ms
step:190/1845 train_time:6546ms step_avg:34.45ms
step:191/1845 train_time:6577ms step_avg:34.43ms
step:192/1845 train_time:6614ms step_avg:34.45ms
step:193/1845 train_time:6645ms step_avg:34.43ms
step:194/1845 train_time:6682ms step_avg:34.44ms
step:195/1845 train_time:6713ms step_avg:34.43ms
step:196/1845 train_time:6750ms step_avg:34.44ms
step:197/1845 train_time:6780ms step_avg:34.42ms
step:198/1845 train_time:6818ms step_avg:34.44ms
step:199/1845 train_time:6849ms step_avg:34.42ms
step:200/1845 train_time:6886ms step_avg:34.43ms
step:201/1845 train_time:6917ms step_avg:34.41ms
step:202/1845 train_time:6953ms step_avg:34.42ms
step:203/1845 train_time:6984ms step_avg:34.41ms
step:204/1845 train_time:7022ms step_avg:34.42ms
step:205/1845 train_time:7053ms step_avg:34.40ms
step:206/1845 train_time:7089ms step_avg:34.41ms
step:207/1845 train_time:7120ms step_avg:34.40ms
step:208/1845 train_time:7157ms step_avg:34.41ms
step:209/1845 train_time:7188ms step_avg:34.39ms
step:210/1845 train_time:7226ms step_avg:34.41ms
step:211/1845 train_time:7257ms step_avg:34.39ms
step:212/1845 train_time:7293ms step_avg:34.40ms
step:213/1845 train_time:7324ms step_avg:34.39ms
step:214/1845 train_time:7361ms step_avg:34.40ms
step:215/1845 train_time:7392ms step_avg:34.38ms
step:216/1845 train_time:7429ms step_avg:34.39ms
step:217/1845 train_time:7460ms step_avg:34.38ms
step:218/1845 train_time:7497ms step_avg:34.39ms
step:219/1845 train_time:7528ms step_avg:34.38ms
step:220/1845 train_time:7565ms step_avg:34.39ms
step:221/1845 train_time:7596ms step_avg:34.37ms
step:222/1845 train_time:7633ms step_avg:34.38ms
step:223/1845 train_time:7664ms step_avg:34.37ms
step:224/1845 train_time:7701ms step_avg:34.38ms
step:225/1845 train_time:7733ms step_avg:34.37ms
step:226/1845 train_time:7769ms step_avg:34.38ms
step:227/1845 train_time:7800ms step_avg:34.36ms
step:228/1845 train_time:7837ms step_avg:34.37ms
step:229/1845 train_time:7868ms step_avg:34.36ms
step:230/1845 train_time:7905ms step_avg:34.37ms
step:231/1845 train_time:7936ms step_avg:34.36ms
step:232/1845 train_time:7973ms step_avg:34.37ms
step:233/1845 train_time:8004ms step_avg:34.35ms
step:234/1845 train_time:8041ms step_avg:34.36ms
step:235/1845 train_time:8072ms step_avg:34.35ms
step:236/1845 train_time:8108ms step_avg:34.36ms
step:237/1845 train_time:8139ms step_avg:34.34ms
step:238/1845 train_time:8176ms step_avg:34.35ms
step:239/1845 train_time:8207ms step_avg:34.34ms
step:240/1845 train_time:8246ms step_avg:34.36ms
step:241/1845 train_time:8277ms step_avg:34.34ms
step:242/1845 train_time:8314ms step_avg:34.35ms
step:243/1845 train_time:8345ms step_avg:34.34ms
step:244/1845 train_time:8383ms step_avg:34.36ms
step:245/1845 train_time:8414ms step_avg:34.34ms
step:246/1845 train_time:8453ms step_avg:34.36ms
step:247/1845 train_time:8483ms step_avg:34.35ms
step:248/1845 train_time:8522ms step_avg:34.36ms
step:249/1845 train_time:8553ms step_avg:34.35ms
step:250/1845 train_time:8591ms step_avg:34.37ms
step:250/1845 val_loss:4.6038 train_time:8630ms step_avg:34.52ms
step:251/1845 train_time:8656ms step_avg:34.49ms
step:252/1845 train_time:8682ms step_avg:34.45ms
step:253/1845 train_time:8706ms step_avg:34.41ms
step:254/1845 train_time:8731ms step_avg:34.38ms
step:255/1845 train_time:8762ms step_avg:34.36ms
step:256/1845 train_time:8801ms step_avg:34.38ms
step:257/1845 train_time:8831ms step_avg:34.36ms
step:258/1845 train_time:8870ms step_avg:34.38ms
step:259/1845 train_time:8902ms step_avg:34.37ms
step:260/1845 train_time:8938ms step_avg:34.38ms
step:261/1845 train_time:8970ms step_avg:34.37ms
step:262/1845 train_time:9007ms step_avg:34.38ms
step:263/1845 train_time:9041ms step_avg:34.37ms
step:264/1845 train_time:9078ms step_avg:34.39ms
step:265/1845 train_time:9110ms step_avg:34.38ms
step:266/1845 train_time:9145ms step_avg:34.38ms
step:267/1845 train_time:9176ms step_avg:34.37ms
step:268/1845 train_time:9210ms step_avg:34.37ms
step:269/1845 train_time:9241ms step_avg:34.35ms
step:270/1845 train_time:9277ms step_avg:34.36ms
step:271/1845 train_time:9307ms step_avg:34.34ms
step:272/1845 train_time:9344ms step_avg:34.35ms
step:273/1845 train_time:9375ms step_avg:34.34ms
step:274/1845 train_time:9412ms step_avg:34.35ms
step:275/1845 train_time:9442ms step_avg:34.34ms
step:276/1845 train_time:9479ms step_avg:34.34ms
step:277/1845 train_time:9510ms step_avg:34.33ms
step:278/1845 train_time:9547ms step_avg:34.34ms
step:279/1845 train_time:9578ms step_avg:34.33ms
step:280/1845 train_time:9615ms step_avg:34.34ms
step:281/1845 train_time:9646ms step_avg:34.33ms
step:282/1845 train_time:9683ms step_avg:34.34ms
step:283/1845 train_time:9714ms step_avg:34.32ms
step:284/1845 train_time:9751ms step_avg:34.33ms
step:285/1845 train_time:9781ms step_avg:34.32ms
step:286/1845 train_time:9819ms step_avg:34.33ms
step:287/1845 train_time:9850ms step_avg:34.32ms
step:288/1845 train_time:9887ms step_avg:34.33ms
step:289/1845 train_time:9918ms step_avg:34.32ms
step:290/1845 train_time:9955ms step_avg:34.33ms
step:291/1845 train_time:9986ms step_avg:34.31ms
step:292/1845 train_time:10023ms step_avg:34.33ms
step:293/1845 train_time:10054ms step_avg:34.31ms
step:294/1845 train_time:10092ms step_avg:34.33ms
step:295/1845 train_time:10125ms step_avg:34.32ms
step:296/1845 train_time:10159ms step_avg:34.32ms
step:297/1845 train_time:10190ms step_avg:34.31ms
step:298/1845 train_time:10227ms step_avg:34.32ms
step:299/1845 train_time:10257ms step_avg:34.31ms
step:300/1845 train_time:10294ms step_avg:34.31ms
step:301/1845 train_time:10325ms step_avg:34.30ms
step:302/1845 train_time:10362ms step_avg:34.31ms
step:303/1845 train_time:10393ms step_avg:34.30ms
step:304/1845 train_time:10430ms step_avg:34.31ms
step:305/1845 train_time:10461ms step_avg:34.30ms
step:306/1845 train_time:10497ms step_avg:34.31ms
step:307/1845 train_time:10529ms step_avg:34.30ms
step:308/1845 train_time:10565ms step_avg:34.30ms
step:309/1845 train_time:10596ms step_avg:34.29ms
step:310/1845 train_time:10633ms step_avg:34.30ms
step:311/1845 train_time:10664ms step_avg:34.29ms
step:312/1845 train_time:10701ms step_avg:34.30ms
step:313/1845 train_time:10732ms step_avg:34.29ms
step:314/1845 train_time:10769ms step_avg:34.30ms
step:315/1845 train_time:10800ms step_avg:34.29ms
step:316/1845 train_time:10838ms step_avg:34.30ms
step:317/1845 train_time:10868ms step_avg:34.29ms
step:318/1845 train_time:10905ms step_avg:34.29ms
step:319/1845 train_time:10936ms step_avg:34.28ms
step:320/1845 train_time:10973ms step_avg:34.29ms
step:321/1845 train_time:11004ms step_avg:34.28ms
step:322/1845 train_time:11041ms step_avg:34.29ms
step:323/1845 train_time:11072ms step_avg:34.28ms
step:324/1845 train_time:11109ms step_avg:34.29ms
step:325/1845 train_time:11140ms step_avg:34.28ms
step:326/1845 train_time:11177ms step_avg:34.29ms
step:327/1845 train_time:11208ms step_avg:34.28ms
step:328/1845 train_time:11245ms step_avg:34.28ms
step:329/1845 train_time:11276ms step_avg:34.27ms
step:330/1845 train_time:11313ms step_avg:34.28ms
step:331/1845 train_time:11344ms step_avg:34.27ms
step:332/1845 train_time:11380ms step_avg:34.28ms
step:333/1845 train_time:11411ms step_avg:34.27ms
step:334/1845 train_time:11448ms step_avg:34.28ms
step:335/1845 train_time:11479ms step_avg:34.27ms
step:336/1845 train_time:11516ms step_avg:34.27ms
step:337/1845 train_time:11547ms step_avg:34.26ms
step:338/1845 train_time:11584ms step_avg:34.27ms
step:339/1845 train_time:11615ms step_avg:34.26ms
step:340/1845 train_time:11652ms step_avg:34.27ms
step:341/1845 train_time:11683ms step_avg:34.26ms
step:342/1845 train_time:11720ms step_avg:34.27ms
step:343/1845 train_time:11751ms step_avg:34.26ms
step:344/1845 train_time:11787ms step_avg:34.27ms
step:345/1845 train_time:11819ms step_avg:34.26ms
step:346/1845 train_time:11856ms step_avg:34.26ms
step:347/1845 train_time:11887ms step_avg:34.26ms
step:348/1845 train_time:11924ms step_avg:34.26ms
step:349/1845 train_time:11955ms step_avg:34.25ms
step:350/1845 train_time:11991ms step_avg:34.26ms
step:351/1845 train_time:12022ms step_avg:34.25ms
step:352/1845 train_time:12059ms step_avg:34.26ms
step:353/1845 train_time:12090ms step_avg:34.25ms
step:354/1845 train_time:12128ms step_avg:34.26ms
step:355/1845 train_time:12159ms step_avg:34.25ms
step:356/1845 train_time:12196ms step_avg:34.26ms
step:357/1845 train_time:12227ms step_avg:34.25ms
step:358/1845 train_time:12263ms step_avg:34.26ms
step:359/1845 train_time:12295ms step_avg:34.25ms
step:360/1845 train_time:12331ms step_avg:34.25ms
step:361/1845 train_time:12362ms step_avg:34.24ms
step:362/1845 train_time:12399ms step_avg:34.25ms
step:363/1845 train_time:12430ms step_avg:34.24ms
step:364/1845 train_time:12467ms step_avg:34.25ms
step:365/1845 train_time:12498ms step_avg:34.24ms
step:366/1845 train_time:12535ms step_avg:34.25ms
step:367/1845 train_time:12566ms step_avg:34.24ms
step:368/1845 train_time:12603ms step_avg:34.25ms
step:369/1845 train_time:12634ms step_avg:34.24ms
step:370/1845 train_time:12671ms step_avg:34.25ms
step:371/1845 train_time:12702ms step_avg:34.24ms
step:372/1845 train_time:12739ms step_avg:34.25ms
step:373/1845 train_time:12770ms step_avg:34.24ms
step:374/1845 train_time:12807ms step_avg:34.24ms
step:375/1845 train_time:12838ms step_avg:34.23ms
step:376/1845 train_time:12875ms step_avg:34.24ms
step:377/1845 train_time:12905ms step_avg:34.23ms
step:378/1845 train_time:12943ms step_avg:34.24ms
step:379/1845 train_time:12974ms step_avg:34.23ms
step:380/1845 train_time:13011ms step_avg:34.24ms
step:381/1845 train_time:13042ms step_avg:34.23ms
step:382/1845 train_time:13078ms step_avg:34.24ms
step:383/1845 train_time:13109ms step_avg:34.23ms
step:384/1845 train_time:13146ms step_avg:34.24ms
step:385/1845 train_time:13177ms step_avg:34.23ms
step:386/1845 train_time:13214ms step_avg:34.23ms
step:387/1845 train_time:13245ms step_avg:34.23ms
step:388/1845 train_time:13282ms step_avg:34.23ms
step:389/1845 train_time:13313ms step_avg:34.22ms
step:390/1845 train_time:13350ms step_avg:34.23ms
step:391/1845 train_time:13381ms step_avg:34.22ms
step:392/1845 train_time:13418ms step_avg:34.23ms
step:393/1845 train_time:13449ms step_avg:34.22ms
step:394/1845 train_time:13485ms step_avg:34.23ms
step:395/1845 train_time:13517ms step_avg:34.22ms
step:396/1845 train_time:13553ms step_avg:34.23ms
step:397/1845 train_time:13584ms step_avg:34.22ms
step:398/1845 train_time:13621ms step_avg:34.22ms
step:399/1845 train_time:13652ms step_avg:34.22ms
step:400/1845 train_time:13689ms step_avg:34.22ms
step:401/1845 train_time:13720ms step_avg:34.21ms
step:402/1845 train_time:13757ms step_avg:34.22ms
step:403/1845 train_time:13788ms step_avg:34.21ms
step:404/1845 train_time:13824ms step_avg:34.22ms
step:405/1845 train_time:13856ms step_avg:34.21ms
step:406/1845 train_time:13893ms step_avg:34.22ms
step:407/1845 train_time:13923ms step_avg:34.21ms
step:408/1845 train_time:13961ms step_avg:34.22ms
step:409/1845 train_time:13991ms step_avg:34.21ms
step:410/1845 train_time:14029ms step_avg:34.22ms
step:411/1845 train_time:14060ms step_avg:34.21ms
step:412/1845 train_time:14096ms step_avg:34.21ms
step:413/1845 train_time:14127ms step_avg:34.21ms
step:414/1845 train_time:14164ms step_avg:34.21ms
step:415/1845 train_time:14195ms step_avg:34.21ms
step:416/1845 train_time:14232ms step_avg:34.21ms
step:417/1845 train_time:14263ms step_avg:34.20ms
step:418/1845 train_time:14300ms step_avg:34.21ms
step:419/1845 train_time:14331ms step_avg:34.20ms
step:420/1845 train_time:14368ms step_avg:34.21ms
step:421/1845 train_time:14399ms step_avg:34.20ms
step:422/1845 train_time:14437ms step_avg:34.21ms
step:423/1845 train_time:14468ms step_avg:34.20ms
step:424/1845 train_time:14504ms step_avg:34.21ms
step:425/1845 train_time:14535ms step_avg:34.20ms
step:426/1845 train_time:14572ms step_avg:34.21ms
step:427/1845 train_time:14603ms step_avg:34.20ms
step:428/1845 train_time:14640ms step_avg:34.21ms
step:429/1845 train_time:14671ms step_avg:34.20ms
step:430/1845 train_time:14708ms step_avg:34.21ms
step:431/1845 train_time:14740ms step_avg:34.20ms
step:432/1845 train_time:14776ms step_avg:34.20ms
step:433/1845 train_time:14808ms step_avg:34.20ms
step:434/1845 train_time:14844ms step_avg:34.20ms
step:435/1845 train_time:14875ms step_avg:34.20ms
step:436/1845 train_time:14912ms step_avg:34.20ms
step:437/1845 train_time:14943ms step_avg:34.20ms
step:438/1845 train_time:14980ms step_avg:34.20ms
step:439/1845 train_time:15011ms step_avg:34.19ms
step:440/1845 train_time:15048ms step_avg:34.20ms
step:441/1845 train_time:15079ms step_avg:34.19ms
step:442/1845 train_time:15115ms step_avg:34.20ms
step:443/1845 train_time:15146ms step_avg:34.19ms
step:444/1845 train_time:15183ms step_avg:34.20ms
step:445/1845 train_time:15214ms step_avg:34.19ms
step:446/1845 train_time:15251ms step_avg:34.20ms
step:447/1845 train_time:15282ms step_avg:34.19ms
step:448/1845 train_time:15319ms step_avg:34.19ms
step:449/1845 train_time:15350ms step_avg:34.19ms
step:450/1845 train_time:15387ms step_avg:34.19ms
step:451/1845 train_time:15418ms step_avg:34.19ms
step:452/1845 train_time:15455ms step_avg:34.19ms
step:453/1845 train_time:15486ms step_avg:34.19ms
step:454/1845 train_time:15524ms step_avg:34.19ms
step:455/1845 train_time:15557ms step_avg:34.19ms
step:456/1845 train_time:15591ms step_avg:34.19ms
step:457/1845 train_time:15622ms step_avg:34.18ms
step:458/1845 train_time:15659ms step_avg:34.19ms
step:459/1845 train_time:15689ms step_avg:34.18ms
step:460/1845 train_time:15727ms step_avg:34.19ms
step:461/1845 train_time:15757ms step_avg:34.18ms
step:462/1845 train_time:15795ms step_avg:34.19ms
step:463/1845 train_time:15826ms step_avg:34.18ms
step:464/1845 train_time:15863ms step_avg:34.19ms
step:465/1845 train_time:15893ms step_avg:34.18ms
step:466/1845 train_time:15930ms step_avg:34.19ms
step:467/1845 train_time:15961ms step_avg:34.18ms
step:468/1845 train_time:15998ms step_avg:34.18ms
step:469/1845 train_time:16028ms step_avg:34.18ms
step:470/1845 train_time:16066ms step_avg:34.18ms
step:471/1845 train_time:16097ms step_avg:34.18ms
step:472/1845 train_time:16134ms step_avg:34.18ms
step:473/1845 train_time:16164ms step_avg:34.17ms
step:474/1845 train_time:16202ms step_avg:34.18ms
step:475/1845 train_time:16233ms step_avg:34.17ms
step:476/1845 train_time:16270ms step_avg:34.18ms
step:477/1845 train_time:16301ms step_avg:34.17ms
step:478/1845 train_time:16338ms step_avg:34.18ms
step:479/1845 train_time:16369ms step_avg:34.17ms
step:480/1845 train_time:16406ms step_avg:34.18ms
step:481/1845 train_time:16437ms step_avg:34.17ms
step:482/1845 train_time:16475ms step_avg:34.18ms
step:483/1845 train_time:16506ms step_avg:34.17ms
step:484/1845 train_time:16543ms step_avg:34.18ms
step:485/1845 train_time:16574ms step_avg:34.17ms
step:486/1845 train_time:16611ms step_avg:34.18ms
step:487/1845 train_time:16642ms step_avg:34.17ms
step:488/1845 train_time:16679ms step_avg:34.18ms
step:489/1845 train_time:16709ms step_avg:34.17ms
step:490/1845 train_time:16746ms step_avg:34.18ms
step:491/1845 train_time:16777ms step_avg:34.17ms
step:492/1845 train_time:16814ms step_avg:34.18ms
step:493/1845 train_time:16845ms step_avg:34.17ms
step:494/1845 train_time:16882ms step_avg:34.17ms
step:495/1845 train_time:16913ms step_avg:34.17ms
step:496/1845 train_time:16951ms step_avg:34.18ms
step:497/1845 train_time:16982ms step_avg:34.17ms
step:498/1845 train_time:17018ms step_avg:34.17ms
step:499/1845 train_time:17049ms step_avg:34.17ms
step:500/1845 train_time:17086ms step_avg:34.17ms
step:500/1845 val_loss:4.2981 train_time:17124ms step_avg:34.25ms
step:501/1845 train_time:17149ms step_avg:34.23ms
step:502/1845 train_time:17175ms step_avg:34.21ms
step:503/1845 train_time:17199ms step_avg:34.19ms
step:504/1845 train_time:17228ms step_avg:34.18ms
step:505/1845 train_time:17256ms step_avg:34.17ms
step:506/1845 train_time:17293ms step_avg:34.18ms
step:507/1845 train_time:17324ms step_avg:34.17ms
step:508/1845 train_time:17362ms step_avg:34.18ms
step:509/1845 train_time:17394ms step_avg:34.17ms
step:510/1845 train_time:17434ms step_avg:34.18ms
step:511/1845 train_time:17468ms step_avg:34.18ms
step:512/1845 train_time:17506ms step_avg:34.19ms
step:513/1845 train_time:17541ms step_avg:34.19ms
step:514/1845 train_time:17580ms step_avg:34.20ms
step:515/1845 train_time:17613ms step_avg:34.20ms
step:516/1845 train_time:17647ms step_avg:34.20ms
step:517/1845 train_time:17678ms step_avg:34.19ms
step:518/1845 train_time:17714ms step_avg:34.20ms
step:519/1845 train_time:17745ms step_avg:34.19ms
step:520/1845 train_time:17779ms step_avg:34.19ms
step:521/1845 train_time:17810ms step_avg:34.18ms
step:522/1845 train_time:17844ms step_avg:34.18ms
step:523/1845 train_time:17874ms step_avg:34.18ms
step:524/1845 train_time:17907ms step_avg:34.17ms
step:525/1845 train_time:17937ms step_avg:34.17ms
step:526/1845 train_time:17971ms step_avg:34.17ms
step:527/1845 train_time:18001ms step_avg:34.16ms
step:528/1845 train_time:18039ms step_avg:34.17ms
step:529/1845 train_time:18070ms step_avg:34.16ms
step:530/1845 train_time:18107ms step_avg:34.16ms
step:531/1845 train_time:18138ms step_avg:34.16ms
step:532/1845 train_time:18175ms step_avg:34.16ms
step:533/1845 train_time:18206ms step_avg:34.16ms
step:534/1845 train_time:18244ms step_avg:34.16ms
step:535/1845 train_time:18274ms step_avg:34.16ms
step:536/1845 train_time:18311ms step_avg:34.16ms
step:537/1845 train_time:18342ms step_avg:34.16ms
step:538/1845 train_time:18379ms step_avg:34.16ms
step:539/1845 train_time:18411ms step_avg:34.16ms
step:540/1845 train_time:18448ms step_avg:34.16ms
step:541/1845 train_time:18478ms step_avg:34.16ms
step:542/1845 train_time:18516ms step_avg:34.16ms
step:543/1845 train_time:18547ms step_avg:34.16ms
step:544/1845 train_time:18584ms step_avg:34.16ms
step:545/1845 train_time:18615ms step_avg:34.16ms
step:546/1845 train_time:18652ms step_avg:34.16ms
step:547/1845 train_time:18683ms step_avg:34.16ms
step:548/1845 train_time:18720ms step_avg:34.16ms
step:549/1845 train_time:18751ms step_avg:34.16ms
step:550/1845 train_time:18788ms step_avg:34.16ms
step:551/1845 train_time:18819ms step_avg:34.15ms
step:552/1845 train_time:18856ms step_avg:34.16ms
step:553/1845 train_time:18887ms step_avg:34.15ms
step:554/1845 train_time:18925ms step_avg:34.16ms
step:555/1845 train_time:18956ms step_avg:34.16ms
step:556/1845 train_time:18992ms step_avg:34.16ms
step:557/1845 train_time:19023ms step_avg:34.15ms
step:558/1845 train_time:19060ms step_avg:34.16ms
step:559/1845 train_time:19091ms step_avg:34.15ms
step:560/1845 train_time:19128ms step_avg:34.16ms
step:561/1845 train_time:19159ms step_avg:34.15ms
step:562/1845 train_time:19195ms step_avg:34.16ms
step:563/1845 train_time:19226ms step_avg:34.15ms
step:564/1845 train_time:19264ms step_avg:34.16ms
step:565/1845 train_time:19295ms step_avg:34.15ms
step:566/1845 train_time:19332ms step_avg:34.16ms
step:567/1845 train_time:19364ms step_avg:34.15ms
step:568/1845 train_time:19400ms step_avg:34.16ms
step:569/1845 train_time:19432ms step_avg:34.15ms
step:570/1845 train_time:19468ms step_avg:34.15ms
step:571/1845 train_time:19499ms step_avg:34.15ms
step:572/1845 train_time:19537ms step_avg:34.16ms
step:573/1845 train_time:19567ms step_avg:34.15ms
step:574/1845 train_time:19605ms step_avg:34.15ms
step:575/1845 train_time:19635ms step_avg:34.15ms
step:576/1845 train_time:19672ms step_avg:34.15ms
step:577/1845 train_time:19703ms step_avg:34.15ms
step:578/1845 train_time:19741ms step_avg:34.15ms
step:579/1845 train_time:19771ms step_avg:34.15ms
step:580/1845 train_time:19808ms step_avg:34.15ms
step:581/1845 train_time:19839ms step_avg:34.15ms
step:582/1845 train_time:19876ms step_avg:34.15ms
step:583/1845 train_time:19906ms step_avg:34.14ms
step:584/1845 train_time:19944ms step_avg:34.15ms
step:585/1845 train_time:19974ms step_avg:34.14ms
step:586/1845 train_time:20012ms step_avg:34.15ms
step:587/1845 train_time:20043ms step_avg:34.14ms
step:588/1845 train_time:20080ms step_avg:34.15ms
step:589/1845 train_time:20110ms step_avg:34.14ms
step:590/1845 train_time:20147ms step_avg:34.15ms
step:591/1845 train_time:20179ms step_avg:34.14ms
step:592/1845 train_time:20216ms step_avg:34.15ms
step:593/1845 train_time:20247ms step_avg:34.14ms
step:594/1845 train_time:20284ms step_avg:34.15ms
step:595/1845 train_time:20315ms step_avg:34.14ms
step:596/1845 train_time:20352ms step_avg:34.15ms
step:597/1845 train_time:20383ms step_avg:34.14ms
step:598/1845 train_time:20420ms step_avg:34.15ms
step:599/1845 train_time:20451ms step_avg:34.14ms
step:600/1845 train_time:20488ms step_avg:34.15ms
step:601/1845 train_time:20519ms step_avg:34.14ms
step:602/1845 train_time:20556ms step_avg:34.15ms
step:603/1845 train_time:20589ms step_avg:34.14ms
step:604/1845 train_time:20646ms step_avg:34.18ms
step:605/1845 train_time:20706ms step_avg:34.22ms
step:606/1845 train_time:20769ms step_avg:34.27ms
step:607/1845 train_time:20828ms step_avg:34.31ms
step:608/1845 train_time:20892ms step_avg:34.36ms
step:609/1845 train_time:20951ms step_avg:34.40ms
step:610/1845 train_time:21014ms step_avg:34.45ms
step:611/1845 train_time:21074ms step_avg:34.49ms
step:612/1845 train_time:21138ms step_avg:34.54ms
step:613/1845 train_time:21198ms step_avg:34.58ms
step:614/1845 train_time:21261ms step_avg:34.63ms
step:615/1845 train_time:21321ms step_avg:34.67ms
step:616/1845 train_time:21385ms step_avg:34.72ms
step:617/1845 train_time:21445ms step_avg:34.76ms
step:618/1845 train_time:21507ms step_avg:34.80ms
step:619/1845 train_time:21567ms step_avg:34.84ms
step:620/1845 train_time:21630ms step_avg:34.89ms
step:621/1845 train_time:21690ms step_avg:34.93ms
step:622/1845 train_time:21753ms step_avg:34.97ms
step:623/1845 train_time:21813ms step_avg:35.01ms
step:624/1845 train_time:21875ms step_avg:35.06ms
step:625/1845 train_time:21935ms step_avg:35.10ms
step:626/1845 train_time:21998ms step_avg:35.14ms
step:627/1845 train_time:22058ms step_avg:35.18ms
step:628/1845 train_time:22122ms step_avg:35.23ms
step:629/1845 train_time:22181ms step_avg:35.26ms
step:630/1845 train_time:22245ms step_avg:35.31ms
step:631/1845 train_time:22305ms step_avg:35.35ms
step:632/1845 train_time:22367ms step_avg:35.39ms
step:633/1845 train_time:22427ms step_avg:35.43ms
step:634/1845 train_time:22490ms step_avg:35.47ms
step:635/1845 train_time:22550ms step_avg:35.51ms
step:636/1845 train_time:22612ms step_avg:35.55ms
step:637/1845 train_time:22673ms step_avg:35.59ms
step:638/1845 train_time:22735ms step_avg:35.64ms
step:639/1845 train_time:22796ms step_avg:35.67ms
step:640/1845 train_time:22858ms step_avg:35.72ms
step:641/1845 train_time:22918ms step_avg:35.75ms
step:642/1845 train_time:22980ms step_avg:35.80ms
step:643/1845 train_time:23041ms step_avg:35.83ms
step:644/1845 train_time:23103ms step_avg:35.87ms
step:645/1845 train_time:23162ms step_avg:35.91ms
step:646/1845 train_time:23225ms step_avg:35.95ms
step:647/1845 train_time:23285ms step_avg:35.99ms
step:648/1845 train_time:23348ms step_avg:36.03ms
step:649/1845 train_time:23408ms step_avg:36.07ms
step:650/1845 train_time:23471ms step_avg:36.11ms
step:651/1845 train_time:23532ms step_avg:36.15ms
step:652/1845 train_time:23595ms step_avg:36.19ms
step:653/1845 train_time:23654ms step_avg:36.22ms
step:654/1845 train_time:23718ms step_avg:36.27ms
step:655/1845 train_time:23777ms step_avg:36.30ms
step:656/1845 train_time:23840ms step_avg:36.34ms
step:657/1845 train_time:23900ms step_avg:36.38ms
step:658/1845 train_time:23963ms step_avg:36.42ms
step:659/1845 train_time:24023ms step_avg:36.45ms
step:660/1845 train_time:24086ms step_avg:36.49ms
step:661/1845 train_time:24146ms step_avg:36.53ms
step:662/1845 train_time:24208ms step_avg:36.57ms
step:663/1845 train_time:24269ms step_avg:36.60ms
step:664/1845 train_time:24331ms step_avg:36.64ms
step:665/1845 train_time:24391ms step_avg:36.68ms
step:666/1845 train_time:24454ms step_avg:36.72ms
step:667/1845 train_time:24514ms step_avg:36.75ms
step:668/1845 train_time:24577ms step_avg:36.79ms
step:669/1845 train_time:24637ms step_avg:36.83ms
step:670/1845 train_time:24701ms step_avg:36.87ms
step:671/1845 train_time:24760ms step_avg:36.90ms
step:672/1845 train_time:24823ms step_avg:36.94ms
step:673/1845 train_time:24882ms step_avg:36.97ms
step:674/1845 train_time:24945ms step_avg:37.01ms
step:675/1845 train_time:25005ms step_avg:37.04ms
step:676/1845 train_time:25068ms step_avg:37.08ms
step:677/1845 train_time:25128ms step_avg:37.12ms
step:678/1845 train_time:25191ms step_avg:37.16ms
step:679/1845 train_time:25251ms step_avg:37.19ms
step:680/1845 train_time:25314ms step_avg:37.23ms
step:681/1845 train_time:25375ms step_avg:37.26ms
step:682/1845 train_time:25437ms step_avg:37.30ms
step:683/1845 train_time:25497ms step_avg:37.33ms
step:684/1845 train_time:25559ms step_avg:37.37ms
step:685/1845 train_time:25619ms step_avg:37.40ms
step:686/1845 train_time:25683ms step_avg:37.44ms
step:687/1845 train_time:25743ms step_avg:37.47ms
step:688/1845 train_time:25806ms step_avg:37.51ms
step:689/1845 train_time:25866ms step_avg:37.54ms
step:690/1845 train_time:25929ms step_avg:37.58ms
step:691/1845 train_time:25988ms step_avg:37.61ms
step:692/1845 train_time:26051ms step_avg:37.65ms
step:693/1845 train_time:26112ms step_avg:37.68ms
step:694/1845 train_time:26174ms step_avg:37.72ms
step:695/1845 train_time:26234ms step_avg:37.75ms
step:696/1845 train_time:26297ms step_avg:37.78ms
step:697/1845 train_time:26357ms step_avg:37.82ms
step:698/1845 train_time:26420ms step_avg:37.85ms
step:699/1845 train_time:26480ms step_avg:37.88ms
step:700/1845 train_time:26543ms step_avg:37.92ms
step:701/1845 train_time:26603ms step_avg:37.95ms
step:702/1845 train_time:26666ms step_avg:37.99ms
step:703/1845 train_time:26726ms step_avg:38.02ms
step:704/1845 train_time:26788ms step_avg:38.05ms
step:705/1845 train_time:26848ms step_avg:38.08ms
step:706/1845 train_time:26911ms step_avg:38.12ms
step:707/1845 train_time:26971ms step_avg:38.15ms
step:708/1845 train_time:27034ms step_avg:38.18ms
step:709/1845 train_time:27094ms step_avg:38.21ms
step:710/1845 train_time:27156ms step_avg:38.25ms
step:711/1845 train_time:27216ms step_avg:38.28ms
step:712/1845 train_time:27279ms step_avg:38.31ms
step:713/1845 train_time:27340ms step_avg:38.34ms
step:714/1845 train_time:27402ms step_avg:38.38ms
step:715/1845 train_time:27462ms step_avg:38.41ms
step:716/1845 train_time:27525ms step_avg:38.44ms
step:717/1845 train_time:27584ms step_avg:38.47ms
step:718/1845 train_time:27646ms step_avg:38.50ms
step:719/1845 train_time:27706ms step_avg:38.53ms
step:720/1845 train_time:27767ms step_avg:38.57ms
step:721/1845 train_time:27828ms step_avg:38.60ms
step:722/1845 train_time:27892ms step_avg:38.63ms
step:723/1845 train_time:27952ms step_avg:38.66ms
step:724/1845 train_time:28014ms step_avg:38.69ms
step:725/1845 train_time:28075ms step_avg:38.72ms
step:726/1845 train_time:28137ms step_avg:38.76ms
step:727/1845 train_time:28197ms step_avg:38.79ms
step:728/1845 train_time:28260ms step_avg:38.82ms
step:729/1845 train_time:28320ms step_avg:38.85ms
step:730/1845 train_time:28383ms step_avg:38.88ms
step:731/1845 train_time:28443ms step_avg:38.91ms
step:732/1845 train_time:28505ms step_avg:38.94ms
step:733/1845 train_time:28565ms step_avg:38.97ms
step:734/1845 train_time:28628ms step_avg:39.00ms
step:735/1845 train_time:28688ms step_avg:39.03ms
step:736/1845 train_time:28750ms step_avg:39.06ms
step:737/1845 train_time:28810ms step_avg:39.09ms
step:738/1845 train_time:28874ms step_avg:39.12ms
step:739/1845 train_time:28934ms step_avg:39.15ms
step:740/1845 train_time:28996ms step_avg:39.18ms
step:741/1845 train_time:29055ms step_avg:39.21ms
step:742/1845 train_time:29118ms step_avg:39.24ms
step:743/1845 train_time:29178ms step_avg:39.27ms
step:744/1845 train_time:29241ms step_avg:39.30ms
step:745/1845 train_time:29301ms step_avg:39.33ms
step:746/1845 train_time:29363ms step_avg:39.36ms
step:747/1845 train_time:29423ms step_avg:39.39ms
step:748/1845 train_time:29486ms step_avg:39.42ms
step:749/1845 train_time:29546ms step_avg:39.45ms
step:750/1845 train_time:29608ms step_avg:39.48ms
step:750/1845 val_loss:4.0264 train_time:29678ms step_avg:39.57ms
step:751/1845 train_time:29704ms step_avg:39.55ms
step:752/1845 train_time:29732ms step_avg:39.54ms
step:753/1845 train_time:29793ms step_avg:39.57ms
step:754/1845 train_time:29858ms step_avg:39.60ms
step:755/1845 train_time:29919ms step_avg:39.63ms
step:756/1845 train_time:29981ms step_avg:39.66ms
step:757/1845 train_time:30040ms step_avg:39.68ms
step:758/1845 train_time:30102ms step_avg:39.71ms
step:759/1845 train_time:30162ms step_avg:39.74ms
step:760/1845 train_time:30224ms step_avg:39.77ms
step:761/1845 train_time:30284ms step_avg:39.79ms
step:762/1845 train_time:30347ms step_avg:39.83ms
step:763/1845 train_time:30406ms step_avg:39.85ms
step:764/1845 train_time:30469ms step_avg:39.88ms
step:765/1845 train_time:30528ms step_avg:39.91ms
step:766/1845 train_time:30591ms step_avg:39.94ms
step:767/1845 train_time:30651ms step_avg:39.96ms
step:768/1845 train_time:30716ms step_avg:39.99ms
step:769/1845 train_time:30777ms step_avg:40.02ms
step:770/1845 train_time:30839ms step_avg:40.05ms
step:771/1845 train_time:30899ms step_avg:40.08ms
step:772/1845 train_time:30962ms step_avg:40.11ms
step:773/1845 train_time:31022ms step_avg:40.13ms
step:774/1845 train_time:31084ms step_avg:40.16ms
step:775/1845 train_time:31144ms step_avg:40.19ms
step:776/1845 train_time:31206ms step_avg:40.21ms
step:777/1845 train_time:31266ms step_avg:40.24ms
step:778/1845 train_time:31328ms step_avg:40.27ms
step:779/1845 train_time:31388ms step_avg:40.29ms
step:780/1845 train_time:31450ms step_avg:40.32ms
step:781/1845 train_time:31510ms step_avg:40.35ms
step:782/1845 train_time:31572ms step_avg:40.37ms
step:783/1845 train_time:31632ms step_avg:40.40ms
step:784/1845 train_time:31696ms step_avg:40.43ms
step:785/1845 train_time:31756ms step_avg:40.45ms
step:786/1845 train_time:31819ms step_avg:40.48ms
step:787/1845 train_time:31879ms step_avg:40.51ms
step:788/1845 train_time:31942ms step_avg:40.54ms
step:789/1845 train_time:32001ms step_avg:40.56ms
step:790/1845 train_time:32064ms step_avg:40.59ms
step:791/1845 train_time:32124ms step_avg:40.61ms
step:792/1845 train_time:32186ms step_avg:40.64ms
step:793/1845 train_time:32246ms step_avg:40.66ms
step:794/1845 train_time:32308ms step_avg:40.69ms
step:795/1845 train_time:32368ms step_avg:40.71ms
step:796/1845 train_time:32431ms step_avg:40.74ms
step:797/1845 train_time:32490ms step_avg:40.77ms
step:798/1845 train_time:32552ms step_avg:40.79ms
step:799/1845 train_time:32612ms step_avg:40.82ms
step:800/1845 train_time:32675ms step_avg:40.84ms
step:801/1845 train_time:32735ms step_avg:40.87ms
step:802/1845 train_time:32798ms step_avg:40.89ms
step:803/1845 train_time:32858ms step_avg:40.92ms
step:804/1845 train_time:32920ms step_avg:40.95ms
step:805/1845 train_time:32981ms step_avg:40.97ms
step:806/1845 train_time:33043ms step_avg:41.00ms
step:807/1845 train_time:33102ms step_avg:41.02ms
step:808/1845 train_time:33165ms step_avg:41.05ms
step:809/1845 train_time:33225ms step_avg:41.07ms
step:810/1845 train_time:33287ms step_avg:41.10ms
step:811/1845 train_time:33347ms step_avg:41.12ms
step:812/1845 train_time:33409ms step_avg:41.14ms
step:813/1845 train_time:33469ms step_avg:41.17ms
step:814/1845 train_time:33532ms step_avg:41.19ms
step:815/1845 train_time:33592ms step_avg:41.22ms
step:816/1845 train_time:33655ms step_avg:41.24ms
step:817/1845 train_time:33715ms step_avg:41.27ms
step:818/1845 train_time:33777ms step_avg:41.29ms
step:819/1845 train_time:33837ms step_avg:41.32ms
step:820/1845 train_time:33899ms step_avg:41.34ms
step:821/1845 train_time:33959ms step_avg:41.36ms
step:822/1845 train_time:34022ms step_avg:41.39ms
step:823/1845 train_time:34082ms step_avg:41.41ms
step:824/1845 train_time:34144ms step_avg:41.44ms
step:825/1845 train_time:34204ms step_avg:41.46ms
step:826/1845 train_time:34267ms step_avg:41.49ms
step:827/1845 train_time:34327ms step_avg:41.51ms
step:828/1845 train_time:34389ms step_avg:41.53ms
step:829/1845 train_time:34449ms step_avg:41.55ms
step:830/1845 train_time:34511ms step_avg:41.58ms
step:831/1845 train_time:34571ms step_avg:41.60ms
step:832/1845 train_time:34634ms step_avg:41.63ms
step:833/1845 train_time:34694ms step_avg:41.65ms
step:834/1845 train_time:34757ms step_avg:41.68ms
step:835/1845 train_time:34817ms step_avg:41.70ms
step:836/1845 train_time:34880ms step_avg:41.72ms
step:837/1845 train_time:34941ms step_avg:41.75ms
step:838/1845 train_time:35003ms step_avg:41.77ms
step:839/1845 train_time:35063ms step_avg:41.79ms
step:840/1845 train_time:35125ms step_avg:41.82ms
step:841/1845 train_time:35185ms step_avg:41.84ms
step:842/1845 train_time:35248ms step_avg:41.86ms
step:843/1845 train_time:35308ms step_avg:41.88ms
step:844/1845 train_time:35371ms step_avg:41.91ms
step:845/1845 train_time:35431ms step_avg:41.93ms
step:846/1845 train_time:35494ms step_avg:41.95ms
step:847/1845 train_time:35554ms step_avg:41.98ms
step:848/1845 train_time:35616ms step_avg:42.00ms
step:849/1845 train_time:35676ms step_avg:42.02ms
step:850/1845 train_time:35739ms step_avg:42.05ms
step:851/1845 train_time:35798ms step_avg:42.07ms
step:852/1845 train_time:35861ms step_avg:42.09ms
step:853/1845 train_time:35921ms step_avg:42.11ms
step:854/1845 train_time:35984ms step_avg:42.14ms
step:855/1845 train_time:36044ms step_avg:42.16ms
step:856/1845 train_time:36106ms step_avg:42.18ms
step:857/1845 train_time:36166ms step_avg:42.20ms
step:858/1845 train_time:36229ms step_avg:42.23ms
step:859/1845 train_time:36289ms step_avg:42.25ms
step:860/1845 train_time:36352ms step_avg:42.27ms
step:861/1845 train_time:36411ms step_avg:42.29ms
step:862/1845 train_time:36474ms step_avg:42.31ms
step:863/1845 train_time:36534ms step_avg:42.33ms
step:864/1845 train_time:36597ms step_avg:42.36ms
step:865/1845 train_time:36657ms step_avg:42.38ms
step:866/1845 train_time:36719ms step_avg:42.40ms
step:867/1845 train_time:36779ms step_avg:42.42ms
step:868/1845 train_time:36841ms step_avg:42.44ms
step:869/1845 train_time:36901ms step_avg:42.46ms
step:870/1845 train_time:36963ms step_avg:42.49ms
step:871/1845 train_time:37023ms step_avg:42.51ms
step:872/1845 train_time:37085ms step_avg:42.53ms
step:873/1845 train_time:37146ms step_avg:42.55ms
step:874/1845 train_time:37209ms step_avg:42.57ms
step:875/1845 train_time:37268ms step_avg:42.59ms
step:876/1845 train_time:37331ms step_avg:42.62ms
step:877/1845 train_time:37391ms step_avg:42.63ms
step:878/1845 train_time:37453ms step_avg:42.66ms
step:879/1845 train_time:37513ms step_avg:42.68ms
step:880/1845 train_time:37576ms step_avg:42.70ms
step:881/1845 train_time:37635ms step_avg:42.72ms
step:882/1845 train_time:37698ms step_avg:42.74ms
step:883/1845 train_time:37758ms step_avg:42.76ms
step:884/1845 train_time:37820ms step_avg:42.78ms
step:885/1845 train_time:37880ms step_avg:42.80ms
step:886/1845 train_time:37943ms step_avg:42.82ms
step:887/1845 train_time:38002ms step_avg:42.84ms
step:888/1845 train_time:38065ms step_avg:42.87ms
step:889/1845 train_time:38125ms step_avg:42.89ms
step:890/1845 train_time:38187ms step_avg:42.91ms
step:891/1845 train_time:38247ms step_avg:42.93ms
step:892/1845 train_time:38309ms step_avg:42.95ms
step:893/1845 train_time:38369ms step_avg:42.97ms
step:894/1845 train_time:38431ms step_avg:42.99ms
step:895/1845 train_time:38492ms step_avg:43.01ms
step:896/1845 train_time:38555ms step_avg:43.03ms
step:897/1845 train_time:38615ms step_avg:43.05ms
step:898/1845 train_time:38678ms step_avg:43.07ms
step:899/1845 train_time:38739ms step_avg:43.09ms
step:900/1845 train_time:38801ms step_avg:43.11ms
step:901/1845 train_time:38861ms step_avg:43.13ms
step:902/1845 train_time:38923ms step_avg:43.15ms
step:903/1845 train_time:38983ms step_avg:43.17ms
step:904/1845 train_time:39046ms step_avg:43.19ms
step:905/1845 train_time:39106ms step_avg:43.21ms
step:906/1845 train_time:39168ms step_avg:43.23ms
step:907/1845 train_time:39228ms step_avg:43.25ms
step:908/1845 train_time:39290ms step_avg:43.27ms
step:909/1845 train_time:39350ms step_avg:43.29ms
step:910/1845 train_time:39412ms step_avg:43.31ms
step:911/1845 train_time:39472ms step_avg:43.33ms
step:912/1845 train_time:39535ms step_avg:43.35ms
step:913/1845 train_time:39595ms step_avg:43.37ms
step:914/1845 train_time:39658ms step_avg:43.39ms
step:915/1845 train_time:39718ms step_avg:43.41ms
step:916/1845 train_time:39781ms step_avg:43.43ms
step:917/1845 train_time:39841ms step_avg:43.45ms
step:918/1845 train_time:39903ms step_avg:43.47ms
step:919/1845 train_time:39962ms step_avg:43.48ms
step:920/1845 train_time:40026ms step_avg:43.51ms
step:921/1845 train_time:40086ms step_avg:43.52ms
step:922/1845 train_time:40148ms step_avg:43.54ms
step:923/1845 train_time:40207ms step_avg:43.56ms
step:924/1845 train_time:40270ms step_avg:43.58ms
step:925/1845 train_time:40329ms step_avg:43.60ms
step:926/1845 train_time:40392ms step_avg:43.62ms
step:927/1845 train_time:40453ms step_avg:43.64ms
step:928/1845 train_time:40515ms step_avg:43.66ms
step:929/1845 train_time:40575ms step_avg:43.68ms
step:930/1845 train_time:40638ms step_avg:43.70ms
step:931/1845 train_time:40698ms step_avg:43.71ms
step:932/1845 train_time:40760ms step_avg:43.73ms
step:933/1845 train_time:40820ms step_avg:43.75ms
step:934/1845 train_time:40882ms step_avg:43.77ms
step:935/1845 train_time:40943ms step_avg:43.79ms
step:936/1845 train_time:41005ms step_avg:43.81ms
step:937/1845 train_time:41065ms step_avg:43.83ms
step:938/1845 train_time:41127ms step_avg:43.85ms
step:939/1845 train_time:41187ms step_avg:43.86ms
step:940/1845 train_time:41251ms step_avg:43.88ms
step:941/1845 train_time:41310ms step_avg:43.90ms
step:942/1845 train_time:41373ms step_avg:43.92ms
step:943/1845 train_time:41433ms step_avg:43.94ms
step:944/1845 train_time:41495ms step_avg:43.96ms
step:945/1845 train_time:41555ms step_avg:43.97ms
step:946/1845 train_time:41618ms step_avg:43.99ms
step:947/1845 train_time:41678ms step_avg:44.01ms
step:948/1845 train_time:41740ms step_avg:44.03ms
step:949/1845 train_time:41800ms step_avg:44.05ms
step:950/1845 train_time:41863ms step_avg:44.07ms
step:951/1845 train_time:41923ms step_avg:44.08ms
step:952/1845 train_time:41985ms step_avg:44.10ms
step:953/1845 train_time:42046ms step_avg:44.12ms
step:954/1845 train_time:42108ms step_avg:44.14ms
step:955/1845 train_time:42168ms step_avg:44.16ms
step:956/1845 train_time:42230ms step_avg:44.17ms
step:957/1845 train_time:42290ms step_avg:44.19ms
step:958/1845 train_time:42353ms step_avg:44.21ms
step:959/1845 train_time:42413ms step_avg:44.23ms
step:960/1845 train_time:42475ms step_avg:44.24ms
step:961/1845 train_time:42535ms step_avg:44.26ms
step:962/1845 train_time:42597ms step_avg:44.28ms
step:963/1845 train_time:42657ms step_avg:44.30ms
step:964/1845 train_time:42719ms step_avg:44.31ms
step:965/1845 train_time:42779ms step_avg:44.33ms
step:966/1845 train_time:42842ms step_avg:44.35ms
step:967/1845 train_time:42902ms step_avg:44.37ms
step:968/1845 train_time:42964ms step_avg:44.38ms
step:969/1845 train_time:43024ms step_avg:44.40ms
step:970/1845 train_time:43086ms step_avg:44.42ms
step:971/1845 train_time:43146ms step_avg:44.44ms
step:972/1845 train_time:43208ms step_avg:44.45ms
step:973/1845 train_time:43268ms step_avg:44.47ms
step:974/1845 train_time:43332ms step_avg:44.49ms
step:975/1845 train_time:43392ms step_avg:44.50ms
step:976/1845 train_time:43454ms step_avg:44.52ms
step:977/1845 train_time:43514ms step_avg:44.54ms
step:978/1845 train_time:43576ms step_avg:44.56ms
step:979/1845 train_time:43636ms step_avg:44.57ms
step:980/1845 train_time:43699ms step_avg:44.59ms
step:981/1845 train_time:43759ms step_avg:44.61ms
step:982/1845 train_time:43821ms step_avg:44.62ms
step:983/1845 train_time:43881ms step_avg:44.64ms
step:984/1845 train_time:43944ms step_avg:44.66ms
step:985/1845 train_time:44004ms step_avg:44.67ms
step:986/1845 train_time:44067ms step_avg:44.69ms
step:987/1845 train_time:44127ms step_avg:44.71ms
step:988/1845 train_time:44189ms step_avg:44.73ms
step:989/1845 train_time:44249ms step_avg:44.74ms
step:990/1845 train_time:44311ms step_avg:44.76ms
step:991/1845 train_time:44371ms step_avg:44.77ms
step:992/1845 train_time:44434ms step_avg:44.79ms
step:993/1845 train_time:44494ms step_avg:44.81ms
step:994/1845 train_time:44557ms step_avg:44.83ms
step:995/1845 train_time:44617ms step_avg:44.84ms
step:996/1845 train_time:44679ms step_avg:44.86ms
step:997/1845 train_time:44740ms step_avg:44.87ms
step:998/1845 train_time:44802ms step_avg:44.89ms
step:999/1845 train_time:44862ms step_avg:44.91ms
step:1000/1845 train_time:44925ms step_avg:44.93ms
step:1000/1845 val_loss:3.7760 train_time:44995ms step_avg:44.99ms
step:1001/1845 train_time:45021ms step_avg:44.98ms
step:1002/1845 train_time:45049ms step_avg:44.96ms
step:1003/1845 train_time:45109ms step_avg:44.97ms
step:1004/1845 train_time:45173ms step_avg:44.99ms
step:1005/1845 train_time:45233ms step_avg:45.01ms
step:1006/1845 train_time:45295ms step_avg:45.02ms
step:1007/1845 train_time:45355ms step_avg:45.04ms
step:1008/1845 train_time:45417ms step_avg:45.06ms
step:1009/1845 train_time:45478ms step_avg:45.07ms
step:1010/1845 train_time:45542ms step_avg:45.09ms
step:1011/1845 train_time:45602ms step_avg:45.11ms
step:1012/1845 train_time:45664ms step_avg:45.12ms
step:1013/1845 train_time:45723ms step_avg:45.14ms
step:1014/1845 train_time:45786ms step_avg:45.15ms
step:1015/1845 train_time:45846ms step_avg:45.17ms
step:1016/1845 train_time:45909ms step_avg:45.19ms
step:1017/1845 train_time:45969ms step_avg:45.20ms
step:1018/1845 train_time:46032ms step_avg:45.22ms
step:1019/1845 train_time:46092ms step_avg:45.23ms
step:1020/1845 train_time:46155ms step_avg:45.25ms
step:1021/1845 train_time:46215ms step_avg:45.26ms
step:1022/1845 train_time:46278ms step_avg:45.28ms
step:1023/1845 train_time:46337ms step_avg:45.30ms
step:1024/1845 train_time:46399ms step_avg:45.31ms
step:1025/1845 train_time:46460ms step_avg:45.33ms
step:1026/1845 train_time:46522ms step_avg:45.34ms
step:1027/1845 train_time:46583ms step_avg:45.36ms
step:1028/1845 train_time:46645ms step_avg:45.37ms
step:1029/1845 train_time:46705ms step_avg:45.39ms
step:1030/1845 train_time:46767ms step_avg:45.40ms
step:1031/1845 train_time:46826ms step_avg:45.42ms
step:1032/1845 train_time:46888ms step_avg:45.43ms
step:1033/1845 train_time:46949ms step_avg:45.45ms
step:1034/1845 train_time:47012ms step_avg:45.47ms
step:1035/1845 train_time:47072ms step_avg:45.48ms
step:1036/1845 train_time:47135ms step_avg:45.50ms
step:1037/1845 train_time:47196ms step_avg:45.51ms
step:1038/1845 train_time:47258ms step_avg:45.53ms
step:1039/1845 train_time:47317ms step_avg:45.54ms
step:1040/1845 train_time:47380ms step_avg:45.56ms
step:1041/1845 train_time:47439ms step_avg:45.57ms
step:1042/1845 train_time:47503ms step_avg:45.59ms
step:1043/1845 train_time:47561ms step_avg:45.60ms
step:1044/1845 train_time:47625ms step_avg:45.62ms
step:1045/1845 train_time:47685ms step_avg:45.63ms
step:1046/1845 train_time:47748ms step_avg:45.65ms
step:1047/1845 train_time:47807ms step_avg:45.66ms
step:1048/1845 train_time:47869ms step_avg:45.68ms
step:1049/1845 train_time:47930ms step_avg:45.69ms
step:1050/1845 train_time:47993ms step_avg:45.71ms
step:1051/1845 train_time:48053ms step_avg:45.72ms
step:1052/1845 train_time:48116ms step_avg:45.74ms
step:1053/1845 train_time:48176ms step_avg:45.75ms
step:1054/1845 train_time:48238ms step_avg:45.77ms
step:1055/1845 train_time:48298ms step_avg:45.78ms
step:1056/1845 train_time:48360ms step_avg:45.80ms
step:1057/1845 train_time:48420ms step_avg:45.81ms
step:1058/1845 train_time:48483ms step_avg:45.83ms
step:1059/1845 train_time:48543ms step_avg:45.84ms
step:1060/1845 train_time:48605ms step_avg:45.85ms
step:1061/1845 train_time:48664ms step_avg:45.87ms
step:1062/1845 train_time:48727ms step_avg:45.88ms
step:1063/1845 train_time:48786ms step_avg:45.89ms
step:1064/1845 train_time:48849ms step_avg:45.91ms
step:1065/1845 train_time:48909ms step_avg:45.92ms
step:1066/1845 train_time:48972ms step_avg:45.94ms
step:1067/1845 train_time:49031ms step_avg:45.95ms
step:1068/1845 train_time:49095ms step_avg:45.97ms
step:1069/1845 train_time:49156ms step_avg:45.98ms
step:1070/1845 train_time:49218ms step_avg:46.00ms
step:1071/1845 train_time:49278ms step_avg:46.01ms
step:1072/1845 train_time:49340ms step_avg:46.03ms
step:1073/1845 train_time:49400ms step_avg:46.04ms
step:1074/1845 train_time:49463ms step_avg:46.05ms
step:1075/1845 train_time:49522ms step_avg:46.07ms
step:1076/1845 train_time:49585ms step_avg:46.08ms
step:1077/1845 train_time:49644ms step_avg:46.09ms
step:1078/1845 train_time:49708ms step_avg:46.11ms
step:1079/1845 train_time:49767ms step_avg:46.12ms
step:1080/1845 train_time:49829ms step_avg:46.14ms
step:1081/1845 train_time:49889ms step_avg:46.15ms
step:1082/1845 train_time:49951ms step_avg:46.17ms
step:1083/1845 train_time:50011ms step_avg:46.18ms
step:1084/1845 train_time:50074ms step_avg:46.19ms
step:1085/1845 train_time:50135ms step_avg:46.21ms
step:1086/1845 train_time:50198ms step_avg:46.22ms
step:1087/1845 train_time:50257ms step_avg:46.24ms
step:1088/1845 train_time:50320ms step_avg:46.25ms
step:1089/1845 train_time:50380ms step_avg:46.26ms
step:1090/1845 train_time:50442ms step_avg:46.28ms
step:1091/1845 train_time:50502ms step_avg:46.29ms
step:1092/1845 train_time:50564ms step_avg:46.30ms
step:1093/1845 train_time:50624ms step_avg:46.32ms
step:1094/1845 train_time:50687ms step_avg:46.33ms
step:1095/1845 train_time:50747ms step_avg:46.34ms
step:1096/1845 train_time:50809ms step_avg:46.36ms
step:1097/1845 train_time:50869ms step_avg:46.37ms
step:1098/1845 train_time:50931ms step_avg:46.39ms
step:1099/1845 train_time:50991ms step_avg:46.40ms
step:1100/1845 train_time:51054ms step_avg:46.41ms
step:1101/1845 train_time:51114ms step_avg:46.42ms
step:1102/1845 train_time:51176ms step_avg:46.44ms
step:1103/1845 train_time:51236ms step_avg:46.45ms
step:1104/1845 train_time:51299ms step_avg:46.47ms
step:1105/1845 train_time:51359ms step_avg:46.48ms
step:1106/1845 train_time:51421ms step_avg:46.49ms
step:1107/1845 train_time:51481ms step_avg:46.50ms
step:1108/1845 train_time:51543ms step_avg:46.52ms
step:1109/1845 train_time:51603ms step_avg:46.53ms
step:1110/1845 train_time:51665ms step_avg:46.54ms
step:1111/1845 train_time:51725ms step_avg:46.56ms
step:1112/1845 train_time:51787ms step_avg:46.57ms
step:1113/1845 train_time:51847ms step_avg:46.58ms
step:1114/1845 train_time:51910ms step_avg:46.60ms
step:1115/1845 train_time:51969ms step_avg:46.61ms
step:1116/1845 train_time:52032ms step_avg:46.62ms
step:1117/1845 train_time:52092ms step_avg:46.64ms
step:1118/1845 train_time:52155ms step_avg:46.65ms
step:1119/1845 train_time:52216ms step_avg:46.66ms
step:1120/1845 train_time:52279ms step_avg:46.68ms
step:1121/1845 train_time:52339ms step_avg:46.69ms
step:1122/1845 train_time:52401ms step_avg:46.70ms
step:1123/1845 train_time:52462ms step_avg:46.72ms
step:1124/1845 train_time:52523ms step_avg:46.73ms
step:1125/1845 train_time:52582ms step_avg:46.74ms
step:1126/1845 train_time:52646ms step_avg:46.75ms
step:1127/1845 train_time:52706ms step_avg:46.77ms
step:1128/1845 train_time:52768ms step_avg:46.78ms
step:1129/1845 train_time:52828ms step_avg:46.79ms
step:1130/1845 train_time:52891ms step_avg:46.81ms
step:1131/1845 train_time:52951ms step_avg:46.82ms
step:1132/1845 train_time:53014ms step_avg:46.83ms
step:1133/1845 train_time:53074ms step_avg:46.84ms
step:1134/1845 train_time:53137ms step_avg:46.86ms
step:1135/1845 train_time:53196ms step_avg:46.87ms
step:1136/1845 train_time:53259ms step_avg:46.88ms
step:1137/1845 train_time:53319ms step_avg:46.89ms
step:1138/1845 train_time:53381ms step_avg:46.91ms
step:1139/1845 train_time:53442ms step_avg:46.92ms
step:1140/1845 train_time:53504ms step_avg:46.93ms
step:1141/1845 train_time:53563ms step_avg:46.94ms
step:1142/1845 train_time:53626ms step_avg:46.96ms
step:1143/1845 train_time:53686ms step_avg:46.97ms
step:1144/1845 train_time:53748ms step_avg:46.98ms
step:1145/1845 train_time:53808ms step_avg:46.99ms
step:1146/1845 train_time:53870ms step_avg:47.01ms
step:1147/1845 train_time:53930ms step_avg:47.02ms
step:1148/1845 train_time:53992ms step_avg:47.03ms
step:1149/1845 train_time:54052ms step_avg:47.04ms
step:1150/1845 train_time:54115ms step_avg:47.06ms
step:1151/1845 train_time:54175ms step_avg:47.07ms
step:1152/1845 train_time:54236ms step_avg:47.08ms
step:1153/1845 train_time:54297ms step_avg:47.09ms
step:1154/1845 train_time:54360ms step_avg:47.11ms
step:1155/1845 train_time:54420ms step_avg:47.12ms
step:1156/1845 train_time:54482ms step_avg:47.13ms
step:1157/1845 train_time:54542ms step_avg:47.14ms
step:1158/1845 train_time:54605ms step_avg:47.15ms
step:1159/1845 train_time:54665ms step_avg:47.17ms
step:1160/1845 train_time:54727ms step_avg:47.18ms
step:1161/1845 train_time:54787ms step_avg:47.19ms
step:1162/1845 train_time:54849ms step_avg:47.20ms
step:1163/1845 train_time:54909ms step_avg:47.21ms
step:1164/1845 train_time:54972ms step_avg:47.23ms
step:1165/1845 train_time:55032ms step_avg:47.24ms
step:1166/1845 train_time:55094ms step_avg:47.25ms
step:1167/1845 train_time:55154ms step_avg:47.26ms
step:1168/1845 train_time:55216ms step_avg:47.27ms
step:1169/1845 train_time:55277ms step_avg:47.29ms
step:1170/1845 train_time:55340ms step_avg:47.30ms
step:1171/1845 train_time:55400ms step_avg:47.31ms
step:1172/1845 train_time:55461ms step_avg:47.32ms
step:1173/1845 train_time:55521ms step_avg:47.33ms
step:1174/1845 train_time:55584ms step_avg:47.35ms
step:1175/1845 train_time:55644ms step_avg:47.36ms
step:1176/1845 train_time:55707ms step_avg:47.37ms
step:1177/1845 train_time:55766ms step_avg:47.38ms
step:1178/1845 train_time:55829ms step_avg:47.39ms
step:1179/1845 train_time:55889ms step_avg:47.40ms
step:1180/1845 train_time:55951ms step_avg:47.42ms
step:1181/1845 train_time:56012ms step_avg:47.43ms
step:1182/1845 train_time:56074ms step_avg:47.44ms
step:1183/1845 train_time:56134ms step_avg:47.45ms
step:1184/1845 train_time:56197ms step_avg:47.46ms
step:1185/1845 train_time:56256ms step_avg:47.47ms
step:1186/1845 train_time:56319ms step_avg:47.49ms
step:1187/1845 train_time:56379ms step_avg:47.50ms
step:1188/1845 train_time:56441ms step_avg:47.51ms
step:1189/1845 train_time:56501ms step_avg:47.52ms
step:1190/1845 train_time:56563ms step_avg:47.53ms
step:1191/1845 train_time:56624ms step_avg:47.54ms
step:1192/1845 train_time:56686ms step_avg:47.56ms
step:1193/1845 train_time:56745ms step_avg:47.57ms
step:1194/1845 train_time:56808ms step_avg:47.58ms
step:1195/1845 train_time:56868ms step_avg:47.59ms
step:1196/1845 train_time:56931ms step_avg:47.60ms
step:1197/1845 train_time:56991ms step_avg:47.61ms
step:1198/1845 train_time:57053ms step_avg:47.62ms
step:1199/1845 train_time:57113ms step_avg:47.63ms
step:1200/1845 train_time:57176ms step_avg:47.65ms
step:1201/1845 train_time:57236ms step_avg:47.66ms
step:1202/1845 train_time:57299ms step_avg:47.67ms
step:1203/1845 train_time:57358ms step_avg:47.68ms
step:1204/1845 train_time:57421ms step_avg:47.69ms
step:1205/1845 train_time:57482ms step_avg:47.70ms
step:1206/1845 train_time:57569ms step_avg:47.74ms
step:1207/1845 train_time:57658ms step_avg:47.77ms
step:1208/1845 train_time:57746ms step_avg:47.80ms
step:1209/1845 train_time:57832ms step_avg:47.83ms
step:1210/1845 train_time:57922ms step_avg:47.87ms
step:1211/1845 train_time:58008ms step_avg:47.90ms
step:1212/1845 train_time:58097ms step_avg:47.94ms
step:1213/1845 train_time:58185ms step_avg:47.97ms
step:1214/1845 train_time:58274ms step_avg:48.00ms
step:1215/1845 train_time:58361ms step_avg:48.03ms
step:1216/1845 train_time:58451ms step_avg:48.07ms
step:1217/1845 train_time:58537ms step_avg:48.10ms
step:1218/1845 train_time:58625ms step_avg:48.13ms
step:1219/1845 train_time:58711ms step_avg:48.16ms
step:1220/1845 train_time:58802ms step_avg:48.20ms
step:1221/1845 train_time:58888ms step_avg:48.23ms
step:1222/1845 train_time:58978ms step_avg:48.26ms
step:1223/1845 train_time:59066ms step_avg:48.30ms
step:1224/1845 train_time:59153ms step_avg:48.33ms
step:1225/1845 train_time:59239ms step_avg:48.36ms
step:1226/1845 train_time:59330ms step_avg:48.39ms
step:1227/1845 train_time:59415ms step_avg:48.42ms
step:1228/1845 train_time:59505ms step_avg:48.46ms
step:1229/1845 train_time:59590ms step_avg:48.49ms
step:1230/1845 train_time:59679ms step_avg:48.52ms
step:1231/1845 train_time:59767ms step_avg:48.55ms
step:1232/1845 train_time:59854ms step_avg:48.58ms
step:1233/1845 train_time:59940ms step_avg:48.61ms
step:1234/1845 train_time:60030ms step_avg:48.65ms
step:1235/1845 train_time:60117ms step_avg:48.68ms
step:1236/1845 train_time:60206ms step_avg:48.71ms
step:1237/1845 train_time:60292ms step_avg:48.74ms
step:1238/1845 train_time:60381ms step_avg:48.77ms
step:1239/1845 train_time:60468ms step_avg:48.80ms
step:1240/1845 train_time:60557ms step_avg:48.84ms
step:1241/1845 train_time:60642ms step_avg:48.87ms
step:1242/1845 train_time:60731ms step_avg:48.90ms
step:1243/1845 train_time:60818ms step_avg:48.93ms
step:1244/1845 train_time:60907ms step_avg:48.96ms
step:1245/1845 train_time:60993ms step_avg:48.99ms
step:1246/1845 train_time:61083ms step_avg:49.02ms
step:1247/1845 train_time:61169ms step_avg:49.05ms
step:1248/1845 train_time:61257ms step_avg:49.08ms
step:1249/1845 train_time:61345ms step_avg:49.11ms
step:1250/1845 train_time:61433ms step_avg:49.15ms
step:1250/1845 val_loss:3.5359 train_time:61529ms step_avg:49.22ms
step:1251/1845 train_time:61556ms step_avg:49.21ms
step:1252/1845 train_time:61609ms step_avg:49.21ms
step:1253/1845 train_time:61696ms step_avg:49.24ms
step:1254/1845 train_time:61787ms step_avg:49.27ms
step:1255/1845 train_time:61874ms step_avg:49.30ms
step:1256/1845 train_time:61962ms step_avg:49.33ms
step:1257/1845 train_time:62047ms step_avg:49.36ms
step:1258/1845 train_time:62135ms step_avg:49.39ms
step:1259/1845 train_time:62220ms step_avg:49.42ms
step:1260/1845 train_time:62309ms step_avg:49.45ms
step:1261/1845 train_time:62396ms step_avg:49.48ms
step:1262/1845 train_time:62488ms step_avg:49.52ms
step:1263/1845 train_time:62577ms step_avg:49.55ms
step:1264/1845 train_time:62667ms step_avg:49.58ms
step:1265/1845 train_time:62753ms step_avg:49.61ms
step:1266/1845 train_time:62841ms step_avg:49.64ms
step:1267/1845 train_time:62926ms step_avg:49.67ms
step:1268/1845 train_time:63015ms step_avg:49.70ms
step:1269/1845 train_time:63100ms step_avg:49.72ms
step:1270/1845 train_time:63188ms step_avg:49.75ms
step:1271/1845 train_time:63273ms step_avg:49.78ms
step:1272/1845 train_time:63362ms step_avg:49.81ms
step:1273/1845 train_time:63448ms step_avg:49.84ms
step:1274/1845 train_time:63538ms step_avg:49.87ms
step:1275/1845 train_time:63625ms step_avg:49.90ms
step:1276/1845 train_time:63715ms step_avg:49.93ms
step:1277/1845 train_time:63801ms step_avg:49.96ms
step:1278/1845 train_time:63890ms step_avg:49.99ms
step:1279/1845 train_time:63976ms step_avg:50.02ms
step:1280/1845 train_time:64066ms step_avg:50.05ms
step:1281/1845 train_time:64151ms step_avg:50.08ms
step:1282/1845 train_time:64239ms step_avg:50.11ms
step:1283/1845 train_time:64325ms step_avg:50.14ms
step:1284/1845 train_time:64413ms step_avg:50.17ms
step:1285/1845 train_time:64500ms step_avg:50.19ms
step:1286/1845 train_time:64591ms step_avg:50.23ms
step:1287/1845 train_time:64677ms step_avg:50.25ms
step:1288/1845 train_time:64768ms step_avg:50.29ms
step:1289/1845 train_time:64854ms step_avg:50.31ms
step:1290/1845 train_time:64943ms step_avg:50.34ms
step:1291/1845 train_time:65029ms step_avg:50.37ms
step:1292/1845 train_time:65117ms step_avg:50.40ms
step:1293/1845 train_time:65204ms step_avg:50.43ms
step:1294/1845 train_time:65293ms step_avg:50.46ms
step:1295/1845 train_time:65379ms step_avg:50.49ms
step:1296/1845 train_time:65468ms step_avg:50.52ms
step:1297/1845 train_time:65554ms step_avg:50.54ms
step:1298/1845 train_time:65645ms step_avg:50.57ms
step:1299/1845 train_time:65732ms step_avg:50.60ms
step:1300/1845 train_time:65820ms step_avg:50.63ms
step:1301/1845 train_time:65906ms step_avg:50.66ms
step:1302/1845 train_time:65994ms step_avg:50.69ms
step:1303/1845 train_time:66080ms step_avg:50.71ms
step:1304/1845 train_time:66169ms step_avg:50.74ms
step:1305/1845 train_time:66254ms step_avg:50.77ms
step:1306/1845 train_time:66343ms step_avg:50.80ms
step:1307/1845 train_time:66430ms step_avg:50.83ms
step:1308/1845 train_time:66518ms step_avg:50.85ms
step:1309/1845 train_time:66605ms step_avg:50.88ms
step:1310/1845 train_time:66695ms step_avg:50.91ms
step:1311/1845 train_time:66782ms step_avg:50.94ms
step:1312/1845 train_time:66871ms step_avg:50.97ms
step:1313/1845 train_time:66957ms step_avg:51.00ms
step:1314/1845 train_time:67046ms step_avg:51.02ms
step:1315/1845 train_time:67132ms step_avg:51.05ms
step:1316/1845 train_time:67220ms step_avg:51.08ms
step:1317/1845 train_time:67307ms step_avg:51.11ms
step:1318/1845 train_time:67394ms step_avg:51.13ms
step:1319/1845 train_time:67481ms step_avg:51.16ms
step:1320/1845 train_time:67571ms step_avg:51.19ms
step:1321/1845 train_time:67657ms step_avg:51.22ms
step:1322/1845 train_time:67746ms step_avg:51.25ms
step:1323/1845 train_time:67833ms step_avg:51.27ms
step:1324/1845 train_time:67921ms step_avg:51.30ms
step:1325/1845 train_time:68008ms step_avg:51.33ms
step:1326/1845 train_time:68095ms step_avg:51.35ms
step:1327/1845 train_time:68182ms step_avg:51.38ms
step:1328/1845 train_time:68270ms step_avg:51.41ms
step:1329/1845 train_time:68355ms step_avg:51.43ms
step:1330/1845 train_time:68446ms step_avg:51.46ms
step:1331/1845 train_time:68533ms step_avg:51.49ms
step:1332/1845 train_time:68621ms step_avg:51.52ms
step:1333/1845 train_time:68708ms step_avg:51.54ms
step:1334/1845 train_time:68796ms step_avg:51.57ms
step:1335/1845 train_time:68883ms step_avg:51.60ms
step:1336/1845 train_time:68973ms step_avg:51.63ms
step:1337/1845 train_time:69058ms step_avg:51.65ms
step:1338/1845 train_time:69148ms step_avg:51.68ms
step:1339/1845 train_time:69233ms step_avg:51.71ms
step:1340/1845 train_time:69323ms step_avg:51.73ms
step:1341/1845 train_time:69409ms step_avg:51.76ms
step:1342/1845 train_time:69497ms step_avg:51.79ms
step:1343/1845 train_time:69583ms step_avg:51.81ms
step:1344/1845 train_time:69673ms step_avg:51.84ms
step:1345/1845 train_time:69759ms step_avg:51.87ms
step:1346/1845 train_time:69848ms step_avg:51.89ms
step:1347/1845 train_time:69935ms step_avg:51.92ms
step:1348/1845 train_time:70025ms step_avg:51.95ms
step:1349/1845 train_time:70110ms step_avg:51.97ms
step:1350/1845 train_time:70198ms step_avg:52.00ms
step:1351/1845 train_time:70284ms step_avg:52.02ms
step:1352/1845 train_time:70373ms step_avg:52.05ms
step:1353/1845 train_time:70458ms step_avg:52.08ms
step:1354/1845 train_time:70548ms step_avg:52.10ms
step:1355/1845 train_time:70634ms step_avg:52.13ms
step:1356/1845 train_time:70725ms step_avg:52.16ms
step:1357/1845 train_time:70811ms step_avg:52.18ms
step:1358/1845 train_time:70899ms step_avg:52.21ms
step:1359/1845 train_time:70985ms step_avg:52.23ms
step:1360/1845 train_time:71075ms step_avg:52.26ms
step:1361/1845 train_time:71159ms step_avg:52.28ms
step:1362/1845 train_time:71249ms step_avg:52.31ms
step:1363/1845 train_time:71335ms step_avg:52.34ms
step:1364/1845 train_time:71423ms step_avg:52.36ms
step:1365/1845 train_time:71510ms step_avg:52.39ms
step:1366/1845 train_time:71597ms step_avg:52.41ms
step:1367/1845 train_time:71684ms step_avg:52.44ms
step:1368/1845 train_time:71774ms step_avg:52.47ms
step:1369/1845 train_time:71859ms step_avg:52.49ms
step:1370/1845 train_time:71949ms step_avg:52.52ms
step:1371/1845 train_time:72036ms step_avg:52.54ms
step:1372/1845 train_time:72124ms step_avg:52.57ms
step:1373/1845 train_time:72210ms step_avg:52.59ms
step:1374/1845 train_time:72298ms step_avg:52.62ms
step:1375/1845 train_time:72384ms step_avg:52.64ms
step:1376/1845 train_time:72473ms step_avg:52.67ms
step:1377/1845 train_time:72558ms step_avg:52.69ms
step:1378/1845 train_time:72649ms step_avg:52.72ms
step:1379/1845 train_time:72735ms step_avg:52.74ms
step:1380/1845 train_time:72825ms step_avg:52.77ms
step:1381/1845 train_time:72913ms step_avg:52.80ms
step:1382/1845 train_time:73002ms step_avg:52.82ms
step:1383/1845 train_time:73087ms step_avg:52.85ms
step:1384/1845 train_time:73175ms step_avg:52.87ms
step:1385/1845 train_time:73262ms step_avg:52.90ms
step:1386/1845 train_time:73351ms step_avg:52.92ms
step:1387/1845 train_time:73437ms step_avg:52.95ms
step:1388/1845 train_time:73526ms step_avg:52.97ms
step:1389/1845 train_time:73613ms step_avg:53.00ms
step:1390/1845 train_time:73701ms step_avg:53.02ms
step:1391/1845 train_time:73788ms step_avg:53.05ms
step:1392/1845 train_time:73876ms step_avg:53.07ms
step:1393/1845 train_time:73964ms step_avg:53.10ms
step:1394/1845 train_time:74053ms step_avg:53.12ms
step:1395/1845 train_time:74139ms step_avg:53.15ms
step:1396/1845 train_time:74228ms step_avg:53.17ms
step:1397/1845 train_time:74313ms step_avg:53.19ms
step:1398/1845 train_time:74403ms step_avg:53.22ms
step:1399/1845 train_time:74489ms step_avg:53.24ms
step:1400/1845 train_time:74578ms step_avg:53.27ms
step:1401/1845 train_time:74665ms step_avg:53.29ms
step:1402/1845 train_time:74753ms step_avg:53.32ms
step:1403/1845 train_time:74839ms step_avg:53.34ms
step:1404/1845 train_time:74928ms step_avg:53.37ms
step:1405/1845 train_time:75015ms step_avg:53.39ms
step:1406/1845 train_time:75103ms step_avg:53.42ms
step:1407/1845 train_time:75189ms step_avg:53.44ms
step:1408/1845 train_time:75277ms step_avg:53.46ms
step:1409/1845 train_time:75362ms step_avg:53.49ms
step:1410/1845 train_time:75453ms step_avg:53.51ms
step:1411/1845 train_time:75539ms step_avg:53.54ms
step:1412/1845 train_time:75629ms step_avg:53.56ms
step:1413/1845 train_time:75715ms step_avg:53.58ms
step:1414/1845 train_time:75805ms step_avg:53.61ms
step:1415/1845 train_time:75891ms step_avg:53.63ms
step:1416/1845 train_time:75979ms step_avg:53.66ms
step:1417/1845 train_time:76065ms step_avg:53.68ms
step:1418/1845 train_time:76154ms step_avg:53.71ms
step:1419/1845 train_time:76240ms step_avg:53.73ms
step:1420/1845 train_time:76329ms step_avg:53.75ms
step:1421/1845 train_time:76415ms step_avg:53.78ms
step:1422/1845 train_time:76506ms step_avg:53.80ms
step:1423/1845 train_time:76592ms step_avg:53.82ms
step:1424/1845 train_time:76680ms step_avg:53.85ms
step:1425/1845 train_time:76767ms step_avg:53.87ms
step:1426/1845 train_time:76855ms step_avg:53.90ms
step:1427/1845 train_time:76941ms step_avg:53.92ms
step:1428/1845 train_time:77029ms step_avg:53.94ms
step:1429/1845 train_time:77114ms step_avg:53.96ms
step:1430/1845 train_time:77205ms step_avg:53.99ms
step:1431/1845 train_time:77290ms step_avg:54.01ms
step:1432/1845 train_time:77378ms step_avg:54.04ms
step:1433/1845 train_time:77465ms step_avg:54.06ms
step:1434/1845 train_time:77555ms step_avg:54.08ms
step:1435/1845 train_time:77640ms step_avg:54.10ms
step:1436/1845 train_time:77730ms step_avg:54.13ms
step:1437/1845 train_time:77816ms step_avg:54.15ms
step:1438/1845 train_time:77906ms step_avg:54.18ms
step:1439/1845 train_time:77991ms step_avg:54.20ms
step:1440/1845 train_time:78080ms step_avg:54.22ms
step:1441/1845 train_time:78166ms step_avg:54.24ms
step:1442/1845 train_time:78255ms step_avg:54.27ms
step:1443/1845 train_time:78341ms step_avg:54.29ms
step:1444/1845 train_time:78429ms step_avg:54.31ms
step:1445/1845 train_time:78515ms step_avg:54.34ms
step:1446/1845 train_time:78604ms step_avg:54.36ms
step:1447/1845 train_time:78690ms step_avg:54.38ms
step:1448/1845 train_time:78778ms step_avg:54.40ms
step:1449/1845 train_time:78865ms step_avg:54.43ms
step:1450/1845 train_time:78953ms step_avg:54.45ms
step:1451/1845 train_time:79041ms step_avg:54.47ms
step:1452/1845 train_time:79131ms step_avg:54.50ms
step:1453/1845 train_time:79216ms step_avg:54.52ms
step:1454/1845 train_time:79305ms step_avg:54.54ms
step:1455/1845 train_time:79392ms step_avg:54.56ms
step:1456/1845 train_time:79480ms step_avg:54.59ms
step:1457/1845 train_time:79566ms step_avg:54.61ms
step:1458/1845 train_time:79654ms step_avg:54.63ms
step:1459/1845 train_time:79741ms step_avg:54.65ms
step:1460/1845 train_time:79831ms step_avg:54.68ms
step:1461/1845 train_time:79917ms step_avg:54.70ms
step:1462/1845 train_time:80007ms step_avg:54.72ms
step:1463/1845 train_time:80093ms step_avg:54.75ms
step:1464/1845 train_time:80180ms step_avg:54.77ms
step:1465/1845 train_time:80267ms step_avg:54.79ms
step:1466/1845 train_time:80355ms step_avg:54.81ms
step:1467/1845 train_time:80442ms step_avg:54.83ms
step:1468/1845 train_time:80531ms step_avg:54.86ms
step:1469/1845 train_time:80616ms step_avg:54.88ms
step:1470/1845 train_time:80706ms step_avg:54.90ms
step:1471/1845 train_time:80793ms step_avg:54.92ms
step:1472/1845 train_time:80882ms step_avg:54.95ms
step:1473/1845 train_time:80967ms step_avg:54.97ms
step:1474/1845 train_time:81055ms step_avg:54.99ms
step:1475/1845 train_time:81142ms step_avg:55.01ms
step:1476/1845 train_time:81232ms step_avg:55.04ms
step:1477/1845 train_time:81317ms step_avg:55.06ms
step:1478/1845 train_time:81408ms step_avg:55.08ms
step:1479/1845 train_time:81494ms step_avg:55.10ms
step:1480/1845 train_time:81582ms step_avg:55.12ms
step:1481/1845 train_time:81668ms step_avg:55.14ms
step:1482/1845 train_time:81756ms step_avg:55.17ms
step:1483/1845 train_time:81843ms step_avg:55.19ms
step:1484/1845 train_time:81933ms step_avg:55.21ms
step:1485/1845 train_time:82019ms step_avg:55.23ms
step:1486/1845 train_time:82108ms step_avg:55.25ms
step:1487/1845 train_time:82194ms step_avg:55.27ms
step:1488/1845 train_time:82283ms step_avg:55.30ms
step:1489/1845 train_time:82369ms step_avg:55.32ms
step:1490/1845 train_time:82458ms step_avg:55.34ms
step:1491/1845 train_time:82544ms step_avg:55.36ms
step:1492/1845 train_time:82633ms step_avg:55.38ms
step:1493/1845 train_time:82720ms step_avg:55.41ms
step:1494/1845 train_time:82810ms step_avg:55.43ms
step:1495/1845 train_time:82896ms step_avg:55.45ms
step:1496/1845 train_time:82985ms step_avg:55.47ms
step:1497/1845 train_time:83072ms step_avg:55.49ms
step:1498/1845 train_time:83159ms step_avg:55.51ms
step:1499/1845 train_time:83246ms step_avg:55.53ms
step:1500/1845 train_time:83335ms step_avg:55.56ms
step:1500/1845 val_loss:3.4039 train_time:83432ms step_avg:55.62ms
step:1501/1845 train_time:83459ms step_avg:55.60ms
step:1502/1845 train_time:83513ms step_avg:55.60ms
step:1503/1845 train_time:83600ms step_avg:55.62ms
step:1504/1845 train_time:83690ms step_avg:55.65ms
step:1505/1845 train_time:83776ms step_avg:55.67ms
step:1506/1845 train_time:83862ms step_avg:55.69ms
step:1507/1845 train_time:83948ms step_avg:55.71ms
step:1508/1845 train_time:84036ms step_avg:55.73ms
step:1509/1845 train_time:84122ms step_avg:55.75ms
step:1510/1845 train_time:84212ms step_avg:55.77ms
step:1511/1845 train_time:84297ms step_avg:55.79ms
step:1512/1845 train_time:84386ms step_avg:55.81ms
step:1513/1845 train_time:84473ms step_avg:55.83ms
step:1514/1845 train_time:84564ms step_avg:55.85ms
step:1515/1845 train_time:84652ms step_avg:55.88ms
step:1516/1845 train_time:84741ms step_avg:55.90ms
step:1517/1845 train_time:84827ms step_avg:55.92ms
step:1518/1845 train_time:84915ms step_avg:55.94ms
step:1519/1845 train_time:85000ms step_avg:55.96ms
step:1520/1845 train_time:85088ms step_avg:55.98ms
step:1521/1845 train_time:85174ms step_avg:56.00ms
step:1522/1845 train_time:85263ms step_avg:56.02ms
step:1523/1845 train_time:85349ms step_avg:56.04ms
step:1524/1845 train_time:85440ms step_avg:56.06ms
step:1525/1845 train_time:85527ms step_avg:56.08ms
step:1526/1845 train_time:85618ms step_avg:56.11ms
step:1527/1845 train_time:85704ms step_avg:56.13ms
step:1528/1845 train_time:85793ms step_avg:56.15ms
step:1529/1845 train_time:85878ms step_avg:56.17ms
step:1530/1845 train_time:85966ms step_avg:56.19ms
step:1531/1845 train_time:86052ms step_avg:56.21ms
step:1532/1845 train_time:86141ms step_avg:56.23ms
step:1533/1845 train_time:86226ms step_avg:56.25ms
step:1534/1845 train_time:86316ms step_avg:56.27ms
step:1535/1845 train_time:86402ms step_avg:56.29ms
step:1536/1845 train_time:86491ms step_avg:56.31ms
step:1537/1845 train_time:86579ms step_avg:56.33ms
step:1538/1845 train_time:86667ms step_avg:56.35ms
step:1539/1845 train_time:86754ms step_avg:56.37ms
step:1540/1845 train_time:86842ms step_avg:56.39ms
step:1541/1845 train_time:86927ms step_avg:56.41ms
step:1542/1845 train_time:87016ms step_avg:56.43ms
step:1543/1845 train_time:87101ms step_avg:56.45ms
step:1544/1845 train_time:87191ms step_avg:56.47ms
step:1545/1845 train_time:87276ms step_avg:56.49ms
step:1546/1845 train_time:87364ms step_avg:56.51ms
step:1547/1845 train_time:87452ms step_avg:56.53ms
step:1548/1845 train_time:87542ms step_avg:56.55ms
step:1549/1845 train_time:87628ms step_avg:56.57ms
step:1550/1845 train_time:87718ms step_avg:56.59ms
step:1551/1845 train_time:87804ms step_avg:56.61ms
step:1552/1845 train_time:87892ms step_avg:56.63ms
step:1553/1845 train_time:87978ms step_avg:56.65ms
step:1554/1845 train_time:88066ms step_avg:56.67ms
step:1555/1845 train_time:88153ms step_avg:56.69ms
step:1556/1845 train_time:88242ms step_avg:56.71ms
step:1557/1845 train_time:88327ms step_avg:56.73ms
step:1558/1845 train_time:88417ms step_avg:56.75ms
step:1559/1845 train_time:88504ms step_avg:56.77ms
step:1560/1845 train_time:88594ms step_avg:56.79ms
step:1561/1845 train_time:88681ms step_avg:56.81ms
step:1562/1845 train_time:88769ms step_avg:56.83ms
step:1563/1845 train_time:88855ms step_avg:56.85ms
step:1564/1845 train_time:88943ms step_avg:56.87ms
step:1565/1845 train_time:89030ms step_avg:56.89ms
step:1566/1845 train_time:89119ms step_avg:56.91ms
step:1567/1845 train_time:89204ms step_avg:56.93ms
step:1568/1845 train_time:89294ms step_avg:56.95ms
step:1569/1845 train_time:89380ms step_avg:56.97ms
step:1570/1845 train_time:89468ms step_avg:56.99ms
step:1571/1845 train_time:89555ms step_avg:57.00ms
step:1572/1845 train_time:89643ms step_avg:57.03ms
step:1573/1845 train_time:89731ms step_avg:57.04ms
step:1574/1845 train_time:89821ms step_avg:57.07ms
step:1575/1845 train_time:89906ms step_avg:57.08ms
step:1576/1845 train_time:89995ms step_avg:57.10ms
step:1577/1845 train_time:90081ms step_avg:57.12ms
step:1578/1845 train_time:90169ms step_avg:57.14ms
step:1579/1845 train_time:90255ms step_avg:57.16ms
step:1580/1845 train_time:90343ms step_avg:57.18ms
step:1581/1845 train_time:90429ms step_avg:57.20ms
step:1582/1845 train_time:90520ms step_avg:57.22ms
step:1583/1845 train_time:90605ms step_avg:57.24ms
step:1584/1845 train_time:90694ms step_avg:57.26ms
step:1585/1845 train_time:90781ms step_avg:57.28ms
step:1586/1845 train_time:90870ms step_avg:57.29ms
step:1587/1845 train_time:90956ms step_avg:57.31ms
step:1588/1845 train_time:91044ms step_avg:57.33ms
step:1589/1845 train_time:91129ms step_avg:57.35ms
step:1590/1845 train_time:91220ms step_avg:57.37ms
step:1591/1845 train_time:91305ms step_avg:57.39ms
step:1592/1845 train_time:91395ms step_avg:57.41ms
step:1593/1845 train_time:91481ms step_avg:57.43ms
step:1594/1845 train_time:91570ms step_avg:57.45ms
step:1595/1845 train_time:91657ms step_avg:57.46ms
step:1596/1845 train_time:91745ms step_avg:57.48ms
step:1597/1845 train_time:91831ms step_avg:57.50ms
step:1598/1845 train_time:91921ms step_avg:57.52ms
step:1599/1845 train_time:92007ms step_avg:57.54ms
step:1600/1845 train_time:92096ms step_avg:57.56ms
step:1601/1845 train_time:92183ms step_avg:57.58ms
step:1602/1845 train_time:92272ms step_avg:57.60ms
step:1603/1845 train_time:92357ms step_avg:57.62ms
step:1604/1845 train_time:92445ms step_avg:57.63ms
step:1605/1845 train_time:92532ms step_avg:57.65ms
step:1606/1845 train_time:92622ms step_avg:57.67ms
step:1607/1845 train_time:92707ms step_avg:57.69ms
step:1608/1845 train_time:92797ms step_avg:57.71ms
step:1609/1845 train_time:92883ms step_avg:57.73ms
step:1610/1845 train_time:92971ms step_avg:57.75ms
step:1611/1845 train_time:93059ms step_avg:57.76ms
step:1612/1845 train_time:93147ms step_avg:57.78ms
step:1613/1845 train_time:93233ms step_avg:57.80ms
step:1614/1845 train_time:93324ms step_avg:57.82ms
step:1615/1845 train_time:93409ms step_avg:57.84ms
step:1616/1845 train_time:93498ms step_avg:57.86ms
step:1617/1845 train_time:93584ms step_avg:57.88ms
step:1618/1845 train_time:93674ms step_avg:57.89ms
step:1619/1845 train_time:93761ms step_avg:57.91ms
step:1620/1845 train_time:93849ms step_avg:57.93ms
step:1621/1845 train_time:93935ms step_avg:57.95ms
step:1622/1845 train_time:94025ms step_avg:57.97ms
step:1623/1845 train_time:94111ms step_avg:57.99ms
step:1624/1845 train_time:94200ms step_avg:58.00ms
step:1625/1845 train_time:94286ms step_avg:58.02ms
step:1626/1845 train_time:94374ms step_avg:58.04ms
step:1627/1845 train_time:94461ms step_avg:58.06ms
step:1628/1845 train_time:94550ms step_avg:58.08ms
step:1629/1845 train_time:94636ms step_avg:58.09ms
step:1630/1845 train_time:94724ms step_avg:58.11ms
step:1631/1845 train_time:94810ms step_avg:58.13ms
step:1632/1845 train_time:94899ms step_avg:58.15ms
step:1633/1845 train_time:94985ms step_avg:58.17ms
step:1634/1845 train_time:95075ms step_avg:58.19ms
step:1635/1845 train_time:95162ms step_avg:58.20ms
step:1636/1845 train_time:95250ms step_avg:58.22ms
step:1637/1845 train_time:95336ms step_avg:58.24ms
step:1638/1845 train_time:95424ms step_avg:58.26ms
step:1639/1845 train_time:95512ms step_avg:58.27ms
step:1640/1845 train_time:95604ms step_avg:58.30ms
step:1641/1845 train_time:95690ms step_avg:58.31ms
step:1642/1845 train_time:95778ms step_avg:58.33ms
step:1643/1845 train_time:95864ms step_avg:58.35ms
step:1644/1845 train_time:95953ms step_avg:58.37ms
step:1645/1845 train_time:96039ms step_avg:58.38ms
step:1646/1845 train_time:96128ms step_avg:58.40ms
step:1647/1845 train_time:96214ms step_avg:58.42ms
step:1648/1845 train_time:96302ms step_avg:58.44ms
step:1649/1845 train_time:96388ms step_avg:58.45ms
step:1650/1845 train_time:96478ms step_avg:58.47ms
step:1651/1845 train_time:96563ms step_avg:58.49ms
step:1652/1845 train_time:96653ms step_avg:58.51ms
step:1653/1845 train_time:96738ms step_avg:58.52ms
step:1654/1845 train_time:96827ms step_avg:58.54ms
step:1655/1845 train_time:96913ms step_avg:58.56ms
step:1656/1845 train_time:97002ms step_avg:58.58ms
step:1657/1845 train_time:97088ms step_avg:58.59ms
step:1658/1845 train_time:97179ms step_avg:58.61ms
step:1659/1845 train_time:97265ms step_avg:58.63ms
step:1660/1845 train_time:97356ms step_avg:58.65ms
step:1661/1845 train_time:97442ms step_avg:58.66ms
step:1662/1845 train_time:97531ms step_avg:58.68ms
step:1663/1845 train_time:97618ms step_avg:58.70ms
step:1664/1845 train_time:97706ms step_avg:58.72ms
step:1665/1845 train_time:97792ms step_avg:58.73ms
step:1666/1845 train_time:97881ms step_avg:58.75ms
step:1667/1845 train_time:97967ms step_avg:58.77ms
step:1668/1845 train_time:98056ms step_avg:58.79ms
step:1669/1845 train_time:98142ms step_avg:58.80ms
step:1670/1845 train_time:98231ms step_avg:58.82ms
step:1671/1845 train_time:98318ms step_avg:58.84ms
step:1672/1845 train_time:98405ms step_avg:58.85ms
step:1673/1845 train_time:98492ms step_avg:58.87ms
step:1674/1845 train_time:98582ms step_avg:58.89ms
step:1675/1845 train_time:98668ms step_avg:58.91ms
step:1676/1845 train_time:98758ms step_avg:58.92ms
step:1677/1845 train_time:98844ms step_avg:58.94ms
step:1678/1845 train_time:98933ms step_avg:58.96ms
step:1679/1845 train_time:99021ms step_avg:58.98ms
step:1680/1845 train_time:99109ms step_avg:58.99ms
step:1681/1845 train_time:99194ms step_avg:59.01ms
step:1682/1845 train_time:99284ms step_avg:59.03ms
step:1683/1845 train_time:99370ms step_avg:59.04ms
step:1684/1845 train_time:99459ms step_avg:59.06ms
step:1685/1845 train_time:99544ms step_avg:59.08ms
step:1686/1845 train_time:99634ms step_avg:59.10ms
step:1687/1845 train_time:99722ms step_avg:59.11ms
step:1688/1845 train_time:99809ms step_avg:59.13ms
step:1689/1845 train_time:99896ms step_avg:59.15ms
step:1690/1845 train_time:99985ms step_avg:59.16ms
step:1691/1845 train_time:100071ms step_avg:59.18ms
step:1692/1845 train_time:100161ms step_avg:59.20ms
step:1693/1845 train_time:100246ms step_avg:59.21ms
step:1694/1845 train_time:100336ms step_avg:59.23ms
step:1695/1845 train_time:100422ms step_avg:59.25ms
step:1696/1845 train_time:100510ms step_avg:59.26ms
step:1697/1845 train_time:100597ms step_avg:59.28ms
step:1698/1845 train_time:100685ms step_avg:59.30ms
step:1699/1845 train_time:100771ms step_avg:59.31ms
step:1700/1845 train_time:100860ms step_avg:59.33ms
step:1701/1845 train_time:100946ms step_avg:59.35ms
step:1702/1845 train_time:101036ms step_avg:59.36ms
step:1703/1845 train_time:101122ms step_avg:59.38ms
step:1704/1845 train_time:101211ms step_avg:59.40ms
step:1705/1845 train_time:101297ms step_avg:59.41ms
step:1706/1845 train_time:101385ms step_avg:59.43ms
step:1707/1845 train_time:101472ms step_avg:59.44ms
step:1708/1845 train_time:101562ms step_avg:59.46ms
step:1709/1845 train_time:101648ms step_avg:59.48ms
step:1710/1845 train_time:101737ms step_avg:59.50ms
step:1711/1845 train_time:101824ms step_avg:59.51ms
step:1712/1845 train_time:101913ms step_avg:59.53ms
step:1713/1845 train_time:101999ms step_avg:59.54ms
step:1714/1845 train_time:102087ms step_avg:59.56ms
step:1715/1845 train_time:102174ms step_avg:59.58ms
step:1716/1845 train_time:102262ms step_avg:59.59ms
step:1717/1845 train_time:102349ms step_avg:59.61ms
step:1718/1845 train_time:102437ms step_avg:59.63ms
step:1719/1845 train_time:102523ms step_avg:59.64ms
step:1720/1845 train_time:102612ms step_avg:59.66ms
step:1721/1845 train_time:102698ms step_avg:59.67ms
step:1722/1845 train_time:102787ms step_avg:59.69ms
step:1723/1845 train_time:102873ms step_avg:59.71ms
step:1724/1845 train_time:102963ms step_avg:59.72ms
step:1725/1845 train_time:103048ms step_avg:59.74ms
step:1726/1845 train_time:103138ms step_avg:59.76ms
step:1727/1845 train_time:103224ms step_avg:59.77ms
step:1728/1845 train_time:103313ms step_avg:59.79ms
step:1729/1845 train_time:103400ms step_avg:59.80ms
step:1730/1845 train_time:103488ms step_avg:59.82ms
step:1731/1845 train_time:103574ms step_avg:59.83ms
step:1732/1845 train_time:103663ms step_avg:59.85ms
step:1733/1845 train_time:103749ms step_avg:59.87ms
step:1734/1845 train_time:103838ms step_avg:59.88ms
step:1735/1845 train_time:103924ms step_avg:59.90ms
step:1736/1845 train_time:104012ms step_avg:59.92ms
step:1737/1845 train_time:104100ms step_avg:59.93ms
step:1738/1845 train_time:104189ms step_avg:59.95ms
step:1739/1845 train_time:104275ms step_avg:59.96ms
step:1740/1845 train_time:104363ms step_avg:59.98ms
step:1741/1845 train_time:104450ms step_avg:59.99ms
step:1742/1845 train_time:104540ms step_avg:60.01ms
step:1743/1845 train_time:104625ms step_avg:60.03ms
step:1744/1845 train_time:104715ms step_avg:60.04ms
step:1745/1845 train_time:104801ms step_avg:60.06ms
step:1746/1845 train_time:104890ms step_avg:60.07ms
step:1747/1845 train_time:104975ms step_avg:60.09ms
step:1748/1845 train_time:105064ms step_avg:60.11ms
step:1749/1845 train_time:105150ms step_avg:60.12ms
step:1750/1845 train_time:105239ms step_avg:60.14ms
step:1750/1845 val_loss:3.3051 train_time:105336ms step_avg:60.19ms
step:1751/1845 train_time:105363ms step_avg:60.17ms
step:1752/1845 train_time:105416ms step_avg:60.17ms
step:1753/1845 train_time:105505ms step_avg:60.19ms
step:1754/1845 train_time:105595ms step_avg:60.20ms
step:1755/1845 train_time:105680ms step_avg:60.22ms
step:1756/1845 train_time:105769ms step_avg:60.23ms
step:1757/1845 train_time:105854ms step_avg:60.25ms
step:1758/1845 train_time:105944ms step_avg:60.26ms
step:1759/1845 train_time:106029ms step_avg:60.28ms
step:1760/1845 train_time:106117ms step_avg:60.29ms
step:1761/1845 train_time:106204ms step_avg:60.31ms
step:1762/1845 train_time:106295ms step_avg:60.33ms
step:1763/1845 train_time:106383ms step_avg:60.34ms
step:1764/1845 train_time:106474ms step_avg:60.36ms
step:1765/1845 train_time:106561ms step_avg:60.37ms
step:1766/1845 train_time:106650ms step_avg:60.39ms
step:1767/1845 train_time:106735ms step_avg:60.40ms
step:1768/1845 train_time:106823ms step_avg:60.42ms
step:1769/1845 train_time:106910ms step_avg:60.44ms
step:1770/1845 train_time:106997ms step_avg:60.45ms
step:1771/1845 train_time:107083ms step_avg:60.46ms
step:1772/1845 train_time:107172ms step_avg:60.48ms
step:1773/1845 train_time:107259ms step_avg:60.50ms
step:1774/1845 train_time:107349ms step_avg:60.51ms
step:1775/1845 train_time:107436ms step_avg:60.53ms
step:1776/1845 train_time:107526ms step_avg:60.54ms
step:1777/1845 train_time:107612ms step_avg:60.56ms
step:1778/1845 train_time:107701ms step_avg:60.57ms
step:1779/1845 train_time:107787ms step_avg:60.59ms
step:1780/1845 train_time:107875ms step_avg:60.60ms
step:1781/1845 train_time:107960ms step_avg:60.62ms
step:1782/1845 train_time:108050ms step_avg:60.63ms
step:1783/1845 train_time:108135ms step_avg:60.65ms
step:1784/1845 train_time:108226ms step_avg:60.66ms
step:1785/1845 train_time:108313ms step_avg:60.68ms
step:1786/1845 train_time:108404ms step_avg:60.70ms
step:1787/1845 train_time:108491ms step_avg:60.71ms
step:1788/1845 train_time:108581ms step_avg:60.73ms
step:1789/1845 train_time:108666ms step_avg:60.74ms
step:1790/1845 train_time:108754ms step_avg:60.76ms
step:1791/1845 train_time:108840ms step_avg:60.77ms
step:1792/1845 train_time:108928ms step_avg:60.79ms
step:1793/1845 train_time:109013ms step_avg:60.80ms
step:1794/1845 train_time:109104ms step_avg:60.82ms
step:1795/1845 train_time:109190ms step_avg:60.83ms
step:1796/1845 train_time:109279ms step_avg:60.85ms
step:1797/1845 train_time:109366ms step_avg:60.86ms
step:1798/1845 train_time:109454ms step_avg:60.88ms
step:1799/1845 train_time:109541ms step_avg:60.89ms
step:1800/1845 train_time:109631ms step_avg:60.91ms
step:1801/1845 train_time:109716ms step_avg:60.92ms
step:1802/1845 train_time:109806ms step_avg:60.94ms
step:1803/1845 train_time:109892ms step_avg:60.95ms
step:1804/1845 train_time:109981ms step_avg:60.96ms
step:1805/1845 train_time:110067ms step_avg:60.98ms
step:1806/1845 train_time:110156ms step_avg:60.99ms
step:1807/1845 train_time:110242ms step_avg:61.01ms
step:1808/1845 train_time:110331ms step_avg:61.02ms
step:1809/1845 train_time:110417ms step_avg:61.04ms
step:1810/1845 train_time:110507ms step_avg:61.05ms
step:1811/1845 train_time:110594ms step_avg:61.07ms
step:1812/1845 train_time:110684ms step_avg:61.08ms
step:1813/1845 train_time:110771ms step_avg:61.10ms
step:1814/1845 train_time:110859ms step_avg:61.11ms
step:1815/1845 train_time:110945ms step_avg:61.13ms
step:1816/1845 train_time:111033ms step_avg:61.14ms
step:1817/1845 train_time:111119ms step_avg:61.16ms
step:1818/1845 train_time:111210ms step_avg:61.17ms
step:1819/1845 train_time:111295ms step_avg:61.18ms
step:1820/1845 train_time:111385ms step_avg:61.20ms
step:1821/1845 train_time:111471ms step_avg:61.21ms
step:1822/1845 train_time:111560ms step_avg:61.23ms
step:1823/1845 train_time:111648ms step_avg:61.24ms
step:1824/1845 train_time:111737ms step_avg:61.26ms
step:1825/1845 train_time:111822ms step_avg:61.27ms
step:1826/1845 train_time:111912ms step_avg:61.29ms
step:1827/1845 train_time:111997ms step_avg:61.30ms
step:1828/1845 train_time:112086ms step_avg:61.32ms
step:1829/1845 train_time:112174ms step_avg:61.33ms
step:1830/1845 train_time:112264ms step_avg:61.35ms
step:1831/1845 train_time:112350ms step_avg:61.36ms
step:1832/1845 train_time:112439ms step_avg:61.37ms
step:1833/1845 train_time:112526ms step_avg:61.39ms
step:1834/1845 train_time:112614ms step_avg:61.40ms
step:1835/1845 train_time:112701ms step_avg:61.42ms
step:1836/1845 train_time:112790ms step_avg:61.43ms
step:1837/1845 train_time:112877ms step_avg:61.45ms
step:1838/1845 train_time:112966ms step_avg:61.46ms
step:1839/1845 train_time:113052ms step_avg:61.47ms
step:1840/1845 train_time:113141ms step_avg:61.49ms
step:1841/1845 train_time:113227ms step_avg:61.50ms
step:1842/1845 train_time:113315ms step_avg:61.52ms
step:1843/1845 train_time:113403ms step_avg:61.53ms
step:1844/1845 train_time:113492ms step_avg:61.55ms
step:1845/1845 train_time:113578ms step_avg:61.56ms
step:1845/1845 val_loss:3.2787 train_time:113674ms step_avg:61.61ms
peak memory allocated: 29801 MiB reserved: 45178 MiB

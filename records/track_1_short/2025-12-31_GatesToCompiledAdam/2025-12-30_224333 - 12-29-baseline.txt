# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29-baseline"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return
            
        #Debug: print firing order (only on rank 0)
        # if dist.get_rank() == 0:
        #     if not hasattr(self, '_hook_counter'):
        #         self._hook_counter = 0
        #     if self._hook_counter > -1: # Signal to disable this printout               
        #         self._hook_counter += 1
        #         label = getattr(param, 'label', None)
        #         print(f"{self._hook_counter}: {label} shape={tuple(param.shape)}")


        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        # self._hook_counter = -1 # 0 to reset counter for next step, -1 to disable.
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

if master_process:
    import wandb
    wandb.login()
    wandb.init(
        project=WANDB_PROJECT,
        name=args.run_id,
        config=args
    )
    wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        if master_process:
            # Log to wandb
            wandb.log({
                "val/loss": val_loss.item(),
                "time/training_time_ms": training_time_ms,
                "time/step_avg_ms": training_time_ms / (step + 1),
            }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process:
            final_metrics = {
                "final/val_loss": float(val_loss.item()),
                "final/train_time_ms": int(training_time_ms),
                "final/train_time": training_time_ms / 1000,
                "final/steps_total": int(step),
            }
            wandb.log(final_metrics)
            wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
if master_process:
    wandb.save(f"{args.log_dir}/{args.run_id}.txt")
    wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 06:43:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   39C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   38C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   32C    P0            114W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          152622      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          152623      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          152624      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          152625      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          152626      C   .../envs/speedrun/bin/python3.12       1612MiB |
|    5   N/A  N/A          152627      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          152628      C   .../envs/speedrun/bin/python3.12       1592MiB |
|    7   N/A  N/A          152629      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8302 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:54ms step_avg:54.04ms
step:2/1845 train_time:82ms step_avg:40.79ms
step:3/1845 train_time:106ms step_avg:35.39ms
step:4/1845 train_time:133ms step_avg:33.32ms
step:5/1845 train_time:163ms step_avg:32.64ms
step:6/1845 train_time:277ms step_avg:46.15ms
step:7/1845 train_time:308ms step_avg:44.00ms
step:8/1845 train_time:341ms step_avg:42.65ms
step:9/1845 train_time:372ms step_avg:41.39ms
step:10/1845 train_time:404ms step_avg:40.43ms
step:11/1845 train_time:430ms step_avg:39.10ms
step:12/1845 train_time:458ms step_avg:38.18ms
step:13/1845 train_time:484ms step_avg:37.24ms
step:14/1845 train_time:518ms step_avg:36.99ms
step:15/1845 train_time:552ms step_avg:36.78ms
step:16/1845 train_time:587ms step_avg:36.67ms
step:17/1845 train_time:620ms step_avg:36.45ms
step:18/1845 train_time:654ms step_avg:36.32ms
step:19/1845 train_time:688ms step_avg:36.19ms
step:20/1845 train_time:722ms step_avg:36.08ms
step:21/1845 train_time:756ms step_avg:35.99ms
step:22/1845 train_time:790ms step_avg:35.93ms
step:23/1845 train_time:824ms step_avg:35.82ms
step:24/1845 train_time:858ms step_avg:35.75ms
step:25/1845 train_time:892ms step_avg:35.67ms
step:26/1845 train_time:926ms step_avg:35.61ms
step:27/1845 train_time:960ms step_avg:35.55ms
step:28/1845 train_time:994ms step_avg:35.49ms
step:29/1845 train_time:1028ms step_avg:35.44ms
step:30/1845 train_time:1062ms step_avg:35.40ms
step:31/1845 train_time:1096ms step_avg:35.35ms
step:32/1845 train_time:1130ms step_avg:35.31ms
step:33/1845 train_time:1164ms step_avg:35.28ms
step:34/1845 train_time:1199ms step_avg:35.26ms
step:35/1845 train_time:1233ms step_avg:35.23ms
step:36/1845 train_time:1267ms step_avg:35.21ms
step:37/1845 train_time:1302ms step_avg:35.19ms
step:38/1845 train_time:1337ms step_avg:35.18ms
step:39/1845 train_time:1371ms step_avg:35.15ms
step:40/1845 train_time:1405ms step_avg:35.13ms
step:41/1845 train_time:1439ms step_avg:35.10ms
step:42/1845 train_time:1474ms step_avg:35.09ms
step:43/1845 train_time:1507ms step_avg:35.06ms
step:44/1845 train_time:1542ms step_avg:35.04ms
step:45/1845 train_time:1578ms step_avg:35.06ms
step:46/1845 train_time:1614ms step_avg:35.08ms
step:47/1845 train_time:1650ms step_avg:35.10ms
step:48/1845 train_time:1686ms step_avg:35.13ms
step:49/1845 train_time:1723ms step_avg:35.16ms
step:50/1845 train_time:1758ms step_avg:35.16ms
step:51/1845 train_time:1795ms step_avg:35.20ms
step:52/1845 train_time:1830ms step_avg:35.19ms
step:53/1845 train_time:1866ms step_avg:35.20ms
step:54/1845 train_time:1901ms step_avg:35.21ms
step:55/1845 train_time:1937ms step_avg:35.22ms
step:56/1845 train_time:1972ms step_avg:35.21ms
step:57/1845 train_time:2008ms step_avg:35.22ms
step:58/1845 train_time:2043ms step_avg:35.22ms
step:59/1845 train_time:2078ms step_avg:35.22ms
step:60/1845 train_time:2114ms step_avg:35.23ms
step:61/1845 train_time:2150ms step_avg:35.25ms
step:62/1845 train_time:2185ms step_avg:35.24ms
step:63/1845 train_time:2220ms step_avg:35.24ms
step:64/1845 train_time:2255ms step_avg:35.24ms
step:65/1845 train_time:2290ms step_avg:35.24ms
step:66/1845 train_time:2325ms step_avg:35.23ms
step:67/1845 train_time:2361ms step_avg:35.23ms
step:68/1845 train_time:2395ms step_avg:35.23ms
step:69/1845 train_time:2430ms step_avg:35.22ms
step:70/1845 train_time:2466ms step_avg:35.23ms
step:71/1845 train_time:2501ms step_avg:35.23ms
step:72/1845 train_time:2536ms step_avg:35.22ms
step:73/1845 train_time:2572ms step_avg:35.23ms
step:74/1845 train_time:2607ms step_avg:35.23ms
step:75/1845 train_time:2642ms step_avg:35.22ms
step:76/1845 train_time:2677ms step_avg:35.22ms
step:77/1845 train_time:2712ms step_avg:35.22ms
step:78/1845 train_time:2747ms step_avg:35.22ms
step:79/1845 train_time:2783ms step_avg:35.22ms
step:80/1845 train_time:2818ms step_avg:35.22ms
step:81/1845 train_time:2854ms step_avg:35.23ms
step:82/1845 train_time:2889ms step_avg:35.23ms
step:83/1845 train_time:2924ms step_avg:35.23ms
step:84/1845 train_time:2959ms step_avg:35.23ms
step:85/1845 train_time:2994ms step_avg:35.23ms
step:86/1845 train_time:3029ms step_avg:35.22ms
step:87/1845 train_time:3065ms step_avg:35.23ms
step:88/1845 train_time:3100ms step_avg:35.22ms
step:89/1845 train_time:3135ms step_avg:35.22ms
step:90/1845 train_time:3171ms step_avg:35.23ms
step:91/1845 train_time:3206ms step_avg:35.23ms
step:92/1845 train_time:3242ms step_avg:35.24ms
step:93/1845 train_time:3279ms step_avg:35.26ms
step:94/1845 train_time:3314ms step_avg:35.26ms
step:95/1845 train_time:3351ms step_avg:35.27ms
step:96/1845 train_time:3386ms step_avg:35.27ms
step:97/1845 train_time:3422ms step_avg:35.27ms
step:98/1845 train_time:3458ms step_avg:35.28ms
step:99/1845 train_time:3494ms step_avg:35.29ms
step:100/1845 train_time:3530ms step_avg:35.30ms
step:101/1845 train_time:3567ms step_avg:35.32ms
step:102/1845 train_time:3603ms step_avg:35.32ms
step:103/1845 train_time:3638ms step_avg:35.32ms
step:104/1845 train_time:3674ms step_avg:35.33ms
step:105/1845 train_time:3709ms step_avg:35.32ms
step:106/1845 train_time:3744ms step_avg:35.32ms
step:107/1845 train_time:3779ms step_avg:35.32ms
step:108/1845 train_time:3814ms step_avg:35.31ms
step:109/1845 train_time:3849ms step_avg:35.31ms
step:110/1845 train_time:3884ms step_avg:35.31ms
step:111/1845 train_time:3919ms step_avg:35.30ms
step:112/1845 train_time:3953ms step_avg:35.29ms
step:113/1845 train_time:3988ms step_avg:35.29ms
step:114/1845 train_time:4024ms step_avg:35.30ms
step:115/1845 train_time:4060ms step_avg:35.31ms
step:116/1845 train_time:4096ms step_avg:35.31ms
step:117/1845 train_time:4131ms step_avg:35.31ms
step:118/1845 train_time:4166ms step_avg:35.31ms
step:119/1845 train_time:4202ms step_avg:35.31ms
step:120/1845 train_time:4236ms step_avg:35.30ms
step:121/1845 train_time:4272ms step_avg:35.31ms
step:122/1845 train_time:4307ms step_avg:35.31ms
step:123/1845 train_time:4342ms step_avg:35.30ms
step:124/1845 train_time:4378ms step_avg:35.30ms
step:125/1845 train_time:4413ms step_avg:35.30ms
step:126/1845 train_time:4448ms step_avg:35.30ms
step:127/1845 train_time:4483ms step_avg:35.30ms
step:128/1845 train_time:4519ms step_avg:35.30ms
step:129/1845 train_time:4554ms step_avg:35.30ms
step:130/1845 train_time:4589ms step_avg:35.30ms
step:131/1845 train_time:4624ms step_avg:35.29ms
step:132/1845 train_time:4659ms step_avg:35.29ms
step:133/1845 train_time:4693ms step_avg:35.29ms
step:134/1845 train_time:4729ms step_avg:35.29ms
step:135/1845 train_time:4764ms step_avg:35.29ms
step:136/1845 train_time:4800ms step_avg:35.29ms
step:137/1845 train_time:4835ms step_avg:35.29ms
step:138/1845 train_time:4870ms step_avg:35.29ms
step:139/1845 train_time:4906ms step_avg:35.29ms
step:140/1845 train_time:4940ms step_avg:35.29ms
step:141/1845 train_time:4977ms step_avg:35.30ms
step:142/1845 train_time:5012ms step_avg:35.29ms
step:143/1845 train_time:5047ms step_avg:35.29ms
step:144/1845 train_time:5082ms step_avg:35.29ms
step:145/1845 train_time:5117ms step_avg:35.29ms
step:146/1845 train_time:5152ms step_avg:35.29ms
step:147/1845 train_time:5187ms step_avg:35.28ms
step:148/1845 train_time:5222ms step_avg:35.28ms
step:149/1845 train_time:5257ms step_avg:35.28ms
step:150/1845 train_time:5292ms step_avg:35.28ms
step:151/1845 train_time:5327ms step_avg:35.28ms
step:152/1845 train_time:5362ms step_avg:35.28ms
step:153/1845 train_time:5397ms step_avg:35.28ms
step:154/1845 train_time:5432ms step_avg:35.27ms
step:155/1845 train_time:5467ms step_avg:35.27ms
step:156/1845 train_time:5503ms step_avg:35.27ms
step:157/1845 train_time:5538ms step_avg:35.27ms
step:158/1845 train_time:5573ms step_avg:35.27ms
step:159/1845 train_time:5608ms step_avg:35.27ms
step:160/1845 train_time:5643ms step_avg:35.27ms
step:161/1845 train_time:5679ms step_avg:35.27ms
step:162/1845 train_time:5714ms step_avg:35.27ms
step:163/1845 train_time:5749ms step_avg:35.27ms
step:164/1845 train_time:5784ms step_avg:35.27ms
step:165/1845 train_time:5819ms step_avg:35.27ms
step:166/1845 train_time:5854ms step_avg:35.27ms
step:167/1845 train_time:5890ms step_avg:35.27ms
step:168/1845 train_time:5925ms step_avg:35.27ms
step:169/1845 train_time:5960ms step_avg:35.26ms
step:170/1845 train_time:5995ms step_avg:35.26ms
step:171/1845 train_time:6030ms step_avg:35.26ms
step:172/1845 train_time:6065ms step_avg:35.26ms
step:173/1845 train_time:6101ms step_avg:35.27ms
step:174/1845 train_time:6137ms step_avg:35.27ms
step:175/1845 train_time:6172ms step_avg:35.27ms
step:176/1845 train_time:6207ms step_avg:35.27ms
step:177/1845 train_time:6242ms step_avg:35.27ms
step:178/1845 train_time:6279ms step_avg:35.28ms
step:179/1845 train_time:6315ms step_avg:35.28ms
step:180/1845 train_time:6350ms step_avg:35.28ms
step:181/1845 train_time:6387ms step_avg:35.29ms
step:182/1845 train_time:6422ms step_avg:35.29ms
step:183/1845 train_time:6457ms step_avg:35.29ms
step:184/1845 train_time:6494ms step_avg:35.29ms
step:185/1845 train_time:6530ms step_avg:35.30ms
step:186/1845 train_time:6565ms step_avg:35.30ms
step:187/1845 train_time:6602ms step_avg:35.30ms
step:188/1845 train_time:6638ms step_avg:35.31ms
step:189/1845 train_time:6673ms step_avg:35.31ms
step:190/1845 train_time:6709ms step_avg:35.31ms
step:191/1845 train_time:6745ms step_avg:35.31ms
step:192/1845 train_time:6780ms step_avg:35.31ms
step:193/1845 train_time:6815ms step_avg:35.31ms
step:194/1845 train_time:6850ms step_avg:35.31ms
step:195/1845 train_time:6887ms step_avg:35.32ms
step:196/1845 train_time:6921ms step_avg:35.31ms
step:197/1845 train_time:6957ms step_avg:35.31ms
step:198/1845 train_time:6992ms step_avg:35.31ms
step:199/1845 train_time:7027ms step_avg:35.31ms
step:200/1845 train_time:7062ms step_avg:35.31ms
step:201/1845 train_time:7097ms step_avg:35.31ms
step:202/1845 train_time:7132ms step_avg:35.31ms
step:203/1845 train_time:7167ms step_avg:35.31ms
step:204/1845 train_time:7202ms step_avg:35.31ms
step:205/1845 train_time:7237ms step_avg:35.30ms
step:206/1845 train_time:7271ms step_avg:35.30ms
step:207/1845 train_time:7305ms step_avg:35.29ms
step:208/1845 train_time:7339ms step_avg:35.28ms
step:209/1845 train_time:7372ms step_avg:35.27ms
step:210/1845 train_time:7406ms step_avg:35.27ms
step:211/1845 train_time:7440ms step_avg:35.26ms
step:212/1845 train_time:7475ms step_avg:35.26ms
step:213/1845 train_time:7508ms step_avg:35.25ms
step:214/1845 train_time:7543ms step_avg:35.25ms
step:215/1845 train_time:7576ms step_avg:35.24ms
step:216/1845 train_time:7610ms step_avg:35.23ms
step:217/1845 train_time:7644ms step_avg:35.23ms
step:218/1845 train_time:7678ms step_avg:35.22ms
step:219/1845 train_time:7712ms step_avg:35.21ms
step:220/1845 train_time:7746ms step_avg:35.21ms
step:221/1845 train_time:7780ms step_avg:35.21ms
step:222/1845 train_time:7816ms step_avg:35.21ms
step:223/1845 train_time:7852ms step_avg:35.21ms
step:224/1845 train_time:7888ms step_avg:35.21ms
step:225/1845 train_time:7923ms step_avg:35.21ms
step:226/1845 train_time:7958ms step_avg:35.21ms
step:227/1845 train_time:7994ms step_avg:35.22ms
step:228/1845 train_time:8029ms step_avg:35.22ms
step:229/1845 train_time:8065ms step_avg:35.22ms
step:230/1845 train_time:8100ms step_avg:35.22ms
step:231/1845 train_time:8136ms step_avg:35.22ms
step:232/1845 train_time:8170ms step_avg:35.22ms
step:233/1845 train_time:8205ms step_avg:35.22ms
step:234/1845 train_time:8240ms step_avg:35.21ms
step:235/1845 train_time:8275ms step_avg:35.21ms
step:236/1845 train_time:8310ms step_avg:35.21ms
step:237/1845 train_time:8345ms step_avg:35.21ms
step:238/1845 train_time:8380ms step_avg:35.21ms
step:239/1845 train_time:8416ms step_avg:35.21ms
step:240/1845 train_time:8451ms step_avg:35.21ms
step:241/1845 train_time:8487ms step_avg:35.22ms
step:242/1845 train_time:8522ms step_avg:35.21ms
step:243/1845 train_time:8557ms step_avg:35.21ms
step:244/1845 train_time:8592ms step_avg:35.21ms
step:245/1845 train_time:8627ms step_avg:35.21ms
step:246/1845 train_time:8662ms step_avg:35.21ms
step:247/1845 train_time:8697ms step_avg:35.21ms
step:248/1845 train_time:8732ms step_avg:35.21ms
step:249/1845 train_time:8767ms step_avg:35.21ms
step:250/1845 train_time:8802ms step_avg:35.21ms
step:250/1845 val_loss:4.6112 train_time:8839ms step_avg:35.35ms
step:251/1845 train_time:8866ms step_avg:35.32ms
step:252/1845 train_time:8894ms step_avg:35.29ms
step:253/1845 train_time:8918ms step_avg:35.25ms
step:254/1845 train_time:8945ms step_avg:35.22ms
step:255/1845 train_time:8977ms step_avg:35.20ms
step:256/1845 train_time:9011ms step_avg:35.20ms
step:257/1845 train_time:9045ms step_avg:35.20ms
step:258/1845 train_time:9079ms step_avg:35.19ms
step:259/1845 train_time:9113ms step_avg:35.19ms
step:260/1845 train_time:9147ms step_avg:35.18ms
step:261/1845 train_time:9181ms step_avg:35.18ms
step:262/1845 train_time:9215ms step_avg:35.17ms
step:263/1845 train_time:9249ms step_avg:35.17ms
step:264/1845 train_time:9283ms step_avg:35.16ms
step:265/1845 train_time:9316ms step_avg:35.16ms
step:266/1845 train_time:9350ms step_avg:35.15ms
step:267/1845 train_time:9384ms step_avg:35.15ms
step:268/1845 train_time:9418ms step_avg:35.14ms
step:269/1845 train_time:9452ms step_avg:35.14ms
step:270/1845 train_time:9486ms step_avg:35.13ms
step:271/1845 train_time:9519ms step_avg:35.13ms
step:272/1845 train_time:9554ms step_avg:35.12ms
step:273/1845 train_time:9587ms step_avg:35.12ms
step:274/1845 train_time:9621ms step_avg:35.11ms
step:275/1845 train_time:9655ms step_avg:35.11ms
step:276/1845 train_time:9689ms step_avg:35.11ms
step:277/1845 train_time:9723ms step_avg:35.10ms
step:278/1845 train_time:9757ms step_avg:35.10ms
step:279/1845 train_time:9791ms step_avg:35.09ms
step:280/1845 train_time:9825ms step_avg:35.09ms
step:281/1845 train_time:9859ms step_avg:35.09ms
step:282/1845 train_time:9894ms step_avg:35.08ms
step:283/1845 train_time:9928ms step_avg:35.08ms
step:284/1845 train_time:9962ms step_avg:35.08ms
step:285/1845 train_time:9996ms step_avg:35.07ms
step:286/1845 train_time:10030ms step_avg:35.07ms
step:287/1845 train_time:10064ms step_avg:35.07ms
step:288/1845 train_time:10098ms step_avg:35.06ms
step:289/1845 train_time:10132ms step_avg:35.06ms
step:290/1845 train_time:10166ms step_avg:35.05ms
step:291/1845 train_time:10200ms step_avg:35.05ms
step:292/1845 train_time:10234ms step_avg:35.05ms
step:293/1845 train_time:10268ms step_avg:35.04ms
step:294/1845 train_time:10302ms step_avg:35.04ms
step:295/1845 train_time:10338ms step_avg:35.04ms
step:296/1845 train_time:10373ms step_avg:35.04ms
step:297/1845 train_time:10409ms step_avg:35.05ms
step:298/1845 train_time:10444ms step_avg:35.05ms
step:299/1845 train_time:10479ms step_avg:35.05ms
step:300/1845 train_time:10514ms step_avg:35.05ms
step:301/1845 train_time:10548ms step_avg:35.04ms
step:302/1845 train_time:10583ms step_avg:35.04ms
step:303/1845 train_time:10618ms step_avg:35.04ms
step:304/1845 train_time:10653ms step_avg:35.04ms
step:305/1845 train_time:10689ms step_avg:35.04ms
step:306/1845 train_time:10724ms step_avg:35.05ms
step:307/1845 train_time:10759ms step_avg:35.05ms
step:308/1845 train_time:10795ms step_avg:35.05ms
step:309/1845 train_time:10830ms step_avg:35.05ms
step:310/1845 train_time:10864ms step_avg:35.05ms
step:311/1845 train_time:10901ms step_avg:35.05ms
step:312/1845 train_time:10936ms step_avg:35.05ms
step:313/1845 train_time:10973ms step_avg:35.06ms
step:314/1845 train_time:11008ms step_avg:35.06ms
step:315/1845 train_time:11043ms step_avg:35.06ms
step:316/1845 train_time:11079ms step_avg:35.06ms
step:317/1845 train_time:11115ms step_avg:35.06ms
step:318/1845 train_time:11150ms step_avg:35.06ms
step:319/1845 train_time:11186ms step_avg:35.07ms
step:320/1845 train_time:11222ms step_avg:35.07ms
step:321/1845 train_time:11257ms step_avg:35.07ms
step:322/1845 train_time:11294ms step_avg:35.07ms
step:323/1845 train_time:11330ms step_avg:35.08ms
step:324/1845 train_time:11365ms step_avg:35.08ms
step:325/1845 train_time:11401ms step_avg:35.08ms
step:326/1845 train_time:11436ms step_avg:35.08ms
step:327/1845 train_time:11471ms step_avg:35.08ms
step:328/1845 train_time:11507ms step_avg:35.08ms
step:329/1845 train_time:11543ms step_avg:35.08ms
step:330/1845 train_time:11577ms step_avg:35.08ms
step:331/1845 train_time:11613ms step_avg:35.09ms
step:332/1845 train_time:11648ms step_avg:35.09ms
step:333/1845 train_time:11683ms step_avg:35.08ms
step:334/1845 train_time:11719ms step_avg:35.09ms
step:335/1845 train_time:11754ms step_avg:35.09ms
step:336/1845 train_time:11789ms step_avg:35.09ms
step:337/1845 train_time:11824ms step_avg:35.09ms
step:338/1845 train_time:11859ms step_avg:35.09ms
step:339/1845 train_time:11894ms step_avg:35.09ms
step:340/1845 train_time:11929ms step_avg:35.09ms
step:341/1845 train_time:11964ms step_avg:35.09ms
step:342/1845 train_time:12000ms step_avg:35.09ms
step:343/1845 train_time:12036ms step_avg:35.09ms
step:344/1845 train_time:12070ms step_avg:35.09ms
step:345/1845 train_time:12105ms step_avg:35.09ms
step:346/1845 train_time:12140ms step_avg:35.09ms
step:347/1845 train_time:12175ms step_avg:35.09ms
step:348/1845 train_time:12211ms step_avg:35.09ms
step:349/1845 train_time:12246ms step_avg:35.09ms
step:350/1845 train_time:12280ms step_avg:35.09ms
step:351/1845 train_time:12315ms step_avg:35.09ms
step:352/1845 train_time:12350ms step_avg:35.09ms
step:353/1845 train_time:12385ms step_avg:35.08ms
step:354/1845 train_time:12420ms step_avg:35.08ms
step:355/1845 train_time:12455ms step_avg:35.08ms
step:356/1845 train_time:12490ms step_avg:35.08ms
step:357/1845 train_time:12525ms step_avg:35.08ms
step:358/1845 train_time:12560ms step_avg:35.08ms
step:359/1845 train_time:12595ms step_avg:35.08ms
step:360/1845 train_time:12631ms step_avg:35.08ms
step:361/1845 train_time:12666ms step_avg:35.08ms
step:362/1845 train_time:12701ms step_avg:35.09ms
step:363/1845 train_time:12736ms step_avg:35.09ms
step:364/1845 train_time:12771ms step_avg:35.09ms
step:365/1845 train_time:12807ms step_avg:35.09ms
step:366/1845 train_time:12842ms step_avg:35.09ms
step:367/1845 train_time:12877ms step_avg:35.09ms
step:368/1845 train_time:12913ms step_avg:35.09ms
step:369/1845 train_time:12948ms step_avg:35.09ms
step:370/1845 train_time:12983ms step_avg:35.09ms
step:371/1845 train_time:13018ms step_avg:35.09ms
step:372/1845 train_time:13053ms step_avg:35.09ms
step:373/1845 train_time:13087ms step_avg:35.09ms
step:374/1845 train_time:13122ms step_avg:35.09ms
step:375/1845 train_time:13157ms step_avg:35.09ms
step:376/1845 train_time:13193ms step_avg:35.09ms
step:377/1845 train_time:13228ms step_avg:35.09ms
step:378/1845 train_time:13263ms step_avg:35.09ms
step:379/1845 train_time:13298ms step_avg:35.09ms
step:380/1845 train_time:13333ms step_avg:35.09ms
step:381/1845 train_time:13368ms step_avg:35.09ms
step:382/1845 train_time:13403ms step_avg:35.09ms
step:383/1845 train_time:13438ms step_avg:35.09ms
step:384/1845 train_time:13473ms step_avg:35.09ms
step:385/1845 train_time:13508ms step_avg:35.09ms
step:386/1845 train_time:13543ms step_avg:35.08ms
step:387/1845 train_time:13578ms step_avg:35.09ms
step:388/1845 train_time:13614ms step_avg:35.09ms
step:389/1845 train_time:13649ms step_avg:35.09ms
step:390/1845 train_time:13683ms step_avg:35.09ms
step:391/1845 train_time:13718ms step_avg:35.08ms
step:392/1845 train_time:13752ms step_avg:35.08ms
step:393/1845 train_time:13786ms step_avg:35.08ms
step:394/1845 train_time:13820ms step_avg:35.08ms
step:395/1845 train_time:13854ms step_avg:35.07ms
step:396/1845 train_time:13889ms step_avg:35.07ms
step:397/1845 train_time:13925ms step_avg:35.08ms
step:398/1845 train_time:13961ms step_avg:35.08ms
step:399/1845 train_time:13996ms step_avg:35.08ms
step:400/1845 train_time:14031ms step_avg:35.08ms
step:401/1845 train_time:14066ms step_avg:35.08ms
step:402/1845 train_time:14102ms step_avg:35.08ms
step:403/1845 train_time:14138ms step_avg:35.08ms
step:404/1845 train_time:14173ms step_avg:35.08ms
step:405/1845 train_time:14208ms step_avg:35.08ms
step:406/1845 train_time:14243ms step_avg:35.08ms
step:407/1845 train_time:14279ms step_avg:35.08ms
step:408/1845 train_time:14314ms step_avg:35.08ms
step:409/1845 train_time:14349ms step_avg:35.08ms
step:410/1845 train_time:14384ms step_avg:35.08ms
step:411/1845 train_time:14419ms step_avg:35.08ms
step:412/1845 train_time:14454ms step_avg:35.08ms
step:413/1845 train_time:14490ms step_avg:35.08ms
step:414/1845 train_time:14525ms step_avg:35.08ms
step:415/1845 train_time:14560ms step_avg:35.08ms
step:416/1845 train_time:14594ms step_avg:35.08ms
step:417/1845 train_time:14629ms step_avg:35.08ms
step:418/1845 train_time:14664ms step_avg:35.08ms
step:419/1845 train_time:14699ms step_avg:35.08ms
step:420/1845 train_time:14734ms step_avg:35.08ms
step:421/1845 train_time:14769ms step_avg:35.08ms
step:422/1845 train_time:14804ms step_avg:35.08ms
step:423/1845 train_time:14839ms step_avg:35.08ms
step:424/1845 train_time:14874ms step_avg:35.08ms
step:425/1845 train_time:14909ms step_avg:35.08ms
step:426/1845 train_time:14944ms step_avg:35.08ms
step:427/1845 train_time:14979ms step_avg:35.08ms
step:428/1845 train_time:15014ms step_avg:35.08ms
step:429/1845 train_time:15049ms step_avg:35.08ms
step:430/1845 train_time:15084ms step_avg:35.08ms
step:431/1845 train_time:15119ms step_avg:35.08ms
step:432/1845 train_time:15154ms step_avg:35.08ms
step:433/1845 train_time:15189ms step_avg:35.08ms
step:434/1845 train_time:15224ms step_avg:35.08ms
step:435/1845 train_time:15258ms step_avg:35.08ms
step:436/1845 train_time:15293ms step_avg:35.08ms
step:437/1845 train_time:15328ms step_avg:35.08ms
step:438/1845 train_time:15363ms step_avg:35.08ms
step:439/1845 train_time:15398ms step_avg:35.08ms
step:440/1845 train_time:15433ms step_avg:35.08ms
step:441/1845 train_time:15468ms step_avg:35.08ms
step:442/1845 train_time:15503ms step_avg:35.07ms
step:443/1845 train_time:15538ms step_avg:35.08ms
step:444/1845 train_time:15573ms step_avg:35.07ms
step:445/1845 train_time:15609ms step_avg:35.08ms
step:446/1845 train_time:15644ms step_avg:35.08ms
step:447/1845 train_time:15679ms step_avg:35.08ms
step:448/1845 train_time:15714ms step_avg:35.08ms
step:449/1845 train_time:15749ms step_avg:35.08ms
step:450/1845 train_time:15784ms step_avg:35.08ms
step:451/1845 train_time:15819ms step_avg:35.08ms
step:452/1845 train_time:15854ms step_avg:35.08ms
step:453/1845 train_time:15889ms step_avg:35.08ms
step:454/1845 train_time:15925ms step_avg:35.08ms
step:455/1845 train_time:15960ms step_avg:35.08ms
step:456/1845 train_time:15995ms step_avg:35.08ms
step:457/1845 train_time:16030ms step_avg:35.08ms
step:458/1845 train_time:16065ms step_avg:35.08ms
step:459/1845 train_time:16100ms step_avg:35.08ms
step:460/1845 train_time:16135ms step_avg:35.08ms
step:461/1845 train_time:16170ms step_avg:35.08ms
step:462/1845 train_time:16205ms step_avg:35.07ms
step:463/1845 train_time:16240ms step_avg:35.08ms
step:464/1845 train_time:16275ms step_avg:35.08ms
step:465/1845 train_time:16310ms step_avg:35.08ms
step:466/1845 train_time:16345ms step_avg:35.08ms
step:467/1845 train_time:16380ms step_avg:35.08ms
step:468/1845 train_time:16415ms step_avg:35.08ms
step:469/1845 train_time:16451ms step_avg:35.08ms
step:470/1845 train_time:16485ms step_avg:35.08ms
step:471/1845 train_time:16521ms step_avg:35.08ms
step:472/1845 train_time:16556ms step_avg:35.08ms
step:473/1845 train_time:16591ms step_avg:35.08ms
step:474/1845 train_time:16627ms step_avg:35.08ms
step:475/1845 train_time:16662ms step_avg:35.08ms
step:476/1845 train_time:16697ms step_avg:35.08ms
step:477/1845 train_time:16734ms step_avg:35.08ms
step:478/1845 train_time:16769ms step_avg:35.08ms
step:479/1845 train_time:16804ms step_avg:35.08ms
step:480/1845 train_time:16841ms step_avg:35.09ms
step:481/1845 train_time:16877ms step_avg:35.09ms
step:482/1845 train_time:16912ms step_avg:35.09ms
step:483/1845 train_time:16949ms step_avg:35.09ms
step:484/1845 train_time:16984ms step_avg:35.09ms
step:485/1845 train_time:17019ms step_avg:35.09ms
step:486/1845 train_time:17055ms step_avg:35.09ms
step:487/1845 train_time:17091ms step_avg:35.09ms
step:488/1845 train_time:17126ms step_avg:35.09ms
step:489/1845 train_time:17162ms step_avg:35.10ms
step:490/1845 train_time:17199ms step_avg:35.10ms
step:491/1845 train_time:17232ms step_avg:35.10ms
step:492/1845 train_time:17267ms step_avg:35.10ms
step:493/1845 train_time:17303ms step_avg:35.10ms
step:494/1845 train_time:17338ms step_avg:35.10ms
step:495/1845 train_time:17373ms step_avg:35.10ms
step:496/1845 train_time:17407ms step_avg:35.09ms
step:497/1845 train_time:17442ms step_avg:35.10ms
step:498/1845 train_time:17477ms step_avg:35.09ms
step:499/1845 train_time:17512ms step_avg:35.09ms
step:500/1845 train_time:17546ms step_avg:35.09ms
step:500/1845 val_loss:4.2896 train_time:17583ms step_avg:35.17ms
step:501/1845 train_time:17610ms step_avg:35.15ms
step:502/1845 train_time:17637ms step_avg:35.13ms
step:503/1845 train_time:17662ms step_avg:35.11ms
step:504/1845 train_time:17689ms step_avg:35.10ms
step:505/1845 train_time:17722ms step_avg:35.09ms
step:506/1845 train_time:17757ms step_avg:35.09ms
step:507/1845 train_time:17790ms step_avg:35.09ms
step:508/1845 train_time:17824ms step_avg:35.09ms
step:509/1845 train_time:17858ms step_avg:35.08ms
step:510/1845 train_time:17892ms step_avg:35.08ms
step:511/1845 train_time:17926ms step_avg:35.08ms
step:512/1845 train_time:17960ms step_avg:35.08ms
step:513/1845 train_time:17993ms step_avg:35.07ms
step:514/1845 train_time:18028ms step_avg:35.07ms
step:515/1845 train_time:18061ms step_avg:35.07ms
step:516/1845 train_time:18095ms step_avg:35.07ms
step:517/1845 train_time:18129ms step_avg:35.07ms
step:518/1845 train_time:18165ms step_avg:35.07ms
step:519/1845 train_time:18196ms step_avg:35.06ms
step:520/1845 train_time:18231ms step_avg:35.06ms
step:521/1845 train_time:18264ms step_avg:35.06ms
step:522/1845 train_time:18298ms step_avg:35.05ms
step:523/1845 train_time:18331ms step_avg:35.05ms
step:524/1845 train_time:18365ms step_avg:35.05ms
step:525/1845 train_time:18399ms step_avg:35.05ms
step:526/1845 train_time:18433ms step_avg:35.04ms
step:527/1845 train_time:18467ms step_avg:35.04ms
step:528/1845 train_time:18502ms step_avg:35.04ms
step:529/1845 train_time:18536ms step_avg:35.04ms
step:530/1845 train_time:18572ms step_avg:35.04ms
step:531/1845 train_time:18604ms step_avg:35.04ms
step:532/1845 train_time:18640ms step_avg:35.04ms
step:533/1845 train_time:18673ms step_avg:35.03ms
step:534/1845 train_time:18707ms step_avg:35.03ms
step:535/1845 train_time:18742ms step_avg:35.03ms
step:536/1845 train_time:18778ms step_avg:35.03ms
step:537/1845 train_time:18812ms step_avg:35.03ms
step:538/1845 train_time:18849ms step_avg:35.04ms
step:539/1845 train_time:18885ms step_avg:35.04ms
step:540/1845 train_time:18925ms step_avg:35.05ms
step:541/1845 train_time:18960ms step_avg:35.05ms
step:542/1845 train_time:18998ms step_avg:35.05ms
step:543/1845 train_time:19032ms step_avg:35.05ms
step:544/1845 train_time:19070ms step_avg:35.06ms
step:545/1845 train_time:19105ms step_avg:35.05ms
step:546/1845 train_time:19143ms step_avg:35.06ms
step:547/1845 train_time:19178ms step_avg:35.06ms
step:548/1845 train_time:19216ms step_avg:35.07ms
step:549/1845 train_time:19251ms step_avg:35.07ms
step:550/1845 train_time:19289ms step_avg:35.07ms
step:551/1845 train_time:19324ms step_avg:35.07ms
step:552/1845 train_time:19363ms step_avg:35.08ms
step:553/1845 train_time:19399ms step_avg:35.08ms
step:554/1845 train_time:19439ms step_avg:35.09ms
step:555/1845 train_time:19475ms step_avg:35.09ms
step:556/1845 train_time:19514ms step_avg:35.10ms
step:557/1845 train_time:19550ms step_avg:35.10ms
step:558/1845 train_time:19588ms step_avg:35.10ms
step:559/1845 train_time:19624ms step_avg:35.10ms
step:560/1845 train_time:19662ms step_avg:35.11ms
step:561/1845 train_time:19696ms step_avg:35.11ms
step:562/1845 train_time:19735ms step_avg:35.11ms
step:563/1845 train_time:19770ms step_avg:35.12ms
step:564/1845 train_time:19809ms step_avg:35.12ms
step:565/1845 train_time:19844ms step_avg:35.12ms
step:566/1845 train_time:19883ms step_avg:35.13ms
step:567/1845 train_time:19917ms step_avg:35.13ms
step:568/1845 train_time:19955ms step_avg:35.13ms
step:569/1845 train_time:19990ms step_avg:35.13ms
step:570/1845 train_time:20028ms step_avg:35.14ms
step:571/1845 train_time:20062ms step_avg:35.14ms
step:572/1845 train_time:20100ms step_avg:35.14ms
step:573/1845 train_time:20134ms step_avg:35.14ms
step:574/1845 train_time:20172ms step_avg:35.14ms
step:575/1845 train_time:20206ms step_avg:35.14ms
step:576/1845 train_time:20244ms step_avg:35.14ms
step:577/1845 train_time:20277ms step_avg:35.14ms
step:578/1845 train_time:20315ms step_avg:35.15ms
step:579/1845 train_time:20349ms step_avg:35.15ms
step:580/1845 train_time:20386ms step_avg:35.15ms
step:581/1845 train_time:20421ms step_avg:35.15ms
step:582/1845 train_time:20459ms step_avg:35.15ms
step:583/1845 train_time:20493ms step_avg:35.15ms
step:584/1845 train_time:20530ms step_avg:35.15ms
step:585/1845 train_time:20564ms step_avg:35.15ms
step:586/1845 train_time:20602ms step_avg:35.16ms
step:587/1845 train_time:20636ms step_avg:35.15ms
step:588/1845 train_time:20674ms step_avg:35.16ms
step:589/1845 train_time:20708ms step_avg:35.16ms
step:590/1845 train_time:20745ms step_avg:35.16ms
step:591/1845 train_time:20780ms step_avg:35.16ms
step:592/1845 train_time:20818ms step_avg:35.17ms
step:593/1845 train_time:20853ms step_avg:35.16ms
step:594/1845 train_time:20891ms step_avg:35.17ms
step:595/1845 train_time:20926ms step_avg:35.17ms
step:596/1845 train_time:20964ms step_avg:35.18ms
step:597/1845 train_time:20999ms step_avg:35.17ms
step:598/1845 train_time:21037ms step_avg:35.18ms
step:599/1845 train_time:21072ms step_avg:35.18ms
step:600/1845 train_time:21112ms step_avg:35.19ms
step:601/1845 train_time:21147ms step_avg:35.19ms
step:602/1845 train_time:21185ms step_avg:35.19ms
step:603/1845 train_time:21220ms step_avg:35.19ms
step:604/1845 train_time:21258ms step_avg:35.20ms
step:605/1845 train_time:21309ms step_avg:35.22ms
step:606/1845 train_time:21370ms step_avg:35.26ms
step:607/1845 train_time:21430ms step_avg:35.31ms
step:608/1845 train_time:21491ms step_avg:35.35ms
step:609/1845 train_time:21552ms step_avg:35.39ms
step:610/1845 train_time:21613ms step_avg:35.43ms
step:611/1845 train_time:21676ms step_avg:35.48ms
step:612/1845 train_time:21736ms step_avg:35.52ms
step:613/1845 train_time:21799ms step_avg:35.56ms
step:614/1845 train_time:21860ms step_avg:35.60ms
step:615/1845 train_time:21922ms step_avg:35.65ms
step:616/1845 train_time:21984ms step_avg:35.69ms
step:617/1845 train_time:22046ms step_avg:35.73ms
step:618/1845 train_time:22106ms step_avg:35.77ms
step:619/1845 train_time:22169ms step_avg:35.81ms
step:620/1845 train_time:22229ms step_avg:35.85ms
step:621/1845 train_time:22292ms step_avg:35.90ms
step:622/1845 train_time:22352ms step_avg:35.94ms
step:623/1845 train_time:22413ms step_avg:35.98ms
step:624/1845 train_time:22474ms step_avg:36.02ms
step:625/1845 train_time:22535ms step_avg:36.06ms
step:626/1845 train_time:22595ms step_avg:36.09ms
step:627/1845 train_time:22657ms step_avg:36.13ms
step:628/1845 train_time:22717ms step_avg:36.17ms
step:629/1845 train_time:22780ms step_avg:36.22ms
step:630/1845 train_time:22841ms step_avg:36.25ms
step:631/1845 train_time:22903ms step_avg:36.30ms
step:632/1845 train_time:22965ms step_avg:36.34ms
step:633/1845 train_time:23027ms step_avg:36.38ms
step:634/1845 train_time:23089ms step_avg:36.42ms
step:635/1845 train_time:23151ms step_avg:36.46ms
step:636/1845 train_time:23212ms step_avg:36.50ms
step:637/1845 train_time:23274ms step_avg:36.54ms
step:638/1845 train_time:23334ms step_avg:36.57ms
step:639/1845 train_time:23396ms step_avg:36.61ms
step:640/1845 train_time:23457ms step_avg:36.65ms
step:641/1845 train_time:23519ms step_avg:36.69ms
step:642/1845 train_time:23580ms step_avg:36.73ms
step:643/1845 train_time:23642ms step_avg:36.77ms
step:644/1845 train_time:23702ms step_avg:36.80ms
step:645/1845 train_time:23764ms step_avg:36.84ms
step:646/1845 train_time:23824ms step_avg:36.88ms
step:647/1845 train_time:23887ms step_avg:36.92ms
step:648/1845 train_time:23949ms step_avg:36.96ms
step:649/1845 train_time:24011ms step_avg:37.00ms
step:650/1845 train_time:24073ms step_avg:37.04ms
step:651/1845 train_time:24135ms step_avg:37.07ms
step:652/1845 train_time:24196ms step_avg:37.11ms
step:653/1845 train_time:24257ms step_avg:37.15ms
step:654/1845 train_time:24318ms step_avg:37.18ms
step:655/1845 train_time:24380ms step_avg:37.22ms
step:656/1845 train_time:24440ms step_avg:37.26ms
step:657/1845 train_time:24503ms step_avg:37.30ms
step:658/1845 train_time:24564ms step_avg:37.33ms
step:659/1845 train_time:24625ms step_avg:37.37ms
step:660/1845 train_time:24687ms step_avg:37.40ms
step:661/1845 train_time:24748ms step_avg:37.44ms
step:662/1845 train_time:24809ms step_avg:37.48ms
step:663/1845 train_time:24871ms step_avg:37.51ms
step:664/1845 train_time:24933ms step_avg:37.55ms
step:665/1845 train_time:24995ms step_avg:37.59ms
step:666/1845 train_time:25056ms step_avg:37.62ms
step:667/1845 train_time:25117ms step_avg:37.66ms
step:668/1845 train_time:25179ms step_avg:37.69ms
step:669/1845 train_time:25240ms step_avg:37.73ms
step:670/1845 train_time:25301ms step_avg:37.76ms
step:671/1845 train_time:25363ms step_avg:37.80ms
step:672/1845 train_time:25424ms step_avg:37.83ms
step:673/1845 train_time:25486ms step_avg:37.87ms
step:674/1845 train_time:25547ms step_avg:37.90ms
step:675/1845 train_time:25608ms step_avg:37.94ms
step:676/1845 train_time:25670ms step_avg:37.97ms
step:677/1845 train_time:25731ms step_avg:38.01ms
step:678/1845 train_time:25792ms step_avg:38.04ms
step:679/1845 train_time:25854ms step_avg:38.08ms
step:680/1845 train_time:25915ms step_avg:38.11ms
step:681/1845 train_time:25977ms step_avg:38.15ms
step:682/1845 train_time:26038ms step_avg:38.18ms
step:683/1845 train_time:26100ms step_avg:38.21ms
step:684/1845 train_time:26161ms step_avg:38.25ms
step:685/1845 train_time:26222ms step_avg:38.28ms
step:686/1845 train_time:26284ms step_avg:38.31ms
step:687/1845 train_time:26345ms step_avg:38.35ms
step:688/1845 train_time:26406ms step_avg:38.38ms
step:689/1845 train_time:26468ms step_avg:38.41ms
step:690/1845 train_time:26529ms step_avg:38.45ms
step:691/1845 train_time:26590ms step_avg:38.48ms
step:692/1845 train_time:26651ms step_avg:38.51ms
step:693/1845 train_time:26712ms step_avg:38.55ms
step:694/1845 train_time:26774ms step_avg:38.58ms
step:695/1845 train_time:26835ms step_avg:38.61ms
step:696/1845 train_time:26896ms step_avg:38.64ms
step:697/1845 train_time:26958ms step_avg:38.68ms
step:698/1845 train_time:27019ms step_avg:38.71ms
step:699/1845 train_time:27080ms step_avg:38.74ms
step:700/1845 train_time:27141ms step_avg:38.77ms
step:701/1845 train_time:27203ms step_avg:38.81ms
step:702/1845 train_time:27264ms step_avg:38.84ms
step:703/1845 train_time:27325ms step_avg:38.87ms
step:704/1845 train_time:27386ms step_avg:38.90ms
step:705/1845 train_time:27448ms step_avg:38.93ms
step:706/1845 train_time:27509ms step_avg:38.96ms
step:707/1845 train_time:27570ms step_avg:39.00ms
step:708/1845 train_time:27631ms step_avg:39.03ms
step:709/1845 train_time:27693ms step_avg:39.06ms
step:710/1845 train_time:27754ms step_avg:39.09ms
step:711/1845 train_time:27816ms step_avg:39.12ms
step:712/1845 train_time:27877ms step_avg:39.15ms
step:713/1845 train_time:27939ms step_avg:39.18ms
step:714/1845 train_time:27999ms step_avg:39.21ms
step:715/1845 train_time:28061ms step_avg:39.25ms
step:716/1845 train_time:28122ms step_avg:39.28ms
step:717/1845 train_time:28184ms step_avg:39.31ms
step:718/1845 train_time:28244ms step_avg:39.34ms
step:719/1845 train_time:28307ms step_avg:39.37ms
step:720/1845 train_time:28368ms step_avg:39.40ms
step:721/1845 train_time:28429ms step_avg:39.43ms
step:722/1845 train_time:28491ms step_avg:39.46ms
step:723/1845 train_time:28552ms step_avg:39.49ms
step:724/1845 train_time:28613ms step_avg:39.52ms
step:725/1845 train_time:28675ms step_avg:39.55ms
step:726/1845 train_time:28736ms step_avg:39.58ms
step:727/1845 train_time:28797ms step_avg:39.61ms
step:728/1845 train_time:28859ms step_avg:39.64ms
step:729/1845 train_time:28921ms step_avg:39.67ms
step:730/1845 train_time:28983ms step_avg:39.70ms
step:731/1845 train_time:29044ms step_avg:39.73ms
step:732/1845 train_time:29105ms step_avg:39.76ms
step:733/1845 train_time:29168ms step_avg:39.79ms
step:734/1845 train_time:29228ms step_avg:39.82ms
step:735/1845 train_time:29290ms step_avg:39.85ms
step:736/1845 train_time:29351ms step_avg:39.88ms
step:737/1845 train_time:29412ms step_avg:39.91ms
step:738/1845 train_time:29473ms step_avg:39.94ms
step:739/1845 train_time:29535ms step_avg:39.97ms
step:740/1845 train_time:29595ms step_avg:39.99ms
step:741/1845 train_time:29658ms step_avg:40.02ms
step:742/1845 train_time:29718ms step_avg:40.05ms
step:743/1845 train_time:29780ms step_avg:40.08ms
step:744/1845 train_time:29840ms step_avg:40.11ms
step:745/1845 train_time:29903ms step_avg:40.14ms
step:746/1845 train_time:29964ms step_avg:40.17ms
step:747/1845 train_time:30026ms step_avg:40.19ms
step:748/1845 train_time:30087ms step_avg:40.22ms
step:749/1845 train_time:30149ms step_avg:40.25ms
step:750/1845 train_time:30209ms step_avg:40.28ms
step:750/1845 val_loss:4.0268 train_time:30270ms step_avg:40.36ms
step:751/1845 train_time:30297ms step_avg:40.34ms
step:752/1845 train_time:30332ms step_avg:40.33ms
step:753/1845 train_time:30396ms step_avg:40.37ms
step:754/1845 train_time:30457ms step_avg:40.39ms
step:755/1845 train_time:30521ms step_avg:40.43ms
step:756/1845 train_time:30582ms step_avg:40.45ms
step:757/1845 train_time:30644ms step_avg:40.48ms
step:758/1845 train_time:30705ms step_avg:40.51ms
step:759/1845 train_time:30766ms step_avg:40.53ms
step:760/1845 train_time:30826ms step_avg:40.56ms
step:761/1845 train_time:30888ms step_avg:40.59ms
step:762/1845 train_time:30948ms step_avg:40.61ms
step:763/1845 train_time:31009ms step_avg:40.64ms
step:764/1845 train_time:31070ms step_avg:40.67ms
step:765/1845 train_time:31131ms step_avg:40.69ms
step:766/1845 train_time:31191ms step_avg:40.72ms
step:767/1845 train_time:31255ms step_avg:40.75ms
step:768/1845 train_time:31317ms step_avg:40.78ms
step:769/1845 train_time:31379ms step_avg:40.80ms
step:770/1845 train_time:31440ms step_avg:40.83ms
step:771/1845 train_time:31503ms step_avg:40.86ms
step:772/1845 train_time:31564ms step_avg:40.89ms
step:773/1845 train_time:31626ms step_avg:40.91ms
step:774/1845 train_time:31686ms step_avg:40.94ms
step:775/1845 train_time:31748ms step_avg:40.97ms
step:776/1845 train_time:31809ms step_avg:40.99ms
step:777/1845 train_time:31870ms step_avg:41.02ms
step:778/1845 train_time:31930ms step_avg:41.04ms
step:779/1845 train_time:31992ms step_avg:41.07ms
step:780/1845 train_time:32052ms step_avg:41.09ms
step:781/1845 train_time:32114ms step_avg:41.12ms
step:782/1845 train_time:32174ms step_avg:41.14ms
step:783/1845 train_time:32238ms step_avg:41.17ms
step:784/1845 train_time:32298ms step_avg:41.20ms
step:785/1845 train_time:32360ms step_avg:41.22ms
step:786/1845 train_time:32422ms step_avg:41.25ms
step:787/1845 train_time:32484ms step_avg:41.28ms
step:788/1845 train_time:32545ms step_avg:41.30ms
step:789/1845 train_time:32607ms step_avg:41.33ms
step:790/1845 train_time:32668ms step_avg:41.35ms
step:791/1845 train_time:32729ms step_avg:41.38ms
step:792/1845 train_time:32790ms step_avg:41.40ms
step:793/1845 train_time:32852ms step_avg:41.43ms
step:794/1845 train_time:32912ms step_avg:41.45ms
step:795/1845 train_time:32973ms step_avg:41.48ms
step:796/1845 train_time:33034ms step_avg:41.50ms
step:797/1845 train_time:33095ms step_avg:41.52ms
step:798/1845 train_time:33156ms step_avg:41.55ms
step:799/1845 train_time:33217ms step_avg:41.57ms
step:800/1845 train_time:33278ms step_avg:41.60ms
step:801/1845 train_time:33340ms step_avg:41.62ms
step:802/1845 train_time:33402ms step_avg:41.65ms
step:803/1845 train_time:33464ms step_avg:41.67ms
step:804/1845 train_time:33526ms step_avg:41.70ms
step:805/1845 train_time:33588ms step_avg:41.72ms
step:806/1845 train_time:33648ms step_avg:41.75ms
step:807/1845 train_time:33710ms step_avg:41.77ms
step:808/1845 train_time:33770ms step_avg:41.80ms
step:809/1845 train_time:33832ms step_avg:41.82ms
step:810/1845 train_time:33892ms step_avg:41.84ms
step:811/1845 train_time:33954ms step_avg:41.87ms
step:812/1845 train_time:34014ms step_avg:41.89ms
step:813/1845 train_time:34076ms step_avg:41.91ms
step:814/1845 train_time:34137ms step_avg:41.94ms
step:815/1845 train_time:34198ms step_avg:41.96ms
step:816/1845 train_time:34258ms step_avg:41.98ms
step:817/1845 train_time:34320ms step_avg:42.01ms
step:818/1845 train_time:34381ms step_avg:42.03ms
step:819/1845 train_time:34444ms step_avg:42.06ms
step:820/1845 train_time:34505ms step_avg:42.08ms
step:821/1845 train_time:34569ms step_avg:42.11ms
step:822/1845 train_time:34629ms step_avg:42.13ms
step:823/1845 train_time:34690ms step_avg:42.15ms
step:824/1845 train_time:34748ms step_avg:42.17ms
step:825/1845 train_time:34810ms step_avg:42.19ms
step:826/1845 train_time:34873ms step_avg:42.22ms
step:827/1845 train_time:34934ms step_avg:42.24ms
step:828/1845 train_time:34995ms step_avg:42.26ms
step:829/1845 train_time:35057ms step_avg:42.29ms
step:830/1845 train_time:35118ms step_avg:42.31ms
step:831/1845 train_time:35179ms step_avg:42.33ms
step:832/1845 train_time:35240ms step_avg:42.36ms
step:833/1845 train_time:35302ms step_avg:42.38ms
step:834/1845 train_time:35364ms step_avg:42.40ms
step:835/1845 train_time:35426ms step_avg:42.43ms
step:836/1845 train_time:35487ms step_avg:42.45ms
step:837/1845 train_time:35548ms step_avg:42.47ms
step:838/1845 train_time:35609ms step_avg:42.49ms
step:839/1845 train_time:35671ms step_avg:42.52ms
step:840/1845 train_time:35733ms step_avg:42.54ms
step:841/1845 train_time:35794ms step_avg:42.56ms
step:842/1845 train_time:35855ms step_avg:42.58ms
step:843/1845 train_time:35917ms step_avg:42.61ms
step:844/1845 train_time:35978ms step_avg:42.63ms
step:845/1845 train_time:36040ms step_avg:42.65ms
step:846/1845 train_time:36102ms step_avg:42.67ms
step:847/1845 train_time:36162ms step_avg:42.69ms
step:848/1845 train_time:36224ms step_avg:42.72ms
step:849/1845 train_time:36286ms step_avg:42.74ms
step:850/1845 train_time:36347ms step_avg:42.76ms
step:851/1845 train_time:36408ms step_avg:42.78ms
step:852/1845 train_time:36469ms step_avg:42.80ms
step:853/1845 train_time:36531ms step_avg:42.83ms
step:854/1845 train_time:36592ms step_avg:42.85ms
step:855/1845 train_time:36655ms step_avg:42.87ms
step:856/1845 train_time:36715ms step_avg:42.89ms
step:857/1845 train_time:36778ms step_avg:42.91ms
step:858/1845 train_time:36839ms step_avg:42.94ms
step:859/1845 train_time:36900ms step_avg:42.96ms
step:860/1845 train_time:36961ms step_avg:42.98ms
step:861/1845 train_time:37023ms step_avg:43.00ms
step:862/1845 train_time:37084ms step_avg:43.02ms
step:863/1845 train_time:37145ms step_avg:43.04ms
step:864/1845 train_time:37207ms step_avg:43.06ms
step:865/1845 train_time:37268ms step_avg:43.08ms
step:866/1845 train_time:37328ms step_avg:43.10ms
step:867/1845 train_time:37390ms step_avg:43.13ms
step:868/1845 train_time:37451ms step_avg:43.15ms
step:869/1845 train_time:37514ms step_avg:43.17ms
step:870/1845 train_time:37575ms step_avg:43.19ms
step:871/1845 train_time:37636ms step_avg:43.21ms
step:872/1845 train_time:37697ms step_avg:43.23ms
step:873/1845 train_time:37759ms step_avg:43.25ms
step:874/1845 train_time:37819ms step_avg:43.27ms
step:875/1845 train_time:37882ms step_avg:43.29ms
step:876/1845 train_time:37943ms step_avg:43.31ms
step:877/1845 train_time:38004ms step_avg:43.33ms
step:878/1845 train_time:38065ms step_avg:43.35ms
step:879/1845 train_time:38126ms step_avg:43.37ms
step:880/1845 train_time:38187ms step_avg:43.39ms
step:881/1845 train_time:38248ms step_avg:43.41ms
step:882/1845 train_time:38309ms step_avg:43.43ms
step:883/1845 train_time:38371ms step_avg:43.46ms
step:884/1845 train_time:38432ms step_avg:43.48ms
step:885/1845 train_time:38494ms step_avg:43.50ms
step:886/1845 train_time:38555ms step_avg:43.52ms
step:887/1845 train_time:38617ms step_avg:43.54ms
step:888/1845 train_time:38677ms step_avg:43.56ms
step:889/1845 train_time:38740ms step_avg:43.58ms
step:890/1845 train_time:38801ms step_avg:43.60ms
step:891/1845 train_time:38863ms step_avg:43.62ms
step:892/1845 train_time:38924ms step_avg:43.64ms
step:893/1845 train_time:38986ms step_avg:43.66ms
step:894/1845 train_time:39046ms step_avg:43.68ms
step:895/1845 train_time:39108ms step_avg:43.70ms
step:896/1845 train_time:39168ms step_avg:43.71ms
step:897/1845 train_time:39230ms step_avg:43.73ms
step:898/1845 train_time:39290ms step_avg:43.75ms
step:899/1845 train_time:39352ms step_avg:43.77ms
step:900/1845 train_time:39414ms step_avg:43.79ms
step:901/1845 train_time:39475ms step_avg:43.81ms
step:902/1845 train_time:39535ms step_avg:43.83ms
step:903/1845 train_time:39597ms step_avg:43.85ms
step:904/1845 train_time:39658ms step_avg:43.87ms
step:905/1845 train_time:39720ms step_avg:43.89ms
step:906/1845 train_time:39782ms step_avg:43.91ms
step:907/1845 train_time:39844ms step_avg:43.93ms
step:908/1845 train_time:39905ms step_avg:43.95ms
step:909/1845 train_time:39967ms step_avg:43.97ms
step:910/1845 train_time:40027ms step_avg:43.99ms
step:911/1845 train_time:40089ms step_avg:44.01ms
step:912/1845 train_time:40150ms step_avg:44.02ms
step:913/1845 train_time:40212ms step_avg:44.04ms
step:914/1845 train_time:40272ms step_avg:44.06ms
step:915/1845 train_time:40335ms step_avg:44.08ms
step:916/1845 train_time:40395ms step_avg:44.10ms
step:917/1845 train_time:40457ms step_avg:44.12ms
step:918/1845 train_time:40518ms step_avg:44.14ms
step:919/1845 train_time:40580ms step_avg:44.16ms
step:920/1845 train_time:40640ms step_avg:44.17ms
step:921/1845 train_time:40702ms step_avg:44.19ms
step:922/1845 train_time:40764ms step_avg:44.21ms
step:923/1845 train_time:40825ms step_avg:44.23ms
step:924/1845 train_time:40887ms step_avg:44.25ms
step:925/1845 train_time:40948ms step_avg:44.27ms
step:926/1845 train_time:41008ms step_avg:44.29ms
step:927/1845 train_time:41071ms step_avg:44.30ms
step:928/1845 train_time:41131ms step_avg:44.32ms
step:929/1845 train_time:41194ms step_avg:44.34ms
step:930/1845 train_time:41256ms step_avg:44.36ms
step:931/1845 train_time:41317ms step_avg:44.38ms
step:932/1845 train_time:41378ms step_avg:44.40ms
step:933/1845 train_time:41440ms step_avg:44.42ms
step:934/1845 train_time:41500ms step_avg:44.43ms
step:935/1845 train_time:41561ms step_avg:44.45ms
step:936/1845 train_time:41622ms step_avg:44.47ms
step:937/1845 train_time:41684ms step_avg:44.49ms
step:938/1845 train_time:41745ms step_avg:44.50ms
step:939/1845 train_time:41806ms step_avg:44.52ms
step:940/1845 train_time:41867ms step_avg:44.54ms
step:941/1845 train_time:41929ms step_avg:44.56ms
step:942/1845 train_time:41989ms step_avg:44.57ms
step:943/1845 train_time:42051ms step_avg:44.59ms
step:944/1845 train_time:42112ms step_avg:44.61ms
step:945/1845 train_time:42173ms step_avg:44.63ms
step:946/1845 train_time:42234ms step_avg:44.65ms
step:947/1845 train_time:42296ms step_avg:44.66ms
step:948/1845 train_time:42356ms step_avg:44.68ms
step:949/1845 train_time:42418ms step_avg:44.70ms
step:950/1845 train_time:42479ms step_avg:44.71ms
step:951/1845 train_time:42541ms step_avg:44.73ms
step:952/1845 train_time:42602ms step_avg:44.75ms
step:953/1845 train_time:42664ms step_avg:44.77ms
step:954/1845 train_time:42726ms step_avg:44.79ms
step:955/1845 train_time:42787ms step_avg:44.80ms
step:956/1845 train_time:42848ms step_avg:44.82ms
step:957/1845 train_time:42910ms step_avg:44.84ms
step:958/1845 train_time:42971ms step_avg:44.85ms
step:959/1845 train_time:43032ms step_avg:44.87ms
step:960/1845 train_time:43092ms step_avg:44.89ms
step:961/1845 train_time:43155ms step_avg:44.91ms
step:962/1845 train_time:43216ms step_avg:44.92ms
step:963/1845 train_time:43277ms step_avg:44.94ms
step:964/1845 train_time:43338ms step_avg:44.96ms
step:965/1845 train_time:43399ms step_avg:44.97ms
step:966/1845 train_time:43460ms step_avg:44.99ms
step:967/1845 train_time:43521ms step_avg:45.01ms
step:968/1845 train_time:43583ms step_avg:45.02ms
step:969/1845 train_time:43644ms step_avg:45.04ms
step:970/1845 train_time:43705ms step_avg:45.06ms
step:971/1845 train_time:43767ms step_avg:45.07ms
step:972/1845 train_time:43829ms step_avg:45.09ms
step:973/1845 train_time:43890ms step_avg:45.11ms
step:974/1845 train_time:43951ms step_avg:45.12ms
step:975/1845 train_time:44013ms step_avg:45.14ms
step:976/1845 train_time:44073ms step_avg:45.16ms
step:977/1845 train_time:44135ms step_avg:45.17ms
step:978/1845 train_time:44195ms step_avg:45.19ms
step:979/1845 train_time:44258ms step_avg:45.21ms
step:980/1845 train_time:44319ms step_avg:45.22ms
step:981/1845 train_time:44380ms step_avg:45.24ms
step:982/1845 train_time:44440ms step_avg:45.26ms
step:983/1845 train_time:44503ms step_avg:45.27ms
step:984/1845 train_time:44564ms step_avg:45.29ms
step:985/1845 train_time:44626ms step_avg:45.31ms
step:986/1845 train_time:44687ms step_avg:45.32ms
step:987/1845 train_time:44748ms step_avg:45.34ms
step:988/1845 train_time:44810ms step_avg:45.35ms
step:989/1845 train_time:44872ms step_avg:45.37ms
step:990/1845 train_time:44933ms step_avg:45.39ms
step:991/1845 train_time:44994ms step_avg:45.40ms
step:992/1845 train_time:45055ms step_avg:45.42ms
step:993/1845 train_time:45117ms step_avg:45.43ms
step:994/1845 train_time:45177ms step_avg:45.45ms
step:995/1845 train_time:45239ms step_avg:45.47ms
step:996/1845 train_time:45299ms step_avg:45.48ms
step:997/1845 train_time:45361ms step_avg:45.50ms
step:998/1845 train_time:45423ms step_avg:45.51ms
step:999/1845 train_time:45484ms step_avg:45.53ms
step:1000/1845 train_time:45545ms step_avg:45.55ms
step:1000/1845 val_loss:3.7844 train_time:45606ms step_avg:45.61ms
step:1001/1845 train_time:45633ms step_avg:45.59ms
step:1002/1845 train_time:45668ms step_avg:45.58ms
step:1003/1845 train_time:45732ms step_avg:45.60ms
step:1004/1845 train_time:45794ms step_avg:45.61ms
step:1005/1845 train_time:45856ms step_avg:45.63ms
step:1006/1845 train_time:45918ms step_avg:45.64ms
step:1007/1845 train_time:45979ms step_avg:45.66ms
step:1008/1845 train_time:46039ms step_avg:45.67ms
step:1009/1845 train_time:46101ms step_avg:45.69ms
step:1010/1845 train_time:46161ms step_avg:45.70ms
step:1011/1845 train_time:46221ms step_avg:45.72ms
step:1012/1845 train_time:46282ms step_avg:45.73ms
step:1013/1845 train_time:46343ms step_avg:45.75ms
step:1014/1845 train_time:46404ms step_avg:45.76ms
step:1015/1845 train_time:46466ms step_avg:45.78ms
step:1016/1845 train_time:46526ms step_avg:45.79ms
step:1017/1845 train_time:46590ms step_avg:45.81ms
step:1018/1845 train_time:46651ms step_avg:45.83ms
step:1019/1845 train_time:46716ms step_avg:45.84ms
step:1020/1845 train_time:46777ms step_avg:45.86ms
step:1021/1845 train_time:46839ms step_avg:45.88ms
step:1022/1845 train_time:46900ms step_avg:45.89ms
step:1023/1845 train_time:46961ms step_avg:45.91ms
step:1024/1845 train_time:47021ms step_avg:45.92ms
step:1025/1845 train_time:47083ms step_avg:45.93ms
step:1026/1845 train_time:47144ms step_avg:45.95ms
step:1027/1845 train_time:47205ms step_avg:45.96ms
step:1028/1845 train_time:47266ms step_avg:45.98ms
step:1029/1845 train_time:47327ms step_avg:45.99ms
step:1030/1845 train_time:47388ms step_avg:46.01ms
step:1031/1845 train_time:47449ms step_avg:46.02ms
step:1032/1845 train_time:47510ms step_avg:46.04ms
step:1033/1845 train_time:47573ms step_avg:46.05ms
step:1034/1845 train_time:47634ms step_avg:46.07ms
step:1035/1845 train_time:47697ms step_avg:46.08ms
step:1036/1845 train_time:47758ms step_avg:46.10ms
step:1037/1845 train_time:47820ms step_avg:46.11ms
step:1038/1845 train_time:47881ms step_avg:46.13ms
step:1039/1845 train_time:47943ms step_avg:46.14ms
step:1040/1845 train_time:48003ms step_avg:46.16ms
step:1041/1845 train_time:48065ms step_avg:46.17ms
step:1042/1845 train_time:48125ms step_avg:46.19ms
step:1043/1845 train_time:48186ms step_avg:46.20ms
step:1044/1845 train_time:48247ms step_avg:46.21ms
step:1045/1845 train_time:48308ms step_avg:46.23ms
step:1046/1845 train_time:48369ms step_avg:46.24ms
step:1047/1845 train_time:48430ms step_avg:46.26ms
step:1048/1845 train_time:48492ms step_avg:46.27ms
step:1049/1845 train_time:48554ms step_avg:46.29ms
step:1050/1845 train_time:48615ms step_avg:46.30ms
step:1051/1845 train_time:48678ms step_avg:46.32ms
step:1052/1845 train_time:48740ms step_avg:46.33ms
step:1053/1845 train_time:48801ms step_avg:46.35ms
step:1054/1845 train_time:48862ms step_avg:46.36ms
step:1055/1845 train_time:48924ms step_avg:46.37ms
step:1056/1845 train_time:48986ms step_avg:46.39ms
step:1057/1845 train_time:49047ms step_avg:46.40ms
step:1058/1845 train_time:49108ms step_avg:46.42ms
step:1059/1845 train_time:49170ms step_avg:46.43ms
step:1060/1845 train_time:49231ms step_avg:46.44ms
step:1061/1845 train_time:49293ms step_avg:46.46ms
step:1062/1845 train_time:49354ms step_avg:46.47ms
step:1063/1845 train_time:49415ms step_avg:46.49ms
step:1064/1845 train_time:49476ms step_avg:46.50ms
step:1065/1845 train_time:49537ms step_avg:46.51ms
step:1066/1845 train_time:49598ms step_avg:46.53ms
step:1067/1845 train_time:49661ms step_avg:46.54ms
step:1068/1845 train_time:49721ms step_avg:46.56ms
step:1069/1845 train_time:49784ms step_avg:46.57ms
step:1070/1845 train_time:49844ms step_avg:46.58ms
step:1071/1845 train_time:49906ms step_avg:46.60ms
step:1072/1845 train_time:49967ms step_avg:46.61ms
step:1073/1845 train_time:50029ms step_avg:46.62ms
step:1074/1845 train_time:50090ms step_avg:46.64ms
step:1075/1845 train_time:50152ms step_avg:46.65ms
step:1076/1845 train_time:50213ms step_avg:46.67ms
step:1077/1845 train_time:50275ms step_avg:46.68ms
step:1078/1845 train_time:50336ms step_avg:46.69ms
step:1079/1845 train_time:50398ms step_avg:46.71ms
step:1080/1845 train_time:50458ms step_avg:46.72ms
step:1081/1845 train_time:50520ms step_avg:46.73ms
step:1082/1845 train_time:50581ms step_avg:46.75ms
step:1083/1845 train_time:50643ms step_avg:46.76ms
step:1084/1845 train_time:50703ms step_avg:46.77ms
step:1085/1845 train_time:50765ms step_avg:46.79ms
step:1086/1845 train_time:50826ms step_avg:46.80ms
step:1087/1845 train_time:50888ms step_avg:46.82ms
step:1088/1845 train_time:50949ms step_avg:46.83ms
step:1089/1845 train_time:51011ms step_avg:46.84ms
step:1090/1845 train_time:51072ms step_avg:46.85ms
step:1091/1845 train_time:51133ms step_avg:46.87ms
step:1092/1845 train_time:51195ms step_avg:46.88ms
step:1093/1845 train_time:51257ms step_avg:46.90ms
step:1094/1845 train_time:51318ms step_avg:46.91ms
step:1095/1845 train_time:51379ms step_avg:46.92ms
step:1096/1845 train_time:51437ms step_avg:46.93ms
step:1097/1845 train_time:51499ms step_avg:46.95ms
step:1098/1845 train_time:51562ms step_avg:46.96ms
step:1099/1845 train_time:51622ms step_avg:46.97ms
step:1100/1845 train_time:51685ms step_avg:46.99ms
step:1101/1845 train_time:51747ms step_avg:47.00ms
step:1102/1845 train_time:51807ms step_avg:47.01ms
step:1103/1845 train_time:51869ms step_avg:47.03ms
step:1104/1845 train_time:51929ms step_avg:47.04ms
step:1105/1845 train_time:51991ms step_avg:47.05ms
step:1106/1845 train_time:52052ms step_avg:47.06ms
step:1107/1845 train_time:52113ms step_avg:47.08ms
step:1108/1845 train_time:52174ms step_avg:47.09ms
step:1109/1845 train_time:52237ms step_avg:47.10ms
step:1110/1845 train_time:52298ms step_avg:47.11ms
step:1111/1845 train_time:52359ms step_avg:47.13ms
step:1112/1845 train_time:52419ms step_avg:47.14ms
step:1113/1845 train_time:52481ms step_avg:47.15ms
step:1114/1845 train_time:52542ms step_avg:47.16ms
step:1115/1845 train_time:52603ms step_avg:47.18ms
step:1116/1845 train_time:52665ms step_avg:47.19ms
step:1117/1845 train_time:52726ms step_avg:47.20ms
step:1118/1845 train_time:52786ms step_avg:47.21ms
step:1119/1845 train_time:52849ms step_avg:47.23ms
step:1120/1845 train_time:52910ms step_avg:47.24ms
step:1121/1845 train_time:52971ms step_avg:47.25ms
step:1122/1845 train_time:53032ms step_avg:47.27ms
step:1123/1845 train_time:53094ms step_avg:47.28ms
step:1124/1845 train_time:53155ms step_avg:47.29ms
step:1125/1845 train_time:53217ms step_avg:47.30ms
step:1126/1845 train_time:53279ms step_avg:47.32ms
step:1127/1845 train_time:53340ms step_avg:47.33ms
step:1128/1845 train_time:53401ms step_avg:47.34ms
step:1129/1845 train_time:53462ms step_avg:47.35ms
step:1130/1845 train_time:53523ms step_avg:47.37ms
step:1131/1845 train_time:53585ms step_avg:47.38ms
step:1132/1845 train_time:53646ms step_avg:47.39ms
step:1133/1845 train_time:53707ms step_avg:47.40ms
step:1134/1845 train_time:53768ms step_avg:47.41ms
step:1135/1845 train_time:53830ms step_avg:47.43ms
step:1136/1845 train_time:53891ms step_avg:47.44ms
step:1137/1845 train_time:53953ms step_avg:47.45ms
step:1138/1845 train_time:54013ms step_avg:47.46ms
step:1139/1845 train_time:54076ms step_avg:47.48ms
step:1140/1845 train_time:54136ms step_avg:47.49ms
step:1141/1845 train_time:54198ms step_avg:47.50ms
step:1142/1845 train_time:54259ms step_avg:47.51ms
step:1143/1845 train_time:54321ms step_avg:47.52ms
step:1144/1845 train_time:54382ms step_avg:47.54ms
step:1145/1845 train_time:54444ms step_avg:47.55ms
step:1146/1845 train_time:54504ms step_avg:47.56ms
step:1147/1845 train_time:54566ms step_avg:47.57ms
step:1148/1845 train_time:54627ms step_avg:47.58ms
step:1149/1845 train_time:54689ms step_avg:47.60ms
step:1150/1845 train_time:54750ms step_avg:47.61ms
step:1151/1845 train_time:54811ms step_avg:47.62ms
step:1152/1845 train_time:54872ms step_avg:47.63ms
step:1153/1845 train_time:54934ms step_avg:47.64ms
step:1154/1845 train_time:54995ms step_avg:47.66ms
step:1155/1845 train_time:55057ms step_avg:47.67ms
step:1156/1845 train_time:55118ms step_avg:47.68ms
step:1157/1845 train_time:55179ms step_avg:47.69ms
step:1158/1845 train_time:55240ms step_avg:47.70ms
step:1159/1845 train_time:55302ms step_avg:47.72ms
step:1160/1845 train_time:55363ms step_avg:47.73ms
step:1161/1845 train_time:55424ms step_avg:47.74ms
step:1162/1845 train_time:55485ms step_avg:47.75ms
step:1163/1845 train_time:55547ms step_avg:47.76ms
step:1164/1845 train_time:55608ms step_avg:47.77ms
step:1165/1845 train_time:55669ms step_avg:47.78ms
step:1166/1845 train_time:55730ms step_avg:47.80ms
step:1167/1845 train_time:55792ms step_avg:47.81ms
step:1168/1845 train_time:55853ms step_avg:47.82ms
step:1169/1845 train_time:55914ms step_avg:47.83ms
step:1170/1845 train_time:55974ms step_avg:47.84ms
step:1171/1845 train_time:56037ms step_avg:47.85ms
step:1172/1845 train_time:56098ms step_avg:47.86ms
step:1173/1845 train_time:56160ms step_avg:47.88ms
step:1174/1845 train_time:56221ms step_avg:47.89ms
step:1175/1845 train_time:56282ms step_avg:47.90ms
step:1176/1845 train_time:56343ms step_avg:47.91ms
step:1177/1845 train_time:56405ms step_avg:47.92ms
step:1178/1845 train_time:56466ms step_avg:47.93ms
step:1179/1845 train_time:56529ms step_avg:47.95ms
step:1180/1845 train_time:56590ms step_avg:47.96ms
step:1181/1845 train_time:56651ms step_avg:47.97ms
step:1182/1845 train_time:56712ms step_avg:47.98ms
step:1183/1845 train_time:56774ms step_avg:47.99ms
step:1184/1845 train_time:56835ms step_avg:48.00ms
step:1185/1845 train_time:56897ms step_avg:48.01ms
step:1186/1845 train_time:56957ms step_avg:48.02ms
step:1187/1845 train_time:57020ms step_avg:48.04ms
step:1188/1845 train_time:57080ms step_avg:48.05ms
step:1189/1845 train_time:57141ms step_avg:48.06ms
step:1190/1845 train_time:57202ms step_avg:48.07ms
step:1191/1845 train_time:57263ms step_avg:48.08ms
step:1192/1845 train_time:57324ms step_avg:48.09ms
step:1193/1845 train_time:57386ms step_avg:48.10ms
step:1194/1845 train_time:57447ms step_avg:48.11ms
step:1195/1845 train_time:57508ms step_avg:48.12ms
step:1196/1845 train_time:57569ms step_avg:48.13ms
step:1197/1845 train_time:57631ms step_avg:48.15ms
step:1198/1845 train_time:57691ms step_avg:48.16ms
step:1199/1845 train_time:57753ms step_avg:48.17ms
step:1200/1845 train_time:57814ms step_avg:48.18ms
step:1201/1845 train_time:57876ms step_avg:48.19ms
step:1202/1845 train_time:57937ms step_avg:48.20ms
step:1203/1845 train_time:57999ms step_avg:48.21ms
step:1204/1845 train_time:58060ms step_avg:48.22ms
step:1205/1845 train_time:58122ms step_avg:48.23ms
step:1206/1845 train_time:58210ms step_avg:48.27ms
step:1207/1845 train_time:58298ms step_avg:48.30ms
step:1208/1845 train_time:58386ms step_avg:48.33ms
step:1209/1845 train_time:58474ms step_avg:48.37ms
step:1210/1845 train_time:58561ms step_avg:48.40ms
step:1211/1845 train_time:58648ms step_avg:48.43ms
step:1212/1845 train_time:58734ms step_avg:48.46ms
step:1213/1845 train_time:58823ms step_avg:48.49ms
step:1214/1845 train_time:58911ms step_avg:48.53ms
step:1215/1845 train_time:58997ms step_avg:48.56ms
step:1216/1845 train_time:59085ms step_avg:48.59ms
step:1217/1845 train_time:59174ms step_avg:48.62ms
step:1218/1845 train_time:59262ms step_avg:48.66ms
step:1219/1845 train_time:59351ms step_avg:48.69ms
step:1220/1845 train_time:59438ms step_avg:48.72ms
step:1221/1845 train_time:59526ms step_avg:48.75ms
step:1222/1845 train_time:59613ms step_avg:48.78ms
step:1223/1845 train_time:59702ms step_avg:48.82ms
step:1224/1845 train_time:59789ms step_avg:48.85ms
step:1225/1845 train_time:59877ms step_avg:48.88ms
step:1226/1845 train_time:59964ms step_avg:48.91ms
step:1227/1845 train_time:60053ms step_avg:48.94ms
step:1228/1845 train_time:60141ms step_avg:48.97ms
step:1229/1845 train_time:60229ms step_avg:49.01ms
step:1230/1845 train_time:60316ms step_avg:49.04ms
step:1231/1845 train_time:60404ms step_avg:49.07ms
step:1232/1845 train_time:60492ms step_avg:49.10ms
step:1233/1845 train_time:60580ms step_avg:49.13ms
step:1234/1845 train_time:60667ms step_avg:49.16ms
step:1235/1845 train_time:60756ms step_avg:49.20ms
step:1236/1845 train_time:60844ms step_avg:49.23ms
step:1237/1845 train_time:60931ms step_avg:49.26ms
step:1238/1845 train_time:61019ms step_avg:49.29ms
step:1239/1845 train_time:61106ms step_avg:49.32ms
step:1240/1845 train_time:61193ms step_avg:49.35ms
step:1241/1845 train_time:61281ms step_avg:49.38ms
step:1242/1845 train_time:61369ms step_avg:49.41ms
step:1243/1845 train_time:61459ms step_avg:49.44ms
step:1244/1845 train_time:61546ms step_avg:49.47ms
step:1245/1845 train_time:61634ms step_avg:49.51ms
step:1246/1845 train_time:61721ms step_avg:49.54ms
step:1247/1845 train_time:61810ms step_avg:49.57ms
step:1248/1845 train_time:61897ms step_avg:49.60ms
step:1249/1845 train_time:61985ms step_avg:49.63ms
step:1250/1845 train_time:62072ms step_avg:49.66ms
step:1250/1845 val_loss:3.5338 train_time:62159ms step_avg:49.73ms
step:1251/1845 train_time:62185ms step_avg:49.71ms
step:1252/1845 train_time:62248ms step_avg:49.72ms
step:1253/1845 train_time:62342ms step_avg:49.75ms
step:1254/1845 train_time:62430ms step_avg:49.78ms
step:1255/1845 train_time:62518ms step_avg:49.81ms
step:1256/1845 train_time:62603ms step_avg:49.84ms
step:1257/1845 train_time:62691ms step_avg:49.87ms
step:1258/1845 train_time:62777ms step_avg:49.90ms
step:1259/1845 train_time:62865ms step_avg:49.93ms
step:1260/1845 train_time:62952ms step_avg:49.96ms
step:1261/1845 train_time:63039ms step_avg:49.99ms
step:1262/1845 train_time:63129ms step_avg:50.02ms
step:1263/1845 train_time:63218ms step_avg:50.05ms
step:1264/1845 train_time:63307ms step_avg:50.08ms
step:1265/1845 train_time:63397ms step_avg:50.12ms
step:1266/1845 train_time:63484ms step_avg:50.15ms
step:1267/1845 train_time:63572ms step_avg:50.17ms
step:1268/1845 train_time:63659ms step_avg:50.20ms
step:1269/1845 train_time:63745ms step_avg:50.23ms
step:1270/1845 train_time:63831ms step_avg:50.26ms
step:1271/1845 train_time:63919ms step_avg:50.29ms
step:1272/1845 train_time:64006ms step_avg:50.32ms
step:1273/1845 train_time:64094ms step_avg:50.35ms
step:1274/1845 train_time:64183ms step_avg:50.38ms
step:1275/1845 train_time:64272ms step_avg:50.41ms
step:1276/1845 train_time:64360ms step_avg:50.44ms
step:1277/1845 train_time:64449ms step_avg:50.47ms
step:1278/1845 train_time:64536ms step_avg:50.50ms
step:1279/1845 train_time:64624ms step_avg:50.53ms
step:1280/1845 train_time:64710ms step_avg:50.55ms
step:1281/1845 train_time:64799ms step_avg:50.58ms
step:1282/1845 train_time:64885ms step_avg:50.61ms
step:1283/1845 train_time:64973ms step_avg:50.64ms
step:1284/1845 train_time:65060ms step_avg:50.67ms
step:1285/1845 train_time:65149ms step_avg:50.70ms
step:1286/1845 train_time:65239ms step_avg:50.73ms
step:1287/1845 train_time:65328ms step_avg:50.76ms
step:1288/1845 train_time:65416ms step_avg:50.79ms
step:1289/1845 train_time:65503ms step_avg:50.82ms
step:1290/1845 train_time:65590ms step_avg:50.85ms
step:1291/1845 train_time:65678ms step_avg:50.87ms
step:1292/1845 train_time:65764ms step_avg:50.90ms
step:1293/1845 train_time:65852ms step_avg:50.93ms
step:1294/1845 train_time:65939ms step_avg:50.96ms
step:1295/1845 train_time:66026ms step_avg:50.99ms
step:1296/1845 train_time:66115ms step_avg:51.01ms
step:1297/1845 train_time:66203ms step_avg:51.04ms
step:1298/1845 train_time:66292ms step_avg:51.07ms
step:1299/1845 train_time:66380ms step_avg:51.10ms
step:1300/1845 train_time:66468ms step_avg:51.13ms
step:1301/1845 train_time:66556ms step_avg:51.16ms
step:1302/1845 train_time:66642ms step_avg:51.18ms
step:1303/1845 train_time:66731ms step_avg:51.21ms
step:1304/1845 train_time:66818ms step_avg:51.24ms
step:1305/1845 train_time:66904ms step_avg:51.27ms
step:1306/1845 train_time:66992ms step_avg:51.30ms
step:1307/1845 train_time:67080ms step_avg:51.32ms
step:1308/1845 train_time:67168ms step_avg:51.35ms
step:1309/1845 train_time:67258ms step_avg:51.38ms
step:1310/1845 train_time:67345ms step_avg:51.41ms
step:1311/1845 train_time:67434ms step_avg:51.44ms
step:1312/1845 train_time:67521ms step_avg:51.46ms
step:1313/1845 train_time:67610ms step_avg:51.49ms
step:1314/1845 train_time:67697ms step_avg:51.52ms
step:1315/1845 train_time:67784ms step_avg:51.55ms
step:1316/1845 train_time:67872ms step_avg:51.57ms
step:1317/1845 train_time:67960ms step_avg:51.60ms
step:1318/1845 train_time:68047ms step_avg:51.63ms
step:1319/1845 train_time:68134ms step_avg:51.66ms
step:1320/1845 train_time:68222ms step_avg:51.68ms
step:1321/1845 train_time:68310ms step_avg:51.71ms
step:1322/1845 train_time:68399ms step_avg:51.74ms
step:1323/1845 train_time:68486ms step_avg:51.77ms
step:1324/1845 train_time:68574ms step_avg:51.79ms
step:1325/1845 train_time:68662ms step_avg:51.82ms
step:1326/1845 train_time:68749ms step_avg:51.85ms
step:1327/1845 train_time:68837ms step_avg:51.87ms
step:1328/1845 train_time:68923ms step_avg:51.90ms
step:1329/1845 train_time:69012ms step_avg:51.93ms
step:1330/1845 train_time:69100ms step_avg:51.95ms
step:1331/1845 train_time:69188ms step_avg:51.98ms
step:1332/1845 train_time:69276ms step_avg:52.01ms
step:1333/1845 train_time:69364ms step_avg:52.04ms
step:1334/1845 train_time:69452ms step_avg:52.06ms
step:1335/1845 train_time:69540ms step_avg:52.09ms
step:1336/1845 train_time:69629ms step_avg:52.12ms
step:1337/1845 train_time:69715ms step_avg:52.14ms
step:1338/1845 train_time:69802ms step_avg:52.17ms
step:1339/1845 train_time:69890ms step_avg:52.20ms
step:1340/1845 train_time:69978ms step_avg:52.22ms
step:1341/1845 train_time:70066ms step_avg:52.25ms
step:1342/1845 train_time:70153ms step_avg:52.27ms
step:1343/1845 train_time:70241ms step_avg:52.30ms
step:1344/1845 train_time:70329ms step_avg:52.33ms
step:1345/1845 train_time:70417ms step_avg:52.35ms
step:1346/1845 train_time:70503ms step_avg:52.38ms
step:1347/1845 train_time:70592ms step_avg:52.41ms
step:1348/1845 train_time:70680ms step_avg:52.43ms
step:1349/1845 train_time:70768ms step_avg:52.46ms
step:1350/1845 train_time:70856ms step_avg:52.49ms
step:1351/1845 train_time:70943ms step_avg:52.51ms
step:1352/1845 train_time:71032ms step_avg:52.54ms
step:1353/1845 train_time:71118ms step_avg:52.56ms
step:1354/1845 train_time:71206ms step_avg:52.59ms
step:1355/1845 train_time:71294ms step_avg:52.62ms
step:1356/1845 train_time:71382ms step_avg:52.64ms
step:1357/1845 train_time:71470ms step_avg:52.67ms
step:1358/1845 train_time:71558ms step_avg:52.69ms
step:1359/1845 train_time:71644ms step_avg:52.72ms
step:1360/1845 train_time:71732ms step_avg:52.74ms
step:1361/1845 train_time:71820ms step_avg:52.77ms
step:1362/1845 train_time:71908ms step_avg:52.80ms
step:1363/1845 train_time:71996ms step_avg:52.82ms
step:1364/1845 train_time:72083ms step_avg:52.85ms
step:1365/1845 train_time:72171ms step_avg:52.87ms
step:1366/1845 train_time:72258ms step_avg:52.90ms
step:1367/1845 train_time:72347ms step_avg:52.92ms
step:1368/1845 train_time:72435ms step_avg:52.95ms
step:1369/1845 train_time:72522ms step_avg:52.97ms
step:1370/1845 train_time:72610ms step_avg:53.00ms
step:1371/1845 train_time:72699ms step_avg:53.03ms
step:1372/1845 train_time:72786ms step_avg:53.05ms
step:1373/1845 train_time:72873ms step_avg:53.08ms
step:1374/1845 train_time:72961ms step_avg:53.10ms
step:1375/1845 train_time:73049ms step_avg:53.13ms
step:1376/1845 train_time:73136ms step_avg:53.15ms
step:1377/1845 train_time:73225ms step_avg:53.18ms
step:1378/1845 train_time:73312ms step_avg:53.20ms
step:1379/1845 train_time:73401ms step_avg:53.23ms
step:1380/1845 train_time:73489ms step_avg:53.25ms
step:1381/1845 train_time:73577ms step_avg:53.28ms
step:1382/1845 train_time:73664ms step_avg:53.30ms
step:1383/1845 train_time:73752ms step_avg:53.33ms
step:1384/1845 train_time:73839ms step_avg:53.35ms
step:1385/1845 train_time:73926ms step_avg:53.38ms
step:1386/1845 train_time:74014ms step_avg:53.40ms
step:1387/1845 train_time:74102ms step_avg:53.43ms
step:1388/1845 train_time:74190ms step_avg:53.45ms
step:1389/1845 train_time:74277ms step_avg:53.48ms
step:1390/1845 train_time:74365ms step_avg:53.50ms
step:1391/1845 train_time:74453ms step_avg:53.53ms
step:1392/1845 train_time:74541ms step_avg:53.55ms
step:1393/1845 train_time:74630ms step_avg:53.57ms
step:1394/1845 train_time:74717ms step_avg:53.60ms
step:1395/1845 train_time:74805ms step_avg:53.62ms
step:1396/1845 train_time:74892ms step_avg:53.65ms
step:1397/1845 train_time:74981ms step_avg:53.67ms
step:1398/1845 train_time:75067ms step_avg:53.70ms
step:1399/1845 train_time:75155ms step_avg:53.72ms
step:1400/1845 train_time:75243ms step_avg:53.74ms
step:1401/1845 train_time:75332ms step_avg:53.77ms
step:1402/1845 train_time:75419ms step_avg:53.79ms
step:1403/1845 train_time:75508ms step_avg:53.82ms
step:1404/1845 train_time:75596ms step_avg:53.84ms
step:1405/1845 train_time:75684ms step_avg:53.87ms
step:1406/1845 train_time:75771ms step_avg:53.89ms
step:1407/1845 train_time:75859ms step_avg:53.92ms
step:1408/1845 train_time:75947ms step_avg:53.94ms
step:1409/1845 train_time:76034ms step_avg:53.96ms
step:1410/1845 train_time:76121ms step_avg:53.99ms
step:1411/1845 train_time:76210ms step_avg:54.01ms
step:1412/1845 train_time:76298ms step_avg:54.04ms
step:1413/1845 train_time:76385ms step_avg:54.06ms
step:1414/1845 train_time:76472ms step_avg:54.08ms
step:1415/1845 train_time:76560ms step_avg:54.11ms
step:1416/1845 train_time:76646ms step_avg:54.13ms
step:1417/1845 train_time:76735ms step_avg:54.15ms
step:1418/1845 train_time:76820ms step_avg:54.17ms
step:1419/1845 train_time:76909ms step_avg:54.20ms
step:1420/1845 train_time:76997ms step_avg:54.22ms
step:1421/1845 train_time:77085ms step_avg:54.25ms
step:1422/1845 train_time:77173ms step_avg:54.27ms
step:1423/1845 train_time:77262ms step_avg:54.29ms
step:1424/1845 train_time:77349ms step_avg:54.32ms
step:1425/1845 train_time:77436ms step_avg:54.34ms
step:1426/1845 train_time:77523ms step_avg:54.36ms
step:1427/1845 train_time:77612ms step_avg:54.39ms
step:1428/1845 train_time:77701ms step_avg:54.41ms
step:1429/1845 train_time:77788ms step_avg:54.44ms
step:1430/1845 train_time:77876ms step_avg:54.46ms
step:1431/1845 train_time:77964ms step_avg:54.48ms
step:1432/1845 train_time:78051ms step_avg:54.50ms
step:1433/1845 train_time:78139ms step_avg:54.53ms
step:1434/1845 train_time:78227ms step_avg:54.55ms
step:1435/1845 train_time:78315ms step_avg:54.58ms
step:1436/1845 train_time:78403ms step_avg:54.60ms
step:1437/1845 train_time:78490ms step_avg:54.62ms
step:1438/1845 train_time:78577ms step_avg:54.64ms
step:1439/1845 train_time:78665ms step_avg:54.67ms
step:1440/1845 train_time:78753ms step_avg:54.69ms
step:1441/1845 train_time:78841ms step_avg:54.71ms
step:1442/1845 train_time:78929ms step_avg:54.74ms
step:1443/1845 train_time:79017ms step_avg:54.76ms
step:1444/1845 train_time:79103ms step_avg:54.78ms
step:1445/1845 train_time:79192ms step_avg:54.80ms
step:1446/1845 train_time:79280ms step_avg:54.83ms
step:1447/1845 train_time:79368ms step_avg:54.85ms
step:1448/1845 train_time:79456ms step_avg:54.87ms
step:1449/1845 train_time:79543ms step_avg:54.90ms
step:1450/1845 train_time:79631ms step_avg:54.92ms
step:1451/1845 train_time:79719ms step_avg:54.94ms
step:1452/1845 train_time:79806ms step_avg:54.96ms
step:1453/1845 train_time:79894ms step_avg:54.99ms
step:1454/1845 train_time:79981ms step_avg:55.01ms
step:1455/1845 train_time:80069ms step_avg:55.03ms
step:1456/1845 train_time:80157ms step_avg:55.05ms
step:1457/1845 train_time:80244ms step_avg:55.07ms
step:1458/1845 train_time:80332ms step_avg:55.10ms
step:1459/1845 train_time:80420ms step_avg:55.12ms
step:1460/1845 train_time:80507ms step_avg:55.14ms
step:1461/1845 train_time:80595ms step_avg:55.16ms
step:1462/1845 train_time:80682ms step_avg:55.19ms
step:1463/1845 train_time:80771ms step_avg:55.21ms
step:1464/1845 train_time:80859ms step_avg:55.23ms
step:1465/1845 train_time:80946ms step_avg:55.25ms
step:1466/1845 train_time:81033ms step_avg:55.27ms
step:1467/1845 train_time:81121ms step_avg:55.30ms
step:1468/1845 train_time:81209ms step_avg:55.32ms
step:1469/1845 train_time:81297ms step_avg:55.34ms
step:1470/1845 train_time:81384ms step_avg:55.36ms
step:1471/1845 train_time:81471ms step_avg:55.38ms
step:1472/1845 train_time:81560ms step_avg:55.41ms
step:1473/1845 train_time:81646ms step_avg:55.43ms
step:1474/1845 train_time:81734ms step_avg:55.45ms
step:1475/1845 train_time:81822ms step_avg:55.47ms
step:1476/1845 train_time:81910ms step_avg:55.49ms
step:1477/1845 train_time:81997ms step_avg:55.52ms
step:1478/1845 train_time:82085ms step_avg:55.54ms
step:1479/1845 train_time:82173ms step_avg:55.56ms
step:1480/1845 train_time:82260ms step_avg:55.58ms
step:1481/1845 train_time:82348ms step_avg:55.60ms
step:1482/1845 train_time:82437ms step_avg:55.63ms
step:1483/1845 train_time:82523ms step_avg:55.65ms
step:1484/1845 train_time:82611ms step_avg:55.67ms
step:1485/1845 train_time:82699ms step_avg:55.69ms
step:1486/1845 train_time:82787ms step_avg:55.71ms
step:1487/1845 train_time:82874ms step_avg:55.73ms
step:1488/1845 train_time:82962ms step_avg:55.75ms
step:1489/1845 train_time:83050ms step_avg:55.78ms
step:1490/1845 train_time:83138ms step_avg:55.80ms
step:1491/1845 train_time:83226ms step_avg:55.82ms
step:1492/1845 train_time:83313ms step_avg:55.84ms
step:1493/1845 train_time:83401ms step_avg:55.86ms
step:1494/1845 train_time:83490ms step_avg:55.88ms
step:1495/1845 train_time:83577ms step_avg:55.90ms
step:1496/1845 train_time:83665ms step_avg:55.93ms
step:1497/1845 train_time:83752ms step_avg:55.95ms
step:1498/1845 train_time:83840ms step_avg:55.97ms
step:1499/1845 train_time:83927ms step_avg:55.99ms
step:1500/1845 train_time:84015ms step_avg:56.01ms
step:1500/1845 val_loss:3.4033 train_time:84101ms step_avg:56.07ms
step:1501/1845 train_time:84130ms step_avg:56.05ms
step:1502/1845 train_time:84190ms step_avg:56.05ms
step:1503/1845 train_time:84282ms step_avg:56.08ms
step:1504/1845 train_time:84370ms step_avg:56.10ms
step:1505/1845 train_time:84459ms step_avg:56.12ms
step:1506/1845 train_time:84546ms step_avg:56.14ms
step:1507/1845 train_time:84633ms step_avg:56.16ms
step:1508/1845 train_time:84720ms step_avg:56.18ms
step:1509/1845 train_time:84806ms step_avg:56.20ms
step:1510/1845 train_time:84894ms step_avg:56.22ms
step:1511/1845 train_time:84980ms step_avg:56.24ms
step:1512/1845 train_time:85069ms step_avg:56.26ms
step:1513/1845 train_time:85159ms step_avg:56.29ms
step:1514/1845 train_time:85248ms step_avg:56.31ms
step:1515/1845 train_time:85338ms step_avg:56.33ms
step:1516/1845 train_time:85426ms step_avg:56.35ms
step:1517/1845 train_time:85512ms step_avg:56.37ms
step:1518/1845 train_time:85600ms step_avg:56.39ms
step:1519/1845 train_time:85687ms step_avg:56.41ms
step:1520/1845 train_time:85773ms step_avg:56.43ms
step:1521/1845 train_time:85860ms step_avg:56.45ms
step:1522/1845 train_time:85947ms step_avg:56.47ms
step:1523/1845 train_time:86034ms step_avg:56.49ms
step:1524/1845 train_time:86125ms step_avg:56.51ms
step:1525/1845 train_time:86213ms step_avg:56.53ms
step:1526/1845 train_time:86301ms step_avg:56.55ms
step:1527/1845 train_time:86389ms step_avg:56.57ms
step:1528/1845 train_time:86477ms step_avg:56.59ms
step:1529/1845 train_time:86565ms step_avg:56.62ms
step:1530/1845 train_time:86652ms step_avg:56.64ms
step:1531/1845 train_time:86739ms step_avg:56.66ms
step:1532/1845 train_time:86826ms step_avg:56.68ms
step:1533/1845 train_time:86912ms step_avg:56.69ms
step:1534/1845 train_time:87001ms step_avg:56.72ms
step:1535/1845 train_time:87088ms step_avg:56.74ms
step:1536/1845 train_time:87177ms step_avg:56.76ms
step:1537/1845 train_time:87267ms step_avg:56.78ms
step:1538/1845 train_time:87354ms step_avg:56.80ms
step:1539/1845 train_time:87444ms step_avg:56.82ms
step:1540/1845 train_time:87530ms step_avg:56.84ms
step:1541/1845 train_time:87619ms step_avg:56.86ms
step:1542/1845 train_time:87707ms step_avg:56.88ms
step:1543/1845 train_time:87794ms step_avg:56.90ms
step:1544/1845 train_time:87882ms step_avg:56.92ms
step:1545/1845 train_time:87969ms step_avg:56.94ms
step:1546/1845 train_time:88057ms step_avg:56.96ms
step:1547/1845 train_time:88144ms step_avg:56.98ms
step:1548/1845 train_time:88231ms step_avg:57.00ms
step:1549/1845 train_time:88321ms step_avg:57.02ms
step:1550/1845 train_time:88408ms step_avg:57.04ms
step:1551/1845 train_time:88497ms step_avg:57.06ms
step:1552/1845 train_time:88585ms step_avg:57.08ms
step:1553/1845 train_time:88673ms step_avg:57.10ms
step:1554/1845 train_time:88760ms step_avg:57.12ms
step:1555/1845 train_time:88847ms step_avg:57.14ms
step:1556/1845 train_time:88936ms step_avg:57.16ms
step:1557/1845 train_time:89022ms step_avg:57.18ms
step:1558/1845 train_time:89109ms step_avg:57.19ms
step:1559/1845 train_time:89199ms step_avg:57.22ms
step:1560/1845 train_time:89287ms step_avg:57.23ms
step:1561/1845 train_time:89376ms step_avg:57.26ms
step:1562/1845 train_time:89464ms step_avg:57.28ms
step:1563/1845 train_time:89550ms step_avg:57.29ms
step:1564/1845 train_time:89637ms step_avg:57.31ms
step:1565/1845 train_time:89725ms step_avg:57.33ms
step:1566/1845 train_time:89813ms step_avg:57.35ms
step:1567/1845 train_time:89900ms step_avg:57.37ms
step:1568/1845 train_time:89988ms step_avg:57.39ms
step:1569/1845 train_time:90075ms step_avg:57.41ms
step:1570/1845 train_time:90162ms step_avg:57.43ms
step:1571/1845 train_time:90252ms step_avg:57.45ms
step:1572/1845 train_time:90340ms step_avg:57.47ms
step:1573/1845 train_time:90428ms step_avg:57.49ms
step:1574/1845 train_time:90515ms step_avg:57.51ms
step:1575/1845 train_time:90604ms step_avg:57.53ms
step:1576/1845 train_time:90690ms step_avg:57.54ms
step:1577/1845 train_time:90778ms step_avg:57.56ms
step:1578/1845 train_time:90867ms step_avg:57.58ms
step:1579/1845 train_time:90954ms step_avg:57.60ms
step:1580/1845 train_time:91041ms step_avg:57.62ms
step:1581/1845 train_time:91128ms step_avg:57.64ms
step:1582/1845 train_time:91217ms step_avg:57.66ms
step:1583/1845 train_time:91304ms step_avg:57.68ms
step:1584/1845 train_time:91391ms step_avg:57.70ms
step:1585/1845 train_time:91479ms step_avg:57.72ms
step:1586/1845 train_time:91567ms step_avg:57.73ms
step:1587/1845 train_time:91657ms step_avg:57.75ms
step:1588/1845 train_time:91743ms step_avg:57.77ms
step:1589/1845 train_time:91831ms step_avg:57.79ms
step:1590/1845 train_time:91918ms step_avg:57.81ms
step:1591/1845 train_time:92005ms step_avg:57.83ms
step:1592/1845 train_time:92092ms step_avg:57.85ms
step:1593/1845 train_time:92181ms step_avg:57.87ms
step:1594/1845 train_time:92269ms step_avg:57.88ms
step:1595/1845 train_time:92357ms step_avg:57.90ms
step:1596/1845 train_time:92445ms step_avg:57.92ms
step:1597/1845 train_time:92532ms step_avg:57.94ms
step:1598/1845 train_time:92620ms step_avg:57.96ms
step:1599/1845 train_time:92708ms step_avg:57.98ms
step:1600/1845 train_time:92796ms step_avg:58.00ms
step:1601/1845 train_time:92884ms step_avg:58.02ms
step:1602/1845 train_time:92971ms step_avg:58.03ms
step:1603/1845 train_time:93057ms step_avg:58.05ms
step:1604/1845 train_time:93144ms step_avg:58.07ms
step:1605/1845 train_time:93233ms step_avg:58.09ms
step:1606/1845 train_time:93322ms step_avg:58.11ms
step:1607/1845 train_time:93409ms step_avg:58.13ms
step:1608/1845 train_time:93497ms step_avg:58.15ms
step:1609/1845 train_time:93585ms step_avg:58.16ms
step:1610/1845 train_time:93673ms step_avg:58.18ms
step:1611/1845 train_time:93761ms step_avg:58.20ms
step:1612/1845 train_time:93849ms step_avg:58.22ms
step:1613/1845 train_time:93936ms step_avg:58.24ms
step:1614/1845 train_time:94024ms step_avg:58.26ms
step:1615/1845 train_time:94111ms step_avg:58.27ms
step:1616/1845 train_time:94198ms step_avg:58.29ms
step:1617/1845 train_time:94286ms step_avg:58.31ms
step:1618/1845 train_time:94374ms step_avg:58.33ms
step:1619/1845 train_time:94463ms step_avg:58.35ms
step:1620/1845 train_time:94549ms step_avg:58.36ms
step:1621/1845 train_time:94638ms step_avg:58.38ms
step:1622/1845 train_time:94727ms step_avg:58.40ms
step:1623/1845 train_time:94815ms step_avg:58.42ms
step:1624/1845 train_time:94902ms step_avg:58.44ms
step:1625/1845 train_time:94991ms step_avg:58.46ms
step:1626/1845 train_time:95078ms step_avg:58.47ms
step:1627/1845 train_time:95167ms step_avg:58.49ms
step:1628/1845 train_time:95253ms step_avg:58.51ms
step:1629/1845 train_time:95342ms step_avg:58.53ms
step:1630/1845 train_time:95429ms step_avg:58.55ms
step:1631/1845 train_time:95517ms step_avg:58.56ms
step:1632/1845 train_time:95605ms step_avg:58.58ms
step:1633/1845 train_time:95693ms step_avg:58.60ms
step:1634/1845 train_time:95781ms step_avg:58.62ms
step:1635/1845 train_time:95868ms step_avg:58.63ms
step:1636/1845 train_time:95955ms step_avg:58.65ms
step:1637/1845 train_time:96043ms step_avg:58.67ms
step:1638/1845 train_time:96131ms step_avg:58.69ms
step:1639/1845 train_time:96219ms step_avg:58.71ms
step:1640/1845 train_time:96307ms step_avg:58.72ms
step:1641/1845 train_time:96394ms step_avg:58.74ms
step:1642/1845 train_time:96482ms step_avg:58.76ms
step:1643/1845 train_time:96569ms step_avg:58.78ms
step:1644/1845 train_time:96657ms step_avg:58.79ms
step:1645/1845 train_time:96746ms step_avg:58.81ms
step:1646/1845 train_time:96833ms step_avg:58.83ms
step:1647/1845 train_time:96921ms step_avg:58.85ms
step:1648/1845 train_time:97008ms step_avg:58.86ms
step:1649/1845 train_time:97098ms step_avg:58.88ms
step:1650/1845 train_time:97185ms step_avg:58.90ms
step:1651/1845 train_time:97272ms step_avg:58.92ms
step:1652/1845 train_time:97361ms step_avg:58.93ms
step:1653/1845 train_time:97449ms step_avg:58.95ms
step:1654/1845 train_time:97537ms step_avg:58.97ms
step:1655/1845 train_time:97624ms step_avg:58.99ms
step:1656/1845 train_time:97712ms step_avg:59.00ms
step:1657/1845 train_time:97802ms step_avg:59.02ms
step:1658/1845 train_time:97889ms step_avg:59.04ms
step:1659/1845 train_time:97976ms step_avg:59.06ms
step:1660/1845 train_time:98064ms step_avg:59.07ms
step:1661/1845 train_time:98152ms step_avg:59.09ms
step:1662/1845 train_time:98239ms step_avg:59.11ms
step:1663/1845 train_time:98327ms step_avg:59.13ms
step:1664/1845 train_time:98416ms step_avg:59.14ms
step:1665/1845 train_time:98503ms step_avg:59.16ms
step:1666/1845 train_time:98589ms step_avg:59.18ms
step:1667/1845 train_time:98679ms step_avg:59.20ms
step:1668/1845 train_time:98767ms step_avg:59.21ms
step:1669/1845 train_time:98854ms step_avg:59.23ms
step:1670/1845 train_time:98942ms step_avg:59.25ms
step:1671/1845 train_time:99029ms step_avg:59.26ms
step:1672/1845 train_time:99117ms step_avg:59.28ms
step:1673/1845 train_time:99204ms step_avg:59.30ms
step:1674/1845 train_time:99291ms step_avg:59.31ms
step:1675/1845 train_time:99379ms step_avg:59.33ms
step:1676/1845 train_time:99468ms step_avg:59.35ms
step:1677/1845 train_time:99555ms step_avg:59.36ms
step:1678/1845 train_time:99643ms step_avg:59.38ms
step:1679/1845 train_time:99729ms step_avg:59.40ms
step:1680/1845 train_time:99817ms step_avg:59.42ms
step:1681/1845 train_time:99905ms step_avg:59.43ms
step:1682/1845 train_time:99992ms step_avg:59.45ms
step:1683/1845 train_time:100081ms step_avg:59.47ms
step:1684/1845 train_time:100167ms step_avg:59.48ms
step:1685/1845 train_time:100256ms step_avg:59.50ms
step:1686/1845 train_time:100345ms step_avg:59.52ms
step:1687/1845 train_time:100432ms step_avg:59.53ms
step:1688/1845 train_time:100520ms step_avg:59.55ms
step:1689/1845 train_time:100607ms step_avg:59.57ms
step:1690/1845 train_time:100695ms step_avg:59.58ms
step:1691/1845 train_time:100782ms step_avg:59.60ms
step:1692/1845 train_time:100869ms step_avg:59.62ms
step:1693/1845 train_time:100958ms step_avg:59.63ms
step:1694/1845 train_time:101046ms step_avg:59.65ms
step:1695/1845 train_time:101133ms step_avg:59.67ms
step:1696/1845 train_time:101221ms step_avg:59.68ms
step:1697/1845 train_time:101309ms step_avg:59.70ms
step:1698/1845 train_time:101396ms step_avg:59.72ms
step:1699/1845 train_time:101484ms step_avg:59.73ms
step:1700/1845 train_time:101571ms step_avg:59.75ms
step:1701/1845 train_time:101659ms step_avg:59.76ms
step:1702/1845 train_time:101747ms step_avg:59.78ms
step:1703/1845 train_time:101835ms step_avg:59.80ms
step:1704/1845 train_time:101923ms step_avg:59.81ms
step:1705/1845 train_time:102010ms step_avg:59.83ms
step:1706/1845 train_time:102098ms step_avg:59.85ms
step:1707/1845 train_time:102185ms step_avg:59.86ms
step:1708/1845 train_time:102272ms step_avg:59.88ms
step:1709/1845 train_time:102361ms step_avg:59.90ms
step:1710/1845 train_time:102448ms step_avg:59.91ms
step:1711/1845 train_time:102535ms step_avg:59.93ms
step:1712/1845 train_time:102624ms step_avg:59.94ms
step:1713/1845 train_time:102710ms step_avg:59.96ms
step:1714/1845 train_time:102799ms step_avg:59.98ms
step:1715/1845 train_time:102887ms step_avg:59.99ms
step:1716/1845 train_time:102973ms step_avg:60.01ms
step:1717/1845 train_time:103062ms step_avg:60.02ms
step:1718/1845 train_time:103150ms step_avg:60.04ms
step:1719/1845 train_time:103237ms step_avg:60.06ms
step:1720/1845 train_time:103325ms step_avg:60.07ms
step:1721/1845 train_time:103414ms step_avg:60.09ms
step:1722/1845 train_time:103501ms step_avg:60.11ms
step:1723/1845 train_time:103588ms step_avg:60.12ms
step:1724/1845 train_time:103676ms step_avg:60.14ms
step:1725/1845 train_time:103763ms step_avg:60.15ms
step:1726/1845 train_time:103850ms step_avg:60.17ms
step:1727/1845 train_time:103938ms step_avg:60.18ms
step:1728/1845 train_time:104026ms step_avg:60.20ms
step:1729/1845 train_time:104114ms step_avg:60.22ms
step:1730/1845 train_time:104202ms step_avg:60.23ms
step:1731/1845 train_time:104289ms step_avg:60.25ms
step:1732/1845 train_time:104376ms step_avg:60.26ms
step:1733/1845 train_time:104464ms step_avg:60.28ms
step:1734/1845 train_time:104552ms step_avg:60.30ms
step:1735/1845 train_time:104639ms step_avg:60.31ms
step:1736/1845 train_time:104727ms step_avg:60.33ms
step:1737/1845 train_time:104815ms step_avg:60.34ms
step:1738/1845 train_time:104903ms step_avg:60.36ms
step:1739/1845 train_time:104989ms step_avg:60.37ms
step:1740/1845 train_time:105077ms step_avg:60.39ms
step:1741/1845 train_time:105165ms step_avg:60.41ms
step:1742/1845 train_time:105252ms step_avg:60.42ms
step:1743/1845 train_time:105340ms step_avg:60.44ms
step:1744/1845 train_time:105428ms step_avg:60.45ms
step:1745/1845 train_time:105514ms step_avg:60.47ms
step:1746/1845 train_time:105602ms step_avg:60.48ms
step:1747/1845 train_time:105690ms step_avg:60.50ms
step:1748/1845 train_time:105777ms step_avg:60.51ms
step:1749/1845 train_time:105867ms step_avg:60.53ms
step:1750/1845 train_time:105954ms step_avg:60.55ms
step:1750/1845 val_loss:3.3040 train_time:106041ms step_avg:60.59ms
step:1751/1845 train_time:106070ms step_avg:60.58ms
step:1752/1845 train_time:106132ms step_avg:60.58ms
step:1753/1845 train_time:106225ms step_avg:60.60ms
step:1754/1845 train_time:106312ms step_avg:60.61ms
step:1755/1845 train_time:106399ms step_avg:60.63ms
step:1756/1845 train_time:106486ms step_avg:60.64ms
step:1757/1845 train_time:106573ms step_avg:60.66ms
step:1758/1845 train_time:106660ms step_avg:60.67ms
step:1759/1845 train_time:106748ms step_avg:60.69ms
step:1760/1845 train_time:106835ms step_avg:60.70ms
step:1761/1845 train_time:106922ms step_avg:60.72ms
step:1762/1845 train_time:107011ms step_avg:60.73ms
step:1763/1845 train_time:107104ms step_avg:60.75ms
step:1764/1845 train_time:107193ms step_avg:60.77ms
step:1765/1845 train_time:107281ms step_avg:60.78ms
step:1766/1845 train_time:107370ms step_avg:60.80ms
step:1767/1845 train_time:107456ms step_avg:60.81ms
step:1768/1845 train_time:107543ms step_avg:60.83ms
step:1769/1845 train_time:107630ms step_avg:60.84ms
step:1770/1845 train_time:107716ms step_avg:60.86ms
step:1771/1845 train_time:107805ms step_avg:60.87ms
step:1772/1845 train_time:107892ms step_avg:60.89ms
step:1773/1845 train_time:107979ms step_avg:60.90ms
step:1774/1845 train_time:108068ms step_avg:60.92ms
step:1775/1845 train_time:108157ms step_avg:60.93ms
step:1776/1845 train_time:108245ms step_avg:60.95ms
step:1777/1845 train_time:108334ms step_avg:60.96ms
step:1778/1845 train_time:108422ms step_avg:60.98ms
step:1779/1845 train_time:108508ms step_avg:60.99ms
step:1780/1845 train_time:108595ms step_avg:61.01ms
step:1781/1845 train_time:108683ms step_avg:61.02ms
step:1782/1845 train_time:108771ms step_avg:61.04ms
step:1783/1845 train_time:108857ms step_avg:61.05ms
step:1784/1845 train_time:108944ms step_avg:61.07ms
step:1785/1845 train_time:109034ms step_avg:61.08ms
step:1786/1845 train_time:109122ms step_avg:61.10ms
step:1787/1845 train_time:109212ms step_avg:61.11ms
step:1788/1845 train_time:109299ms step_avg:61.13ms
step:1789/1845 train_time:109387ms step_avg:61.14ms
step:1790/1845 train_time:109474ms step_avg:61.16ms
step:1791/1845 train_time:109562ms step_avg:61.17ms
step:1792/1845 train_time:109650ms step_avg:61.19ms
step:1793/1845 train_time:109737ms step_avg:61.20ms
step:1794/1845 train_time:109824ms step_avg:61.22ms
step:1795/1845 train_time:109912ms step_avg:61.23ms
step:1796/1845 train_time:109998ms step_avg:61.25ms
step:1797/1845 train_time:110087ms step_avg:61.26ms
step:1798/1845 train_time:110176ms step_avg:61.28ms
step:1799/1845 train_time:110263ms step_avg:61.29ms
step:1800/1845 train_time:110352ms step_avg:61.31ms
step:1801/1845 train_time:110439ms step_avg:61.32ms
step:1802/1845 train_time:110526ms step_avg:61.34ms
step:1803/1845 train_time:110615ms step_avg:61.35ms
step:1804/1845 train_time:110703ms step_avg:61.37ms
step:1805/1845 train_time:110790ms step_avg:61.38ms
step:1806/1845 train_time:110877ms step_avg:61.39ms
step:1807/1845 train_time:110965ms step_avg:61.41ms
step:1808/1845 train_time:111054ms step_avg:61.42ms
step:1809/1845 train_time:111142ms step_avg:61.44ms
step:1810/1845 train_time:111230ms step_avg:61.45ms
step:1811/1845 train_time:111318ms step_avg:61.47ms
step:1812/1845 train_time:111405ms step_avg:61.48ms
step:1813/1845 train_time:111493ms step_avg:61.50ms
step:1814/1845 train_time:111581ms step_avg:61.51ms
step:1815/1845 train_time:111669ms step_avg:61.53ms
step:1816/1845 train_time:111756ms step_avg:61.54ms
step:1817/1845 train_time:111844ms step_avg:61.55ms
step:1818/1845 train_time:111932ms step_avg:61.57ms
step:1819/1845 train_time:112021ms step_avg:61.58ms
step:1820/1845 train_time:112109ms step_avg:61.60ms
step:1821/1845 train_time:112197ms step_avg:61.61ms
step:1822/1845 train_time:112286ms step_avg:61.63ms
step:1823/1845 train_time:112373ms step_avg:61.64ms
step:1824/1845 train_time:112461ms step_avg:61.66ms
step:1825/1845 train_time:112549ms step_avg:61.67ms
step:1826/1845 train_time:112636ms step_avg:61.68ms
step:1827/1845 train_time:112724ms step_avg:61.70ms
step:1828/1845 train_time:112813ms step_avg:61.71ms
step:1829/1845 train_time:112901ms step_avg:61.73ms
step:1830/1845 train_time:112988ms step_avg:61.74ms
step:1831/1845 train_time:113076ms step_avg:61.76ms
step:1832/1845 train_time:113163ms step_avg:61.77ms
step:1833/1845 train_time:113252ms step_avg:61.78ms
step:1834/1845 train_time:113339ms step_avg:61.80ms
step:1835/1845 train_time:113427ms step_avg:61.81ms
step:1836/1845 train_time:113514ms step_avg:61.83ms
step:1837/1845 train_time:113602ms step_avg:61.84ms
step:1838/1845 train_time:113690ms step_avg:61.86ms
step:1839/1845 train_time:113778ms step_avg:61.87ms
step:1840/1845 train_time:113866ms step_avg:61.88ms
step:1841/1845 train_time:113954ms step_avg:61.90ms
step:1842/1845 train_time:114043ms step_avg:61.91ms
step:1843/1845 train_time:114130ms step_avg:61.93ms
step:1844/1845 train_time:114217ms step_avg:61.94ms
step:1845/1845 train_time:114306ms step_avg:61.95ms
step:1845/1845 val_loss:3.2771 train_time:114391ms step_avg:62.00ms
peak memory allocated: 29709 MiB reserved: 44498 MiB

# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29 - gates-banked-compiled-adam - bf16 gates - fp32 exp_avg"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            #exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device) # Testing making the optimizer state fp32.
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay (lr as weight decay schedule)
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0))
                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()  # Testing leaving these as fp32
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

if master_process:
    import wandb
    wandb.login()
    wandb.init(
        project=WANDB_PROJECT,
        name=args.run_id,
        config=args
    )
    wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        if master_process:
            # Log to wandb
            wandb.log({
                "val/loss": val_loss.item(),
                "time/training_time_ms": training_time_ms,
                "time/step_avg_ms": training_time_ms / (step + 1),
            }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process:
            final_metrics = {
                "final/val_loss": float(val_loss.item()),
                "final/train_time_ms": int(training_time_ms),
                "final/train_time": training_time_ms / 1000,
                "final/steps_total": int(step),
            }
            wandb.log(final_metrics)
            wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
if master_process:
    wandb.save(f"{args.log_dir}/{args.run_id}.txt")
    wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 07:42:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   39C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   33C    P0            113W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          368306      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          368307      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          368308      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          368309      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          368310      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    5   N/A  N/A          368311      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          368312      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    7   N/A  N/A          368313      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8304 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:59ms step_avg:59.05ms
step:2/1845 train_time:86ms step_avg:43.17ms
step:3/1845 train_time:110ms step_avg:36.65ms
step:4/1845 train_time:136ms step_avg:34.04ms
step:5/1845 train_time:160ms step_avg:32.05ms
step:6/1845 train_time:280ms step_avg:46.65ms
step:7/1845 train_time:311ms step_avg:44.45ms
step:8/1845 train_time:347ms step_avg:43.32ms
step:9/1845 train_time:378ms step_avg:42.05ms
step:10/1845 train_time:414ms step_avg:41.39ms
step:11/1845 train_time:447ms step_avg:40.65ms
step:12/1845 train_time:484ms step_avg:40.36ms
step:13/1845 train_time:518ms step_avg:39.84ms
step:14/1845 train_time:555ms step_avg:39.66ms
step:15/1845 train_time:589ms step_avg:39.24ms
step:16/1845 train_time:629ms step_avg:39.32ms
step:17/1845 train_time:665ms step_avg:39.13ms
step:18/1845 train_time:704ms step_avg:39.10ms
step:19/1845 train_time:736ms step_avg:38.76ms
step:20/1845 train_time:774ms step_avg:38.70ms
step:21/1845 train_time:807ms step_avg:38.44ms
step:22/1845 train_time:843ms step_avg:38.30ms
step:23/1845 train_time:874ms step_avg:38.02ms
step:24/1845 train_time:910ms step_avg:37.90ms
step:25/1845 train_time:941ms step_avg:37.64ms
step:26/1845 train_time:976ms step_avg:37.54ms
step:27/1845 train_time:1007ms step_avg:37.31ms
step:28/1845 train_time:1042ms step_avg:37.23ms
step:29/1845 train_time:1074ms step_avg:37.04ms
step:30/1845 train_time:1109ms step_avg:36.96ms
step:31/1845 train_time:1140ms step_avg:36.79ms
step:32/1845 train_time:1175ms step_avg:36.70ms
step:33/1845 train_time:1205ms step_avg:36.52ms
step:34/1845 train_time:1239ms step_avg:36.44ms
step:35/1845 train_time:1270ms step_avg:36.28ms
step:36/1845 train_time:1311ms step_avg:36.41ms
step:37/1845 train_time:1344ms step_avg:36.31ms
step:38/1845 train_time:1381ms step_avg:36.34ms
step:39/1845 train_time:1414ms step_avg:36.25ms
step:40/1845 train_time:1449ms step_avg:36.23ms
step:41/1845 train_time:1482ms step_avg:36.14ms
step:42/1845 train_time:1518ms step_avg:36.14ms
step:43/1845 train_time:1549ms step_avg:36.03ms
step:44/1845 train_time:1584ms step_avg:36.00ms
step:45/1845 train_time:1615ms step_avg:35.88ms
step:46/1845 train_time:1648ms step_avg:35.84ms
step:47/1845 train_time:1679ms step_avg:35.72ms
step:48/1845 train_time:1714ms step_avg:35.71ms
step:49/1845 train_time:1745ms step_avg:35.62ms
step:50/1845 train_time:1780ms step_avg:35.60ms
step:51/1845 train_time:1812ms step_avg:35.52ms
step:52/1845 train_time:1846ms step_avg:35.50ms
step:53/1845 train_time:1877ms step_avg:35.42ms
step:54/1845 train_time:1912ms step_avg:35.40ms
step:55/1845 train_time:1942ms step_avg:35.32ms
step:56/1845 train_time:1977ms step_avg:35.30ms
step:57/1845 train_time:2008ms step_avg:35.23ms
step:58/1845 train_time:2042ms step_avg:35.21ms
step:59/1845 train_time:2073ms step_avg:35.14ms
step:60/1845 train_time:2109ms step_avg:35.14ms
step:61/1845 train_time:2139ms step_avg:35.07ms
step:62/1845 train_time:2173ms step_avg:35.04ms
step:63/1845 train_time:2202ms step_avg:34.96ms
step:64/1845 train_time:2241ms step_avg:35.01ms
step:65/1845 train_time:2271ms step_avg:34.93ms
step:66/1845 train_time:2309ms step_avg:34.99ms
step:67/1845 train_time:2339ms step_avg:34.91ms
step:68/1845 train_time:2377ms step_avg:34.96ms
step:69/1845 train_time:2410ms step_avg:34.93ms
step:70/1845 train_time:2448ms step_avg:34.98ms
step:71/1845 train_time:2479ms step_avg:34.91ms
step:72/1845 train_time:2514ms step_avg:34.92ms
step:73/1845 train_time:2544ms step_avg:34.85ms
step:74/1845 train_time:2583ms step_avg:34.90ms
step:75/1845 train_time:2613ms step_avg:34.84ms
step:76/1845 train_time:2650ms step_avg:34.87ms
step:77/1845 train_time:2680ms step_avg:34.81ms
step:78/1845 train_time:2718ms step_avg:34.85ms
step:79/1845 train_time:2748ms step_avg:34.78ms
step:80/1845 train_time:2787ms step_avg:34.84ms
step:81/1845 train_time:2817ms step_avg:34.78ms
step:82/1845 train_time:2855ms step_avg:34.82ms
step:83/1845 train_time:2884ms step_avg:34.75ms
step:84/1845 train_time:2921ms step_avg:34.77ms
step:85/1845 train_time:2952ms step_avg:34.73ms
step:86/1845 train_time:2990ms step_avg:34.77ms
step:87/1845 train_time:3020ms step_avg:34.72ms
step:88/1845 train_time:3060ms step_avg:34.77ms
step:89/1845 train_time:3092ms step_avg:34.74ms
step:90/1845 train_time:3128ms step_avg:34.75ms
step:91/1845 train_time:3160ms step_avg:34.72ms
step:92/1845 train_time:3196ms step_avg:34.74ms
step:93/1845 train_time:3228ms step_avg:34.71ms
step:94/1845 train_time:3264ms step_avg:34.73ms
step:95/1845 train_time:3296ms step_avg:34.70ms
step:96/1845 train_time:3332ms step_avg:34.70ms
step:97/1845 train_time:3364ms step_avg:34.68ms
step:98/1845 train_time:3400ms step_avg:34.69ms
step:99/1845 train_time:3432ms step_avg:34.66ms
step:100/1845 train_time:3468ms step_avg:34.68ms
step:101/1845 train_time:3500ms step_avg:34.65ms
step:102/1845 train_time:3536ms step_avg:34.67ms
step:103/1845 train_time:3569ms step_avg:34.65ms
step:104/1845 train_time:3604ms step_avg:34.65ms
step:105/1845 train_time:3636ms step_avg:34.63ms
step:106/1845 train_time:3672ms step_avg:34.64ms
step:107/1845 train_time:3704ms step_avg:34.61ms
step:108/1845 train_time:3740ms step_avg:34.63ms
step:109/1845 train_time:3771ms step_avg:34.60ms
step:110/1845 train_time:3809ms step_avg:34.62ms
step:111/1845 train_time:3841ms step_avg:34.60ms
step:112/1845 train_time:3877ms step_avg:34.62ms
step:113/1845 train_time:3909ms step_avg:34.59ms
step:114/1845 train_time:3945ms step_avg:34.60ms
step:115/1845 train_time:3977ms step_avg:34.58ms
step:116/1845 train_time:4013ms step_avg:34.59ms
step:117/1845 train_time:4045ms step_avg:34.57ms
step:118/1845 train_time:4081ms step_avg:34.59ms
step:119/1845 train_time:4113ms step_avg:34.56ms
step:120/1845 train_time:4149ms step_avg:34.57ms
step:121/1845 train_time:4181ms step_avg:34.55ms
step:122/1845 train_time:4217ms step_avg:34.57ms
step:123/1845 train_time:4250ms step_avg:34.55ms
step:124/1845 train_time:4286ms step_avg:34.56ms
step:125/1845 train_time:4319ms step_avg:34.55ms
step:126/1845 train_time:4354ms step_avg:34.55ms
step:127/1845 train_time:4386ms step_avg:34.53ms
step:128/1845 train_time:4422ms step_avg:34.55ms
step:129/1845 train_time:4454ms step_avg:34.52ms
step:130/1845 train_time:4491ms step_avg:34.54ms
step:131/1845 train_time:4522ms step_avg:34.52ms
step:132/1845 train_time:4558ms step_avg:34.53ms
step:133/1845 train_time:4590ms step_avg:34.51ms
step:134/1845 train_time:4626ms step_avg:34.52ms
step:135/1845 train_time:4658ms step_avg:34.51ms
step:136/1845 train_time:4695ms step_avg:34.52ms
step:137/1845 train_time:4727ms step_avg:34.50ms
step:138/1845 train_time:4763ms step_avg:34.52ms
step:139/1845 train_time:4795ms step_avg:34.50ms
step:140/1845 train_time:4831ms step_avg:34.51ms
step:141/1845 train_time:4863ms step_avg:34.49ms
step:142/1845 train_time:4899ms step_avg:34.50ms
step:143/1845 train_time:4931ms step_avg:34.48ms
step:144/1845 train_time:4967ms step_avg:34.50ms
step:145/1845 train_time:4999ms step_avg:34.48ms
step:146/1845 train_time:5035ms step_avg:34.49ms
step:147/1845 train_time:5067ms step_avg:34.47ms
step:148/1845 train_time:5104ms step_avg:34.48ms
step:149/1845 train_time:5136ms step_avg:34.47ms
step:150/1845 train_time:5172ms step_avg:34.48ms
step:151/1845 train_time:5204ms step_avg:34.46ms
step:152/1845 train_time:5239ms step_avg:34.47ms
step:153/1845 train_time:5270ms step_avg:34.45ms
step:154/1845 train_time:5307ms step_avg:34.46ms
step:155/1845 train_time:5339ms step_avg:34.44ms
step:156/1845 train_time:5376ms step_avg:34.46ms
step:157/1845 train_time:5408ms step_avg:34.45ms
step:158/1845 train_time:5444ms step_avg:34.45ms
step:159/1845 train_time:5475ms step_avg:34.44ms
step:160/1845 train_time:5512ms step_avg:34.45ms
step:161/1845 train_time:5544ms step_avg:34.43ms
step:162/1845 train_time:5580ms step_avg:34.45ms
step:163/1845 train_time:5612ms step_avg:34.43ms
step:164/1845 train_time:5647ms step_avg:34.44ms
step:165/1845 train_time:5680ms step_avg:34.42ms
step:166/1845 train_time:5715ms step_avg:34.43ms
step:167/1845 train_time:5748ms step_avg:34.42ms
step:168/1845 train_time:5784ms step_avg:34.43ms
step:169/1845 train_time:5816ms step_avg:34.41ms
step:170/1845 train_time:5851ms step_avg:34.42ms
step:171/1845 train_time:5884ms step_avg:34.41ms
step:172/1845 train_time:5919ms step_avg:34.41ms
step:173/1845 train_time:5951ms step_avg:34.40ms
step:174/1845 train_time:5988ms step_avg:34.41ms
step:175/1845 train_time:6021ms step_avg:34.41ms
step:176/1845 train_time:6056ms step_avg:34.41ms
step:177/1845 train_time:6089ms step_avg:34.40ms
step:178/1845 train_time:6126ms step_avg:34.41ms
step:179/1845 train_time:6158ms step_avg:34.40ms
step:180/1845 train_time:6194ms step_avg:34.41ms
step:181/1845 train_time:6226ms step_avg:34.40ms
step:182/1845 train_time:6261ms step_avg:34.40ms
step:183/1845 train_time:6293ms step_avg:34.39ms
step:184/1845 train_time:6328ms step_avg:34.39ms
step:185/1845 train_time:6360ms step_avg:34.38ms
step:186/1845 train_time:6396ms step_avg:34.39ms
step:187/1845 train_time:6427ms step_avg:34.37ms
step:188/1845 train_time:6463ms step_avg:34.38ms
step:189/1845 train_time:6494ms step_avg:34.36ms
step:190/1845 train_time:6530ms step_avg:34.37ms
step:191/1845 train_time:6562ms step_avg:34.35ms
step:192/1845 train_time:6598ms step_avg:34.37ms
step:193/1845 train_time:6629ms step_avg:34.35ms
step:194/1845 train_time:6666ms step_avg:34.36ms
step:195/1845 train_time:6698ms step_avg:34.35ms
step:196/1845 train_time:6734ms step_avg:34.36ms
step:197/1845 train_time:6765ms step_avg:34.34ms
step:198/1845 train_time:6802ms step_avg:34.36ms
step:199/1845 train_time:6833ms step_avg:34.34ms
step:200/1845 train_time:6870ms step_avg:34.35ms
step:201/1845 train_time:6901ms step_avg:34.33ms
step:202/1845 train_time:6938ms step_avg:34.35ms
step:203/1845 train_time:6969ms step_avg:34.33ms
step:204/1845 train_time:7006ms step_avg:34.34ms
step:205/1845 train_time:7037ms step_avg:34.33ms
step:206/1845 train_time:7074ms step_avg:34.34ms
step:207/1845 train_time:7106ms step_avg:34.33ms
step:208/1845 train_time:7142ms step_avg:34.34ms
step:209/1845 train_time:7173ms step_avg:34.32ms
step:210/1845 train_time:7211ms step_avg:34.34ms
step:211/1845 train_time:7243ms step_avg:34.33ms
step:212/1845 train_time:7279ms step_avg:34.34ms
step:213/1845 train_time:7311ms step_avg:34.33ms
step:214/1845 train_time:7347ms step_avg:34.33ms
step:215/1845 train_time:7378ms step_avg:34.32ms
step:216/1845 train_time:7415ms step_avg:34.33ms
step:217/1845 train_time:7446ms step_avg:34.31ms
step:218/1845 train_time:7483ms step_avg:34.32ms
step:219/1845 train_time:7515ms step_avg:34.31ms
step:220/1845 train_time:7550ms step_avg:34.32ms
step:221/1845 train_time:7581ms step_avg:34.30ms
step:222/1845 train_time:7618ms step_avg:34.31ms
step:223/1845 train_time:7649ms step_avg:34.30ms
step:224/1845 train_time:7686ms step_avg:34.31ms
step:225/1845 train_time:7718ms step_avg:34.30ms
step:226/1845 train_time:7754ms step_avg:34.31ms
step:227/1845 train_time:7786ms step_avg:34.30ms
step:228/1845 train_time:7822ms step_avg:34.31ms
step:229/1845 train_time:7854ms step_avg:34.30ms
step:230/1845 train_time:7890ms step_avg:34.31ms
step:231/1845 train_time:7922ms step_avg:34.29ms
step:232/1845 train_time:7957ms step_avg:34.30ms
step:233/1845 train_time:7990ms step_avg:34.29ms
step:234/1845 train_time:8025ms step_avg:34.30ms
step:235/1845 train_time:8058ms step_avg:34.29ms
step:236/1845 train_time:8094ms step_avg:34.30ms
step:237/1845 train_time:8126ms step_avg:34.29ms
step:238/1845 train_time:8162ms step_avg:34.29ms
step:239/1845 train_time:8194ms step_avg:34.28ms
step:240/1845 train_time:8229ms step_avg:34.29ms
step:241/1845 train_time:8261ms step_avg:34.28ms
step:242/1845 train_time:8297ms step_avg:34.28ms
step:243/1845 train_time:8329ms step_avg:34.28ms
step:244/1845 train_time:8365ms step_avg:34.28ms
step:245/1845 train_time:8397ms step_avg:34.27ms
step:246/1845 train_time:8433ms step_avg:34.28ms
step:247/1845 train_time:8465ms step_avg:34.27ms
step:248/1845 train_time:8501ms step_avg:34.28ms
step:249/1845 train_time:8533ms step_avg:34.27ms
step:250/1845 train_time:8569ms step_avg:34.28ms
step:250/1845 val_loss:4.6050 train_time:8606ms step_avg:34.42ms
step:251/1845 train_time:8633ms step_avg:34.39ms
step:252/1845 train_time:8660ms step_avg:34.36ms
step:253/1845 train_time:8684ms step_avg:34.32ms
step:254/1845 train_time:8711ms step_avg:34.30ms
step:255/1845 train_time:8737ms step_avg:34.26ms
step:256/1845 train_time:8774ms step_avg:34.27ms
step:257/1845 train_time:8805ms step_avg:34.26ms
step:258/1845 train_time:8845ms step_avg:34.28ms
step:259/1845 train_time:8878ms step_avg:34.28ms
step:260/1845 train_time:8914ms step_avg:34.28ms
step:261/1845 train_time:8945ms step_avg:34.27ms
step:262/1845 train_time:8980ms step_avg:34.27ms
step:263/1845 train_time:9009ms step_avg:34.26ms
step:264/1845 train_time:9047ms step_avg:34.27ms
step:265/1845 train_time:9077ms step_avg:34.25ms
step:266/1845 train_time:9116ms step_avg:34.27ms
step:267/1845 train_time:9147ms step_avg:34.26ms
step:268/1845 train_time:9183ms step_avg:34.27ms
step:269/1845 train_time:9216ms step_avg:34.26ms
step:270/1845 train_time:9251ms step_avg:34.26ms
step:271/1845 train_time:9282ms step_avg:34.25ms
step:272/1845 train_time:9319ms step_avg:34.26ms
step:273/1845 train_time:9351ms step_avg:34.25ms
step:274/1845 train_time:9386ms step_avg:34.26ms
step:275/1845 train_time:9418ms step_avg:34.25ms
step:276/1845 train_time:9454ms step_avg:34.25ms
step:277/1845 train_time:9486ms step_avg:34.25ms
step:278/1845 train_time:9522ms step_avg:34.25ms
step:279/1845 train_time:9553ms step_avg:34.24ms
step:280/1845 train_time:9590ms step_avg:34.25ms
step:281/1845 train_time:9622ms step_avg:34.24ms
step:282/1845 train_time:9658ms step_avg:34.25ms
step:283/1845 train_time:9690ms step_avg:34.24ms
step:284/1845 train_time:9723ms step_avg:34.24ms
step:285/1845 train_time:9755ms step_avg:34.23ms
step:286/1845 train_time:9795ms step_avg:34.25ms
step:287/1845 train_time:9828ms step_avg:34.24ms
step:288/1845 train_time:9864ms step_avg:34.25ms
step:289/1845 train_time:9896ms step_avg:34.24ms
step:290/1845 train_time:9931ms step_avg:34.25ms
step:291/1845 train_time:9963ms step_avg:34.24ms
step:292/1845 train_time:9999ms step_avg:34.24ms
step:293/1845 train_time:10031ms step_avg:34.23ms
step:294/1845 train_time:10067ms step_avg:34.24ms
step:295/1845 train_time:10099ms step_avg:34.23ms
step:296/1845 train_time:10134ms step_avg:34.24ms
step:297/1845 train_time:10166ms step_avg:34.23ms
step:298/1845 train_time:10202ms step_avg:34.24ms
step:299/1845 train_time:10234ms step_avg:34.23ms
step:300/1845 train_time:10270ms step_avg:34.23ms
step:301/1845 train_time:10302ms step_avg:34.23ms
step:302/1845 train_time:10337ms step_avg:34.23ms
step:303/1845 train_time:10370ms step_avg:34.22ms
step:304/1845 train_time:10405ms step_avg:34.23ms
step:305/1845 train_time:10437ms step_avg:34.22ms
step:306/1845 train_time:10473ms step_avg:34.23ms
step:307/1845 train_time:10504ms step_avg:34.22ms
step:308/1845 train_time:10540ms step_avg:34.22ms
step:309/1845 train_time:10572ms step_avg:34.21ms
step:310/1845 train_time:10609ms step_avg:34.22ms
step:311/1845 train_time:10641ms step_avg:34.21ms
step:312/1845 train_time:10677ms step_avg:34.22ms
step:313/1845 train_time:10709ms step_avg:34.21ms
step:314/1845 train_time:10744ms step_avg:34.22ms
step:315/1845 train_time:10776ms step_avg:34.21ms
step:316/1845 train_time:10812ms step_avg:34.21ms
step:317/1845 train_time:10843ms step_avg:34.20ms
step:318/1845 train_time:10880ms step_avg:34.21ms
step:319/1845 train_time:10911ms step_avg:34.21ms
step:320/1845 train_time:10948ms step_avg:34.21ms
step:321/1845 train_time:10979ms step_avg:34.20ms
step:322/1845 train_time:11016ms step_avg:34.21ms
step:323/1845 train_time:11048ms step_avg:34.21ms
step:324/1845 train_time:11084ms step_avg:34.21ms
step:325/1845 train_time:11115ms step_avg:34.20ms
step:326/1845 train_time:11152ms step_avg:34.21ms
step:327/1845 train_time:11183ms step_avg:34.20ms
step:328/1845 train_time:11220ms step_avg:34.21ms
step:329/1845 train_time:11251ms step_avg:34.20ms
step:330/1845 train_time:11289ms step_avg:34.21ms
step:331/1845 train_time:11320ms step_avg:34.20ms
step:332/1845 train_time:11356ms step_avg:34.21ms
step:333/1845 train_time:11388ms step_avg:34.20ms
step:334/1845 train_time:11424ms step_avg:34.20ms
step:335/1845 train_time:11456ms step_avg:34.20ms
step:336/1845 train_time:11492ms step_avg:34.20ms
step:337/1845 train_time:11523ms step_avg:34.19ms
step:338/1845 train_time:11559ms step_avg:34.20ms
step:339/1845 train_time:11591ms step_avg:34.19ms
step:340/1845 train_time:11627ms step_avg:34.20ms
step:341/1845 train_time:11659ms step_avg:34.19ms
step:342/1845 train_time:11695ms step_avg:34.20ms
step:343/1845 train_time:11726ms step_avg:34.19ms
step:344/1845 train_time:11763ms step_avg:34.19ms
step:345/1845 train_time:11795ms step_avg:34.19ms
step:346/1845 train_time:11830ms step_avg:34.19ms
step:347/1845 train_time:11862ms step_avg:34.18ms
step:348/1845 train_time:11898ms step_avg:34.19ms
step:349/1845 train_time:11930ms step_avg:34.18ms
step:350/1845 train_time:11966ms step_avg:34.19ms
step:351/1845 train_time:11998ms step_avg:34.18ms
step:352/1845 train_time:12034ms step_avg:34.19ms
step:353/1845 train_time:12065ms step_avg:34.18ms
step:354/1845 train_time:12102ms step_avg:34.19ms
step:355/1845 train_time:12134ms step_avg:34.18ms
step:356/1845 train_time:12169ms step_avg:34.18ms
step:357/1845 train_time:12201ms step_avg:34.18ms
step:358/1845 train_time:12237ms step_avg:34.18ms
step:359/1845 train_time:12269ms step_avg:34.18ms
step:360/1845 train_time:12306ms step_avg:34.18ms
step:361/1845 train_time:12338ms step_avg:34.18ms
step:362/1845 train_time:12373ms step_avg:34.18ms
step:363/1845 train_time:12406ms step_avg:34.17ms
step:364/1845 train_time:12441ms step_avg:34.18ms
step:365/1845 train_time:12472ms step_avg:34.17ms
step:366/1845 train_time:12509ms step_avg:34.18ms
step:367/1845 train_time:12540ms step_avg:34.17ms
step:368/1845 train_time:12576ms step_avg:34.17ms
step:369/1845 train_time:12609ms step_avg:34.17ms
step:370/1845 train_time:12644ms step_avg:34.17ms
step:371/1845 train_time:12677ms step_avg:34.17ms
step:372/1845 train_time:12713ms step_avg:34.17ms
step:373/1845 train_time:12744ms step_avg:34.17ms
step:374/1845 train_time:12780ms step_avg:34.17ms
step:375/1845 train_time:12812ms step_avg:34.17ms
step:376/1845 train_time:12848ms step_avg:34.17ms
step:377/1845 train_time:12879ms step_avg:34.16ms
step:378/1845 train_time:12916ms step_avg:34.17ms
step:379/1845 train_time:12948ms step_avg:34.16ms
step:380/1845 train_time:12984ms step_avg:34.17ms
step:381/1845 train_time:13016ms step_avg:34.16ms
step:382/1845 train_time:13051ms step_avg:34.17ms
step:383/1845 train_time:13082ms step_avg:34.16ms
step:384/1845 train_time:13119ms step_avg:34.16ms
step:385/1845 train_time:13151ms step_avg:34.16ms
step:386/1845 train_time:13188ms step_avg:34.17ms
step:387/1845 train_time:13220ms step_avg:34.16ms
step:388/1845 train_time:13256ms step_avg:34.16ms
step:389/1845 train_time:13289ms step_avg:34.16ms
step:390/1845 train_time:13325ms step_avg:34.17ms
step:391/1845 train_time:13356ms step_avg:34.16ms
step:392/1845 train_time:13392ms step_avg:34.16ms
step:393/1845 train_time:13424ms step_avg:34.16ms
step:394/1845 train_time:13459ms step_avg:34.16ms
step:395/1845 train_time:13491ms step_avg:34.16ms
step:396/1845 train_time:13527ms step_avg:34.16ms
step:397/1845 train_time:13558ms step_avg:34.15ms
step:398/1845 train_time:13595ms step_avg:34.16ms
step:399/1845 train_time:13627ms step_avg:34.15ms
step:400/1845 train_time:13663ms step_avg:34.16ms
step:401/1845 train_time:13695ms step_avg:34.15ms
step:402/1845 train_time:13730ms step_avg:34.15ms
step:403/1845 train_time:13761ms step_avg:34.15ms
step:404/1845 train_time:13798ms step_avg:34.15ms
step:405/1845 train_time:13829ms step_avg:34.14ms
step:406/1845 train_time:13865ms step_avg:34.15ms
step:407/1845 train_time:13898ms step_avg:34.15ms
step:408/1845 train_time:13933ms step_avg:34.15ms
step:409/1845 train_time:13965ms step_avg:34.14ms
step:410/1845 train_time:14001ms step_avg:34.15ms
step:411/1845 train_time:14033ms step_avg:34.14ms
step:412/1845 train_time:14069ms step_avg:34.15ms
step:413/1845 train_time:14101ms step_avg:34.14ms
step:414/1845 train_time:14137ms step_avg:34.15ms
step:415/1845 train_time:14168ms step_avg:34.14ms
step:416/1845 train_time:14205ms step_avg:34.15ms
step:417/1845 train_time:14236ms step_avg:34.14ms
step:418/1845 train_time:14272ms step_avg:34.14ms
step:419/1845 train_time:14303ms step_avg:34.14ms
step:420/1845 train_time:14340ms step_avg:34.14ms
step:421/1845 train_time:14371ms step_avg:34.14ms
step:422/1845 train_time:14408ms step_avg:34.14ms
step:423/1845 train_time:14440ms step_avg:34.14ms
step:424/1845 train_time:14476ms step_avg:34.14ms
step:425/1845 train_time:14508ms step_avg:34.14ms
step:426/1845 train_time:14543ms step_avg:34.14ms
step:427/1845 train_time:14575ms step_avg:34.13ms
step:428/1845 train_time:14611ms step_avg:34.14ms
step:429/1845 train_time:14643ms step_avg:34.13ms
step:430/1845 train_time:14679ms step_avg:34.14ms
step:431/1845 train_time:14711ms step_avg:34.13ms
step:432/1845 train_time:14747ms step_avg:34.14ms
step:433/1845 train_time:14779ms step_avg:34.13ms
step:434/1845 train_time:14816ms step_avg:34.14ms
step:435/1845 train_time:14848ms step_avg:34.13ms
step:436/1845 train_time:14883ms step_avg:34.14ms
step:437/1845 train_time:14915ms step_avg:34.13ms
step:438/1845 train_time:14950ms step_avg:34.13ms
step:439/1845 train_time:14981ms step_avg:34.13ms
step:440/1845 train_time:15019ms step_avg:34.13ms
step:441/1845 train_time:15051ms step_avg:34.13ms
step:442/1845 train_time:15087ms step_avg:34.13ms
step:443/1845 train_time:15119ms step_avg:34.13ms
step:444/1845 train_time:15155ms step_avg:34.13ms
step:445/1845 train_time:15187ms step_avg:34.13ms
step:446/1845 train_time:15222ms step_avg:34.13ms
step:447/1845 train_time:15254ms step_avg:34.12ms
step:448/1845 train_time:15290ms step_avg:34.13ms
step:449/1845 train_time:15322ms step_avg:34.13ms
step:450/1845 train_time:15358ms step_avg:34.13ms
step:451/1845 train_time:15390ms step_avg:34.12ms
step:452/1845 train_time:15426ms step_avg:34.13ms
step:453/1845 train_time:15458ms step_avg:34.12ms
step:454/1845 train_time:15494ms step_avg:34.13ms
step:455/1845 train_time:15526ms step_avg:34.12ms
step:456/1845 train_time:15562ms step_avg:34.13ms
step:457/1845 train_time:15595ms step_avg:34.12ms
step:458/1845 train_time:15630ms step_avg:34.13ms
step:459/1845 train_time:15661ms step_avg:34.12ms
step:460/1845 train_time:15697ms step_avg:34.12ms
step:461/1845 train_time:15729ms step_avg:34.12ms
step:462/1845 train_time:15764ms step_avg:34.12ms
step:463/1845 train_time:15796ms step_avg:34.12ms
step:464/1845 train_time:15833ms step_avg:34.12ms
step:465/1845 train_time:15865ms step_avg:34.12ms
step:466/1845 train_time:15901ms step_avg:34.12ms
step:467/1845 train_time:15932ms step_avg:34.12ms
step:468/1845 train_time:15968ms step_avg:34.12ms
step:469/1845 train_time:15999ms step_avg:34.11ms
step:470/1845 train_time:16037ms step_avg:34.12ms
step:471/1845 train_time:16069ms step_avg:34.12ms
step:472/1845 train_time:16105ms step_avg:34.12ms
step:473/1845 train_time:16136ms step_avg:34.11ms
step:474/1845 train_time:16172ms step_avg:34.12ms
step:475/1845 train_time:16204ms step_avg:34.11ms
step:476/1845 train_time:16240ms step_avg:34.12ms
step:477/1845 train_time:16271ms step_avg:34.11ms
step:478/1845 train_time:16308ms step_avg:34.12ms
step:479/1845 train_time:16339ms step_avg:34.11ms
step:480/1845 train_time:16376ms step_avg:34.12ms
step:481/1845 train_time:16408ms step_avg:34.11ms
step:482/1845 train_time:16443ms step_avg:34.11ms
step:483/1845 train_time:16475ms step_avg:34.11ms
step:484/1845 train_time:16512ms step_avg:34.12ms
step:485/1845 train_time:16543ms step_avg:34.11ms
step:486/1845 train_time:16579ms step_avg:34.11ms
step:487/1845 train_time:16611ms step_avg:34.11ms
step:488/1845 train_time:16647ms step_avg:34.11ms
step:489/1845 train_time:16679ms step_avg:34.11ms
step:490/1845 train_time:16715ms step_avg:34.11ms
step:491/1845 train_time:16747ms step_avg:34.11ms
step:492/1845 train_time:16783ms step_avg:34.11ms
step:493/1845 train_time:16815ms step_avg:34.11ms
step:494/1845 train_time:16851ms step_avg:34.11ms
step:495/1845 train_time:16882ms step_avg:34.11ms
step:496/1845 train_time:16919ms step_avg:34.11ms
step:497/1845 train_time:16951ms step_avg:34.11ms
step:498/1845 train_time:16986ms step_avg:34.11ms
step:499/1845 train_time:17019ms step_avg:34.11ms
step:500/1845 train_time:17054ms step_avg:34.11ms
step:500/1845 val_loss:4.2824 train_time:17092ms step_avg:34.18ms
step:501/1845 train_time:17118ms step_avg:34.17ms
step:502/1845 train_time:17145ms step_avg:34.15ms
step:503/1845 train_time:17169ms step_avg:34.13ms
step:504/1845 train_time:17197ms step_avg:34.12ms
step:505/1845 train_time:17222ms step_avg:34.10ms
step:506/1845 train_time:17261ms step_avg:34.11ms
step:507/1845 train_time:17292ms step_avg:34.11ms
step:508/1845 train_time:17330ms step_avg:34.11ms
step:509/1845 train_time:17363ms step_avg:34.11ms
step:510/1845 train_time:17400ms step_avg:34.12ms
step:511/1845 train_time:17433ms step_avg:34.12ms
step:512/1845 train_time:17469ms step_avg:34.12ms
step:513/1845 train_time:17502ms step_avg:34.12ms
step:514/1845 train_time:17539ms step_avg:34.12ms
step:515/1845 train_time:17572ms step_avg:34.12ms
step:516/1845 train_time:17606ms step_avg:34.12ms
step:517/1845 train_time:17637ms step_avg:34.11ms
step:518/1845 train_time:17672ms step_avg:34.12ms
step:519/1845 train_time:17703ms step_avg:34.11ms
step:520/1845 train_time:17737ms step_avg:34.11ms
step:521/1845 train_time:17768ms step_avg:34.10ms
step:522/1845 train_time:17804ms step_avg:34.11ms
step:523/1845 train_time:17835ms step_avg:34.10ms
step:524/1845 train_time:17872ms step_avg:34.11ms
step:525/1845 train_time:17903ms step_avg:34.10ms
step:526/1845 train_time:17939ms step_avg:34.10ms
step:527/1845 train_time:17971ms step_avg:34.10ms
step:528/1845 train_time:18010ms step_avg:34.11ms
step:529/1845 train_time:18045ms step_avg:34.11ms
step:530/1845 train_time:18081ms step_avg:34.12ms
step:531/1845 train_time:18113ms step_avg:34.11ms
step:532/1845 train_time:18148ms step_avg:34.11ms
step:533/1845 train_time:18180ms step_avg:34.11ms
step:534/1845 train_time:18216ms step_avg:34.11ms
step:535/1845 train_time:18247ms step_avg:34.11ms
step:536/1845 train_time:18282ms step_avg:34.11ms
step:537/1845 train_time:18314ms step_avg:34.10ms
step:538/1845 train_time:18349ms step_avg:34.11ms
step:539/1845 train_time:18380ms step_avg:34.10ms
step:540/1845 train_time:18417ms step_avg:34.11ms
step:541/1845 train_time:18448ms step_avg:34.10ms
step:542/1845 train_time:18487ms step_avg:34.11ms
step:543/1845 train_time:18520ms step_avg:34.11ms
step:544/1845 train_time:18556ms step_avg:34.11ms
step:545/1845 train_time:18588ms step_avg:34.11ms
step:546/1845 train_time:18625ms step_avg:34.11ms
step:547/1845 train_time:18657ms step_avg:34.11ms
step:548/1845 train_time:18695ms step_avg:34.12ms
step:549/1845 train_time:18727ms step_avg:34.11ms
step:550/1845 train_time:18765ms step_avg:34.12ms
step:551/1845 train_time:18797ms step_avg:34.11ms
step:552/1845 train_time:18834ms step_avg:34.12ms
step:553/1845 train_time:18865ms step_avg:34.11ms
step:554/1845 train_time:18904ms step_avg:34.12ms
step:555/1845 train_time:18936ms step_avg:34.12ms
step:556/1845 train_time:18973ms step_avg:34.12ms
step:557/1845 train_time:19005ms step_avg:34.12ms
step:558/1845 train_time:19042ms step_avg:34.13ms
step:559/1845 train_time:19074ms step_avg:34.12ms
step:560/1845 train_time:19113ms step_avg:34.13ms
step:561/1845 train_time:19145ms step_avg:34.13ms
step:562/1845 train_time:19183ms step_avg:34.13ms
step:563/1845 train_time:19214ms step_avg:34.13ms
step:564/1845 train_time:19250ms step_avg:34.13ms
step:565/1845 train_time:19282ms step_avg:34.13ms
step:566/1845 train_time:19318ms step_avg:34.13ms
step:567/1845 train_time:19350ms step_avg:34.13ms
step:568/1845 train_time:19385ms step_avg:34.13ms
step:569/1845 train_time:19417ms step_avg:34.12ms
step:570/1845 train_time:19454ms step_avg:34.13ms
step:571/1845 train_time:19486ms step_avg:34.13ms
step:572/1845 train_time:19524ms step_avg:34.13ms
step:573/1845 train_time:19560ms step_avg:34.14ms
step:574/1845 train_time:19597ms step_avg:34.14ms
step:575/1845 train_time:19632ms step_avg:34.14ms
step:576/1845 train_time:19669ms step_avg:34.15ms
step:577/1845 train_time:19703ms step_avg:34.15ms
step:578/1845 train_time:19740ms step_avg:34.15ms
step:579/1845 train_time:19774ms step_avg:34.15ms
step:580/1845 train_time:19811ms step_avg:34.16ms
step:581/1845 train_time:19845ms step_avg:34.16ms
step:582/1845 train_time:19882ms step_avg:34.16ms
step:583/1845 train_time:19916ms step_avg:34.16ms
step:584/1845 train_time:19953ms step_avg:34.17ms
step:585/1845 train_time:19986ms step_avg:34.16ms
step:586/1845 train_time:20024ms step_avg:34.17ms
step:587/1845 train_time:20057ms step_avg:34.17ms
step:588/1845 train_time:20095ms step_avg:34.17ms
step:589/1845 train_time:20128ms step_avg:34.17ms
step:590/1845 train_time:20165ms step_avg:34.18ms
step:591/1845 train_time:20199ms step_avg:34.18ms
step:592/1845 train_time:20236ms step_avg:34.18ms
step:593/1845 train_time:20270ms step_avg:34.18ms
step:594/1845 train_time:20308ms step_avg:34.19ms
step:595/1845 train_time:20342ms step_avg:34.19ms
step:596/1845 train_time:20379ms step_avg:34.19ms
step:597/1845 train_time:20413ms step_avg:34.19ms
step:598/1845 train_time:20450ms step_avg:34.20ms
step:599/1845 train_time:20483ms step_avg:34.20ms
step:600/1845 train_time:20521ms step_avg:34.20ms
step:601/1845 train_time:20554ms step_avg:34.20ms
step:602/1845 train_time:20591ms step_avg:34.20ms
step:603/1845 train_time:20626ms step_avg:34.21ms
step:604/1845 train_time:20665ms step_avg:34.21ms
step:605/1845 train_time:20706ms step_avg:34.22ms
step:606/1845 train_time:20768ms step_avg:34.27ms
step:607/1845 train_time:20826ms step_avg:34.31ms
step:608/1845 train_time:20890ms step_avg:34.36ms
step:609/1845 train_time:20950ms step_avg:34.40ms
step:610/1845 train_time:21014ms step_avg:34.45ms
step:611/1845 train_time:21074ms step_avg:34.49ms
step:612/1845 train_time:21137ms step_avg:34.54ms
step:613/1845 train_time:21198ms step_avg:34.58ms
step:614/1845 train_time:21261ms step_avg:34.63ms
step:615/1845 train_time:21321ms step_avg:34.67ms
step:616/1845 train_time:21384ms step_avg:34.71ms
step:617/1845 train_time:21444ms step_avg:34.76ms
step:618/1845 train_time:21508ms step_avg:34.80ms
step:619/1845 train_time:21568ms step_avg:34.84ms
step:620/1845 train_time:21630ms step_avg:34.89ms
step:621/1845 train_time:21690ms step_avg:34.93ms
step:622/1845 train_time:21752ms step_avg:34.97ms
step:623/1845 train_time:21812ms step_avg:35.01ms
step:624/1845 train_time:21874ms step_avg:35.05ms
step:625/1845 train_time:21934ms step_avg:35.10ms
step:626/1845 train_time:21997ms step_avg:35.14ms
step:627/1845 train_time:22057ms step_avg:35.18ms
step:628/1845 train_time:22120ms step_avg:35.22ms
step:629/1845 train_time:22179ms step_avg:35.26ms
step:630/1845 train_time:22243ms step_avg:35.31ms
step:631/1845 train_time:22303ms step_avg:35.34ms
step:632/1845 train_time:22365ms step_avg:35.39ms
step:633/1845 train_time:22426ms step_avg:35.43ms
step:634/1845 train_time:22489ms step_avg:35.47ms
step:635/1845 train_time:22548ms step_avg:35.51ms
step:636/1845 train_time:22612ms step_avg:35.55ms
step:637/1845 train_time:22672ms step_avg:35.59ms
step:638/1845 train_time:22734ms step_avg:35.63ms
step:639/1845 train_time:22794ms step_avg:35.67ms
step:640/1845 train_time:22856ms step_avg:35.71ms
step:641/1845 train_time:22915ms step_avg:35.75ms
step:642/1845 train_time:22978ms step_avg:35.79ms
step:643/1845 train_time:23038ms step_avg:35.83ms
step:644/1845 train_time:23101ms step_avg:35.87ms
step:645/1845 train_time:23160ms step_avg:35.91ms
step:646/1845 train_time:23223ms step_avg:35.95ms
step:647/1845 train_time:23283ms step_avg:35.99ms
step:648/1845 train_time:23347ms step_avg:36.03ms
step:649/1845 train_time:23406ms step_avg:36.06ms
step:650/1845 train_time:23470ms step_avg:36.11ms
step:651/1845 train_time:23529ms step_avg:36.14ms
step:652/1845 train_time:23593ms step_avg:36.18ms
step:653/1845 train_time:23653ms step_avg:36.22ms
step:654/1845 train_time:23715ms step_avg:36.26ms
step:655/1845 train_time:23775ms step_avg:36.30ms
step:656/1845 train_time:23838ms step_avg:36.34ms
step:657/1845 train_time:23897ms step_avg:36.37ms
step:658/1845 train_time:23960ms step_avg:36.41ms
step:659/1845 train_time:24019ms step_avg:36.45ms
step:660/1845 train_time:24082ms step_avg:36.49ms
step:661/1845 train_time:24142ms step_avg:36.52ms
step:662/1845 train_time:24204ms step_avg:36.56ms
step:663/1845 train_time:24264ms step_avg:36.60ms
step:664/1845 train_time:24327ms step_avg:36.64ms
step:665/1845 train_time:24387ms step_avg:36.67ms
step:666/1845 train_time:24451ms step_avg:36.71ms
step:667/1845 train_time:24510ms step_avg:36.75ms
step:668/1845 train_time:24573ms step_avg:36.79ms
step:669/1845 train_time:24633ms step_avg:36.82ms
step:670/1845 train_time:24696ms step_avg:36.86ms
step:671/1845 train_time:24756ms step_avg:36.89ms
step:672/1845 train_time:24818ms step_avg:36.93ms
step:673/1845 train_time:24878ms step_avg:36.97ms
step:674/1845 train_time:24940ms step_avg:37.00ms
step:675/1845 train_time:25000ms step_avg:37.04ms
step:676/1845 train_time:25062ms step_avg:37.07ms
step:677/1845 train_time:25122ms step_avg:37.11ms
step:678/1845 train_time:25185ms step_avg:37.15ms
step:679/1845 train_time:25245ms step_avg:37.18ms
step:680/1845 train_time:25308ms step_avg:37.22ms
step:681/1845 train_time:25368ms step_avg:37.25ms
step:682/1845 train_time:25431ms step_avg:37.29ms
step:683/1845 train_time:25491ms step_avg:37.32ms
step:684/1845 train_time:25554ms step_avg:37.36ms
step:685/1845 train_time:25615ms step_avg:37.39ms
step:686/1845 train_time:25677ms step_avg:37.43ms
step:687/1845 train_time:25737ms step_avg:37.46ms
step:688/1845 train_time:25799ms step_avg:37.50ms
step:689/1845 train_time:25859ms step_avg:37.53ms
step:690/1845 train_time:25922ms step_avg:37.57ms
step:691/1845 train_time:25982ms step_avg:37.60ms
step:692/1845 train_time:26046ms step_avg:37.64ms
step:693/1845 train_time:26104ms step_avg:37.67ms
step:694/1845 train_time:26167ms step_avg:37.71ms
step:695/1845 train_time:26227ms step_avg:37.74ms
step:696/1845 train_time:26290ms step_avg:37.77ms
step:697/1845 train_time:26350ms step_avg:37.81ms
step:698/1845 train_time:26413ms step_avg:37.84ms
step:699/1845 train_time:26474ms step_avg:37.87ms
step:700/1845 train_time:26537ms step_avg:37.91ms
step:701/1845 train_time:26596ms step_avg:37.94ms
step:702/1845 train_time:26659ms step_avg:37.98ms
step:703/1845 train_time:26719ms step_avg:38.01ms
step:704/1845 train_time:26782ms step_avg:38.04ms
step:705/1845 train_time:26842ms step_avg:38.07ms
step:706/1845 train_time:26904ms step_avg:38.11ms
step:707/1845 train_time:26964ms step_avg:38.14ms
step:708/1845 train_time:27027ms step_avg:38.17ms
step:709/1845 train_time:27087ms step_avg:38.20ms
step:710/1845 train_time:27150ms step_avg:38.24ms
step:711/1845 train_time:27210ms step_avg:38.27ms
step:712/1845 train_time:27273ms step_avg:38.30ms
step:713/1845 train_time:27333ms step_avg:38.34ms
step:714/1845 train_time:27396ms step_avg:38.37ms
step:715/1845 train_time:27456ms step_avg:38.40ms
step:716/1845 train_time:27518ms step_avg:38.43ms
step:717/1845 train_time:27578ms step_avg:38.46ms
step:718/1845 train_time:27641ms step_avg:38.50ms
step:719/1845 train_time:27702ms step_avg:38.53ms
step:720/1845 train_time:27765ms step_avg:38.56ms
step:721/1845 train_time:27824ms step_avg:38.59ms
step:722/1845 train_time:27888ms step_avg:38.63ms
step:723/1845 train_time:27948ms step_avg:38.66ms
step:724/1845 train_time:28010ms step_avg:38.69ms
step:725/1845 train_time:28071ms step_avg:38.72ms
step:726/1845 train_time:28134ms step_avg:38.75ms
step:727/1845 train_time:28194ms step_avg:38.78ms
step:728/1845 train_time:28257ms step_avg:38.81ms
step:729/1845 train_time:28317ms step_avg:38.84ms
step:730/1845 train_time:28380ms step_avg:38.88ms
step:731/1845 train_time:28440ms step_avg:38.91ms
step:732/1845 train_time:28503ms step_avg:38.94ms
step:733/1845 train_time:28563ms step_avg:38.97ms
step:734/1845 train_time:28626ms step_avg:39.00ms
step:735/1845 train_time:28686ms step_avg:39.03ms
step:736/1845 train_time:28749ms step_avg:39.06ms
step:737/1845 train_time:28809ms step_avg:39.09ms
step:738/1845 train_time:28873ms step_avg:39.12ms
step:739/1845 train_time:28933ms step_avg:39.15ms
step:740/1845 train_time:28996ms step_avg:39.18ms
step:741/1845 train_time:29056ms step_avg:39.21ms
step:742/1845 train_time:29118ms step_avg:39.24ms
step:743/1845 train_time:29178ms step_avg:39.27ms
step:744/1845 train_time:29241ms step_avg:39.30ms
step:745/1845 train_time:29300ms step_avg:39.33ms
step:746/1845 train_time:29364ms step_avg:39.36ms
step:747/1845 train_time:29424ms step_avg:39.39ms
step:748/1845 train_time:29488ms step_avg:39.42ms
step:749/1845 train_time:29549ms step_avg:39.45ms
step:750/1845 train_time:29612ms step_avg:39.48ms
step:750/1845 val_loss:4.0191 train_time:29681ms step_avg:39.57ms
step:751/1845 train_time:29708ms step_avg:39.56ms
step:752/1845 train_time:29736ms step_avg:39.54ms
step:753/1845 train_time:29795ms step_avg:39.57ms
step:754/1845 train_time:29859ms step_avg:39.60ms
step:755/1845 train_time:29918ms step_avg:39.63ms
step:756/1845 train_time:29981ms step_avg:39.66ms
step:757/1845 train_time:30041ms step_avg:39.68ms
step:758/1845 train_time:30103ms step_avg:39.71ms
step:759/1845 train_time:30162ms step_avg:39.74ms
step:760/1845 train_time:30224ms step_avg:39.77ms
step:761/1845 train_time:30284ms step_avg:39.79ms
step:762/1845 train_time:30346ms step_avg:39.82ms
step:763/1845 train_time:30406ms step_avg:39.85ms
step:764/1845 train_time:30469ms step_avg:39.88ms
step:765/1845 train_time:30528ms step_avg:39.91ms
step:766/1845 train_time:30593ms step_avg:39.94ms
step:767/1845 train_time:30656ms step_avg:39.97ms
step:768/1845 train_time:30719ms step_avg:40.00ms
step:769/1845 train_time:30780ms step_avg:40.03ms
step:770/1845 train_time:30843ms step_avg:40.06ms
step:771/1845 train_time:30903ms step_avg:40.08ms
step:772/1845 train_time:30965ms step_avg:40.11ms
step:773/1845 train_time:31025ms step_avg:40.14ms
step:774/1845 train_time:31087ms step_avg:40.16ms
step:775/1845 train_time:31147ms step_avg:40.19ms
step:776/1845 train_time:31210ms step_avg:40.22ms
step:777/1845 train_time:31270ms step_avg:40.24ms
step:778/1845 train_time:31332ms step_avg:40.27ms
step:779/1845 train_time:31393ms step_avg:40.30ms
step:780/1845 train_time:31455ms step_avg:40.33ms
step:781/1845 train_time:31515ms step_avg:40.35ms
step:782/1845 train_time:31578ms step_avg:40.38ms
step:783/1845 train_time:31639ms step_avg:40.41ms
step:784/1845 train_time:31702ms step_avg:40.44ms
step:785/1845 train_time:31763ms step_avg:40.46ms
step:786/1845 train_time:31825ms step_avg:40.49ms
step:787/1845 train_time:31885ms step_avg:40.52ms
step:788/1845 train_time:31948ms step_avg:40.54ms
step:789/1845 train_time:32009ms step_avg:40.57ms
step:790/1845 train_time:32072ms step_avg:40.60ms
step:791/1845 train_time:32132ms step_avg:40.62ms
step:792/1845 train_time:32194ms step_avg:40.65ms
step:793/1845 train_time:32254ms step_avg:40.67ms
step:794/1845 train_time:32317ms step_avg:40.70ms
step:795/1845 train_time:32377ms step_avg:40.73ms
step:796/1845 train_time:32439ms step_avg:40.75ms
step:797/1845 train_time:32499ms step_avg:40.78ms
step:798/1845 train_time:32562ms step_avg:40.80ms
step:799/1845 train_time:32621ms step_avg:40.83ms
step:800/1845 train_time:32684ms step_avg:40.86ms
step:801/1845 train_time:32745ms step_avg:40.88ms
step:802/1845 train_time:32808ms step_avg:40.91ms
step:803/1845 train_time:32868ms step_avg:40.93ms
step:804/1845 train_time:32930ms step_avg:40.96ms
step:805/1845 train_time:32990ms step_avg:40.98ms
step:806/1845 train_time:33053ms step_avg:41.01ms
step:807/1845 train_time:33113ms step_avg:41.03ms
step:808/1845 train_time:33175ms step_avg:41.06ms
step:809/1845 train_time:33235ms step_avg:41.08ms
step:810/1845 train_time:33298ms step_avg:41.11ms
step:811/1845 train_time:33358ms step_avg:41.13ms
step:812/1845 train_time:33420ms step_avg:41.16ms
step:813/1845 train_time:33481ms step_avg:41.18ms
step:814/1845 train_time:33544ms step_avg:41.21ms
step:815/1845 train_time:33604ms step_avg:41.23ms
step:816/1845 train_time:33666ms step_avg:41.26ms
step:817/1845 train_time:33726ms step_avg:41.28ms
step:818/1845 train_time:33789ms step_avg:41.31ms
step:819/1845 train_time:33849ms step_avg:41.33ms
step:820/1845 train_time:33913ms step_avg:41.36ms
step:821/1845 train_time:33972ms step_avg:41.38ms
step:822/1845 train_time:34035ms step_avg:41.41ms
step:823/1845 train_time:34095ms step_avg:41.43ms
step:824/1845 train_time:34157ms step_avg:41.45ms
step:825/1845 train_time:34217ms step_avg:41.48ms
step:826/1845 train_time:34280ms step_avg:41.50ms
step:827/1845 train_time:34340ms step_avg:41.52ms
step:828/1845 train_time:34403ms step_avg:41.55ms
step:829/1845 train_time:34463ms step_avg:41.57ms
step:830/1845 train_time:34525ms step_avg:41.60ms
step:831/1845 train_time:34585ms step_avg:41.62ms
step:832/1845 train_time:34648ms step_avg:41.64ms
step:833/1845 train_time:34709ms step_avg:41.67ms
step:834/1845 train_time:34772ms step_avg:41.69ms
step:835/1845 train_time:34832ms step_avg:41.71ms
step:836/1845 train_time:34894ms step_avg:41.74ms
step:837/1845 train_time:34954ms step_avg:41.76ms
step:838/1845 train_time:35017ms step_avg:41.79ms
step:839/1845 train_time:35077ms step_avg:41.81ms
step:840/1845 train_time:35138ms step_avg:41.83ms
step:841/1845 train_time:35199ms step_avg:41.85ms
step:842/1845 train_time:35262ms step_avg:41.88ms
step:843/1845 train_time:35322ms step_avg:41.90ms
step:844/1845 train_time:35385ms step_avg:41.93ms
step:845/1845 train_time:35445ms step_avg:41.95ms
step:846/1845 train_time:35507ms step_avg:41.97ms
step:847/1845 train_time:35567ms step_avg:41.99ms
step:848/1845 train_time:35630ms step_avg:42.02ms
step:849/1845 train_time:35691ms step_avg:42.04ms
step:850/1845 train_time:35754ms step_avg:42.06ms
step:851/1845 train_time:35814ms step_avg:42.08ms
step:852/1845 train_time:35877ms step_avg:42.11ms
step:853/1845 train_time:35937ms step_avg:42.13ms
step:854/1845 train_time:36000ms step_avg:42.16ms
step:855/1845 train_time:36060ms step_avg:42.18ms
step:856/1845 train_time:36123ms step_avg:42.20ms
step:857/1845 train_time:36183ms step_avg:42.22ms
step:858/1845 train_time:36245ms step_avg:42.24ms
step:859/1845 train_time:36305ms step_avg:42.26ms
step:860/1845 train_time:36368ms step_avg:42.29ms
step:861/1845 train_time:36428ms step_avg:42.31ms
step:862/1845 train_time:36490ms step_avg:42.33ms
step:863/1845 train_time:36550ms step_avg:42.35ms
step:864/1845 train_time:36613ms step_avg:42.38ms
step:865/1845 train_time:36672ms step_avg:42.40ms
step:866/1845 train_time:36735ms step_avg:42.42ms
step:867/1845 train_time:36796ms step_avg:42.44ms
step:868/1845 train_time:36859ms step_avg:42.46ms
step:869/1845 train_time:36919ms step_avg:42.48ms
step:870/1845 train_time:36981ms step_avg:42.51ms
step:871/1845 train_time:37041ms step_avg:42.53ms
step:872/1845 train_time:37103ms step_avg:42.55ms
step:873/1845 train_time:37163ms step_avg:42.57ms
step:874/1845 train_time:37226ms step_avg:42.59ms
step:875/1845 train_time:37286ms step_avg:42.61ms
step:876/1845 train_time:37348ms step_avg:42.63ms
step:877/1845 train_time:37408ms step_avg:42.65ms
step:878/1845 train_time:37471ms step_avg:42.68ms
step:879/1845 train_time:37531ms step_avg:42.70ms
step:880/1845 train_time:37595ms step_avg:42.72ms
step:881/1845 train_time:37654ms step_avg:42.74ms
step:882/1845 train_time:37716ms step_avg:42.76ms
step:883/1845 train_time:37776ms step_avg:42.78ms
step:884/1845 train_time:37840ms step_avg:42.81ms
step:885/1845 train_time:37900ms step_avg:42.82ms
step:886/1845 train_time:37963ms step_avg:42.85ms
step:887/1845 train_time:38024ms step_avg:42.87ms
step:888/1845 train_time:38086ms step_avg:42.89ms
step:889/1845 train_time:38145ms step_avg:42.91ms
step:890/1845 train_time:38208ms step_avg:42.93ms
step:891/1845 train_time:38268ms step_avg:42.95ms
step:892/1845 train_time:38332ms step_avg:42.97ms
step:893/1845 train_time:38393ms step_avg:42.99ms
step:894/1845 train_time:38455ms step_avg:43.01ms
step:895/1845 train_time:38515ms step_avg:43.03ms
step:896/1845 train_time:38578ms step_avg:43.06ms
step:897/1845 train_time:38638ms step_avg:43.07ms
step:898/1845 train_time:38701ms step_avg:43.10ms
step:899/1845 train_time:38761ms step_avg:43.12ms
step:900/1845 train_time:38824ms step_avg:43.14ms
step:901/1845 train_time:38884ms step_avg:43.16ms
step:902/1845 train_time:38946ms step_avg:43.18ms
step:903/1845 train_time:39006ms step_avg:43.20ms
step:904/1845 train_time:39069ms step_avg:43.22ms
step:905/1845 train_time:39128ms step_avg:43.24ms
step:906/1845 train_time:39191ms step_avg:43.26ms
step:907/1845 train_time:39252ms step_avg:43.28ms
step:908/1845 train_time:39315ms step_avg:43.30ms
step:909/1845 train_time:39375ms step_avg:43.32ms
step:910/1845 train_time:39438ms step_avg:43.34ms
step:911/1845 train_time:39498ms step_avg:43.36ms
step:912/1845 train_time:39560ms step_avg:43.38ms
step:913/1845 train_time:39620ms step_avg:43.40ms
step:914/1845 train_time:39683ms step_avg:43.42ms
step:915/1845 train_time:39743ms step_avg:43.44ms
step:916/1845 train_time:39806ms step_avg:43.46ms
step:917/1845 train_time:39866ms step_avg:43.47ms
step:918/1845 train_time:39928ms step_avg:43.49ms
step:919/1845 train_time:39988ms step_avg:43.51ms
step:920/1845 train_time:40050ms step_avg:43.53ms
step:921/1845 train_time:40110ms step_avg:43.55ms
step:922/1845 train_time:40174ms step_avg:43.57ms
step:923/1845 train_time:40234ms step_avg:43.59ms
step:924/1845 train_time:40297ms step_avg:43.61ms
step:925/1845 train_time:40357ms step_avg:43.63ms
step:926/1845 train_time:40420ms step_avg:43.65ms
step:927/1845 train_time:40480ms step_avg:43.67ms
step:928/1845 train_time:40542ms step_avg:43.69ms
step:929/1845 train_time:40602ms step_avg:43.71ms
step:930/1845 train_time:40665ms step_avg:43.73ms
step:931/1845 train_time:40725ms step_avg:43.74ms
step:932/1845 train_time:40787ms step_avg:43.76ms
step:933/1845 train_time:40847ms step_avg:43.78ms
step:934/1845 train_time:40910ms step_avg:43.80ms
step:935/1845 train_time:40970ms step_avg:43.82ms
step:936/1845 train_time:41032ms step_avg:43.84ms
step:937/1845 train_time:41093ms step_avg:43.86ms
step:938/1845 train_time:41155ms step_avg:43.88ms
step:939/1845 train_time:41215ms step_avg:43.89ms
step:940/1845 train_time:41277ms step_avg:43.91ms
step:941/1845 train_time:41337ms step_avg:43.93ms
step:942/1845 train_time:41400ms step_avg:43.95ms
step:943/1845 train_time:41460ms step_avg:43.97ms
step:944/1845 train_time:41523ms step_avg:43.99ms
step:945/1845 train_time:41584ms step_avg:44.00ms
step:946/1845 train_time:41647ms step_avg:44.02ms
step:947/1845 train_time:41706ms step_avg:44.04ms
step:948/1845 train_time:41769ms step_avg:44.06ms
step:949/1845 train_time:41829ms step_avg:44.08ms
step:950/1845 train_time:41892ms step_avg:44.10ms
step:951/1845 train_time:41951ms step_avg:44.11ms
step:952/1845 train_time:42015ms step_avg:44.13ms
step:953/1845 train_time:42075ms step_avg:44.15ms
step:954/1845 train_time:42137ms step_avg:44.17ms
step:955/1845 train_time:42197ms step_avg:44.19ms
step:956/1845 train_time:42259ms step_avg:44.20ms
step:957/1845 train_time:42319ms step_avg:44.22ms
step:958/1845 train_time:42382ms step_avg:44.24ms
step:959/1845 train_time:42442ms step_avg:44.26ms
step:960/1845 train_time:42504ms step_avg:44.28ms
step:961/1845 train_time:42564ms step_avg:44.29ms
step:962/1845 train_time:42627ms step_avg:44.31ms
step:963/1845 train_time:42687ms step_avg:44.33ms
step:964/1845 train_time:42749ms step_avg:44.35ms
step:965/1845 train_time:42809ms step_avg:44.36ms
step:966/1845 train_time:42872ms step_avg:44.38ms
step:967/1845 train_time:42933ms step_avg:44.40ms
step:968/1845 train_time:42996ms step_avg:44.42ms
step:969/1845 train_time:43056ms step_avg:44.43ms
step:970/1845 train_time:43118ms step_avg:44.45ms
step:971/1845 train_time:43179ms step_avg:44.47ms
step:972/1845 train_time:43241ms step_avg:44.49ms
step:973/1845 train_time:43301ms step_avg:44.50ms
step:974/1845 train_time:43364ms step_avg:44.52ms
step:975/1845 train_time:43423ms step_avg:44.54ms
step:976/1845 train_time:43486ms step_avg:44.56ms
step:977/1845 train_time:43546ms step_avg:44.57ms
step:978/1845 train_time:43609ms step_avg:44.59ms
step:979/1845 train_time:43669ms step_avg:44.61ms
step:980/1845 train_time:43731ms step_avg:44.62ms
step:981/1845 train_time:43791ms step_avg:44.64ms
step:982/1845 train_time:43853ms step_avg:44.66ms
step:983/1845 train_time:43914ms step_avg:44.67ms
step:984/1845 train_time:43977ms step_avg:44.69ms
step:985/1845 train_time:44037ms step_avg:44.71ms
step:986/1845 train_time:44099ms step_avg:44.73ms
step:987/1845 train_time:44159ms step_avg:44.74ms
step:988/1845 train_time:44222ms step_avg:44.76ms
step:989/1845 train_time:44282ms step_avg:44.77ms
step:990/1845 train_time:44344ms step_avg:44.79ms
step:991/1845 train_time:44405ms step_avg:44.81ms
step:992/1845 train_time:44467ms step_avg:44.83ms
step:993/1845 train_time:44527ms step_avg:44.84ms
step:994/1845 train_time:44590ms step_avg:44.86ms
step:995/1845 train_time:44649ms step_avg:44.87ms
step:996/1845 train_time:44712ms step_avg:44.89ms
step:997/1845 train_time:44772ms step_avg:44.91ms
step:998/1845 train_time:44835ms step_avg:44.92ms
step:999/1845 train_time:44895ms step_avg:44.94ms
step:1000/1845 train_time:44957ms step_avg:44.96ms
step:1000/1845 val_loss:3.7845 train_time:45027ms step_avg:45.03ms
step:1001/1845 train_time:45054ms step_avg:45.01ms
step:1002/1845 train_time:45082ms step_avg:44.99ms
step:1003/1845 train_time:45141ms step_avg:45.01ms
step:1004/1845 train_time:45206ms step_avg:45.03ms
step:1005/1845 train_time:45266ms step_avg:45.04ms
step:1006/1845 train_time:45329ms step_avg:45.06ms
step:1007/1845 train_time:45390ms step_avg:45.07ms
step:1008/1845 train_time:45452ms step_avg:45.09ms
step:1009/1845 train_time:45512ms step_avg:45.11ms
step:1010/1845 train_time:45574ms step_avg:45.12ms
step:1011/1845 train_time:45634ms step_avg:45.14ms
step:1012/1845 train_time:45697ms step_avg:45.15ms
step:1013/1845 train_time:45756ms step_avg:45.17ms
step:1014/1845 train_time:45819ms step_avg:45.19ms
step:1015/1845 train_time:45879ms step_avg:45.20ms
step:1016/1845 train_time:45941ms step_avg:45.22ms
step:1017/1845 train_time:46003ms step_avg:45.23ms
step:1018/1845 train_time:46065ms step_avg:45.25ms
step:1019/1845 train_time:46126ms step_avg:45.27ms
step:1020/1845 train_time:46188ms step_avg:45.28ms
step:1021/1845 train_time:46249ms step_avg:45.30ms
step:1022/1845 train_time:46312ms step_avg:45.31ms
step:1023/1845 train_time:46372ms step_avg:45.33ms
step:1024/1845 train_time:46434ms step_avg:45.35ms
step:1025/1845 train_time:46494ms step_avg:45.36ms
step:1026/1845 train_time:46556ms step_avg:45.38ms
step:1027/1845 train_time:46616ms step_avg:45.39ms
step:1028/1845 train_time:46677ms step_avg:45.41ms
step:1029/1845 train_time:46737ms step_avg:45.42ms
step:1030/1845 train_time:46800ms step_avg:45.44ms
step:1031/1845 train_time:46860ms step_avg:45.45ms
step:1032/1845 train_time:46923ms step_avg:45.47ms
step:1033/1845 train_time:46983ms step_avg:45.48ms
step:1034/1845 train_time:47046ms step_avg:45.50ms
step:1035/1845 train_time:47106ms step_avg:45.51ms
step:1036/1845 train_time:47169ms step_avg:45.53ms
step:1037/1845 train_time:47229ms step_avg:45.54ms
step:1038/1845 train_time:47291ms step_avg:45.56ms
step:1039/1845 train_time:47351ms step_avg:45.57ms
step:1040/1845 train_time:47413ms step_avg:45.59ms
step:1041/1845 train_time:47473ms step_avg:45.60ms
step:1042/1845 train_time:47536ms step_avg:45.62ms
step:1043/1845 train_time:47595ms step_avg:45.63ms
step:1044/1845 train_time:47658ms step_avg:45.65ms
step:1045/1845 train_time:47717ms step_avg:45.66ms
step:1046/1845 train_time:47780ms step_avg:45.68ms
step:1047/1845 train_time:47840ms step_avg:45.69ms
step:1048/1845 train_time:47902ms step_avg:45.71ms
step:1049/1845 train_time:47963ms step_avg:45.72ms
step:1050/1845 train_time:48025ms step_avg:45.74ms
step:1051/1845 train_time:48086ms step_avg:45.75ms
step:1052/1845 train_time:48148ms step_avg:45.77ms
step:1053/1845 train_time:48208ms step_avg:45.78ms
step:1054/1845 train_time:48272ms step_avg:45.80ms
step:1055/1845 train_time:48332ms step_avg:45.81ms
step:1056/1845 train_time:48395ms step_avg:45.83ms
step:1057/1845 train_time:48455ms step_avg:45.84ms
step:1058/1845 train_time:48517ms step_avg:45.86ms
step:1059/1845 train_time:48578ms step_avg:45.87ms
step:1060/1845 train_time:48640ms step_avg:45.89ms
step:1061/1845 train_time:48699ms step_avg:45.90ms
step:1062/1845 train_time:48761ms step_avg:45.91ms
step:1063/1845 train_time:48821ms step_avg:45.93ms
step:1064/1845 train_time:48883ms step_avg:45.94ms
step:1065/1845 train_time:48943ms step_avg:45.96ms
step:1066/1845 train_time:49006ms step_avg:45.97ms
step:1067/1845 train_time:49066ms step_avg:45.98ms
step:1068/1845 train_time:49129ms step_avg:46.00ms
step:1069/1845 train_time:49189ms step_avg:46.01ms
step:1070/1845 train_time:49252ms step_avg:46.03ms
step:1071/1845 train_time:49312ms step_avg:46.04ms
step:1072/1845 train_time:49375ms step_avg:46.06ms
step:1073/1845 train_time:49435ms step_avg:46.07ms
step:1074/1845 train_time:49497ms step_avg:46.09ms
step:1075/1845 train_time:49557ms step_avg:46.10ms
step:1076/1845 train_time:49620ms step_avg:46.11ms
step:1077/1845 train_time:49680ms step_avg:46.13ms
step:1078/1845 train_time:49743ms step_avg:46.14ms
step:1079/1845 train_time:49802ms step_avg:46.16ms
step:1080/1845 train_time:49865ms step_avg:46.17ms
step:1081/1845 train_time:49926ms step_avg:46.18ms
step:1082/1845 train_time:49988ms step_avg:46.20ms
step:1083/1845 train_time:50047ms step_avg:46.21ms
step:1084/1845 train_time:50110ms step_avg:46.23ms
step:1085/1845 train_time:50169ms step_avg:46.24ms
step:1086/1845 train_time:50233ms step_avg:46.25ms
step:1087/1845 train_time:50292ms step_avg:46.27ms
step:1088/1845 train_time:50356ms step_avg:46.28ms
step:1089/1845 train_time:50416ms step_avg:46.30ms
step:1090/1845 train_time:50478ms step_avg:46.31ms
step:1091/1845 train_time:50537ms step_avg:46.32ms
step:1092/1845 train_time:50600ms step_avg:46.34ms
step:1093/1845 train_time:50660ms step_avg:46.35ms
step:1094/1845 train_time:50722ms step_avg:46.36ms
step:1095/1845 train_time:50782ms step_avg:46.38ms
step:1096/1845 train_time:50845ms step_avg:46.39ms
step:1097/1845 train_time:50905ms step_avg:46.40ms
step:1098/1845 train_time:50967ms step_avg:46.42ms
step:1099/1845 train_time:51027ms step_avg:46.43ms
step:1100/1845 train_time:51090ms step_avg:46.45ms
step:1101/1845 train_time:51150ms step_avg:46.46ms
step:1102/1845 train_time:51213ms step_avg:46.47ms
step:1103/1845 train_time:51273ms step_avg:46.48ms
step:1104/1845 train_time:51336ms step_avg:46.50ms
step:1105/1845 train_time:51396ms step_avg:46.51ms
step:1106/1845 train_time:51458ms step_avg:46.53ms
step:1107/1845 train_time:51518ms step_avg:46.54ms
step:1108/1845 train_time:51581ms step_avg:46.55ms
step:1109/1845 train_time:51641ms step_avg:46.56ms
step:1110/1845 train_time:51703ms step_avg:46.58ms
step:1111/1845 train_time:51762ms step_avg:46.59ms
step:1112/1845 train_time:51825ms step_avg:46.60ms
step:1113/1845 train_time:51885ms step_avg:46.62ms
step:1114/1845 train_time:51947ms step_avg:46.63ms
step:1115/1845 train_time:52007ms step_avg:46.64ms
step:1116/1845 train_time:52069ms step_avg:46.66ms
step:1117/1845 train_time:52129ms step_avg:46.67ms
step:1118/1845 train_time:52192ms step_avg:46.68ms
step:1119/1845 train_time:52252ms step_avg:46.70ms
step:1120/1845 train_time:52316ms step_avg:46.71ms
step:1121/1845 train_time:52376ms step_avg:46.72ms
step:1122/1845 train_time:52439ms step_avg:46.74ms
step:1123/1845 train_time:52498ms step_avg:46.75ms
step:1124/1845 train_time:52560ms step_avg:46.76ms
step:1125/1845 train_time:52621ms step_avg:46.77ms
step:1126/1845 train_time:52683ms step_avg:46.79ms
step:1127/1845 train_time:52743ms step_avg:46.80ms
step:1128/1845 train_time:52805ms step_avg:46.81ms
step:1129/1845 train_time:52865ms step_avg:46.82ms
step:1130/1845 train_time:52928ms step_avg:46.84ms
step:1131/1845 train_time:52988ms step_avg:46.85ms
step:1132/1845 train_time:53051ms step_avg:46.86ms
step:1133/1845 train_time:53110ms step_avg:46.88ms
step:1134/1845 train_time:53173ms step_avg:46.89ms
step:1135/1845 train_time:53233ms step_avg:46.90ms
step:1136/1845 train_time:53297ms step_avg:46.92ms
step:1137/1845 train_time:53357ms step_avg:46.93ms
step:1138/1845 train_time:53420ms step_avg:46.94ms
step:1139/1845 train_time:53480ms step_avg:46.95ms
step:1140/1845 train_time:53542ms step_avg:46.97ms
step:1141/1845 train_time:53601ms step_avg:46.98ms
step:1142/1845 train_time:53664ms step_avg:46.99ms
step:1143/1845 train_time:53724ms step_avg:47.00ms
step:1144/1845 train_time:53786ms step_avg:47.02ms
step:1145/1845 train_time:53846ms step_avg:47.03ms
step:1146/1845 train_time:53909ms step_avg:47.04ms
step:1147/1845 train_time:53969ms step_avg:47.05ms
step:1148/1845 train_time:54032ms step_avg:47.07ms
step:1149/1845 train_time:54092ms step_avg:47.08ms
step:1150/1845 train_time:54155ms step_avg:47.09ms
step:1151/1845 train_time:54214ms step_avg:47.10ms
step:1152/1845 train_time:54276ms step_avg:47.11ms
step:1153/1845 train_time:54336ms step_avg:47.13ms
step:1154/1845 train_time:54399ms step_avg:47.14ms
step:1155/1845 train_time:54459ms step_avg:47.15ms
step:1156/1845 train_time:54521ms step_avg:47.16ms
step:1157/1845 train_time:54581ms step_avg:47.17ms
step:1158/1845 train_time:54644ms step_avg:47.19ms
step:1159/1845 train_time:54704ms step_avg:47.20ms
step:1160/1845 train_time:54766ms step_avg:47.21ms
step:1161/1845 train_time:54826ms step_avg:47.22ms
step:1162/1845 train_time:54889ms step_avg:47.24ms
step:1163/1845 train_time:54949ms step_avg:47.25ms
step:1164/1845 train_time:55012ms step_avg:47.26ms
step:1165/1845 train_time:55072ms step_avg:47.27ms
step:1166/1845 train_time:55135ms step_avg:47.29ms
step:1167/1845 train_time:55194ms step_avg:47.30ms
step:1168/1845 train_time:55257ms step_avg:47.31ms
step:1169/1845 train_time:55317ms step_avg:47.32ms
step:1170/1845 train_time:55380ms step_avg:47.33ms
step:1171/1845 train_time:55440ms step_avg:47.34ms
step:1172/1845 train_time:55502ms step_avg:47.36ms
step:1173/1845 train_time:55562ms step_avg:47.37ms
step:1174/1845 train_time:55624ms step_avg:47.38ms
step:1175/1845 train_time:55684ms step_avg:47.39ms
step:1176/1845 train_time:55747ms step_avg:47.40ms
step:1177/1845 train_time:55807ms step_avg:47.41ms
step:1178/1845 train_time:55869ms step_avg:47.43ms
step:1179/1845 train_time:55929ms step_avg:47.44ms
step:1180/1845 train_time:55992ms step_avg:47.45ms
step:1181/1845 train_time:56053ms step_avg:47.46ms
step:1182/1845 train_time:56116ms step_avg:47.48ms
step:1183/1845 train_time:56176ms step_avg:47.49ms
step:1184/1845 train_time:56238ms step_avg:47.50ms
step:1185/1845 train_time:56298ms step_avg:47.51ms
step:1186/1845 train_time:56360ms step_avg:47.52ms
step:1187/1845 train_time:56420ms step_avg:47.53ms
step:1188/1845 train_time:56483ms step_avg:47.54ms
step:1189/1845 train_time:56543ms step_avg:47.56ms
step:1190/1845 train_time:56605ms step_avg:47.57ms
step:1191/1845 train_time:56665ms step_avg:47.58ms
step:1192/1845 train_time:56728ms step_avg:47.59ms
step:1193/1845 train_time:56788ms step_avg:47.60ms
step:1194/1845 train_time:56850ms step_avg:47.61ms
step:1195/1845 train_time:56910ms step_avg:47.62ms
step:1196/1845 train_time:56972ms step_avg:47.64ms
step:1197/1845 train_time:57032ms step_avg:47.65ms
step:1198/1845 train_time:57095ms step_avg:47.66ms
step:1199/1845 train_time:57155ms step_avg:47.67ms
step:1200/1845 train_time:57217ms step_avg:47.68ms
step:1201/1845 train_time:57277ms step_avg:47.69ms
step:1202/1845 train_time:57340ms step_avg:47.70ms
step:1203/1845 train_time:57399ms step_avg:47.71ms
step:1204/1845 train_time:57462ms step_avg:47.73ms
step:1205/1845 train_time:57524ms step_avg:47.74ms
step:1206/1845 train_time:57610ms step_avg:47.77ms
step:1207/1845 train_time:57696ms step_avg:47.80ms
step:1208/1845 train_time:57785ms step_avg:47.84ms
step:1209/1845 train_time:57871ms step_avg:47.87ms
step:1210/1845 train_time:57959ms step_avg:47.90ms
step:1211/1845 train_time:58048ms step_avg:47.93ms
step:1212/1845 train_time:58138ms step_avg:47.97ms
step:1213/1845 train_time:58225ms step_avg:48.00ms
step:1214/1845 train_time:58315ms step_avg:48.04ms
step:1215/1845 train_time:58402ms step_avg:48.07ms
step:1216/1845 train_time:58493ms step_avg:48.10ms
step:1217/1845 train_time:58578ms step_avg:48.13ms
step:1218/1845 train_time:58667ms step_avg:48.17ms
step:1219/1845 train_time:58753ms step_avg:48.20ms
step:1220/1845 train_time:58842ms step_avg:48.23ms
step:1221/1845 train_time:58927ms step_avg:48.26ms
step:1222/1845 train_time:59017ms step_avg:48.30ms
step:1223/1845 train_time:59105ms step_avg:48.33ms
step:1224/1845 train_time:59194ms step_avg:48.36ms
step:1225/1845 train_time:59281ms step_avg:48.39ms
step:1226/1845 train_time:59371ms step_avg:48.43ms
step:1227/1845 train_time:59457ms step_avg:48.46ms
step:1228/1845 train_time:59547ms step_avg:48.49ms
step:1229/1845 train_time:59633ms step_avg:48.52ms
step:1230/1845 train_time:59721ms step_avg:48.55ms
step:1231/1845 train_time:59808ms step_avg:48.58ms
step:1232/1845 train_time:59896ms step_avg:48.62ms
step:1233/1845 train_time:59983ms step_avg:48.65ms
step:1234/1845 train_time:60073ms step_avg:48.68ms
step:1235/1845 train_time:60159ms step_avg:48.71ms
step:1236/1845 train_time:60248ms step_avg:48.74ms
step:1237/1845 train_time:60334ms step_avg:48.77ms
step:1238/1845 train_time:60424ms step_avg:48.81ms
step:1239/1845 train_time:60511ms step_avg:48.84ms
step:1240/1845 train_time:60599ms step_avg:48.87ms
step:1241/1845 train_time:60685ms step_avg:48.90ms
step:1242/1845 train_time:60774ms step_avg:48.93ms
step:1243/1845 train_time:60859ms step_avg:48.96ms
step:1244/1845 train_time:60950ms step_avg:49.00ms
step:1245/1845 train_time:61036ms step_avg:49.02ms
step:1246/1845 train_time:61126ms step_avg:49.06ms
step:1247/1845 train_time:61212ms step_avg:49.09ms
step:1248/1845 train_time:61301ms step_avg:49.12ms
step:1249/1845 train_time:61388ms step_avg:49.15ms
step:1250/1845 train_time:61476ms step_avg:49.18ms
step:1250/1845 val_loss:3.5353 train_time:61574ms step_avg:49.26ms
step:1251/1845 train_time:61601ms step_avg:49.24ms
step:1252/1845 train_time:61655ms step_avg:49.25ms
step:1253/1845 train_time:61742ms step_avg:49.28ms
step:1254/1845 train_time:61833ms step_avg:49.31ms
step:1255/1845 train_time:61918ms step_avg:49.34ms
step:1256/1845 train_time:62006ms step_avg:49.37ms
step:1257/1845 train_time:62091ms step_avg:49.40ms
step:1258/1845 train_time:62179ms step_avg:49.43ms
step:1259/1845 train_time:62264ms step_avg:49.46ms
step:1260/1845 train_time:62355ms step_avg:49.49ms
step:1261/1845 train_time:62440ms step_avg:49.52ms
step:1262/1845 train_time:62532ms step_avg:49.55ms
step:1263/1845 train_time:62620ms step_avg:49.58ms
step:1264/1845 train_time:62710ms step_avg:49.61ms
step:1265/1845 train_time:62797ms step_avg:49.64ms
step:1266/1845 train_time:62884ms step_avg:49.67ms
step:1267/1845 train_time:62971ms step_avg:49.70ms
step:1268/1845 train_time:63059ms step_avg:49.73ms
step:1269/1845 train_time:63145ms step_avg:49.76ms
step:1270/1845 train_time:63233ms step_avg:49.79ms
step:1271/1845 train_time:63319ms step_avg:49.82ms
step:1272/1845 train_time:63409ms step_avg:49.85ms
step:1273/1845 train_time:63495ms step_avg:49.88ms
step:1274/1845 train_time:63584ms step_avg:49.91ms
step:1275/1845 train_time:63672ms step_avg:49.94ms
step:1276/1845 train_time:63762ms step_avg:49.97ms
step:1277/1845 train_time:63850ms step_avg:50.00ms
step:1278/1845 train_time:63938ms step_avg:50.03ms
step:1279/1845 train_time:64023ms step_avg:50.06ms
step:1280/1845 train_time:64112ms step_avg:50.09ms
step:1281/1845 train_time:64198ms step_avg:50.12ms
step:1282/1845 train_time:64286ms step_avg:50.15ms
step:1283/1845 train_time:64372ms step_avg:50.17ms
step:1284/1845 train_time:64460ms step_avg:50.20ms
step:1285/1845 train_time:64546ms step_avg:50.23ms
step:1286/1845 train_time:64636ms step_avg:50.26ms
step:1287/1845 train_time:64722ms step_avg:50.29ms
step:1288/1845 train_time:64815ms step_avg:50.32ms
step:1289/1845 train_time:64900ms step_avg:50.35ms
step:1290/1845 train_time:64990ms step_avg:50.38ms
step:1291/1845 train_time:65076ms step_avg:50.41ms
step:1292/1845 train_time:65164ms step_avg:50.44ms
step:1293/1845 train_time:65251ms step_avg:50.47ms
step:1294/1845 train_time:65339ms step_avg:50.49ms
step:1295/1845 train_time:65426ms step_avg:50.52ms
step:1296/1845 train_time:65516ms step_avg:50.55ms
step:1297/1845 train_time:65601ms step_avg:50.58ms
step:1298/1845 train_time:65692ms step_avg:50.61ms
step:1299/1845 train_time:65780ms step_avg:50.64ms
step:1300/1845 train_time:65868ms step_avg:50.67ms
step:1301/1845 train_time:65954ms step_avg:50.70ms
step:1302/1845 train_time:66042ms step_avg:50.72ms
step:1303/1845 train_time:66128ms step_avg:50.75ms
step:1304/1845 train_time:66219ms step_avg:50.78ms
step:1305/1845 train_time:66304ms step_avg:50.81ms
step:1306/1845 train_time:66394ms step_avg:50.84ms
step:1307/1845 train_time:66480ms step_avg:50.86ms
step:1308/1845 train_time:66569ms step_avg:50.89ms
step:1309/1845 train_time:66656ms step_avg:50.92ms
step:1310/1845 train_time:66744ms step_avg:50.95ms
step:1311/1845 train_time:66831ms step_avg:50.98ms
step:1312/1845 train_time:66919ms step_avg:51.01ms
step:1313/1845 train_time:67005ms step_avg:51.03ms
step:1314/1845 train_time:67093ms step_avg:51.06ms
step:1315/1845 train_time:67180ms step_avg:51.09ms
step:1316/1845 train_time:67269ms step_avg:51.12ms
step:1317/1845 train_time:67355ms step_avg:51.14ms
step:1318/1845 train_time:67442ms step_avg:51.17ms
step:1319/1845 train_time:67530ms step_avg:51.20ms
step:1320/1845 train_time:67620ms step_avg:51.23ms
step:1321/1845 train_time:67706ms step_avg:51.25ms
step:1322/1845 train_time:67796ms step_avg:51.28ms
step:1323/1845 train_time:67882ms step_avg:51.31ms
step:1324/1845 train_time:67971ms step_avg:51.34ms
step:1325/1845 train_time:68057ms step_avg:51.36ms
step:1326/1845 train_time:68145ms step_avg:51.39ms
step:1327/1845 train_time:68231ms step_avg:51.42ms
step:1328/1845 train_time:68320ms step_avg:51.45ms
step:1329/1845 train_time:68406ms step_avg:51.47ms
step:1330/1845 train_time:68496ms step_avg:51.50ms
step:1331/1845 train_time:68583ms step_avg:51.53ms
step:1332/1845 train_time:68673ms step_avg:51.56ms
step:1333/1845 train_time:68760ms step_avg:51.58ms
step:1334/1845 train_time:68849ms step_avg:51.61ms
step:1335/1845 train_time:68934ms step_avg:51.64ms
step:1336/1845 train_time:69023ms step_avg:51.66ms
step:1337/1845 train_time:69109ms step_avg:51.69ms
step:1338/1845 train_time:69198ms step_avg:51.72ms
step:1339/1845 train_time:69284ms step_avg:51.74ms
step:1340/1845 train_time:69374ms step_avg:51.77ms
step:1341/1845 train_time:69460ms step_avg:51.80ms
step:1342/1845 train_time:69549ms step_avg:51.82ms
step:1343/1845 train_time:69636ms step_avg:51.85ms
step:1344/1845 train_time:69724ms step_avg:51.88ms
step:1345/1845 train_time:69809ms step_avg:51.90ms
step:1346/1845 train_time:69899ms step_avg:51.93ms
step:1347/1845 train_time:69985ms step_avg:51.96ms
step:1348/1845 train_time:70075ms step_avg:51.98ms
step:1349/1845 train_time:70161ms step_avg:52.01ms
step:1350/1845 train_time:70250ms step_avg:52.04ms
step:1351/1845 train_time:70336ms step_avg:52.06ms
step:1352/1845 train_time:70425ms step_avg:52.09ms
step:1353/1845 train_time:70512ms step_avg:52.12ms
step:1354/1845 train_time:70601ms step_avg:52.14ms
step:1355/1845 train_time:70688ms step_avg:52.17ms
step:1356/1845 train_time:70777ms step_avg:52.20ms
step:1357/1845 train_time:70864ms step_avg:52.22ms
step:1358/1845 train_time:70954ms step_avg:52.25ms
step:1359/1845 train_time:71040ms step_avg:52.27ms
step:1360/1845 train_time:71128ms step_avg:52.30ms
step:1361/1845 train_time:71215ms step_avg:52.33ms
step:1362/1845 train_time:71303ms step_avg:52.35ms
step:1363/1845 train_time:71390ms step_avg:52.38ms
step:1364/1845 train_time:71478ms step_avg:52.40ms
step:1365/1845 train_time:71564ms step_avg:52.43ms
step:1366/1845 train_time:71654ms step_avg:52.46ms
step:1367/1845 train_time:71741ms step_avg:52.48ms
step:1368/1845 train_time:71830ms step_avg:52.51ms
step:1369/1845 train_time:71916ms step_avg:52.53ms
step:1370/1845 train_time:72004ms step_avg:52.56ms
step:1371/1845 train_time:72091ms step_avg:52.58ms
step:1372/1845 train_time:72179ms step_avg:52.61ms
step:1373/1845 train_time:72266ms step_avg:52.63ms
step:1374/1845 train_time:72356ms step_avg:52.66ms
step:1375/1845 train_time:72441ms step_avg:52.68ms
step:1376/1845 train_time:72531ms step_avg:52.71ms
step:1377/1845 train_time:72618ms step_avg:52.74ms
step:1378/1845 train_time:72708ms step_avg:52.76ms
step:1379/1845 train_time:72794ms step_avg:52.79ms
step:1380/1845 train_time:72882ms step_avg:52.81ms
step:1381/1845 train_time:72969ms step_avg:52.84ms
step:1382/1845 train_time:73058ms step_avg:52.86ms
step:1383/1845 train_time:73144ms step_avg:52.89ms
step:1384/1845 train_time:73234ms step_avg:52.92ms
step:1385/1845 train_time:73321ms step_avg:52.94ms
step:1386/1845 train_time:73409ms step_avg:52.96ms
step:1387/1845 train_time:73495ms step_avg:52.99ms
step:1388/1845 train_time:73583ms step_avg:53.01ms
step:1389/1845 train_time:73671ms step_avg:53.04ms
step:1390/1845 train_time:73760ms step_avg:53.06ms
step:1391/1845 train_time:73847ms step_avg:53.09ms
step:1392/1845 train_time:73936ms step_avg:53.11ms
step:1393/1845 train_time:74022ms step_avg:53.14ms
step:1394/1845 train_time:74111ms step_avg:53.16ms
step:1395/1845 train_time:74197ms step_avg:53.19ms
step:1396/1845 train_time:74285ms step_avg:53.21ms
step:1397/1845 train_time:74371ms step_avg:53.24ms
step:1398/1845 train_time:74459ms step_avg:53.26ms
step:1399/1845 train_time:74546ms step_avg:53.28ms
step:1400/1845 train_time:74636ms step_avg:53.31ms
step:1401/1845 train_time:74722ms step_avg:53.33ms
step:1402/1845 train_time:74812ms step_avg:53.36ms
step:1403/1845 train_time:74898ms step_avg:53.38ms
step:1404/1845 train_time:74988ms step_avg:53.41ms
step:1405/1845 train_time:75073ms step_avg:53.43ms
step:1406/1845 train_time:75161ms step_avg:53.46ms
step:1407/1845 train_time:75248ms step_avg:53.48ms
step:1408/1845 train_time:75338ms step_avg:53.51ms
step:1409/1845 train_time:75424ms step_avg:53.53ms
step:1410/1845 train_time:75514ms step_avg:53.56ms
step:1411/1845 train_time:75600ms step_avg:53.58ms
step:1412/1845 train_time:75689ms step_avg:53.60ms
step:1413/1845 train_time:75774ms step_avg:53.63ms
step:1414/1845 train_time:75862ms step_avg:53.65ms
step:1415/1845 train_time:75949ms step_avg:53.67ms
step:1416/1845 train_time:76039ms step_avg:53.70ms
step:1417/1845 train_time:76126ms step_avg:53.72ms
step:1418/1845 train_time:76214ms step_avg:53.75ms
step:1419/1845 train_time:76300ms step_avg:53.77ms
step:1420/1845 train_time:76390ms step_avg:53.80ms
step:1421/1845 train_time:76477ms step_avg:53.82ms
step:1422/1845 train_time:76565ms step_avg:53.84ms
step:1423/1845 train_time:76653ms step_avg:53.87ms
step:1424/1845 train_time:76741ms step_avg:53.89ms
step:1425/1845 train_time:76827ms step_avg:53.91ms
step:1426/1845 train_time:76916ms step_avg:53.94ms
step:1427/1845 train_time:77001ms step_avg:53.96ms
step:1428/1845 train_time:77091ms step_avg:53.99ms
step:1429/1845 train_time:77177ms step_avg:54.01ms
step:1430/1845 train_time:77267ms step_avg:54.03ms
step:1431/1845 train_time:77353ms step_avg:54.06ms
step:1432/1845 train_time:77441ms step_avg:54.08ms
step:1433/1845 train_time:77528ms step_avg:54.10ms
step:1434/1845 train_time:77617ms step_avg:54.13ms
step:1435/1845 train_time:77703ms step_avg:54.15ms
step:1436/1845 train_time:77794ms step_avg:54.17ms
step:1437/1845 train_time:77878ms step_avg:54.20ms
step:1438/1845 train_time:77969ms step_avg:54.22ms
step:1439/1845 train_time:78055ms step_avg:54.24ms
step:1440/1845 train_time:78143ms step_avg:54.27ms
step:1441/1845 train_time:78230ms step_avg:54.29ms
step:1442/1845 train_time:78319ms step_avg:54.31ms
step:1443/1845 train_time:78405ms step_avg:54.33ms
step:1444/1845 train_time:78496ms step_avg:54.36ms
step:1445/1845 train_time:78582ms step_avg:54.38ms
step:1446/1845 train_time:78671ms step_avg:54.41ms
step:1447/1845 train_time:78757ms step_avg:54.43ms
step:1448/1845 train_time:78846ms step_avg:54.45ms
step:1449/1845 train_time:78931ms step_avg:54.47ms
step:1450/1845 train_time:79019ms step_avg:54.50ms
step:1451/1845 train_time:79104ms step_avg:54.52ms
step:1452/1845 train_time:79194ms step_avg:54.54ms
step:1453/1845 train_time:79280ms step_avg:54.56ms
step:1454/1845 train_time:79369ms step_avg:54.59ms
step:1455/1845 train_time:79457ms step_avg:54.61ms
step:1456/1845 train_time:79544ms step_avg:54.63ms
step:1457/1845 train_time:79631ms step_avg:54.65ms
step:1458/1845 train_time:79720ms step_avg:54.68ms
step:1459/1845 train_time:79807ms step_avg:54.70ms
step:1460/1845 train_time:79896ms step_avg:54.72ms
step:1461/1845 train_time:79983ms step_avg:54.75ms
step:1462/1845 train_time:80071ms step_avg:54.77ms
step:1463/1845 train_time:80157ms step_avg:54.79ms
step:1464/1845 train_time:80245ms step_avg:54.81ms
step:1465/1845 train_time:80332ms step_avg:54.83ms
step:1466/1845 train_time:80420ms step_avg:54.86ms
step:1467/1845 train_time:80506ms step_avg:54.88ms
step:1468/1845 train_time:80596ms step_avg:54.90ms
step:1469/1845 train_time:80682ms step_avg:54.92ms
step:1470/1845 train_time:80772ms step_avg:54.95ms
step:1471/1845 train_time:80859ms step_avg:54.97ms
step:1472/1845 train_time:80948ms step_avg:54.99ms
step:1473/1845 train_time:81034ms step_avg:55.01ms
step:1474/1845 train_time:81123ms step_avg:55.04ms
step:1475/1845 train_time:81210ms step_avg:55.06ms
step:1476/1845 train_time:81299ms step_avg:55.08ms
step:1477/1845 train_time:81385ms step_avg:55.10ms
step:1478/1845 train_time:81475ms step_avg:55.13ms
step:1479/1845 train_time:81560ms step_avg:55.15ms
step:1480/1845 train_time:81650ms step_avg:55.17ms
step:1481/1845 train_time:81737ms step_avg:55.19ms
step:1482/1845 train_time:81825ms step_avg:55.21ms
step:1483/1845 train_time:81913ms step_avg:55.23ms
step:1484/1845 train_time:82001ms step_avg:55.26ms
step:1485/1845 train_time:82087ms step_avg:55.28ms
step:1486/1845 train_time:82177ms step_avg:55.30ms
step:1487/1845 train_time:82262ms step_avg:55.32ms
step:1488/1845 train_time:82351ms step_avg:55.34ms
step:1489/1845 train_time:82437ms step_avg:55.36ms
step:1490/1845 train_time:82526ms step_avg:55.39ms
step:1491/1845 train_time:82613ms step_avg:55.41ms
step:1492/1845 train_time:82701ms step_avg:55.43ms
step:1493/1845 train_time:82788ms step_avg:55.45ms
step:1494/1845 train_time:82878ms step_avg:55.47ms
step:1495/1845 train_time:82965ms step_avg:55.49ms
step:1496/1845 train_time:83054ms step_avg:55.52ms
step:1497/1845 train_time:83140ms step_avg:55.54ms
step:1498/1845 train_time:83229ms step_avg:55.56ms
step:1499/1845 train_time:83315ms step_avg:55.58ms
step:1500/1845 train_time:83403ms step_avg:55.60ms
step:1500/1845 val_loss:3.4042 train_time:83502ms step_avg:55.67ms
step:1501/1845 train_time:83528ms step_avg:55.65ms
step:1502/1845 train_time:83582ms step_avg:55.65ms
step:1503/1845 train_time:83668ms step_avg:55.67ms
step:1504/1845 train_time:83760ms step_avg:55.69ms
step:1505/1845 train_time:83845ms step_avg:55.71ms
step:1506/1845 train_time:83933ms step_avg:55.73ms
step:1507/1845 train_time:84018ms step_avg:55.75ms
step:1508/1845 train_time:84106ms step_avg:55.77ms
step:1509/1845 train_time:84192ms step_avg:55.79ms
step:1510/1845 train_time:84280ms step_avg:55.81ms
step:1511/1845 train_time:84365ms step_avg:55.83ms
step:1512/1845 train_time:84456ms step_avg:55.86ms
step:1513/1845 train_time:84543ms step_avg:55.88ms
step:1514/1845 train_time:84631ms step_avg:55.90ms
step:1515/1845 train_time:84720ms step_avg:55.92ms
step:1516/1845 train_time:84808ms step_avg:55.94ms
step:1517/1845 train_time:84895ms step_avg:55.96ms
step:1518/1845 train_time:84983ms step_avg:55.98ms
step:1519/1845 train_time:85069ms step_avg:56.00ms
step:1520/1845 train_time:85158ms step_avg:56.02ms
step:1521/1845 train_time:85243ms step_avg:56.04ms
step:1522/1845 train_time:85331ms step_avg:56.07ms
step:1523/1845 train_time:85417ms step_avg:56.08ms
step:1524/1845 train_time:85507ms step_avg:56.11ms
step:1525/1845 train_time:85593ms step_avg:56.13ms
step:1526/1845 train_time:85685ms step_avg:56.15ms
step:1527/1845 train_time:85771ms step_avg:56.17ms
step:1528/1845 train_time:85860ms step_avg:56.19ms
step:1529/1845 train_time:85946ms step_avg:56.21ms
step:1530/1845 train_time:86034ms step_avg:56.23ms
step:1531/1845 train_time:86119ms step_avg:56.25ms
step:1532/1845 train_time:86208ms step_avg:56.27ms
step:1533/1845 train_time:86294ms step_avg:56.29ms
step:1534/1845 train_time:86383ms step_avg:56.31ms
step:1535/1845 train_time:86469ms step_avg:56.33ms
step:1536/1845 train_time:86558ms step_avg:56.35ms
step:1537/1845 train_time:86646ms step_avg:56.37ms
step:1538/1845 train_time:86735ms step_avg:56.39ms
step:1539/1845 train_time:86820ms step_avg:56.41ms
step:1540/1845 train_time:86908ms step_avg:56.43ms
step:1541/1845 train_time:86995ms step_avg:56.45ms
step:1542/1845 train_time:87085ms step_avg:56.48ms
step:1543/1845 train_time:87170ms step_avg:56.49ms
step:1544/1845 train_time:87259ms step_avg:56.51ms
step:1545/1845 train_time:87345ms step_avg:56.53ms
step:1546/1845 train_time:87434ms step_avg:56.55ms
step:1547/1845 train_time:87521ms step_avg:56.57ms
step:1548/1845 train_time:87609ms step_avg:56.60ms
step:1549/1845 train_time:87697ms step_avg:56.61ms
step:1550/1845 train_time:87785ms step_avg:56.64ms
step:1551/1845 train_time:87871ms step_avg:56.65ms
step:1552/1845 train_time:87960ms step_avg:56.68ms
step:1553/1845 train_time:88046ms step_avg:56.69ms
step:1554/1845 train_time:88134ms step_avg:56.71ms
step:1555/1845 train_time:88220ms step_avg:56.73ms
step:1556/1845 train_time:88308ms step_avg:56.75ms
step:1557/1845 train_time:88395ms step_avg:56.77ms
step:1558/1845 train_time:88485ms step_avg:56.79ms
step:1559/1845 train_time:88572ms step_avg:56.81ms
step:1560/1845 train_time:88661ms step_avg:56.83ms
step:1561/1845 train_time:88747ms step_avg:56.85ms
step:1562/1845 train_time:88837ms step_avg:56.87ms
step:1563/1845 train_time:88922ms step_avg:56.89ms
step:1564/1845 train_time:89010ms step_avg:56.91ms
step:1565/1845 train_time:89097ms step_avg:56.93ms
step:1566/1845 train_time:89185ms step_avg:56.95ms
step:1567/1845 train_time:89270ms step_avg:56.97ms
step:1568/1845 train_time:89361ms step_avg:56.99ms
step:1569/1845 train_time:89446ms step_avg:57.01ms
step:1570/1845 train_time:89535ms step_avg:57.03ms
step:1571/1845 train_time:89622ms step_avg:57.05ms
step:1572/1845 train_time:89710ms step_avg:57.07ms
step:1573/1845 train_time:89799ms step_avg:57.09ms
step:1574/1845 train_time:89887ms step_avg:57.11ms
step:1575/1845 train_time:89972ms step_avg:57.13ms
step:1576/1845 train_time:90062ms step_avg:57.15ms
step:1577/1845 train_time:90147ms step_avg:57.16ms
step:1578/1845 train_time:90238ms step_avg:57.18ms
step:1579/1845 train_time:90324ms step_avg:57.20ms
step:1580/1845 train_time:90412ms step_avg:57.22ms
step:1581/1845 train_time:90499ms step_avg:57.24ms
step:1582/1845 train_time:90588ms step_avg:57.26ms
step:1583/1845 train_time:90674ms step_avg:57.28ms
step:1584/1845 train_time:90764ms step_avg:57.30ms
step:1585/1845 train_time:90850ms step_avg:57.32ms
step:1586/1845 train_time:90939ms step_avg:57.34ms
step:1587/1845 train_time:91024ms step_avg:57.36ms
step:1588/1845 train_time:91114ms step_avg:57.38ms
step:1589/1845 train_time:91200ms step_avg:57.39ms
step:1590/1845 train_time:91288ms step_avg:57.41ms
step:1591/1845 train_time:91373ms step_avg:57.43ms
step:1592/1845 train_time:91464ms step_avg:57.45ms
step:1593/1845 train_time:91550ms step_avg:57.47ms
step:1594/1845 train_time:91639ms step_avg:57.49ms
step:1595/1845 train_time:91725ms step_avg:57.51ms
step:1596/1845 train_time:91814ms step_avg:57.53ms
step:1597/1845 train_time:91900ms step_avg:57.55ms
step:1598/1845 train_time:91988ms step_avg:57.56ms
step:1599/1845 train_time:92075ms step_avg:57.58ms
step:1600/1845 train_time:92165ms step_avg:57.60ms
step:1601/1845 train_time:92250ms step_avg:57.62ms
step:1602/1845 train_time:92339ms step_avg:57.64ms
step:1603/1845 train_time:92425ms step_avg:57.66ms
step:1604/1845 train_time:92514ms step_avg:57.68ms
step:1605/1845 train_time:92601ms step_avg:57.70ms
step:1606/1845 train_time:92689ms step_avg:57.71ms
step:1607/1845 train_time:92775ms step_avg:57.73ms
step:1608/1845 train_time:92864ms step_avg:57.75ms
step:1609/1845 train_time:92950ms step_avg:57.77ms
step:1610/1845 train_time:93040ms step_avg:57.79ms
step:1611/1845 train_time:93126ms step_avg:57.81ms
step:1612/1845 train_time:93215ms step_avg:57.83ms
step:1613/1845 train_time:93300ms step_avg:57.84ms
step:1614/1845 train_time:93389ms step_avg:57.86ms
step:1615/1845 train_time:93475ms step_avg:57.88ms
step:1616/1845 train_time:93566ms step_avg:57.90ms
step:1617/1845 train_time:93652ms step_avg:57.92ms
step:1618/1845 train_time:93742ms step_avg:57.94ms
step:1619/1845 train_time:93828ms step_avg:57.95ms
step:1620/1845 train_time:93916ms step_avg:57.97ms
step:1621/1845 train_time:94003ms step_avg:57.99ms
step:1622/1845 train_time:94091ms step_avg:58.01ms
step:1623/1845 train_time:94177ms step_avg:58.03ms
step:1624/1845 train_time:94267ms step_avg:58.05ms
step:1625/1845 train_time:94353ms step_avg:58.06ms
step:1626/1845 train_time:94442ms step_avg:58.08ms
step:1627/1845 train_time:94528ms step_avg:58.10ms
step:1628/1845 train_time:94618ms step_avg:58.12ms
step:1629/1845 train_time:94705ms step_avg:58.14ms
step:1630/1845 train_time:94794ms step_avg:58.16ms
step:1631/1845 train_time:94880ms step_avg:58.17ms
step:1632/1845 train_time:94969ms step_avg:58.19ms
step:1633/1845 train_time:95054ms step_avg:58.21ms
step:1634/1845 train_time:95144ms step_avg:58.23ms
step:1635/1845 train_time:95230ms step_avg:58.24ms
step:1636/1845 train_time:95319ms step_avg:58.26ms
step:1637/1845 train_time:95405ms step_avg:58.28ms
step:1638/1845 train_time:95493ms step_avg:58.30ms
step:1639/1845 train_time:95579ms step_avg:58.32ms
step:1640/1845 train_time:95669ms step_avg:58.33ms
step:1641/1845 train_time:95755ms step_avg:58.35ms
step:1642/1845 train_time:95844ms step_avg:58.37ms
step:1643/1845 train_time:95930ms step_avg:58.39ms
step:1644/1845 train_time:96019ms step_avg:58.41ms
step:1645/1845 train_time:96106ms step_avg:58.42ms
step:1646/1845 train_time:96195ms step_avg:58.44ms
step:1647/1845 train_time:96280ms step_avg:58.46ms
step:1648/1845 train_time:96368ms step_avg:58.48ms
step:1649/1845 train_time:96455ms step_avg:58.49ms
step:1650/1845 train_time:96544ms step_avg:58.51ms
step:1651/1845 train_time:96630ms step_avg:58.53ms
step:1652/1845 train_time:96721ms step_avg:58.55ms
step:1653/1845 train_time:96807ms step_avg:58.56ms
step:1654/1845 train_time:96897ms step_avg:58.58ms
step:1655/1845 train_time:96984ms step_avg:58.60ms
step:1656/1845 train_time:97072ms step_avg:58.62ms
step:1657/1845 train_time:97158ms step_avg:58.63ms
step:1658/1845 train_time:97247ms step_avg:58.65ms
step:1659/1845 train_time:97332ms step_avg:58.67ms
step:1660/1845 train_time:97423ms step_avg:58.69ms
step:1661/1845 train_time:97508ms step_avg:58.70ms
step:1662/1845 train_time:97598ms step_avg:58.72ms
step:1663/1845 train_time:97685ms step_avg:58.74ms
step:1664/1845 train_time:97774ms step_avg:58.76ms
step:1665/1845 train_time:97861ms step_avg:58.78ms
step:1666/1845 train_time:97949ms step_avg:58.79ms
step:1667/1845 train_time:98035ms step_avg:58.81ms
step:1668/1845 train_time:98125ms step_avg:58.83ms
step:1669/1845 train_time:98210ms step_avg:58.84ms
step:1670/1845 train_time:98299ms step_avg:58.86ms
step:1671/1845 train_time:98386ms step_avg:58.88ms
step:1672/1845 train_time:98474ms step_avg:58.90ms
step:1673/1845 train_time:98560ms step_avg:58.91ms
step:1674/1845 train_time:98648ms step_avg:58.93ms
step:1675/1845 train_time:98735ms step_avg:58.95ms
step:1676/1845 train_time:98824ms step_avg:58.96ms
step:1677/1845 train_time:98911ms step_avg:58.98ms
step:1678/1845 train_time:99000ms step_avg:59.00ms
step:1679/1845 train_time:99086ms step_avg:59.01ms
step:1680/1845 train_time:99175ms step_avg:59.03ms
step:1681/1845 train_time:99261ms step_avg:59.05ms
step:1682/1845 train_time:99350ms step_avg:59.07ms
step:1683/1845 train_time:99436ms step_avg:59.08ms
step:1684/1845 train_time:99526ms step_avg:59.10ms
step:1685/1845 train_time:99611ms step_avg:59.12ms
step:1686/1845 train_time:99702ms step_avg:59.14ms
step:1687/1845 train_time:99788ms step_avg:59.15ms
step:1688/1845 train_time:99877ms step_avg:59.17ms
step:1689/1845 train_time:99963ms step_avg:59.18ms
step:1690/1845 train_time:100050ms step_avg:59.20ms
step:1691/1845 train_time:100138ms step_avg:59.22ms
step:1692/1845 train_time:100226ms step_avg:59.24ms
step:1693/1845 train_time:100313ms step_avg:59.25ms
step:1694/1845 train_time:100403ms step_avg:59.27ms
step:1695/1845 train_time:100489ms step_avg:59.29ms
step:1696/1845 train_time:100578ms step_avg:59.30ms
step:1697/1845 train_time:100665ms step_avg:59.32ms
step:1698/1845 train_time:100752ms step_avg:59.34ms
step:1699/1845 train_time:100839ms step_avg:59.35ms
step:1700/1845 train_time:100928ms step_avg:59.37ms
step:1701/1845 train_time:101014ms step_avg:59.38ms
step:1702/1845 train_time:101103ms step_avg:59.40ms
step:1703/1845 train_time:101189ms step_avg:59.42ms
step:1704/1845 train_time:101279ms step_avg:59.44ms
step:1705/1845 train_time:101365ms step_avg:59.45ms
step:1706/1845 train_time:101454ms step_avg:59.47ms
step:1707/1845 train_time:101540ms step_avg:59.48ms
step:1708/1845 train_time:101628ms step_avg:59.50ms
step:1709/1845 train_time:101714ms step_avg:59.52ms
step:1710/1845 train_time:101805ms step_avg:59.53ms
step:1711/1845 train_time:101890ms step_avg:59.55ms
step:1712/1845 train_time:101979ms step_avg:59.57ms
step:1713/1845 train_time:102065ms step_avg:59.58ms
step:1714/1845 train_time:102153ms step_avg:59.60ms
step:1715/1845 train_time:102239ms step_avg:59.61ms
step:1716/1845 train_time:102328ms step_avg:59.63ms
step:1717/1845 train_time:102415ms step_avg:59.65ms
step:1718/1845 train_time:102504ms step_avg:59.66ms
step:1719/1845 train_time:102590ms step_avg:59.68ms
step:1720/1845 train_time:102680ms step_avg:59.70ms
step:1721/1845 train_time:102766ms step_avg:59.71ms
step:1722/1845 train_time:102855ms step_avg:59.73ms
step:1723/1845 train_time:102940ms step_avg:59.74ms
step:1724/1845 train_time:103029ms step_avg:59.76ms
step:1725/1845 train_time:103115ms step_avg:59.78ms
step:1726/1845 train_time:103205ms step_avg:59.79ms
step:1727/1845 train_time:103291ms step_avg:59.81ms
step:1728/1845 train_time:103380ms step_avg:59.83ms
step:1729/1845 train_time:103467ms step_avg:59.84ms
step:1730/1845 train_time:103556ms step_avg:59.86ms
step:1731/1845 train_time:103642ms step_avg:59.87ms
step:1732/1845 train_time:103730ms step_avg:59.89ms
step:1733/1845 train_time:103816ms step_avg:59.91ms
step:1734/1845 train_time:103906ms step_avg:59.92ms
step:1735/1845 train_time:103992ms step_avg:59.94ms
step:1736/1845 train_time:104082ms step_avg:59.95ms
step:1737/1845 train_time:104168ms step_avg:59.97ms
step:1738/1845 train_time:104257ms step_avg:59.99ms
step:1739/1845 train_time:104343ms step_avg:60.00ms
step:1740/1845 train_time:104432ms step_avg:60.02ms
step:1741/1845 train_time:104519ms step_avg:60.03ms
step:1742/1845 train_time:104608ms step_avg:60.05ms
step:1743/1845 train_time:104694ms step_avg:60.07ms
step:1744/1845 train_time:104784ms step_avg:60.08ms
step:1745/1845 train_time:104869ms step_avg:60.10ms
step:1746/1845 train_time:104958ms step_avg:60.11ms
step:1747/1845 train_time:105044ms step_avg:60.13ms
step:1748/1845 train_time:105133ms step_avg:60.14ms
step:1749/1845 train_time:105219ms step_avg:60.16ms
step:1750/1845 train_time:105307ms step_avg:60.18ms
step:1750/1845 val_loss:3.3037 train_time:105405ms step_avg:60.23ms
step:1751/1845 train_time:105429ms step_avg:60.21ms
step:1752/1845 train_time:105484ms step_avg:60.21ms
step:1753/1845 train_time:105569ms step_avg:60.22ms
step:1754/1845 train_time:105656ms step_avg:60.24ms
step:1755/1845 train_time:105742ms step_avg:60.25ms
step:1756/1845 train_time:105831ms step_avg:60.27ms
step:1757/1845 train_time:105916ms step_avg:60.28ms
step:1758/1845 train_time:106004ms step_avg:60.30ms
step:1759/1845 train_time:106090ms step_avg:60.31ms
step:1760/1845 train_time:106178ms step_avg:60.33ms
step:1761/1845 train_time:106264ms step_avg:60.34ms
step:1762/1845 train_time:106355ms step_avg:60.36ms
step:1763/1845 train_time:106443ms step_avg:60.38ms
step:1764/1845 train_time:106535ms step_avg:60.39ms
step:1765/1845 train_time:106621ms step_avg:60.41ms
step:1766/1845 train_time:106710ms step_avg:60.42ms
step:1767/1845 train_time:106795ms step_avg:60.44ms
step:1768/1845 train_time:106884ms step_avg:60.45ms
step:1769/1845 train_time:106970ms step_avg:60.47ms
step:1770/1845 train_time:107057ms step_avg:60.48ms
step:1771/1845 train_time:107144ms step_avg:60.50ms
step:1772/1845 train_time:107233ms step_avg:60.52ms
step:1773/1845 train_time:107319ms step_avg:60.53ms
step:1774/1845 train_time:107410ms step_avg:60.55ms
step:1775/1845 train_time:107497ms step_avg:60.56ms
step:1776/1845 train_time:107587ms step_avg:60.58ms
step:1777/1845 train_time:107673ms step_avg:60.59ms
step:1778/1845 train_time:107762ms step_avg:60.61ms
step:1779/1845 train_time:107849ms step_avg:60.62ms
step:1780/1845 train_time:107936ms step_avg:60.64ms
step:1781/1845 train_time:108022ms step_avg:60.65ms
step:1782/1845 train_time:108111ms step_avg:60.67ms
step:1783/1845 train_time:108196ms step_avg:60.68ms
step:1784/1845 train_time:108285ms step_avg:60.70ms
step:1785/1845 train_time:108373ms step_avg:60.71ms
step:1786/1845 train_time:108463ms step_avg:60.73ms
step:1787/1845 train_time:108549ms step_avg:60.74ms
step:1788/1845 train_time:108637ms step_avg:60.76ms
step:1789/1845 train_time:108725ms step_avg:60.77ms
step:1790/1845 train_time:108813ms step_avg:60.79ms
step:1791/1845 train_time:108899ms step_avg:60.80ms
step:1792/1845 train_time:108989ms step_avg:60.82ms
step:1793/1845 train_time:109074ms step_avg:60.83ms
step:1794/1845 train_time:109163ms step_avg:60.85ms
step:1795/1845 train_time:109249ms step_avg:60.86ms
step:1796/1845 train_time:109338ms step_avg:60.88ms
step:1797/1845 train_time:109426ms step_avg:60.89ms
step:1798/1845 train_time:109515ms step_avg:60.91ms
step:1799/1845 train_time:109601ms step_avg:60.92ms
step:1800/1845 train_time:109691ms step_avg:60.94ms
step:1801/1845 train_time:109777ms step_avg:60.95ms
step:1802/1845 train_time:109867ms step_avg:60.97ms
step:1803/1845 train_time:109953ms step_avg:60.98ms
step:1804/1845 train_time:110041ms step_avg:61.00ms
step:1805/1845 train_time:110127ms step_avg:61.01ms
step:1806/1845 train_time:110217ms step_avg:61.03ms
step:1807/1845 train_time:110302ms step_avg:61.04ms
step:1808/1845 train_time:110392ms step_avg:61.06ms
step:1809/1845 train_time:110478ms step_avg:61.07ms
step:1810/1845 train_time:110567ms step_avg:61.09ms
step:1811/1845 train_time:110654ms step_avg:61.10ms
step:1812/1845 train_time:110744ms step_avg:61.12ms
step:1813/1845 train_time:110832ms step_avg:61.13ms
step:1814/1845 train_time:110919ms step_avg:61.15ms
step:1815/1845 train_time:111007ms step_avg:61.16ms
step:1816/1845 train_time:111096ms step_avg:61.18ms
step:1817/1845 train_time:111182ms step_avg:61.19ms
step:1818/1845 train_time:111271ms step_avg:61.21ms
step:1819/1845 train_time:111357ms step_avg:61.22ms
step:1820/1845 train_time:111447ms step_avg:61.23ms
step:1821/1845 train_time:111535ms step_avg:61.25ms
step:1822/1845 train_time:111624ms step_avg:61.26ms
step:1823/1845 train_time:111709ms step_avg:61.28ms
step:1824/1845 train_time:111798ms step_avg:61.29ms
step:1825/1845 train_time:111885ms step_avg:61.31ms
step:1826/1845 train_time:111973ms step_avg:61.32ms
step:1827/1845 train_time:112060ms step_avg:61.34ms
step:1828/1845 train_time:112150ms step_avg:61.35ms
step:1829/1845 train_time:112236ms step_avg:61.36ms
step:1830/1845 train_time:112326ms step_avg:61.38ms
step:1831/1845 train_time:112412ms step_avg:61.39ms
step:1832/1845 train_time:112502ms step_avg:61.41ms
step:1833/1845 train_time:112589ms step_avg:61.42ms
step:1834/1845 train_time:112677ms step_avg:61.44ms
step:1835/1845 train_time:112764ms step_avg:61.45ms
step:1836/1845 train_time:112853ms step_avg:61.47ms
step:1837/1845 train_time:112939ms step_avg:61.48ms
step:1838/1845 train_time:113029ms step_avg:61.50ms
step:1839/1845 train_time:113115ms step_avg:61.51ms
step:1840/1845 train_time:113204ms step_avg:61.52ms
step:1841/1845 train_time:113291ms step_avg:61.54ms
step:1842/1845 train_time:113378ms step_avg:61.55ms
step:1843/1845 train_time:113466ms step_avg:61.57ms
step:1844/1845 train_time:113555ms step_avg:61.58ms
step:1845/1845 train_time:113642ms step_avg:61.59ms
step:1845/1845 val_loss:3.2765 train_time:113738ms step_avg:61.65ms
peak memory allocated: 29801 MiB reserved: 44438 MiB

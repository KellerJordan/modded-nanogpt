# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29-baseline-no-wandb"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return
            
        #Debug: print firing order (only on rank 0)
        # if dist.get_rank() == 0:
        #     if not hasattr(self, '_hook_counter'):
        #         self._hook_counter = 0
        #     if self._hook_counter > -1: # Signal to disable this printout               
        #         self._hook_counter += 1
        #         label = getattr(param, 'label', None)
        #         print(f"{self._hook_counter}: {label} shape={tuple(param.shape)}")


        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        # self._hook_counter = -1 # 0 to reset counter for next step, -1 to disable.
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# if master_process:
#     import wandb
#     wandb.login()
#     wandb.init(
#         project=WANDB_PROJECT,
#         name=args.run_id,
#         config=args
#     )
#     wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        # if master_process:
        #     # Log to wandb
        #     wandb.log({
        #         "val/loss": val_loss.item(),
        #         "time/training_time_ms": training_time_ms,
        #         "time/step_avg_ms": training_time_ms / (step + 1),
        #     }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        # if master_process:
        #     final_metrics = {
        #         "final/val_loss": float(val_loss.item()),
        #         "final/train_time_ms": int(training_time_ms),
        #         "final/train_time": training_time_ms / 1000,
        #         "final/steps_total": int(step),
        #     }
        #     wandb.log(final_metrics)
        #     wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
# if master_process:
#     wandb.save(f"{args.log_dir}/{args.run_id}.txt")
#     wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 07:15:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          255769      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          255770      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          255771      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          255772      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          255773      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    5   N/A  N/A          255774      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          255775      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    7   N/A  N/A          255776      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8311 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:69ms step_avg:69.37ms
step:2/1845 train_time:97ms step_avg:48.53ms
step:3/1845 train_time:122ms step_avg:40.73ms
step:4/1845 train_time:150ms step_avg:37.50ms
step:5/1845 train_time:178ms step_avg:35.50ms
step:6/1845 train_time:292ms step_avg:48.60ms
step:7/1845 train_time:322ms step_avg:45.97ms
step:8/1845 train_time:355ms step_avg:44.33ms
step:9/1845 train_time:384ms step_avg:42.70ms
step:10/1845 train_time:420ms step_avg:42.01ms
step:11/1845 train_time:451ms step_avg:41.03ms
step:12/1845 train_time:486ms step_avg:40.52ms
step:13/1845 train_time:517ms step_avg:39.80ms
step:14/1845 train_time:551ms step_avg:39.38ms
step:15/1845 train_time:582ms step_avg:38.80ms
step:16/1845 train_time:618ms step_avg:38.64ms
step:17/1845 train_time:656ms step_avg:38.56ms
step:18/1845 train_time:690ms step_avg:38.31ms
step:19/1845 train_time:726ms step_avg:38.22ms
step:20/1845 train_time:762ms step_avg:38.10ms
step:21/1845 train_time:798ms step_avg:38.02ms
step:22/1845 train_time:834ms step_avg:37.92ms
step:23/1845 train_time:870ms step_avg:37.81ms
step:24/1845 train_time:908ms step_avg:37.82ms
step:25/1845 train_time:945ms step_avg:37.79ms
step:26/1845 train_time:983ms step_avg:37.80ms
step:27/1845 train_time:1015ms step_avg:37.58ms
step:28/1845 train_time:1052ms step_avg:37.56ms
step:29/1845 train_time:1086ms step_avg:37.44ms
step:30/1845 train_time:1122ms step_avg:37.42ms
step:31/1845 train_time:1156ms step_avg:37.30ms
step:32/1845 train_time:1192ms step_avg:37.27ms
step:33/1845 train_time:1225ms step_avg:37.13ms
step:34/1845 train_time:1263ms step_avg:37.14ms
step:35/1845 train_time:1296ms step_avg:37.04ms
step:36/1845 train_time:1333ms step_avg:37.02ms
step:37/1845 train_time:1365ms step_avg:36.89ms
step:38/1845 train_time:1401ms step_avg:36.86ms
step:39/1845 train_time:1433ms step_avg:36.75ms
step:40/1845 train_time:1468ms step_avg:36.71ms
step:41/1845 train_time:1500ms step_avg:36.58ms
step:42/1845 train_time:1536ms step_avg:36.56ms
step:43/1845 train_time:1567ms step_avg:36.45ms
step:44/1845 train_time:1603ms step_avg:36.43ms
step:45/1845 train_time:1636ms step_avg:36.36ms
step:46/1845 train_time:1673ms step_avg:36.38ms
step:47/1845 train_time:1707ms step_avg:36.33ms
step:48/1845 train_time:1745ms step_avg:36.35ms
step:49/1845 train_time:1779ms step_avg:36.31ms
step:50/1845 train_time:1816ms step_avg:36.32ms
step:51/1845 train_time:1850ms step_avg:36.27ms
step:52/1845 train_time:1888ms step_avg:36.30ms
step:53/1845 train_time:1921ms step_avg:36.25ms
step:54/1845 train_time:1958ms step_avg:36.27ms
step:55/1845 train_time:1992ms step_avg:36.22ms
step:56/1845 train_time:2029ms step_avg:36.23ms
step:57/1845 train_time:2063ms step_avg:36.19ms
step:58/1845 train_time:2098ms step_avg:36.17ms
step:59/1845 train_time:2132ms step_avg:36.13ms
step:60/1845 train_time:2169ms step_avg:36.15ms
step:61/1845 train_time:2203ms step_avg:36.12ms
step:62/1845 train_time:2241ms step_avg:36.14ms
step:63/1845 train_time:2275ms step_avg:36.11ms
step:64/1845 train_time:2312ms step_avg:36.13ms
step:65/1845 train_time:2346ms step_avg:36.09ms
step:66/1845 train_time:2383ms step_avg:36.10ms
step:67/1845 train_time:2416ms step_avg:36.07ms
step:68/1845 train_time:2453ms step_avg:36.08ms
step:69/1845 train_time:2486ms step_avg:36.03ms
step:70/1845 train_time:2523ms step_avg:36.04ms
step:71/1845 train_time:2558ms step_avg:36.02ms
step:72/1845 train_time:2595ms step_avg:36.04ms
step:73/1845 train_time:2630ms step_avg:36.03ms
step:74/1845 train_time:2667ms step_avg:36.04ms
step:75/1845 train_time:2701ms step_avg:36.01ms
step:76/1845 train_time:2738ms step_avg:36.03ms
step:77/1845 train_time:2771ms step_avg:35.98ms
step:78/1845 train_time:2807ms step_avg:35.99ms
step:79/1845 train_time:2840ms step_avg:35.95ms
step:80/1845 train_time:2877ms step_avg:35.96ms
step:81/1845 train_time:2909ms step_avg:35.92ms
step:82/1845 train_time:2946ms step_avg:35.93ms
step:83/1845 train_time:2979ms step_avg:35.89ms
step:84/1845 train_time:3015ms step_avg:35.89ms
step:85/1845 train_time:3048ms step_avg:35.86ms
step:86/1845 train_time:3086ms step_avg:35.88ms
step:87/1845 train_time:3119ms step_avg:35.86ms
step:88/1845 train_time:3156ms step_avg:35.87ms
step:89/1845 train_time:3189ms step_avg:35.83ms
step:90/1845 train_time:3225ms step_avg:35.83ms
step:91/1845 train_time:3258ms step_avg:35.80ms
step:92/1845 train_time:3295ms step_avg:35.81ms
step:93/1845 train_time:3327ms step_avg:35.77ms
step:94/1845 train_time:3363ms step_avg:35.78ms
step:95/1845 train_time:3395ms step_avg:35.74ms
step:96/1845 train_time:3431ms step_avg:35.74ms
step:97/1845 train_time:3464ms step_avg:35.71ms
step:98/1845 train_time:3500ms step_avg:35.72ms
step:99/1845 train_time:3532ms step_avg:35.68ms
step:100/1845 train_time:3568ms step_avg:35.68ms
step:101/1845 train_time:3600ms step_avg:35.65ms
step:102/1845 train_time:3636ms step_avg:35.65ms
step:103/1845 train_time:3668ms step_avg:35.62ms
step:104/1845 train_time:3704ms step_avg:35.61ms
step:105/1845 train_time:3737ms step_avg:35.59ms
step:106/1845 train_time:3772ms step_avg:35.59ms
step:107/1845 train_time:3804ms step_avg:35.55ms
step:108/1845 train_time:3841ms step_avg:35.56ms
step:109/1845 train_time:3873ms step_avg:35.53ms
step:110/1845 train_time:3909ms step_avg:35.54ms
step:111/1845 train_time:3942ms step_avg:35.52ms
step:112/1845 train_time:3979ms step_avg:35.53ms
step:113/1845 train_time:4011ms step_avg:35.50ms
step:114/1845 train_time:4047ms step_avg:35.50ms
step:115/1845 train_time:4081ms step_avg:35.48ms
step:116/1845 train_time:4118ms step_avg:35.50ms
step:117/1845 train_time:4151ms step_avg:35.48ms
step:118/1845 train_time:4188ms step_avg:35.49ms
step:119/1845 train_time:4221ms step_avg:35.47ms
step:120/1845 train_time:4257ms step_avg:35.48ms
step:121/1845 train_time:4290ms step_avg:35.45ms
step:122/1845 train_time:4326ms step_avg:35.46ms
step:123/1845 train_time:4359ms step_avg:35.44ms
step:124/1845 train_time:4395ms step_avg:35.44ms
step:125/1845 train_time:4427ms step_avg:35.42ms
step:126/1845 train_time:4464ms step_avg:35.43ms
step:127/1845 train_time:4496ms step_avg:35.40ms
step:128/1845 train_time:4531ms step_avg:35.40ms
step:129/1845 train_time:4564ms step_avg:35.38ms
step:130/1845 train_time:4600ms step_avg:35.39ms
step:131/1845 train_time:4632ms step_avg:35.36ms
step:132/1845 train_time:4668ms step_avg:35.37ms
step:133/1845 train_time:4701ms step_avg:35.35ms
step:134/1845 train_time:4738ms step_avg:35.36ms
step:135/1845 train_time:4771ms step_avg:35.34ms
step:136/1845 train_time:4808ms step_avg:35.35ms
step:137/1845 train_time:4840ms step_avg:35.33ms
step:138/1845 train_time:4877ms step_avg:35.34ms
step:139/1845 train_time:4912ms step_avg:35.34ms
step:140/1845 train_time:4950ms step_avg:35.36ms
step:141/1845 train_time:4983ms step_avg:35.34ms
step:142/1845 train_time:5020ms step_avg:35.35ms
step:143/1845 train_time:5052ms step_avg:35.33ms
step:144/1845 train_time:5089ms step_avg:35.34ms
step:145/1845 train_time:5123ms step_avg:35.33ms
step:146/1845 train_time:5161ms step_avg:35.35ms
step:147/1845 train_time:5196ms step_avg:35.35ms
step:148/1845 train_time:5233ms step_avg:35.36ms
step:149/1845 train_time:5265ms step_avg:35.34ms
step:150/1845 train_time:5302ms step_avg:35.35ms
step:151/1845 train_time:5336ms step_avg:35.34ms
step:152/1845 train_time:5373ms step_avg:35.35ms
step:153/1845 train_time:5406ms step_avg:35.33ms
step:154/1845 train_time:5443ms step_avg:35.34ms
step:155/1845 train_time:5475ms step_avg:35.32ms
step:156/1845 train_time:5512ms step_avg:35.33ms
step:157/1845 train_time:5546ms step_avg:35.33ms
step:158/1845 train_time:5583ms step_avg:35.33ms
step:159/1845 train_time:5616ms step_avg:35.32ms
step:160/1845 train_time:5655ms step_avg:35.34ms
step:161/1845 train_time:5689ms step_avg:35.33ms
step:162/1845 train_time:5726ms step_avg:35.34ms
step:163/1845 train_time:5759ms step_avg:35.33ms
step:164/1845 train_time:5796ms step_avg:35.34ms
step:165/1845 train_time:5830ms step_avg:35.33ms
step:166/1845 train_time:5866ms step_avg:35.34ms
step:167/1845 train_time:5900ms step_avg:35.33ms
step:168/1845 train_time:5938ms step_avg:35.35ms
step:169/1845 train_time:5972ms step_avg:35.34ms
step:170/1845 train_time:6008ms step_avg:35.34ms
step:171/1845 train_time:6040ms step_avg:35.32ms
step:172/1845 train_time:6076ms step_avg:35.33ms
step:173/1845 train_time:6111ms step_avg:35.32ms
step:174/1845 train_time:6148ms step_avg:35.33ms
step:175/1845 train_time:6180ms step_avg:35.31ms
step:176/1845 train_time:6217ms step_avg:35.32ms
step:177/1845 train_time:6249ms step_avg:35.30ms
step:178/1845 train_time:6284ms step_avg:35.30ms
step:179/1845 train_time:6316ms step_avg:35.29ms
step:180/1845 train_time:6354ms step_avg:35.30ms
step:181/1845 train_time:6386ms step_avg:35.28ms
step:182/1845 train_time:6422ms step_avg:35.29ms
step:183/1845 train_time:6454ms step_avg:35.27ms
step:184/1845 train_time:6489ms step_avg:35.27ms
step:185/1845 train_time:6521ms step_avg:35.25ms
step:186/1845 train_time:6555ms step_avg:35.24ms
step:187/1845 train_time:6586ms step_avg:35.22ms
step:188/1845 train_time:6620ms step_avg:35.21ms
step:189/1845 train_time:6650ms step_avg:35.19ms
step:190/1845 train_time:6685ms step_avg:35.18ms
step:191/1845 train_time:6715ms step_avg:35.16ms
step:192/1845 train_time:6748ms step_avg:35.15ms
step:193/1845 train_time:6779ms step_avg:35.12ms
step:194/1845 train_time:6811ms step_avg:35.11ms
step:195/1845 train_time:6842ms step_avg:35.09ms
step:196/1845 train_time:6878ms step_avg:35.09ms
step:197/1845 train_time:6914ms step_avg:35.10ms
step:198/1845 train_time:6949ms step_avg:35.10ms
step:199/1845 train_time:6985ms step_avg:35.10ms
step:200/1845 train_time:7019ms step_avg:35.10ms
step:201/1845 train_time:7055ms step_avg:35.10ms
step:202/1845 train_time:7090ms step_avg:35.10ms
step:203/1845 train_time:7125ms step_avg:35.10ms
step:204/1845 train_time:7160ms step_avg:35.10ms
step:205/1845 train_time:7196ms step_avg:35.10ms
step:206/1845 train_time:7230ms step_avg:35.10ms
step:207/1845 train_time:7266ms step_avg:35.10ms
step:208/1845 train_time:7301ms step_avg:35.10ms
step:209/1845 train_time:7336ms step_avg:35.10ms
step:210/1845 train_time:7371ms step_avg:35.10ms
step:211/1845 train_time:7405ms step_avg:35.10ms
step:212/1845 train_time:7441ms step_avg:35.10ms
step:213/1845 train_time:7475ms step_avg:35.09ms
step:214/1845 train_time:7510ms step_avg:35.09ms
step:215/1845 train_time:7545ms step_avg:35.09ms
step:216/1845 train_time:7581ms step_avg:35.10ms
step:217/1845 train_time:7614ms step_avg:35.09ms
step:218/1845 train_time:7649ms step_avg:35.09ms
step:219/1845 train_time:7683ms step_avg:35.08ms
step:220/1845 train_time:7719ms step_avg:35.08ms
step:221/1845 train_time:7753ms step_avg:35.08ms
step:222/1845 train_time:7788ms step_avg:35.08ms
step:223/1845 train_time:7822ms step_avg:35.08ms
step:224/1845 train_time:7857ms step_avg:35.08ms
step:225/1845 train_time:7891ms step_avg:35.07ms
step:226/1845 train_time:7927ms step_avg:35.08ms
step:227/1845 train_time:7961ms step_avg:35.07ms
step:228/1845 train_time:7997ms step_avg:35.08ms
step:229/1845 train_time:8032ms step_avg:35.08ms
step:230/1845 train_time:8070ms step_avg:35.09ms
step:231/1845 train_time:8103ms step_avg:35.08ms
step:232/1845 train_time:8138ms step_avg:35.08ms
step:233/1845 train_time:8171ms step_avg:35.07ms
step:234/1845 train_time:8206ms step_avg:35.07ms
step:235/1845 train_time:8243ms step_avg:35.07ms
step:236/1845 train_time:8278ms step_avg:35.08ms
step:237/1845 train_time:8312ms step_avg:35.07ms
step:238/1845 train_time:8348ms step_avg:35.08ms
step:239/1845 train_time:8384ms step_avg:35.08ms
step:240/1845 train_time:8420ms step_avg:35.08ms
step:241/1845 train_time:8454ms step_avg:35.08ms
step:242/1845 train_time:8489ms step_avg:35.08ms
step:243/1845 train_time:8524ms step_avg:35.08ms
step:244/1845 train_time:8560ms step_avg:35.08ms
step:245/1845 train_time:8593ms step_avg:35.07ms
step:246/1845 train_time:8629ms step_avg:35.08ms
step:247/1845 train_time:8663ms step_avg:35.07ms
step:248/1845 train_time:8699ms step_avg:35.08ms
step:249/1845 train_time:8733ms step_avg:35.07ms
step:250/1845 train_time:8770ms step_avg:35.08ms
step:250/1845 val_loss:4.6073 train_time:8803ms step_avg:35.21ms
step:251/1845 train_time:8827ms step_avg:35.17ms
step:252/1845 train_time:8851ms step_avg:35.12ms
step:253/1845 train_time:8873ms step_avg:35.07ms
step:254/1845 train_time:8906ms step_avg:35.06ms
step:255/1845 train_time:8941ms step_avg:35.06ms
step:256/1845 train_time:8975ms step_avg:35.06ms
step:257/1845 train_time:9009ms step_avg:35.06ms
step:258/1845 train_time:9045ms step_avg:35.06ms
step:259/1845 train_time:9078ms step_avg:35.05ms
step:260/1845 train_time:9112ms step_avg:35.05ms
step:261/1845 train_time:9146ms step_avg:35.04ms
step:262/1845 train_time:9181ms step_avg:35.04ms
step:263/1845 train_time:9214ms step_avg:35.03ms
step:264/1845 train_time:9248ms step_avg:35.03ms
step:265/1845 train_time:9281ms step_avg:35.02ms
step:266/1845 train_time:9315ms step_avg:35.02ms
step:267/1845 train_time:9349ms step_avg:35.02ms
step:268/1845 train_time:9385ms step_avg:35.02ms
step:269/1845 train_time:9417ms step_avg:35.01ms
step:270/1845 train_time:9452ms step_avg:35.01ms
step:271/1845 train_time:9485ms step_avg:35.00ms
step:272/1845 train_time:9522ms step_avg:35.01ms
step:273/1845 train_time:9555ms step_avg:35.00ms
step:274/1845 train_time:9592ms step_avg:35.01ms
step:275/1845 train_time:9625ms step_avg:35.00ms
step:276/1845 train_time:9662ms step_avg:35.01ms
step:277/1845 train_time:9695ms step_avg:35.00ms
step:278/1845 train_time:9732ms step_avg:35.01ms
step:279/1845 train_time:9765ms step_avg:35.00ms
step:280/1845 train_time:9802ms step_avg:35.01ms
step:281/1845 train_time:9835ms step_avg:35.00ms
step:282/1845 train_time:9872ms step_avg:35.01ms
step:283/1845 train_time:9904ms step_avg:35.00ms
step:284/1845 train_time:9940ms step_avg:35.00ms
step:285/1845 train_time:9973ms step_avg:34.99ms
step:286/1845 train_time:10010ms step_avg:35.00ms
step:287/1845 train_time:10043ms step_avg:34.99ms
step:288/1845 train_time:10079ms step_avg:35.00ms
step:289/1845 train_time:10112ms step_avg:34.99ms
step:290/1845 train_time:10148ms step_avg:34.99ms
step:291/1845 train_time:10180ms step_avg:34.98ms
step:292/1845 train_time:10215ms step_avg:34.98ms
step:293/1845 train_time:10248ms step_avg:34.98ms
step:294/1845 train_time:10284ms step_avg:34.98ms
step:295/1845 train_time:10316ms step_avg:34.97ms
step:296/1845 train_time:10353ms step_avg:34.98ms
step:297/1845 train_time:10385ms step_avg:34.97ms
step:298/1845 train_time:10421ms step_avg:34.97ms
step:299/1845 train_time:10453ms step_avg:34.96ms
step:300/1845 train_time:10491ms step_avg:34.97ms
step:301/1845 train_time:10522ms step_avg:34.96ms
step:302/1845 train_time:10558ms step_avg:34.96ms
step:303/1845 train_time:10590ms step_avg:34.95ms
step:304/1845 train_time:10626ms step_avg:34.95ms
step:305/1845 train_time:10659ms step_avg:34.95ms
step:306/1845 train_time:10696ms step_avg:34.95ms
step:307/1845 train_time:10729ms step_avg:34.95ms
step:308/1845 train_time:10766ms step_avg:34.95ms
step:309/1845 train_time:10799ms step_avg:34.95ms
step:310/1845 train_time:10835ms step_avg:34.95ms
step:311/1845 train_time:10869ms step_avg:34.95ms
step:312/1845 train_time:10906ms step_avg:34.96ms
step:313/1845 train_time:10939ms step_avg:34.95ms
step:314/1845 train_time:10977ms step_avg:34.96ms
step:315/1845 train_time:11011ms step_avg:34.96ms
step:316/1845 train_time:11048ms step_avg:34.96ms
step:317/1845 train_time:11081ms step_avg:34.96ms
step:318/1845 train_time:11119ms step_avg:34.96ms
step:319/1845 train_time:11152ms step_avg:34.96ms
step:320/1845 train_time:11189ms step_avg:34.97ms
step:321/1845 train_time:11222ms step_avg:34.96ms
step:322/1845 train_time:11260ms step_avg:34.97ms
step:323/1845 train_time:11294ms step_avg:34.97ms
step:324/1845 train_time:11331ms step_avg:34.97ms
step:325/1845 train_time:11365ms step_avg:34.97ms
step:326/1845 train_time:11400ms step_avg:34.97ms
step:327/1845 train_time:11433ms step_avg:34.96ms
step:328/1845 train_time:11471ms step_avg:34.97ms
step:329/1845 train_time:11504ms step_avg:34.97ms
step:330/1845 train_time:11542ms step_avg:34.98ms
step:331/1845 train_time:11576ms step_avg:34.97ms
step:332/1845 train_time:11613ms step_avg:34.98ms
step:333/1845 train_time:11646ms step_avg:34.97ms
step:334/1845 train_time:11683ms step_avg:34.98ms
step:335/1845 train_time:11716ms step_avg:34.97ms
step:336/1845 train_time:11752ms step_avg:34.98ms
step:337/1845 train_time:11785ms step_avg:34.97ms
step:338/1845 train_time:11822ms step_avg:34.98ms
step:339/1845 train_time:11854ms step_avg:34.97ms
step:340/1845 train_time:11890ms step_avg:34.97ms
step:341/1845 train_time:11924ms step_avg:34.97ms
step:342/1845 train_time:11961ms step_avg:34.97ms
step:343/1845 train_time:11993ms step_avg:34.97ms
step:344/1845 train_time:12030ms step_avg:34.97ms
step:345/1845 train_time:12063ms step_avg:34.97ms
step:346/1845 train_time:12101ms step_avg:34.97ms
step:347/1845 train_time:12135ms step_avg:34.97ms
step:348/1845 train_time:12172ms step_avg:34.98ms
step:349/1845 train_time:12206ms step_avg:34.97ms
step:350/1845 train_time:12242ms step_avg:34.98ms
step:351/1845 train_time:12276ms step_avg:34.97ms
step:352/1845 train_time:12313ms step_avg:34.98ms
step:353/1845 train_time:12345ms step_avg:34.97ms
step:354/1845 train_time:12381ms step_avg:34.97ms
step:355/1845 train_time:12414ms step_avg:34.97ms
step:356/1845 train_time:12450ms step_avg:34.97ms
step:357/1845 train_time:12483ms step_avg:34.97ms
step:358/1845 train_time:12520ms step_avg:34.97ms
step:359/1845 train_time:12554ms step_avg:34.97ms
step:360/1845 train_time:12592ms step_avg:34.98ms
step:361/1845 train_time:12625ms step_avg:34.97ms
step:362/1845 train_time:12662ms step_avg:34.98ms
step:363/1845 train_time:12695ms step_avg:34.97ms
step:364/1845 train_time:12731ms step_avg:34.97ms
step:365/1845 train_time:12764ms step_avg:34.97ms
step:366/1845 train_time:12800ms step_avg:34.97ms
step:367/1845 train_time:12832ms step_avg:34.97ms
step:368/1845 train_time:12869ms step_avg:34.97ms
step:369/1845 train_time:12902ms step_avg:34.97ms
step:370/1845 train_time:12938ms step_avg:34.97ms
step:371/1845 train_time:12971ms step_avg:34.96ms
step:372/1845 train_time:13008ms step_avg:34.97ms
step:373/1845 train_time:13040ms step_avg:34.96ms
step:374/1845 train_time:13077ms step_avg:34.96ms
step:375/1845 train_time:13109ms step_avg:34.96ms
step:376/1845 train_time:13146ms step_avg:34.96ms
step:377/1845 train_time:13179ms step_avg:34.96ms
step:378/1845 train_time:13215ms step_avg:34.96ms
step:379/1845 train_time:13248ms step_avg:34.95ms
step:380/1845 train_time:13284ms step_avg:34.96ms
step:381/1845 train_time:13316ms step_avg:34.95ms
step:382/1845 train_time:13352ms step_avg:34.95ms
step:383/1845 train_time:13385ms step_avg:34.95ms
step:384/1845 train_time:13421ms step_avg:34.95ms
step:385/1845 train_time:13454ms step_avg:34.95ms
step:386/1845 train_time:13491ms step_avg:34.95ms
step:387/1845 train_time:13524ms step_avg:34.95ms
step:388/1845 train_time:13560ms step_avg:34.95ms
step:389/1845 train_time:13594ms step_avg:34.95ms
step:390/1845 train_time:13631ms step_avg:34.95ms
step:391/1845 train_time:13664ms step_avg:34.95ms
step:392/1845 train_time:13702ms step_avg:34.95ms
step:393/1845 train_time:13736ms step_avg:34.95ms
step:394/1845 train_time:13773ms step_avg:34.96ms
step:395/1845 train_time:13806ms step_avg:34.95ms
step:396/1845 train_time:13844ms step_avg:34.96ms
step:397/1845 train_time:13878ms step_avg:34.96ms
step:398/1845 train_time:13915ms step_avg:34.96ms
step:399/1845 train_time:13949ms step_avg:34.96ms
step:400/1845 train_time:13986ms step_avg:34.97ms
step:401/1845 train_time:14019ms step_avg:34.96ms
step:402/1845 train_time:14056ms step_avg:34.97ms
step:403/1845 train_time:14091ms step_avg:34.96ms
step:404/1845 train_time:14128ms step_avg:34.97ms
step:405/1845 train_time:14161ms step_avg:34.97ms
step:406/1845 train_time:14198ms step_avg:34.97ms
step:407/1845 train_time:14232ms step_avg:34.97ms
step:408/1845 train_time:14269ms step_avg:34.97ms
step:409/1845 train_time:14303ms step_avg:34.97ms
step:410/1845 train_time:14340ms step_avg:34.98ms
step:411/1845 train_time:14373ms step_avg:34.97ms
step:412/1845 train_time:14410ms step_avg:34.98ms
step:413/1845 train_time:14444ms step_avg:34.97ms
step:414/1845 train_time:14481ms step_avg:34.98ms
step:415/1845 train_time:14516ms step_avg:34.98ms
step:416/1845 train_time:14553ms step_avg:34.98ms
step:417/1845 train_time:14587ms step_avg:34.98ms
step:418/1845 train_time:14624ms step_avg:34.99ms
step:419/1845 train_time:14658ms step_avg:34.98ms
step:420/1845 train_time:14696ms step_avg:34.99ms
step:421/1845 train_time:14730ms step_avg:34.99ms
step:422/1845 train_time:14768ms step_avg:34.99ms
step:423/1845 train_time:14801ms step_avg:34.99ms
step:424/1845 train_time:14837ms step_avg:34.99ms
step:425/1845 train_time:14871ms step_avg:34.99ms
step:426/1845 train_time:14909ms step_avg:35.00ms
step:427/1845 train_time:14943ms step_avg:35.00ms
step:428/1845 train_time:14981ms step_avg:35.00ms
step:429/1845 train_time:15015ms step_avg:35.00ms
step:430/1845 train_time:15052ms step_avg:35.01ms
step:431/1845 train_time:15087ms step_avg:35.01ms
step:432/1845 train_time:15125ms step_avg:35.01ms
step:433/1845 train_time:15159ms step_avg:35.01ms
step:434/1845 train_time:15197ms step_avg:35.02ms
step:435/1845 train_time:15230ms step_avg:35.01ms
step:436/1845 train_time:15267ms step_avg:35.02ms
step:437/1845 train_time:15300ms step_avg:35.01ms
step:438/1845 train_time:15337ms step_avg:35.02ms
step:439/1845 train_time:15371ms step_avg:35.01ms
step:440/1845 train_time:15409ms step_avg:35.02ms
step:441/1845 train_time:15442ms step_avg:35.02ms
step:442/1845 train_time:15480ms step_avg:35.02ms
step:443/1845 train_time:15514ms step_avg:35.02ms
step:444/1845 train_time:15551ms step_avg:35.03ms
step:445/1845 train_time:15585ms step_avg:35.02ms
step:446/1845 train_time:15622ms step_avg:35.03ms
step:447/1845 train_time:15655ms step_avg:35.02ms
step:448/1845 train_time:15693ms step_avg:35.03ms
step:449/1845 train_time:15727ms step_avg:35.03ms
step:450/1845 train_time:15764ms step_avg:35.03ms
step:451/1845 train_time:15798ms step_avg:35.03ms
step:452/1845 train_time:15835ms step_avg:35.03ms
step:453/1845 train_time:15868ms step_avg:35.03ms
step:454/1845 train_time:15905ms step_avg:35.03ms
step:455/1845 train_time:15938ms step_avg:35.03ms
step:456/1845 train_time:15975ms step_avg:35.03ms
step:457/1845 train_time:16009ms step_avg:35.03ms
step:458/1845 train_time:16047ms step_avg:35.04ms
step:459/1845 train_time:16080ms step_avg:35.03ms
step:460/1845 train_time:16118ms step_avg:35.04ms
step:461/1845 train_time:16152ms step_avg:35.04ms
step:462/1845 train_time:16189ms step_avg:35.04ms
step:463/1845 train_time:16222ms step_avg:35.04ms
step:464/1845 train_time:16259ms step_avg:35.04ms
step:465/1845 train_time:16293ms step_avg:35.04ms
step:466/1845 train_time:16329ms step_avg:35.04ms
step:467/1845 train_time:16362ms step_avg:35.04ms
step:468/1845 train_time:16398ms step_avg:35.04ms
step:469/1845 train_time:16431ms step_avg:35.03ms
step:470/1845 train_time:16468ms step_avg:35.04ms
step:471/1845 train_time:16502ms step_avg:35.04ms
step:472/1845 train_time:16539ms step_avg:35.04ms
step:473/1845 train_time:16572ms step_avg:35.04ms
step:474/1845 train_time:16610ms step_avg:35.04ms
step:475/1845 train_time:16644ms step_avg:35.04ms
step:476/1845 train_time:16682ms step_avg:35.05ms
step:477/1845 train_time:16717ms step_avg:35.05ms
step:478/1845 train_time:16755ms step_avg:35.05ms
step:479/1845 train_time:16789ms step_avg:35.05ms
step:480/1845 train_time:16826ms step_avg:35.05ms
step:481/1845 train_time:16860ms step_avg:35.05ms
step:482/1845 train_time:16897ms step_avg:35.06ms
step:483/1845 train_time:16930ms step_avg:35.05ms
step:484/1845 train_time:16968ms step_avg:35.06ms
step:485/1845 train_time:17002ms step_avg:35.05ms
step:486/1845 train_time:17038ms step_avg:35.06ms
step:487/1845 train_time:17071ms step_avg:35.05ms
step:488/1845 train_time:17111ms step_avg:35.06ms
step:489/1845 train_time:17144ms step_avg:35.06ms
step:490/1845 train_time:17181ms step_avg:35.06ms
step:491/1845 train_time:17216ms step_avg:35.06ms
step:492/1845 train_time:17253ms step_avg:35.07ms
step:493/1845 train_time:17285ms step_avg:35.06ms
step:494/1845 train_time:17322ms step_avg:35.07ms
step:495/1845 train_time:17355ms step_avg:35.06ms
step:496/1845 train_time:17392ms step_avg:35.07ms
step:497/1845 train_time:17427ms step_avg:35.06ms
step:498/1845 train_time:17464ms step_avg:35.07ms
step:499/1845 train_time:17497ms step_avg:35.06ms
step:500/1845 train_time:17534ms step_avg:35.07ms
step:500/1845 val_loss:4.2923 train_time:17537ms step_avg:35.07ms
step:501/1845 train_time:17563ms step_avg:35.06ms
step:502/1845 train_time:17590ms step_avg:35.04ms
step:503/1845 train_time:17615ms step_avg:35.02ms
step:504/1845 train_time:17643ms step_avg:35.01ms
step:505/1845 train_time:17676ms step_avg:35.00ms
step:506/1845 train_time:17712ms step_avg:35.00ms
step:507/1845 train_time:17745ms step_avg:35.00ms
step:508/1845 train_time:17781ms step_avg:35.00ms
step:509/1845 train_time:17815ms step_avg:35.00ms
step:510/1845 train_time:17851ms step_avg:35.00ms
step:511/1845 train_time:17884ms step_avg:35.00ms
step:512/1845 train_time:17918ms step_avg:35.00ms
step:513/1845 train_time:17950ms step_avg:34.99ms
step:514/1845 train_time:17986ms step_avg:34.99ms
step:515/1845 train_time:18018ms step_avg:34.99ms
step:516/1845 train_time:18054ms step_avg:34.99ms
step:517/1845 train_time:18088ms step_avg:34.99ms
step:518/1845 train_time:18125ms step_avg:34.99ms
step:519/1845 train_time:18158ms step_avg:34.99ms
step:520/1845 train_time:18196ms step_avg:34.99ms
step:521/1845 train_time:18229ms step_avg:34.99ms
step:522/1845 train_time:18267ms step_avg:34.99ms
step:523/1845 train_time:18301ms step_avg:34.99ms
step:524/1845 train_time:18338ms step_avg:35.00ms
step:525/1845 train_time:18371ms step_avg:34.99ms
step:526/1845 train_time:18409ms step_avg:35.00ms
step:527/1845 train_time:18443ms step_avg:35.00ms
step:528/1845 train_time:18480ms step_avg:35.00ms
step:529/1845 train_time:18514ms step_avg:35.00ms
step:530/1845 train_time:18551ms step_avg:35.00ms
step:531/1845 train_time:18585ms step_avg:35.00ms
step:532/1845 train_time:18623ms step_avg:35.01ms
step:533/1845 train_time:18657ms step_avg:35.00ms
step:534/1845 train_time:18695ms step_avg:35.01ms
step:535/1845 train_time:18729ms step_avg:35.01ms
step:536/1845 train_time:18767ms step_avg:35.01ms
step:537/1845 train_time:18800ms step_avg:35.01ms
step:538/1845 train_time:18837ms step_avg:35.01ms
step:539/1845 train_time:18872ms step_avg:35.01ms
step:540/1845 train_time:18909ms step_avg:35.02ms
step:541/1845 train_time:18943ms step_avg:35.02ms
step:542/1845 train_time:18981ms step_avg:35.02ms
step:543/1845 train_time:19014ms step_avg:35.02ms
step:544/1845 train_time:19051ms step_avg:35.02ms
step:545/1845 train_time:19085ms step_avg:35.02ms
step:546/1845 train_time:19122ms step_avg:35.02ms
step:547/1845 train_time:19155ms step_avg:35.02ms
step:548/1845 train_time:19194ms step_avg:35.03ms
step:549/1845 train_time:19228ms step_avg:35.02ms
step:550/1845 train_time:19266ms step_avg:35.03ms
step:551/1845 train_time:19301ms step_avg:35.03ms
step:552/1845 train_time:19339ms step_avg:35.03ms
step:553/1845 train_time:19373ms step_avg:35.03ms
step:554/1845 train_time:19411ms step_avg:35.04ms
step:555/1845 train_time:19445ms step_avg:35.04ms
step:556/1845 train_time:19484ms step_avg:35.04ms
step:557/1845 train_time:19517ms step_avg:35.04ms
step:558/1845 train_time:19555ms step_avg:35.04ms
step:559/1845 train_time:19589ms step_avg:35.04ms
step:560/1845 train_time:19627ms step_avg:35.05ms
step:561/1845 train_time:19661ms step_avg:35.05ms
step:562/1845 train_time:19699ms step_avg:35.05ms
step:563/1845 train_time:19733ms step_avg:35.05ms
step:564/1845 train_time:19771ms step_avg:35.06ms
step:565/1845 train_time:19805ms step_avg:35.05ms
step:566/1845 train_time:19843ms step_avg:35.06ms
step:567/1845 train_time:19877ms step_avg:35.06ms
step:568/1845 train_time:19913ms step_avg:35.06ms
step:569/1845 train_time:19945ms step_avg:35.05ms
step:570/1845 train_time:19980ms step_avg:35.05ms
step:571/1845 train_time:20011ms step_avg:35.05ms
step:572/1845 train_time:20045ms step_avg:35.04ms
step:573/1845 train_time:20076ms step_avg:35.04ms
step:574/1845 train_time:20109ms step_avg:35.03ms
step:575/1845 train_time:20139ms step_avg:35.02ms
step:576/1845 train_time:20172ms step_avg:35.02ms
step:577/1845 train_time:20202ms step_avg:35.01ms
step:578/1845 train_time:20234ms step_avg:35.01ms
step:579/1845 train_time:20264ms step_avg:35.00ms
step:580/1845 train_time:20297ms step_avg:34.99ms
step:581/1845 train_time:20326ms step_avg:34.98ms
step:582/1845 train_time:20361ms step_avg:34.98ms
step:583/1845 train_time:20398ms step_avg:34.99ms
step:584/1845 train_time:20433ms step_avg:34.99ms
step:585/1845 train_time:20470ms step_avg:34.99ms
step:586/1845 train_time:20507ms step_avg:35.00ms
step:587/1845 train_time:20542ms step_avg:35.00ms
step:588/1845 train_time:20580ms step_avg:35.00ms
step:589/1845 train_time:20615ms step_avg:35.00ms
step:590/1845 train_time:20650ms step_avg:35.00ms
step:591/1845 train_time:20687ms step_avg:35.00ms
step:592/1845 train_time:20723ms step_avg:35.01ms
step:593/1845 train_time:20757ms step_avg:35.00ms
step:594/1845 train_time:20794ms step_avg:35.01ms
step:595/1845 train_time:20828ms step_avg:35.01ms
step:596/1845 train_time:20865ms step_avg:35.01ms
step:597/1845 train_time:20898ms step_avg:35.01ms
step:598/1845 train_time:20934ms step_avg:35.01ms
step:599/1845 train_time:20968ms step_avg:35.01ms
step:600/1845 train_time:21005ms step_avg:35.01ms
step:601/1845 train_time:21041ms step_avg:35.01ms
step:602/1845 train_time:21079ms step_avg:35.01ms
step:603/1845 train_time:21116ms step_avg:35.02ms
step:604/1845 train_time:21176ms step_avg:35.06ms
step:605/1845 train_time:21236ms step_avg:35.10ms
step:606/1845 train_time:21297ms step_avg:35.14ms
step:607/1845 train_time:21358ms step_avg:35.19ms
step:608/1845 train_time:21418ms step_avg:35.23ms
step:609/1845 train_time:21480ms step_avg:35.27ms
step:610/1845 train_time:21540ms step_avg:35.31ms
step:611/1845 train_time:21604ms step_avg:35.36ms
step:612/1845 train_time:21664ms step_avg:35.40ms
step:613/1845 train_time:21727ms step_avg:35.44ms
step:614/1845 train_time:21788ms step_avg:35.49ms
step:615/1845 train_time:21850ms step_avg:35.53ms
step:616/1845 train_time:21911ms step_avg:35.57ms
step:617/1845 train_time:21974ms step_avg:35.61ms
step:618/1845 train_time:22036ms step_avg:35.66ms
step:619/1845 train_time:22098ms step_avg:35.70ms
step:620/1845 train_time:22159ms step_avg:35.74ms
step:621/1845 train_time:22219ms step_avg:35.78ms
step:622/1845 train_time:22280ms step_avg:35.82ms
step:623/1845 train_time:22341ms step_avg:35.86ms
step:624/1845 train_time:22402ms step_avg:35.90ms
step:625/1845 train_time:22464ms step_avg:35.94ms
step:626/1845 train_time:22524ms step_avg:35.98ms
step:627/1845 train_time:22586ms step_avg:36.02ms
step:628/1845 train_time:22647ms step_avg:36.06ms
step:629/1845 train_time:22709ms step_avg:36.10ms
step:630/1845 train_time:22769ms step_avg:36.14ms
step:631/1845 train_time:22831ms step_avg:36.18ms
step:632/1845 train_time:22893ms step_avg:36.22ms
step:633/1845 train_time:22956ms step_avg:36.27ms
step:634/1845 train_time:23017ms step_avg:36.30ms
step:635/1845 train_time:23079ms step_avg:36.34ms
step:636/1845 train_time:23140ms step_avg:36.38ms
step:637/1845 train_time:23202ms step_avg:36.42ms
step:638/1845 train_time:23263ms step_avg:36.46ms
step:639/1845 train_time:23324ms step_avg:36.50ms
step:640/1845 train_time:23385ms step_avg:36.54ms
step:641/1845 train_time:23447ms step_avg:36.58ms
step:642/1845 train_time:23508ms step_avg:36.62ms
step:643/1845 train_time:23569ms step_avg:36.65ms
step:644/1845 train_time:23630ms step_avg:36.69ms
step:645/1845 train_time:23691ms step_avg:36.73ms
step:646/1845 train_time:23752ms step_avg:36.77ms
step:647/1845 train_time:23814ms step_avg:36.81ms
step:648/1845 train_time:23875ms step_avg:36.84ms
step:649/1845 train_time:23937ms step_avg:36.88ms
step:650/1845 train_time:23998ms step_avg:36.92ms
step:651/1845 train_time:24060ms step_avg:36.96ms
step:652/1845 train_time:24122ms step_avg:37.00ms
step:653/1845 train_time:24183ms step_avg:37.03ms
step:654/1845 train_time:24244ms step_avg:37.07ms
step:655/1845 train_time:24306ms step_avg:37.11ms
step:656/1845 train_time:24367ms step_avg:37.14ms
step:657/1845 train_time:24428ms step_avg:37.18ms
step:658/1845 train_time:24490ms step_avg:37.22ms
step:659/1845 train_time:24551ms step_avg:37.25ms
step:660/1845 train_time:24612ms step_avg:37.29ms
step:661/1845 train_time:24673ms step_avg:37.33ms
step:662/1845 train_time:24734ms step_avg:37.36ms
step:663/1845 train_time:24796ms step_avg:37.40ms
step:664/1845 train_time:24857ms step_avg:37.44ms
step:665/1845 train_time:24919ms step_avg:37.47ms
step:666/1845 train_time:24980ms step_avg:37.51ms
step:667/1845 train_time:25042ms step_avg:37.54ms
step:668/1845 train_time:25103ms step_avg:37.58ms
step:669/1845 train_time:25164ms step_avg:37.61ms
step:670/1845 train_time:25225ms step_avg:37.65ms
step:671/1845 train_time:25286ms step_avg:37.68ms
step:672/1845 train_time:25347ms step_avg:37.72ms
step:673/1845 train_time:25409ms step_avg:37.75ms
step:674/1845 train_time:25470ms step_avg:37.79ms
step:675/1845 train_time:25532ms step_avg:37.83ms
step:676/1845 train_time:25592ms step_avg:37.86ms
step:677/1845 train_time:25654ms step_avg:37.89ms
step:678/1845 train_time:25715ms step_avg:37.93ms
step:679/1845 train_time:25777ms step_avg:37.96ms
step:680/1845 train_time:25837ms step_avg:38.00ms
step:681/1845 train_time:25900ms step_avg:38.03ms
step:682/1845 train_time:25961ms step_avg:38.07ms
step:683/1845 train_time:26023ms step_avg:38.10ms
step:684/1845 train_time:26084ms step_avg:38.13ms
step:685/1845 train_time:26145ms step_avg:38.17ms
step:686/1845 train_time:26206ms step_avg:38.20ms
step:687/1845 train_time:26268ms step_avg:38.24ms
step:688/1845 train_time:26329ms step_avg:38.27ms
step:689/1845 train_time:26391ms step_avg:38.30ms
step:690/1845 train_time:26452ms step_avg:38.34ms
step:691/1845 train_time:26514ms step_avg:38.37ms
step:692/1845 train_time:26574ms step_avg:38.40ms
step:693/1845 train_time:26636ms step_avg:38.44ms
step:694/1845 train_time:26697ms step_avg:38.47ms
step:695/1845 train_time:26758ms step_avg:38.50ms
step:696/1845 train_time:26818ms step_avg:38.53ms
step:697/1845 train_time:26881ms step_avg:38.57ms
step:698/1845 train_time:26942ms step_avg:38.60ms
step:699/1845 train_time:27004ms step_avg:38.63ms
step:700/1845 train_time:27065ms step_avg:38.66ms
step:701/1845 train_time:27127ms step_avg:38.70ms
step:702/1845 train_time:27187ms step_avg:38.73ms
step:703/1845 train_time:27249ms step_avg:38.76ms
step:704/1845 train_time:27311ms step_avg:38.79ms
step:705/1845 train_time:27372ms step_avg:38.83ms
step:706/1845 train_time:27433ms step_avg:38.86ms
step:707/1845 train_time:27494ms step_avg:38.89ms
step:708/1845 train_time:27555ms step_avg:38.92ms
step:709/1845 train_time:27617ms step_avg:38.95ms
step:710/1845 train_time:27677ms step_avg:38.98ms
step:711/1845 train_time:27739ms step_avg:39.01ms
step:712/1845 train_time:27800ms step_avg:39.04ms
step:713/1845 train_time:27863ms step_avg:39.08ms
step:714/1845 train_time:27923ms step_avg:39.11ms
step:715/1845 train_time:27985ms step_avg:39.14ms
step:716/1845 train_time:28045ms step_avg:39.17ms
step:717/1845 train_time:28107ms step_avg:39.20ms
step:718/1845 train_time:28168ms step_avg:39.23ms
step:719/1845 train_time:28230ms step_avg:39.26ms
step:720/1845 train_time:28291ms step_avg:39.29ms
step:721/1845 train_time:28353ms step_avg:39.32ms
step:722/1845 train_time:28413ms step_avg:39.35ms
step:723/1845 train_time:28475ms step_avg:39.38ms
step:724/1845 train_time:28537ms step_avg:39.42ms
step:725/1845 train_time:28598ms step_avg:39.45ms
step:726/1845 train_time:28659ms step_avg:39.48ms
step:727/1845 train_time:28721ms step_avg:39.51ms
step:728/1845 train_time:28782ms step_avg:39.54ms
step:729/1845 train_time:28844ms step_avg:39.57ms
step:730/1845 train_time:28905ms step_avg:39.60ms
step:731/1845 train_time:28967ms step_avg:39.63ms
step:732/1845 train_time:29028ms step_avg:39.66ms
step:733/1845 train_time:29090ms step_avg:39.69ms
step:734/1845 train_time:29151ms step_avg:39.72ms
step:735/1845 train_time:29213ms step_avg:39.75ms
step:736/1845 train_time:29273ms step_avg:39.77ms
step:737/1845 train_time:29335ms step_avg:39.80ms
step:738/1845 train_time:29396ms step_avg:39.83ms
step:739/1845 train_time:29458ms step_avg:39.86ms
step:740/1845 train_time:29519ms step_avg:39.89ms
step:741/1845 train_time:29580ms step_avg:39.92ms
step:742/1845 train_time:29641ms step_avg:39.95ms
step:743/1845 train_time:29703ms step_avg:39.98ms
step:744/1845 train_time:29764ms step_avg:40.01ms
step:745/1845 train_time:29826ms step_avg:40.03ms
step:746/1845 train_time:29886ms step_avg:40.06ms
step:747/1845 train_time:29948ms step_avg:40.09ms
step:748/1845 train_time:30008ms step_avg:40.12ms
step:749/1845 train_time:30070ms step_avg:40.15ms
step:750/1845 train_time:30131ms step_avg:40.18ms
step:750/1845 val_loss:4.0226 train_time:30191ms step_avg:40.26ms
step:751/1845 train_time:30218ms step_avg:40.24ms
step:752/1845 train_time:30254ms step_avg:40.23ms
step:753/1845 train_time:30316ms step_avg:40.26ms
step:754/1845 train_time:30379ms step_avg:40.29ms
step:755/1845 train_time:30441ms step_avg:40.32ms
step:756/1845 train_time:30503ms step_avg:40.35ms
step:757/1845 train_time:30564ms step_avg:40.38ms
step:758/1845 train_time:30625ms step_avg:40.40ms
step:759/1845 train_time:30686ms step_avg:40.43ms
step:760/1845 train_time:30746ms step_avg:40.46ms
step:761/1845 train_time:30808ms step_avg:40.48ms
step:762/1845 train_time:30868ms step_avg:40.51ms
step:763/1845 train_time:30929ms step_avg:40.54ms
step:764/1845 train_time:30990ms step_avg:40.56ms
step:765/1845 train_time:31051ms step_avg:40.59ms
step:766/1845 train_time:31112ms step_avg:40.62ms
step:767/1845 train_time:31176ms step_avg:40.65ms
step:768/1845 train_time:31236ms step_avg:40.67ms
step:769/1845 train_time:31300ms step_avg:40.70ms
step:770/1845 train_time:31361ms step_avg:40.73ms
step:771/1845 train_time:31423ms step_avg:40.76ms
step:772/1845 train_time:31484ms step_avg:40.78ms
step:773/1845 train_time:31546ms step_avg:40.81ms
step:774/1845 train_time:31606ms step_avg:40.83ms
step:775/1845 train_time:31668ms step_avg:40.86ms
step:776/1845 train_time:31729ms step_avg:40.89ms
step:777/1845 train_time:31790ms step_avg:40.91ms
step:778/1845 train_time:31850ms step_avg:40.94ms
step:779/1845 train_time:31912ms step_avg:40.97ms
step:780/1845 train_time:31972ms step_avg:40.99ms
step:781/1845 train_time:32033ms step_avg:41.02ms
step:782/1845 train_time:32094ms step_avg:41.04ms
step:783/1845 train_time:32156ms step_avg:41.07ms
step:784/1845 train_time:32218ms step_avg:41.09ms
step:785/1845 train_time:32281ms step_avg:41.12ms
step:786/1845 train_time:32342ms step_avg:41.15ms
step:787/1845 train_time:32403ms step_avg:41.17ms
step:788/1845 train_time:32464ms step_avg:41.20ms
step:789/1845 train_time:32526ms step_avg:41.22ms
step:790/1845 train_time:32587ms step_avg:41.25ms
step:791/1845 train_time:32649ms step_avg:41.28ms
step:792/1845 train_time:32710ms step_avg:41.30ms
step:793/1845 train_time:32771ms step_avg:41.33ms
step:794/1845 train_time:32831ms step_avg:41.35ms
step:795/1845 train_time:32893ms step_avg:41.38ms
step:796/1845 train_time:32954ms step_avg:41.40ms
step:797/1845 train_time:33015ms step_avg:41.42ms
step:798/1845 train_time:33076ms step_avg:41.45ms
step:799/1845 train_time:33137ms step_avg:41.47ms
step:800/1845 train_time:33198ms step_avg:41.50ms
step:801/1845 train_time:33259ms step_avg:41.52ms
step:802/1845 train_time:33321ms step_avg:41.55ms
step:803/1845 train_time:33383ms step_avg:41.57ms
step:804/1845 train_time:33443ms step_avg:41.60ms
step:805/1845 train_time:33505ms step_avg:41.62ms
step:806/1845 train_time:33566ms step_avg:41.65ms
step:807/1845 train_time:33628ms step_avg:41.67ms
step:808/1845 train_time:33688ms step_avg:41.69ms
step:809/1845 train_time:33750ms step_avg:41.72ms
step:810/1845 train_time:33811ms step_avg:41.74ms
step:811/1845 train_time:33872ms step_avg:41.77ms
step:812/1845 train_time:33933ms step_avg:41.79ms
step:813/1845 train_time:33995ms step_avg:41.81ms
step:814/1845 train_time:34055ms step_avg:41.84ms
step:815/1845 train_time:34117ms step_avg:41.86ms
step:816/1845 train_time:34178ms step_avg:41.88ms
step:817/1845 train_time:34239ms step_avg:41.91ms
step:818/1845 train_time:34300ms step_avg:41.93ms
step:819/1845 train_time:34362ms step_avg:41.96ms
step:820/1845 train_time:34423ms step_avg:41.98ms
step:821/1845 train_time:34485ms step_avg:42.00ms
step:822/1845 train_time:34545ms step_avg:42.03ms
step:823/1845 train_time:34607ms step_avg:42.05ms
step:824/1845 train_time:34667ms step_avg:42.07ms
step:825/1845 train_time:34730ms step_avg:42.10ms
step:826/1845 train_time:34791ms step_avg:42.12ms
step:827/1845 train_time:34853ms step_avg:42.14ms
step:828/1845 train_time:34914ms step_avg:42.17ms
step:829/1845 train_time:34975ms step_avg:42.19ms
step:830/1845 train_time:35036ms step_avg:42.21ms
step:831/1845 train_time:35097ms step_avg:42.23ms
step:832/1845 train_time:35157ms step_avg:42.26ms
step:833/1845 train_time:35219ms step_avg:42.28ms
step:834/1845 train_time:35280ms step_avg:42.30ms
step:835/1845 train_time:35342ms step_avg:42.33ms
step:836/1845 train_time:35403ms step_avg:42.35ms
step:837/1845 train_time:35465ms step_avg:42.37ms
step:838/1845 train_time:35526ms step_avg:42.39ms
step:839/1845 train_time:35588ms step_avg:42.42ms
step:840/1845 train_time:35649ms step_avg:42.44ms
step:841/1845 train_time:35710ms step_avg:42.46ms
step:842/1845 train_time:35771ms step_avg:42.48ms
step:843/1845 train_time:35833ms step_avg:42.51ms
step:844/1845 train_time:35893ms step_avg:42.53ms
step:845/1845 train_time:35955ms step_avg:42.55ms
step:846/1845 train_time:36015ms step_avg:42.57ms
step:847/1845 train_time:36078ms step_avg:42.59ms
step:848/1845 train_time:36138ms step_avg:42.62ms
step:849/1845 train_time:36200ms step_avg:42.64ms
step:850/1845 train_time:36261ms step_avg:42.66ms
step:851/1845 train_time:36323ms step_avg:42.68ms
step:852/1845 train_time:36384ms step_avg:42.70ms
step:853/1845 train_time:36445ms step_avg:42.73ms
step:854/1845 train_time:36507ms step_avg:42.75ms
step:855/1845 train_time:36568ms step_avg:42.77ms
step:856/1845 train_time:36629ms step_avg:42.79ms
step:857/1845 train_time:36691ms step_avg:42.81ms
step:858/1845 train_time:36752ms step_avg:42.83ms
step:859/1845 train_time:36814ms step_avg:42.86ms
step:860/1845 train_time:36874ms step_avg:42.88ms
step:861/1845 train_time:36937ms step_avg:42.90ms
step:862/1845 train_time:36997ms step_avg:42.92ms
step:863/1845 train_time:37059ms step_avg:42.94ms
step:864/1845 train_time:37119ms step_avg:42.96ms
step:865/1845 train_time:37181ms step_avg:42.98ms
step:866/1845 train_time:37242ms step_avg:43.00ms
step:867/1845 train_time:37304ms step_avg:43.03ms
step:868/1845 train_time:37365ms step_avg:43.05ms
step:869/1845 train_time:37426ms step_avg:43.07ms
step:870/1845 train_time:37488ms step_avg:43.09ms
step:871/1845 train_time:37549ms step_avg:43.11ms
step:872/1845 train_time:37611ms step_avg:43.13ms
step:873/1845 train_time:37672ms step_avg:43.15ms
step:874/1845 train_time:37733ms step_avg:43.17ms
step:875/1845 train_time:37795ms step_avg:43.19ms
step:876/1845 train_time:37856ms step_avg:43.21ms
step:877/1845 train_time:37917ms step_avg:43.24ms
step:878/1845 train_time:37978ms step_avg:43.26ms
step:879/1845 train_time:38039ms step_avg:43.28ms
step:880/1845 train_time:38099ms step_avg:43.29ms
step:881/1845 train_time:38162ms step_avg:43.32ms
step:882/1845 train_time:38223ms step_avg:43.34ms
step:883/1845 train_time:38284ms step_avg:43.36ms
step:884/1845 train_time:38345ms step_avg:43.38ms
step:885/1845 train_time:38406ms step_avg:43.40ms
step:886/1845 train_time:38467ms step_avg:43.42ms
step:887/1845 train_time:38529ms step_avg:43.44ms
step:888/1845 train_time:38590ms step_avg:43.46ms
step:889/1845 train_time:38651ms step_avg:43.48ms
step:890/1845 train_time:38712ms step_avg:43.50ms
step:891/1845 train_time:38774ms step_avg:43.52ms
step:892/1845 train_time:38835ms step_avg:43.54ms
step:893/1845 train_time:38897ms step_avg:43.56ms
step:894/1845 train_time:38957ms step_avg:43.58ms
step:895/1845 train_time:39020ms step_avg:43.60ms
step:896/1845 train_time:39080ms step_avg:43.62ms
step:897/1845 train_time:39142ms step_avg:43.64ms
step:898/1845 train_time:39203ms step_avg:43.66ms
step:899/1845 train_time:39264ms step_avg:43.68ms
step:900/1845 train_time:39324ms step_avg:43.69ms
step:901/1845 train_time:39386ms step_avg:43.71ms
step:902/1845 train_time:39447ms step_avg:43.73ms
step:903/1845 train_time:39509ms step_avg:43.75ms
step:904/1845 train_time:39570ms step_avg:43.77ms
step:905/1845 train_time:39631ms step_avg:43.79ms
step:906/1845 train_time:39692ms step_avg:43.81ms
step:907/1845 train_time:39755ms step_avg:43.83ms
step:908/1845 train_time:39816ms step_avg:43.85ms
step:909/1845 train_time:39877ms step_avg:43.87ms
step:910/1845 train_time:39938ms step_avg:43.89ms
step:911/1845 train_time:40000ms step_avg:43.91ms
step:912/1845 train_time:40061ms step_avg:43.93ms
step:913/1845 train_time:40122ms step_avg:43.95ms
step:914/1845 train_time:40182ms step_avg:43.96ms
step:915/1845 train_time:40244ms step_avg:43.98ms
step:916/1845 train_time:40305ms step_avg:44.00ms
step:917/1845 train_time:40367ms step_avg:44.02ms
step:918/1845 train_time:40428ms step_avg:44.04ms
step:919/1845 train_time:40490ms step_avg:44.06ms
step:920/1845 train_time:40551ms step_avg:44.08ms
step:921/1845 train_time:40612ms step_avg:44.10ms
step:922/1845 train_time:40673ms step_avg:44.11ms
step:923/1845 train_time:40735ms step_avg:44.13ms
step:924/1845 train_time:40795ms step_avg:44.15ms
step:925/1845 train_time:40857ms step_avg:44.17ms
step:926/1845 train_time:40918ms step_avg:44.19ms
step:927/1845 train_time:40979ms step_avg:44.21ms
step:928/1845 train_time:41041ms step_avg:44.23ms
step:929/1845 train_time:41103ms step_avg:44.24ms
step:930/1845 train_time:41164ms step_avg:44.26ms
step:931/1845 train_time:41225ms step_avg:44.28ms
step:932/1845 train_time:41286ms step_avg:44.30ms
step:933/1845 train_time:41347ms step_avg:44.32ms
step:934/1845 train_time:41408ms step_avg:44.33ms
step:935/1845 train_time:41470ms step_avg:44.35ms
step:936/1845 train_time:41531ms step_avg:44.37ms
step:937/1845 train_time:41593ms step_avg:44.39ms
step:938/1845 train_time:41654ms step_avg:44.41ms
step:939/1845 train_time:41716ms step_avg:44.43ms
step:940/1845 train_time:41776ms step_avg:44.44ms
step:941/1845 train_time:41838ms step_avg:44.46ms
step:942/1845 train_time:41899ms step_avg:44.48ms
step:943/1845 train_time:41961ms step_avg:44.50ms
step:944/1845 train_time:42022ms step_avg:44.51ms
step:945/1845 train_time:42084ms step_avg:44.53ms
step:946/1845 train_time:42145ms step_avg:44.55ms
step:947/1845 train_time:42206ms step_avg:44.57ms
step:948/1845 train_time:42266ms step_avg:44.58ms
step:949/1845 train_time:42329ms step_avg:44.60ms
step:950/1845 train_time:42389ms step_avg:44.62ms
step:951/1845 train_time:42451ms step_avg:44.64ms
step:952/1845 train_time:42512ms step_avg:44.66ms
step:953/1845 train_time:42574ms step_avg:44.67ms
step:954/1845 train_time:42634ms step_avg:44.69ms
step:955/1845 train_time:42696ms step_avg:44.71ms
step:956/1845 train_time:42757ms step_avg:44.72ms
step:957/1845 train_time:42818ms step_avg:44.74ms
step:958/1845 train_time:42879ms step_avg:44.76ms
step:959/1845 train_time:42940ms step_avg:44.78ms
step:960/1845 train_time:43002ms step_avg:44.79ms
step:961/1845 train_time:43063ms step_avg:44.81ms
step:962/1845 train_time:43125ms step_avg:44.83ms
step:963/1845 train_time:43186ms step_avg:44.85ms
step:964/1845 train_time:43247ms step_avg:44.86ms
step:965/1845 train_time:43308ms step_avg:44.88ms
step:966/1845 train_time:43369ms step_avg:44.90ms
step:967/1845 train_time:43431ms step_avg:44.91ms
step:968/1845 train_time:43492ms step_avg:44.93ms
step:969/1845 train_time:43553ms step_avg:44.95ms
step:970/1845 train_time:43613ms step_avg:44.96ms
step:971/1845 train_time:43675ms step_avg:44.98ms
step:972/1845 train_time:43735ms step_avg:45.00ms
step:973/1845 train_time:43797ms step_avg:45.01ms
step:974/1845 train_time:43857ms step_avg:45.03ms
step:975/1845 train_time:43920ms step_avg:45.05ms
step:976/1845 train_time:43981ms step_avg:45.06ms
step:977/1845 train_time:44043ms step_avg:45.08ms
step:978/1845 train_time:44103ms step_avg:45.10ms
step:979/1845 train_time:44165ms step_avg:45.11ms
step:980/1845 train_time:44226ms step_avg:45.13ms
step:981/1845 train_time:44288ms step_avg:45.15ms
step:982/1845 train_time:44348ms step_avg:45.16ms
step:983/1845 train_time:44410ms step_avg:45.18ms
step:984/1845 train_time:44471ms step_avg:45.19ms
step:985/1845 train_time:44533ms step_avg:45.21ms
step:986/1845 train_time:44594ms step_avg:45.23ms
step:987/1845 train_time:44656ms step_avg:45.24ms
step:988/1845 train_time:44716ms step_avg:45.26ms
step:989/1845 train_time:44778ms step_avg:45.28ms
step:990/1845 train_time:44838ms step_avg:45.29ms
step:991/1845 train_time:44900ms step_avg:45.31ms
step:992/1845 train_time:44960ms step_avg:45.32ms
step:993/1845 train_time:45022ms step_avg:45.34ms
step:994/1845 train_time:45084ms step_avg:45.36ms
step:995/1845 train_time:45145ms step_avg:45.37ms
step:996/1845 train_time:45206ms step_avg:45.39ms
step:997/1845 train_time:45268ms step_avg:45.40ms
step:998/1845 train_time:45329ms step_avg:45.42ms
step:999/1845 train_time:45391ms step_avg:45.44ms
step:1000/1845 train_time:45452ms step_avg:45.45ms
step:1000/1845 val_loss:3.7818 train_time:45513ms step_avg:45.51ms
step:1001/1845 train_time:45539ms step_avg:45.49ms
step:1002/1845 train_time:45574ms step_avg:45.48ms
step:1003/1845 train_time:45639ms step_avg:45.50ms
step:1004/1845 train_time:45701ms step_avg:45.52ms
step:1005/1845 train_time:45763ms step_avg:45.54ms
step:1006/1845 train_time:45824ms step_avg:45.55ms
step:1007/1845 train_time:45885ms step_avg:45.57ms
step:1008/1845 train_time:45946ms step_avg:45.58ms
step:1009/1845 train_time:46007ms step_avg:45.60ms
step:1010/1845 train_time:46068ms step_avg:45.61ms
step:1011/1845 train_time:46131ms step_avg:45.63ms
step:1012/1845 train_time:46191ms step_avg:45.64ms
step:1013/1845 train_time:46252ms step_avg:45.66ms
step:1014/1845 train_time:46312ms step_avg:45.67ms
step:1015/1845 train_time:46374ms step_avg:45.69ms
step:1016/1845 train_time:46434ms step_avg:45.70ms
step:1017/1845 train_time:46498ms step_avg:45.72ms
step:1018/1845 train_time:46559ms step_avg:45.74ms
step:1019/1845 train_time:46622ms step_avg:45.75ms
step:1020/1845 train_time:46683ms step_avg:45.77ms
step:1021/1845 train_time:46745ms step_avg:45.78ms
step:1022/1845 train_time:46805ms step_avg:45.80ms
step:1023/1845 train_time:46867ms step_avg:45.81ms
step:1024/1845 train_time:46928ms step_avg:45.83ms
step:1025/1845 train_time:46989ms step_avg:45.84ms
step:1026/1845 train_time:47049ms step_avg:45.86ms
step:1027/1845 train_time:47112ms step_avg:45.87ms
step:1028/1845 train_time:47172ms step_avg:45.89ms
step:1029/1845 train_time:47234ms step_avg:45.90ms
step:1030/1845 train_time:47294ms step_avg:45.92ms
step:1031/1845 train_time:47355ms step_avg:45.93ms
step:1032/1845 train_time:47416ms step_avg:45.95ms
step:1033/1845 train_time:47478ms step_avg:45.96ms
step:1034/1845 train_time:47539ms step_avg:45.98ms
step:1035/1845 train_time:47603ms step_avg:45.99ms
step:1036/1845 train_time:47664ms step_avg:46.01ms
step:1037/1845 train_time:47726ms step_avg:46.02ms
step:1038/1845 train_time:47786ms step_avg:46.04ms
step:1039/1845 train_time:47848ms step_avg:46.05ms
step:1040/1845 train_time:47909ms step_avg:46.07ms
step:1041/1845 train_time:47970ms step_avg:46.08ms
step:1042/1845 train_time:48031ms step_avg:46.10ms
step:1043/1845 train_time:48092ms step_avg:46.11ms
step:1044/1845 train_time:48153ms step_avg:46.12ms
step:1045/1845 train_time:48215ms step_avg:46.14ms
step:1046/1845 train_time:48275ms step_avg:46.15ms
step:1047/1845 train_time:48337ms step_avg:46.17ms
step:1048/1845 train_time:48398ms step_avg:46.18ms
step:1049/1845 train_time:48460ms step_avg:46.20ms
step:1050/1845 train_time:48522ms step_avg:46.21ms
step:1051/1845 train_time:48583ms step_avg:46.23ms
step:1052/1845 train_time:48645ms step_avg:46.24ms
step:1053/1845 train_time:48706ms step_avg:46.25ms
step:1054/1845 train_time:48767ms step_avg:46.27ms
step:1055/1845 train_time:48829ms step_avg:46.28ms
step:1056/1845 train_time:48890ms step_avg:46.30ms
step:1057/1845 train_time:48951ms step_avg:46.31ms
step:1058/1845 train_time:49011ms step_avg:46.32ms
step:1059/1845 train_time:49073ms step_avg:46.34ms
step:1060/1845 train_time:49134ms step_avg:46.35ms
step:1061/1845 train_time:49195ms step_avg:46.37ms
step:1062/1845 train_time:49255ms step_avg:46.38ms
step:1063/1845 train_time:49317ms step_avg:46.39ms
step:1064/1845 train_time:49378ms step_avg:46.41ms
step:1065/1845 train_time:49439ms step_avg:46.42ms
step:1066/1845 train_time:49501ms step_avg:46.44ms
step:1067/1845 train_time:49563ms step_avg:46.45ms
step:1068/1845 train_time:49624ms step_avg:46.46ms
step:1069/1845 train_time:49686ms step_avg:46.48ms
step:1070/1845 train_time:49746ms step_avg:46.49ms
step:1071/1845 train_time:49808ms step_avg:46.51ms
step:1072/1845 train_time:49869ms step_avg:46.52ms
step:1073/1845 train_time:49931ms step_avg:46.53ms
step:1074/1845 train_time:49992ms step_avg:46.55ms
step:1075/1845 train_time:50053ms step_avg:46.56ms
step:1076/1845 train_time:50114ms step_avg:46.57ms
step:1077/1845 train_time:50175ms step_avg:46.59ms
step:1078/1845 train_time:50237ms step_avg:46.60ms
step:1079/1845 train_time:50298ms step_avg:46.62ms
step:1080/1845 train_time:50359ms step_avg:46.63ms
step:1081/1845 train_time:50420ms step_avg:46.64ms
step:1082/1845 train_time:50481ms step_avg:46.66ms
step:1083/1845 train_time:50543ms step_avg:46.67ms
step:1084/1845 train_time:50604ms step_avg:46.68ms
step:1085/1845 train_time:50666ms step_avg:46.70ms
step:1086/1845 train_time:50727ms step_avg:46.71ms
step:1087/1845 train_time:50790ms step_avg:46.72ms
step:1088/1845 train_time:50850ms step_avg:46.74ms
step:1089/1845 train_time:50912ms step_avg:46.75ms
step:1090/1845 train_time:50973ms step_avg:46.76ms
step:1091/1845 train_time:51035ms step_avg:46.78ms
step:1092/1845 train_time:51095ms step_avg:46.79ms
step:1093/1845 train_time:51157ms step_avg:46.80ms
step:1094/1845 train_time:51218ms step_avg:46.82ms
step:1095/1845 train_time:51279ms step_avg:46.83ms
step:1096/1845 train_time:51340ms step_avg:46.84ms
step:1097/1845 train_time:51402ms step_avg:46.86ms
step:1098/1845 train_time:51463ms step_avg:46.87ms
step:1099/1845 train_time:51525ms step_avg:46.88ms
step:1100/1845 train_time:51585ms step_avg:46.90ms
step:1101/1845 train_time:51646ms step_avg:46.91ms
step:1102/1845 train_time:51707ms step_avg:46.92ms
step:1103/1845 train_time:51769ms step_avg:46.93ms
step:1104/1845 train_time:51829ms step_avg:46.95ms
step:1105/1845 train_time:51891ms step_avg:46.96ms
step:1106/1845 train_time:51952ms step_avg:46.97ms
step:1107/1845 train_time:52014ms step_avg:46.99ms
step:1108/1845 train_time:52075ms step_avg:47.00ms
step:1109/1845 train_time:52137ms step_avg:47.01ms
step:1110/1845 train_time:52198ms step_avg:47.03ms
step:1111/1845 train_time:52259ms step_avg:47.04ms
step:1112/1845 train_time:52320ms step_avg:47.05ms
step:1113/1845 train_time:52381ms step_avg:47.06ms
step:1114/1845 train_time:52442ms step_avg:47.08ms
step:1115/1845 train_time:52504ms step_avg:47.09ms
step:1116/1845 train_time:52565ms step_avg:47.10ms
step:1117/1845 train_time:52627ms step_avg:47.11ms
step:1118/1845 train_time:52687ms step_avg:47.13ms
step:1119/1845 train_time:52749ms step_avg:47.14ms
step:1120/1845 train_time:52809ms step_avg:47.15ms
step:1121/1845 train_time:52871ms step_avg:47.16ms
step:1122/1845 train_time:52932ms step_avg:47.18ms
step:1123/1845 train_time:52993ms step_avg:47.19ms
step:1124/1845 train_time:53054ms step_avg:47.20ms
step:1125/1845 train_time:53116ms step_avg:47.21ms
step:1126/1845 train_time:53177ms step_avg:47.23ms
step:1127/1845 train_time:53238ms step_avg:47.24ms
step:1128/1845 train_time:53299ms step_avg:47.25ms
step:1129/1845 train_time:53361ms step_avg:47.26ms
step:1130/1845 train_time:53421ms step_avg:47.28ms
step:1131/1845 train_time:53482ms step_avg:47.29ms
step:1132/1845 train_time:53545ms step_avg:47.30ms
step:1133/1845 train_time:53606ms step_avg:47.31ms
step:1134/1845 train_time:53667ms step_avg:47.33ms
step:1135/1845 train_time:53729ms step_avg:47.34ms
step:1136/1845 train_time:53790ms step_avg:47.35ms
step:1137/1845 train_time:53851ms step_avg:47.36ms
step:1138/1845 train_time:53912ms step_avg:47.37ms
step:1139/1845 train_time:53973ms step_avg:47.39ms
step:1140/1845 train_time:54034ms step_avg:47.40ms
step:1141/1845 train_time:54096ms step_avg:47.41ms
step:1142/1845 train_time:54156ms step_avg:47.42ms
step:1143/1845 train_time:54218ms step_avg:47.43ms
step:1144/1845 train_time:54279ms step_avg:47.45ms
step:1145/1845 train_time:54341ms step_avg:47.46ms
step:1146/1845 train_time:54401ms step_avg:47.47ms
step:1147/1845 train_time:54464ms step_avg:47.48ms
step:1148/1845 train_time:54525ms step_avg:47.50ms
step:1149/1845 train_time:54586ms step_avg:47.51ms
step:1150/1845 train_time:54647ms step_avg:47.52ms
step:1151/1845 train_time:54709ms step_avg:47.53ms
step:1152/1845 train_time:54770ms step_avg:47.54ms
step:1153/1845 train_time:54831ms step_avg:47.56ms
step:1154/1845 train_time:54892ms step_avg:47.57ms
step:1155/1845 train_time:54953ms step_avg:47.58ms
step:1156/1845 train_time:55014ms step_avg:47.59ms
step:1157/1845 train_time:55076ms step_avg:47.60ms
step:1158/1845 train_time:55137ms step_avg:47.61ms
step:1159/1845 train_time:55198ms step_avg:47.63ms
step:1160/1845 train_time:55260ms step_avg:47.64ms
step:1161/1845 train_time:55321ms step_avg:47.65ms
step:1162/1845 train_time:55382ms step_avg:47.66ms
step:1163/1845 train_time:55445ms step_avg:47.67ms
step:1164/1845 train_time:55505ms step_avg:47.68ms
step:1165/1845 train_time:55567ms step_avg:47.70ms
step:1166/1845 train_time:55627ms step_avg:47.71ms
step:1167/1845 train_time:55690ms step_avg:47.72ms
step:1168/1845 train_time:55750ms step_avg:47.73ms
step:1169/1845 train_time:55812ms step_avg:47.74ms
step:1170/1845 train_time:55873ms step_avg:47.75ms
step:1171/1845 train_time:55934ms step_avg:47.77ms
step:1172/1845 train_time:55995ms step_avg:47.78ms
step:1173/1845 train_time:56056ms step_avg:47.79ms
step:1174/1845 train_time:56118ms step_avg:47.80ms
step:1175/1845 train_time:56179ms step_avg:47.81ms
step:1176/1845 train_time:56240ms step_avg:47.82ms
step:1177/1845 train_time:56301ms step_avg:47.83ms
step:1178/1845 train_time:56363ms step_avg:47.85ms
step:1179/1845 train_time:56424ms step_avg:47.86ms
step:1180/1845 train_time:56485ms step_avg:47.87ms
step:1181/1845 train_time:56547ms step_avg:47.88ms
step:1182/1845 train_time:56608ms step_avg:47.89ms
step:1183/1845 train_time:56670ms step_avg:47.90ms
step:1184/1845 train_time:56730ms step_avg:47.91ms
step:1185/1845 train_time:56792ms step_avg:47.93ms
step:1186/1845 train_time:56853ms step_avg:47.94ms
step:1187/1845 train_time:56914ms step_avg:47.95ms
step:1188/1845 train_time:56975ms step_avg:47.96ms
step:1189/1845 train_time:57037ms step_avg:47.97ms
step:1190/1845 train_time:57097ms step_avg:47.98ms
step:1191/1845 train_time:57159ms step_avg:47.99ms
step:1192/1845 train_time:57219ms step_avg:48.00ms
step:1193/1845 train_time:57282ms step_avg:48.01ms
step:1194/1845 train_time:57344ms step_avg:48.03ms
step:1195/1845 train_time:57405ms step_avg:48.04ms
step:1196/1845 train_time:57467ms step_avg:48.05ms
step:1197/1845 train_time:57528ms step_avg:48.06ms
step:1198/1845 train_time:57589ms step_avg:48.07ms
step:1199/1845 train_time:57651ms step_avg:48.08ms
step:1200/1845 train_time:57711ms step_avg:48.09ms
step:1201/1845 train_time:57773ms step_avg:48.10ms
step:1202/1845 train_time:57834ms step_avg:48.11ms
step:1203/1845 train_time:57896ms step_avg:48.13ms
step:1204/1845 train_time:57957ms step_avg:48.14ms
step:1205/1845 train_time:58019ms step_avg:48.15ms
step:1206/1845 train_time:58106ms step_avg:48.18ms
step:1207/1845 train_time:58193ms step_avg:48.21ms
step:1208/1845 train_time:58281ms step_avg:48.25ms
step:1209/1845 train_time:58370ms step_avg:48.28ms
step:1210/1845 train_time:58458ms step_avg:48.31ms
step:1211/1845 train_time:58546ms step_avg:48.35ms
step:1212/1845 train_time:58633ms step_avg:48.38ms
step:1213/1845 train_time:58722ms step_avg:48.41ms
step:1214/1845 train_time:58809ms step_avg:48.44ms
step:1215/1845 train_time:58896ms step_avg:48.47ms
step:1216/1845 train_time:58983ms step_avg:48.51ms
step:1217/1845 train_time:59072ms step_avg:48.54ms
step:1218/1845 train_time:59159ms step_avg:48.57ms
step:1219/1845 train_time:59247ms step_avg:48.60ms
step:1220/1845 train_time:59335ms step_avg:48.64ms
step:1221/1845 train_time:59422ms step_avg:48.67ms
step:1222/1845 train_time:59510ms step_avg:48.70ms
step:1223/1845 train_time:59598ms step_avg:48.73ms
step:1224/1845 train_time:59685ms step_avg:48.76ms
step:1225/1845 train_time:59773ms step_avg:48.79ms
step:1226/1845 train_time:59859ms step_avg:48.82ms
step:1227/1845 train_time:59948ms step_avg:48.86ms
step:1228/1845 train_time:60035ms step_avg:48.89ms
step:1229/1845 train_time:60124ms step_avg:48.92ms
step:1230/1845 train_time:60211ms step_avg:48.95ms
step:1231/1845 train_time:60300ms step_avg:48.98ms
step:1232/1845 train_time:60388ms step_avg:49.02ms
step:1233/1845 train_time:60476ms step_avg:49.05ms
step:1234/1845 train_time:60565ms step_avg:49.08ms
step:1235/1845 train_time:60652ms step_avg:49.11ms
step:1236/1845 train_time:60738ms step_avg:49.14ms
step:1237/1845 train_time:60827ms step_avg:49.17ms
step:1238/1845 train_time:60914ms step_avg:49.20ms
step:1239/1845 train_time:61003ms step_avg:49.24ms
step:1240/1845 train_time:61090ms step_avg:49.27ms
step:1241/1845 train_time:61177ms step_avg:49.30ms
step:1242/1845 train_time:61265ms step_avg:49.33ms
step:1243/1845 train_time:61353ms step_avg:49.36ms
step:1244/1845 train_time:61440ms step_avg:49.39ms
step:1245/1845 train_time:61527ms step_avg:49.42ms
step:1246/1845 train_time:61615ms step_avg:49.45ms
step:1247/1845 train_time:61703ms step_avg:49.48ms
step:1248/1845 train_time:61791ms step_avg:49.51ms
step:1249/1845 train_time:61879ms step_avg:49.54ms
step:1250/1845 train_time:61965ms step_avg:49.57ms
step:1250/1845 val_loss:3.5345 train_time:62053ms step_avg:49.64ms
step:1251/1845 train_time:62080ms step_avg:49.62ms
step:1252/1845 train_time:62140ms step_avg:49.63ms
step:1253/1845 train_time:62229ms step_avg:49.66ms
step:1254/1845 train_time:62322ms step_avg:49.70ms
step:1255/1845 train_time:62414ms step_avg:49.73ms
step:1256/1845 train_time:62500ms step_avg:49.76ms
step:1257/1845 train_time:62588ms step_avg:49.79ms
step:1258/1845 train_time:62674ms step_avg:49.82ms
step:1259/1845 train_time:62761ms step_avg:49.85ms
step:1260/1845 train_time:62847ms step_avg:49.88ms
step:1261/1845 train_time:62935ms step_avg:49.91ms
step:1262/1845 train_time:63027ms step_avg:49.94ms
step:1263/1845 train_time:63116ms step_avg:49.97ms
step:1264/1845 train_time:63203ms step_avg:50.00ms
step:1265/1845 train_time:63293ms step_avg:50.03ms
step:1266/1845 train_time:63382ms step_avg:50.06ms
step:1267/1845 train_time:63471ms step_avg:50.10ms
step:1268/1845 train_time:63558ms step_avg:50.12ms
step:1269/1845 train_time:63646ms step_avg:50.15ms
step:1270/1845 train_time:63732ms step_avg:50.18ms
step:1271/1845 train_time:63819ms step_avg:50.21ms
step:1272/1845 train_time:63906ms step_avg:50.24ms
step:1273/1845 train_time:63995ms step_avg:50.27ms
step:1274/1845 train_time:64084ms step_avg:50.30ms
step:1275/1845 train_time:64172ms step_avg:50.33ms
step:1276/1845 train_time:64260ms step_avg:50.36ms
step:1277/1845 train_time:64349ms step_avg:50.39ms
step:1278/1845 train_time:64436ms step_avg:50.42ms
step:1279/1845 train_time:64526ms step_avg:50.45ms
step:1280/1845 train_time:64613ms step_avg:50.48ms
step:1281/1845 train_time:64701ms step_avg:50.51ms
step:1282/1845 train_time:64788ms step_avg:50.54ms
step:1283/1845 train_time:64875ms step_avg:50.56ms
step:1284/1845 train_time:64963ms step_avg:50.59ms
step:1285/1845 train_time:65051ms step_avg:50.62ms
step:1286/1845 train_time:65139ms step_avg:50.65ms
step:1287/1845 train_time:65227ms step_avg:50.68ms
step:1288/1845 train_time:65315ms step_avg:50.71ms
step:1289/1845 train_time:65404ms step_avg:50.74ms
step:1290/1845 train_time:65492ms step_avg:50.77ms
step:1291/1845 train_time:65579ms step_avg:50.80ms
step:1292/1845 train_time:65666ms step_avg:50.82ms
step:1293/1845 train_time:65753ms step_avg:50.85ms
step:1294/1845 train_time:65840ms step_avg:50.88ms
step:1295/1845 train_time:65928ms step_avg:50.91ms
step:1296/1845 train_time:66016ms step_avg:50.94ms
step:1297/1845 train_time:66103ms step_avg:50.97ms
step:1298/1845 train_time:66192ms step_avg:51.00ms
step:1299/1845 train_time:66279ms step_avg:51.02ms
step:1300/1845 train_time:66366ms step_avg:51.05ms
step:1301/1845 train_time:66454ms step_avg:51.08ms
step:1302/1845 train_time:66541ms step_avg:51.11ms
step:1303/1845 train_time:66630ms step_avg:51.14ms
step:1304/1845 train_time:66717ms step_avg:51.16ms
step:1305/1845 train_time:66805ms step_avg:51.19ms
step:1306/1845 train_time:66892ms step_avg:51.22ms
step:1307/1845 train_time:66980ms step_avg:51.25ms
step:1308/1845 train_time:67066ms step_avg:51.27ms
step:1309/1845 train_time:67156ms step_avg:51.30ms
step:1310/1845 train_time:67243ms step_avg:51.33ms
step:1311/1845 train_time:67332ms step_avg:51.36ms
step:1312/1845 train_time:67419ms step_avg:51.39ms
step:1313/1845 train_time:67507ms step_avg:51.41ms
step:1314/1845 train_time:67594ms step_avg:51.44ms
step:1315/1845 train_time:67681ms step_avg:51.47ms
step:1316/1845 train_time:67768ms step_avg:51.50ms
step:1317/1845 train_time:67855ms step_avg:51.52ms
step:1318/1845 train_time:67942ms step_avg:51.55ms
step:1319/1845 train_time:68031ms step_avg:51.58ms
step:1320/1845 train_time:68119ms step_avg:51.61ms
step:1321/1845 train_time:68206ms step_avg:51.63ms
step:1322/1845 train_time:68295ms step_avg:51.66ms
step:1323/1845 train_time:68382ms step_avg:51.69ms
step:1324/1845 train_time:68470ms step_avg:51.71ms
step:1325/1845 train_time:68558ms step_avg:51.74ms
step:1326/1845 train_time:68645ms step_avg:51.77ms
step:1327/1845 train_time:68733ms step_avg:51.80ms
step:1328/1845 train_time:68821ms step_avg:51.82ms
step:1329/1845 train_time:68909ms step_avg:51.85ms
step:1330/1845 train_time:68996ms step_avg:51.88ms
step:1331/1845 train_time:69085ms step_avg:51.90ms
step:1332/1845 train_time:69172ms step_avg:51.93ms
step:1333/1845 train_time:69260ms step_avg:51.96ms
step:1334/1845 train_time:69347ms step_avg:51.98ms
step:1335/1845 train_time:69434ms step_avg:52.01ms
step:1336/1845 train_time:69522ms step_avg:52.04ms
step:1337/1845 train_time:69610ms step_avg:52.06ms
step:1338/1845 train_time:69697ms step_avg:52.09ms
step:1339/1845 train_time:69785ms step_avg:52.12ms
step:1340/1845 train_time:69874ms step_avg:52.14ms
step:1341/1845 train_time:69961ms step_avg:52.17ms
step:1342/1845 train_time:70048ms step_avg:52.20ms
step:1343/1845 train_time:70135ms step_avg:52.22ms
step:1344/1845 train_time:70222ms step_avg:52.25ms
step:1345/1845 train_time:70312ms step_avg:52.28ms
step:1346/1845 train_time:70400ms step_avg:52.30ms
step:1347/1845 train_time:70488ms step_avg:52.33ms
step:1348/1845 train_time:70575ms step_avg:52.36ms
step:1349/1845 train_time:70663ms step_avg:52.38ms
step:1350/1845 train_time:70750ms step_avg:52.41ms
step:1351/1845 train_time:70838ms step_avg:52.43ms
step:1352/1845 train_time:70925ms step_avg:52.46ms
step:1353/1845 train_time:71013ms step_avg:52.49ms
step:1354/1845 train_time:71102ms step_avg:52.51ms
step:1355/1845 train_time:71190ms step_avg:52.54ms
step:1356/1845 train_time:71278ms step_avg:52.56ms
step:1357/1845 train_time:71365ms step_avg:52.59ms
step:1358/1845 train_time:71452ms step_avg:52.62ms
step:1359/1845 train_time:71540ms step_avg:52.64ms
step:1360/1845 train_time:71626ms step_avg:52.67ms
step:1361/1845 train_time:71715ms step_avg:52.69ms
step:1362/1845 train_time:71803ms step_avg:52.72ms
step:1363/1845 train_time:71890ms step_avg:52.74ms
step:1364/1845 train_time:71978ms step_avg:52.77ms
step:1365/1845 train_time:72067ms step_avg:52.80ms
step:1366/1845 train_time:72154ms step_avg:52.82ms
step:1367/1845 train_time:72242ms step_avg:52.85ms
step:1368/1845 train_time:72330ms step_avg:52.87ms
step:1369/1845 train_time:72417ms step_avg:52.90ms
step:1370/1845 train_time:72504ms step_avg:52.92ms
step:1371/1845 train_time:72592ms step_avg:52.95ms
step:1372/1845 train_time:72680ms step_avg:52.97ms
step:1373/1845 train_time:72767ms step_avg:53.00ms
step:1374/1845 train_time:72855ms step_avg:53.02ms
step:1375/1845 train_time:72944ms step_avg:53.05ms
step:1376/1845 train_time:73031ms step_avg:53.07ms
step:1377/1845 train_time:73120ms step_avg:53.10ms
step:1378/1845 train_time:73207ms step_avg:53.13ms
step:1379/1845 train_time:73295ms step_avg:53.15ms
step:1380/1845 train_time:73383ms step_avg:53.18ms
step:1381/1845 train_time:73470ms step_avg:53.20ms
step:1382/1845 train_time:73557ms step_avg:53.23ms
step:1383/1845 train_time:73644ms step_avg:53.25ms
step:1384/1845 train_time:73731ms step_avg:53.27ms
step:1385/1845 train_time:73820ms step_avg:53.30ms
step:1386/1845 train_time:73907ms step_avg:53.32ms
step:1387/1845 train_time:73995ms step_avg:53.35ms
step:1388/1845 train_time:74083ms step_avg:53.37ms
step:1389/1845 train_time:74170ms step_avg:53.40ms
step:1390/1845 train_time:74258ms step_avg:53.42ms
step:1391/1845 train_time:74345ms step_avg:53.45ms
step:1392/1845 train_time:74433ms step_avg:53.47ms
step:1393/1845 train_time:74520ms step_avg:53.50ms
step:1394/1845 train_time:74607ms step_avg:53.52ms
step:1395/1845 train_time:74695ms step_avg:53.54ms
step:1396/1845 train_time:74782ms step_avg:53.57ms
step:1397/1845 train_time:74870ms step_avg:53.59ms
step:1398/1845 train_time:74957ms step_avg:53.62ms
step:1399/1845 train_time:75046ms step_avg:53.64ms
step:1400/1845 train_time:75134ms step_avg:53.67ms
step:1401/1845 train_time:75222ms step_avg:53.69ms
step:1402/1845 train_time:75311ms step_avg:53.72ms
step:1403/1845 train_time:75397ms step_avg:53.74ms
step:1404/1845 train_time:75485ms step_avg:53.76ms
step:1405/1845 train_time:75572ms step_avg:53.79ms
step:1406/1845 train_time:75659ms step_avg:53.81ms
step:1407/1845 train_time:75748ms step_avg:53.84ms
step:1408/1845 train_time:75836ms step_avg:53.86ms
step:1409/1845 train_time:75925ms step_avg:53.89ms
step:1410/1845 train_time:76012ms step_avg:53.91ms
step:1411/1845 train_time:76100ms step_avg:53.93ms
step:1412/1845 train_time:76187ms step_avg:53.96ms
step:1413/1845 train_time:76274ms step_avg:53.98ms
step:1414/1845 train_time:76362ms step_avg:54.00ms
step:1415/1845 train_time:76450ms step_avg:54.03ms
step:1416/1845 train_time:76538ms step_avg:54.05ms
step:1417/1845 train_time:76625ms step_avg:54.08ms
step:1418/1845 train_time:76713ms step_avg:54.10ms
step:1419/1845 train_time:76800ms step_avg:54.12ms
step:1420/1845 train_time:76887ms step_avg:54.15ms
step:1421/1845 train_time:76976ms step_avg:54.17ms
step:1422/1845 train_time:77063ms step_avg:54.19ms
step:1423/1845 train_time:77151ms step_avg:54.22ms
step:1424/1845 train_time:77240ms step_avg:54.24ms
step:1425/1845 train_time:77327ms step_avg:54.26ms
step:1426/1845 train_time:77415ms step_avg:54.29ms
step:1427/1845 train_time:77503ms step_avg:54.31ms
step:1428/1845 train_time:77590ms step_avg:54.34ms
step:1429/1845 train_time:77677ms step_avg:54.36ms
step:1430/1845 train_time:77765ms step_avg:54.38ms
step:1431/1845 train_time:77854ms step_avg:54.41ms
step:1432/1845 train_time:77942ms step_avg:54.43ms
step:1433/1845 train_time:78028ms step_avg:54.45ms
step:1434/1845 train_time:78116ms step_avg:54.47ms
step:1435/1845 train_time:78204ms step_avg:54.50ms
step:1436/1845 train_time:78292ms step_avg:54.52ms
step:1437/1845 train_time:78379ms step_avg:54.54ms
step:1438/1845 train_time:78466ms step_avg:54.57ms
step:1439/1845 train_time:78554ms step_avg:54.59ms
step:1440/1845 train_time:78642ms step_avg:54.61ms
step:1441/1845 train_time:78730ms step_avg:54.64ms
step:1442/1845 train_time:78817ms step_avg:54.66ms
step:1443/1845 train_time:78906ms step_avg:54.68ms
step:1444/1845 train_time:78994ms step_avg:54.70ms
step:1445/1845 train_time:79081ms step_avg:54.73ms
step:1446/1845 train_time:79168ms step_avg:54.75ms
step:1447/1845 train_time:79257ms step_avg:54.77ms
step:1448/1845 train_time:79344ms step_avg:54.80ms
step:1449/1845 train_time:79431ms step_avg:54.82ms
step:1450/1845 train_time:79519ms step_avg:54.84ms
step:1451/1845 train_time:79607ms step_avg:54.86ms
step:1452/1845 train_time:79694ms step_avg:54.89ms
step:1453/1845 train_time:79782ms step_avg:54.91ms
step:1454/1845 train_time:79869ms step_avg:54.93ms
step:1455/1845 train_time:79956ms step_avg:54.95ms
step:1456/1845 train_time:80044ms step_avg:54.98ms
step:1457/1845 train_time:80131ms step_avg:55.00ms
step:1458/1845 train_time:80220ms step_avg:55.02ms
step:1459/1845 train_time:80307ms step_avg:55.04ms
step:1460/1845 train_time:80396ms step_avg:55.07ms
step:1461/1845 train_time:80484ms step_avg:55.09ms
step:1462/1845 train_time:80571ms step_avg:55.11ms
step:1463/1845 train_time:80658ms step_avg:55.13ms
step:1464/1845 train_time:80745ms step_avg:55.15ms
step:1465/1845 train_time:80835ms step_avg:55.18ms
step:1466/1845 train_time:80922ms step_avg:55.20ms
step:1467/1845 train_time:81009ms step_avg:55.22ms
step:1468/1845 train_time:81096ms step_avg:55.24ms
step:1469/1845 train_time:81184ms step_avg:55.27ms
step:1470/1845 train_time:81272ms step_avg:55.29ms
step:1471/1845 train_time:81360ms step_avg:55.31ms
step:1472/1845 train_time:81446ms step_avg:55.33ms
step:1473/1845 train_time:81535ms step_avg:55.35ms
step:1474/1845 train_time:81623ms step_avg:55.38ms
step:1475/1845 train_time:81710ms step_avg:55.40ms
step:1476/1845 train_time:81799ms step_avg:55.42ms
step:1477/1845 train_time:81886ms step_avg:55.44ms
step:1478/1845 train_time:81974ms step_avg:55.46ms
step:1479/1845 train_time:82061ms step_avg:55.48ms
step:1480/1845 train_time:82147ms step_avg:55.50ms
step:1481/1845 train_time:82236ms step_avg:55.53ms
step:1482/1845 train_time:82323ms step_avg:55.55ms
step:1483/1845 train_time:82412ms step_avg:55.57ms
step:1484/1845 train_time:82499ms step_avg:55.59ms
step:1485/1845 train_time:82587ms step_avg:55.61ms
step:1486/1845 train_time:82674ms step_avg:55.64ms
step:1487/1845 train_time:82763ms step_avg:55.66ms
step:1488/1845 train_time:82850ms step_avg:55.68ms
step:1489/1845 train_time:82938ms step_avg:55.70ms
step:1490/1845 train_time:83025ms step_avg:55.72ms
step:1491/1845 train_time:83113ms step_avg:55.74ms
step:1492/1845 train_time:83201ms step_avg:55.76ms
step:1493/1845 train_time:83288ms step_avg:55.79ms
step:1494/1845 train_time:83376ms step_avg:55.81ms
step:1495/1845 train_time:83465ms step_avg:55.83ms
step:1496/1845 train_time:83552ms step_avg:55.85ms
step:1497/1845 train_time:83640ms step_avg:55.87ms
step:1498/1845 train_time:83727ms step_avg:55.89ms
step:1499/1845 train_time:83815ms step_avg:55.91ms
step:1500/1845 train_time:83902ms step_avg:55.93ms
step:1500/1845 val_loss:3.4033 train_time:83989ms step_avg:55.99ms
step:1501/1845 train_time:84018ms step_avg:55.97ms
step:1502/1845 train_time:84079ms step_avg:55.98ms
step:1503/1845 train_time:84171ms step_avg:56.00ms
step:1504/1845 train_time:84262ms step_avg:56.02ms
step:1505/1845 train_time:84349ms step_avg:56.05ms
step:1506/1845 train_time:84435ms step_avg:56.07ms
step:1507/1845 train_time:84522ms step_avg:56.09ms
step:1508/1845 train_time:84609ms step_avg:56.11ms
step:1509/1845 train_time:84695ms step_avg:56.13ms
step:1510/1845 train_time:84783ms step_avg:56.15ms
step:1511/1845 train_time:84870ms step_avg:56.17ms
step:1512/1845 train_time:84959ms step_avg:56.19ms
step:1513/1845 train_time:85049ms step_avg:56.21ms
step:1514/1845 train_time:85137ms step_avg:56.23ms
step:1515/1845 train_time:85226ms step_avg:56.25ms
step:1516/1845 train_time:85312ms step_avg:56.27ms
step:1517/1845 train_time:85400ms step_avg:56.30ms
step:1518/1845 train_time:85488ms step_avg:56.32ms
step:1519/1845 train_time:85574ms step_avg:56.34ms
step:1520/1845 train_time:85661ms step_avg:56.36ms
step:1521/1845 train_time:85748ms step_avg:56.38ms
step:1522/1845 train_time:85835ms step_avg:56.40ms
step:1523/1845 train_time:85924ms step_avg:56.42ms
step:1524/1845 train_time:86012ms step_avg:56.44ms
step:1525/1845 train_time:86103ms step_avg:56.46ms
step:1526/1845 train_time:86190ms step_avg:56.48ms
step:1527/1845 train_time:86278ms step_avg:56.50ms
step:1528/1845 train_time:86366ms step_avg:56.52ms
step:1529/1845 train_time:86453ms step_avg:56.54ms
step:1530/1845 train_time:86541ms step_avg:56.56ms
step:1531/1845 train_time:86628ms step_avg:56.58ms
step:1532/1845 train_time:86714ms step_avg:56.60ms
step:1533/1845 train_time:86801ms step_avg:56.62ms
step:1534/1845 train_time:86889ms step_avg:56.64ms
step:1535/1845 train_time:86977ms step_avg:56.66ms
step:1536/1845 train_time:87065ms step_avg:56.68ms
step:1537/1845 train_time:87154ms step_avg:56.70ms
step:1538/1845 train_time:87242ms step_avg:56.72ms
step:1539/1845 train_time:87330ms step_avg:56.74ms
step:1540/1845 train_time:87417ms step_avg:56.76ms
step:1541/1845 train_time:87504ms step_avg:56.78ms
step:1542/1845 train_time:87591ms step_avg:56.80ms
step:1543/1845 train_time:87678ms step_avg:56.82ms
step:1544/1845 train_time:87766ms step_avg:56.84ms
step:1545/1845 train_time:87854ms step_avg:56.86ms
step:1546/1845 train_time:87941ms step_avg:56.88ms
step:1547/1845 train_time:88030ms step_avg:56.90ms
step:1548/1845 train_time:88117ms step_avg:56.92ms
step:1549/1845 train_time:88206ms step_avg:56.94ms
step:1550/1845 train_time:88293ms step_avg:56.96ms
step:1551/1845 train_time:88381ms step_avg:56.98ms
step:1552/1845 train_time:88468ms step_avg:57.00ms
step:1553/1845 train_time:88555ms step_avg:57.02ms
step:1554/1845 train_time:88643ms step_avg:57.04ms
step:1555/1845 train_time:88730ms step_avg:57.06ms
step:1556/1845 train_time:88819ms step_avg:57.08ms
step:1557/1845 train_time:88907ms step_avg:57.10ms
step:1558/1845 train_time:88994ms step_avg:57.12ms
step:1559/1845 train_time:89083ms step_avg:57.14ms
step:1560/1845 train_time:89170ms step_avg:57.16ms
step:1561/1845 train_time:89259ms step_avg:57.18ms
step:1562/1845 train_time:89347ms step_avg:57.20ms
step:1563/1845 train_time:89434ms step_avg:57.22ms
step:1564/1845 train_time:89522ms step_avg:57.24ms
step:1565/1845 train_time:89609ms step_avg:57.26ms
step:1566/1845 train_time:89696ms step_avg:57.28ms
step:1567/1845 train_time:89783ms step_avg:57.30ms
step:1568/1845 train_time:89871ms step_avg:57.32ms
step:1569/1845 train_time:89958ms step_avg:57.33ms
step:1570/1845 train_time:90046ms step_avg:57.35ms
step:1571/1845 train_time:90134ms step_avg:57.37ms
step:1572/1845 train_time:90222ms step_avg:57.39ms
step:1573/1845 train_time:90310ms step_avg:57.41ms
step:1574/1845 train_time:90397ms step_avg:57.43ms
step:1575/1845 train_time:90485ms step_avg:57.45ms
step:1576/1845 train_time:90572ms step_avg:57.47ms
step:1577/1845 train_time:90659ms step_avg:57.49ms
step:1578/1845 train_time:90748ms step_avg:57.51ms
step:1579/1845 train_time:90833ms step_avg:57.53ms
step:1580/1845 train_time:90921ms step_avg:57.54ms
step:1581/1845 train_time:91010ms step_avg:57.56ms
step:1582/1845 train_time:91098ms step_avg:57.58ms
step:1583/1845 train_time:91185ms step_avg:57.60ms
step:1584/1845 train_time:91272ms step_avg:57.62ms
step:1585/1845 train_time:91360ms step_avg:57.64ms
step:1586/1845 train_time:91448ms step_avg:57.66ms
step:1587/1845 train_time:91536ms step_avg:57.68ms
step:1588/1845 train_time:91623ms step_avg:57.70ms
step:1589/1845 train_time:91711ms step_avg:57.72ms
step:1590/1845 train_time:91799ms step_avg:57.74ms
step:1591/1845 train_time:91886ms step_avg:57.75ms
step:1592/1845 train_time:91973ms step_avg:57.77ms
step:1593/1845 train_time:92061ms step_avg:57.79ms
step:1594/1845 train_time:92150ms step_avg:57.81ms
step:1595/1845 train_time:92237ms step_avg:57.83ms
step:1596/1845 train_time:92326ms step_avg:57.85ms
step:1597/1845 train_time:92414ms step_avg:57.87ms
step:1598/1845 train_time:92502ms step_avg:57.89ms
step:1599/1845 train_time:92589ms step_avg:57.90ms
step:1600/1845 train_time:92676ms step_avg:57.92ms
step:1601/1845 train_time:92764ms step_avg:57.94ms
step:1602/1845 train_time:92852ms step_avg:57.96ms
step:1603/1845 train_time:92939ms step_avg:57.98ms
step:1604/1845 train_time:93027ms step_avg:58.00ms
step:1605/1845 train_time:93114ms step_avg:58.02ms
step:1606/1845 train_time:93202ms step_avg:58.03ms
step:1607/1845 train_time:93290ms step_avg:58.05ms
step:1608/1845 train_time:93378ms step_avg:58.07ms
step:1609/1845 train_time:93466ms step_avg:58.09ms
step:1610/1845 train_time:93552ms step_avg:58.11ms
step:1611/1845 train_time:93640ms step_avg:58.13ms
step:1612/1845 train_time:93728ms step_avg:58.14ms
step:1613/1845 train_time:93815ms step_avg:58.16ms
step:1614/1845 train_time:93903ms step_avg:58.18ms
step:1615/1845 train_time:93991ms step_avg:58.20ms
step:1616/1845 train_time:94079ms step_avg:58.22ms
step:1617/1845 train_time:94167ms step_avg:58.24ms
step:1618/1845 train_time:94254ms step_avg:58.25ms
step:1619/1845 train_time:94342ms step_avg:58.27ms
step:1620/1845 train_time:94430ms step_avg:58.29ms
step:1621/1845 train_time:94517ms step_avg:58.31ms
step:1622/1845 train_time:94605ms step_avg:58.33ms
step:1623/1845 train_time:94693ms step_avg:58.34ms
step:1624/1845 train_time:94780ms step_avg:58.36ms
step:1625/1845 train_time:94868ms step_avg:58.38ms
step:1626/1845 train_time:94954ms step_avg:58.40ms
step:1627/1845 train_time:95043ms step_avg:58.42ms
step:1628/1845 train_time:95130ms step_avg:58.43ms
step:1629/1845 train_time:95218ms step_avg:58.45ms
step:1630/1845 train_time:95307ms step_avg:58.47ms
step:1631/1845 train_time:95394ms step_avg:58.49ms
step:1632/1845 train_time:95481ms step_avg:58.51ms
step:1633/1845 train_time:95569ms step_avg:58.52ms
step:1634/1845 train_time:95657ms step_avg:58.54ms
step:1635/1845 train_time:95744ms step_avg:58.56ms
step:1636/1845 train_time:95831ms step_avg:58.58ms
step:1637/1845 train_time:95920ms step_avg:58.59ms
step:1638/1845 train_time:96007ms step_avg:58.61ms
step:1639/1845 train_time:96094ms step_avg:58.63ms
step:1640/1845 train_time:96182ms step_avg:58.65ms
step:1641/1845 train_time:96270ms step_avg:58.67ms
step:1642/1845 train_time:96358ms step_avg:58.68ms
step:1643/1845 train_time:96445ms step_avg:58.70ms
step:1644/1845 train_time:96531ms step_avg:58.72ms
step:1645/1845 train_time:96620ms step_avg:58.74ms
step:1646/1845 train_time:96708ms step_avg:58.75ms
step:1647/1845 train_time:96795ms step_avg:58.77ms
step:1648/1845 train_time:96883ms step_avg:58.79ms
step:1649/1845 train_time:96971ms step_avg:58.81ms
step:1650/1845 train_time:97058ms step_avg:58.82ms
step:1651/1845 train_time:97146ms step_avg:58.84ms
step:1652/1845 train_time:97234ms step_avg:58.86ms
step:1653/1845 train_time:97322ms step_avg:58.88ms
step:1654/1845 train_time:97410ms step_avg:58.89ms
step:1655/1845 train_time:97499ms step_avg:58.91ms
step:1656/1845 train_time:97586ms step_avg:58.93ms
step:1657/1845 train_time:97674ms step_avg:58.95ms
step:1658/1845 train_time:97762ms step_avg:58.96ms
step:1659/1845 train_time:97849ms step_avg:58.98ms
step:1660/1845 train_time:97936ms step_avg:59.00ms
step:1661/1845 train_time:98023ms step_avg:59.01ms
step:1662/1845 train_time:98111ms step_avg:59.03ms
step:1663/1845 train_time:98199ms step_avg:59.05ms
step:1664/1845 train_time:98287ms step_avg:59.07ms
step:1665/1845 train_time:98376ms step_avg:59.08ms
step:1666/1845 train_time:98463ms step_avg:59.10ms
step:1667/1845 train_time:98552ms step_avg:59.12ms
step:1668/1845 train_time:98640ms step_avg:59.14ms
step:1669/1845 train_time:98728ms step_avg:59.15ms
step:1670/1845 train_time:98815ms step_avg:59.17ms
step:1671/1845 train_time:98903ms step_avg:59.19ms
step:1672/1845 train_time:98990ms step_avg:59.20ms
step:1673/1845 train_time:99078ms step_avg:59.22ms
step:1674/1845 train_time:99166ms step_avg:59.24ms
step:1675/1845 train_time:99253ms step_avg:59.26ms
step:1676/1845 train_time:99341ms step_avg:59.27ms
step:1677/1845 train_time:99429ms step_avg:59.29ms
step:1678/1845 train_time:99516ms step_avg:59.31ms
step:1679/1845 train_time:99604ms step_avg:59.32ms
step:1680/1845 train_time:99691ms step_avg:59.34ms
step:1681/1845 train_time:99780ms step_avg:59.36ms
step:1682/1845 train_time:99868ms step_avg:59.37ms
step:1683/1845 train_time:99955ms step_avg:59.39ms
step:1684/1845 train_time:100042ms step_avg:59.41ms
step:1685/1845 train_time:100129ms step_avg:59.42ms
step:1686/1845 train_time:100217ms step_avg:59.44ms
step:1687/1845 train_time:100305ms step_avg:59.46ms
step:1688/1845 train_time:100393ms step_avg:59.47ms
step:1689/1845 train_time:100482ms step_avg:59.49ms
step:1690/1845 train_time:100569ms step_avg:59.51ms
step:1691/1845 train_time:100657ms step_avg:59.53ms
step:1692/1845 train_time:100744ms step_avg:59.54ms
step:1693/1845 train_time:100831ms step_avg:59.56ms
step:1694/1845 train_time:100919ms step_avg:59.57ms
step:1695/1845 train_time:101006ms step_avg:59.59ms
step:1696/1845 train_time:101093ms step_avg:59.61ms
step:1697/1845 train_time:101181ms step_avg:59.62ms
step:1698/1845 train_time:101269ms step_avg:59.64ms
step:1699/1845 train_time:101357ms step_avg:59.66ms
step:1700/1845 train_time:101444ms step_avg:59.67ms
step:1701/1845 train_time:101533ms step_avg:59.69ms
step:1702/1845 train_time:101621ms step_avg:59.71ms
step:1703/1845 train_time:101710ms step_avg:59.72ms
step:1704/1845 train_time:101796ms step_avg:59.74ms
step:1705/1845 train_time:101884ms step_avg:59.76ms
step:1706/1845 train_time:101972ms step_avg:59.77ms
step:1707/1845 train_time:102059ms step_avg:59.79ms
step:1708/1845 train_time:102147ms step_avg:59.81ms
step:1709/1845 train_time:102235ms step_avg:59.82ms
step:1710/1845 train_time:102324ms step_avg:59.84ms
step:1711/1845 train_time:102411ms step_avg:59.85ms
step:1712/1845 train_time:102499ms step_avg:59.87ms
step:1713/1845 train_time:102586ms step_avg:59.89ms
step:1714/1845 train_time:102673ms step_avg:59.90ms
step:1715/1845 train_time:102761ms step_avg:59.92ms
step:1716/1845 train_time:102849ms step_avg:59.94ms
step:1717/1845 train_time:102936ms step_avg:59.95ms
step:1718/1845 train_time:103023ms step_avg:59.97ms
step:1719/1845 train_time:103110ms step_avg:59.98ms
step:1720/1845 train_time:103198ms step_avg:60.00ms
step:1721/1845 train_time:103286ms step_avg:60.02ms
step:1722/1845 train_time:103373ms step_avg:60.03ms
step:1723/1845 train_time:103461ms step_avg:60.05ms
step:1724/1845 train_time:103549ms step_avg:60.06ms
step:1725/1845 train_time:103636ms step_avg:60.08ms
step:1726/1845 train_time:103724ms step_avg:60.10ms
step:1727/1845 train_time:103812ms step_avg:60.11ms
step:1728/1845 train_time:103900ms step_avg:60.13ms
step:1729/1845 train_time:103987ms step_avg:60.14ms
step:1730/1845 train_time:104074ms step_avg:60.16ms
step:1731/1845 train_time:104162ms step_avg:60.17ms
step:1732/1845 train_time:104250ms step_avg:60.19ms
step:1733/1845 train_time:104338ms step_avg:60.21ms
step:1734/1845 train_time:104426ms step_avg:60.22ms
step:1735/1845 train_time:104513ms step_avg:60.24ms
step:1736/1845 train_time:104601ms step_avg:60.25ms
step:1737/1845 train_time:104688ms step_avg:60.27ms
step:1738/1845 train_time:104775ms step_avg:60.28ms
step:1739/1845 train_time:104863ms step_avg:60.30ms
step:1740/1845 train_time:104951ms step_avg:60.32ms
step:1741/1845 train_time:105039ms step_avg:60.33ms
step:1742/1845 train_time:105127ms step_avg:60.35ms
step:1743/1845 train_time:105215ms step_avg:60.36ms
step:1744/1845 train_time:105303ms step_avg:60.38ms
step:1745/1845 train_time:105390ms step_avg:60.40ms
step:1746/1845 train_time:105478ms step_avg:60.41ms
step:1747/1845 train_time:105565ms step_avg:60.43ms
step:1748/1845 train_time:105653ms step_avg:60.44ms
step:1749/1845 train_time:105740ms step_avg:60.46ms
step:1750/1845 train_time:105828ms step_avg:60.47ms
step:1750/1845 val_loss:3.3040 train_time:105914ms step_avg:60.52ms
step:1751/1845 train_time:105944ms step_avg:60.50ms
step:1752/1845 train_time:106004ms step_avg:60.50ms
step:1753/1845 train_time:106092ms step_avg:60.52ms
step:1754/1845 train_time:106180ms step_avg:60.54ms
step:1755/1845 train_time:106268ms step_avg:60.55ms
step:1756/1845 train_time:106355ms step_avg:60.57ms
step:1757/1845 train_time:106441ms step_avg:60.58ms
step:1758/1845 train_time:106527ms step_avg:60.60ms
step:1759/1845 train_time:106614ms step_avg:60.61ms
step:1760/1845 train_time:106701ms step_avg:60.63ms
step:1761/1845 train_time:106790ms step_avg:60.64ms
step:1762/1845 train_time:106880ms step_avg:60.66ms
step:1763/1845 train_time:106970ms step_avg:60.67ms
step:1764/1845 train_time:107057ms step_avg:60.69ms
step:1765/1845 train_time:107146ms step_avg:60.71ms
step:1766/1845 train_time:107233ms step_avg:60.72ms
step:1767/1845 train_time:107321ms step_avg:60.74ms
step:1768/1845 train_time:107407ms step_avg:60.75ms
step:1769/1845 train_time:107494ms step_avg:60.77ms
step:1770/1845 train_time:107581ms step_avg:60.78ms
step:1771/1845 train_time:107669ms step_avg:60.80ms
step:1772/1845 train_time:107757ms step_avg:60.81ms
step:1773/1845 train_time:107845ms step_avg:60.83ms
step:1774/1845 train_time:107934ms step_avg:60.84ms
step:1775/1845 train_time:108022ms step_avg:60.86ms
step:1776/1845 train_time:108111ms step_avg:60.87ms
step:1777/1845 train_time:108198ms step_avg:60.89ms
step:1778/1845 train_time:108286ms step_avg:60.90ms
step:1779/1845 train_time:108374ms step_avg:60.92ms
step:1780/1845 train_time:108459ms step_avg:60.93ms
step:1781/1845 train_time:108547ms step_avg:60.95ms
step:1782/1845 train_time:108634ms step_avg:60.96ms
step:1783/1845 train_time:108721ms step_avg:60.98ms
step:1784/1845 train_time:108811ms step_avg:60.99ms
step:1785/1845 train_time:108899ms step_avg:61.01ms
step:1786/1845 train_time:108987ms step_avg:61.02ms
step:1787/1845 train_time:109075ms step_avg:61.04ms
step:1788/1845 train_time:109164ms step_avg:61.05ms
step:1789/1845 train_time:109252ms step_avg:61.07ms
step:1790/1845 train_time:109338ms step_avg:61.08ms
step:1791/1845 train_time:109425ms step_avg:61.10ms
step:1792/1845 train_time:109512ms step_avg:61.11ms
step:1793/1845 train_time:109600ms step_avg:61.13ms
step:1794/1845 train_time:109687ms step_avg:61.14ms
step:1795/1845 train_time:109775ms step_avg:61.16ms
step:1796/1845 train_time:109864ms step_avg:61.17ms
step:1797/1845 train_time:109952ms step_avg:61.19ms
step:1798/1845 train_time:110039ms step_avg:61.20ms
step:1799/1845 train_time:110128ms step_avg:61.22ms
step:1800/1845 train_time:110215ms step_avg:61.23ms
step:1801/1845 train_time:110302ms step_avg:61.24ms
step:1802/1845 train_time:110389ms step_avg:61.26ms
step:1803/1845 train_time:110478ms step_avg:61.27ms
step:1804/1845 train_time:110565ms step_avg:61.29ms
step:1805/1845 train_time:110652ms step_avg:61.30ms
step:1806/1845 train_time:110740ms step_avg:61.32ms
step:1807/1845 train_time:110829ms step_avg:61.33ms
step:1808/1845 train_time:110916ms step_avg:61.35ms
step:1809/1845 train_time:111005ms step_avg:61.36ms
step:1810/1845 train_time:111093ms step_avg:61.38ms
step:1811/1845 train_time:111182ms step_avg:61.39ms
step:1812/1845 train_time:111270ms step_avg:61.41ms
step:1813/1845 train_time:111358ms step_avg:61.42ms
step:1814/1845 train_time:111446ms step_avg:61.44ms
step:1815/1845 train_time:111533ms step_avg:61.45ms
step:1816/1845 train_time:111620ms step_avg:61.46ms
step:1817/1845 train_time:111708ms step_avg:61.48ms
step:1818/1845 train_time:111796ms step_avg:61.49ms
step:1819/1845 train_time:111885ms step_avg:61.51ms
step:1820/1845 train_time:111974ms step_avg:61.52ms
step:1821/1845 train_time:112063ms step_avg:61.54ms
step:1822/1845 train_time:112150ms step_avg:61.55ms
step:1823/1845 train_time:112238ms step_avg:61.57ms
step:1824/1845 train_time:112326ms step_avg:61.58ms
step:1825/1845 train_time:112413ms step_avg:61.60ms
step:1826/1845 train_time:112500ms step_avg:61.61ms
step:1827/1845 train_time:112588ms step_avg:61.62ms
step:1828/1845 train_time:112675ms step_avg:61.64ms
step:1829/1845 train_time:112763ms step_avg:61.65ms
step:1830/1845 train_time:112852ms step_avg:61.67ms
step:1831/1845 train_time:112939ms step_avg:61.68ms
step:1832/1845 train_time:113028ms step_avg:61.70ms
step:1833/1845 train_time:113116ms step_avg:61.71ms
step:1834/1845 train_time:113204ms step_avg:61.73ms
step:1835/1845 train_time:113292ms step_avg:61.74ms
step:1836/1845 train_time:113379ms step_avg:61.75ms
step:1837/1845 train_time:113468ms step_avg:61.77ms
step:1838/1845 train_time:113556ms step_avg:61.78ms
step:1839/1845 train_time:113643ms step_avg:61.80ms
step:1840/1845 train_time:113731ms step_avg:61.81ms
step:1841/1845 train_time:113819ms step_avg:61.82ms
step:1842/1845 train_time:113907ms step_avg:61.84ms
step:1843/1845 train_time:113996ms step_avg:61.85ms
step:1844/1845 train_time:114083ms step_avg:61.87ms
step:1845/1845 train_time:114171ms step_avg:61.88ms
step:1845/1845 val_loss:3.2776 train_time:114256ms step_avg:61.93ms
peak memory allocated: 29405 MiB reserved: 44458 MiB

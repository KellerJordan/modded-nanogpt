# Small helper for timestamping.
from zoneinfo import ZoneInfo
import datetime as dt

def get_timestamp(timezone_str: str = "America/Los_Angeles") -> str:
    tz = ZoneInfo(timezone_str)
    now = dt.datetime.now(tz)
    return now.strftime("%Y-%m-%d_%H%M%S")

# ================================
use_fa3 = True # Disable for GH200
WANDB_PROJECT = "modded-nanogpt-adam"
GROUP_NAME =    "12-29 - gates-banked-compiled-adam - bf16 gates - fp32 exp_avg"  # i.e., experiment name.
LOG_DIR =       f"logs/{GROUP_NAME}"
RUN_ID =        f"{get_timestamp()} - {GROUP_NAME}" 

# ================================

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
if use_fa3:
    from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            #exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device) # Testing making the optimizer state fp32.
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay (lr as weight decay schedule)
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0))
                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

if use_fa3:
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'attn_gate_bank', 've_gate_bank', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            #self.start_transition() # Freeze scalar weights during transition

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)
                if isinstance(opt, DistAdam):
                    opt.freeze_timer = 0

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    log_dir: str = LOG_DIR
    run_id: str = RUN_ID
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs(args.log_dir, exist_ok=True)
    logfile = f"{args.log_dir}/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()  # Testing leaving these as fp32
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

if master_process:
    import wandb
    wandb.login()
    wandb.init(
        project=WANDB_PROJECT,
        name=args.run_id,
        config=args
    )
    wandb.define_metric("final/*", step_metric=None, summary="last")  # one-off scalars

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# ----------- Profiling Code -----------
# from torch.profiler import profile, ProfilerActivity, schedule

# profile_steps = 14 # Total steps to run for the profiling process.

# def trace_handler(prof: torch.profiler.profile):
#     path_prefix = f"{LOG_DIR}/traces/"
#     os.makedirs(path_prefix, exist_ok=True)
#     prof.export_chrome_trace(f"{path_prefix}/{RUN_ID}-rank{rank}.json.gz")

# prof_ctx = torch.profiler.profile(
#     activities=[
#         ProfilerActivity.CPU,
#         ProfilerActivity.CUDA,
#     ],
#     schedule=schedule(
#         wait=5, # Wait 5 steps before profiling.
#         warmup=5, # Warmup for 5 steps.
#         active=profile_steps - 10 # Activate for the remaining steps.
#     ),
#     on_trace_ready=trace_handler, # Callback when traces are ready, handler saves to disk.
#     with_stack=False, # Disable file and line number recording for cleaner traces.
#     record_shapes=True, # Record the shapes of the tensors in the graph.
# )

# # Enter the profiler context before the training loop
# prof_ctx.__enter__()
# for step in range(profile_steps):  
# ----------------------------------------

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        if master_process:
            # Log to wandb
            wandb.log({
                "val/loss": val_loss.item(),
                "time/training_time_ms": training_time_ms,
                "time/step_avg_ms": training_time_ms / (step + 1),
            }, step=step)

        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process:
            final_metrics = {
                "final/val_loss": float(val_loss.item()),
                "final/train_time_ms": int(training_time_ms),
                "final/train_time": training_time_ms / 1000,
                "final/steps_total": int(step),
            }
            wandb.log(final_metrics)
            wandb.run.summary.update(final_metrics)

        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

    #prof_ctx.step()

# After we exit our loop we exit the profiler context
#prof_ctx.__exit__(None, None, None)
        
if master_process:
    wandb.save(f"{args.log_dir}/{args.run_id}.txt")
    wandb.finish()

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 31 07:35:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:08:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:09:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   33C    P0            114W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          340341      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    1   N/A  N/A          340342      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    2   N/A  N/A          340343      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    3   N/A  N/A          340344      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    4   N/A  N/A          340345      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    5   N/A  N/A          340346      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    6   N/A  N/A          340347      C   .../envs/speedrun/bin/python3.12       1512MiB |
|    7   N/A  N/A          340348      C   .../envs/speedrun/bin/python3.12       1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8310 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:60ms step_avg:60.01ms
step:2/1845 train_time:88ms step_avg:43.99ms
step:3/1845 train_time:112ms step_avg:37.49ms
step:4/1845 train_time:141ms step_avg:35.15ms
step:5/1845 train_time:171ms step_avg:34.16ms
step:6/1845 train_time:283ms step_avg:47.16ms
step:7/1845 train_time:315ms step_avg:45.00ms
step:8/1845 train_time:352ms step_avg:44.05ms
step:9/1845 train_time:386ms step_avg:42.94ms
step:10/1845 train_time:422ms step_avg:42.23ms
step:11/1845 train_time:456ms step_avg:41.42ms
step:12/1845 train_time:495ms step_avg:41.23ms
step:13/1845 train_time:530ms step_avg:40.74ms
step:14/1845 train_time:566ms step_avg:40.41ms
step:15/1845 train_time:599ms step_avg:39.95ms
step:16/1845 train_time:639ms step_avg:39.94ms
step:17/1845 train_time:673ms step_avg:39.60ms
step:18/1845 train_time:711ms step_avg:39.49ms
step:19/1845 train_time:744ms step_avg:39.18ms
step:20/1845 train_time:782ms step_avg:39.11ms
step:21/1845 train_time:815ms step_avg:38.82ms
step:22/1845 train_time:852ms step_avg:38.75ms
step:23/1845 train_time:887ms step_avg:38.55ms
step:24/1845 train_time:924ms step_avg:38.51ms
step:25/1845 train_time:958ms step_avg:38.31ms
step:26/1845 train_time:996ms step_avg:38.31ms
step:27/1845 train_time:1030ms step_avg:38.13ms
step:28/1845 train_time:1067ms step_avg:38.11ms
step:29/1845 train_time:1101ms step_avg:37.98ms
step:30/1845 train_time:1138ms step_avg:37.94ms
step:31/1845 train_time:1173ms step_avg:37.85ms
step:32/1845 train_time:1211ms step_avg:37.84ms
step:33/1845 train_time:1245ms step_avg:37.71ms
step:34/1845 train_time:1282ms step_avg:37.72ms
step:35/1845 train_time:1316ms step_avg:37.59ms
step:36/1845 train_time:1353ms step_avg:37.59ms
step:37/1845 train_time:1387ms step_avg:37.48ms
step:38/1845 train_time:1425ms step_avg:37.49ms
step:39/1845 train_time:1458ms step_avg:37.40ms
step:40/1845 train_time:1496ms step_avg:37.41ms
step:41/1845 train_time:1530ms step_avg:37.31ms
step:42/1845 train_time:1567ms step_avg:37.32ms
step:43/1845 train_time:1602ms step_avg:37.25ms
step:44/1845 train_time:1639ms step_avg:37.25ms
step:45/1845 train_time:1672ms step_avg:37.16ms
step:46/1845 train_time:1710ms step_avg:37.18ms
step:47/1845 train_time:1744ms step_avg:37.10ms
step:48/1845 train_time:1783ms step_avg:37.15ms
step:49/1845 train_time:1817ms step_avg:37.08ms
step:50/1845 train_time:1855ms step_avg:37.10ms
step:51/1845 train_time:1889ms step_avg:37.05ms
step:52/1845 train_time:1927ms step_avg:37.05ms
step:53/1845 train_time:1960ms step_avg:36.98ms
step:54/1845 train_time:1998ms step_avg:37.00ms
step:55/1845 train_time:2031ms step_avg:36.94ms
step:56/1845 train_time:2069ms step_avg:36.95ms
step:57/1845 train_time:2103ms step_avg:36.90ms
step:58/1845 train_time:2140ms step_avg:36.90ms
step:59/1845 train_time:2174ms step_avg:36.84ms
step:60/1845 train_time:2212ms step_avg:36.87ms
step:61/1845 train_time:2246ms step_avg:36.82ms
step:62/1845 train_time:2284ms step_avg:36.84ms
step:63/1845 train_time:2317ms step_avg:36.78ms
step:64/1845 train_time:2354ms step_avg:36.78ms
step:65/1845 train_time:2388ms step_avg:36.73ms
step:66/1845 train_time:2425ms step_avg:36.74ms
step:67/1845 train_time:2458ms step_avg:36.69ms
step:68/1845 train_time:2495ms step_avg:36.69ms
step:69/1845 train_time:2528ms step_avg:36.64ms
step:70/1845 train_time:2565ms step_avg:36.65ms
step:71/1845 train_time:2598ms step_avg:36.60ms
step:72/1845 train_time:2633ms step_avg:36.57ms
step:73/1845 train_time:2667ms step_avg:36.53ms
step:74/1845 train_time:2704ms step_avg:36.54ms
step:75/1845 train_time:2738ms step_avg:36.51ms
step:76/1845 train_time:2776ms step_avg:36.53ms
step:77/1845 train_time:2809ms step_avg:36.48ms
step:78/1845 train_time:2846ms step_avg:36.49ms
step:79/1845 train_time:2881ms step_avg:36.46ms
step:80/1845 train_time:2918ms step_avg:36.48ms
step:81/1845 train_time:2952ms step_avg:36.44ms
step:82/1845 train_time:2989ms step_avg:36.46ms
step:83/1845 train_time:3025ms step_avg:36.45ms
step:84/1845 train_time:3063ms step_avg:36.47ms
step:85/1845 train_time:3098ms step_avg:36.45ms
step:86/1845 train_time:3139ms step_avg:36.50ms
step:87/1845 train_time:3176ms step_avg:36.50ms
step:88/1845 train_time:3216ms step_avg:36.54ms
step:89/1845 train_time:3252ms step_avg:36.53ms
step:90/1845 train_time:3291ms step_avg:36.57ms
step:91/1845 train_time:3327ms step_avg:36.56ms
step:92/1845 train_time:3366ms step_avg:36.59ms
step:93/1845 train_time:3402ms step_avg:36.59ms
step:94/1845 train_time:3442ms step_avg:36.61ms
step:95/1845 train_time:3477ms step_avg:36.60ms
step:96/1845 train_time:3519ms step_avg:36.66ms
step:97/1845 train_time:3555ms step_avg:36.65ms
step:98/1845 train_time:3595ms step_avg:36.68ms
step:99/1845 train_time:3630ms step_avg:36.67ms
step:100/1845 train_time:3670ms step_avg:36.70ms
step:101/1845 train_time:3705ms step_avg:36.69ms
step:102/1845 train_time:3744ms step_avg:36.71ms
step:103/1845 train_time:3780ms step_avg:36.70ms
step:104/1845 train_time:3820ms step_avg:36.73ms
step:105/1845 train_time:3854ms step_avg:36.71ms
step:106/1845 train_time:3894ms step_avg:36.74ms
step:107/1845 train_time:3930ms step_avg:36.73ms
step:108/1845 train_time:3970ms step_avg:36.76ms
step:109/1845 train_time:4006ms step_avg:36.75ms
step:110/1845 train_time:4046ms step_avg:36.78ms
step:111/1845 train_time:4081ms step_avg:36.77ms
step:112/1845 train_time:4120ms step_avg:36.79ms
step:113/1845 train_time:4156ms step_avg:36.78ms
step:114/1845 train_time:4195ms step_avg:36.80ms
step:115/1845 train_time:4231ms step_avg:36.79ms
step:116/1845 train_time:4269ms step_avg:36.80ms
step:117/1845 train_time:4305ms step_avg:36.80ms
step:118/1845 train_time:4345ms step_avg:36.82ms
step:119/1845 train_time:4380ms step_avg:36.81ms
step:120/1845 train_time:4420ms step_avg:36.83ms
step:121/1845 train_time:4456ms step_avg:36.82ms
step:122/1845 train_time:4495ms step_avg:36.84ms
step:123/1845 train_time:4530ms step_avg:36.83ms
step:124/1845 train_time:4567ms step_avg:36.83ms
step:125/1845 train_time:4601ms step_avg:36.81ms
step:126/1845 train_time:4640ms step_avg:36.82ms
step:127/1845 train_time:4675ms step_avg:36.81ms
step:128/1845 train_time:4716ms step_avg:36.84ms
step:129/1845 train_time:4752ms step_avg:36.83ms
step:130/1845 train_time:4791ms step_avg:36.85ms
step:131/1845 train_time:4826ms step_avg:36.84ms
step:132/1845 train_time:4865ms step_avg:36.85ms
step:133/1845 train_time:4900ms step_avg:36.84ms
step:134/1845 train_time:4940ms step_avg:36.87ms
step:135/1845 train_time:4976ms step_avg:36.86ms
step:136/1845 train_time:5016ms step_avg:36.88ms
step:137/1845 train_time:5051ms step_avg:36.87ms
step:138/1845 train_time:5092ms step_avg:36.90ms
step:139/1845 train_time:5128ms step_avg:36.89ms
step:140/1845 train_time:5167ms step_avg:36.91ms
step:141/1845 train_time:5203ms step_avg:36.90ms
step:142/1845 train_time:5242ms step_avg:36.92ms
step:143/1845 train_time:5278ms step_avg:36.91ms
step:144/1845 train_time:5319ms step_avg:36.94ms
step:145/1845 train_time:5355ms step_avg:36.93ms
step:146/1845 train_time:5394ms step_avg:36.94ms
step:147/1845 train_time:5429ms step_avg:36.93ms
step:148/1845 train_time:5469ms step_avg:36.95ms
step:149/1845 train_time:5504ms step_avg:36.94ms
step:150/1845 train_time:5544ms step_avg:36.96ms
step:151/1845 train_time:5578ms step_avg:36.94ms
step:152/1845 train_time:5616ms step_avg:36.95ms
step:153/1845 train_time:5650ms step_avg:36.93ms
step:154/1845 train_time:5688ms step_avg:36.94ms
step:155/1845 train_time:5722ms step_avg:36.92ms
step:156/1845 train_time:5760ms step_avg:36.92ms
step:157/1845 train_time:5795ms step_avg:36.91ms
step:158/1845 train_time:5833ms step_avg:36.92ms
step:159/1845 train_time:5867ms step_avg:36.90ms
step:160/1845 train_time:5905ms step_avg:36.91ms
step:161/1845 train_time:5939ms step_avg:36.89ms
step:162/1845 train_time:5977ms step_avg:36.89ms
step:163/1845 train_time:6011ms step_avg:36.88ms
step:164/1845 train_time:6049ms step_avg:36.88ms
step:165/1845 train_time:6083ms step_avg:36.87ms
step:166/1845 train_time:6122ms step_avg:36.88ms
step:167/1845 train_time:6156ms step_avg:36.86ms
step:168/1845 train_time:6194ms step_avg:36.87ms
step:169/1845 train_time:6229ms step_avg:36.86ms
step:170/1845 train_time:6267ms step_avg:36.87ms
step:171/1845 train_time:6302ms step_avg:36.85ms
step:172/1845 train_time:6341ms step_avg:36.86ms
step:173/1845 train_time:6375ms step_avg:36.85ms
step:174/1845 train_time:6413ms step_avg:36.86ms
step:175/1845 train_time:6448ms step_avg:36.84ms
step:176/1845 train_time:6486ms step_avg:36.85ms
step:177/1845 train_time:6521ms step_avg:36.84ms
step:178/1845 train_time:6560ms step_avg:36.85ms
step:179/1845 train_time:6594ms step_avg:36.84ms
step:180/1845 train_time:6633ms step_avg:36.85ms
step:181/1845 train_time:6667ms step_avg:36.83ms
step:182/1845 train_time:6705ms step_avg:36.84ms
step:183/1845 train_time:6740ms step_avg:36.83ms
step:184/1845 train_time:6780ms step_avg:36.85ms
step:185/1845 train_time:6814ms step_avg:36.83ms
step:186/1845 train_time:6853ms step_avg:36.84ms
step:187/1845 train_time:6888ms step_avg:36.83ms
step:188/1845 train_time:6926ms step_avg:36.84ms
step:189/1845 train_time:6961ms step_avg:36.83ms
step:190/1845 train_time:6999ms step_avg:36.83ms
step:191/1845 train_time:7033ms step_avg:36.82ms
step:192/1845 train_time:7071ms step_avg:36.83ms
step:193/1845 train_time:7106ms step_avg:36.82ms
step:194/1845 train_time:7144ms step_avg:36.82ms
step:195/1845 train_time:7179ms step_avg:36.81ms
step:196/1845 train_time:7217ms step_avg:36.82ms
step:197/1845 train_time:7251ms step_avg:36.81ms
step:198/1845 train_time:7289ms step_avg:36.82ms
step:199/1845 train_time:7324ms step_avg:36.81ms
step:200/1845 train_time:7362ms step_avg:36.81ms
step:201/1845 train_time:7397ms step_avg:36.80ms
step:202/1845 train_time:7434ms step_avg:36.80ms
step:203/1845 train_time:7468ms step_avg:36.79ms
step:204/1845 train_time:7506ms step_avg:36.79ms
step:205/1845 train_time:7540ms step_avg:36.78ms
step:206/1845 train_time:7579ms step_avg:36.79ms
step:207/1845 train_time:7612ms step_avg:36.77ms
step:208/1845 train_time:7650ms step_avg:36.78ms
step:209/1845 train_time:7684ms step_avg:36.76ms
step:210/1845 train_time:7721ms step_avg:36.77ms
step:211/1845 train_time:7756ms step_avg:36.76ms
step:212/1845 train_time:7794ms step_avg:36.76ms
step:213/1845 train_time:7828ms step_avg:36.75ms
step:214/1845 train_time:7866ms step_avg:36.76ms
step:215/1845 train_time:7900ms step_avg:36.74ms
step:216/1845 train_time:7938ms step_avg:36.75ms
step:217/1845 train_time:7972ms step_avg:36.74ms
step:218/1845 train_time:8010ms step_avg:36.74ms
step:219/1845 train_time:8044ms step_avg:36.73ms
step:220/1845 train_time:8082ms step_avg:36.74ms
step:221/1845 train_time:8117ms step_avg:36.73ms
step:222/1845 train_time:8155ms step_avg:36.74ms
step:223/1845 train_time:8189ms step_avg:36.72ms
step:224/1845 train_time:8228ms step_avg:36.73ms
step:225/1845 train_time:8262ms step_avg:36.72ms
step:226/1845 train_time:8300ms step_avg:36.73ms
step:227/1845 train_time:8334ms step_avg:36.72ms
step:228/1845 train_time:8373ms step_avg:36.72ms
step:229/1845 train_time:8407ms step_avg:36.71ms
step:230/1845 train_time:8444ms step_avg:36.71ms
step:231/1845 train_time:8479ms step_avg:36.70ms
step:232/1845 train_time:8517ms step_avg:36.71ms
step:233/1845 train_time:8552ms step_avg:36.70ms
step:234/1845 train_time:8590ms step_avg:36.71ms
step:235/1845 train_time:8624ms step_avg:36.70ms
step:236/1845 train_time:8662ms step_avg:36.70ms
step:237/1845 train_time:8696ms step_avg:36.69ms
step:238/1845 train_time:8734ms step_avg:36.70ms
step:239/1845 train_time:8768ms step_avg:36.68ms
step:240/1845 train_time:8806ms step_avg:36.69ms
step:241/1845 train_time:8840ms step_avg:36.68ms
step:242/1845 train_time:8878ms step_avg:36.69ms
step:243/1845 train_time:8913ms step_avg:36.68ms
step:244/1845 train_time:8951ms step_avg:36.68ms
step:245/1845 train_time:8985ms step_avg:36.67ms
step:246/1845 train_time:9024ms step_avg:36.68ms
step:247/1845 train_time:9058ms step_avg:36.67ms
step:248/1845 train_time:9096ms step_avg:36.68ms
step:249/1845 train_time:9131ms step_avg:36.67ms
step:250/1845 train_time:9168ms step_avg:36.67ms
step:250/1845 val_loss:4.6080 train_time:9171ms step_avg:36.68ms
step:251/1845 train_time:9197ms step_avg:36.64ms
step:252/1845 train_time:9224ms step_avg:36.60ms
step:253/1845 train_time:9249ms step_avg:36.56ms
step:254/1845 train_time:9277ms step_avg:36.52ms
step:255/1845 train_time:9301ms step_avg:36.48ms
step:256/1845 train_time:9339ms step_avg:36.48ms
step:257/1845 train_time:9369ms step_avg:36.46ms
step:258/1845 train_time:9408ms step_avg:36.47ms
step:259/1845 train_time:9441ms step_avg:36.45ms
step:260/1845 train_time:9477ms step_avg:36.45ms
step:261/1845 train_time:9510ms step_avg:36.44ms
step:262/1845 train_time:9547ms step_avg:36.44ms
step:263/1845 train_time:9580ms step_avg:36.43ms
step:264/1845 train_time:9616ms step_avg:36.42ms
step:265/1845 train_time:9649ms step_avg:36.41ms
step:266/1845 train_time:9686ms step_avg:36.41ms
step:267/1845 train_time:9720ms step_avg:36.40ms
step:268/1845 train_time:9758ms step_avg:36.41ms
step:269/1845 train_time:9792ms step_avg:36.40ms
step:270/1845 train_time:9830ms step_avg:36.41ms
step:271/1845 train_time:9864ms step_avg:36.40ms
step:272/1845 train_time:9901ms step_avg:36.40ms
step:273/1845 train_time:9934ms step_avg:36.39ms
step:274/1845 train_time:9973ms step_avg:36.40ms
step:275/1845 train_time:10007ms step_avg:36.39ms
step:276/1845 train_time:10044ms step_avg:36.39ms
step:277/1845 train_time:10079ms step_avg:36.39ms
step:278/1845 train_time:10118ms step_avg:36.39ms
step:279/1845 train_time:10152ms step_avg:36.39ms
step:280/1845 train_time:10190ms step_avg:36.39ms
step:281/1845 train_time:10225ms step_avg:36.39ms
step:282/1845 train_time:10263ms step_avg:36.39ms
step:283/1845 train_time:10298ms step_avg:36.39ms
step:284/1845 train_time:10336ms step_avg:36.39ms
step:285/1845 train_time:10370ms step_avg:36.39ms
step:286/1845 train_time:10408ms step_avg:36.39ms
step:287/1845 train_time:10443ms step_avg:36.39ms
step:288/1845 train_time:10482ms step_avg:36.39ms
step:289/1845 train_time:10516ms step_avg:36.39ms
step:290/1845 train_time:10552ms step_avg:36.39ms
step:291/1845 train_time:10587ms step_avg:36.38ms
step:292/1845 train_time:10625ms step_avg:36.39ms
step:293/1845 train_time:10659ms step_avg:36.38ms
step:294/1845 train_time:10696ms step_avg:36.38ms
step:295/1845 train_time:10728ms step_avg:36.37ms
step:296/1845 train_time:10765ms step_avg:36.37ms
step:297/1845 train_time:10798ms step_avg:36.36ms
step:298/1845 train_time:10835ms step_avg:36.36ms
step:299/1845 train_time:10867ms step_avg:36.35ms
step:300/1845 train_time:10905ms step_avg:36.35ms
step:301/1845 train_time:10941ms step_avg:36.35ms
step:302/1845 train_time:10979ms step_avg:36.36ms
step:303/1845 train_time:11015ms step_avg:36.35ms
step:304/1845 train_time:11053ms step_avg:36.36ms
step:305/1845 train_time:11088ms step_avg:36.35ms
step:306/1845 train_time:11126ms step_avg:36.36ms
step:307/1845 train_time:11161ms step_avg:36.35ms
step:308/1845 train_time:11199ms step_avg:36.36ms
step:309/1845 train_time:11234ms step_avg:36.35ms
step:310/1845 train_time:11271ms step_avg:36.36ms
step:311/1845 train_time:11306ms step_avg:36.35ms
step:312/1845 train_time:11344ms step_avg:36.36ms
step:313/1845 train_time:11377ms step_avg:36.35ms
step:314/1845 train_time:11416ms step_avg:36.36ms
step:315/1845 train_time:11451ms step_avg:36.35ms
step:316/1845 train_time:11489ms step_avg:36.36ms
step:317/1845 train_time:11524ms step_avg:36.35ms
step:318/1845 train_time:11560ms step_avg:36.35ms
step:319/1845 train_time:11594ms step_avg:36.35ms
step:320/1845 train_time:11630ms step_avg:36.35ms
step:321/1845 train_time:11665ms step_avg:36.34ms
step:322/1845 train_time:11702ms step_avg:36.34ms
step:323/1845 train_time:11737ms step_avg:36.34ms
step:324/1845 train_time:11775ms step_avg:36.34ms
step:325/1845 train_time:11810ms step_avg:36.34ms
step:326/1845 train_time:11849ms step_avg:36.35ms
step:327/1845 train_time:11883ms step_avg:36.34ms
step:328/1845 train_time:11922ms step_avg:36.35ms
step:329/1845 train_time:11957ms step_avg:36.34ms
step:330/1845 train_time:11996ms step_avg:36.35ms
step:331/1845 train_time:12030ms step_avg:36.34ms
step:332/1845 train_time:12068ms step_avg:36.35ms
step:333/1845 train_time:12103ms step_avg:36.35ms
step:334/1845 train_time:12141ms step_avg:36.35ms
step:335/1845 train_time:12176ms step_avg:36.35ms
step:336/1845 train_time:12213ms step_avg:36.35ms
step:337/1845 train_time:12247ms step_avg:36.34ms
step:338/1845 train_time:12286ms step_avg:36.35ms
step:339/1845 train_time:12319ms step_avg:36.34ms
step:340/1845 train_time:12358ms step_avg:36.35ms
step:341/1845 train_time:12392ms step_avg:36.34ms
step:342/1845 train_time:12430ms step_avg:36.34ms
step:343/1845 train_time:12463ms step_avg:36.34ms
step:344/1845 train_time:12501ms step_avg:36.34ms
step:345/1845 train_time:12535ms step_avg:36.33ms
step:346/1845 train_time:12572ms step_avg:36.34ms
step:347/1845 train_time:12607ms step_avg:36.33ms
step:348/1845 train_time:12642ms step_avg:36.33ms
step:349/1845 train_time:12676ms step_avg:36.32ms
step:350/1845 train_time:12714ms step_avg:36.33ms
step:351/1845 train_time:12748ms step_avg:36.32ms
step:352/1845 train_time:12787ms step_avg:36.33ms
step:353/1845 train_time:12822ms step_avg:36.32ms
step:354/1845 train_time:12860ms step_avg:36.33ms
step:355/1845 train_time:12895ms step_avg:36.32ms
step:356/1845 train_time:12933ms step_avg:36.33ms
step:357/1845 train_time:12967ms step_avg:36.32ms
step:358/1845 train_time:13005ms step_avg:36.33ms
step:359/1845 train_time:13039ms step_avg:36.32ms
step:360/1845 train_time:13076ms step_avg:36.32ms
step:361/1845 train_time:13110ms step_avg:36.32ms
step:362/1845 train_time:13148ms step_avg:36.32ms
step:363/1845 train_time:13182ms step_avg:36.31ms
step:364/1845 train_time:13220ms step_avg:36.32ms
step:365/1845 train_time:13254ms step_avg:36.31ms
step:366/1845 train_time:13292ms step_avg:36.32ms
step:367/1845 train_time:13326ms step_avg:36.31ms
step:368/1845 train_time:13364ms step_avg:36.31ms
step:369/1845 train_time:13398ms step_avg:36.31ms
step:370/1845 train_time:13436ms step_avg:36.31ms
step:371/1845 train_time:13469ms step_avg:36.30ms
step:372/1845 train_time:13507ms step_avg:36.31ms
step:373/1845 train_time:13541ms step_avg:36.30ms
step:374/1845 train_time:13578ms step_avg:36.30ms
step:375/1845 train_time:13612ms step_avg:36.30ms
step:376/1845 train_time:13650ms step_avg:36.30ms
step:377/1845 train_time:13683ms step_avg:36.30ms
step:378/1845 train_time:13721ms step_avg:36.30ms
step:379/1845 train_time:13756ms step_avg:36.29ms
step:380/1845 train_time:13793ms step_avg:36.30ms
step:381/1845 train_time:13828ms step_avg:36.29ms
step:382/1845 train_time:13866ms step_avg:36.30ms
step:383/1845 train_time:13899ms step_avg:36.29ms
step:384/1845 train_time:13937ms step_avg:36.29ms
step:385/1845 train_time:13971ms step_avg:36.29ms
step:386/1845 train_time:14010ms step_avg:36.29ms
step:387/1845 train_time:14044ms step_avg:36.29ms
step:388/1845 train_time:14082ms step_avg:36.29ms
step:389/1845 train_time:14116ms step_avg:36.29ms
step:390/1845 train_time:14154ms step_avg:36.29ms
step:391/1845 train_time:14188ms step_avg:36.29ms
step:392/1845 train_time:14227ms step_avg:36.29ms
step:393/1845 train_time:14261ms step_avg:36.29ms
step:394/1845 train_time:14299ms step_avg:36.29ms
step:395/1845 train_time:14333ms step_avg:36.29ms
step:396/1845 train_time:14370ms step_avg:36.29ms
step:397/1845 train_time:14405ms step_avg:36.28ms
step:398/1845 train_time:14442ms step_avg:36.29ms
step:399/1845 train_time:14477ms step_avg:36.28ms
step:400/1845 train_time:14515ms step_avg:36.29ms
step:401/1845 train_time:14549ms step_avg:36.28ms
step:402/1845 train_time:14587ms step_avg:36.29ms
step:403/1845 train_time:14620ms step_avg:36.28ms
step:404/1845 train_time:14658ms step_avg:36.28ms
step:405/1845 train_time:14692ms step_avg:36.28ms
step:406/1845 train_time:14730ms step_avg:36.28ms
step:407/1845 train_time:14764ms step_avg:36.27ms
step:408/1845 train_time:14802ms step_avg:36.28ms
step:409/1845 train_time:14835ms step_avg:36.27ms
step:410/1845 train_time:14873ms step_avg:36.28ms
step:411/1845 train_time:14907ms step_avg:36.27ms
step:412/1845 train_time:14944ms step_avg:36.27ms
step:413/1845 train_time:14978ms step_avg:36.27ms
step:414/1845 train_time:15016ms step_avg:36.27ms
step:415/1845 train_time:15050ms step_avg:36.27ms
step:416/1845 train_time:15088ms step_avg:36.27ms
step:417/1845 train_time:15122ms step_avg:36.26ms
step:418/1845 train_time:15159ms step_avg:36.27ms
step:419/1845 train_time:15194ms step_avg:36.26ms
step:420/1845 train_time:15232ms step_avg:36.27ms
step:421/1845 train_time:15266ms step_avg:36.26ms
step:422/1845 train_time:15304ms step_avg:36.27ms
step:423/1845 train_time:15338ms step_avg:36.26ms
step:424/1845 train_time:15376ms step_avg:36.26ms
step:425/1845 train_time:15411ms step_avg:36.26ms
step:426/1845 train_time:15448ms step_avg:36.26ms
step:427/1845 train_time:15482ms step_avg:36.26ms
step:428/1845 train_time:15520ms step_avg:36.26ms
step:429/1845 train_time:15555ms step_avg:36.26ms
step:430/1845 train_time:15596ms step_avg:36.27ms
step:431/1845 train_time:15629ms step_avg:36.26ms
step:432/1845 train_time:15666ms step_avg:36.26ms
step:433/1845 train_time:15699ms step_avg:36.26ms
step:434/1845 train_time:15736ms step_avg:36.26ms
step:435/1845 train_time:15769ms step_avg:36.25ms
step:436/1845 train_time:15806ms step_avg:36.25ms
step:437/1845 train_time:15839ms step_avg:36.25ms
step:438/1845 train_time:15876ms step_avg:36.25ms
step:439/1845 train_time:15909ms step_avg:36.24ms
step:440/1845 train_time:15946ms step_avg:36.24ms
step:441/1845 train_time:15979ms step_avg:36.23ms
step:442/1845 train_time:16017ms step_avg:36.24ms
step:443/1845 train_time:16050ms step_avg:36.23ms
step:444/1845 train_time:16087ms step_avg:36.23ms
step:445/1845 train_time:16120ms step_avg:36.22ms
step:446/1845 train_time:16157ms step_avg:36.23ms
step:447/1845 train_time:16190ms step_avg:36.22ms
step:448/1845 train_time:16227ms step_avg:36.22ms
step:449/1845 train_time:16260ms step_avg:36.21ms
step:450/1845 train_time:16297ms step_avg:36.21ms
step:451/1845 train_time:16330ms step_avg:36.21ms
step:452/1845 train_time:16366ms step_avg:36.21ms
step:453/1845 train_time:16400ms step_avg:36.20ms
step:454/1845 train_time:16436ms step_avg:36.20ms
step:455/1845 train_time:16469ms step_avg:36.20ms
step:456/1845 train_time:16506ms step_avg:36.20ms
step:457/1845 train_time:16539ms step_avg:36.19ms
step:458/1845 train_time:16576ms step_avg:36.19ms
step:459/1845 train_time:16609ms step_avg:36.19ms
step:460/1845 train_time:16646ms step_avg:36.19ms
step:461/1845 train_time:16679ms step_avg:36.18ms
step:462/1845 train_time:16716ms step_avg:36.18ms
step:463/1845 train_time:16748ms step_avg:36.17ms
step:464/1845 train_time:16785ms step_avg:36.17ms
step:465/1845 train_time:16819ms step_avg:36.17ms
step:466/1845 train_time:16856ms step_avg:36.17ms
step:467/1845 train_time:16889ms step_avg:36.16ms
step:468/1845 train_time:16926ms step_avg:36.17ms
step:469/1845 train_time:16959ms step_avg:36.16ms
step:470/1845 train_time:16996ms step_avg:36.16ms
step:471/1845 train_time:17029ms step_avg:36.16ms
step:472/1845 train_time:17064ms step_avg:36.15ms
step:473/1845 train_time:17097ms step_avg:36.15ms
step:474/1845 train_time:17134ms step_avg:36.15ms
step:475/1845 train_time:17167ms step_avg:36.14ms
step:476/1845 train_time:17204ms step_avg:36.14ms
step:477/1845 train_time:17237ms step_avg:36.14ms
step:478/1845 train_time:17273ms step_avg:36.14ms
step:479/1845 train_time:17306ms step_avg:36.13ms
step:480/1845 train_time:17343ms step_avg:36.13ms
step:481/1845 train_time:17375ms step_avg:36.12ms
step:482/1845 train_time:17412ms step_avg:36.12ms
step:483/1845 train_time:17445ms step_avg:36.12ms
step:484/1845 train_time:17482ms step_avg:36.12ms
step:485/1845 train_time:17515ms step_avg:36.11ms
step:486/1845 train_time:17552ms step_avg:36.11ms
step:487/1845 train_time:17583ms step_avg:36.10ms
step:488/1845 train_time:17612ms step_avg:36.09ms
step:489/1845 train_time:17638ms step_avg:36.07ms
step:490/1845 train_time:17664ms step_avg:36.05ms
step:491/1845 train_time:17688ms step_avg:36.02ms
step:492/1845 train_time:17719ms step_avg:36.01ms
step:493/1845 train_time:17750ms step_avg:36.00ms
step:494/1845 train_time:17786ms step_avg:36.00ms
step:495/1845 train_time:17819ms step_avg:36.00ms
step:496/1845 train_time:17857ms step_avg:36.00ms
step:497/1845 train_time:17888ms step_avg:35.99ms
step:498/1845 train_time:17925ms step_avg:35.99ms
step:499/1845 train_time:17956ms step_avg:35.98ms
step:500/1845 train_time:17992ms step_avg:35.98ms
step:500/1845 val_loss:4.2954 train_time:18030ms step_avg:36.06ms
step:501/1845 train_time:18056ms step_avg:36.04ms
step:502/1845 train_time:18082ms step_avg:36.02ms
step:503/1845 train_time:18105ms step_avg:35.99ms
step:504/1845 train_time:18131ms step_avg:35.97ms
step:505/1845 train_time:18160ms step_avg:35.96ms
step:506/1845 train_time:18196ms step_avg:35.96ms
step:507/1845 train_time:18229ms step_avg:35.96ms
step:508/1845 train_time:18266ms step_avg:35.96ms
step:509/1845 train_time:18298ms step_avg:35.95ms
step:510/1845 train_time:18334ms step_avg:35.95ms
step:511/1845 train_time:18365ms step_avg:35.94ms
step:512/1845 train_time:18401ms step_avg:35.94ms
step:513/1845 train_time:18433ms step_avg:35.93ms
step:514/1845 train_time:18471ms step_avg:35.94ms
step:515/1845 train_time:18502ms step_avg:35.93ms
step:516/1845 train_time:18539ms step_avg:35.93ms
step:517/1845 train_time:18569ms step_avg:35.92ms
step:518/1845 train_time:18607ms step_avg:35.92ms
step:519/1845 train_time:18639ms step_avg:35.91ms
step:520/1845 train_time:18676ms step_avg:35.91ms
step:521/1845 train_time:18709ms step_avg:35.91ms
step:522/1845 train_time:18745ms step_avg:35.91ms
step:523/1845 train_time:18778ms step_avg:35.91ms
step:524/1845 train_time:18816ms step_avg:35.91ms
step:525/1845 train_time:18849ms step_avg:35.90ms
step:526/1845 train_time:18885ms step_avg:35.90ms
step:527/1845 train_time:18917ms step_avg:35.90ms
step:528/1845 train_time:18954ms step_avg:35.90ms
step:529/1845 train_time:18987ms step_avg:35.89ms
step:530/1845 train_time:19023ms step_avg:35.89ms
step:531/1845 train_time:19054ms step_avg:35.88ms
step:532/1845 train_time:19092ms step_avg:35.89ms
step:533/1845 train_time:19122ms step_avg:35.88ms
step:534/1845 train_time:19155ms step_avg:35.87ms
step:535/1845 train_time:19185ms step_avg:35.86ms
step:536/1845 train_time:19222ms step_avg:35.86ms
step:537/1845 train_time:19255ms step_avg:35.86ms
step:538/1845 train_time:19291ms step_avg:35.86ms
step:539/1845 train_time:19323ms step_avg:35.85ms
step:540/1845 train_time:19358ms step_avg:35.85ms
step:541/1845 train_time:19391ms step_avg:35.84ms
step:542/1845 train_time:19427ms step_avg:35.84ms
step:543/1845 train_time:19459ms step_avg:35.84ms
step:544/1845 train_time:19494ms step_avg:35.84ms
step:545/1845 train_time:19526ms step_avg:35.83ms
step:546/1845 train_time:19562ms step_avg:35.83ms
step:547/1845 train_time:19594ms step_avg:35.82ms
step:548/1845 train_time:19631ms step_avg:35.82ms
step:549/1845 train_time:19664ms step_avg:35.82ms
step:550/1845 train_time:19701ms step_avg:35.82ms
step:551/1845 train_time:19735ms step_avg:35.82ms
step:552/1845 train_time:19771ms step_avg:35.82ms
step:553/1845 train_time:19805ms step_avg:35.81ms
step:554/1845 train_time:19842ms step_avg:35.82ms
step:555/1845 train_time:19876ms step_avg:35.81ms
step:556/1845 train_time:19913ms step_avg:35.81ms
step:557/1845 train_time:19947ms step_avg:35.81ms
step:558/1845 train_time:19983ms step_avg:35.81ms
step:559/1845 train_time:20017ms step_avg:35.81ms
step:560/1845 train_time:20054ms step_avg:35.81ms
step:561/1845 train_time:20088ms step_avg:35.81ms
step:562/1845 train_time:20125ms step_avg:35.81ms
step:563/1845 train_time:20155ms step_avg:35.80ms
step:564/1845 train_time:20188ms step_avg:35.79ms
step:565/1845 train_time:20219ms step_avg:35.79ms
step:566/1845 train_time:20254ms step_avg:35.78ms
step:567/1845 train_time:20286ms step_avg:35.78ms
step:568/1845 train_time:20319ms step_avg:35.77ms
step:569/1845 train_time:20348ms step_avg:35.76ms
step:570/1845 train_time:20382ms step_avg:35.76ms
step:571/1845 train_time:20413ms step_avg:35.75ms
step:572/1845 train_time:20448ms step_avg:35.75ms
step:573/1845 train_time:20478ms step_avg:35.74ms
step:574/1845 train_time:20511ms step_avg:35.73ms
step:575/1845 train_time:20540ms step_avg:35.72ms
step:576/1845 train_time:20576ms step_avg:35.72ms
step:577/1845 train_time:20606ms step_avg:35.71ms
step:578/1845 train_time:20644ms step_avg:35.72ms
step:579/1845 train_time:20674ms step_avg:35.71ms
step:580/1845 train_time:20712ms step_avg:35.71ms
step:581/1845 train_time:20741ms step_avg:35.70ms
step:582/1845 train_time:20779ms step_avg:35.70ms
step:583/1845 train_time:20809ms step_avg:35.69ms
step:584/1845 train_time:20847ms step_avg:35.70ms
step:585/1845 train_time:20877ms step_avg:35.69ms
step:586/1845 train_time:20915ms step_avg:35.69ms
step:587/1845 train_time:20944ms step_avg:35.68ms
step:588/1845 train_time:20982ms step_avg:35.68ms
step:589/1845 train_time:21012ms step_avg:35.67ms
step:590/1845 train_time:21050ms step_avg:35.68ms
step:591/1845 train_time:21080ms step_avg:35.67ms
step:592/1845 train_time:21118ms step_avg:35.67ms
step:593/1845 train_time:21148ms step_avg:35.66ms
step:594/1845 train_time:21185ms step_avg:35.67ms
step:595/1845 train_time:21215ms step_avg:35.66ms
step:596/1845 train_time:21253ms step_avg:35.66ms
step:597/1845 train_time:21283ms step_avg:35.65ms
step:598/1845 train_time:21321ms step_avg:35.65ms
step:599/1845 train_time:21351ms step_avg:35.64ms
step:600/1845 train_time:21389ms step_avg:35.65ms
step:601/1845 train_time:21419ms step_avg:35.64ms
step:602/1845 train_time:21457ms step_avg:35.64ms
step:603/1845 train_time:21490ms step_avg:35.64ms
step:604/1845 train_time:21548ms step_avg:35.68ms
step:605/1845 train_time:21608ms step_avg:35.72ms
step:606/1845 train_time:21670ms step_avg:35.76ms
step:607/1845 train_time:21730ms step_avg:35.80ms
step:608/1845 train_time:21794ms step_avg:35.84ms
step:609/1845 train_time:21854ms step_avg:35.89ms
step:610/1845 train_time:21917ms step_avg:35.93ms
step:611/1845 train_time:21977ms step_avg:35.97ms
step:612/1845 train_time:22039ms step_avg:36.01ms
step:613/1845 train_time:22100ms step_avg:36.05ms
step:614/1845 train_time:22161ms step_avg:36.09ms
step:615/1845 train_time:22221ms step_avg:36.13ms
step:616/1845 train_time:22284ms step_avg:36.18ms
step:617/1845 train_time:22344ms step_avg:36.21ms
step:618/1845 train_time:22407ms step_avg:36.26ms
step:619/1845 train_time:22467ms step_avg:36.30ms
step:620/1845 train_time:22529ms step_avg:36.34ms
step:621/1845 train_time:22588ms step_avg:36.37ms
step:622/1845 train_time:22651ms step_avg:36.42ms
step:623/1845 train_time:22711ms step_avg:36.45ms
step:624/1845 train_time:22774ms step_avg:36.50ms
step:625/1845 train_time:22834ms step_avg:36.53ms
step:626/1845 train_time:22896ms step_avg:36.58ms
step:627/1845 train_time:22956ms step_avg:36.61ms
step:628/1845 train_time:23018ms step_avg:36.65ms
step:629/1845 train_time:23079ms step_avg:36.69ms
step:630/1845 train_time:23141ms step_avg:36.73ms
step:631/1845 train_time:23201ms step_avg:36.77ms
step:632/1845 train_time:23264ms step_avg:36.81ms
step:633/1845 train_time:23324ms step_avg:36.85ms
step:634/1845 train_time:23387ms step_avg:36.89ms
step:635/1845 train_time:23447ms step_avg:36.92ms
step:636/1845 train_time:23510ms step_avg:36.97ms
step:637/1845 train_time:23570ms step_avg:37.00ms
step:638/1845 train_time:23633ms step_avg:37.04ms
step:639/1845 train_time:23693ms step_avg:37.08ms
step:640/1845 train_time:23756ms step_avg:37.12ms
step:641/1845 train_time:23816ms step_avg:37.15ms
step:642/1845 train_time:23879ms step_avg:37.19ms
step:643/1845 train_time:23939ms step_avg:37.23ms
step:644/1845 train_time:24001ms step_avg:37.27ms
step:645/1845 train_time:24061ms step_avg:37.30ms
step:646/1845 train_time:24123ms step_avg:37.34ms
step:647/1845 train_time:24184ms step_avg:37.38ms
step:648/1845 train_time:24246ms step_avg:37.42ms
step:649/1845 train_time:24306ms step_avg:37.45ms
step:650/1845 train_time:24369ms step_avg:37.49ms
step:651/1845 train_time:24428ms step_avg:37.52ms
step:652/1845 train_time:24491ms step_avg:37.56ms
step:653/1845 train_time:24551ms step_avg:37.60ms
step:654/1845 train_time:24614ms step_avg:37.64ms
step:655/1845 train_time:24673ms step_avg:37.67ms
step:656/1845 train_time:24736ms step_avg:37.71ms
step:657/1845 train_time:24796ms step_avg:37.74ms
step:658/1845 train_time:24858ms step_avg:37.78ms
step:659/1845 train_time:24918ms step_avg:37.81ms
step:660/1845 train_time:24980ms step_avg:37.85ms
step:661/1845 train_time:25040ms step_avg:37.88ms
step:662/1845 train_time:25103ms step_avg:37.92ms
step:663/1845 train_time:25163ms step_avg:37.95ms
step:664/1845 train_time:25226ms step_avg:37.99ms
step:665/1845 train_time:25286ms step_avg:38.02ms
step:666/1845 train_time:25348ms step_avg:38.06ms
step:667/1845 train_time:25407ms step_avg:38.09ms
step:668/1845 train_time:25470ms step_avg:38.13ms
step:669/1845 train_time:25530ms step_avg:38.16ms
step:670/1845 train_time:25593ms step_avg:38.20ms
step:671/1845 train_time:25654ms step_avg:38.23ms
step:672/1845 train_time:25717ms step_avg:38.27ms
step:673/1845 train_time:25777ms step_avg:38.30ms
step:674/1845 train_time:25840ms step_avg:38.34ms
step:675/1845 train_time:25900ms step_avg:38.37ms
step:676/1845 train_time:25961ms step_avg:38.40ms
step:677/1845 train_time:26021ms step_avg:38.44ms
step:678/1845 train_time:26084ms step_avg:38.47ms
step:679/1845 train_time:26144ms step_avg:38.50ms
step:680/1845 train_time:26207ms step_avg:38.54ms
step:681/1845 train_time:26266ms step_avg:38.57ms
step:682/1845 train_time:26329ms step_avg:38.61ms
step:683/1845 train_time:26389ms step_avg:38.64ms
step:684/1845 train_time:26452ms step_avg:38.67ms
step:685/1845 train_time:26512ms step_avg:38.70ms
step:686/1845 train_time:26574ms step_avg:38.74ms
step:687/1845 train_time:26634ms step_avg:38.77ms
step:688/1845 train_time:26697ms step_avg:38.80ms
step:689/1845 train_time:26758ms step_avg:38.84ms
step:690/1845 train_time:26820ms step_avg:38.87ms
step:691/1845 train_time:26880ms step_avg:38.90ms
step:692/1845 train_time:26942ms step_avg:38.93ms
step:693/1845 train_time:27002ms step_avg:38.96ms
step:694/1845 train_time:27064ms step_avg:39.00ms
step:695/1845 train_time:27124ms step_avg:39.03ms
step:696/1845 train_time:27187ms step_avg:39.06ms
step:697/1845 train_time:27246ms step_avg:39.09ms
step:698/1845 train_time:27310ms step_avg:39.13ms
step:699/1845 train_time:27370ms step_avg:39.16ms
step:700/1845 train_time:27433ms step_avg:39.19ms
step:701/1845 train_time:27492ms step_avg:39.22ms
step:702/1845 train_time:27555ms step_avg:39.25ms
step:703/1845 train_time:27614ms step_avg:39.28ms
step:704/1845 train_time:27677ms step_avg:39.31ms
step:705/1845 train_time:27737ms step_avg:39.34ms
step:706/1845 train_time:27800ms step_avg:39.38ms
step:707/1845 train_time:27860ms step_avg:39.41ms
step:708/1845 train_time:27922ms step_avg:39.44ms
step:709/1845 train_time:27982ms step_avg:39.47ms
step:710/1845 train_time:28044ms step_avg:39.50ms
step:711/1845 train_time:28104ms step_avg:39.53ms
step:712/1845 train_time:28167ms step_avg:39.56ms
step:713/1845 train_time:28226ms step_avg:39.59ms
step:714/1845 train_time:28289ms step_avg:39.62ms
step:715/1845 train_time:28349ms step_avg:39.65ms
step:716/1845 train_time:28411ms step_avg:39.68ms
step:717/1845 train_time:28471ms step_avg:39.71ms
step:718/1845 train_time:28533ms step_avg:39.74ms
step:719/1845 train_time:28593ms step_avg:39.77ms
step:720/1845 train_time:28655ms step_avg:39.80ms
step:721/1845 train_time:28715ms step_avg:39.83ms
step:722/1845 train_time:28778ms step_avg:39.86ms
step:723/1845 train_time:28838ms step_avg:39.89ms
step:724/1845 train_time:28902ms step_avg:39.92ms
step:725/1845 train_time:28961ms step_avg:39.95ms
step:726/1845 train_time:29024ms step_avg:39.98ms
step:727/1845 train_time:29085ms step_avg:40.01ms
step:728/1845 train_time:29147ms step_avg:40.04ms
step:729/1845 train_time:29206ms step_avg:40.06ms
step:730/1845 train_time:29269ms step_avg:40.09ms
step:731/1845 train_time:29329ms step_avg:40.12ms
step:732/1845 train_time:29391ms step_avg:40.15ms
step:733/1845 train_time:29452ms step_avg:40.18ms
step:734/1845 train_time:29515ms step_avg:40.21ms
step:735/1845 train_time:29574ms step_avg:40.24ms
step:736/1845 train_time:29637ms step_avg:40.27ms
step:737/1845 train_time:29697ms step_avg:40.29ms
step:738/1845 train_time:29760ms step_avg:40.32ms
step:739/1845 train_time:29820ms step_avg:40.35ms
step:740/1845 train_time:29882ms step_avg:40.38ms
step:741/1845 train_time:29943ms step_avg:40.41ms
step:742/1845 train_time:30005ms step_avg:40.44ms
step:743/1845 train_time:30064ms step_avg:40.46ms
step:744/1845 train_time:30127ms step_avg:40.49ms
step:745/1845 train_time:30187ms step_avg:40.52ms
step:746/1845 train_time:30250ms step_avg:40.55ms
step:747/1845 train_time:30310ms step_avg:40.58ms
step:748/1845 train_time:30373ms step_avg:40.60ms
step:749/1845 train_time:30433ms step_avg:40.63ms
step:750/1845 train_time:30495ms step_avg:40.66ms
step:750/1845 val_loss:4.0347 train_time:30565ms step_avg:40.75ms
step:751/1845 train_time:30589ms step_avg:40.73ms
step:752/1845 train_time:30619ms step_avg:40.72ms
step:753/1845 train_time:30679ms step_avg:40.74ms
step:754/1845 train_time:30743ms step_avg:40.77ms
step:755/1845 train_time:30804ms step_avg:40.80ms
step:756/1845 train_time:30867ms step_avg:40.83ms
step:757/1845 train_time:30928ms step_avg:40.86ms
step:758/1845 train_time:30990ms step_avg:40.88ms
step:759/1845 train_time:31049ms step_avg:40.91ms
step:760/1845 train_time:31111ms step_avg:40.94ms
step:761/1845 train_time:31170ms step_avg:40.96ms
step:762/1845 train_time:31232ms step_avg:40.99ms
step:763/1845 train_time:31291ms step_avg:41.01ms
step:764/1845 train_time:31354ms step_avg:41.04ms
step:765/1845 train_time:31413ms step_avg:41.06ms
step:766/1845 train_time:31477ms step_avg:41.09ms
step:767/1845 train_time:31538ms step_avg:41.12ms
step:768/1845 train_time:31602ms step_avg:41.15ms
step:769/1845 train_time:31662ms step_avg:41.17ms
step:770/1845 train_time:31725ms step_avg:41.20ms
step:771/1845 train_time:31786ms step_avg:41.23ms
step:772/1845 train_time:31849ms step_avg:41.26ms
step:773/1845 train_time:31909ms step_avg:41.28ms
step:774/1845 train_time:31971ms step_avg:41.31ms
step:775/1845 train_time:32031ms step_avg:41.33ms
step:776/1845 train_time:32093ms step_avg:41.36ms
step:777/1845 train_time:32153ms step_avg:41.38ms
step:778/1845 train_time:32215ms step_avg:41.41ms
step:779/1845 train_time:32275ms step_avg:41.43ms
step:780/1845 train_time:32337ms step_avg:41.46ms
step:781/1845 train_time:32397ms step_avg:41.48ms
step:782/1845 train_time:32460ms step_avg:41.51ms
step:783/1845 train_time:32520ms step_avg:41.53ms
step:784/1845 train_time:32583ms step_avg:41.56ms
step:785/1845 train_time:32643ms step_avg:41.58ms
step:786/1845 train_time:32706ms step_avg:41.61ms
step:787/1845 train_time:32767ms step_avg:41.64ms
step:788/1845 train_time:32830ms step_avg:41.66ms
step:789/1845 train_time:32890ms step_avg:41.69ms
step:790/1845 train_time:32953ms step_avg:41.71ms
step:791/1845 train_time:33012ms step_avg:41.74ms
step:792/1845 train_time:33075ms step_avg:41.76ms
step:793/1845 train_time:33134ms step_avg:41.78ms
step:794/1845 train_time:33197ms step_avg:41.81ms
step:795/1845 train_time:33256ms step_avg:41.83ms
step:796/1845 train_time:33319ms step_avg:41.86ms
step:797/1845 train_time:33379ms step_avg:41.88ms
step:798/1845 train_time:33442ms step_avg:41.91ms
step:799/1845 train_time:33503ms step_avg:41.93ms
step:800/1845 train_time:33565ms step_avg:41.96ms
step:801/1845 train_time:33625ms step_avg:41.98ms
step:802/1845 train_time:33687ms step_avg:42.00ms
step:803/1845 train_time:33747ms step_avg:42.03ms
step:804/1845 train_time:33810ms step_avg:42.05ms
step:805/1845 train_time:33870ms step_avg:42.07ms
step:806/1845 train_time:33933ms step_avg:42.10ms
step:807/1845 train_time:33992ms step_avg:42.12ms
step:808/1845 train_time:34055ms step_avg:42.15ms
step:809/1845 train_time:34115ms step_avg:42.17ms
step:810/1845 train_time:34177ms step_avg:42.19ms
step:811/1845 train_time:34237ms step_avg:42.22ms
step:812/1845 train_time:34300ms step_avg:42.24ms
step:813/1845 train_time:34360ms step_avg:42.26ms
step:814/1845 train_time:34422ms step_avg:42.29ms
step:815/1845 train_time:34483ms step_avg:42.31ms
step:816/1845 train_time:34546ms step_avg:42.34ms
step:817/1845 train_time:34606ms step_avg:42.36ms
step:818/1845 train_time:34668ms step_avg:42.38ms
step:819/1845 train_time:34728ms step_avg:42.40ms
step:820/1845 train_time:34791ms step_avg:42.43ms
step:821/1845 train_time:34851ms step_avg:42.45ms
step:822/1845 train_time:34914ms step_avg:42.47ms
step:823/1845 train_time:34974ms step_avg:42.50ms
step:824/1845 train_time:35036ms step_avg:42.52ms
step:825/1845 train_time:35096ms step_avg:42.54ms
step:826/1845 train_time:35159ms step_avg:42.57ms
step:827/1845 train_time:35219ms step_avg:42.59ms
step:828/1845 train_time:35282ms step_avg:42.61ms
step:829/1845 train_time:35342ms step_avg:42.63ms
step:830/1845 train_time:35404ms step_avg:42.66ms
step:831/1845 train_time:35464ms step_avg:42.68ms
step:832/1845 train_time:35527ms step_avg:42.70ms
step:833/1845 train_time:35587ms step_avg:42.72ms
step:834/1845 train_time:35650ms step_avg:42.75ms
step:835/1845 train_time:35710ms step_avg:42.77ms
step:836/1845 train_time:35773ms step_avg:42.79ms
step:837/1845 train_time:35833ms step_avg:42.81ms
step:838/1845 train_time:35895ms step_avg:42.83ms
step:839/1845 train_time:35956ms step_avg:42.86ms
step:840/1845 train_time:36018ms step_avg:42.88ms
step:841/1845 train_time:36078ms step_avg:42.90ms
step:842/1845 train_time:36141ms step_avg:42.92ms
step:843/1845 train_time:36201ms step_avg:42.94ms
step:844/1845 train_time:36263ms step_avg:42.97ms
step:845/1845 train_time:36323ms step_avg:42.99ms
step:846/1845 train_time:36386ms step_avg:43.01ms
step:847/1845 train_time:36446ms step_avg:43.03ms
step:848/1845 train_time:36508ms step_avg:43.05ms
step:849/1845 train_time:36569ms step_avg:43.07ms
step:850/1845 train_time:36631ms step_avg:43.10ms
step:851/1845 train_time:36692ms step_avg:43.12ms
step:852/1845 train_time:36754ms step_avg:43.14ms
step:853/1845 train_time:36815ms step_avg:43.16ms
step:854/1845 train_time:36877ms step_avg:43.18ms
step:855/1845 train_time:36938ms step_avg:43.20ms
step:856/1845 train_time:37000ms step_avg:43.22ms
step:857/1845 train_time:37061ms step_avg:43.24ms
step:858/1845 train_time:37123ms step_avg:43.27ms
step:859/1845 train_time:37183ms step_avg:43.29ms
step:860/1845 train_time:37246ms step_avg:43.31ms
step:861/1845 train_time:37305ms step_avg:43.33ms
step:862/1845 train_time:37368ms step_avg:43.35ms
step:863/1845 train_time:37428ms step_avg:43.37ms
step:864/1845 train_time:37490ms step_avg:43.39ms
step:865/1845 train_time:37550ms step_avg:43.41ms
step:866/1845 train_time:37613ms step_avg:43.43ms
step:867/1845 train_time:37673ms step_avg:43.45ms
step:868/1845 train_time:37736ms step_avg:43.47ms
step:869/1845 train_time:37796ms step_avg:43.49ms
step:870/1845 train_time:37859ms step_avg:43.52ms
step:871/1845 train_time:37918ms step_avg:43.53ms
step:872/1845 train_time:37981ms step_avg:43.56ms
step:873/1845 train_time:38042ms step_avg:43.58ms
step:874/1845 train_time:38104ms step_avg:43.60ms
step:875/1845 train_time:38164ms step_avg:43.62ms
step:876/1845 train_time:38226ms step_avg:43.64ms
step:877/1845 train_time:38286ms step_avg:43.66ms
step:878/1845 train_time:38348ms step_avg:43.68ms
step:879/1845 train_time:38408ms step_avg:43.69ms
step:880/1845 train_time:38471ms step_avg:43.72ms
step:881/1845 train_time:38530ms step_avg:43.73ms
step:882/1845 train_time:38592ms step_avg:43.76ms
step:883/1845 train_time:38653ms step_avg:43.77ms
step:884/1845 train_time:38715ms step_avg:43.80ms
step:885/1845 train_time:38775ms step_avg:43.81ms
step:886/1845 train_time:38838ms step_avg:43.84ms
step:887/1845 train_time:38898ms step_avg:43.85ms
step:888/1845 train_time:38961ms step_avg:43.88ms
step:889/1845 train_time:39022ms step_avg:43.89ms
step:890/1845 train_time:39084ms step_avg:43.91ms
step:891/1845 train_time:39145ms step_avg:43.93ms
step:892/1845 train_time:39207ms step_avg:43.95ms
step:893/1845 train_time:39267ms step_avg:43.97ms
step:894/1845 train_time:39330ms step_avg:43.99ms
step:895/1845 train_time:39390ms step_avg:44.01ms
step:896/1845 train_time:39452ms step_avg:44.03ms
step:897/1845 train_time:39512ms step_avg:44.05ms
step:898/1845 train_time:39575ms step_avg:44.07ms
step:899/1845 train_time:39635ms step_avg:44.09ms
step:900/1845 train_time:39698ms step_avg:44.11ms
step:901/1845 train_time:39758ms step_avg:44.13ms
step:902/1845 train_time:39820ms step_avg:44.15ms
step:903/1845 train_time:39880ms step_avg:44.16ms
step:904/1845 train_time:39942ms step_avg:44.18ms
step:905/1845 train_time:40003ms step_avg:44.20ms
step:906/1845 train_time:40065ms step_avg:44.22ms
step:907/1845 train_time:40126ms step_avg:44.24ms
step:908/1845 train_time:40188ms step_avg:44.26ms
step:909/1845 train_time:40248ms step_avg:44.28ms
step:910/1845 train_time:40310ms step_avg:44.30ms
step:911/1845 train_time:40370ms step_avg:44.31ms
step:912/1845 train_time:40433ms step_avg:44.33ms
step:913/1845 train_time:40493ms step_avg:44.35ms
step:914/1845 train_time:40555ms step_avg:44.37ms
step:915/1845 train_time:40615ms step_avg:44.39ms
step:916/1845 train_time:40678ms step_avg:44.41ms
step:917/1845 train_time:40738ms step_avg:44.43ms
step:918/1845 train_time:40801ms step_avg:44.45ms
step:919/1845 train_time:40861ms step_avg:44.46ms
step:920/1845 train_time:40923ms step_avg:44.48ms
step:921/1845 train_time:40982ms step_avg:44.50ms
step:922/1845 train_time:41046ms step_avg:44.52ms
step:923/1845 train_time:41106ms step_avg:44.53ms
step:924/1845 train_time:41168ms step_avg:44.55ms
step:925/1845 train_time:41229ms step_avg:44.57ms
step:926/1845 train_time:41291ms step_avg:44.59ms
step:927/1845 train_time:41351ms step_avg:44.61ms
step:928/1845 train_time:41413ms step_avg:44.63ms
step:929/1845 train_time:41472ms step_avg:44.64ms
step:930/1845 train_time:41535ms step_avg:44.66ms
step:931/1845 train_time:41596ms step_avg:44.68ms
step:932/1845 train_time:41659ms step_avg:44.70ms
step:933/1845 train_time:41719ms step_avg:44.71ms
step:934/1845 train_time:41782ms step_avg:44.73ms
step:935/1845 train_time:41842ms step_avg:44.75ms
step:936/1845 train_time:41903ms step_avg:44.77ms
step:937/1845 train_time:41963ms step_avg:44.78ms
step:938/1845 train_time:42026ms step_avg:44.80ms
step:939/1845 train_time:42086ms step_avg:44.82ms
step:940/1845 train_time:42149ms step_avg:44.84ms
step:941/1845 train_time:42209ms step_avg:44.86ms
step:942/1845 train_time:42271ms step_avg:44.87ms
step:943/1845 train_time:42331ms step_avg:44.89ms
step:944/1845 train_time:42394ms step_avg:44.91ms
step:945/1845 train_time:42454ms step_avg:44.92ms
step:946/1845 train_time:42516ms step_avg:44.94ms
step:947/1845 train_time:42576ms step_avg:44.96ms
step:948/1845 train_time:42638ms step_avg:44.98ms
step:949/1845 train_time:42698ms step_avg:44.99ms
step:950/1845 train_time:42761ms step_avg:45.01ms
step:951/1845 train_time:42821ms step_avg:45.03ms
step:952/1845 train_time:42882ms step_avg:45.04ms
step:953/1845 train_time:42943ms step_avg:45.06ms
step:954/1845 train_time:43005ms step_avg:45.08ms
step:955/1845 train_time:43066ms step_avg:45.09ms
step:956/1845 train_time:43128ms step_avg:45.11ms
step:957/1845 train_time:43189ms step_avg:45.13ms
step:958/1845 train_time:43252ms step_avg:45.15ms
step:959/1845 train_time:43311ms step_avg:45.16ms
step:960/1845 train_time:43374ms step_avg:45.18ms
step:961/1845 train_time:43434ms step_avg:45.20ms
step:962/1845 train_time:43497ms step_avg:45.21ms
step:963/1845 train_time:43557ms step_avg:45.23ms
step:964/1845 train_time:43619ms step_avg:45.25ms
step:965/1845 train_time:43680ms step_avg:45.26ms
step:966/1845 train_time:43742ms step_avg:45.28ms
step:967/1845 train_time:43803ms step_avg:45.30ms
step:968/1845 train_time:43865ms step_avg:45.32ms
step:969/1845 train_time:43925ms step_avg:45.33ms
step:970/1845 train_time:43987ms step_avg:45.35ms
step:971/1845 train_time:44048ms step_avg:45.36ms
step:972/1845 train_time:44110ms step_avg:45.38ms
step:973/1845 train_time:44170ms step_avg:45.40ms
step:974/1845 train_time:44233ms step_avg:45.41ms
step:975/1845 train_time:44292ms step_avg:45.43ms
step:976/1845 train_time:44355ms step_avg:45.45ms
step:977/1845 train_time:44416ms step_avg:45.46ms
step:978/1845 train_time:44478ms step_avg:45.48ms
step:979/1845 train_time:44538ms step_avg:45.49ms
step:980/1845 train_time:44600ms step_avg:45.51ms
step:981/1845 train_time:44660ms step_avg:45.53ms
step:982/1845 train_time:44723ms step_avg:45.54ms
step:983/1845 train_time:44783ms step_avg:45.56ms
step:984/1845 train_time:44845ms step_avg:45.57ms
step:985/1845 train_time:44905ms step_avg:45.59ms
step:986/1845 train_time:44967ms step_avg:45.61ms
step:987/1845 train_time:45027ms step_avg:45.62ms
step:988/1845 train_time:45090ms step_avg:45.64ms
step:989/1845 train_time:45149ms step_avg:45.65ms
step:990/1845 train_time:45212ms step_avg:45.67ms
step:991/1845 train_time:45271ms step_avg:45.68ms
step:992/1845 train_time:45334ms step_avg:45.70ms
step:993/1845 train_time:45395ms step_avg:45.71ms
step:994/1845 train_time:45457ms step_avg:45.73ms
step:995/1845 train_time:45517ms step_avg:45.75ms
step:996/1845 train_time:45580ms step_avg:45.76ms
step:997/1845 train_time:45641ms step_avg:45.78ms
step:998/1845 train_time:45703ms step_avg:45.79ms
step:999/1845 train_time:45763ms step_avg:45.81ms
step:1000/1845 train_time:45826ms step_avg:45.83ms
step:1000/1845 val_loss:3.7818 train_time:45895ms step_avg:45.89ms
step:1001/1845 train_time:45921ms step_avg:45.87ms
step:1002/1845 train_time:45948ms step_avg:45.86ms
step:1003/1845 train_time:46007ms step_avg:45.87ms
step:1004/1845 train_time:46072ms step_avg:45.89ms
step:1005/1845 train_time:46132ms step_avg:45.90ms
step:1006/1845 train_time:46195ms step_avg:45.92ms
step:1007/1845 train_time:46254ms step_avg:45.93ms
step:1008/1845 train_time:46316ms step_avg:45.95ms
step:1009/1845 train_time:46375ms step_avg:45.96ms
step:1010/1845 train_time:46437ms step_avg:45.98ms
step:1011/1845 train_time:46497ms step_avg:45.99ms
step:1012/1845 train_time:46559ms step_avg:46.01ms
step:1013/1845 train_time:46618ms step_avg:46.02ms
step:1014/1845 train_time:46680ms step_avg:46.04ms
step:1015/1845 train_time:46740ms step_avg:46.05ms
step:1016/1845 train_time:46805ms step_avg:46.07ms
step:1017/1845 train_time:46868ms step_avg:46.09ms
step:1018/1845 train_time:46932ms step_avg:46.10ms
step:1019/1845 train_time:46993ms step_avg:46.12ms
step:1020/1845 train_time:47055ms step_avg:46.13ms
step:1021/1845 train_time:47116ms step_avg:46.15ms
step:1022/1845 train_time:47179ms step_avg:46.16ms
step:1023/1845 train_time:47238ms step_avg:46.18ms
step:1024/1845 train_time:47300ms step_avg:46.19ms
step:1025/1845 train_time:47359ms step_avg:46.20ms
step:1026/1845 train_time:47422ms step_avg:46.22ms
step:1027/1845 train_time:47482ms step_avg:46.23ms
step:1028/1845 train_time:47544ms step_avg:46.25ms
step:1029/1845 train_time:47604ms step_avg:46.26ms
step:1030/1845 train_time:47667ms step_avg:46.28ms
step:1031/1845 train_time:47727ms step_avg:46.29ms
step:1032/1845 train_time:47789ms step_avg:46.31ms
step:1033/1845 train_time:47850ms step_avg:46.32ms
step:1034/1845 train_time:47913ms step_avg:46.34ms
step:1035/1845 train_time:47973ms step_avg:46.35ms
step:1036/1845 train_time:48036ms step_avg:46.37ms
step:1037/1845 train_time:48096ms step_avg:46.38ms
step:1038/1845 train_time:48159ms step_avg:46.40ms
step:1039/1845 train_time:48219ms step_avg:46.41ms
step:1040/1845 train_time:48281ms step_avg:46.42ms
step:1041/1845 train_time:48340ms step_avg:46.44ms
step:1042/1845 train_time:48403ms step_avg:46.45ms
step:1043/1845 train_time:48463ms step_avg:46.47ms
step:1044/1845 train_time:48525ms step_avg:46.48ms
step:1045/1845 train_time:48586ms step_avg:46.49ms
step:1046/1845 train_time:48648ms step_avg:46.51ms
step:1047/1845 train_time:48708ms step_avg:46.52ms
step:1048/1845 train_time:48772ms step_avg:46.54ms
step:1049/1845 train_time:48832ms step_avg:46.55ms
step:1050/1845 train_time:48895ms step_avg:46.57ms
step:1051/1845 train_time:48955ms step_avg:46.58ms
step:1052/1845 train_time:49018ms step_avg:46.60ms
step:1053/1845 train_time:49078ms step_avg:46.61ms
step:1054/1845 train_time:49141ms step_avg:46.62ms
step:1055/1845 train_time:49201ms step_avg:46.64ms
step:1056/1845 train_time:49263ms step_avg:46.65ms
step:1057/1845 train_time:49323ms step_avg:46.66ms
step:1058/1845 train_time:49385ms step_avg:46.68ms
step:1059/1845 train_time:49445ms step_avg:46.69ms
step:1060/1845 train_time:49509ms step_avg:46.71ms
step:1061/1845 train_time:49568ms step_avg:46.72ms
step:1062/1845 train_time:49631ms step_avg:46.73ms
step:1063/1845 train_time:49690ms step_avg:46.75ms
step:1064/1845 train_time:49753ms step_avg:46.76ms
step:1065/1845 train_time:49813ms step_avg:46.77ms
step:1066/1845 train_time:49875ms step_avg:46.79ms
step:1067/1845 train_time:49935ms step_avg:46.80ms
step:1068/1845 train_time:49999ms step_avg:46.82ms
step:1069/1845 train_time:50059ms step_avg:46.83ms
step:1070/1845 train_time:50122ms step_avg:46.84ms
step:1071/1845 train_time:50182ms step_avg:46.86ms
step:1072/1845 train_time:50244ms step_avg:46.87ms
step:1073/1845 train_time:50304ms step_avg:46.88ms
step:1074/1845 train_time:50366ms step_avg:46.90ms
step:1075/1845 train_time:50426ms step_avg:46.91ms
step:1076/1845 train_time:50490ms step_avg:46.92ms
step:1077/1845 train_time:50549ms step_avg:46.94ms
step:1078/1845 train_time:50612ms step_avg:46.95ms
step:1079/1845 train_time:50671ms step_avg:46.96ms
step:1080/1845 train_time:50733ms step_avg:46.98ms
step:1081/1845 train_time:50793ms step_avg:46.99ms
step:1082/1845 train_time:50855ms step_avg:47.00ms
step:1083/1845 train_time:50916ms step_avg:47.01ms
step:1084/1845 train_time:50979ms step_avg:47.03ms
step:1085/1845 train_time:51039ms step_avg:47.04ms
step:1086/1845 train_time:51102ms step_avg:47.06ms
step:1087/1845 train_time:51162ms step_avg:47.07ms
step:1088/1845 train_time:51225ms step_avg:47.08ms
step:1089/1845 train_time:51284ms step_avg:47.09ms
step:1090/1845 train_time:51347ms step_avg:47.11ms
step:1091/1845 train_time:51406ms step_avg:47.12ms
step:1092/1845 train_time:51469ms step_avg:47.13ms
step:1093/1845 train_time:51530ms step_avg:47.15ms
step:1094/1845 train_time:51592ms step_avg:47.16ms
step:1095/1845 train_time:51651ms step_avg:47.17ms
step:1096/1845 train_time:51715ms step_avg:47.19ms
step:1097/1845 train_time:51775ms step_avg:47.20ms
step:1098/1845 train_time:51837ms step_avg:47.21ms
step:1099/1845 train_time:51898ms step_avg:47.22ms
step:1100/1845 train_time:51960ms step_avg:47.24ms
step:1101/1845 train_time:52020ms step_avg:47.25ms
step:1102/1845 train_time:52082ms step_avg:47.26ms
step:1103/1845 train_time:52143ms step_avg:47.27ms
step:1104/1845 train_time:52205ms step_avg:47.29ms
step:1105/1845 train_time:52265ms step_avg:47.30ms
step:1106/1845 train_time:52327ms step_avg:47.31ms
step:1107/1845 train_time:52387ms step_avg:47.32ms
step:1108/1845 train_time:52449ms step_avg:47.34ms
step:1109/1845 train_time:52509ms step_avg:47.35ms
step:1110/1845 train_time:52572ms step_avg:47.36ms
step:1111/1845 train_time:52632ms step_avg:47.37ms
step:1112/1845 train_time:52694ms step_avg:47.39ms
step:1113/1845 train_time:52753ms step_avg:47.40ms
step:1114/1845 train_time:52816ms step_avg:47.41ms
step:1115/1845 train_time:52877ms step_avg:47.42ms
step:1116/1845 train_time:52940ms step_avg:47.44ms
step:1117/1845 train_time:53000ms step_avg:47.45ms
step:1118/1845 train_time:53063ms step_avg:47.46ms
step:1119/1845 train_time:53123ms step_avg:47.47ms
step:1120/1845 train_time:53185ms step_avg:47.49ms
step:1121/1845 train_time:53245ms step_avg:47.50ms
step:1122/1845 train_time:53308ms step_avg:47.51ms
step:1123/1845 train_time:53368ms step_avg:47.52ms
step:1124/1845 train_time:53431ms step_avg:47.54ms
step:1125/1845 train_time:53491ms step_avg:47.55ms
step:1126/1845 train_time:53553ms step_avg:47.56ms
step:1127/1845 train_time:53613ms step_avg:47.57ms
step:1128/1845 train_time:53675ms step_avg:47.58ms
step:1129/1845 train_time:53735ms step_avg:47.59ms
step:1130/1845 train_time:53797ms step_avg:47.61ms
step:1131/1845 train_time:53857ms step_avg:47.62ms
step:1132/1845 train_time:53920ms step_avg:47.63ms
step:1133/1845 train_time:53979ms step_avg:47.64ms
step:1134/1845 train_time:54043ms step_avg:47.66ms
step:1135/1845 train_time:54103ms step_avg:47.67ms
step:1136/1845 train_time:54165ms step_avg:47.68ms
step:1137/1845 train_time:54225ms step_avg:47.69ms
step:1138/1845 train_time:54287ms step_avg:47.70ms
step:1139/1845 train_time:54347ms step_avg:47.71ms
step:1140/1845 train_time:54409ms step_avg:47.73ms
step:1141/1845 train_time:54470ms step_avg:47.74ms
step:1142/1845 train_time:54532ms step_avg:47.75ms
step:1143/1845 train_time:54592ms step_avg:47.76ms
step:1144/1845 train_time:54653ms step_avg:47.77ms
step:1145/1845 train_time:54713ms step_avg:47.78ms
step:1146/1845 train_time:54776ms step_avg:47.80ms
step:1147/1845 train_time:54836ms step_avg:47.81ms
step:1148/1845 train_time:54899ms step_avg:47.82ms
step:1149/1845 train_time:54959ms step_avg:47.83ms
step:1150/1845 train_time:55022ms step_avg:47.85ms
step:1151/1845 train_time:55083ms step_avg:47.86ms
step:1152/1845 train_time:55145ms step_avg:47.87ms
step:1153/1845 train_time:55205ms step_avg:47.88ms
step:1154/1845 train_time:55267ms step_avg:47.89ms
step:1155/1845 train_time:55327ms step_avg:47.90ms
step:1156/1845 train_time:55390ms step_avg:47.92ms
step:1157/1845 train_time:55450ms step_avg:47.93ms
step:1158/1845 train_time:55513ms step_avg:47.94ms
step:1159/1845 train_time:55572ms step_avg:47.95ms
step:1160/1845 train_time:55635ms step_avg:47.96ms
step:1161/1845 train_time:55694ms step_avg:47.97ms
step:1162/1845 train_time:55756ms step_avg:47.98ms
step:1163/1845 train_time:55817ms step_avg:47.99ms
step:1164/1845 train_time:55879ms step_avg:48.01ms
step:1165/1845 train_time:55940ms step_avg:48.02ms
step:1166/1845 train_time:56002ms step_avg:48.03ms
step:1167/1845 train_time:56062ms step_avg:48.04ms
step:1168/1845 train_time:56125ms step_avg:48.05ms
step:1169/1845 train_time:56185ms step_avg:48.06ms
step:1170/1845 train_time:56247ms step_avg:48.07ms
step:1171/1845 train_time:56307ms step_avg:48.08ms
step:1172/1845 train_time:56370ms step_avg:48.10ms
step:1173/1845 train_time:56429ms step_avg:48.11ms
step:1174/1845 train_time:56492ms step_avg:48.12ms
step:1175/1845 train_time:56552ms step_avg:48.13ms
step:1176/1845 train_time:56615ms step_avg:48.14ms
step:1177/1845 train_time:56674ms step_avg:48.15ms
step:1178/1845 train_time:56736ms step_avg:48.16ms
step:1179/1845 train_time:56796ms step_avg:48.17ms
step:1180/1845 train_time:56859ms step_avg:48.19ms
step:1181/1845 train_time:56919ms step_avg:48.20ms
step:1182/1845 train_time:56981ms step_avg:48.21ms
step:1183/1845 train_time:57042ms step_avg:48.22ms
step:1184/1845 train_time:57105ms step_avg:48.23ms
step:1185/1845 train_time:57166ms step_avg:48.24ms
step:1186/1845 train_time:57228ms step_avg:48.25ms
step:1187/1845 train_time:57288ms step_avg:48.26ms
step:1188/1845 train_time:57349ms step_avg:48.27ms
step:1189/1845 train_time:57410ms step_avg:48.28ms
step:1190/1845 train_time:57472ms step_avg:48.30ms
step:1191/1845 train_time:57532ms step_avg:48.31ms
step:1192/1845 train_time:57595ms step_avg:48.32ms
step:1193/1845 train_time:57655ms step_avg:48.33ms
step:1194/1845 train_time:57718ms step_avg:48.34ms
step:1195/1845 train_time:57777ms step_avg:48.35ms
step:1196/1845 train_time:57840ms step_avg:48.36ms
step:1197/1845 train_time:57900ms step_avg:48.37ms
step:1198/1845 train_time:57963ms step_avg:48.38ms
step:1199/1845 train_time:58022ms step_avg:48.39ms
step:1200/1845 train_time:58086ms step_avg:48.40ms
step:1201/1845 train_time:58145ms step_avg:48.41ms
step:1202/1845 train_time:58208ms step_avg:48.43ms
step:1203/1845 train_time:58268ms step_avg:48.44ms
step:1204/1845 train_time:58330ms step_avg:48.45ms
step:1205/1845 train_time:58391ms step_avg:48.46ms
step:1206/1845 train_time:58478ms step_avg:48.49ms
step:1207/1845 train_time:58565ms step_avg:48.52ms
step:1208/1845 train_time:58654ms step_avg:48.55ms
step:1209/1845 train_time:58741ms step_avg:48.59ms
step:1210/1845 train_time:58832ms step_avg:48.62ms
step:1211/1845 train_time:58919ms step_avg:48.65ms
step:1212/1845 train_time:59010ms step_avg:48.69ms
step:1213/1845 train_time:59097ms step_avg:48.72ms
step:1214/1845 train_time:59186ms step_avg:48.75ms
step:1215/1845 train_time:59273ms step_avg:48.78ms
step:1216/1845 train_time:59361ms step_avg:48.82ms
step:1217/1845 train_time:59447ms step_avg:48.85ms
step:1218/1845 train_time:59535ms step_avg:48.88ms
step:1219/1845 train_time:59621ms step_avg:48.91ms
step:1220/1845 train_time:59711ms step_avg:48.94ms
step:1221/1845 train_time:59797ms step_avg:48.97ms
step:1222/1845 train_time:59888ms step_avg:49.01ms
step:1223/1845 train_time:59975ms step_avg:49.04ms
step:1224/1845 train_time:60063ms step_avg:49.07ms
step:1225/1845 train_time:60149ms step_avg:49.10ms
step:1226/1845 train_time:60238ms step_avg:49.13ms
step:1227/1845 train_time:60324ms step_avg:49.16ms
step:1228/1845 train_time:60413ms step_avg:49.20ms
step:1229/1845 train_time:60499ms step_avg:49.23ms
step:1230/1845 train_time:60588ms step_avg:49.26ms
step:1231/1845 train_time:60675ms step_avg:49.29ms
step:1232/1845 train_time:60763ms step_avg:49.32ms
step:1233/1845 train_time:60850ms step_avg:49.35ms
step:1234/1845 train_time:60939ms step_avg:49.38ms
step:1235/1845 train_time:61025ms step_avg:49.41ms
step:1236/1845 train_time:61115ms step_avg:49.45ms
step:1237/1845 train_time:61201ms step_avg:49.48ms
step:1238/1845 train_time:61290ms step_avg:49.51ms
step:1239/1845 train_time:61377ms step_avg:49.54ms
step:1240/1845 train_time:61466ms step_avg:49.57ms
step:1241/1845 train_time:61552ms step_avg:49.60ms
step:1242/1845 train_time:61639ms step_avg:49.63ms
step:1243/1845 train_time:61726ms step_avg:49.66ms
step:1244/1845 train_time:61816ms step_avg:49.69ms
step:1245/1845 train_time:61902ms step_avg:49.72ms
step:1246/1845 train_time:61992ms step_avg:49.75ms
step:1247/1845 train_time:62079ms step_avg:49.78ms
step:1248/1845 train_time:62168ms step_avg:49.81ms
step:1249/1845 train_time:62256ms step_avg:49.85ms
step:1250/1845 train_time:62345ms step_avg:49.88ms
step:1250/1845 val_loss:3.5401 train_time:62441ms step_avg:49.95ms
step:1251/1845 train_time:62468ms step_avg:49.93ms
step:1252/1845 train_time:62521ms step_avg:49.94ms
step:1253/1845 train_time:62609ms step_avg:49.97ms
step:1254/1845 train_time:62699ms step_avg:50.00ms
step:1255/1845 train_time:62785ms step_avg:50.03ms
step:1256/1845 train_time:62873ms step_avg:50.06ms
step:1257/1845 train_time:62959ms step_avg:50.09ms
step:1258/1845 train_time:63047ms step_avg:50.12ms
step:1259/1845 train_time:63132ms step_avg:50.14ms
step:1260/1845 train_time:63220ms step_avg:50.17ms
step:1261/1845 train_time:63305ms step_avg:50.20ms
step:1262/1845 train_time:63396ms step_avg:50.23ms
step:1263/1845 train_time:63486ms step_avg:50.27ms
step:1264/1845 train_time:63577ms step_avg:50.30ms
step:1265/1845 train_time:63664ms step_avg:50.33ms
step:1266/1845 train_time:63752ms step_avg:50.36ms
step:1267/1845 train_time:63837ms step_avg:50.38ms
step:1268/1845 train_time:63925ms step_avg:50.41ms
step:1269/1845 train_time:64011ms step_avg:50.44ms
step:1270/1845 train_time:64099ms step_avg:50.47ms
step:1271/1845 train_time:64185ms step_avg:50.50ms
step:1272/1845 train_time:64274ms step_avg:50.53ms
step:1273/1845 train_time:64361ms step_avg:50.56ms
step:1274/1845 train_time:64449ms step_avg:50.59ms
step:1275/1845 train_time:64538ms step_avg:50.62ms
step:1276/1845 train_time:64627ms step_avg:50.65ms
step:1277/1845 train_time:64714ms step_avg:50.68ms
step:1278/1845 train_time:64803ms step_avg:50.71ms
step:1279/1845 train_time:64888ms step_avg:50.73ms
step:1280/1845 train_time:64978ms step_avg:50.76ms
step:1281/1845 train_time:65064ms step_avg:50.79ms
step:1282/1845 train_time:65152ms step_avg:50.82ms
step:1283/1845 train_time:65238ms step_avg:50.85ms
step:1284/1845 train_time:65326ms step_avg:50.88ms
step:1285/1845 train_time:65413ms step_avg:50.91ms
step:1286/1845 train_time:65503ms step_avg:50.94ms
step:1287/1845 train_time:65589ms step_avg:50.96ms
step:1288/1845 train_time:65680ms step_avg:50.99ms
step:1289/1845 train_time:65767ms step_avg:51.02ms
step:1290/1845 train_time:65856ms step_avg:51.05ms
step:1291/1845 train_time:65942ms step_avg:51.08ms
step:1292/1845 train_time:66030ms step_avg:51.11ms
step:1293/1845 train_time:66116ms step_avg:51.13ms
step:1294/1845 train_time:66204ms step_avg:51.16ms
step:1295/1845 train_time:66291ms step_avg:51.19ms
step:1296/1845 train_time:66380ms step_avg:51.22ms
step:1297/1845 train_time:66466ms step_avg:51.25ms
step:1298/1845 train_time:66556ms step_avg:51.28ms
step:1299/1845 train_time:66643ms step_avg:51.30ms
step:1300/1845 train_time:66732ms step_avg:51.33ms
step:1301/1845 train_time:66819ms step_avg:51.36ms
step:1302/1845 train_time:66907ms step_avg:51.39ms
step:1303/1845 train_time:66995ms step_avg:51.42ms
step:1304/1845 train_time:67084ms step_avg:51.44ms
step:1305/1845 train_time:67169ms step_avg:51.47ms
step:1306/1845 train_time:67259ms step_avg:51.50ms
step:1307/1845 train_time:67344ms step_avg:51.53ms
step:1308/1845 train_time:67433ms step_avg:51.55ms
step:1309/1845 train_time:67519ms step_avg:51.58ms
step:1310/1845 train_time:67608ms step_avg:51.61ms
step:1311/1845 train_time:67696ms step_avg:51.64ms
step:1312/1845 train_time:67785ms step_avg:51.67ms
step:1313/1845 train_time:67872ms step_avg:51.69ms
step:1314/1845 train_time:67961ms step_avg:51.72ms
step:1315/1845 train_time:68047ms step_avg:51.75ms
step:1316/1845 train_time:68135ms step_avg:51.77ms
step:1317/1845 train_time:68222ms step_avg:51.80ms
step:1318/1845 train_time:68309ms step_avg:51.83ms
step:1319/1845 train_time:68395ms step_avg:51.85ms
step:1320/1845 train_time:68485ms step_avg:51.88ms
step:1321/1845 train_time:68571ms step_avg:51.91ms
step:1322/1845 train_time:68661ms step_avg:51.94ms
step:1323/1845 train_time:68746ms step_avg:51.96ms
step:1324/1845 train_time:68836ms step_avg:51.99ms
step:1325/1845 train_time:68924ms step_avg:52.02ms
step:1326/1845 train_time:69011ms step_avg:52.04ms
step:1327/1845 train_time:69098ms step_avg:52.07ms
step:1328/1845 train_time:69186ms step_avg:52.10ms
step:1329/1845 train_time:69271ms step_avg:52.12ms
step:1330/1845 train_time:69360ms step_avg:52.15ms
step:1331/1845 train_time:69446ms step_avg:52.18ms
step:1332/1845 train_time:69536ms step_avg:52.20ms
step:1333/1845 train_time:69623ms step_avg:52.23ms
step:1334/1845 train_time:69711ms step_avg:52.26ms
step:1335/1845 train_time:69798ms step_avg:52.28ms
step:1336/1845 train_time:69887ms step_avg:52.31ms
step:1337/1845 train_time:69974ms step_avg:52.34ms
step:1338/1845 train_time:70063ms step_avg:52.36ms
step:1339/1845 train_time:70148ms step_avg:52.39ms
step:1340/1845 train_time:70238ms step_avg:52.42ms
step:1341/1845 train_time:70323ms step_avg:52.44ms
step:1342/1845 train_time:70413ms step_avg:52.47ms
step:1343/1845 train_time:70499ms step_avg:52.49ms
step:1344/1845 train_time:70587ms step_avg:52.52ms
step:1345/1845 train_time:70673ms step_avg:52.55ms
step:1346/1845 train_time:70763ms step_avg:52.57ms
step:1347/1845 train_time:70849ms step_avg:52.60ms
step:1348/1845 train_time:70939ms step_avg:52.63ms
step:1349/1845 train_time:71026ms step_avg:52.65ms
step:1350/1845 train_time:71114ms step_avg:52.68ms
step:1351/1845 train_time:71202ms step_avg:52.70ms
step:1352/1845 train_time:71290ms step_avg:52.73ms
step:1353/1845 train_time:71376ms step_avg:52.75ms
step:1354/1845 train_time:71465ms step_avg:52.78ms
step:1355/1845 train_time:71551ms step_avg:52.81ms
step:1356/1845 train_time:71640ms step_avg:52.83ms
step:1357/1845 train_time:71727ms step_avg:52.86ms
step:1358/1845 train_time:71816ms step_avg:52.88ms
step:1359/1845 train_time:71903ms step_avg:52.91ms
step:1360/1845 train_time:71993ms step_avg:52.94ms
step:1361/1845 train_time:72079ms step_avg:52.96ms
step:1362/1845 train_time:72168ms step_avg:52.99ms
step:1363/1845 train_time:72253ms step_avg:53.01ms
step:1364/1845 train_time:72343ms step_avg:53.04ms
step:1365/1845 train_time:72429ms step_avg:53.06ms
step:1366/1845 train_time:72519ms step_avg:53.09ms
step:1367/1845 train_time:72605ms step_avg:53.11ms
step:1368/1845 train_time:72695ms step_avg:53.14ms
step:1369/1845 train_time:72782ms step_avg:53.16ms
step:1370/1845 train_time:72871ms step_avg:53.19ms
step:1371/1845 train_time:72957ms step_avg:53.21ms
step:1372/1845 train_time:73046ms step_avg:53.24ms
step:1373/1845 train_time:73131ms step_avg:53.26ms
step:1374/1845 train_time:73220ms step_avg:53.29ms
step:1375/1845 train_time:73305ms step_avg:53.31ms
step:1376/1845 train_time:73395ms step_avg:53.34ms
step:1377/1845 train_time:73482ms step_avg:53.36ms
step:1378/1845 train_time:73571ms step_avg:53.39ms
step:1379/1845 train_time:73657ms step_avg:53.41ms
step:1380/1845 train_time:73746ms step_avg:53.44ms
step:1381/1845 train_time:73833ms step_avg:53.46ms
step:1382/1845 train_time:73922ms step_avg:53.49ms
step:1383/1845 train_time:74008ms step_avg:53.51ms
step:1384/1845 train_time:74099ms step_avg:53.54ms
step:1385/1845 train_time:74185ms step_avg:53.56ms
step:1386/1845 train_time:74273ms step_avg:53.59ms
step:1387/1845 train_time:74359ms step_avg:53.61ms
step:1388/1845 train_time:74447ms step_avg:53.64ms
step:1389/1845 train_time:74534ms step_avg:53.66ms
step:1390/1845 train_time:74624ms step_avg:53.69ms
step:1391/1845 train_time:74710ms step_avg:53.71ms
step:1392/1845 train_time:74800ms step_avg:53.74ms
step:1393/1845 train_time:74886ms step_avg:53.76ms
step:1394/1845 train_time:74975ms step_avg:53.78ms
step:1395/1845 train_time:75062ms step_avg:53.81ms
step:1396/1845 train_time:75150ms step_avg:53.83ms
step:1397/1845 train_time:75237ms step_avg:53.86ms
step:1398/1845 train_time:75324ms step_avg:53.88ms
step:1399/1845 train_time:75411ms step_avg:53.90ms
step:1400/1845 train_time:75501ms step_avg:53.93ms
step:1401/1845 train_time:75586ms step_avg:53.95ms
step:1402/1845 train_time:75676ms step_avg:53.98ms
step:1403/1845 train_time:75762ms step_avg:54.00ms
step:1404/1845 train_time:75851ms step_avg:54.02ms
step:1405/1845 train_time:75937ms step_avg:54.05ms
step:1406/1845 train_time:76025ms step_avg:54.07ms
step:1407/1845 train_time:76112ms step_avg:54.10ms
step:1408/1845 train_time:76202ms step_avg:54.12ms
step:1409/1845 train_time:76288ms step_avg:54.14ms
step:1410/1845 train_time:76377ms step_avg:54.17ms
step:1411/1845 train_time:76464ms step_avg:54.19ms
step:1412/1845 train_time:76552ms step_avg:54.22ms
step:1413/1845 train_time:76638ms step_avg:54.24ms
step:1414/1845 train_time:76727ms step_avg:54.26ms
step:1415/1845 train_time:76813ms step_avg:54.29ms
step:1416/1845 train_time:76902ms step_avg:54.31ms
step:1417/1845 train_time:76988ms step_avg:54.33ms
step:1418/1845 train_time:77077ms step_avg:54.36ms
step:1419/1845 train_time:77163ms step_avg:54.38ms
step:1420/1845 train_time:77253ms step_avg:54.40ms
step:1421/1845 train_time:77339ms step_avg:54.43ms
step:1422/1845 train_time:77427ms step_avg:54.45ms
step:1423/1845 train_time:77514ms step_avg:54.47ms
step:1424/1845 train_time:77603ms step_avg:54.50ms
step:1425/1845 train_time:77689ms step_avg:54.52ms
step:1426/1845 train_time:77778ms step_avg:54.54ms
step:1427/1845 train_time:77864ms step_avg:54.56ms
step:1428/1845 train_time:77953ms step_avg:54.59ms
step:1429/1845 train_time:78038ms step_avg:54.61ms
step:1430/1845 train_time:78127ms step_avg:54.63ms
step:1431/1845 train_time:78214ms step_avg:54.66ms
step:1432/1845 train_time:78304ms step_avg:54.68ms
step:1433/1845 train_time:78391ms step_avg:54.70ms
step:1434/1845 train_time:78480ms step_avg:54.73ms
step:1435/1845 train_time:78565ms step_avg:54.75ms
step:1436/1845 train_time:78655ms step_avg:54.77ms
step:1437/1845 train_time:78740ms step_avg:54.79ms
step:1438/1845 train_time:78828ms step_avg:54.82ms
step:1439/1845 train_time:78915ms step_avg:54.84ms
step:1440/1845 train_time:79004ms step_avg:54.86ms
step:1441/1845 train_time:79090ms step_avg:54.89ms
step:1442/1845 train_time:79180ms step_avg:54.91ms
step:1443/1845 train_time:79266ms step_avg:54.93ms
step:1444/1845 train_time:79355ms step_avg:54.96ms
step:1445/1845 train_time:79442ms step_avg:54.98ms
step:1446/1845 train_time:79530ms step_avg:55.00ms
step:1447/1845 train_time:79617ms step_avg:55.02ms
step:1448/1845 train_time:79705ms step_avg:55.05ms
step:1449/1845 train_time:79791ms step_avg:55.07ms
step:1450/1845 train_time:79880ms step_avg:55.09ms
step:1451/1845 train_time:79966ms step_avg:55.11ms
step:1452/1845 train_time:80056ms step_avg:55.13ms
step:1453/1845 train_time:80141ms step_avg:55.16ms
step:1454/1845 train_time:80230ms step_avg:55.18ms
step:1455/1845 train_time:80318ms step_avg:55.20ms
step:1456/1845 train_time:80406ms step_avg:55.22ms
step:1457/1845 train_time:80493ms step_avg:55.25ms
step:1458/1845 train_time:80582ms step_avg:55.27ms
step:1459/1845 train_time:80668ms step_avg:55.29ms
step:1460/1845 train_time:80758ms step_avg:55.31ms
step:1461/1845 train_time:80844ms step_avg:55.33ms
step:1462/1845 train_time:80933ms step_avg:55.36ms
step:1463/1845 train_time:81019ms step_avg:55.38ms
step:1464/1845 train_time:81107ms step_avg:55.40ms
step:1465/1845 train_time:81193ms step_avg:55.42ms
step:1466/1845 train_time:81282ms step_avg:55.44ms
step:1467/1845 train_time:81368ms step_avg:55.47ms
step:1468/1845 train_time:81457ms step_avg:55.49ms
step:1469/1845 train_time:81544ms step_avg:55.51ms
step:1470/1845 train_time:81633ms step_avg:55.53ms
step:1471/1845 train_time:81719ms step_avg:55.55ms
step:1472/1845 train_time:81807ms step_avg:55.58ms
step:1473/1845 train_time:81893ms step_avg:55.60ms
step:1474/1845 train_time:81983ms step_avg:55.62ms
step:1475/1845 train_time:82068ms step_avg:55.64ms
step:1476/1845 train_time:82158ms step_avg:55.66ms
step:1477/1845 train_time:82243ms step_avg:55.68ms
step:1478/1845 train_time:82332ms step_avg:55.71ms
step:1479/1845 train_time:82418ms step_avg:55.73ms
step:1480/1845 train_time:82507ms step_avg:55.75ms
step:1481/1845 train_time:82593ms step_avg:55.77ms
step:1482/1845 train_time:82683ms step_avg:55.79ms
step:1483/1845 train_time:82769ms step_avg:55.81ms
step:1484/1845 train_time:82859ms step_avg:55.83ms
step:1485/1845 train_time:82945ms step_avg:55.86ms
step:1486/1845 train_time:83034ms step_avg:55.88ms
step:1487/1845 train_time:83120ms step_avg:55.90ms
step:1488/1845 train_time:83207ms step_avg:55.92ms
step:1489/1845 train_time:83295ms step_avg:55.94ms
step:1490/1845 train_time:83385ms step_avg:55.96ms
step:1491/1845 train_time:83470ms step_avg:55.98ms
step:1492/1845 train_time:83561ms step_avg:56.01ms
step:1493/1845 train_time:83646ms step_avg:56.03ms
step:1494/1845 train_time:83737ms step_avg:56.05ms
step:1495/1845 train_time:83824ms step_avg:56.07ms
step:1496/1845 train_time:83912ms step_avg:56.09ms
step:1497/1845 train_time:83998ms step_avg:56.11ms
step:1498/1845 train_time:84086ms step_avg:56.13ms
step:1499/1845 train_time:84172ms step_avg:56.15ms
step:1500/1845 train_time:84262ms step_avg:56.17ms
step:1500/1845 val_loss:3.4039 train_time:84358ms step_avg:56.24ms
step:1501/1845 train_time:84386ms step_avg:56.22ms
step:1502/1845 train_time:84438ms step_avg:56.22ms
step:1503/1845 train_time:84525ms step_avg:56.24ms
step:1504/1845 train_time:84613ms step_avg:56.26ms
step:1505/1845 train_time:84700ms step_avg:56.28ms
step:1506/1845 train_time:84788ms step_avg:56.30ms
step:1507/1845 train_time:84874ms step_avg:56.32ms
step:1508/1845 train_time:84963ms step_avg:56.34ms
step:1509/1845 train_time:85048ms step_avg:56.36ms
step:1510/1845 train_time:85137ms step_avg:56.38ms
step:1511/1845 train_time:85222ms step_avg:56.40ms
step:1512/1845 train_time:85312ms step_avg:56.42ms
step:1513/1845 train_time:85400ms step_avg:56.44ms
step:1514/1845 train_time:85490ms step_avg:56.47ms
step:1515/1845 train_time:85578ms step_avg:56.49ms
step:1516/1845 train_time:85667ms step_avg:56.51ms
step:1517/1845 train_time:85753ms step_avg:56.53ms
step:1518/1845 train_time:85842ms step_avg:56.55ms
step:1519/1845 train_time:85928ms step_avg:56.57ms
step:1520/1845 train_time:86016ms step_avg:56.59ms
step:1521/1845 train_time:86101ms step_avg:56.61ms
step:1522/1845 train_time:86189ms step_avg:56.63ms
step:1523/1845 train_time:86275ms step_avg:56.65ms
step:1524/1845 train_time:86367ms step_avg:56.67ms
step:1525/1845 train_time:86454ms step_avg:56.69ms
step:1526/1845 train_time:86544ms step_avg:56.71ms
step:1527/1845 train_time:86630ms step_avg:56.73ms
step:1528/1845 train_time:86720ms step_avg:56.75ms
step:1529/1845 train_time:86807ms step_avg:56.77ms
step:1530/1845 train_time:86895ms step_avg:56.79ms
step:1531/1845 train_time:86980ms step_avg:56.81ms
step:1532/1845 train_time:87069ms step_avg:56.83ms
step:1533/1845 train_time:87155ms step_avg:56.85ms
step:1534/1845 train_time:87244ms step_avg:56.87ms
step:1535/1845 train_time:87330ms step_avg:56.89ms
step:1536/1845 train_time:87420ms step_avg:56.91ms
step:1537/1845 train_time:87508ms step_avg:56.93ms
step:1538/1845 train_time:87596ms step_avg:56.95ms
step:1539/1845 train_time:87682ms step_avg:56.97ms
step:1540/1845 train_time:87771ms step_avg:56.99ms
step:1541/1845 train_time:87856ms step_avg:57.01ms
step:1542/1845 train_time:87946ms step_avg:57.03ms
step:1543/1845 train_time:88031ms step_avg:57.05ms
step:1544/1845 train_time:88121ms step_avg:57.07ms
step:1545/1845 train_time:88207ms step_avg:57.09ms
step:1546/1845 train_time:88296ms step_avg:57.11ms
step:1547/1845 train_time:88383ms step_avg:57.13ms
step:1548/1845 train_time:88472ms step_avg:57.15ms
step:1549/1845 train_time:88558ms step_avg:57.17ms
step:1550/1845 train_time:88647ms step_avg:57.19ms
step:1551/1845 train_time:88733ms step_avg:57.21ms
step:1552/1845 train_time:88822ms step_avg:57.23ms
step:1553/1845 train_time:88908ms step_avg:57.25ms
step:1554/1845 train_time:88997ms step_avg:57.27ms
step:1555/1845 train_time:89083ms step_avg:57.29ms
step:1556/1845 train_time:89171ms step_avg:57.31ms
step:1557/1845 train_time:89257ms step_avg:57.33ms
step:1558/1845 train_time:89347ms step_avg:57.35ms
step:1559/1845 train_time:89433ms step_avg:57.37ms
step:1560/1845 train_time:89522ms step_avg:57.39ms
step:1561/1845 train_time:89609ms step_avg:57.40ms
step:1562/1845 train_time:89698ms step_avg:57.43ms
step:1563/1845 train_time:89785ms step_avg:57.44ms
step:1564/1845 train_time:89873ms step_avg:57.46ms
step:1565/1845 train_time:89959ms step_avg:57.48ms
step:1566/1845 train_time:90048ms step_avg:57.50ms
step:1567/1845 train_time:90134ms step_avg:57.52ms
step:1568/1845 train_time:90224ms step_avg:57.54ms
step:1569/1845 train_time:90310ms step_avg:57.56ms
step:1570/1845 train_time:90399ms step_avg:57.58ms
step:1571/1845 train_time:90485ms step_avg:57.60ms
step:1572/1845 train_time:90573ms step_avg:57.62ms
step:1573/1845 train_time:90659ms step_avg:57.63ms
step:1574/1845 train_time:90748ms step_avg:57.65ms
step:1575/1845 train_time:90835ms step_avg:57.67ms
step:1576/1845 train_time:90924ms step_avg:57.69ms
step:1577/1845 train_time:91011ms step_avg:57.71ms
step:1578/1845 train_time:91100ms step_avg:57.73ms
step:1579/1845 train_time:91187ms step_avg:57.75ms
step:1580/1845 train_time:91276ms step_avg:57.77ms
step:1581/1845 train_time:91361ms step_avg:57.79ms
step:1582/1845 train_time:91450ms step_avg:57.81ms
step:1583/1845 train_time:91536ms step_avg:57.82ms
step:1584/1845 train_time:91624ms step_avg:57.84ms
step:1585/1845 train_time:91711ms step_avg:57.86ms
step:1586/1845 train_time:91800ms step_avg:57.88ms
step:1587/1845 train_time:91886ms step_avg:57.90ms
step:1588/1845 train_time:91976ms step_avg:57.92ms
step:1589/1845 train_time:92062ms step_avg:57.94ms
step:1590/1845 train_time:92150ms step_avg:57.96ms
step:1591/1845 train_time:92236ms step_avg:57.97ms
step:1592/1845 train_time:92326ms step_avg:57.99ms
step:1593/1845 train_time:92412ms step_avg:58.01ms
step:1594/1845 train_time:92502ms step_avg:58.03ms
step:1595/1845 train_time:92588ms step_avg:58.05ms
step:1596/1845 train_time:92678ms step_avg:58.07ms
step:1597/1845 train_time:92764ms step_avg:58.09ms
step:1598/1845 train_time:92851ms step_avg:58.10ms
step:1599/1845 train_time:92939ms step_avg:58.12ms
step:1600/1845 train_time:93028ms step_avg:58.14ms
step:1601/1845 train_time:93114ms step_avg:58.16ms
step:1602/1845 train_time:93204ms step_avg:58.18ms
step:1603/1845 train_time:93289ms step_avg:58.20ms
step:1604/1845 train_time:93379ms step_avg:58.22ms
step:1605/1845 train_time:93465ms step_avg:58.23ms
step:1606/1845 train_time:93553ms step_avg:58.25ms
step:1607/1845 train_time:93639ms step_avg:58.27ms
step:1608/1845 train_time:93728ms step_avg:58.29ms
step:1609/1845 train_time:93814ms step_avg:58.31ms
step:1610/1845 train_time:93903ms step_avg:58.32ms
step:1611/1845 train_time:93989ms step_avg:58.34ms
step:1612/1845 train_time:94080ms step_avg:58.36ms
step:1613/1845 train_time:94166ms step_avg:58.38ms
step:1614/1845 train_time:94255ms step_avg:58.40ms
step:1615/1845 train_time:94341ms step_avg:58.42ms
step:1616/1845 train_time:94429ms step_avg:58.43ms
step:1617/1845 train_time:94516ms step_avg:58.45ms
step:1618/1845 train_time:94606ms step_avg:58.47ms
step:1619/1845 train_time:94691ms step_avg:58.49ms
step:1620/1845 train_time:94781ms step_avg:58.51ms
step:1621/1845 train_time:94867ms step_avg:58.52ms
step:1622/1845 train_time:94956ms step_avg:58.54ms
step:1623/1845 train_time:95042ms step_avg:58.56ms
step:1624/1845 train_time:95130ms step_avg:58.58ms
step:1625/1845 train_time:95217ms step_avg:58.60ms
step:1626/1845 train_time:95306ms step_avg:58.61ms
step:1627/1845 train_time:95392ms step_avg:58.63ms
step:1628/1845 train_time:95482ms step_avg:58.65ms
step:1629/1845 train_time:95567ms step_avg:58.67ms
step:1630/1845 train_time:95656ms step_avg:58.68ms
step:1631/1845 train_time:95742ms step_avg:58.70ms
step:1632/1845 train_time:95831ms step_avg:58.72ms
step:1633/1845 train_time:95917ms step_avg:58.74ms
step:1634/1845 train_time:96006ms step_avg:58.76ms
step:1635/1845 train_time:96092ms step_avg:58.77ms
step:1636/1845 train_time:96182ms step_avg:58.79ms
step:1637/1845 train_time:96269ms step_avg:58.81ms
step:1638/1845 train_time:96358ms step_avg:58.83ms
step:1639/1845 train_time:96445ms step_avg:58.84ms
step:1640/1845 train_time:96533ms step_avg:58.86ms
step:1641/1845 train_time:96619ms step_avg:58.88ms
step:1642/1845 train_time:96708ms step_avg:58.90ms
step:1643/1845 train_time:96794ms step_avg:58.91ms
step:1644/1845 train_time:96883ms step_avg:58.93ms
step:1645/1845 train_time:96969ms step_avg:58.95ms
step:1646/1845 train_time:97058ms step_avg:58.97ms
step:1647/1845 train_time:97144ms step_avg:58.98ms
step:1648/1845 train_time:97232ms step_avg:59.00ms
step:1649/1845 train_time:97319ms step_avg:59.02ms
step:1650/1845 train_time:97409ms step_avg:59.04ms
step:1651/1845 train_time:97495ms step_avg:59.05ms
step:1652/1845 train_time:97584ms step_avg:59.07ms
step:1653/1845 train_time:97670ms step_avg:59.09ms
step:1654/1845 train_time:97759ms step_avg:59.10ms
step:1655/1845 train_time:97846ms step_avg:59.12ms
step:1656/1845 train_time:97935ms step_avg:59.14ms
step:1657/1845 train_time:98021ms step_avg:59.16ms
step:1658/1845 train_time:98110ms step_avg:59.17ms
step:1659/1845 train_time:98196ms step_avg:59.19ms
step:1660/1845 train_time:98286ms step_avg:59.21ms
step:1661/1845 train_time:98372ms step_avg:59.22ms
step:1662/1845 train_time:98461ms step_avg:59.24ms
step:1663/1845 train_time:98548ms step_avg:59.26ms
step:1664/1845 train_time:98637ms step_avg:59.28ms
step:1665/1845 train_time:98723ms step_avg:59.29ms
step:1666/1845 train_time:98810ms step_avg:59.31ms
step:1667/1845 train_time:98897ms step_avg:59.33ms
step:1668/1845 train_time:98987ms step_avg:59.34ms
step:1669/1845 train_time:99073ms step_avg:59.36ms
step:1670/1845 train_time:99162ms step_avg:59.38ms
step:1671/1845 train_time:99249ms step_avg:59.39ms
step:1672/1845 train_time:99337ms step_avg:59.41ms
step:1673/1845 train_time:99424ms step_avg:59.43ms
step:1674/1845 train_time:99512ms step_avg:59.45ms
step:1675/1845 train_time:99598ms step_avg:59.46ms
step:1676/1845 train_time:99687ms step_avg:59.48ms
step:1677/1845 train_time:99774ms step_avg:59.50ms
step:1678/1845 train_time:99864ms step_avg:59.51ms
step:1679/1845 train_time:99949ms step_avg:59.53ms
step:1680/1845 train_time:100038ms step_avg:59.55ms
step:1681/1845 train_time:100124ms step_avg:59.56ms
step:1682/1845 train_time:100213ms step_avg:59.58ms
step:1683/1845 train_time:100300ms step_avg:59.60ms
step:1684/1845 train_time:100389ms step_avg:59.61ms
step:1685/1845 train_time:100475ms step_avg:59.63ms
step:1686/1845 train_time:100566ms step_avg:59.65ms
step:1687/1845 train_time:100652ms step_avg:59.66ms
step:1688/1845 train_time:100741ms step_avg:59.68ms
step:1689/1845 train_time:100827ms step_avg:59.70ms
step:1690/1845 train_time:100917ms step_avg:59.71ms
step:1691/1845 train_time:101003ms step_avg:59.73ms
step:1692/1845 train_time:101091ms step_avg:59.75ms
step:1693/1845 train_time:101179ms step_avg:59.76ms
step:1694/1845 train_time:101267ms step_avg:59.78ms
step:1695/1845 train_time:101353ms step_avg:59.80ms
step:1696/1845 train_time:101444ms step_avg:59.81ms
step:1697/1845 train_time:101529ms step_avg:59.83ms
step:1698/1845 train_time:101618ms step_avg:59.85ms
step:1699/1845 train_time:101705ms step_avg:59.86ms
step:1700/1845 train_time:101792ms step_avg:59.88ms
step:1701/1845 train_time:101879ms step_avg:59.89ms
step:1702/1845 train_time:101968ms step_avg:59.91ms
step:1703/1845 train_time:102054ms step_avg:59.93ms
step:1704/1845 train_time:102145ms step_avg:59.94ms
step:1705/1845 train_time:102230ms step_avg:59.96ms
step:1706/1845 train_time:102320ms step_avg:59.98ms
step:1707/1845 train_time:102407ms step_avg:59.99ms
step:1708/1845 train_time:102495ms step_avg:60.01ms
step:1709/1845 train_time:102581ms step_avg:60.02ms
step:1710/1845 train_time:102669ms step_avg:60.04ms
step:1711/1845 train_time:102755ms step_avg:60.06ms
step:1712/1845 train_time:102845ms step_avg:60.07ms
step:1713/1845 train_time:102930ms step_avg:60.09ms
step:1714/1845 train_time:103019ms step_avg:60.10ms
step:1715/1845 train_time:103106ms step_avg:60.12ms
step:1716/1845 train_time:103194ms step_avg:60.14ms
step:1717/1845 train_time:103280ms step_avg:60.15ms
step:1718/1845 train_time:103368ms step_avg:60.17ms
step:1719/1845 train_time:103456ms step_avg:60.18ms
step:1720/1845 train_time:103544ms step_avg:60.20ms
step:1721/1845 train_time:103630ms step_avg:60.21ms
step:1722/1845 train_time:103719ms step_avg:60.23ms
step:1723/1845 train_time:103807ms step_avg:60.25ms
step:1724/1845 train_time:103896ms step_avg:60.26ms
step:1725/1845 train_time:103981ms step_avg:60.28ms
step:1726/1845 train_time:104069ms step_avg:60.30ms
step:1727/1845 train_time:104156ms step_avg:60.31ms
step:1728/1845 train_time:104246ms step_avg:60.33ms
step:1729/1845 train_time:104332ms step_avg:60.34ms
step:1730/1845 train_time:104422ms step_avg:60.36ms
step:1731/1845 train_time:104508ms step_avg:60.37ms
step:1732/1845 train_time:104596ms step_avg:60.39ms
step:1733/1845 train_time:104682ms step_avg:60.41ms
step:1734/1845 train_time:104771ms step_avg:60.42ms
step:1735/1845 train_time:104858ms step_avg:60.44ms
step:1736/1845 train_time:104946ms step_avg:60.45ms
step:1737/1845 train_time:105032ms step_avg:60.47ms
step:1738/1845 train_time:105120ms step_avg:60.48ms
step:1739/1845 train_time:105208ms step_avg:60.50ms
step:1740/1845 train_time:105297ms step_avg:60.52ms
step:1741/1845 train_time:105384ms step_avg:60.53ms
step:1742/1845 train_time:105472ms step_avg:60.55ms
step:1743/1845 train_time:105558ms step_avg:60.56ms
step:1744/1845 train_time:105648ms step_avg:60.58ms
step:1745/1845 train_time:105734ms step_avg:60.59ms
step:1746/1845 train_time:105823ms step_avg:60.61ms
step:1747/1845 train_time:105909ms step_avg:60.62ms
step:1748/1845 train_time:105997ms step_avg:60.64ms
step:1749/1845 train_time:106083ms step_avg:60.65ms
step:1750/1845 train_time:106171ms step_avg:60.67ms
step:1750/1845 val_loss:3.3049 train_time:106269ms step_avg:60.73ms
step:1751/1845 train_time:106297ms step_avg:60.71ms
step:1752/1845 train_time:106349ms step_avg:60.70ms
step:1753/1845 train_time:106435ms step_avg:60.72ms
step:1754/1845 train_time:106527ms step_avg:60.73ms
step:1755/1845 train_time:106614ms step_avg:60.75ms
step:1756/1845 train_time:106701ms step_avg:60.76ms
step:1757/1845 train_time:106788ms step_avg:60.78ms
step:1758/1845 train_time:106876ms step_avg:60.79ms
step:1759/1845 train_time:106961ms step_avg:60.81ms
step:1760/1845 train_time:107051ms step_avg:60.82ms
step:1761/1845 train_time:107138ms step_avg:60.84ms
step:1762/1845 train_time:107229ms step_avg:60.86ms
step:1763/1845 train_time:107317ms step_avg:60.87ms
step:1764/1845 train_time:107406ms step_avg:60.89ms
step:1765/1845 train_time:107493ms step_avg:60.90ms
step:1766/1845 train_time:107581ms step_avg:60.92ms
step:1767/1845 train_time:107667ms step_avg:60.93ms
step:1768/1845 train_time:107756ms step_avg:60.95ms
step:1769/1845 train_time:107842ms step_avg:60.96ms
step:1770/1845 train_time:107930ms step_avg:60.98ms
step:1771/1845 train_time:108016ms step_avg:60.99ms
step:1772/1845 train_time:108104ms step_avg:61.01ms
step:1773/1845 train_time:108191ms step_avg:61.02ms
step:1774/1845 train_time:108280ms step_avg:61.04ms
step:1775/1845 train_time:108366ms step_avg:61.05ms
step:1776/1845 train_time:108457ms step_avg:61.07ms
step:1777/1845 train_time:108543ms step_avg:61.08ms
step:1778/1845 train_time:108631ms step_avg:61.10ms
step:1779/1845 train_time:108718ms step_avg:61.11ms
step:1780/1845 train_time:108805ms step_avg:61.13ms
step:1781/1845 train_time:108891ms step_avg:61.14ms
step:1782/1845 train_time:108979ms step_avg:61.16ms
step:1783/1845 train_time:109065ms step_avg:61.17ms
step:1784/1845 train_time:109154ms step_avg:61.19ms
step:1785/1845 train_time:109241ms step_avg:61.20ms
step:1786/1845 train_time:109331ms step_avg:61.22ms
step:1787/1845 train_time:109418ms step_avg:61.23ms
step:1788/1845 train_time:109508ms step_avg:61.25ms
step:1789/1845 train_time:109594ms step_avg:61.26ms
step:1790/1845 train_time:109681ms step_avg:61.27ms
step:1791/1845 train_time:109767ms step_avg:61.29ms
step:1792/1845 train_time:109857ms step_avg:61.30ms
step:1793/1845 train_time:109941ms step_avg:61.32ms
step:1794/1845 train_time:110032ms step_avg:61.33ms
step:1795/1845 train_time:110119ms step_avg:61.35ms
step:1796/1845 train_time:110208ms step_avg:61.36ms
step:1797/1845 train_time:110295ms step_avg:61.38ms
step:1798/1845 train_time:110384ms step_avg:61.39ms
step:1799/1845 train_time:110471ms step_avg:61.41ms
step:1800/1845 train_time:110560ms step_avg:61.42ms
step:1801/1845 train_time:110646ms step_avg:61.44ms
step:1802/1845 train_time:110734ms step_avg:61.45ms
step:1803/1845 train_time:110820ms step_avg:61.46ms
step:1804/1845 train_time:110909ms step_avg:61.48ms
step:1805/1845 train_time:110995ms step_avg:61.49ms
step:1806/1845 train_time:111085ms step_avg:61.51ms
step:1807/1845 train_time:111171ms step_avg:61.52ms
step:1808/1845 train_time:111260ms step_avg:61.54ms
step:1809/1845 train_time:111346ms step_avg:61.55ms
step:1810/1845 train_time:111436ms step_avg:61.57ms
step:1811/1845 train_time:111523ms step_avg:61.58ms
step:1812/1845 train_time:111613ms step_avg:61.60ms
step:1813/1845 train_time:111699ms step_avg:61.61ms
step:1814/1845 train_time:111788ms step_avg:61.63ms
step:1815/1845 train_time:111874ms step_avg:61.64ms
step:1816/1845 train_time:111963ms step_avg:61.65ms
step:1817/1845 train_time:112049ms step_avg:61.67ms
step:1818/1845 train_time:112138ms step_avg:61.68ms
step:1819/1845 train_time:112224ms step_avg:61.70ms
step:1820/1845 train_time:112314ms step_avg:61.71ms
step:1821/1845 train_time:112401ms step_avg:61.72ms
step:1822/1845 train_time:112490ms step_avg:61.74ms
step:1823/1845 train_time:112577ms step_avg:61.75ms
step:1824/1845 train_time:112666ms step_avg:61.77ms
step:1825/1845 train_time:112752ms step_avg:61.78ms
step:1826/1845 train_time:112840ms step_avg:61.80ms
step:1827/1845 train_time:112928ms step_avg:61.81ms
step:1828/1845 train_time:113017ms step_avg:61.83ms
step:1829/1845 train_time:113104ms step_avg:61.84ms
step:1830/1845 train_time:113193ms step_avg:61.85ms
step:1831/1845 train_time:113279ms step_avg:61.87ms
step:1832/1845 train_time:113369ms step_avg:61.88ms
step:1833/1845 train_time:113455ms step_avg:61.90ms
step:1834/1845 train_time:113544ms step_avg:61.91ms
step:1835/1845 train_time:113633ms step_avg:61.93ms
step:1836/1845 train_time:113721ms step_avg:61.94ms
step:1837/1845 train_time:113807ms step_avg:61.95ms
step:1838/1845 train_time:113896ms step_avg:61.97ms
step:1839/1845 train_time:113982ms step_avg:61.98ms
step:1840/1845 train_time:114071ms step_avg:62.00ms
step:1841/1845 train_time:114157ms step_avg:62.01ms
step:1842/1845 train_time:114246ms step_avg:62.02ms
step:1843/1845 train_time:114332ms step_avg:62.04ms
step:1844/1845 train_time:114420ms step_avg:62.05ms
step:1845/1845 train_time:114508ms step_avg:62.06ms
step:1845/1845 val_loss:3.2786 train_time:114605ms step_avg:62.12ms
peak memory allocated: 29801 MiB reserved: 44998 MiB

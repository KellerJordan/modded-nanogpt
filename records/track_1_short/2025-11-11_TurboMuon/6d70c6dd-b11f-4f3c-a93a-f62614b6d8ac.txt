import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 10:16:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   42C    P0            116W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   42C    P0            118W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   42C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3342641      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3342642      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3342643      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3342644      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3342642      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3342643      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3342644      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:164ms step_avg:163.51ms
step:2/2245 train_time:217ms step_avg:108.60ms
step:3/2245 train_time:326ms step_avg:108.77ms
step:4/2245 train_time:445ms step_avg:111.16ms
step:5/2245 train_time:560ms step_avg:111.95ms
step:6/2245 train_time:683ms step_avg:113.75ms
step:7/2245 train_time:799ms step_avg:114.08ms
step:8/2245 train_time:921ms step_avg:115.16ms
step:9/2245 train_time:1037ms step_avg:115.28ms
step:10/2245 train_time:1160ms step_avg:116.03ms
step:11/2245 train_time:1277ms step_avg:116.07ms
step:12/2245 train_time:1400ms step_avg:116.63ms
step:13/2245 train_time:1516ms step_avg:116.60ms
step:14/2245 train_time:1639ms step_avg:117.08ms
step:15/2245 train_time:1756ms step_avg:117.05ms
step:16/2245 train_time:1878ms step_avg:117.39ms
step:17/2245 train_time:1995ms step_avg:117.34ms
step:18/2245 train_time:2117ms step_avg:117.63ms
step:19/2245 train_time:2233ms step_avg:117.53ms
step:20/2245 train_time:2356ms step_avg:117.78ms
step:21/2245 train_time:2472ms step_avg:117.71ms
step:22/2245 train_time:2594ms step_avg:117.93ms
step:23/2245 train_time:2711ms step_avg:117.86ms
step:24/2245 train_time:2833ms step_avg:118.05ms
step:25/2245 train_time:2949ms step_avg:117.97ms
step:26/2245 train_time:3072ms step_avg:118.14ms
step:27/2245 train_time:3187ms step_avg:118.04ms
step:28/2245 train_time:3309ms step_avg:118.18ms
step:29/2245 train_time:3425ms step_avg:118.11ms
step:30/2245 train_time:3547ms step_avg:118.24ms
step:31/2245 train_time:3663ms step_avg:118.15ms
step:32/2245 train_time:3785ms step_avg:118.27ms
step:33/2245 train_time:3901ms step_avg:118.20ms
step:34/2245 train_time:4023ms step_avg:118.32ms
step:35/2245 train_time:4139ms step_avg:118.25ms
step:36/2245 train_time:4260ms step_avg:118.34ms
step:37/2245 train_time:4376ms step_avg:118.28ms
step:38/2245 train_time:4498ms step_avg:118.38ms
step:39/2245 train_time:4614ms step_avg:118.30ms
step:40/2245 train_time:4736ms step_avg:118.39ms
step:41/2245 train_time:4851ms step_avg:118.33ms
step:42/2245 train_time:4973ms step_avg:118.41ms
step:43/2245 train_time:5089ms step_avg:118.35ms
step:44/2245 train_time:5211ms step_avg:118.43ms
step:45/2245 train_time:5327ms step_avg:118.37ms
step:46/2245 train_time:5449ms step_avg:118.45ms
step:47/2245 train_time:5564ms step_avg:118.38ms
step:48/2245 train_time:5686ms step_avg:118.45ms
step:49/2245 train_time:5801ms step_avg:118.39ms
step:50/2245 train_time:5923ms step_avg:118.46ms
step:51/2245 train_time:6039ms step_avg:118.40ms
step:52/2245 train_time:6160ms step_avg:118.47ms
step:53/2245 train_time:6276ms step_avg:118.41ms
step:54/2245 train_time:6398ms step_avg:118.48ms
step:55/2245 train_time:6513ms step_avg:118.41ms
step:56/2245 train_time:6635ms step_avg:118.47ms
step:57/2245 train_time:6750ms step_avg:118.41ms
step:58/2245 train_time:6872ms step_avg:118.48ms
step:59/2245 train_time:6987ms step_avg:118.43ms
step:60/2245 train_time:7109ms step_avg:118.48ms
step:61/2245 train_time:7224ms step_avg:118.43ms
step:62/2245 train_time:7346ms step_avg:118.48ms
step:63/2245 train_time:7461ms step_avg:118.43ms
step:64/2245 train_time:7583ms step_avg:118.49ms
step:65/2245 train_time:7699ms step_avg:118.44ms
step:66/2245 train_time:7820ms step_avg:118.49ms
step:67/2245 train_time:7936ms step_avg:118.44ms
step:68/2245 train_time:8057ms step_avg:118.49ms
step:69/2245 train_time:8172ms step_avg:118.44ms
step:70/2245 train_time:8294ms step_avg:118.48ms
step:71/2245 train_time:8409ms step_avg:118.43ms
step:72/2245 train_time:8531ms step_avg:118.48ms
step:73/2245 train_time:8646ms step_avg:118.44ms
step:74/2245 train_time:8767ms step_avg:118.47ms
step:75/2245 train_time:8882ms step_avg:118.43ms
step:76/2245 train_time:9003ms step_avg:118.47ms
step:77/2245 train_time:9119ms step_avg:118.42ms
step:78/2245 train_time:9240ms step_avg:118.46ms
step:79/2245 train_time:9355ms step_avg:118.42ms
step:80/2245 train_time:9477ms step_avg:118.46ms
step:81/2245 train_time:9592ms step_avg:118.42ms
step:82/2245 train_time:9714ms step_avg:118.46ms
step:83/2245 train_time:9829ms step_avg:118.42ms
step:84/2245 train_time:9950ms step_avg:118.46ms
step:85/2245 train_time:10066ms step_avg:118.42ms
step:86/2245 train_time:10187ms step_avg:118.45ms
step:87/2245 train_time:10302ms step_avg:118.41ms
step:88/2245 train_time:10423ms step_avg:118.44ms
step:89/2245 train_time:10538ms step_avg:118.40ms
step:90/2245 train_time:10659ms step_avg:118.43ms
step:91/2245 train_time:10774ms step_avg:118.40ms
step:92/2245 train_time:10895ms step_avg:118.43ms
step:93/2245 train_time:11010ms step_avg:118.39ms
step:94/2245 train_time:11131ms step_avg:118.42ms
step:95/2245 train_time:11247ms step_avg:118.38ms
step:96/2245 train_time:11368ms step_avg:118.42ms
step:97/2245 train_time:11483ms step_avg:118.38ms
step:98/2245 train_time:11603ms step_avg:118.40ms
step:99/2245 train_time:11719ms step_avg:118.37ms
step:100/2245 train_time:11840ms step_avg:118.40ms
step:101/2245 train_time:11955ms step_avg:118.36ms
step:102/2245 train_time:12076ms step_avg:118.39ms
step:103/2245 train_time:12191ms step_avg:118.36ms
step:104/2245 train_time:12313ms step_avg:118.39ms
step:105/2245 train_time:12428ms step_avg:118.36ms
step:106/2245 train_time:12549ms step_avg:118.38ms
step:107/2245 train_time:12664ms step_avg:118.35ms
step:108/2245 train_time:12785ms step_avg:118.38ms
step:109/2245 train_time:12899ms step_avg:118.34ms
step:110/2245 train_time:13020ms step_avg:118.36ms
step:111/2245 train_time:13134ms step_avg:118.33ms
step:112/2245 train_time:13255ms step_avg:118.35ms
step:113/2245 train_time:13370ms step_avg:118.32ms
step:114/2245 train_time:13492ms step_avg:118.35ms
step:115/2245 train_time:13606ms step_avg:118.32ms
step:116/2245 train_time:13728ms step_avg:118.34ms
step:117/2245 train_time:13842ms step_avg:118.31ms
step:118/2245 train_time:13963ms step_avg:118.33ms
step:119/2245 train_time:14078ms step_avg:118.30ms
step:120/2245 train_time:14199ms step_avg:118.33ms
step:121/2245 train_time:14314ms step_avg:118.30ms
step:122/2245 train_time:14435ms step_avg:118.32ms
step:123/2245 train_time:14550ms step_avg:118.29ms
step:124/2245 train_time:14671ms step_avg:118.31ms
step:125/2245 train_time:14786ms step_avg:118.28ms
step:126/2245 train_time:14906ms step_avg:118.30ms
step:127/2245 train_time:15021ms step_avg:118.27ms
step:128/2245 train_time:15142ms step_avg:118.30ms
step:129/2245 train_time:15257ms step_avg:118.27ms
step:130/2245 train_time:15378ms step_avg:118.29ms
step:131/2245 train_time:15492ms step_avg:118.26ms
step:132/2245 train_time:15613ms step_avg:118.28ms
step:133/2245 train_time:15728ms step_avg:118.25ms
step:134/2245 train_time:15848ms step_avg:118.27ms
step:135/2245 train_time:15963ms step_avg:118.24ms
step:136/2245 train_time:16084ms step_avg:118.26ms
step:137/2245 train_time:16198ms step_avg:118.24ms
step:138/2245 train_time:16319ms step_avg:118.25ms
step:139/2245 train_time:16433ms step_avg:118.23ms
step:140/2245 train_time:16554ms step_avg:118.24ms
step:141/2245 train_time:16669ms step_avg:118.22ms
step:142/2245 train_time:16790ms step_avg:118.24ms
step:143/2245 train_time:16905ms step_avg:118.21ms
step:144/2245 train_time:17025ms step_avg:118.23ms
step:145/2245 train_time:17140ms step_avg:118.21ms
step:146/2245 train_time:17261ms step_avg:118.22ms
step:147/2245 train_time:17375ms step_avg:118.20ms
step:148/2245 train_time:17496ms step_avg:118.21ms
step:149/2245 train_time:17610ms step_avg:118.19ms
step:150/2245 train_time:17731ms step_avg:118.21ms
step:151/2245 train_time:17846ms step_avg:118.18ms
step:152/2245 train_time:17967ms step_avg:118.20ms
step:153/2245 train_time:18081ms step_avg:118.18ms
step:154/2245 train_time:18202ms step_avg:118.20ms
step:155/2245 train_time:18317ms step_avg:118.17ms
step:156/2245 train_time:18437ms step_avg:118.19ms
step:157/2245 train_time:18552ms step_avg:118.16ms
step:158/2245 train_time:18672ms step_avg:118.18ms
step:159/2245 train_time:18787ms step_avg:118.16ms
step:160/2245 train_time:18908ms step_avg:118.17ms
step:161/2245 train_time:19022ms step_avg:118.15ms
step:162/2245 train_time:19143ms step_avg:118.17ms
step:163/2245 train_time:19258ms step_avg:118.14ms
step:164/2245 train_time:19378ms step_avg:118.16ms
step:165/2245 train_time:19492ms step_avg:118.13ms
step:166/2245 train_time:19613ms step_avg:118.15ms
step:167/2245 train_time:19728ms step_avg:118.13ms
step:168/2245 train_time:19849ms step_avg:118.15ms
step:169/2245 train_time:19963ms step_avg:118.12ms
step:170/2245 train_time:20084ms step_avg:118.14ms
step:171/2245 train_time:20198ms step_avg:118.12ms
step:172/2245 train_time:20319ms step_avg:118.13ms
step:173/2245 train_time:20433ms step_avg:118.11ms
step:174/2245 train_time:20554ms step_avg:118.12ms
step:175/2245 train_time:20669ms step_avg:118.11ms
step:176/2245 train_time:20790ms step_avg:118.12ms
step:177/2245 train_time:20903ms step_avg:118.10ms
step:178/2245 train_time:21024ms step_avg:118.11ms
step:179/2245 train_time:21138ms step_avg:118.09ms
step:180/2245 train_time:21259ms step_avg:118.10ms
step:181/2245 train_time:21373ms step_avg:118.08ms
step:182/2245 train_time:21494ms step_avg:118.10ms
step:183/2245 train_time:21609ms step_avg:118.08ms
step:184/2245 train_time:21729ms step_avg:118.09ms
step:185/2245 train_time:21843ms step_avg:118.07ms
step:186/2245 train_time:21964ms step_avg:118.08ms
step:187/2245 train_time:22078ms step_avg:118.06ms
step:188/2245 train_time:22198ms step_avg:118.08ms
step:189/2245 train_time:22313ms step_avg:118.06ms
step:190/2245 train_time:22433ms step_avg:118.07ms
step:191/2245 train_time:22548ms step_avg:118.05ms
step:192/2245 train_time:22668ms step_avg:118.06ms
step:193/2245 train_time:22782ms step_avg:118.04ms
step:194/2245 train_time:22903ms step_avg:118.05ms
step:195/2245 train_time:23017ms step_avg:118.04ms
step:196/2245 train_time:23138ms step_avg:118.05ms
step:197/2245 train_time:23252ms step_avg:118.03ms
step:198/2245 train_time:23372ms step_avg:118.04ms
step:199/2245 train_time:23487ms step_avg:118.02ms
step:200/2245 train_time:23607ms step_avg:118.04ms
step:201/2245 train_time:23721ms step_avg:118.02ms
step:202/2245 train_time:23842ms step_avg:118.03ms
step:203/2245 train_time:23956ms step_avg:118.01ms
step:204/2245 train_time:24077ms step_avg:118.02ms
step:205/2245 train_time:24191ms step_avg:118.01ms
step:206/2245 train_time:24312ms step_avg:118.02ms
step:207/2245 train_time:24426ms step_avg:118.00ms
step:208/2245 train_time:24546ms step_avg:118.01ms
step:209/2245 train_time:24660ms step_avg:117.99ms
step:210/2245 train_time:24781ms step_avg:118.01ms
step:211/2245 train_time:24895ms step_avg:117.99ms
step:212/2245 train_time:25016ms step_avg:118.00ms
step:213/2245 train_time:25131ms step_avg:117.98ms
step:214/2245 train_time:25251ms step_avg:118.00ms
step:215/2245 train_time:25366ms step_avg:117.98ms
step:216/2245 train_time:25487ms step_avg:117.99ms
step:217/2245 train_time:25601ms step_avg:117.97ms
step:218/2245 train_time:25721ms step_avg:117.99ms
step:219/2245 train_time:25835ms step_avg:117.97ms
step:220/2245 train_time:25955ms step_avg:117.98ms
step:221/2245 train_time:26070ms step_avg:117.96ms
step:222/2245 train_time:26190ms step_avg:117.98ms
step:223/2245 train_time:26304ms step_avg:117.96ms
step:224/2245 train_time:26425ms step_avg:117.97ms
step:225/2245 train_time:26539ms step_avg:117.95ms
step:226/2245 train_time:26659ms step_avg:117.96ms
step:227/2245 train_time:26774ms step_avg:117.95ms
step:228/2245 train_time:26894ms step_avg:117.96ms
step:229/2245 train_time:27008ms step_avg:117.94ms
step:230/2245 train_time:27129ms step_avg:117.95ms
step:231/2245 train_time:27243ms step_avg:117.94ms
step:232/2245 train_time:27364ms step_avg:117.95ms
step:233/2245 train_time:27478ms step_avg:117.93ms
step:234/2245 train_time:27599ms step_avg:117.94ms
step:235/2245 train_time:27713ms step_avg:117.93ms
step:236/2245 train_time:27833ms step_avg:117.94ms
step:237/2245 train_time:27948ms step_avg:117.92ms
step:238/2245 train_time:28068ms step_avg:117.93ms
step:239/2245 train_time:28182ms step_avg:117.92ms
step:240/2245 train_time:28303ms step_avg:117.93ms
step:241/2245 train_time:28417ms step_avg:117.91ms
step:242/2245 train_time:28537ms step_avg:117.92ms
step:243/2245 train_time:28651ms step_avg:117.91ms
step:244/2245 train_time:28772ms step_avg:117.92ms
step:245/2245 train_time:28886ms step_avg:117.90ms
step:246/2245 train_time:29006ms step_avg:117.91ms
step:247/2245 train_time:29121ms step_avg:117.90ms
step:248/2245 train_time:29241ms step_avg:117.91ms
step:249/2245 train_time:29355ms step_avg:117.89ms
step:250/2245 train_time:29475ms step_avg:117.90ms
step:250/2245 val_loss:4.0983 train_time:29541ms step_avg:118.16ms
step:251/2245 train_time:29590ms step_avg:117.89ms
step:252/2245 train_time:29710ms step_avg:117.90ms
step:253/2245 train_time:29824ms step_avg:117.88ms
step:254/2245 train_time:29945ms step_avg:117.89ms
step:255/2245 train_time:30059ms step_avg:117.88ms
step:256/2245 train_time:30179ms step_avg:117.89ms
step:257/2245 train_time:30293ms step_avg:117.87ms
step:258/2245 train_time:30414ms step_avg:117.88ms
step:259/2245 train_time:30528ms step_avg:117.87ms
step:260/2245 train_time:30648ms step_avg:117.88ms
step:261/2245 train_time:30762ms step_avg:117.86ms
step:262/2245 train_time:30882ms step_avg:117.87ms
step:263/2245 train_time:30996ms step_avg:117.86ms
step:264/2245 train_time:31117ms step_avg:117.87ms
step:265/2245 train_time:31231ms step_avg:117.85ms
step:266/2245 train_time:31352ms step_avg:117.86ms
step:267/2245 train_time:31466ms step_avg:117.85ms
step:268/2245 train_time:31586ms step_avg:117.86ms
step:269/2245 train_time:31701ms step_avg:117.85ms
step:270/2245 train_time:31821ms step_avg:117.86ms
step:271/2245 train_time:31936ms step_avg:117.84ms
step:272/2245 train_time:32056ms step_avg:117.85ms
step:273/2245 train_time:32170ms step_avg:117.84ms
step:274/2245 train_time:32291ms step_avg:117.85ms
step:275/2245 train_time:32404ms step_avg:117.83ms
step:276/2245 train_time:32525ms step_avg:117.84ms
step:277/2245 train_time:32640ms step_avg:117.83ms
step:278/2245 train_time:32760ms step_avg:117.84ms
step:279/2245 train_time:32874ms step_avg:117.83ms
step:280/2245 train_time:32995ms step_avg:117.84ms
step:281/2245 train_time:33109ms step_avg:117.83ms
step:282/2245 train_time:33229ms step_avg:117.83ms
step:283/2245 train_time:33343ms step_avg:117.82ms
step:284/2245 train_time:33464ms step_avg:117.83ms
step:285/2245 train_time:33578ms step_avg:117.82ms
step:286/2245 train_time:33698ms step_avg:117.83ms
step:287/2245 train_time:33813ms step_avg:117.81ms
step:288/2245 train_time:33933ms step_avg:117.82ms
step:289/2245 train_time:34047ms step_avg:117.81ms
step:290/2245 train_time:34167ms step_avg:117.82ms
step:291/2245 train_time:34282ms step_avg:117.81ms
step:292/2245 train_time:34402ms step_avg:117.82ms
step:293/2245 train_time:34516ms step_avg:117.80ms
step:294/2245 train_time:34636ms step_avg:117.81ms
step:295/2245 train_time:34750ms step_avg:117.80ms
step:296/2245 train_time:34871ms step_avg:117.81ms
step:297/2245 train_time:34985ms step_avg:117.79ms
step:298/2245 train_time:35105ms step_avg:117.80ms
step:299/2245 train_time:35219ms step_avg:117.79ms
step:300/2245 train_time:35340ms step_avg:117.80ms
step:301/2245 train_time:35454ms step_avg:117.79ms
step:302/2245 train_time:35575ms step_avg:117.80ms
step:303/2245 train_time:35689ms step_avg:117.79ms
step:304/2245 train_time:35809ms step_avg:117.79ms
step:305/2245 train_time:35923ms step_avg:117.78ms
step:306/2245 train_time:36043ms step_avg:117.79ms
step:307/2245 train_time:36158ms step_avg:117.78ms
step:308/2245 train_time:36278ms step_avg:117.79ms
step:309/2245 train_time:36393ms step_avg:117.78ms
step:310/2245 train_time:36513ms step_avg:117.78ms
step:311/2245 train_time:36627ms step_avg:117.77ms
step:312/2245 train_time:36747ms step_avg:117.78ms
step:313/2245 train_time:36861ms step_avg:117.77ms
step:314/2245 train_time:36982ms step_avg:117.78ms
step:315/2245 train_time:37095ms step_avg:117.76ms
step:316/2245 train_time:37216ms step_avg:117.77ms
step:317/2245 train_time:37330ms step_avg:117.76ms
step:318/2245 train_time:37450ms step_avg:117.77ms
step:319/2245 train_time:37564ms step_avg:117.76ms
step:320/2245 train_time:37685ms step_avg:117.76ms
step:321/2245 train_time:37799ms step_avg:117.76ms
step:322/2245 train_time:37919ms step_avg:117.76ms
step:323/2245 train_time:38033ms step_avg:117.75ms
step:324/2245 train_time:38153ms step_avg:117.76ms
step:325/2245 train_time:38267ms step_avg:117.74ms
step:326/2245 train_time:38387ms step_avg:117.75ms
step:327/2245 train_time:38501ms step_avg:117.74ms
step:328/2245 train_time:38622ms step_avg:117.75ms
step:329/2245 train_time:38736ms step_avg:117.74ms
step:330/2245 train_time:38857ms step_avg:117.75ms
step:331/2245 train_time:38971ms step_avg:117.74ms
step:332/2245 train_time:39091ms step_avg:117.74ms
step:333/2245 train_time:39205ms step_avg:117.73ms
step:334/2245 train_time:39325ms step_avg:117.74ms
step:335/2245 train_time:39440ms step_avg:117.73ms
step:336/2245 train_time:39560ms step_avg:117.74ms
step:337/2245 train_time:39674ms step_avg:117.73ms
step:338/2245 train_time:39794ms step_avg:117.73ms
step:339/2245 train_time:39908ms step_avg:117.72ms
step:340/2245 train_time:40028ms step_avg:117.73ms
step:341/2245 train_time:40142ms step_avg:117.72ms
step:342/2245 train_time:40262ms step_avg:117.73ms
step:343/2245 train_time:40376ms step_avg:117.72ms
step:344/2245 train_time:40497ms step_avg:117.72ms
step:345/2245 train_time:40611ms step_avg:117.71ms
step:346/2245 train_time:40731ms step_avg:117.72ms
step:347/2245 train_time:40845ms step_avg:117.71ms
step:348/2245 train_time:40966ms step_avg:117.72ms
step:349/2245 train_time:41080ms step_avg:117.71ms
step:350/2245 train_time:41201ms step_avg:117.72ms
step:351/2245 train_time:41315ms step_avg:117.71ms
step:352/2245 train_time:41435ms step_avg:117.71ms
step:353/2245 train_time:41549ms step_avg:117.70ms
step:354/2245 train_time:41670ms step_avg:117.71ms
step:355/2245 train_time:41784ms step_avg:117.70ms
step:356/2245 train_time:41904ms step_avg:117.71ms
step:357/2245 train_time:42018ms step_avg:117.70ms
step:358/2245 train_time:42139ms step_avg:117.71ms
step:359/2245 train_time:42253ms step_avg:117.70ms
step:360/2245 train_time:42373ms step_avg:117.70ms
step:361/2245 train_time:42487ms step_avg:117.69ms
step:362/2245 train_time:42607ms step_avg:117.70ms
step:363/2245 train_time:42721ms step_avg:117.69ms
step:364/2245 train_time:42841ms step_avg:117.70ms
step:365/2245 train_time:42956ms step_avg:117.69ms
step:366/2245 train_time:43076ms step_avg:117.69ms
step:367/2245 train_time:43190ms step_avg:117.68ms
step:368/2245 train_time:43311ms step_avg:117.69ms
step:369/2245 train_time:43424ms step_avg:117.68ms
step:370/2245 train_time:43545ms step_avg:117.69ms
step:371/2245 train_time:43659ms step_avg:117.68ms
step:372/2245 train_time:43779ms step_avg:117.69ms
step:373/2245 train_time:43893ms step_avg:117.68ms
step:374/2245 train_time:44013ms step_avg:117.68ms
step:375/2245 train_time:44127ms step_avg:117.67ms
step:376/2245 train_time:44247ms step_avg:117.68ms
step:377/2245 train_time:44362ms step_avg:117.67ms
step:378/2245 train_time:44482ms step_avg:117.68ms
step:379/2245 train_time:44596ms step_avg:117.67ms
step:380/2245 train_time:44716ms step_avg:117.67ms
step:381/2245 train_time:44830ms step_avg:117.66ms
step:382/2245 train_time:44950ms step_avg:117.67ms
step:383/2245 train_time:45064ms step_avg:117.66ms
step:384/2245 train_time:45184ms step_avg:117.67ms
step:385/2245 train_time:45298ms step_avg:117.66ms
step:386/2245 train_time:45418ms step_avg:117.66ms
step:387/2245 train_time:45533ms step_avg:117.66ms
step:388/2245 train_time:45653ms step_avg:117.66ms
step:389/2245 train_time:45767ms step_avg:117.65ms
step:390/2245 train_time:45887ms step_avg:117.66ms
step:391/2245 train_time:46001ms step_avg:117.65ms
step:392/2245 train_time:46121ms step_avg:117.66ms
step:393/2245 train_time:46236ms step_avg:117.65ms
step:394/2245 train_time:46356ms step_avg:117.66ms
step:395/2245 train_time:46470ms step_avg:117.65ms
step:396/2245 train_time:46591ms step_avg:117.65ms
step:397/2245 train_time:46705ms step_avg:117.64ms
step:398/2245 train_time:46825ms step_avg:117.65ms
step:399/2245 train_time:46940ms step_avg:117.64ms
step:400/2245 train_time:47060ms step_avg:117.65ms
step:401/2245 train_time:47174ms step_avg:117.64ms
step:402/2245 train_time:47294ms step_avg:117.65ms
step:403/2245 train_time:47408ms step_avg:117.64ms
step:404/2245 train_time:47529ms step_avg:117.65ms
step:405/2245 train_time:47643ms step_avg:117.64ms
step:406/2245 train_time:47763ms step_avg:117.64ms
step:407/2245 train_time:47878ms step_avg:117.64ms
step:408/2245 train_time:47998ms step_avg:117.64ms
step:409/2245 train_time:48112ms step_avg:117.63ms
step:410/2245 train_time:48233ms step_avg:117.64ms
step:411/2245 train_time:48346ms step_avg:117.63ms
step:412/2245 train_time:48467ms step_avg:117.64ms
step:413/2245 train_time:48581ms step_avg:117.63ms
step:414/2245 train_time:48701ms step_avg:117.64ms
step:415/2245 train_time:48815ms step_avg:117.63ms
step:416/2245 train_time:48935ms step_avg:117.63ms
step:417/2245 train_time:49049ms step_avg:117.62ms
step:418/2245 train_time:49170ms step_avg:117.63ms
step:419/2245 train_time:49284ms step_avg:117.62ms
step:420/2245 train_time:49404ms step_avg:117.63ms
step:421/2245 train_time:49518ms step_avg:117.62ms
step:422/2245 train_time:49638ms step_avg:117.63ms
step:423/2245 train_time:49752ms step_avg:117.62ms
step:424/2245 train_time:49872ms step_avg:117.62ms
step:425/2245 train_time:49986ms step_avg:117.61ms
step:426/2245 train_time:50106ms step_avg:117.62ms
step:427/2245 train_time:50220ms step_avg:117.61ms
step:428/2245 train_time:50341ms step_avg:117.62ms
step:429/2245 train_time:50455ms step_avg:117.61ms
step:430/2245 train_time:50575ms step_avg:117.62ms
step:431/2245 train_time:50689ms step_avg:117.61ms
step:432/2245 train_time:50809ms step_avg:117.61ms
step:433/2245 train_time:50923ms step_avg:117.61ms
step:434/2245 train_time:51044ms step_avg:117.61ms
step:435/2245 train_time:51158ms step_avg:117.60ms
step:436/2245 train_time:51278ms step_avg:117.61ms
step:437/2245 train_time:51392ms step_avg:117.60ms
step:438/2245 train_time:51513ms step_avg:117.61ms
step:439/2245 train_time:51627ms step_avg:117.60ms
step:440/2245 train_time:51747ms step_avg:117.61ms
step:441/2245 train_time:51861ms step_avg:117.60ms
step:442/2245 train_time:51982ms step_avg:117.61ms
step:443/2245 train_time:52096ms step_avg:117.60ms
step:444/2245 train_time:52216ms step_avg:117.60ms
step:445/2245 train_time:52330ms step_avg:117.60ms
step:446/2245 train_time:52450ms step_avg:117.60ms
step:447/2245 train_time:52564ms step_avg:117.59ms
step:448/2245 train_time:52685ms step_avg:117.60ms
step:449/2245 train_time:52799ms step_avg:117.59ms
step:450/2245 train_time:52920ms step_avg:117.60ms
step:451/2245 train_time:53034ms step_avg:117.59ms
step:452/2245 train_time:53154ms step_avg:117.60ms
step:453/2245 train_time:53268ms step_avg:117.59ms
step:454/2245 train_time:53389ms step_avg:117.60ms
step:455/2245 train_time:53502ms step_avg:117.59ms
step:456/2245 train_time:53623ms step_avg:117.59ms
step:457/2245 train_time:53737ms step_avg:117.59ms
step:458/2245 train_time:53857ms step_avg:117.59ms
step:459/2245 train_time:53971ms step_avg:117.58ms
step:460/2245 train_time:54091ms step_avg:117.59ms
step:461/2245 train_time:54205ms step_avg:117.58ms
step:462/2245 train_time:54325ms step_avg:117.59ms
step:463/2245 train_time:54439ms step_avg:117.58ms
step:464/2245 train_time:54559ms step_avg:117.58ms
step:465/2245 train_time:54673ms step_avg:117.58ms
step:466/2245 train_time:54793ms step_avg:117.58ms
step:467/2245 train_time:54908ms step_avg:117.58ms
step:468/2245 train_time:55028ms step_avg:117.58ms
step:469/2245 train_time:55142ms step_avg:117.57ms
step:470/2245 train_time:55262ms step_avg:117.58ms
step:471/2245 train_time:55376ms step_avg:117.57ms
step:472/2245 train_time:55497ms step_avg:117.58ms
step:473/2245 train_time:55611ms step_avg:117.57ms
step:474/2245 train_time:55731ms step_avg:117.58ms
step:475/2245 train_time:55845ms step_avg:117.57ms
step:476/2245 train_time:55965ms step_avg:117.57ms
step:477/2245 train_time:56079ms step_avg:117.57ms
step:478/2245 train_time:56200ms step_avg:117.57ms
step:479/2245 train_time:56314ms step_avg:117.57ms
step:480/2245 train_time:56434ms step_avg:117.57ms
step:481/2245 train_time:56549ms step_avg:117.56ms
step:482/2245 train_time:56668ms step_avg:117.57ms
step:483/2245 train_time:56783ms step_avg:117.56ms
step:484/2245 train_time:56903ms step_avg:117.57ms
step:485/2245 train_time:57017ms step_avg:117.56ms
step:486/2245 train_time:57137ms step_avg:117.57ms
step:487/2245 train_time:57252ms step_avg:117.56ms
step:488/2245 train_time:57372ms step_avg:117.57ms
step:489/2245 train_time:57486ms step_avg:117.56ms
step:490/2245 train_time:57606ms step_avg:117.56ms
step:491/2245 train_time:57720ms step_avg:117.56ms
step:492/2245 train_time:57840ms step_avg:117.56ms
step:493/2245 train_time:57954ms step_avg:117.55ms
step:494/2245 train_time:58074ms step_avg:117.56ms
step:495/2245 train_time:58188ms step_avg:117.55ms
step:496/2245 train_time:58309ms step_avg:117.56ms
step:497/2245 train_time:58423ms step_avg:117.55ms
step:498/2245 train_time:58543ms step_avg:117.56ms
step:499/2245 train_time:58657ms step_avg:117.55ms
step:500/2245 train_time:58777ms step_avg:117.55ms
step:500/2245 val_loss:3.8292 train_time:58843ms step_avg:117.69ms
step:501/2245 train_time:58893ms step_avg:117.55ms
step:502/2245 train_time:59012ms step_avg:117.55ms
step:503/2245 train_time:59126ms step_avg:117.55ms
step:504/2245 train_time:59247ms step_avg:117.55ms
step:505/2245 train_time:59361ms step_avg:117.55ms
step:506/2245 train_time:59481ms step_avg:117.55ms
step:507/2245 train_time:59595ms step_avg:117.54ms
step:508/2245 train_time:59715ms step_avg:117.55ms
step:509/2245 train_time:59830ms step_avg:117.54ms
step:510/2245 train_time:59950ms step_avg:117.55ms
step:511/2245 train_time:60065ms step_avg:117.54ms
step:512/2245 train_time:60185ms step_avg:117.55ms
step:513/2245 train_time:60299ms step_avg:117.54ms
step:514/2245 train_time:60420ms step_avg:117.55ms
step:515/2245 train_time:60534ms step_avg:117.54ms
step:516/2245 train_time:60654ms step_avg:117.55ms
step:517/2245 train_time:60768ms step_avg:117.54ms
step:518/2245 train_time:60889ms step_avg:117.55ms
step:519/2245 train_time:61002ms step_avg:117.54ms
step:520/2245 train_time:61123ms step_avg:117.54ms
step:521/2245 train_time:61237ms step_avg:117.54ms
step:522/2245 train_time:61357ms step_avg:117.54ms
step:523/2245 train_time:61471ms step_avg:117.53ms
step:524/2245 train_time:61591ms step_avg:117.54ms
step:525/2245 train_time:61705ms step_avg:117.53ms
step:526/2245 train_time:61825ms step_avg:117.54ms
step:527/2245 train_time:61939ms step_avg:117.53ms
step:528/2245 train_time:62060ms step_avg:117.54ms
step:529/2245 train_time:62174ms step_avg:117.53ms
step:530/2245 train_time:62294ms step_avg:117.54ms
step:531/2245 train_time:62408ms step_avg:117.53ms
step:532/2245 train_time:62529ms step_avg:117.53ms
step:533/2245 train_time:62643ms step_avg:117.53ms
step:534/2245 train_time:62763ms step_avg:117.53ms
step:535/2245 train_time:62877ms step_avg:117.53ms
step:536/2245 train_time:62997ms step_avg:117.53ms
step:537/2245 train_time:63111ms step_avg:117.53ms
step:538/2245 train_time:63232ms step_avg:117.53ms
step:539/2245 train_time:63346ms step_avg:117.52ms
step:540/2245 train_time:63466ms step_avg:117.53ms
step:541/2245 train_time:63580ms step_avg:117.52ms
step:542/2245 train_time:63700ms step_avg:117.53ms
step:543/2245 train_time:63814ms step_avg:117.52ms
step:544/2245 train_time:63934ms step_avg:117.53ms
step:545/2245 train_time:64048ms step_avg:117.52ms
step:546/2245 train_time:64169ms step_avg:117.53ms
step:547/2245 train_time:64283ms step_avg:117.52ms
step:548/2245 train_time:64404ms step_avg:117.52ms
step:549/2245 train_time:64518ms step_avg:117.52ms
step:550/2245 train_time:64638ms step_avg:117.52ms
step:551/2245 train_time:64752ms step_avg:117.52ms
step:552/2245 train_time:64873ms step_avg:117.52ms
step:553/2245 train_time:64987ms step_avg:117.52ms
step:554/2245 train_time:65107ms step_avg:117.52ms
step:555/2245 train_time:65221ms step_avg:117.52ms
step:556/2245 train_time:65342ms step_avg:117.52ms
step:557/2245 train_time:65456ms step_avg:117.51ms
step:558/2245 train_time:65576ms step_avg:117.52ms
step:559/2245 train_time:65690ms step_avg:117.51ms
step:560/2245 train_time:65811ms step_avg:117.52ms
step:561/2245 train_time:65925ms step_avg:117.51ms
step:562/2245 train_time:66045ms step_avg:117.52ms
step:563/2245 train_time:66160ms step_avg:117.51ms
step:564/2245 train_time:66280ms step_avg:117.52ms
step:565/2245 train_time:66394ms step_avg:117.51ms
step:566/2245 train_time:66514ms step_avg:117.52ms
step:567/2245 train_time:66629ms step_avg:117.51ms
step:568/2245 train_time:66749ms step_avg:117.52ms
step:569/2245 train_time:66863ms step_avg:117.51ms
step:570/2245 train_time:66984ms step_avg:117.52ms
step:571/2245 train_time:67098ms step_avg:117.51ms
step:572/2245 train_time:67218ms step_avg:117.51ms
step:573/2245 train_time:67332ms step_avg:117.51ms
step:574/2245 train_time:67452ms step_avg:117.51ms
step:575/2245 train_time:67566ms step_avg:117.51ms
step:576/2245 train_time:67687ms step_avg:117.51ms
step:577/2245 train_time:67801ms step_avg:117.51ms
step:578/2245 train_time:67921ms step_avg:117.51ms
step:579/2245 train_time:68036ms step_avg:117.51ms
step:580/2245 train_time:68156ms step_avg:117.51ms
step:581/2245 train_time:68270ms step_avg:117.50ms
step:582/2245 train_time:68391ms step_avg:117.51ms
step:583/2245 train_time:68505ms step_avg:117.50ms
step:584/2245 train_time:68625ms step_avg:117.51ms
step:585/2245 train_time:68740ms step_avg:117.50ms
step:586/2245 train_time:68860ms step_avg:117.51ms
step:587/2245 train_time:68975ms step_avg:117.50ms
step:588/2245 train_time:69095ms step_avg:117.51ms
step:589/2245 train_time:69209ms step_avg:117.50ms
step:590/2245 train_time:69330ms step_avg:117.51ms
step:591/2245 train_time:69444ms step_avg:117.50ms
step:592/2245 train_time:69564ms step_avg:117.51ms
step:593/2245 train_time:69678ms step_avg:117.50ms
step:594/2245 train_time:69799ms step_avg:117.51ms
step:595/2245 train_time:69913ms step_avg:117.50ms
step:596/2245 train_time:70034ms step_avg:117.51ms
step:597/2245 train_time:70148ms step_avg:117.50ms
step:598/2245 train_time:70268ms step_avg:117.51ms
step:599/2245 train_time:70382ms step_avg:117.50ms
step:600/2245 train_time:70503ms step_avg:117.50ms
step:601/2245 train_time:70617ms step_avg:117.50ms
step:602/2245 train_time:70737ms step_avg:117.50ms
step:603/2245 train_time:70851ms step_avg:117.50ms
step:604/2245 train_time:70972ms step_avg:117.50ms
step:605/2245 train_time:71086ms step_avg:117.50ms
step:606/2245 train_time:71207ms step_avg:117.50ms
step:607/2245 train_time:71321ms step_avg:117.50ms
step:608/2245 train_time:71441ms step_avg:117.50ms
step:609/2245 train_time:71555ms step_avg:117.50ms
step:610/2245 train_time:71675ms step_avg:117.50ms
step:611/2245 train_time:71790ms step_avg:117.50ms
step:612/2245 train_time:71910ms step_avg:117.50ms
step:613/2245 train_time:72024ms step_avg:117.49ms
step:614/2245 train_time:72145ms step_avg:117.50ms
step:615/2245 train_time:72258ms step_avg:117.49ms
step:616/2245 train_time:72379ms step_avg:117.50ms
step:617/2245 train_time:72493ms step_avg:117.49ms
step:618/2245 train_time:72613ms step_avg:117.50ms
step:619/2245 train_time:72727ms step_avg:117.49ms
step:620/2245 train_time:72848ms step_avg:117.50ms
step:621/2245 train_time:72962ms step_avg:117.49ms
step:622/2245 train_time:73083ms step_avg:117.50ms
step:623/2245 train_time:73197ms step_avg:117.49ms
step:624/2245 train_time:73317ms step_avg:117.50ms
step:625/2245 train_time:73431ms step_avg:117.49ms
step:626/2245 train_time:73551ms step_avg:117.49ms
step:627/2245 train_time:73665ms step_avg:117.49ms
step:628/2245 train_time:73786ms step_avg:117.49ms
step:629/2245 train_time:73900ms step_avg:117.49ms
step:630/2245 train_time:74020ms step_avg:117.49ms
step:631/2245 train_time:74134ms step_avg:117.49ms
step:632/2245 train_time:74254ms step_avg:117.49ms
step:633/2245 train_time:74369ms step_avg:117.49ms
step:634/2245 train_time:74489ms step_avg:117.49ms
step:635/2245 train_time:74603ms step_avg:117.49ms
step:636/2245 train_time:74724ms step_avg:117.49ms
step:637/2245 train_time:74837ms step_avg:117.48ms
step:638/2245 train_time:74957ms step_avg:117.49ms
step:639/2245 train_time:75072ms step_avg:117.48ms
step:640/2245 train_time:75192ms step_avg:117.49ms
step:641/2245 train_time:75307ms step_avg:117.48ms
step:642/2245 train_time:75427ms step_avg:117.49ms
step:643/2245 train_time:75541ms step_avg:117.48ms
step:644/2245 train_time:75662ms step_avg:117.49ms
step:645/2245 train_time:75776ms step_avg:117.48ms
step:646/2245 train_time:75896ms step_avg:117.49ms
step:647/2245 train_time:76011ms step_avg:117.48ms
step:648/2245 train_time:76131ms step_avg:117.49ms
step:649/2245 train_time:76245ms step_avg:117.48ms
step:650/2245 train_time:76366ms step_avg:117.49ms
step:651/2245 train_time:76480ms step_avg:117.48ms
step:652/2245 train_time:76601ms step_avg:117.49ms
step:653/2245 train_time:76715ms step_avg:117.48ms
step:654/2245 train_time:76835ms step_avg:117.49ms
step:655/2245 train_time:76950ms step_avg:117.48ms
step:656/2245 train_time:77070ms step_avg:117.49ms
step:657/2245 train_time:77185ms step_avg:117.48ms
step:658/2245 train_time:77305ms step_avg:117.48ms
step:659/2245 train_time:77419ms step_avg:117.48ms
step:660/2245 train_time:77539ms step_avg:117.48ms
step:661/2245 train_time:77653ms step_avg:117.48ms
step:662/2245 train_time:77773ms step_avg:117.48ms
step:663/2245 train_time:77887ms step_avg:117.48ms
step:664/2245 train_time:78008ms step_avg:117.48ms
step:665/2245 train_time:78122ms step_avg:117.48ms
step:666/2245 train_time:78243ms step_avg:117.48ms
step:667/2245 train_time:78357ms step_avg:117.48ms
step:668/2245 train_time:78478ms step_avg:117.48ms
step:669/2245 train_time:78592ms step_avg:117.48ms
step:670/2245 train_time:78712ms step_avg:117.48ms
step:671/2245 train_time:78827ms step_avg:117.48ms
step:672/2245 train_time:78947ms step_avg:117.48ms
step:673/2245 train_time:79061ms step_avg:117.48ms
step:674/2245 train_time:79181ms step_avg:117.48ms
step:675/2245 train_time:79296ms step_avg:117.47ms
step:676/2245 train_time:79416ms step_avg:117.48ms
step:677/2245 train_time:79530ms step_avg:117.47ms
step:678/2245 train_time:79651ms step_avg:117.48ms
step:679/2245 train_time:79765ms step_avg:117.47ms
step:680/2245 train_time:79885ms step_avg:117.48ms
step:681/2245 train_time:79999ms step_avg:117.47ms
step:682/2245 train_time:80120ms step_avg:117.48ms
step:683/2245 train_time:80234ms step_avg:117.47ms
step:684/2245 train_time:80354ms step_avg:117.48ms
step:685/2245 train_time:80469ms step_avg:117.47ms
step:686/2245 train_time:80589ms step_avg:117.48ms
step:687/2245 train_time:80703ms step_avg:117.47ms
step:688/2245 train_time:80824ms step_avg:117.48ms
step:689/2245 train_time:80937ms step_avg:117.47ms
step:690/2245 train_time:81057ms step_avg:117.47ms
step:691/2245 train_time:81171ms step_avg:117.47ms
step:692/2245 train_time:81292ms step_avg:117.47ms
step:693/2245 train_time:81406ms step_avg:117.47ms
step:694/2245 train_time:81526ms step_avg:117.47ms
step:695/2245 train_time:81640ms step_avg:117.47ms
step:696/2245 train_time:81760ms step_avg:117.47ms
step:697/2245 train_time:81874ms step_avg:117.47ms
step:698/2245 train_time:81995ms step_avg:117.47ms
step:699/2245 train_time:82109ms step_avg:117.47ms
step:700/2245 train_time:82229ms step_avg:117.47ms
step:701/2245 train_time:82343ms step_avg:117.46ms
step:702/2245 train_time:82463ms step_avg:117.47ms
step:703/2245 train_time:82577ms step_avg:117.46ms
step:704/2245 train_time:82697ms step_avg:117.47ms
step:705/2245 train_time:82811ms step_avg:117.46ms
step:706/2245 train_time:82931ms step_avg:117.47ms
step:707/2245 train_time:83045ms step_avg:117.46ms
step:708/2245 train_time:83166ms step_avg:117.47ms
step:709/2245 train_time:83280ms step_avg:117.46ms
step:710/2245 train_time:83401ms step_avg:117.47ms
step:711/2245 train_time:83514ms step_avg:117.46ms
step:712/2245 train_time:83634ms step_avg:117.46ms
step:713/2245 train_time:83749ms step_avg:117.46ms
step:714/2245 train_time:83869ms step_avg:117.46ms
step:715/2245 train_time:83984ms step_avg:117.46ms
step:716/2245 train_time:84104ms step_avg:117.46ms
step:717/2245 train_time:84218ms step_avg:117.46ms
step:718/2245 train_time:84338ms step_avg:117.46ms
step:719/2245 train_time:84452ms step_avg:117.46ms
step:720/2245 train_time:84573ms step_avg:117.46ms
step:721/2245 train_time:84687ms step_avg:117.46ms
step:722/2245 train_time:84807ms step_avg:117.46ms
step:723/2245 train_time:84921ms step_avg:117.46ms
step:724/2245 train_time:85042ms step_avg:117.46ms
step:725/2245 train_time:85156ms step_avg:117.46ms
step:726/2245 train_time:85276ms step_avg:117.46ms
step:727/2245 train_time:85391ms step_avg:117.46ms
step:728/2245 train_time:85511ms step_avg:117.46ms
step:729/2245 train_time:85625ms step_avg:117.46ms
step:730/2245 train_time:85746ms step_avg:117.46ms
step:731/2245 train_time:85860ms step_avg:117.46ms
step:732/2245 train_time:85981ms step_avg:117.46ms
step:733/2245 train_time:86095ms step_avg:117.46ms
step:734/2245 train_time:86215ms step_avg:117.46ms
step:735/2245 train_time:86330ms step_avg:117.46ms
step:736/2245 train_time:86451ms step_avg:117.46ms
step:737/2245 train_time:86567ms step_avg:117.46ms
step:738/2245 train_time:86688ms step_avg:117.46ms
step:739/2245 train_time:86804ms step_avg:117.46ms
step:740/2245 train_time:86925ms step_avg:117.47ms
step:741/2245 train_time:87041ms step_avg:117.46ms
step:742/2245 train_time:87162ms step_avg:117.47ms
step:743/2245 train_time:87278ms step_avg:117.47ms
step:744/2245 train_time:87399ms step_avg:117.47ms
step:745/2245 train_time:87515ms step_avg:117.47ms
step:746/2245 train_time:87636ms step_avg:117.48ms
step:747/2245 train_time:87752ms step_avg:117.47ms
step:748/2245 train_time:87874ms step_avg:117.48ms
step:749/2245 train_time:87991ms step_avg:117.48ms
step:750/2245 train_time:88112ms step_avg:117.48ms
step:750/2245 val_loss:3.6733 train_time:88178ms step_avg:117.57ms
step:751/2245 train_time:88228ms step_avg:117.48ms
step:752/2245 train_time:88350ms step_avg:117.49ms
step:753/2245 train_time:88465ms step_avg:117.48ms
step:754/2245 train_time:88587ms step_avg:117.49ms
step:755/2245 train_time:88702ms step_avg:117.49ms
step:756/2245 train_time:88823ms step_avg:117.49ms
step:757/2245 train_time:88939ms step_avg:117.49ms
step:758/2245 train_time:89061ms step_avg:117.49ms
step:759/2245 train_time:89176ms step_avg:117.49ms
step:760/2245 train_time:89299ms step_avg:117.50ms
step:761/2245 train_time:89415ms step_avg:117.50ms
step:762/2245 train_time:89537ms step_avg:117.50ms
step:763/2245 train_time:89653ms step_avg:117.50ms
step:764/2245 train_time:89774ms step_avg:117.51ms
step:765/2245 train_time:89890ms step_avg:117.50ms
step:766/2245 train_time:90012ms step_avg:117.51ms
step:767/2245 train_time:90128ms step_avg:117.51ms
step:768/2245 train_time:90250ms step_avg:117.51ms
step:769/2245 train_time:90365ms step_avg:117.51ms
step:770/2245 train_time:90487ms step_avg:117.52ms
step:771/2245 train_time:90602ms step_avg:117.51ms
step:772/2245 train_time:90725ms step_avg:117.52ms
step:773/2245 train_time:90840ms step_avg:117.52ms
step:774/2245 train_time:90962ms step_avg:117.52ms
step:775/2245 train_time:91078ms step_avg:117.52ms
step:776/2245 train_time:91200ms step_avg:117.53ms
step:777/2245 train_time:91316ms step_avg:117.52ms
step:778/2245 train_time:91438ms step_avg:117.53ms
step:779/2245 train_time:91554ms step_avg:117.53ms
step:780/2245 train_time:91677ms step_avg:117.53ms
step:781/2245 train_time:91792ms step_avg:117.53ms
step:782/2245 train_time:91914ms step_avg:117.54ms
step:783/2245 train_time:92030ms step_avg:117.53ms
step:784/2245 train_time:92151ms step_avg:117.54ms
step:785/2245 train_time:92267ms step_avg:117.54ms
step:786/2245 train_time:92389ms step_avg:117.54ms
step:787/2245 train_time:92505ms step_avg:117.54ms
step:788/2245 train_time:92626ms step_avg:117.55ms
step:789/2245 train_time:92742ms step_avg:117.54ms
step:790/2245 train_time:92864ms step_avg:117.55ms
step:791/2245 train_time:92979ms step_avg:117.55ms
step:792/2245 train_time:93101ms step_avg:117.55ms
step:793/2245 train_time:93217ms step_avg:117.55ms
step:794/2245 train_time:93338ms step_avg:117.55ms
step:795/2245 train_time:93454ms step_avg:117.55ms
step:796/2245 train_time:93576ms step_avg:117.56ms
step:797/2245 train_time:93692ms step_avg:117.56ms
step:798/2245 train_time:93815ms step_avg:117.56ms
step:799/2245 train_time:93930ms step_avg:117.56ms
step:800/2245 train_time:94052ms step_avg:117.56ms
step:801/2245 train_time:94167ms step_avg:117.56ms
step:802/2245 train_time:94290ms step_avg:117.57ms
step:803/2245 train_time:94405ms step_avg:117.57ms
step:804/2245 train_time:94527ms step_avg:117.57ms
step:805/2245 train_time:94642ms step_avg:117.57ms
step:806/2245 train_time:94764ms step_avg:117.57ms
step:807/2245 train_time:94880ms step_avg:117.57ms
step:808/2245 train_time:95002ms step_avg:117.58ms
step:809/2245 train_time:95117ms step_avg:117.57ms
step:810/2245 train_time:95239ms step_avg:117.58ms
step:811/2245 train_time:95355ms step_avg:117.58ms
step:812/2245 train_time:95478ms step_avg:117.58ms
step:813/2245 train_time:95593ms step_avg:117.58ms
step:814/2245 train_time:95715ms step_avg:117.59ms
step:815/2245 train_time:95831ms step_avg:117.58ms
step:816/2245 train_time:95952ms step_avg:117.59ms
step:817/2245 train_time:96068ms step_avg:117.59ms
step:818/2245 train_time:96190ms step_avg:117.59ms
step:819/2245 train_time:96305ms step_avg:117.59ms
step:820/2245 train_time:96427ms step_avg:117.59ms
step:821/2245 train_time:96543ms step_avg:117.59ms
step:822/2245 train_time:96665ms step_avg:117.60ms
step:823/2245 train_time:96780ms step_avg:117.59ms
step:824/2245 train_time:96902ms step_avg:117.60ms
step:825/2245 train_time:97018ms step_avg:117.60ms
step:826/2245 train_time:97140ms step_avg:117.60ms
step:827/2245 train_time:97256ms step_avg:117.60ms
step:828/2245 train_time:97379ms step_avg:117.61ms
step:829/2245 train_time:97494ms step_avg:117.60ms
step:830/2245 train_time:97616ms step_avg:117.61ms
step:831/2245 train_time:97732ms step_avg:117.61ms
step:832/2245 train_time:97854ms step_avg:117.61ms
step:833/2245 train_time:97969ms step_avg:117.61ms
step:834/2245 train_time:98091ms step_avg:117.61ms
step:835/2245 train_time:98206ms step_avg:117.61ms
step:836/2245 train_time:98328ms step_avg:117.62ms
step:837/2245 train_time:98444ms step_avg:117.61ms
step:838/2245 train_time:98565ms step_avg:117.62ms
step:839/2245 train_time:98681ms step_avg:117.62ms
step:840/2245 train_time:98803ms step_avg:117.62ms
step:841/2245 train_time:98919ms step_avg:117.62ms
step:842/2245 train_time:99041ms step_avg:117.63ms
step:843/2245 train_time:99157ms step_avg:117.62ms
step:844/2245 train_time:99280ms step_avg:117.63ms
step:845/2245 train_time:99396ms step_avg:117.63ms
step:846/2245 train_time:99518ms step_avg:117.63ms
step:847/2245 train_time:99634ms step_avg:117.63ms
step:848/2245 train_time:99756ms step_avg:117.64ms
step:849/2245 train_time:99871ms step_avg:117.63ms
step:850/2245 train_time:99993ms step_avg:117.64ms
step:851/2245 train_time:100108ms step_avg:117.64ms
step:852/2245 train_time:100230ms step_avg:117.64ms
step:853/2245 train_time:100346ms step_avg:117.64ms
step:854/2245 train_time:100468ms step_avg:117.64ms
step:855/2245 train_time:100583ms step_avg:117.64ms
step:856/2245 train_time:100705ms step_avg:117.65ms
step:857/2245 train_time:100820ms step_avg:117.64ms
step:858/2245 train_time:100943ms step_avg:117.65ms
step:859/2245 train_time:101058ms step_avg:117.65ms
step:860/2245 train_time:101181ms step_avg:117.65ms
step:861/2245 train_time:101296ms step_avg:117.65ms
step:862/2245 train_time:101418ms step_avg:117.65ms
step:863/2245 train_time:101534ms step_avg:117.65ms
step:864/2245 train_time:101656ms step_avg:117.66ms
step:865/2245 train_time:101771ms step_avg:117.65ms
step:866/2245 train_time:101893ms step_avg:117.66ms
step:867/2245 train_time:102008ms step_avg:117.66ms
step:868/2245 train_time:102130ms step_avg:117.66ms
step:869/2245 train_time:102246ms step_avg:117.66ms
step:870/2245 train_time:102368ms step_avg:117.66ms
step:871/2245 train_time:102484ms step_avg:117.66ms
step:872/2245 train_time:102605ms step_avg:117.67ms
step:873/2245 train_time:102721ms step_avg:117.66ms
step:874/2245 train_time:102843ms step_avg:117.67ms
step:875/2245 train_time:102959ms step_avg:117.67ms
step:876/2245 train_time:103081ms step_avg:117.67ms
step:877/2245 train_time:103197ms step_avg:117.67ms
step:878/2245 train_time:103319ms step_avg:117.68ms
step:879/2245 train_time:103435ms step_avg:117.67ms
step:880/2245 train_time:103557ms step_avg:117.68ms
step:881/2245 train_time:103673ms step_avg:117.68ms
step:882/2245 train_time:103796ms step_avg:117.68ms
step:883/2245 train_time:103910ms step_avg:117.68ms
step:884/2245 train_time:104033ms step_avg:117.68ms
step:885/2245 train_time:104148ms step_avg:117.68ms
step:886/2245 train_time:104270ms step_avg:117.69ms
step:887/2245 train_time:104386ms step_avg:117.68ms
step:888/2245 train_time:104508ms step_avg:117.69ms
step:889/2245 train_time:104623ms step_avg:117.69ms
step:890/2245 train_time:104745ms step_avg:117.69ms
step:891/2245 train_time:104862ms step_avg:117.69ms
step:892/2245 train_time:104984ms step_avg:117.69ms
step:893/2245 train_time:105099ms step_avg:117.69ms
step:894/2245 train_time:105222ms step_avg:117.70ms
step:895/2245 train_time:105338ms step_avg:117.70ms
step:896/2245 train_time:105460ms step_avg:117.70ms
step:897/2245 train_time:105576ms step_avg:117.70ms
step:898/2245 train_time:105699ms step_avg:117.70ms
step:899/2245 train_time:105815ms step_avg:117.70ms
step:900/2245 train_time:105938ms step_avg:117.71ms
step:901/2245 train_time:106053ms step_avg:117.71ms
step:902/2245 train_time:106176ms step_avg:117.71ms
step:903/2245 train_time:106291ms step_avg:117.71ms
step:904/2245 train_time:106413ms step_avg:117.71ms
step:905/2245 train_time:106528ms step_avg:117.71ms
step:906/2245 train_time:106650ms step_avg:117.72ms
step:907/2245 train_time:106766ms step_avg:117.71ms
step:908/2245 train_time:106888ms step_avg:117.72ms
step:909/2245 train_time:107004ms step_avg:117.72ms
step:910/2245 train_time:107125ms step_avg:117.72ms
step:911/2245 train_time:107241ms step_avg:117.72ms
step:912/2245 train_time:107363ms step_avg:117.72ms
step:913/2245 train_time:107478ms step_avg:117.72ms
step:914/2245 train_time:107600ms step_avg:117.72ms
step:915/2245 train_time:107716ms step_avg:117.72ms
step:916/2245 train_time:107839ms step_avg:117.73ms
step:917/2245 train_time:107955ms step_avg:117.73ms
step:918/2245 train_time:108077ms step_avg:117.73ms
step:919/2245 train_time:108193ms step_avg:117.73ms
step:920/2245 train_time:108315ms step_avg:117.73ms
step:921/2245 train_time:108431ms step_avg:117.73ms
step:922/2245 train_time:108552ms step_avg:117.74ms
step:923/2245 train_time:108668ms step_avg:117.73ms
step:924/2245 train_time:108790ms step_avg:117.74ms
step:925/2245 train_time:108906ms step_avg:117.74ms
step:926/2245 train_time:109028ms step_avg:117.74ms
step:927/2245 train_time:109143ms step_avg:117.74ms
step:928/2245 train_time:109265ms step_avg:117.74ms
step:929/2245 train_time:109381ms step_avg:117.74ms
step:930/2245 train_time:109502ms step_avg:117.74ms
step:931/2245 train_time:109618ms step_avg:117.74ms
step:932/2245 train_time:109740ms step_avg:117.75ms
step:933/2245 train_time:109855ms step_avg:117.74ms
step:934/2245 train_time:109978ms step_avg:117.75ms
step:935/2245 train_time:110094ms step_avg:117.75ms
step:936/2245 train_time:110215ms step_avg:117.75ms
step:937/2245 train_time:110330ms step_avg:117.75ms
step:938/2245 train_time:110452ms step_avg:117.75ms
step:939/2245 train_time:110568ms step_avg:117.75ms
step:940/2245 train_time:110690ms step_avg:117.76ms
step:941/2245 train_time:110805ms step_avg:117.75ms
step:942/2245 train_time:110928ms step_avg:117.76ms
step:943/2245 train_time:111043ms step_avg:117.75ms
step:944/2245 train_time:111165ms step_avg:117.76ms
step:945/2245 train_time:111281ms step_avg:117.76ms
step:946/2245 train_time:111403ms step_avg:117.76ms
step:947/2245 train_time:111519ms step_avg:117.76ms
step:948/2245 train_time:111641ms step_avg:117.77ms
step:949/2245 train_time:111757ms step_avg:117.76ms
step:950/2245 train_time:111879ms step_avg:117.77ms
step:951/2245 train_time:111995ms step_avg:117.77ms
step:952/2245 train_time:112118ms step_avg:117.77ms
step:953/2245 train_time:112233ms step_avg:117.77ms
step:954/2245 train_time:112355ms step_avg:117.77ms
step:955/2245 train_time:112471ms step_avg:117.77ms
step:956/2245 train_time:112593ms step_avg:117.77ms
step:957/2245 train_time:112708ms step_avg:117.77ms
step:958/2245 train_time:112830ms step_avg:117.78ms
step:959/2245 train_time:112946ms step_avg:117.77ms
step:960/2245 train_time:113068ms step_avg:117.78ms
step:961/2245 train_time:113183ms step_avg:117.78ms
step:962/2245 train_time:113305ms step_avg:117.78ms
step:963/2245 train_time:113421ms step_avg:117.78ms
step:964/2245 train_time:113543ms step_avg:117.78ms
step:965/2245 train_time:113659ms step_avg:117.78ms
step:966/2245 train_time:113781ms step_avg:117.79ms
step:967/2245 train_time:113897ms step_avg:117.78ms
step:968/2245 train_time:114019ms step_avg:117.79ms
step:969/2245 train_time:114135ms step_avg:117.79ms
step:970/2245 train_time:114257ms step_avg:117.79ms
step:971/2245 train_time:114373ms step_avg:117.79ms
step:972/2245 train_time:114495ms step_avg:117.79ms
step:973/2245 train_time:114610ms step_avg:117.79ms
step:974/2245 train_time:114732ms step_avg:117.79ms
step:975/2245 train_time:114847ms step_avg:117.79ms
step:976/2245 train_time:114969ms step_avg:117.80ms
step:977/2245 train_time:115085ms step_avg:117.79ms
step:978/2245 train_time:115207ms step_avg:117.80ms
step:979/2245 train_time:115323ms step_avg:117.80ms
step:980/2245 train_time:115445ms step_avg:117.80ms
step:981/2245 train_time:115561ms step_avg:117.80ms
step:982/2245 train_time:115683ms step_avg:117.80ms
step:983/2245 train_time:115799ms step_avg:117.80ms
step:984/2245 train_time:115922ms step_avg:117.81ms
step:985/2245 train_time:116037ms step_avg:117.80ms
step:986/2245 train_time:116159ms step_avg:117.81ms
step:987/2245 train_time:116275ms step_avg:117.81ms
step:988/2245 train_time:116397ms step_avg:117.81ms
step:989/2245 train_time:116513ms step_avg:117.81ms
step:990/2245 train_time:116635ms step_avg:117.81ms
step:991/2245 train_time:116750ms step_avg:117.81ms
step:992/2245 train_time:116872ms step_avg:117.81ms
step:993/2245 train_time:116988ms step_avg:117.81ms
step:994/2245 train_time:117110ms step_avg:117.82ms
step:995/2245 train_time:117226ms step_avg:117.81ms
step:996/2245 train_time:117348ms step_avg:117.82ms
step:997/2245 train_time:117464ms step_avg:117.82ms
step:998/2245 train_time:117585ms step_avg:117.82ms
step:999/2245 train_time:117701ms step_avg:117.82ms
step:1000/2245 train_time:117823ms step_avg:117.82ms
step:1000/2245 val_loss:3.6037 train_time:117890ms step_avg:117.89ms
step:1001/2245 train_time:117940ms step_avg:117.82ms
step:1002/2245 train_time:118061ms step_avg:117.83ms
step:1003/2245 train_time:118177ms step_avg:117.82ms
step:1004/2245 train_time:118299ms step_avg:117.83ms
step:1005/2245 train_time:118414ms step_avg:117.83ms
step:1006/2245 train_time:118537ms step_avg:117.83ms
step:1007/2245 train_time:118652ms step_avg:117.83ms
step:1008/2245 train_time:118774ms step_avg:117.83ms
step:1009/2245 train_time:118890ms step_avg:117.83ms
step:1010/2245 train_time:119012ms step_avg:117.83ms
step:1011/2245 train_time:119128ms step_avg:117.83ms
step:1012/2245 train_time:119250ms step_avg:117.84ms
step:1013/2245 train_time:119365ms step_avg:117.83ms
step:1014/2245 train_time:119487ms step_avg:117.84ms
step:1015/2245 train_time:119602ms step_avg:117.83ms
step:1016/2245 train_time:119724ms step_avg:117.84ms
step:1017/2245 train_time:119840ms step_avg:117.84ms
step:1018/2245 train_time:119962ms step_avg:117.84ms
step:1019/2245 train_time:120078ms step_avg:117.84ms
step:1020/2245 train_time:120200ms step_avg:117.84ms
step:1021/2245 train_time:120315ms step_avg:117.84ms
step:1022/2245 train_time:120437ms step_avg:117.84ms
step:1023/2245 train_time:120553ms step_avg:117.84ms
step:1024/2245 train_time:120675ms step_avg:117.85ms
step:1025/2245 train_time:120791ms step_avg:117.84ms
step:1026/2245 train_time:120913ms step_avg:117.85ms
step:1027/2245 train_time:121029ms step_avg:117.85ms
step:1028/2245 train_time:121151ms step_avg:117.85ms
step:1029/2245 train_time:121267ms step_avg:117.85ms
step:1030/2245 train_time:121388ms step_avg:117.85ms
step:1031/2245 train_time:121504ms step_avg:117.85ms
step:1032/2245 train_time:121626ms step_avg:117.86ms
step:1033/2245 train_time:121742ms step_avg:117.85ms
step:1034/2245 train_time:121864ms step_avg:117.86ms
step:1035/2245 train_time:121980ms step_avg:117.86ms
step:1036/2245 train_time:122102ms step_avg:117.86ms
step:1037/2245 train_time:122218ms step_avg:117.86ms
step:1038/2245 train_time:122340ms step_avg:117.86ms
step:1039/2245 train_time:122455ms step_avg:117.86ms
step:1040/2245 train_time:122577ms step_avg:117.86ms
step:1041/2245 train_time:122693ms step_avg:117.86ms
step:1042/2245 train_time:122815ms step_avg:117.86ms
step:1043/2245 train_time:122930ms step_avg:117.86ms
step:1044/2245 train_time:123053ms step_avg:117.87ms
step:1045/2245 train_time:123169ms step_avg:117.86ms
step:1046/2245 train_time:123291ms step_avg:117.87ms
step:1047/2245 train_time:123408ms step_avg:117.87ms
step:1048/2245 train_time:123530ms step_avg:117.87ms
step:1049/2245 train_time:123646ms step_avg:117.87ms
step:1050/2245 train_time:123769ms step_avg:117.88ms
step:1051/2245 train_time:123884ms step_avg:117.87ms
step:1052/2245 train_time:124006ms step_avg:117.88ms
step:1053/2245 train_time:124121ms step_avg:117.87ms
step:1054/2245 train_time:124243ms step_avg:117.88ms
step:1055/2245 train_time:124359ms step_avg:117.88ms
step:1056/2245 train_time:124481ms step_avg:117.88ms
step:1057/2245 train_time:124596ms step_avg:117.88ms
step:1058/2245 train_time:124718ms step_avg:117.88ms
step:1059/2245 train_time:124834ms step_avg:117.88ms
step:1060/2245 train_time:124956ms step_avg:117.88ms
step:1061/2245 train_time:125071ms step_avg:117.88ms
step:1062/2245 train_time:125194ms step_avg:117.89ms
step:1063/2245 train_time:125309ms step_avg:117.88ms
step:1064/2245 train_time:125431ms step_avg:117.89ms
step:1065/2245 train_time:125547ms step_avg:117.88ms
step:1066/2245 train_time:125669ms step_avg:117.89ms
step:1067/2245 train_time:125785ms step_avg:117.89ms
step:1068/2245 train_time:125908ms step_avg:117.89ms
step:1069/2245 train_time:126023ms step_avg:117.89ms
step:1070/2245 train_time:126145ms step_avg:117.89ms
step:1071/2245 train_time:126260ms step_avg:117.89ms
step:1072/2245 train_time:126382ms step_avg:117.89ms
step:1073/2245 train_time:126498ms step_avg:117.89ms
step:1074/2245 train_time:126620ms step_avg:117.90ms
step:1075/2245 train_time:126735ms step_avg:117.89ms
step:1076/2245 train_time:126857ms step_avg:117.90ms
step:1077/2245 train_time:126972ms step_avg:117.89ms
step:1078/2245 train_time:127094ms step_avg:117.90ms
step:1079/2245 train_time:127210ms step_avg:117.90ms
step:1080/2245 train_time:127332ms step_avg:117.90ms
step:1081/2245 train_time:127448ms step_avg:117.90ms
step:1082/2245 train_time:127570ms step_avg:117.90ms
step:1083/2245 train_time:127686ms step_avg:117.90ms
step:1084/2245 train_time:127809ms step_avg:117.90ms
step:1085/2245 train_time:127924ms step_avg:117.90ms
step:1086/2245 train_time:128047ms step_avg:117.91ms
step:1087/2245 train_time:128162ms step_avg:117.90ms
step:1088/2245 train_time:128284ms step_avg:117.91ms
step:1089/2245 train_time:128399ms step_avg:117.91ms
step:1090/2245 train_time:128522ms step_avg:117.91ms
step:1091/2245 train_time:128637ms step_avg:117.91ms
step:1092/2245 train_time:128759ms step_avg:117.91ms
step:1093/2245 train_time:128874ms step_avg:117.91ms
step:1094/2245 train_time:128997ms step_avg:117.91ms
step:1095/2245 train_time:129112ms step_avg:117.91ms
step:1096/2245 train_time:129235ms step_avg:117.92ms
step:1097/2245 train_time:129351ms step_avg:117.91ms
step:1098/2245 train_time:129473ms step_avg:117.92ms
step:1099/2245 train_time:129589ms step_avg:117.92ms
step:1100/2245 train_time:129711ms step_avg:117.92ms
step:1101/2245 train_time:129828ms step_avg:117.92ms
step:1102/2245 train_time:129950ms step_avg:117.92ms
step:1103/2245 train_time:130066ms step_avg:117.92ms
step:1104/2245 train_time:130188ms step_avg:117.92ms
step:1105/2245 train_time:130304ms step_avg:117.92ms
step:1106/2245 train_time:130425ms step_avg:117.93ms
step:1107/2245 train_time:130541ms step_avg:117.92ms
step:1108/2245 train_time:130664ms step_avg:117.93ms
step:1109/2245 train_time:130780ms step_avg:117.93ms
step:1110/2245 train_time:130902ms step_avg:117.93ms
step:1111/2245 train_time:131017ms step_avg:117.93ms
step:1112/2245 train_time:131139ms step_avg:117.93ms
step:1113/2245 train_time:131254ms step_avg:117.93ms
step:1114/2245 train_time:131376ms step_avg:117.93ms
step:1115/2245 train_time:131492ms step_avg:117.93ms
step:1116/2245 train_time:131614ms step_avg:117.93ms
step:1117/2245 train_time:131730ms step_avg:117.93ms
step:1118/2245 train_time:131852ms step_avg:117.94ms
step:1119/2245 train_time:131968ms step_avg:117.93ms
step:1120/2245 train_time:132091ms step_avg:117.94ms
step:1121/2245 train_time:132207ms step_avg:117.94ms
step:1122/2245 train_time:132329ms step_avg:117.94ms
step:1123/2245 train_time:132444ms step_avg:117.94ms
step:1124/2245 train_time:132567ms step_avg:117.94ms
step:1125/2245 train_time:132682ms step_avg:117.94ms
step:1126/2245 train_time:132804ms step_avg:117.94ms
step:1127/2245 train_time:132919ms step_avg:117.94ms
step:1128/2245 train_time:133041ms step_avg:117.94ms
step:1129/2245 train_time:133157ms step_avg:117.94ms
step:1130/2245 train_time:133279ms step_avg:117.95ms
step:1131/2245 train_time:133394ms step_avg:117.94ms
step:1132/2245 train_time:133517ms step_avg:117.95ms
step:1133/2245 train_time:133633ms step_avg:117.95ms
step:1134/2245 train_time:133755ms step_avg:117.95ms
step:1135/2245 train_time:133870ms step_avg:117.95ms
step:1136/2245 train_time:133993ms step_avg:117.95ms
step:1137/2245 train_time:134108ms step_avg:117.95ms
step:1138/2245 train_time:134230ms step_avg:117.95ms
step:1139/2245 train_time:134346ms step_avg:117.95ms
step:1140/2245 train_time:134469ms step_avg:117.96ms
step:1141/2245 train_time:134584ms step_avg:117.95ms
step:1142/2245 train_time:134706ms step_avg:117.96ms
step:1143/2245 train_time:134822ms step_avg:117.95ms
step:1144/2245 train_time:134944ms step_avg:117.96ms
step:1145/2245 train_time:135059ms step_avg:117.96ms
step:1146/2245 train_time:135181ms step_avg:117.96ms
step:1147/2245 train_time:135297ms step_avg:117.96ms
step:1148/2245 train_time:135419ms step_avg:117.96ms
step:1149/2245 train_time:135534ms step_avg:117.96ms
step:1150/2245 train_time:135656ms step_avg:117.96ms
step:1151/2245 train_time:135772ms step_avg:117.96ms
step:1152/2245 train_time:135895ms step_avg:117.96ms
step:1153/2245 train_time:136010ms step_avg:117.96ms
step:1154/2245 train_time:136133ms step_avg:117.97ms
step:1155/2245 train_time:136248ms step_avg:117.96ms
step:1156/2245 train_time:136370ms step_avg:117.97ms
step:1157/2245 train_time:136486ms step_avg:117.97ms
step:1158/2245 train_time:136608ms step_avg:117.97ms
step:1159/2245 train_time:136724ms step_avg:117.97ms
step:1160/2245 train_time:136846ms step_avg:117.97ms
step:1161/2245 train_time:136962ms step_avg:117.97ms
step:1162/2245 train_time:137084ms step_avg:117.97ms
step:1163/2245 train_time:137199ms step_avg:117.97ms
step:1164/2245 train_time:137321ms step_avg:117.97ms
step:1165/2245 train_time:137437ms step_avg:117.97ms
step:1166/2245 train_time:137559ms step_avg:117.98ms
step:1167/2245 train_time:137675ms step_avg:117.97ms
step:1168/2245 train_time:137797ms step_avg:117.98ms
step:1169/2245 train_time:137913ms step_avg:117.98ms
step:1170/2245 train_time:138036ms step_avg:117.98ms
step:1171/2245 train_time:138152ms step_avg:117.98ms
step:1172/2245 train_time:138273ms step_avg:117.98ms
step:1173/2245 train_time:138389ms step_avg:117.98ms
step:1174/2245 train_time:138511ms step_avg:117.98ms
step:1175/2245 train_time:138627ms step_avg:117.98ms
step:1176/2245 train_time:138749ms step_avg:117.98ms
step:1177/2245 train_time:138866ms step_avg:117.98ms
step:1178/2245 train_time:138988ms step_avg:117.99ms
step:1179/2245 train_time:139103ms step_avg:117.98ms
step:1180/2245 train_time:139224ms step_avg:117.99ms
step:1181/2245 train_time:139340ms step_avg:117.99ms
step:1182/2245 train_time:139462ms step_avg:117.99ms
step:1183/2245 train_time:139578ms step_avg:117.99ms
step:1184/2245 train_time:139700ms step_avg:117.99ms
step:1185/2245 train_time:139816ms step_avg:117.99ms
step:1186/2245 train_time:139938ms step_avg:117.99ms
step:1187/2245 train_time:140053ms step_avg:117.99ms
step:1188/2245 train_time:140176ms step_avg:117.99ms
step:1189/2245 train_time:140292ms step_avg:117.99ms
step:1190/2245 train_time:140414ms step_avg:118.00ms
step:1191/2245 train_time:140530ms step_avg:117.99ms
step:1192/2245 train_time:140652ms step_avg:118.00ms
step:1193/2245 train_time:140768ms step_avg:118.00ms
step:1194/2245 train_time:140891ms step_avg:118.00ms
step:1195/2245 train_time:141006ms step_avg:118.00ms
step:1196/2245 train_time:141129ms step_avg:118.00ms
step:1197/2245 train_time:141244ms step_avg:118.00ms
step:1198/2245 train_time:141366ms step_avg:118.00ms
step:1199/2245 train_time:141481ms step_avg:118.00ms
step:1200/2245 train_time:141603ms step_avg:118.00ms
step:1201/2245 train_time:141719ms step_avg:118.00ms
step:1202/2245 train_time:141841ms step_avg:118.00ms
step:1203/2245 train_time:141956ms step_avg:118.00ms
step:1204/2245 train_time:142078ms step_avg:118.00ms
step:1205/2245 train_time:142194ms step_avg:118.00ms
step:1206/2245 train_time:142316ms step_avg:118.01ms
step:1207/2245 train_time:142432ms step_avg:118.00ms
step:1208/2245 train_time:142555ms step_avg:118.01ms
step:1209/2245 train_time:142670ms step_avg:118.01ms
step:1210/2245 train_time:142792ms step_avg:118.01ms
step:1211/2245 train_time:142908ms step_avg:118.01ms
step:1212/2245 train_time:143030ms step_avg:118.01ms
step:1213/2245 train_time:143147ms step_avg:118.01ms
step:1214/2245 train_time:143269ms step_avg:118.01ms
step:1215/2245 train_time:143385ms step_avg:118.01ms
step:1216/2245 train_time:143506ms step_avg:118.02ms
step:1217/2245 train_time:143623ms step_avg:118.01ms
step:1218/2245 train_time:143744ms step_avg:118.02ms
step:1219/2245 train_time:143860ms step_avg:118.01ms
step:1220/2245 train_time:143982ms step_avg:118.02ms
step:1221/2245 train_time:144098ms step_avg:118.02ms
step:1222/2245 train_time:144220ms step_avg:118.02ms
step:1223/2245 train_time:144335ms step_avg:118.02ms
step:1224/2245 train_time:144458ms step_avg:118.02ms
step:1225/2245 train_time:144573ms step_avg:118.02ms
step:1226/2245 train_time:144696ms step_avg:118.02ms
step:1227/2245 train_time:144812ms step_avg:118.02ms
step:1228/2245 train_time:144934ms step_avg:118.02ms
step:1229/2245 train_time:145050ms step_avg:118.02ms
step:1230/2245 train_time:145172ms step_avg:118.03ms
step:1231/2245 train_time:145288ms step_avg:118.02ms
step:1232/2245 train_time:145411ms step_avg:118.03ms
step:1233/2245 train_time:145527ms step_avg:118.03ms
step:1234/2245 train_time:145650ms step_avg:118.03ms
step:1235/2245 train_time:145765ms step_avg:118.03ms
step:1236/2245 train_time:145887ms step_avg:118.03ms
step:1237/2245 train_time:146003ms step_avg:118.03ms
step:1238/2245 train_time:146125ms step_avg:118.03ms
step:1239/2245 train_time:146240ms step_avg:118.03ms
step:1240/2245 train_time:146362ms step_avg:118.03ms
step:1241/2245 train_time:146477ms step_avg:118.03ms
step:1242/2245 train_time:146600ms step_avg:118.04ms
step:1243/2245 train_time:146715ms step_avg:118.03ms
step:1244/2245 train_time:146837ms step_avg:118.04ms
step:1245/2245 train_time:146953ms step_avg:118.03ms
step:1246/2245 train_time:147075ms step_avg:118.04ms
step:1247/2245 train_time:147192ms step_avg:118.04ms
step:1248/2245 train_time:147313ms step_avg:118.04ms
step:1249/2245 train_time:147429ms step_avg:118.04ms
step:1250/2245 train_time:147551ms step_avg:118.04ms
step:1250/2245 val_loss:3.5250 train_time:147618ms step_avg:118.09ms
step:1251/2245 train_time:147669ms step_avg:118.04ms
step:1252/2245 train_time:147790ms step_avg:118.04ms
step:1253/2245 train_time:147905ms step_avg:118.04ms
step:1254/2245 train_time:148027ms step_avg:118.04ms
step:1255/2245 train_time:148142ms step_avg:118.04ms
step:1256/2245 train_time:148264ms step_avg:118.04ms
step:1257/2245 train_time:148380ms step_avg:118.04ms
step:1258/2245 train_time:148501ms step_avg:118.05ms
step:1259/2245 train_time:148617ms step_avg:118.04ms
step:1260/2245 train_time:148740ms step_avg:118.05ms
step:1261/2245 train_time:148856ms step_avg:118.05ms
step:1262/2245 train_time:148978ms step_avg:118.05ms
step:1263/2245 train_time:149094ms step_avg:118.05ms
step:1264/2245 train_time:149216ms step_avg:118.05ms
step:1265/2245 train_time:149331ms step_avg:118.05ms
step:1266/2245 train_time:149453ms step_avg:118.05ms
step:1267/2245 train_time:149569ms step_avg:118.05ms
step:1268/2245 train_time:149691ms step_avg:118.05ms
step:1269/2245 train_time:149806ms step_avg:118.05ms
step:1270/2245 train_time:149928ms step_avg:118.05ms
step:1271/2245 train_time:150044ms step_avg:118.05ms
step:1272/2245 train_time:150166ms step_avg:118.06ms
step:1273/2245 train_time:150282ms step_avg:118.05ms
step:1274/2245 train_time:150404ms step_avg:118.06ms
step:1275/2245 train_time:150519ms step_avg:118.05ms
step:1276/2245 train_time:150643ms step_avg:118.06ms
step:1277/2245 train_time:150759ms step_avg:118.06ms
step:1278/2245 train_time:150881ms step_avg:118.06ms
step:1279/2245 train_time:150997ms step_avg:118.06ms
step:1280/2245 train_time:151119ms step_avg:118.06ms
step:1281/2245 train_time:151235ms step_avg:118.06ms
step:1282/2245 train_time:151356ms step_avg:118.06ms
step:1283/2245 train_time:151472ms step_avg:118.06ms
step:1284/2245 train_time:151594ms step_avg:118.06ms
step:1285/2245 train_time:151709ms step_avg:118.06ms
step:1286/2245 train_time:151831ms step_avg:118.06ms
step:1287/2245 train_time:151947ms step_avg:118.06ms
step:1288/2245 train_time:152069ms step_avg:118.07ms
step:1289/2245 train_time:152185ms step_avg:118.06ms
step:1290/2245 train_time:152307ms step_avg:118.07ms
step:1291/2245 train_time:152424ms step_avg:118.07ms
step:1292/2245 train_time:152546ms step_avg:118.07ms
step:1293/2245 train_time:152661ms step_avg:118.07ms
step:1294/2245 train_time:152784ms step_avg:118.07ms
step:1295/2245 train_time:152899ms step_avg:118.07ms
step:1296/2245 train_time:153021ms step_avg:118.07ms
step:1297/2245 train_time:153138ms step_avg:118.07ms
step:1298/2245 train_time:153259ms step_avg:118.07ms
step:1299/2245 train_time:153375ms step_avg:118.07ms
step:1300/2245 train_time:153497ms step_avg:118.07ms
step:1301/2245 train_time:153613ms step_avg:118.07ms
step:1302/2245 train_time:153735ms step_avg:118.08ms
step:1303/2245 train_time:153850ms step_avg:118.07ms
step:1304/2245 train_time:153973ms step_avg:118.08ms
step:1305/2245 train_time:154088ms step_avg:118.08ms
step:1306/2245 train_time:154211ms step_avg:118.08ms
step:1307/2245 train_time:154326ms step_avg:118.08ms
step:1308/2245 train_time:154448ms step_avg:118.08ms
step:1309/2245 train_time:154564ms step_avg:118.08ms
step:1310/2245 train_time:154686ms step_avg:118.08ms
step:1311/2245 train_time:154802ms step_avg:118.08ms
step:1312/2245 train_time:154924ms step_avg:118.08ms
step:1313/2245 train_time:155039ms step_avg:118.08ms
step:1314/2245 train_time:155163ms step_avg:118.08ms
step:1315/2245 train_time:155278ms step_avg:118.08ms
step:1316/2245 train_time:155401ms step_avg:118.09ms
step:1317/2245 train_time:155517ms step_avg:118.08ms
step:1318/2245 train_time:155638ms step_avg:118.09ms
step:1319/2245 train_time:155754ms step_avg:118.08ms
step:1320/2245 train_time:155876ms step_avg:118.09ms
step:1321/2245 train_time:155992ms step_avg:118.09ms
step:1322/2245 train_time:156114ms step_avg:118.09ms
step:1323/2245 train_time:156229ms step_avg:118.09ms
step:1324/2245 train_time:156351ms step_avg:118.09ms
step:1325/2245 train_time:156467ms step_avg:118.09ms
step:1326/2245 train_time:156589ms step_avg:118.09ms
step:1327/2245 train_time:156704ms step_avg:118.09ms
step:1328/2245 train_time:156826ms step_avg:118.09ms
step:1329/2245 train_time:156942ms step_avg:118.09ms
step:1330/2245 train_time:157064ms step_avg:118.09ms
step:1331/2245 train_time:157180ms step_avg:118.09ms
step:1332/2245 train_time:157302ms step_avg:118.09ms
step:1333/2245 train_time:157418ms step_avg:118.09ms
step:1334/2245 train_time:157541ms step_avg:118.10ms
step:1335/2245 train_time:157657ms step_avg:118.10ms
step:1336/2245 train_time:157779ms step_avg:118.10ms
step:1337/2245 train_time:157894ms step_avg:118.10ms
step:1338/2245 train_time:158017ms step_avg:118.10ms
step:1339/2245 train_time:158132ms step_avg:118.10ms
step:1340/2245 train_time:158254ms step_avg:118.10ms
step:1341/2245 train_time:158369ms step_avg:118.10ms
step:1342/2245 train_time:158491ms step_avg:118.10ms
step:1343/2245 train_time:158606ms step_avg:118.10ms
step:1344/2245 train_time:158728ms step_avg:118.10ms
step:1345/2245 train_time:158844ms step_avg:118.10ms
step:1346/2245 train_time:158966ms step_avg:118.10ms
step:1347/2245 train_time:159081ms step_avg:118.10ms
step:1348/2245 train_time:159203ms step_avg:118.10ms
step:1349/2245 train_time:159319ms step_avg:118.10ms
step:1350/2245 train_time:159441ms step_avg:118.10ms
step:1351/2245 train_time:159557ms step_avg:118.10ms
step:1352/2245 train_time:159680ms step_avg:118.11ms
step:1353/2245 train_time:159795ms step_avg:118.10ms
step:1354/2245 train_time:159917ms step_avg:118.11ms
step:1355/2245 train_time:160033ms step_avg:118.11ms
step:1356/2245 train_time:160155ms step_avg:118.11ms
step:1357/2245 train_time:160270ms step_avg:118.11ms
step:1358/2245 train_time:160392ms step_avg:118.11ms
step:1359/2245 train_time:160508ms step_avg:118.11ms
step:1360/2245 train_time:160630ms step_avg:118.11ms
step:1361/2245 train_time:160745ms step_avg:118.11ms
step:1362/2245 train_time:160868ms step_avg:118.11ms
step:1363/2245 train_time:160983ms step_avg:118.11ms
step:1364/2245 train_time:161106ms step_avg:118.11ms
step:1365/2245 train_time:161222ms step_avg:118.11ms
step:1366/2245 train_time:161344ms step_avg:118.11ms
step:1367/2245 train_time:161460ms step_avg:118.11ms
step:1368/2245 train_time:161583ms step_avg:118.12ms
step:1369/2245 train_time:161699ms step_avg:118.11ms
step:1370/2245 train_time:161821ms step_avg:118.12ms
step:1371/2245 train_time:161937ms step_avg:118.12ms
step:1372/2245 train_time:162059ms step_avg:118.12ms
step:1373/2245 train_time:162175ms step_avg:118.12ms
step:1374/2245 train_time:162296ms step_avg:118.12ms
step:1375/2245 train_time:162412ms step_avg:118.12ms
step:1376/2245 train_time:162534ms step_avg:118.12ms
step:1377/2245 train_time:162649ms step_avg:118.12ms
step:1378/2245 train_time:162772ms step_avg:118.12ms
step:1379/2245 train_time:162886ms step_avg:118.12ms
step:1380/2245 train_time:163008ms step_avg:118.12ms
step:1381/2245 train_time:163124ms step_avg:118.12ms
step:1382/2245 train_time:163245ms step_avg:118.12ms
step:1383/2245 train_time:163361ms step_avg:118.12ms
step:1384/2245 train_time:163484ms step_avg:118.12ms
step:1385/2245 train_time:163600ms step_avg:118.12ms
step:1386/2245 train_time:163721ms step_avg:118.13ms
step:1387/2245 train_time:163838ms step_avg:118.12ms
step:1388/2245 train_time:163960ms step_avg:118.13ms
step:1389/2245 train_time:164075ms step_avg:118.12ms
step:1390/2245 train_time:164197ms step_avg:118.13ms
step:1391/2245 train_time:164313ms step_avg:118.13ms
step:1392/2245 train_time:164435ms step_avg:118.13ms
step:1393/2245 train_time:164550ms step_avg:118.13ms
step:1394/2245 train_time:164672ms step_avg:118.13ms
step:1395/2245 train_time:164788ms step_avg:118.13ms
step:1396/2245 train_time:164910ms step_avg:118.13ms
step:1397/2245 train_time:165026ms step_avg:118.13ms
step:1398/2245 train_time:165148ms step_avg:118.13ms
step:1399/2245 train_time:165264ms step_avg:118.13ms
step:1400/2245 train_time:165386ms step_avg:118.13ms
step:1401/2245 train_time:165502ms step_avg:118.13ms
step:1402/2245 train_time:165624ms step_avg:118.13ms
step:1403/2245 train_time:165740ms step_avg:118.13ms
step:1404/2245 train_time:165863ms step_avg:118.14ms
step:1405/2245 train_time:165979ms step_avg:118.13ms
step:1406/2245 train_time:166101ms step_avg:118.14ms
step:1407/2245 train_time:166217ms step_avg:118.14ms
step:1408/2245 train_time:166340ms step_avg:118.14ms
step:1409/2245 train_time:166455ms step_avg:118.14ms
step:1410/2245 train_time:166577ms step_avg:118.14ms
step:1411/2245 train_time:166692ms step_avg:118.14ms
step:1412/2245 train_time:166814ms step_avg:118.14ms
step:1413/2245 train_time:166930ms step_avg:118.14ms
step:1414/2245 train_time:167052ms step_avg:118.14ms
step:1415/2245 train_time:167167ms step_avg:118.14ms
step:1416/2245 train_time:167290ms step_avg:118.14ms
step:1417/2245 train_time:167405ms step_avg:118.14ms
step:1418/2245 train_time:167527ms step_avg:118.14ms
step:1419/2245 train_time:167643ms step_avg:118.14ms
step:1420/2245 train_time:167766ms step_avg:118.14ms
step:1421/2245 train_time:167881ms step_avg:118.14ms
step:1422/2245 train_time:168004ms step_avg:118.15ms
step:1423/2245 train_time:168120ms step_avg:118.14ms
step:1424/2245 train_time:168242ms step_avg:118.15ms
step:1425/2245 train_time:168358ms step_avg:118.15ms
step:1426/2245 train_time:168480ms step_avg:118.15ms
step:1427/2245 train_time:168595ms step_avg:118.15ms
step:1428/2245 train_time:168717ms step_avg:118.15ms
step:1429/2245 train_time:168833ms step_avg:118.15ms
step:1430/2245 train_time:168955ms step_avg:118.15ms
step:1431/2245 train_time:169070ms step_avg:118.15ms
step:1432/2245 train_time:169192ms step_avg:118.15ms
step:1433/2245 train_time:169309ms step_avg:118.15ms
step:1434/2245 train_time:169430ms step_avg:118.15ms
step:1435/2245 train_time:169546ms step_avg:118.15ms
step:1436/2245 train_time:169668ms step_avg:118.15ms
step:1437/2245 train_time:169784ms step_avg:118.15ms
step:1438/2245 train_time:169906ms step_avg:118.15ms
step:1439/2245 train_time:170021ms step_avg:118.15ms
step:1440/2245 train_time:170143ms step_avg:118.16ms
step:1441/2245 train_time:170260ms step_avg:118.15ms
step:1442/2245 train_time:170382ms step_avg:118.16ms
step:1443/2245 train_time:170498ms step_avg:118.16ms
step:1444/2245 train_time:170621ms step_avg:118.16ms
step:1445/2245 train_time:170737ms step_avg:118.16ms
step:1446/2245 train_time:170859ms step_avg:118.16ms
step:1447/2245 train_time:170974ms step_avg:118.16ms
step:1448/2245 train_time:171096ms step_avg:118.16ms
step:1449/2245 train_time:171211ms step_avg:118.16ms
step:1450/2245 train_time:171333ms step_avg:118.16ms
step:1451/2245 train_time:171449ms step_avg:118.16ms
step:1452/2245 train_time:171570ms step_avg:118.16ms
step:1453/2245 train_time:171686ms step_avg:118.16ms
step:1454/2245 train_time:171808ms step_avg:118.16ms
step:1455/2245 train_time:171924ms step_avg:118.16ms
step:1456/2245 train_time:172046ms step_avg:118.16ms
step:1457/2245 train_time:172162ms step_avg:118.16ms
step:1458/2245 train_time:172284ms step_avg:118.16ms
step:1459/2245 train_time:172400ms step_avg:118.16ms
step:1460/2245 train_time:172522ms step_avg:118.17ms
step:1461/2245 train_time:172638ms step_avg:118.16ms
step:1462/2245 train_time:172761ms step_avg:118.17ms
step:1463/2245 train_time:172877ms step_avg:118.17ms
step:1464/2245 train_time:172999ms step_avg:118.17ms
step:1465/2245 train_time:173115ms step_avg:118.17ms
step:1466/2245 train_time:173237ms step_avg:118.17ms
step:1467/2245 train_time:173352ms step_avg:118.17ms
step:1468/2245 train_time:173474ms step_avg:118.17ms
step:1469/2245 train_time:173590ms step_avg:118.17ms
step:1470/2245 train_time:173712ms step_avg:118.17ms
step:1471/2245 train_time:173828ms step_avg:118.17ms
step:1472/2245 train_time:173951ms step_avg:118.17ms
step:1473/2245 train_time:174067ms step_avg:118.17ms
step:1474/2245 train_time:174190ms step_avg:118.18ms
step:1475/2245 train_time:174306ms step_avg:118.17ms
step:1476/2245 train_time:174430ms step_avg:118.18ms
step:1477/2245 train_time:174546ms step_avg:118.18ms
step:1478/2245 train_time:174669ms step_avg:118.18ms
step:1479/2245 train_time:174786ms step_avg:118.18ms
step:1480/2245 train_time:174909ms step_avg:118.18ms
step:1481/2245 train_time:175026ms step_avg:118.18ms
step:1482/2245 train_time:175150ms step_avg:118.18ms
step:1483/2245 train_time:175266ms step_avg:118.18ms
step:1484/2245 train_time:175389ms step_avg:118.19ms
step:1485/2245 train_time:175506ms step_avg:118.19ms
step:1486/2245 train_time:175629ms step_avg:118.19ms
step:1487/2245 train_time:175745ms step_avg:118.19ms
step:1488/2245 train_time:175869ms step_avg:118.19ms
step:1489/2245 train_time:175985ms step_avg:118.19ms
step:1490/2245 train_time:176108ms step_avg:118.19ms
step:1491/2245 train_time:176225ms step_avg:118.19ms
step:1492/2245 train_time:176348ms step_avg:118.20ms
step:1493/2245 train_time:176465ms step_avg:118.19ms
step:1494/2245 train_time:176587ms step_avg:118.20ms
step:1495/2245 train_time:176704ms step_avg:118.20ms
step:1496/2245 train_time:176827ms step_avg:118.20ms
step:1497/2245 train_time:176944ms step_avg:118.20ms
step:1498/2245 train_time:177067ms step_avg:118.20ms
step:1499/2245 train_time:177183ms step_avg:118.20ms
step:1500/2245 train_time:177306ms step_avg:118.20ms
step:1500/2245 val_loss:3.4438 train_time:177374ms step_avg:118.25ms
step:1501/2245 train_time:177424ms step_avg:118.20ms
step:1502/2245 train_time:177546ms step_avg:118.21ms
step:1503/2245 train_time:177662ms step_avg:118.21ms
step:1504/2245 train_time:177785ms step_avg:118.21ms
step:1505/2245 train_time:177901ms step_avg:118.21ms
step:1506/2245 train_time:178024ms step_avg:118.21ms
step:1507/2245 train_time:178141ms step_avg:118.21ms
step:1508/2245 train_time:178263ms step_avg:118.21ms
step:1509/2245 train_time:178381ms step_avg:118.21ms
step:1510/2245 train_time:178504ms step_avg:118.21ms
step:1511/2245 train_time:178620ms step_avg:118.21ms
step:1512/2245 train_time:178743ms step_avg:118.22ms
step:1513/2245 train_time:178859ms step_avg:118.22ms
step:1514/2245 train_time:178982ms step_avg:118.22ms
step:1515/2245 train_time:179098ms step_avg:118.22ms
step:1516/2245 train_time:179221ms step_avg:118.22ms
step:1517/2245 train_time:179338ms step_avg:118.22ms
step:1518/2245 train_time:179461ms step_avg:118.22ms
step:1519/2245 train_time:179578ms step_avg:118.22ms
step:1520/2245 train_time:179701ms step_avg:118.22ms
step:1521/2245 train_time:179818ms step_avg:118.22ms
step:1522/2245 train_time:179941ms step_avg:118.23ms
step:1523/2245 train_time:180057ms step_avg:118.23ms
step:1524/2245 train_time:180180ms step_avg:118.23ms
step:1525/2245 train_time:180296ms step_avg:118.23ms
step:1526/2245 train_time:180420ms step_avg:118.23ms
step:1527/2245 train_time:180537ms step_avg:118.23ms
step:1528/2245 train_time:180659ms step_avg:118.23ms
step:1529/2245 train_time:180776ms step_avg:118.23ms
step:1530/2245 train_time:180898ms step_avg:118.23ms
step:1531/2245 train_time:181015ms step_avg:118.23ms
step:1532/2245 train_time:181137ms step_avg:118.24ms
step:1533/2245 train_time:181255ms step_avg:118.24ms
step:1534/2245 train_time:181378ms step_avg:118.24ms
step:1535/2245 train_time:181495ms step_avg:118.24ms
step:1536/2245 train_time:181618ms step_avg:118.24ms
step:1537/2245 train_time:181735ms step_avg:118.24ms
step:1538/2245 train_time:181858ms step_avg:118.24ms
step:1539/2245 train_time:181975ms step_avg:118.24ms
step:1540/2245 train_time:182098ms step_avg:118.25ms
step:1541/2245 train_time:182215ms step_avg:118.24ms
step:1542/2245 train_time:182338ms step_avg:118.25ms
step:1543/2245 train_time:182455ms step_avg:118.25ms
step:1544/2245 train_time:182578ms step_avg:118.25ms
step:1545/2245 train_time:182694ms step_avg:118.25ms
step:1546/2245 train_time:182818ms step_avg:118.25ms
step:1547/2245 train_time:182934ms step_avg:118.25ms
step:1548/2245 train_time:183057ms step_avg:118.25ms
step:1549/2245 train_time:183174ms step_avg:118.25ms
step:1550/2245 train_time:183298ms step_avg:118.26ms
step:1551/2245 train_time:183414ms step_avg:118.26ms
step:1552/2245 train_time:183537ms step_avg:118.26ms
step:1553/2245 train_time:183654ms step_avg:118.26ms
step:1554/2245 train_time:183777ms step_avg:118.26ms
step:1555/2245 train_time:183893ms step_avg:118.26ms
step:1556/2245 train_time:184016ms step_avg:118.26ms
step:1557/2245 train_time:184133ms step_avg:118.26ms
step:1558/2245 train_time:184256ms step_avg:118.26ms
step:1559/2245 train_time:184373ms step_avg:118.26ms
step:1560/2245 train_time:184495ms step_avg:118.27ms
step:1561/2245 train_time:184612ms step_avg:118.27ms
step:1562/2245 train_time:184735ms step_avg:118.27ms
step:1563/2245 train_time:184851ms step_avg:118.27ms
step:1564/2245 train_time:184974ms step_avg:118.27ms
step:1565/2245 train_time:185091ms step_avg:118.27ms
step:1566/2245 train_time:185213ms step_avg:118.27ms
step:1567/2245 train_time:185330ms step_avg:118.27ms
step:1568/2245 train_time:185453ms step_avg:118.27ms
step:1569/2245 train_time:185569ms step_avg:118.27ms
step:1570/2245 train_time:185692ms step_avg:118.28ms
step:1571/2245 train_time:185809ms step_avg:118.27ms
step:1572/2245 train_time:185931ms step_avg:118.28ms
step:1573/2245 train_time:186048ms step_avg:118.28ms
step:1574/2245 train_time:186171ms step_avg:118.28ms
step:1575/2245 train_time:186288ms step_avg:118.28ms
step:1576/2245 train_time:186411ms step_avg:118.28ms
step:1577/2245 train_time:186527ms step_avg:118.28ms
step:1578/2245 train_time:186650ms step_avg:118.28ms
step:1579/2245 train_time:186767ms step_avg:118.28ms
step:1580/2245 train_time:186890ms step_avg:118.28ms
step:1581/2245 train_time:187006ms step_avg:118.28ms
step:1582/2245 train_time:187129ms step_avg:118.29ms
step:1583/2245 train_time:187246ms step_avg:118.29ms
step:1584/2245 train_time:187369ms step_avg:118.29ms
step:1585/2245 train_time:187485ms step_avg:118.29ms
step:1586/2245 train_time:187608ms step_avg:118.29ms
step:1587/2245 train_time:187724ms step_avg:118.29ms
step:1588/2245 train_time:187848ms step_avg:118.29ms
step:1589/2245 train_time:187964ms step_avg:118.29ms
step:1590/2245 train_time:188087ms step_avg:118.29ms
step:1591/2245 train_time:188204ms step_avg:118.29ms
step:1592/2245 train_time:188327ms step_avg:118.30ms
step:1593/2245 train_time:188443ms step_avg:118.29ms
step:1594/2245 train_time:188566ms step_avg:118.30ms
step:1595/2245 train_time:188683ms step_avg:118.30ms
step:1596/2245 train_time:188806ms step_avg:118.30ms
step:1597/2245 train_time:188922ms step_avg:118.30ms
step:1598/2245 train_time:189044ms step_avg:118.30ms
step:1599/2245 train_time:189161ms step_avg:118.30ms
step:1600/2245 train_time:189284ms step_avg:118.30ms
step:1601/2245 train_time:189402ms step_avg:118.30ms
step:1602/2245 train_time:189525ms step_avg:118.31ms
step:1603/2245 train_time:189641ms step_avg:118.30ms
step:1604/2245 train_time:189764ms step_avg:118.31ms
step:1605/2245 train_time:189880ms step_avg:118.31ms
step:1606/2245 train_time:190003ms step_avg:118.31ms
step:1607/2245 train_time:190119ms step_avg:118.31ms
step:1608/2245 train_time:190242ms step_avg:118.31ms
step:1609/2245 train_time:190359ms step_avg:118.31ms
step:1610/2245 train_time:190482ms step_avg:118.31ms
step:1611/2245 train_time:190599ms step_avg:118.31ms
step:1612/2245 train_time:190722ms step_avg:118.31ms
step:1613/2245 train_time:190838ms step_avg:118.31ms
step:1614/2245 train_time:190962ms step_avg:118.32ms
step:1615/2245 train_time:191078ms step_avg:118.31ms
step:1616/2245 train_time:191202ms step_avg:118.32ms
step:1617/2245 train_time:191318ms step_avg:118.32ms
step:1618/2245 train_time:191442ms step_avg:118.32ms
step:1619/2245 train_time:191559ms step_avg:118.32ms
step:1620/2245 train_time:191681ms step_avg:118.32ms
step:1621/2245 train_time:191798ms step_avg:118.32ms
step:1622/2245 train_time:191921ms step_avg:118.32ms
step:1623/2245 train_time:192038ms step_avg:118.32ms
step:1624/2245 train_time:192161ms step_avg:118.33ms
step:1625/2245 train_time:192277ms step_avg:118.32ms
step:1626/2245 train_time:192401ms step_avg:118.33ms
step:1627/2245 train_time:192517ms step_avg:118.33ms
step:1628/2245 train_time:192641ms step_avg:118.33ms
step:1629/2245 train_time:192757ms step_avg:118.33ms
step:1630/2245 train_time:192880ms step_avg:118.33ms
step:1631/2245 train_time:192997ms step_avg:118.33ms
step:1632/2245 train_time:193120ms step_avg:118.33ms
step:1633/2245 train_time:193237ms step_avg:118.33ms
step:1634/2245 train_time:193361ms step_avg:118.34ms
step:1635/2245 train_time:193477ms step_avg:118.33ms
step:1636/2245 train_time:193600ms step_avg:118.34ms
step:1637/2245 train_time:193716ms step_avg:118.34ms
step:1638/2245 train_time:193840ms step_avg:118.34ms
step:1639/2245 train_time:193956ms step_avg:118.34ms
step:1640/2245 train_time:194079ms step_avg:118.34ms
step:1641/2245 train_time:194196ms step_avg:118.34ms
step:1642/2245 train_time:194319ms step_avg:118.34ms
step:1643/2245 train_time:194436ms step_avg:118.34ms
step:1644/2245 train_time:194560ms step_avg:118.35ms
step:1645/2245 train_time:194677ms step_avg:118.34ms
step:1646/2245 train_time:194800ms step_avg:118.35ms
step:1647/2245 train_time:194916ms step_avg:118.35ms
step:1648/2245 train_time:195039ms step_avg:118.35ms
step:1649/2245 train_time:195156ms step_avg:118.35ms
step:1650/2245 train_time:195280ms step_avg:118.35ms
step:1651/2245 train_time:195396ms step_avg:118.35ms
step:1652/2245 train_time:195519ms step_avg:118.35ms
step:1653/2245 train_time:195636ms step_avg:118.35ms
step:1654/2245 train_time:195759ms step_avg:118.35ms
step:1655/2245 train_time:195875ms step_avg:118.35ms
step:1656/2245 train_time:195998ms step_avg:118.36ms
step:1657/2245 train_time:196115ms step_avg:118.36ms
step:1658/2245 train_time:196238ms step_avg:118.36ms
step:1659/2245 train_time:196355ms step_avg:118.36ms
step:1660/2245 train_time:196478ms step_avg:118.36ms
step:1661/2245 train_time:196594ms step_avg:118.36ms
step:1662/2245 train_time:196717ms step_avg:118.36ms
step:1663/2245 train_time:196835ms step_avg:118.36ms
step:1664/2245 train_time:196957ms step_avg:118.36ms
step:1665/2245 train_time:197074ms step_avg:118.36ms
step:1666/2245 train_time:197198ms step_avg:118.37ms
step:1667/2245 train_time:197315ms step_avg:118.37ms
step:1668/2245 train_time:197438ms step_avg:118.37ms
step:1669/2245 train_time:197555ms step_avg:118.37ms
step:1670/2245 train_time:197677ms step_avg:118.37ms
step:1671/2245 train_time:197794ms step_avg:118.37ms
step:1672/2245 train_time:197918ms step_avg:118.37ms
step:1673/2245 train_time:198034ms step_avg:118.37ms
step:1674/2245 train_time:198158ms step_avg:118.37ms
step:1675/2245 train_time:198274ms step_avg:118.37ms
step:1676/2245 train_time:198398ms step_avg:118.38ms
step:1677/2245 train_time:198514ms step_avg:118.37ms
step:1678/2245 train_time:198638ms step_avg:118.38ms
step:1679/2245 train_time:198755ms step_avg:118.38ms
step:1680/2245 train_time:198878ms step_avg:118.38ms
step:1681/2245 train_time:198995ms step_avg:118.38ms
step:1682/2245 train_time:199118ms step_avg:118.38ms
step:1683/2245 train_time:199235ms step_avg:118.38ms
step:1684/2245 train_time:199358ms step_avg:118.38ms
step:1685/2245 train_time:199475ms step_avg:118.38ms
step:1686/2245 train_time:199598ms step_avg:118.39ms
step:1687/2245 train_time:199715ms step_avg:118.38ms
step:1688/2245 train_time:199838ms step_avg:118.39ms
step:1689/2245 train_time:199956ms step_avg:118.39ms
step:1690/2245 train_time:200078ms step_avg:118.39ms
step:1691/2245 train_time:200196ms step_avg:118.39ms
step:1692/2245 train_time:200319ms step_avg:118.39ms
step:1693/2245 train_time:200436ms step_avg:118.39ms
step:1694/2245 train_time:200559ms step_avg:118.39ms
step:1695/2245 train_time:200675ms step_avg:118.39ms
step:1696/2245 train_time:200798ms step_avg:118.39ms
step:1697/2245 train_time:200914ms step_avg:118.39ms
step:1698/2245 train_time:201037ms step_avg:118.40ms
step:1699/2245 train_time:201154ms step_avg:118.40ms
step:1700/2245 train_time:201277ms step_avg:118.40ms
step:1701/2245 train_time:201394ms step_avg:118.40ms
step:1702/2245 train_time:201518ms step_avg:118.40ms
step:1703/2245 train_time:201634ms step_avg:118.40ms
step:1704/2245 train_time:201757ms step_avg:118.40ms
step:1705/2245 train_time:201873ms step_avg:118.40ms
step:1706/2245 train_time:201997ms step_avg:118.40ms
step:1707/2245 train_time:202113ms step_avg:118.40ms
step:1708/2245 train_time:202237ms step_avg:118.41ms
step:1709/2245 train_time:202354ms step_avg:118.40ms
step:1710/2245 train_time:202477ms step_avg:118.41ms
step:1711/2245 train_time:202594ms step_avg:118.41ms
step:1712/2245 train_time:202717ms step_avg:118.41ms
step:1713/2245 train_time:202835ms step_avg:118.41ms
step:1714/2245 train_time:202958ms step_avg:118.41ms
step:1715/2245 train_time:203075ms step_avg:118.41ms
step:1716/2245 train_time:203198ms step_avg:118.41ms
step:1717/2245 train_time:203315ms step_avg:118.41ms
step:1718/2245 train_time:203437ms step_avg:118.42ms
step:1719/2245 train_time:203554ms step_avg:118.41ms
step:1720/2245 train_time:203676ms step_avg:118.42ms
step:1721/2245 train_time:203793ms step_avg:118.42ms
step:1722/2245 train_time:203916ms step_avg:118.42ms
step:1723/2245 train_time:204033ms step_avg:118.42ms
step:1724/2245 train_time:204156ms step_avg:118.42ms
step:1725/2245 train_time:204273ms step_avg:118.42ms
step:1726/2245 train_time:204396ms step_avg:118.42ms
step:1727/2245 train_time:204512ms step_avg:118.42ms
step:1728/2245 train_time:204636ms step_avg:118.42ms
step:1729/2245 train_time:204752ms step_avg:118.42ms
step:1730/2245 train_time:204875ms step_avg:118.42ms
step:1731/2245 train_time:204991ms step_avg:118.42ms
step:1732/2245 train_time:205114ms step_avg:118.43ms
step:1733/2245 train_time:205230ms step_avg:118.42ms
step:1734/2245 train_time:205354ms step_avg:118.43ms
step:1735/2245 train_time:205470ms step_avg:118.43ms
step:1736/2245 train_time:205594ms step_avg:118.43ms
step:1737/2245 train_time:205710ms step_avg:118.43ms
step:1738/2245 train_time:205832ms step_avg:118.43ms
step:1739/2245 train_time:205949ms step_avg:118.43ms
step:1740/2245 train_time:206071ms step_avg:118.43ms
step:1741/2245 train_time:206188ms step_avg:118.43ms
step:1742/2245 train_time:206311ms step_avg:118.43ms
step:1743/2245 train_time:206427ms step_avg:118.43ms
step:1744/2245 train_time:206550ms step_avg:118.43ms
step:1745/2245 train_time:206666ms step_avg:118.43ms
step:1746/2245 train_time:206789ms step_avg:118.44ms
step:1747/2245 train_time:206906ms step_avg:118.43ms
step:1748/2245 train_time:207028ms step_avg:118.44ms
step:1749/2245 train_time:207145ms step_avg:118.44ms
step:1750/2245 train_time:207268ms step_avg:118.44ms
step:1750/2245 val_loss:3.3791 train_time:207334ms step_avg:118.48ms
step:1751/2245 train_time:207385ms step_avg:118.44ms
step:1752/2245 train_time:207508ms step_avg:118.44ms
step:1753/2245 train_time:207624ms step_avg:118.44ms
step:1754/2245 train_time:207746ms step_avg:118.44ms
step:1755/2245 train_time:207863ms step_avg:118.44ms
step:1756/2245 train_time:207986ms step_avg:118.44ms
step:1757/2245 train_time:208102ms step_avg:118.44ms
step:1758/2245 train_time:208225ms step_avg:118.44ms
step:1759/2245 train_time:208341ms step_avg:118.44ms
step:1760/2245 train_time:208465ms step_avg:118.45ms
step:1761/2245 train_time:208581ms step_avg:118.44ms
step:1762/2245 train_time:208704ms step_avg:118.45ms
step:1763/2245 train_time:208821ms step_avg:118.45ms
step:1764/2245 train_time:208944ms step_avg:118.45ms
step:1765/2245 train_time:209061ms step_avg:118.45ms
step:1766/2245 train_time:209184ms step_avg:118.45ms
step:1767/2245 train_time:209300ms step_avg:118.45ms
step:1768/2245 train_time:209423ms step_avg:118.45ms
step:1769/2245 train_time:209539ms step_avg:118.45ms
step:1770/2245 train_time:209661ms step_avg:118.45ms
step:1771/2245 train_time:209778ms step_avg:118.45ms
step:1772/2245 train_time:209901ms step_avg:118.45ms
step:1773/2245 train_time:210017ms step_avg:118.45ms
step:1774/2245 train_time:210140ms step_avg:118.46ms
step:1775/2245 train_time:210257ms step_avg:118.45ms
step:1776/2245 train_time:210379ms step_avg:118.46ms
step:1777/2245 train_time:210496ms step_avg:118.46ms
step:1778/2245 train_time:210620ms step_avg:118.46ms
step:1779/2245 train_time:210737ms step_avg:118.46ms
step:1780/2245 train_time:210860ms step_avg:118.46ms
step:1781/2245 train_time:210976ms step_avg:118.46ms
step:1782/2245 train_time:211099ms step_avg:118.46ms
step:1783/2245 train_time:211215ms step_avg:118.46ms
step:1784/2245 train_time:211338ms step_avg:118.46ms
step:1785/2245 train_time:211455ms step_avg:118.46ms
step:1786/2245 train_time:211578ms step_avg:118.46ms
step:1787/2245 train_time:211695ms step_avg:118.46ms
step:1788/2245 train_time:211818ms step_avg:118.47ms
step:1789/2245 train_time:211934ms step_avg:118.47ms
step:1790/2245 train_time:212058ms step_avg:118.47ms
step:1791/2245 train_time:212175ms step_avg:118.47ms
step:1792/2245 train_time:212298ms step_avg:118.47ms
step:1793/2245 train_time:212414ms step_avg:118.47ms
step:1794/2245 train_time:212538ms step_avg:118.47ms
step:1795/2245 train_time:212654ms step_avg:118.47ms
step:1796/2245 train_time:212777ms step_avg:118.47ms
step:1797/2245 train_time:212894ms step_avg:118.47ms
step:1798/2245 train_time:213018ms step_avg:118.47ms
step:1799/2245 train_time:213135ms step_avg:118.47ms
step:1800/2245 train_time:213258ms step_avg:118.48ms
step:1801/2245 train_time:213375ms step_avg:118.48ms
step:1802/2245 train_time:213499ms step_avg:118.48ms
step:1803/2245 train_time:213615ms step_avg:118.48ms
step:1804/2245 train_time:213738ms step_avg:118.48ms
step:1805/2245 train_time:213855ms step_avg:118.48ms
step:1806/2245 train_time:213978ms step_avg:118.48ms
step:1807/2245 train_time:214095ms step_avg:118.48ms
step:1808/2245 train_time:214218ms step_avg:118.48ms
step:1809/2245 train_time:214335ms step_avg:118.48ms
step:1810/2245 train_time:214458ms step_avg:118.49ms
step:1811/2245 train_time:214575ms step_avg:118.48ms
step:1812/2245 train_time:214699ms step_avg:118.49ms
step:1813/2245 train_time:214815ms step_avg:118.49ms
step:1814/2245 train_time:214938ms step_avg:118.49ms
step:1815/2245 train_time:215055ms step_avg:118.49ms
step:1816/2245 train_time:215178ms step_avg:118.49ms
step:1817/2245 train_time:215295ms step_avg:118.49ms
step:1818/2245 train_time:215418ms step_avg:118.49ms
step:1819/2245 train_time:215535ms step_avg:118.49ms
step:1820/2245 train_time:215658ms step_avg:118.49ms
step:1821/2245 train_time:215774ms step_avg:118.49ms
step:1822/2245 train_time:215897ms step_avg:118.49ms
step:1823/2245 train_time:216014ms step_avg:118.49ms
step:1824/2245 train_time:216136ms step_avg:118.50ms
step:1825/2245 train_time:216253ms step_avg:118.49ms
step:1826/2245 train_time:216376ms step_avg:118.50ms
step:1827/2245 train_time:216492ms step_avg:118.50ms
step:1828/2245 train_time:216615ms step_avg:118.50ms
step:1829/2245 train_time:216731ms step_avg:118.50ms
step:1830/2245 train_time:216854ms step_avg:118.50ms
step:1831/2245 train_time:216971ms step_avg:118.50ms
step:1832/2245 train_time:217094ms step_avg:118.50ms
step:1833/2245 train_time:217212ms step_avg:118.50ms
step:1834/2245 train_time:217335ms step_avg:118.50ms
step:1835/2245 train_time:217451ms step_avg:118.50ms
step:1836/2245 train_time:217574ms step_avg:118.50ms
step:1837/2245 train_time:217691ms step_avg:118.50ms
step:1838/2245 train_time:217815ms step_avg:118.51ms
step:1839/2245 train_time:217931ms step_avg:118.51ms
step:1840/2245 train_time:218054ms step_avg:118.51ms
step:1841/2245 train_time:218171ms step_avg:118.51ms
step:1842/2245 train_time:218294ms step_avg:118.51ms
step:1843/2245 train_time:218411ms step_avg:118.51ms
step:1844/2245 train_time:218534ms step_avg:118.51ms
step:1845/2245 train_time:218651ms step_avg:118.51ms
step:1846/2245 train_time:218774ms step_avg:118.51ms
step:1847/2245 train_time:218892ms step_avg:118.51ms
step:1848/2245 train_time:219015ms step_avg:118.51ms
step:1849/2245 train_time:219131ms step_avg:118.51ms
step:1850/2245 train_time:219254ms step_avg:118.52ms
step:1851/2245 train_time:219371ms step_avg:118.51ms
step:1852/2245 train_time:219494ms step_avg:118.52ms
step:1853/2245 train_time:219610ms step_avg:118.52ms
step:1854/2245 train_time:219733ms step_avg:118.52ms
step:1855/2245 train_time:219851ms step_avg:118.52ms
step:1856/2245 train_time:219974ms step_avg:118.52ms
step:1857/2245 train_time:220092ms step_avg:118.52ms
step:1858/2245 train_time:220215ms step_avg:118.52ms
step:1859/2245 train_time:220331ms step_avg:118.52ms
step:1860/2245 train_time:220455ms step_avg:118.52ms
step:1861/2245 train_time:220571ms step_avg:118.52ms
step:1862/2245 train_time:220694ms step_avg:118.53ms
step:1863/2245 train_time:220812ms step_avg:118.52ms
step:1864/2245 train_time:220934ms step_avg:118.53ms
step:1865/2245 train_time:221051ms step_avg:118.53ms
step:1866/2245 train_time:221173ms step_avg:118.53ms
step:1867/2245 train_time:221291ms step_avg:118.53ms
step:1868/2245 train_time:221415ms step_avg:118.53ms
step:1869/2245 train_time:221532ms step_avg:118.53ms
step:1870/2245 train_time:221655ms step_avg:118.53ms
step:1871/2245 train_time:221770ms step_avg:118.53ms
step:1872/2245 train_time:221893ms step_avg:118.53ms
step:1873/2245 train_time:222010ms step_avg:118.53ms
step:1874/2245 train_time:222134ms step_avg:118.53ms
step:1875/2245 train_time:222251ms step_avg:118.53ms
step:1876/2245 train_time:222373ms step_avg:118.54ms
step:1877/2245 train_time:222491ms step_avg:118.54ms
step:1878/2245 train_time:222613ms step_avg:118.54ms
step:1879/2245 train_time:222730ms step_avg:118.54ms
step:1880/2245 train_time:222853ms step_avg:118.54ms
step:1881/2245 train_time:222970ms step_avg:118.54ms
step:1882/2245 train_time:223092ms step_avg:118.54ms
step:1883/2245 train_time:223209ms step_avg:118.54ms
step:1884/2245 train_time:223333ms step_avg:118.54ms
step:1885/2245 train_time:223451ms step_avg:118.54ms
step:1886/2245 train_time:223574ms step_avg:118.54ms
step:1887/2245 train_time:223690ms step_avg:118.54ms
step:1888/2245 train_time:223814ms step_avg:118.55ms
step:1889/2245 train_time:223930ms step_avg:118.54ms
step:1890/2245 train_time:224054ms step_avg:118.55ms
step:1891/2245 train_time:224171ms step_avg:118.55ms
step:1892/2245 train_time:224294ms step_avg:118.55ms
step:1893/2245 train_time:224411ms step_avg:118.55ms
step:1894/2245 train_time:224534ms step_avg:118.55ms
step:1895/2245 train_time:224652ms step_avg:118.55ms
step:1896/2245 train_time:224775ms step_avg:118.55ms
step:1897/2245 train_time:224891ms step_avg:118.55ms
step:1898/2245 train_time:225015ms step_avg:118.55ms
step:1899/2245 train_time:225131ms step_avg:118.55ms
step:1900/2245 train_time:225254ms step_avg:118.55ms
step:1901/2245 train_time:225371ms step_avg:118.55ms
step:1902/2245 train_time:225494ms step_avg:118.56ms
step:1903/2245 train_time:225611ms step_avg:118.56ms
step:1904/2245 train_time:225734ms step_avg:118.56ms
step:1905/2245 train_time:225852ms step_avg:118.56ms
step:1906/2245 train_time:225975ms step_avg:118.56ms
step:1907/2245 train_time:226091ms step_avg:118.56ms
step:1908/2245 train_time:226214ms step_avg:118.56ms
step:1909/2245 train_time:226331ms step_avg:118.56ms
step:1910/2245 train_time:226453ms step_avg:118.56ms
step:1911/2245 train_time:226570ms step_avg:118.56ms
step:1912/2245 train_time:226693ms step_avg:118.56ms
step:1913/2245 train_time:226809ms step_avg:118.56ms
step:1914/2245 train_time:226933ms step_avg:118.56ms
step:1915/2245 train_time:227049ms step_avg:118.56ms
step:1916/2245 train_time:227172ms step_avg:118.57ms
step:1917/2245 train_time:227289ms step_avg:118.56ms
step:1918/2245 train_time:227412ms step_avg:118.57ms
step:1919/2245 train_time:227528ms step_avg:118.57ms
step:1920/2245 train_time:227651ms step_avg:118.57ms
step:1921/2245 train_time:227767ms step_avg:118.57ms
step:1922/2245 train_time:227891ms step_avg:118.57ms
step:1923/2245 train_time:228007ms step_avg:118.57ms
step:1924/2245 train_time:228130ms step_avg:118.57ms
step:1925/2245 train_time:228247ms step_avg:118.57ms
step:1926/2245 train_time:228369ms step_avg:118.57ms
step:1927/2245 train_time:228485ms step_avg:118.57ms
step:1928/2245 train_time:228608ms step_avg:118.57ms
step:1929/2245 train_time:228725ms step_avg:118.57ms
step:1930/2245 train_time:228848ms step_avg:118.57ms
step:1931/2245 train_time:228964ms step_avg:118.57ms
step:1932/2245 train_time:229087ms step_avg:118.57ms
step:1933/2245 train_time:229203ms step_avg:118.57ms
step:1934/2245 train_time:229326ms step_avg:118.58ms
step:1935/2245 train_time:229443ms step_avg:118.58ms
step:1936/2245 train_time:229566ms step_avg:118.58ms
step:1937/2245 train_time:229682ms step_avg:118.58ms
step:1938/2245 train_time:229805ms step_avg:118.58ms
step:1939/2245 train_time:229922ms step_avg:118.58ms
step:1940/2245 train_time:230045ms step_avg:118.58ms
step:1941/2245 train_time:230162ms step_avg:118.58ms
step:1942/2245 train_time:230284ms step_avg:118.58ms
step:1943/2245 train_time:230401ms step_avg:118.58ms
step:1944/2245 train_time:230524ms step_avg:118.58ms
step:1945/2245 train_time:230640ms step_avg:118.58ms
step:1946/2245 train_time:230763ms step_avg:118.58ms
step:1947/2245 train_time:230879ms step_avg:118.58ms
step:1948/2245 train_time:231002ms step_avg:118.58ms
step:1949/2245 train_time:231119ms step_avg:118.58ms
step:1950/2245 train_time:231242ms step_avg:118.59ms
step:1951/2245 train_time:231358ms step_avg:118.58ms
step:1952/2245 train_time:231481ms step_avg:118.59ms
step:1953/2245 train_time:231597ms step_avg:118.59ms
step:1954/2245 train_time:231720ms step_avg:118.59ms
step:1955/2245 train_time:231836ms step_avg:118.59ms
step:1956/2245 train_time:231959ms step_avg:118.59ms
step:1957/2245 train_time:232076ms step_avg:118.59ms
step:1958/2245 train_time:232200ms step_avg:118.59ms
step:1959/2245 train_time:232316ms step_avg:118.59ms
step:1960/2245 train_time:232439ms step_avg:118.59ms
step:1961/2245 train_time:232557ms step_avg:118.59ms
step:1962/2245 train_time:232679ms step_avg:118.59ms
step:1963/2245 train_time:232796ms step_avg:118.59ms
step:1964/2245 train_time:232919ms step_avg:118.59ms
step:1965/2245 train_time:233035ms step_avg:118.59ms
step:1966/2245 train_time:233159ms step_avg:118.60ms
step:1967/2245 train_time:233276ms step_avg:118.59ms
step:1968/2245 train_time:233399ms step_avg:118.60ms
step:1969/2245 train_time:233515ms step_avg:118.60ms
step:1970/2245 train_time:233639ms step_avg:118.60ms
step:1971/2245 train_time:233755ms step_avg:118.60ms
step:1972/2245 train_time:233878ms step_avg:118.60ms
step:1973/2245 train_time:233995ms step_avg:118.60ms
step:1974/2245 train_time:234118ms step_avg:118.60ms
step:1975/2245 train_time:234234ms step_avg:118.60ms
step:1976/2245 train_time:234358ms step_avg:118.60ms
step:1977/2245 train_time:234474ms step_avg:118.60ms
step:1978/2245 train_time:234598ms step_avg:118.60ms
step:1979/2245 train_time:234714ms step_avg:118.60ms
step:1980/2245 train_time:234837ms step_avg:118.60ms
step:1981/2245 train_time:234954ms step_avg:118.60ms
step:1982/2245 train_time:235077ms step_avg:118.61ms
step:1983/2245 train_time:235193ms step_avg:118.60ms
step:1984/2245 train_time:235317ms step_avg:118.61ms
step:1985/2245 train_time:235433ms step_avg:118.61ms
step:1986/2245 train_time:235557ms step_avg:118.61ms
step:1987/2245 train_time:235673ms step_avg:118.61ms
step:1988/2245 train_time:235796ms step_avg:118.61ms
step:1989/2245 train_time:235913ms step_avg:118.61ms
step:1990/2245 train_time:236037ms step_avg:118.61ms
step:1991/2245 train_time:236154ms step_avg:118.61ms
step:1992/2245 train_time:236277ms step_avg:118.61ms
step:1993/2245 train_time:236394ms step_avg:118.61ms
step:1994/2245 train_time:236517ms step_avg:118.61ms
step:1995/2245 train_time:236634ms step_avg:118.61ms
step:1996/2245 train_time:236757ms step_avg:118.62ms
step:1997/2245 train_time:236874ms step_avg:118.61ms
step:1998/2245 train_time:236996ms step_avg:118.62ms
step:1999/2245 train_time:237114ms step_avg:118.62ms
step:2000/2245 train_time:237237ms step_avg:118.62ms
step:2000/2245 val_loss:3.3245 train_time:237303ms step_avg:118.65ms
step:2001/2245 train_time:237354ms step_avg:118.62ms
step:2002/2245 train_time:237476ms step_avg:118.62ms
step:2003/2245 train_time:237592ms step_avg:118.62ms
step:2004/2245 train_time:237715ms step_avg:118.62ms
step:2005/2245 train_time:237832ms step_avg:118.62ms
step:2006/2245 train_time:237955ms step_avg:118.62ms
step:2007/2245 train_time:238071ms step_avg:118.62ms
step:2008/2245 train_time:238195ms step_avg:118.62ms
step:2009/2245 train_time:238312ms step_avg:118.62ms
step:2010/2245 train_time:238436ms step_avg:118.62ms
step:2011/2245 train_time:238553ms step_avg:118.62ms
step:2012/2245 train_time:238675ms step_avg:118.63ms
step:2013/2245 train_time:238792ms step_avg:118.62ms
step:2014/2245 train_time:238915ms step_avg:118.63ms
step:2015/2245 train_time:239032ms step_avg:118.63ms
step:2016/2245 train_time:239155ms step_avg:118.63ms
step:2017/2245 train_time:239271ms step_avg:118.63ms
step:2018/2245 train_time:239395ms step_avg:118.63ms
step:2019/2245 train_time:239511ms step_avg:118.63ms
step:2020/2245 train_time:239635ms step_avg:118.63ms
step:2021/2245 train_time:239752ms step_avg:118.63ms
step:2022/2245 train_time:239874ms step_avg:118.63ms
step:2023/2245 train_time:239991ms step_avg:118.63ms
step:2024/2245 train_time:240114ms step_avg:118.63ms
step:2025/2245 train_time:240231ms step_avg:118.63ms
step:2026/2245 train_time:240354ms step_avg:118.63ms
step:2027/2245 train_time:240472ms step_avg:118.63ms
step:2028/2245 train_time:240594ms step_avg:118.64ms
step:2029/2245 train_time:240710ms step_avg:118.64ms
step:2030/2245 train_time:240834ms step_avg:118.64ms
step:2031/2245 train_time:240951ms step_avg:118.64ms
step:2032/2245 train_time:241074ms step_avg:118.64ms
step:2033/2245 train_time:241190ms step_avg:118.64ms
step:2034/2245 train_time:241313ms step_avg:118.64ms
step:2035/2245 train_time:241430ms step_avg:118.64ms
step:2036/2245 train_time:241553ms step_avg:118.64ms
step:2037/2245 train_time:241670ms step_avg:118.64ms
step:2038/2245 train_time:241794ms step_avg:118.64ms
step:2039/2245 train_time:241911ms step_avg:118.64ms
step:2040/2245 train_time:242034ms step_avg:118.64ms
step:2041/2245 train_time:242151ms step_avg:118.64ms
step:2042/2245 train_time:242274ms step_avg:118.65ms
step:2043/2245 train_time:242391ms step_avg:118.64ms
step:2044/2245 train_time:242514ms step_avg:118.65ms
step:2045/2245 train_time:242630ms step_avg:118.65ms
step:2046/2245 train_time:242755ms step_avg:118.65ms
step:2047/2245 train_time:242872ms step_avg:118.65ms
step:2048/2245 train_time:242996ms step_avg:118.65ms
step:2049/2245 train_time:243112ms step_avg:118.65ms
step:2050/2245 train_time:243236ms step_avg:118.65ms
step:2051/2245 train_time:243352ms step_avg:118.65ms
step:2052/2245 train_time:243475ms step_avg:118.65ms
step:2053/2245 train_time:243592ms step_avg:118.65ms
step:2054/2245 train_time:243715ms step_avg:118.65ms
step:2055/2245 train_time:243832ms step_avg:118.65ms
step:2056/2245 train_time:243955ms step_avg:118.66ms
step:2057/2245 train_time:244073ms step_avg:118.65ms
step:2058/2245 train_time:244196ms step_avg:118.66ms
step:2059/2245 train_time:244313ms step_avg:118.66ms
step:2060/2245 train_time:244436ms step_avg:118.66ms
step:2061/2245 train_time:244553ms step_avg:118.66ms
step:2062/2245 train_time:244675ms step_avg:118.66ms
step:2063/2245 train_time:244792ms step_avg:118.66ms
step:2064/2245 train_time:244915ms step_avg:118.66ms
step:2065/2245 train_time:245032ms step_avg:118.66ms
step:2066/2245 train_time:245155ms step_avg:118.66ms
step:2067/2245 train_time:245271ms step_avg:118.66ms
step:2068/2245 train_time:245395ms step_avg:118.66ms
step:2069/2245 train_time:245511ms step_avg:118.66ms
step:2070/2245 train_time:245634ms step_avg:118.66ms
step:2071/2245 train_time:245751ms step_avg:118.66ms
step:2072/2245 train_time:245874ms step_avg:118.66ms
step:2073/2245 train_time:245990ms step_avg:118.66ms
step:2074/2245 train_time:246114ms step_avg:118.67ms
step:2075/2245 train_time:246230ms step_avg:118.66ms
step:2076/2245 train_time:246353ms step_avg:118.67ms
step:2077/2245 train_time:246470ms step_avg:118.67ms
step:2078/2245 train_time:246593ms step_avg:118.67ms
step:2079/2245 train_time:246710ms step_avg:118.67ms
step:2080/2245 train_time:246833ms step_avg:118.67ms
step:2081/2245 train_time:246950ms step_avg:118.67ms
step:2082/2245 train_time:247074ms step_avg:118.67ms
step:2083/2245 train_time:247190ms step_avg:118.67ms
step:2084/2245 train_time:247314ms step_avg:118.67ms
step:2085/2245 train_time:247430ms step_avg:118.67ms
step:2086/2245 train_time:247554ms step_avg:118.67ms
step:2087/2245 train_time:247670ms step_avg:118.67ms
step:2088/2245 train_time:247794ms step_avg:118.68ms
step:2089/2245 train_time:247910ms step_avg:118.67ms
step:2090/2245 train_time:248034ms step_avg:118.68ms
step:2091/2245 train_time:248151ms step_avg:118.68ms
step:2092/2245 train_time:248273ms step_avg:118.68ms
step:2093/2245 train_time:248390ms step_avg:118.68ms
step:2094/2245 train_time:248514ms step_avg:118.68ms
step:2095/2245 train_time:248630ms step_avg:118.68ms
step:2096/2245 train_time:248753ms step_avg:118.68ms
step:2097/2245 train_time:248871ms step_avg:118.68ms
step:2098/2245 train_time:248994ms step_avg:118.68ms
step:2099/2245 train_time:249110ms step_avg:118.68ms
step:2100/2245 train_time:249234ms step_avg:118.68ms
step:2101/2245 train_time:249351ms step_avg:118.68ms
step:2102/2245 train_time:249474ms step_avg:118.68ms
step:2103/2245 train_time:249590ms step_avg:118.68ms
step:2104/2245 train_time:249713ms step_avg:118.68ms
step:2105/2245 train_time:249830ms step_avg:118.68ms
step:2106/2245 train_time:249953ms step_avg:118.69ms
step:2107/2245 train_time:250069ms step_avg:118.69ms
step:2108/2245 train_time:250192ms step_avg:118.69ms
step:2109/2245 train_time:250309ms step_avg:118.69ms
step:2110/2245 train_time:250433ms step_avg:118.69ms
step:2111/2245 train_time:250550ms step_avg:118.69ms
step:2112/2245 train_time:250673ms step_avg:118.69ms
step:2113/2245 train_time:250790ms step_avg:118.69ms
step:2114/2245 train_time:250914ms step_avg:118.69ms
step:2115/2245 train_time:251031ms step_avg:118.69ms
step:2116/2245 train_time:251155ms step_avg:118.69ms
step:2117/2245 train_time:251271ms step_avg:118.69ms
step:2118/2245 train_time:251395ms step_avg:118.69ms
step:2119/2245 train_time:251511ms step_avg:118.69ms
step:2120/2245 train_time:251634ms step_avg:118.70ms
step:2121/2245 train_time:251751ms step_avg:118.69ms
step:2122/2245 train_time:251874ms step_avg:118.70ms
step:2123/2245 train_time:251990ms step_avg:118.70ms
step:2124/2245 train_time:252113ms step_avg:118.70ms
step:2125/2245 train_time:252231ms step_avg:118.70ms
step:2126/2245 train_time:252355ms step_avg:118.70ms
step:2127/2245 train_time:252471ms step_avg:118.70ms
step:2128/2245 train_time:252595ms step_avg:118.70ms
step:2129/2245 train_time:252712ms step_avg:118.70ms
step:2130/2245 train_time:252836ms step_avg:118.70ms
step:2131/2245 train_time:252952ms step_avg:118.70ms
step:2132/2245 train_time:253075ms step_avg:118.70ms
step:2133/2245 train_time:253192ms step_avg:118.70ms
step:2134/2245 train_time:253315ms step_avg:118.70ms
step:2135/2245 train_time:253432ms step_avg:118.70ms
step:2136/2245 train_time:253555ms step_avg:118.71ms
step:2137/2245 train_time:253671ms step_avg:118.70ms
step:2138/2245 train_time:253794ms step_avg:118.71ms
step:2139/2245 train_time:253911ms step_avg:118.71ms
step:2140/2245 train_time:254034ms step_avg:118.71ms
step:2141/2245 train_time:254150ms step_avg:118.71ms
step:2142/2245 train_time:254272ms step_avg:118.71ms
step:2143/2245 train_time:254389ms step_avg:118.71ms
step:2144/2245 train_time:254513ms step_avg:118.71ms
step:2145/2245 train_time:254630ms step_avg:118.71ms
step:2146/2245 train_time:254753ms step_avg:118.71ms
step:2147/2245 train_time:254869ms step_avg:118.71ms
step:2148/2245 train_time:254993ms step_avg:118.71ms
step:2149/2245 train_time:255110ms step_avg:118.71ms
step:2150/2245 train_time:255233ms step_avg:118.71ms
step:2151/2245 train_time:255350ms step_avg:118.71ms
step:2152/2245 train_time:255473ms step_avg:118.71ms
step:2153/2245 train_time:255591ms step_avg:118.71ms
step:2154/2245 train_time:255715ms step_avg:118.72ms
step:2155/2245 train_time:255831ms step_avg:118.71ms
step:2156/2245 train_time:255954ms step_avg:118.72ms
step:2157/2245 train_time:256071ms step_avg:118.72ms
step:2158/2245 train_time:256194ms step_avg:118.72ms
step:2159/2245 train_time:256312ms step_avg:118.72ms
step:2160/2245 train_time:256435ms step_avg:118.72ms
step:2161/2245 train_time:256551ms step_avg:118.72ms
step:2162/2245 train_time:256674ms step_avg:118.72ms
step:2163/2245 train_time:256791ms step_avg:118.72ms
step:2164/2245 train_time:256914ms step_avg:118.72ms
step:2165/2245 train_time:257031ms step_avg:118.72ms
step:2166/2245 train_time:257154ms step_avg:118.72ms
step:2167/2245 train_time:257270ms step_avg:118.72ms
step:2168/2245 train_time:257394ms step_avg:118.72ms
step:2169/2245 train_time:257510ms step_avg:118.72ms
step:2170/2245 train_time:257634ms step_avg:118.73ms
step:2171/2245 train_time:257751ms step_avg:118.72ms
step:2172/2245 train_time:257874ms step_avg:118.73ms
step:2173/2245 train_time:257990ms step_avg:118.73ms
step:2174/2245 train_time:258114ms step_avg:118.73ms
step:2175/2245 train_time:258231ms step_avg:118.73ms
step:2176/2245 train_time:258353ms step_avg:118.73ms
step:2177/2245 train_time:258470ms step_avg:118.73ms
step:2178/2245 train_time:258593ms step_avg:118.73ms
step:2179/2245 train_time:258710ms step_avg:118.73ms
step:2180/2245 train_time:258834ms step_avg:118.73ms
step:2181/2245 train_time:258951ms step_avg:118.73ms
step:2182/2245 train_time:259074ms step_avg:118.73ms
step:2183/2245 train_time:259191ms step_avg:118.73ms
step:2184/2245 train_time:259315ms step_avg:118.73ms
step:2185/2245 train_time:259432ms step_avg:118.73ms
step:2186/2245 train_time:259555ms step_avg:118.74ms
step:2187/2245 train_time:259672ms step_avg:118.73ms
step:2188/2245 train_time:259795ms step_avg:118.74ms
step:2189/2245 train_time:259912ms step_avg:118.74ms
step:2190/2245 train_time:260036ms step_avg:118.74ms
step:2191/2245 train_time:260152ms step_avg:118.74ms
step:2192/2245 train_time:260275ms step_avg:118.74ms
step:2193/2245 train_time:260392ms step_avg:118.74ms
step:2194/2245 train_time:260515ms step_avg:118.74ms
step:2195/2245 train_time:260631ms step_avg:118.74ms
step:2196/2245 train_time:260755ms step_avg:118.74ms
step:2197/2245 train_time:260871ms step_avg:118.74ms
step:2198/2245 train_time:260995ms step_avg:118.74ms
step:2199/2245 train_time:261112ms step_avg:118.74ms
step:2200/2245 train_time:261235ms step_avg:118.74ms
step:2201/2245 train_time:261352ms step_avg:118.74ms
step:2202/2245 train_time:261475ms step_avg:118.74ms
step:2203/2245 train_time:261592ms step_avg:118.74ms
step:2204/2245 train_time:261714ms step_avg:118.75ms
step:2205/2245 train_time:261832ms step_avg:118.74ms
step:2206/2245 train_time:261954ms step_avg:118.75ms
step:2207/2245 train_time:262072ms step_avg:118.75ms
step:2208/2245 train_time:262195ms step_avg:118.75ms
step:2209/2245 train_time:262313ms step_avg:118.75ms
step:2210/2245 train_time:262436ms step_avg:118.75ms
step:2211/2245 train_time:262553ms step_avg:118.75ms
step:2212/2245 train_time:262676ms step_avg:118.75ms
step:2213/2245 train_time:262793ms step_avg:118.75ms
step:2214/2245 train_time:262917ms step_avg:118.75ms
step:2215/2245 train_time:263034ms step_avg:118.75ms
step:2216/2245 train_time:263158ms step_avg:118.75ms
step:2217/2245 train_time:263276ms step_avg:118.75ms
step:2218/2245 train_time:263399ms step_avg:118.76ms
step:2219/2245 train_time:263516ms step_avg:118.75ms
step:2220/2245 train_time:263640ms step_avg:118.76ms
step:2221/2245 train_time:263757ms step_avg:118.76ms
step:2222/2245 train_time:263880ms step_avg:118.76ms
step:2223/2245 train_time:263996ms step_avg:118.76ms
step:2224/2245 train_time:264120ms step_avg:118.76ms
step:2225/2245 train_time:264236ms step_avg:118.76ms
step:2226/2245 train_time:264360ms step_avg:118.76ms
step:2227/2245 train_time:264478ms step_avg:118.76ms
step:2228/2245 train_time:264601ms step_avg:118.76ms
step:2229/2245 train_time:264718ms step_avg:118.76ms
step:2230/2245 train_time:264841ms step_avg:118.76ms
step:2231/2245 train_time:264958ms step_avg:118.76ms
step:2232/2245 train_time:265081ms step_avg:118.76ms
step:2233/2245 train_time:265198ms step_avg:118.76ms
step:2234/2245 train_time:265321ms step_avg:118.77ms
step:2235/2245 train_time:265438ms step_avg:118.76ms
step:2236/2245 train_time:265561ms step_avg:118.77ms
step:2237/2245 train_time:265678ms step_avg:118.77ms
step:2238/2245 train_time:265801ms step_avg:118.77ms
step:2239/2245 train_time:265918ms step_avg:118.77ms
step:2240/2245 train_time:266042ms step_avg:118.77ms
step:2241/2245 train_time:266159ms step_avg:118.77ms
step:2242/2245 train_time:266282ms step_avg:118.77ms
step:2243/2245 train_time:266398ms step_avg:118.77ms
step:2244/2245 train_time:266521ms step_avg:118.77ms
step:2245/2245 train_time:266638ms step_avg:118.77ms
step:2245/2245 val_loss:3.2788 train_time:266707ms step_avg:118.80ms
peak memory allocated: 29050 MiB reserved: 44436 MiB

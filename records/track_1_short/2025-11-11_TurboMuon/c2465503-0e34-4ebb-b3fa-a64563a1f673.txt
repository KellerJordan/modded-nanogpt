import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 09:53:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   42C    P0            116W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   42C    P0            118W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   42C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   41C    P0            120W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3338078      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3338079      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3338080      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3338081      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3338079      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3338080      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3338081      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:168ms step_avg:168.13ms
step:2/2245 train_time:222ms step_avg:111.24ms
step:3/2245 train_time:331ms step_avg:110.47ms
step:4/2245 train_time:450ms step_avg:112.41ms
step:5/2245 train_time:565ms step_avg:113.08ms
step:6/2245 train_time:688ms step_avg:114.72ms
step:7/2245 train_time:805ms step_avg:115.00ms
step:8/2245 train_time:928ms step_avg:115.94ms
step:9/2245 train_time:1044ms step_avg:116.01ms
step:10/2245 train_time:1168ms step_avg:116.82ms
step:11/2245 train_time:1284ms step_avg:116.77ms
step:12/2245 train_time:1408ms step_avg:117.30ms
step:13/2245 train_time:1525ms step_avg:117.29ms
step:14/2245 train_time:1648ms step_avg:117.73ms
step:15/2245 train_time:1765ms step_avg:117.68ms
step:16/2245 train_time:1889ms step_avg:118.04ms
step:17/2245 train_time:2006ms step_avg:118.00ms
step:18/2245 train_time:2130ms step_avg:118.31ms
step:19/2245 train_time:2247ms step_avg:118.24ms
step:20/2245 train_time:2370ms step_avg:118.49ms
step:21/2245 train_time:2487ms step_avg:118.42ms
step:22/2245 train_time:2610ms step_avg:118.64ms
step:23/2245 train_time:2727ms step_avg:118.56ms
step:24/2245 train_time:2850ms step_avg:118.76ms
step:25/2245 train_time:2967ms step_avg:118.69ms
step:26/2245 train_time:3090ms step_avg:118.85ms
step:27/2245 train_time:3207ms step_avg:118.78ms
step:28/2245 train_time:3330ms step_avg:118.92ms
step:29/2245 train_time:3446ms step_avg:118.84ms
step:30/2245 train_time:3569ms step_avg:118.97ms
step:31/2245 train_time:3686ms step_avg:118.90ms
step:32/2245 train_time:3808ms step_avg:119.01ms
step:33/2245 train_time:3925ms step_avg:118.94ms
step:34/2245 train_time:4048ms step_avg:119.06ms
step:35/2245 train_time:4165ms step_avg:119.00ms
step:36/2245 train_time:4288ms step_avg:119.10ms
step:37/2245 train_time:4404ms step_avg:119.04ms
step:38/2245 train_time:4527ms step_avg:119.13ms
step:39/2245 train_time:4644ms step_avg:119.07ms
step:40/2245 train_time:4766ms step_avg:119.16ms
step:41/2245 train_time:4883ms step_avg:119.09ms
step:42/2245 train_time:5005ms step_avg:119.17ms
step:43/2245 train_time:5121ms step_avg:119.10ms
step:44/2245 train_time:5243ms step_avg:119.16ms
step:45/2245 train_time:5359ms step_avg:119.09ms
step:46/2245 train_time:5481ms step_avg:119.14ms
step:47/2245 train_time:5596ms step_avg:119.06ms
step:48/2245 train_time:5718ms step_avg:119.12ms
step:49/2245 train_time:5833ms step_avg:119.04ms
step:50/2245 train_time:5955ms step_avg:119.10ms
step:51/2245 train_time:6071ms step_avg:119.03ms
step:52/2245 train_time:6192ms step_avg:119.08ms
step:53/2245 train_time:6307ms step_avg:119.01ms
step:54/2245 train_time:6429ms step_avg:119.06ms
step:55/2245 train_time:6545ms step_avg:118.99ms
step:56/2245 train_time:6666ms step_avg:119.04ms
step:57/2245 train_time:6782ms step_avg:118.98ms
step:58/2245 train_time:6904ms step_avg:119.03ms
step:59/2245 train_time:7019ms step_avg:118.97ms
step:60/2245 train_time:7141ms step_avg:119.02ms
step:61/2245 train_time:7257ms step_avg:118.96ms
step:62/2245 train_time:7379ms step_avg:119.01ms
step:63/2245 train_time:7494ms step_avg:118.95ms
step:64/2245 train_time:7616ms step_avg:119.00ms
step:65/2245 train_time:7731ms step_avg:118.94ms
step:66/2245 train_time:7853ms step_avg:118.98ms
step:67/2245 train_time:7968ms step_avg:118.93ms
step:68/2245 train_time:8090ms step_avg:118.97ms
step:69/2245 train_time:8205ms step_avg:118.92ms
step:70/2245 train_time:8327ms step_avg:118.95ms
step:71/2245 train_time:8442ms step_avg:118.90ms
step:72/2245 train_time:8563ms step_avg:118.94ms
step:73/2245 train_time:8678ms step_avg:118.88ms
step:74/2245 train_time:8800ms step_avg:118.91ms
step:75/2245 train_time:8915ms step_avg:118.86ms
step:76/2245 train_time:9037ms step_avg:118.90ms
step:77/2245 train_time:9152ms step_avg:118.85ms
step:78/2245 train_time:9273ms step_avg:118.89ms
step:79/2245 train_time:9389ms step_avg:118.84ms
step:80/2245 train_time:9510ms step_avg:118.88ms
step:81/2245 train_time:9625ms step_avg:118.83ms
step:82/2245 train_time:9746ms step_avg:118.86ms
step:83/2245 train_time:9862ms step_avg:118.82ms
step:84/2245 train_time:9984ms step_avg:118.85ms
step:85/2245 train_time:10098ms step_avg:118.81ms
step:86/2245 train_time:10220ms step_avg:118.84ms
step:87/2245 train_time:10335ms step_avg:118.79ms
step:88/2245 train_time:10456ms step_avg:118.82ms
step:89/2245 train_time:10571ms step_avg:118.78ms
step:90/2245 train_time:10693ms step_avg:118.81ms
step:91/2245 train_time:10808ms step_avg:118.77ms
step:92/2245 train_time:10929ms step_avg:118.79ms
step:93/2245 train_time:11044ms step_avg:118.76ms
step:94/2245 train_time:11165ms step_avg:118.78ms
step:95/2245 train_time:11281ms step_avg:118.74ms
step:96/2245 train_time:11402ms step_avg:118.77ms
step:97/2245 train_time:11517ms step_avg:118.73ms
step:98/2245 train_time:11638ms step_avg:118.76ms
step:99/2245 train_time:11753ms step_avg:118.72ms
step:100/2245 train_time:11874ms step_avg:118.74ms
step:101/2245 train_time:11989ms step_avg:118.71ms
step:102/2245 train_time:12111ms step_avg:118.73ms
step:103/2245 train_time:12226ms step_avg:118.70ms
step:104/2245 train_time:12347ms step_avg:118.72ms
step:105/2245 train_time:12462ms step_avg:118.69ms
step:106/2245 train_time:12584ms step_avg:118.71ms
step:107/2245 train_time:12698ms step_avg:118.67ms
step:108/2245 train_time:12819ms step_avg:118.69ms
step:109/2245 train_time:12934ms step_avg:118.66ms
step:110/2245 train_time:13055ms step_avg:118.68ms
step:111/2245 train_time:13170ms step_avg:118.65ms
step:112/2245 train_time:13291ms step_avg:118.67ms
step:113/2245 train_time:13406ms step_avg:118.64ms
step:114/2245 train_time:13527ms step_avg:118.66ms
step:115/2245 train_time:13642ms step_avg:118.63ms
step:116/2245 train_time:13764ms step_avg:118.65ms
step:117/2245 train_time:13878ms step_avg:118.62ms
step:118/2245 train_time:13999ms step_avg:118.63ms
step:119/2245 train_time:14114ms step_avg:118.60ms
step:120/2245 train_time:14235ms step_avg:118.62ms
step:121/2245 train_time:14350ms step_avg:118.60ms
step:122/2245 train_time:14471ms step_avg:118.62ms
step:123/2245 train_time:14586ms step_avg:118.59ms
step:124/2245 train_time:14707ms step_avg:118.61ms
step:125/2245 train_time:14822ms step_avg:118.58ms
step:126/2245 train_time:14943ms step_avg:118.60ms
step:127/2245 train_time:15057ms step_avg:118.56ms
step:128/2245 train_time:15179ms step_avg:118.58ms
step:129/2245 train_time:15293ms step_avg:118.55ms
step:130/2245 train_time:15414ms step_avg:118.57ms
step:131/2245 train_time:15529ms step_avg:118.54ms
step:132/2245 train_time:15650ms step_avg:118.56ms
step:133/2245 train_time:15765ms step_avg:118.53ms
step:134/2245 train_time:15886ms step_avg:118.55ms
step:135/2245 train_time:16000ms step_avg:118.52ms
step:136/2245 train_time:16121ms step_avg:118.54ms
step:137/2245 train_time:16236ms step_avg:118.51ms
step:138/2245 train_time:16357ms step_avg:118.53ms
step:139/2245 train_time:16471ms step_avg:118.50ms
step:140/2245 train_time:16592ms step_avg:118.52ms
step:141/2245 train_time:16707ms step_avg:118.49ms
step:142/2245 train_time:16828ms step_avg:118.51ms
step:143/2245 train_time:16943ms step_avg:118.48ms
step:144/2245 train_time:17064ms step_avg:118.50ms
step:145/2245 train_time:17179ms step_avg:118.47ms
step:146/2245 train_time:17300ms step_avg:118.49ms
step:147/2245 train_time:17414ms step_avg:118.46ms
step:148/2245 train_time:17535ms step_avg:118.48ms
step:149/2245 train_time:17650ms step_avg:118.45ms
step:150/2245 train_time:17770ms step_avg:118.47ms
step:151/2245 train_time:17885ms step_avg:118.44ms
step:152/2245 train_time:18006ms step_avg:118.46ms
step:153/2245 train_time:18121ms step_avg:118.44ms
step:154/2245 train_time:18241ms step_avg:118.45ms
step:155/2245 train_time:18356ms step_avg:118.42ms
step:156/2245 train_time:18476ms step_avg:118.44ms
step:157/2245 train_time:18591ms step_avg:118.42ms
step:158/2245 train_time:18712ms step_avg:118.43ms
step:159/2245 train_time:18827ms step_avg:118.41ms
step:160/2245 train_time:18947ms step_avg:118.42ms
step:161/2245 train_time:19062ms step_avg:118.40ms
step:162/2245 train_time:19183ms step_avg:118.41ms
step:163/2245 train_time:19297ms step_avg:118.39ms
step:164/2245 train_time:19418ms step_avg:118.40ms
step:165/2245 train_time:19532ms step_avg:118.38ms
step:166/2245 train_time:19653ms step_avg:118.39ms
step:167/2245 train_time:19767ms step_avg:118.37ms
step:168/2245 train_time:19888ms step_avg:118.38ms
step:169/2245 train_time:20003ms step_avg:118.36ms
step:170/2245 train_time:20124ms step_avg:118.37ms
step:171/2245 train_time:20238ms step_avg:118.35ms
step:172/2245 train_time:20358ms step_avg:118.36ms
step:173/2245 train_time:20473ms step_avg:118.34ms
step:174/2245 train_time:20593ms step_avg:118.35ms
step:175/2245 train_time:20708ms step_avg:118.33ms
step:176/2245 train_time:20828ms step_avg:118.34ms
step:177/2245 train_time:20942ms step_avg:118.32ms
step:178/2245 train_time:21063ms step_avg:118.33ms
step:179/2245 train_time:21177ms step_avg:118.31ms
step:180/2245 train_time:21298ms step_avg:118.32ms
step:181/2245 train_time:21412ms step_avg:118.30ms
step:182/2245 train_time:21533ms step_avg:118.31ms
step:183/2245 train_time:21647ms step_avg:118.29ms
step:184/2245 train_time:21768ms step_avg:118.30ms
step:185/2245 train_time:21882ms step_avg:118.28ms
step:186/2245 train_time:22003ms step_avg:118.30ms
step:187/2245 train_time:22117ms step_avg:118.27ms
step:188/2245 train_time:22238ms step_avg:118.28ms
step:189/2245 train_time:22352ms step_avg:118.26ms
step:190/2245 train_time:22473ms step_avg:118.28ms
step:191/2245 train_time:22587ms step_avg:118.26ms
step:192/2245 train_time:22708ms step_avg:118.27ms
step:193/2245 train_time:22822ms step_avg:118.25ms
step:194/2245 train_time:22943ms step_avg:118.26ms
step:195/2245 train_time:23057ms step_avg:118.24ms
step:196/2245 train_time:23178ms step_avg:118.25ms
step:197/2245 train_time:23292ms step_avg:118.24ms
step:198/2245 train_time:23413ms step_avg:118.25ms
step:199/2245 train_time:23527ms step_avg:118.23ms
step:200/2245 train_time:23648ms step_avg:118.24ms
step:201/2245 train_time:23762ms step_avg:118.22ms
step:202/2245 train_time:23883ms step_avg:118.23ms
step:203/2245 train_time:23997ms step_avg:118.21ms
step:204/2245 train_time:24117ms step_avg:118.22ms
step:205/2245 train_time:24232ms step_avg:118.20ms
step:206/2245 train_time:24353ms step_avg:118.22ms
step:207/2245 train_time:24467ms step_avg:118.20ms
step:208/2245 train_time:24588ms step_avg:118.21ms
step:209/2245 train_time:24703ms step_avg:118.19ms
step:210/2245 train_time:24824ms step_avg:118.21ms
step:211/2245 train_time:24938ms step_avg:118.19ms
step:212/2245 train_time:25058ms step_avg:118.20ms
step:213/2245 train_time:25173ms step_avg:118.18ms
step:214/2245 train_time:25293ms step_avg:118.19ms
step:215/2245 train_time:25408ms step_avg:118.18ms
step:216/2245 train_time:25529ms step_avg:118.19ms
step:217/2245 train_time:25643ms step_avg:118.17ms
step:218/2245 train_time:25764ms step_avg:118.18ms
step:219/2245 train_time:25878ms step_avg:118.17ms
step:220/2245 train_time:25999ms step_avg:118.18ms
step:221/2245 train_time:26113ms step_avg:118.16ms
step:222/2245 train_time:26234ms step_avg:118.17ms
step:223/2245 train_time:26348ms step_avg:118.15ms
step:224/2245 train_time:26468ms step_avg:118.16ms
step:225/2245 train_time:26583ms step_avg:118.15ms
step:226/2245 train_time:26703ms step_avg:118.16ms
step:227/2245 train_time:26818ms step_avg:118.14ms
step:228/2245 train_time:26938ms step_avg:118.15ms
step:229/2245 train_time:27052ms step_avg:118.13ms
step:230/2245 train_time:27173ms step_avg:118.14ms
step:231/2245 train_time:27287ms step_avg:118.13ms
step:232/2245 train_time:27408ms step_avg:118.14ms
step:233/2245 train_time:27522ms step_avg:118.12ms
step:234/2245 train_time:27642ms step_avg:118.13ms
step:235/2245 train_time:27756ms step_avg:118.11ms
step:236/2245 train_time:27877ms step_avg:118.12ms
step:237/2245 train_time:27991ms step_avg:118.11ms
step:238/2245 train_time:28112ms step_avg:118.12ms
step:239/2245 train_time:28226ms step_avg:118.10ms
step:240/2245 train_time:28347ms step_avg:118.11ms
step:241/2245 train_time:28461ms step_avg:118.09ms
step:242/2245 train_time:28581ms step_avg:118.10ms
step:243/2245 train_time:28695ms step_avg:118.09ms
step:244/2245 train_time:28816ms step_avg:118.10ms
step:245/2245 train_time:28930ms step_avg:118.08ms
step:246/2245 train_time:29051ms step_avg:118.09ms
step:247/2245 train_time:29165ms step_avg:118.08ms
step:248/2245 train_time:29286ms step_avg:118.09ms
step:249/2245 train_time:29400ms step_avg:118.07ms
step:250/2245 train_time:29520ms step_avg:118.08ms
step:250/2245 val_loss:4.0941 train_time:29585ms step_avg:118.34ms
step:251/2245 train_time:29635ms step_avg:118.07ms
step:252/2245 train_time:29755ms step_avg:118.07ms
step:253/2245 train_time:29869ms step_avg:118.06ms
step:254/2245 train_time:29989ms step_avg:118.07ms
step:255/2245 train_time:30104ms step_avg:118.05ms
step:256/2245 train_time:30224ms step_avg:118.06ms
step:257/2245 train_time:30338ms step_avg:118.05ms
step:258/2245 train_time:30459ms step_avg:118.06ms
step:259/2245 train_time:30573ms step_avg:118.04ms
step:260/2245 train_time:30694ms step_avg:118.05ms
step:261/2245 train_time:30808ms step_avg:118.04ms
step:262/2245 train_time:30928ms step_avg:118.05ms
step:263/2245 train_time:31042ms step_avg:118.03ms
step:264/2245 train_time:31163ms step_avg:118.04ms
step:265/2245 train_time:31278ms step_avg:118.03ms
step:266/2245 train_time:31398ms step_avg:118.04ms
step:267/2245 train_time:31512ms step_avg:118.02ms
step:268/2245 train_time:31632ms step_avg:118.03ms
step:269/2245 train_time:31746ms step_avg:118.02ms
step:270/2245 train_time:31867ms step_avg:118.02ms
step:271/2245 train_time:31981ms step_avg:118.01ms
step:272/2245 train_time:32101ms step_avg:118.02ms
step:273/2245 train_time:32216ms step_avg:118.01ms
step:274/2245 train_time:32336ms step_avg:118.01ms
step:275/2245 train_time:32450ms step_avg:118.00ms
step:276/2245 train_time:32571ms step_avg:118.01ms
step:277/2245 train_time:32685ms step_avg:118.00ms
step:278/2245 train_time:32805ms step_avg:118.00ms
step:279/2245 train_time:32919ms step_avg:117.99ms
step:280/2245 train_time:33040ms step_avg:118.00ms
step:281/2245 train_time:33154ms step_avg:117.99ms
step:282/2245 train_time:33274ms step_avg:117.99ms
step:283/2245 train_time:33388ms step_avg:117.98ms
step:284/2245 train_time:33509ms step_avg:117.99ms
step:285/2245 train_time:33623ms step_avg:117.98ms
step:286/2245 train_time:33743ms step_avg:117.98ms
step:287/2245 train_time:33858ms step_avg:117.97ms
step:288/2245 train_time:33979ms step_avg:117.98ms
step:289/2245 train_time:34093ms step_avg:117.97ms
step:290/2245 train_time:34213ms step_avg:117.98ms
step:291/2245 train_time:34328ms step_avg:117.96ms
step:292/2245 train_time:34448ms step_avg:117.97ms
step:293/2245 train_time:34563ms step_avg:117.96ms
step:294/2245 train_time:34684ms step_avg:117.97ms
step:295/2245 train_time:34798ms step_avg:117.96ms
step:296/2245 train_time:34919ms step_avg:117.97ms
step:297/2245 train_time:35033ms step_avg:117.96ms
step:298/2245 train_time:35153ms step_avg:117.96ms
step:299/2245 train_time:35267ms step_avg:117.95ms
step:300/2245 train_time:35388ms step_avg:117.96ms
step:301/2245 train_time:35503ms step_avg:117.95ms
step:302/2245 train_time:35623ms step_avg:117.96ms
step:303/2245 train_time:35737ms step_avg:117.95ms
step:304/2245 train_time:35858ms step_avg:117.95ms
step:305/2245 train_time:35971ms step_avg:117.94ms
step:306/2245 train_time:36091ms step_avg:117.95ms
step:307/2245 train_time:36206ms step_avg:117.93ms
step:308/2245 train_time:36325ms step_avg:117.94ms
step:309/2245 train_time:36439ms step_avg:117.93ms
step:310/2245 train_time:36560ms step_avg:117.93ms
step:311/2245 train_time:36674ms step_avg:117.92ms
step:312/2245 train_time:36794ms step_avg:117.93ms
step:313/2245 train_time:36908ms step_avg:117.92ms
step:314/2245 train_time:37028ms step_avg:117.92ms
step:315/2245 train_time:37142ms step_avg:117.91ms
step:316/2245 train_time:37263ms step_avg:117.92ms
step:317/2245 train_time:37377ms step_avg:117.91ms
step:318/2245 train_time:37498ms step_avg:117.92ms
step:319/2245 train_time:37612ms step_avg:117.91ms
step:320/2245 train_time:37732ms step_avg:117.91ms
step:321/2245 train_time:37846ms step_avg:117.90ms
step:322/2245 train_time:37966ms step_avg:117.91ms
step:323/2245 train_time:38081ms step_avg:117.90ms
step:324/2245 train_time:38201ms step_avg:117.90ms
step:325/2245 train_time:38316ms step_avg:117.89ms
step:326/2245 train_time:38436ms step_avg:117.90ms
step:327/2245 train_time:38549ms step_avg:117.89ms
step:328/2245 train_time:38670ms step_avg:117.90ms
step:329/2245 train_time:38784ms step_avg:117.88ms
step:330/2245 train_time:38904ms step_avg:117.89ms
step:331/2245 train_time:39019ms step_avg:117.88ms
step:332/2245 train_time:39139ms step_avg:117.89ms
step:333/2245 train_time:39253ms step_avg:117.88ms
step:334/2245 train_time:39373ms step_avg:117.88ms
step:335/2245 train_time:39487ms step_avg:117.87ms
step:336/2245 train_time:39608ms step_avg:117.88ms
step:337/2245 train_time:39722ms step_avg:117.87ms
step:338/2245 train_time:39842ms step_avg:117.88ms
step:339/2245 train_time:39957ms step_avg:117.87ms
step:340/2245 train_time:40077ms step_avg:117.87ms
step:341/2245 train_time:40192ms step_avg:117.86ms
step:342/2245 train_time:40312ms step_avg:117.87ms
step:343/2245 train_time:40426ms step_avg:117.86ms
step:344/2245 train_time:40547ms step_avg:117.87ms
step:345/2245 train_time:40660ms step_avg:117.86ms
step:346/2245 train_time:40781ms step_avg:117.86ms
step:347/2245 train_time:40895ms step_avg:117.85ms
step:348/2245 train_time:41015ms step_avg:117.86ms
step:349/2245 train_time:41129ms step_avg:117.85ms
step:350/2245 train_time:41249ms step_avg:117.85ms
step:351/2245 train_time:41363ms step_avg:117.84ms
step:352/2245 train_time:41484ms step_avg:117.85ms
step:353/2245 train_time:41598ms step_avg:117.84ms
step:354/2245 train_time:41719ms step_avg:117.85ms
step:355/2245 train_time:41833ms step_avg:117.84ms
step:356/2245 train_time:41953ms step_avg:117.85ms
step:357/2245 train_time:42067ms step_avg:117.83ms
step:358/2245 train_time:42187ms step_avg:117.84ms
step:359/2245 train_time:42302ms step_avg:117.83ms
step:360/2245 train_time:42422ms step_avg:117.84ms
step:361/2245 train_time:42537ms step_avg:117.83ms
step:362/2245 train_time:42657ms step_avg:117.84ms
step:363/2245 train_time:42771ms step_avg:117.83ms
step:364/2245 train_time:42892ms step_avg:117.83ms
step:365/2245 train_time:43006ms step_avg:117.82ms
step:366/2245 train_time:43126ms step_avg:117.83ms
step:367/2245 train_time:43240ms step_avg:117.82ms
step:368/2245 train_time:43360ms step_avg:117.83ms
step:369/2245 train_time:43475ms step_avg:117.82ms
step:370/2245 train_time:43595ms step_avg:117.82ms
step:371/2245 train_time:43709ms step_avg:117.81ms
step:372/2245 train_time:43829ms step_avg:117.82ms
step:373/2245 train_time:43943ms step_avg:117.81ms
step:374/2245 train_time:44064ms step_avg:117.82ms
step:375/2245 train_time:44178ms step_avg:117.81ms
step:376/2245 train_time:44299ms step_avg:117.82ms
step:377/2245 train_time:44413ms step_avg:117.81ms
step:378/2245 train_time:44534ms step_avg:117.81ms
step:379/2245 train_time:44647ms step_avg:117.80ms
step:380/2245 train_time:44767ms step_avg:117.81ms
step:381/2245 train_time:44882ms step_avg:117.80ms
step:382/2245 train_time:45002ms step_avg:117.81ms
step:383/2245 train_time:45116ms step_avg:117.80ms
step:384/2245 train_time:45237ms step_avg:117.80ms
step:385/2245 train_time:45351ms step_avg:117.79ms
step:386/2245 train_time:45471ms step_avg:117.80ms
step:387/2245 train_time:45585ms step_avg:117.79ms
step:388/2245 train_time:45705ms step_avg:117.80ms
step:389/2245 train_time:45819ms step_avg:117.79ms
step:390/2245 train_time:45939ms step_avg:117.79ms
step:391/2245 train_time:46054ms step_avg:117.78ms
step:392/2245 train_time:46174ms step_avg:117.79ms
step:393/2245 train_time:46288ms step_avg:117.78ms
step:394/2245 train_time:46408ms step_avg:117.79ms
step:395/2245 train_time:46522ms step_avg:117.78ms
step:396/2245 train_time:46643ms step_avg:117.78ms
step:397/2245 train_time:46757ms step_avg:117.77ms
step:398/2245 train_time:46877ms step_avg:117.78ms
step:399/2245 train_time:46991ms step_avg:117.77ms
step:400/2245 train_time:47111ms step_avg:117.78ms
step:401/2245 train_time:47225ms step_avg:117.77ms
step:402/2245 train_time:47345ms step_avg:117.77ms
step:403/2245 train_time:47459ms step_avg:117.76ms
step:404/2245 train_time:47579ms step_avg:117.77ms
step:405/2245 train_time:47694ms step_avg:117.76ms
step:406/2245 train_time:47814ms step_avg:117.77ms
step:407/2245 train_time:47928ms step_avg:117.76ms
step:408/2245 train_time:48048ms step_avg:117.76ms
step:409/2245 train_time:48162ms step_avg:117.76ms
step:410/2245 train_time:48282ms step_avg:117.76ms
step:411/2245 train_time:48397ms step_avg:117.75ms
step:412/2245 train_time:48517ms step_avg:117.76ms
step:413/2245 train_time:48631ms step_avg:117.75ms
step:414/2245 train_time:48752ms step_avg:117.76ms
step:415/2245 train_time:48866ms step_avg:117.75ms
step:416/2245 train_time:48987ms step_avg:117.76ms
step:417/2245 train_time:49101ms step_avg:117.75ms
step:418/2245 train_time:49221ms step_avg:117.75ms
step:419/2245 train_time:49336ms step_avg:117.75ms
step:420/2245 train_time:49456ms step_avg:117.75ms
step:421/2245 train_time:49570ms step_avg:117.74ms
step:422/2245 train_time:49690ms step_avg:117.75ms
step:423/2245 train_time:49804ms step_avg:117.74ms
step:424/2245 train_time:49925ms step_avg:117.75ms
step:425/2245 train_time:50039ms step_avg:117.74ms
step:426/2245 train_time:50160ms step_avg:117.75ms
step:427/2245 train_time:50274ms step_avg:117.74ms
step:428/2245 train_time:50395ms step_avg:117.75ms
step:429/2245 train_time:50508ms step_avg:117.74ms
step:430/2245 train_time:50629ms step_avg:117.74ms
step:431/2245 train_time:50743ms step_avg:117.73ms
step:432/2245 train_time:50864ms step_avg:117.74ms
step:433/2245 train_time:50978ms step_avg:117.73ms
step:434/2245 train_time:51098ms step_avg:117.74ms
step:435/2245 train_time:51213ms step_avg:117.73ms
step:436/2245 train_time:51333ms step_avg:117.74ms
step:437/2245 train_time:51447ms step_avg:117.73ms
step:438/2245 train_time:51568ms step_avg:117.73ms
step:439/2245 train_time:51682ms step_avg:117.73ms
step:440/2245 train_time:51803ms step_avg:117.73ms
step:441/2245 train_time:51917ms step_avg:117.73ms
step:442/2245 train_time:52037ms step_avg:117.73ms
step:443/2245 train_time:52151ms step_avg:117.72ms
step:444/2245 train_time:52271ms step_avg:117.73ms
step:445/2245 train_time:52385ms step_avg:117.72ms
step:446/2245 train_time:52506ms step_avg:117.73ms
step:447/2245 train_time:52620ms step_avg:117.72ms
step:448/2245 train_time:52740ms step_avg:117.72ms
step:449/2245 train_time:52854ms step_avg:117.72ms
step:450/2245 train_time:52975ms step_avg:117.72ms
step:451/2245 train_time:53088ms step_avg:117.71ms
step:452/2245 train_time:53209ms step_avg:117.72ms
step:453/2245 train_time:53323ms step_avg:117.71ms
step:454/2245 train_time:53443ms step_avg:117.72ms
step:455/2245 train_time:53558ms step_avg:117.71ms
step:456/2245 train_time:53678ms step_avg:117.72ms
step:457/2245 train_time:53792ms step_avg:117.71ms
step:458/2245 train_time:53912ms step_avg:117.71ms
step:459/2245 train_time:54026ms step_avg:117.70ms
step:460/2245 train_time:54146ms step_avg:117.71ms
step:461/2245 train_time:54260ms step_avg:117.70ms
step:462/2245 train_time:54381ms step_avg:117.71ms
step:463/2245 train_time:54495ms step_avg:117.70ms
step:464/2245 train_time:54616ms step_avg:117.71ms
step:465/2245 train_time:54730ms step_avg:117.70ms
step:466/2245 train_time:54850ms step_avg:117.70ms
step:467/2245 train_time:54964ms step_avg:117.70ms
step:468/2245 train_time:55085ms step_avg:117.70ms
step:469/2245 train_time:55198ms step_avg:117.69ms
step:470/2245 train_time:55319ms step_avg:117.70ms
step:471/2245 train_time:55433ms step_avg:117.69ms
step:472/2245 train_time:55553ms step_avg:117.70ms
step:473/2245 train_time:55667ms step_avg:117.69ms
step:474/2245 train_time:55788ms step_avg:117.70ms
step:475/2245 train_time:55902ms step_avg:117.69ms
step:476/2245 train_time:56023ms step_avg:117.69ms
step:477/2245 train_time:56137ms step_avg:117.69ms
step:478/2245 train_time:56257ms step_avg:117.69ms
step:479/2245 train_time:56371ms step_avg:117.68ms
step:480/2245 train_time:56491ms step_avg:117.69ms
step:481/2245 train_time:56605ms step_avg:117.68ms
step:482/2245 train_time:56726ms step_avg:117.69ms
step:483/2245 train_time:56840ms step_avg:117.68ms
step:484/2245 train_time:56960ms step_avg:117.69ms
step:485/2245 train_time:57074ms step_avg:117.68ms
step:486/2245 train_time:57195ms step_avg:117.68ms
step:487/2245 train_time:57309ms step_avg:117.68ms
step:488/2245 train_time:57429ms step_avg:117.68ms
step:489/2245 train_time:57543ms step_avg:117.68ms
step:490/2245 train_time:57664ms step_avg:117.68ms
step:491/2245 train_time:57778ms step_avg:117.67ms
step:492/2245 train_time:57898ms step_avg:117.68ms
step:493/2245 train_time:58012ms step_avg:117.67ms
step:494/2245 train_time:58132ms step_avg:117.68ms
step:495/2245 train_time:58246ms step_avg:117.67ms
step:496/2245 train_time:58366ms step_avg:117.67ms
step:497/2245 train_time:58481ms step_avg:117.67ms
step:498/2245 train_time:58601ms step_avg:117.67ms
step:499/2245 train_time:58715ms step_avg:117.67ms
step:500/2245 train_time:58835ms step_avg:117.67ms
step:500/2245 val_loss:3.8286 train_time:58901ms step_avg:117.80ms
step:501/2245 train_time:58951ms step_avg:117.67ms
step:502/2245 train_time:59070ms step_avg:117.67ms
step:503/2245 train_time:59184ms step_avg:117.66ms
step:504/2245 train_time:59304ms step_avg:117.67ms
step:505/2245 train_time:59418ms step_avg:117.66ms
step:506/2245 train_time:59538ms step_avg:117.66ms
step:507/2245 train_time:59653ms step_avg:117.66ms
step:508/2245 train_time:59773ms step_avg:117.66ms
step:509/2245 train_time:59887ms step_avg:117.66ms
step:510/2245 train_time:60008ms step_avg:117.66ms
step:511/2245 train_time:60122ms step_avg:117.66ms
step:512/2245 train_time:60242ms step_avg:117.66ms
step:513/2245 train_time:60356ms step_avg:117.65ms
step:514/2245 train_time:60476ms step_avg:117.66ms
step:515/2245 train_time:60591ms step_avg:117.65ms
step:516/2245 train_time:60712ms step_avg:117.66ms
step:517/2245 train_time:60826ms step_avg:117.65ms
step:518/2245 train_time:60946ms step_avg:117.66ms
step:519/2245 train_time:61060ms step_avg:117.65ms
step:520/2245 train_time:61181ms step_avg:117.66ms
step:521/2245 train_time:61295ms step_avg:117.65ms
step:522/2245 train_time:61416ms step_avg:117.65ms
step:523/2245 train_time:61530ms step_avg:117.65ms
step:524/2245 train_time:61650ms step_avg:117.65ms
step:525/2245 train_time:61764ms step_avg:117.65ms
step:526/2245 train_time:61885ms step_avg:117.65ms
step:527/2245 train_time:61998ms step_avg:117.64ms
step:528/2245 train_time:62118ms step_avg:117.65ms
step:529/2245 train_time:62233ms step_avg:117.64ms
step:530/2245 train_time:62353ms step_avg:117.65ms
step:531/2245 train_time:62467ms step_avg:117.64ms
step:532/2245 train_time:62587ms step_avg:117.65ms
step:533/2245 train_time:62702ms step_avg:117.64ms
step:534/2245 train_time:62822ms step_avg:117.64ms
step:535/2245 train_time:62936ms step_avg:117.64ms
step:536/2245 train_time:63056ms step_avg:117.64ms
step:537/2245 train_time:63170ms step_avg:117.64ms
step:538/2245 train_time:63291ms step_avg:117.64ms
step:539/2245 train_time:63405ms step_avg:117.63ms
step:540/2245 train_time:63525ms step_avg:117.64ms
step:541/2245 train_time:63639ms step_avg:117.63ms
step:542/2245 train_time:63759ms step_avg:117.64ms
step:543/2245 train_time:63874ms step_avg:117.63ms
step:544/2245 train_time:63994ms step_avg:117.64ms
step:545/2245 train_time:64108ms step_avg:117.63ms
step:546/2245 train_time:64229ms step_avg:117.63ms
step:547/2245 train_time:64343ms step_avg:117.63ms
step:548/2245 train_time:64463ms step_avg:117.63ms
step:549/2245 train_time:64578ms step_avg:117.63ms
step:550/2245 train_time:64698ms step_avg:117.63ms
step:551/2245 train_time:64812ms step_avg:117.63ms
step:552/2245 train_time:64932ms step_avg:117.63ms
step:553/2245 train_time:65047ms step_avg:117.62ms
step:554/2245 train_time:65167ms step_avg:117.63ms
step:555/2245 train_time:65281ms step_avg:117.62ms
step:556/2245 train_time:65401ms step_avg:117.63ms
step:557/2245 train_time:65515ms step_avg:117.62ms
step:558/2245 train_time:65636ms step_avg:117.63ms
step:559/2245 train_time:65750ms step_avg:117.62ms
step:560/2245 train_time:65871ms step_avg:117.63ms
step:561/2245 train_time:65985ms step_avg:117.62ms
step:562/2245 train_time:66106ms step_avg:117.63ms
step:563/2245 train_time:66219ms step_avg:117.62ms
step:564/2245 train_time:66340ms step_avg:117.62ms
step:565/2245 train_time:66454ms step_avg:117.62ms
step:566/2245 train_time:66575ms step_avg:117.62ms
step:567/2245 train_time:66689ms step_avg:117.62ms
step:568/2245 train_time:66810ms step_avg:117.62ms
step:569/2245 train_time:66924ms step_avg:117.62ms
step:570/2245 train_time:67044ms step_avg:117.62ms
step:571/2245 train_time:67158ms step_avg:117.61ms
step:572/2245 train_time:67278ms step_avg:117.62ms
step:573/2245 train_time:67392ms step_avg:117.61ms
step:574/2245 train_time:67513ms step_avg:117.62ms
step:575/2245 train_time:67627ms step_avg:117.61ms
step:576/2245 train_time:67747ms step_avg:117.62ms
step:577/2245 train_time:67861ms step_avg:117.61ms
step:578/2245 train_time:67982ms step_avg:117.62ms
step:579/2245 train_time:68096ms step_avg:117.61ms
step:580/2245 train_time:68216ms step_avg:117.61ms
step:581/2245 train_time:68330ms step_avg:117.61ms
step:582/2245 train_time:68451ms step_avg:117.61ms
step:583/2245 train_time:68565ms step_avg:117.61ms
step:584/2245 train_time:68686ms step_avg:117.61ms
step:585/2245 train_time:68800ms step_avg:117.61ms
step:586/2245 train_time:68920ms step_avg:117.61ms
step:587/2245 train_time:69034ms step_avg:117.61ms
step:588/2245 train_time:69155ms step_avg:117.61ms
step:589/2245 train_time:69269ms step_avg:117.60ms
step:590/2245 train_time:69389ms step_avg:117.61ms
step:591/2245 train_time:69504ms step_avg:117.60ms
step:592/2245 train_time:69624ms step_avg:117.61ms
step:593/2245 train_time:69738ms step_avg:117.60ms
step:594/2245 train_time:69858ms step_avg:117.61ms
step:595/2245 train_time:69972ms step_avg:117.60ms
step:596/2245 train_time:70093ms step_avg:117.61ms
step:597/2245 train_time:70207ms step_avg:117.60ms
step:598/2245 train_time:70327ms step_avg:117.60ms
step:599/2245 train_time:70441ms step_avg:117.60ms
step:600/2245 train_time:70561ms step_avg:117.60ms
step:601/2245 train_time:70676ms step_avg:117.60ms
step:602/2245 train_time:70796ms step_avg:117.60ms
step:603/2245 train_time:70910ms step_avg:117.60ms
step:604/2245 train_time:71031ms step_avg:117.60ms
step:605/2245 train_time:71145ms step_avg:117.59ms
step:606/2245 train_time:71266ms step_avg:117.60ms
step:607/2245 train_time:71380ms step_avg:117.59ms
step:608/2245 train_time:71500ms step_avg:117.60ms
step:609/2245 train_time:71614ms step_avg:117.59ms
step:610/2245 train_time:71735ms step_avg:117.60ms
step:611/2245 train_time:71849ms step_avg:117.59ms
step:612/2245 train_time:71969ms step_avg:117.60ms
step:613/2245 train_time:72083ms step_avg:117.59ms
step:614/2245 train_time:72204ms step_avg:117.60ms
step:615/2245 train_time:72318ms step_avg:117.59ms
step:616/2245 train_time:72439ms step_avg:117.60ms
step:617/2245 train_time:72553ms step_avg:117.59ms
step:618/2245 train_time:72674ms step_avg:117.59ms
step:619/2245 train_time:72788ms step_avg:117.59ms
step:620/2245 train_time:72908ms step_avg:117.59ms
step:621/2245 train_time:73022ms step_avg:117.59ms
step:622/2245 train_time:73142ms step_avg:117.59ms
step:623/2245 train_time:73256ms step_avg:117.59ms
step:624/2245 train_time:73377ms step_avg:117.59ms
step:625/2245 train_time:73491ms step_avg:117.59ms
step:626/2245 train_time:73611ms step_avg:117.59ms
step:627/2245 train_time:73725ms step_avg:117.58ms
step:628/2245 train_time:73846ms step_avg:117.59ms
step:629/2245 train_time:73960ms step_avg:117.58ms
step:630/2245 train_time:74080ms step_avg:117.59ms
step:631/2245 train_time:74194ms step_avg:117.58ms
step:632/2245 train_time:74315ms step_avg:117.59ms
step:633/2245 train_time:74428ms step_avg:117.58ms
step:634/2245 train_time:74549ms step_avg:117.58ms
step:635/2245 train_time:74663ms step_avg:117.58ms
step:636/2245 train_time:74784ms step_avg:117.58ms
step:637/2245 train_time:74898ms step_avg:117.58ms
step:638/2245 train_time:75018ms step_avg:117.58ms
step:639/2245 train_time:75133ms step_avg:117.58ms
step:640/2245 train_time:75253ms step_avg:117.58ms
step:641/2245 train_time:75367ms step_avg:117.58ms
step:642/2245 train_time:75488ms step_avg:117.58ms
step:643/2245 train_time:75602ms step_avg:117.58ms
step:644/2245 train_time:75722ms step_avg:117.58ms
step:645/2245 train_time:75837ms step_avg:117.58ms
step:646/2245 train_time:75957ms step_avg:117.58ms
step:647/2245 train_time:76071ms step_avg:117.58ms
step:648/2245 train_time:76192ms step_avg:117.58ms
step:649/2245 train_time:76306ms step_avg:117.57ms
step:650/2245 train_time:76426ms step_avg:117.58ms
step:651/2245 train_time:76540ms step_avg:117.57ms
step:652/2245 train_time:76661ms step_avg:117.58ms
step:653/2245 train_time:76775ms step_avg:117.57ms
step:654/2245 train_time:76895ms step_avg:117.58ms
step:655/2245 train_time:77009ms step_avg:117.57ms
step:656/2245 train_time:77130ms step_avg:117.58ms
step:657/2245 train_time:77244ms step_avg:117.57ms
step:658/2245 train_time:77365ms step_avg:117.58ms
step:659/2245 train_time:77479ms step_avg:117.57ms
step:660/2245 train_time:77599ms step_avg:117.57ms
step:661/2245 train_time:77713ms step_avg:117.57ms
step:662/2245 train_time:77834ms step_avg:117.57ms
step:663/2245 train_time:77948ms step_avg:117.57ms
step:664/2245 train_time:78069ms step_avg:117.57ms
step:665/2245 train_time:78183ms step_avg:117.57ms
step:666/2245 train_time:78304ms step_avg:117.57ms
step:667/2245 train_time:78418ms step_avg:117.57ms
step:668/2245 train_time:78538ms step_avg:117.57ms
step:669/2245 train_time:78652ms step_avg:117.57ms
step:670/2245 train_time:78773ms step_avg:117.57ms
step:671/2245 train_time:78887ms step_avg:117.57ms
step:672/2245 train_time:79008ms step_avg:117.57ms
step:673/2245 train_time:79122ms step_avg:117.57ms
step:674/2245 train_time:79242ms step_avg:117.57ms
step:675/2245 train_time:79357ms step_avg:117.57ms
step:676/2245 train_time:79477ms step_avg:117.57ms
step:677/2245 train_time:79591ms step_avg:117.56ms
step:678/2245 train_time:79712ms step_avg:117.57ms
step:679/2245 train_time:79826ms step_avg:117.56ms
step:680/2245 train_time:79947ms step_avg:117.57ms
step:681/2245 train_time:80061ms step_avg:117.56ms
step:682/2245 train_time:80181ms step_avg:117.57ms
step:683/2245 train_time:80295ms step_avg:117.56ms
step:684/2245 train_time:80416ms step_avg:117.57ms
step:685/2245 train_time:80529ms step_avg:117.56ms
step:686/2245 train_time:80650ms step_avg:117.57ms
step:687/2245 train_time:80764ms step_avg:117.56ms
step:688/2245 train_time:80884ms step_avg:117.56ms
step:689/2245 train_time:80998ms step_avg:117.56ms
step:690/2245 train_time:81119ms step_avg:117.56ms
step:691/2245 train_time:81233ms step_avg:117.56ms
step:692/2245 train_time:81353ms step_avg:117.56ms
step:693/2245 train_time:81467ms step_avg:117.56ms
step:694/2245 train_time:81588ms step_avg:117.56ms
step:695/2245 train_time:81702ms step_avg:117.56ms
step:696/2245 train_time:81823ms step_avg:117.56ms
step:697/2245 train_time:81937ms step_avg:117.56ms
step:698/2245 train_time:82057ms step_avg:117.56ms
step:699/2245 train_time:82171ms step_avg:117.56ms
step:700/2245 train_time:82291ms step_avg:117.56ms
step:701/2245 train_time:82406ms step_avg:117.55ms
step:702/2245 train_time:82526ms step_avg:117.56ms
step:703/2245 train_time:82640ms step_avg:117.55ms
step:704/2245 train_time:82760ms step_avg:117.56ms
step:705/2245 train_time:82874ms step_avg:117.55ms
step:706/2245 train_time:82995ms step_avg:117.56ms
step:707/2245 train_time:83109ms step_avg:117.55ms
step:708/2245 train_time:83230ms step_avg:117.56ms
step:709/2245 train_time:83344ms step_avg:117.55ms
step:710/2245 train_time:83464ms step_avg:117.55ms
step:711/2245 train_time:83578ms step_avg:117.55ms
step:712/2245 train_time:83698ms step_avg:117.55ms
step:713/2245 train_time:83813ms step_avg:117.55ms
step:714/2245 train_time:83933ms step_avg:117.55ms
step:715/2245 train_time:84047ms step_avg:117.55ms
step:716/2245 train_time:84167ms step_avg:117.55ms
step:717/2245 train_time:84282ms step_avg:117.55ms
step:718/2245 train_time:84402ms step_avg:117.55ms
step:719/2245 train_time:84516ms step_avg:117.55ms
step:720/2245 train_time:84637ms step_avg:117.55ms
step:721/2245 train_time:84751ms step_avg:117.55ms
step:722/2245 train_time:84872ms step_avg:117.55ms
step:723/2245 train_time:84985ms step_avg:117.55ms
step:724/2245 train_time:85106ms step_avg:117.55ms
step:725/2245 train_time:85220ms step_avg:117.55ms
step:726/2245 train_time:85340ms step_avg:117.55ms
step:727/2245 train_time:85455ms step_avg:117.54ms
step:728/2245 train_time:85575ms step_avg:117.55ms
step:729/2245 train_time:85690ms step_avg:117.54ms
step:730/2245 train_time:85810ms step_avg:117.55ms
step:731/2245 train_time:85924ms step_avg:117.54ms
step:732/2245 train_time:86045ms step_avg:117.55ms
step:733/2245 train_time:86159ms step_avg:117.54ms
step:734/2245 train_time:86279ms step_avg:117.55ms
step:735/2245 train_time:86393ms step_avg:117.54ms
step:736/2245 train_time:86515ms step_avg:117.55ms
step:737/2245 train_time:86630ms step_avg:117.54ms
step:738/2245 train_time:86752ms step_avg:117.55ms
step:739/2245 train_time:86868ms step_avg:117.55ms
step:740/2245 train_time:86990ms step_avg:117.55ms
step:741/2245 train_time:87105ms step_avg:117.55ms
step:742/2245 train_time:87227ms step_avg:117.56ms
step:743/2245 train_time:87342ms step_avg:117.55ms
step:744/2245 train_time:87465ms step_avg:117.56ms
step:745/2245 train_time:87580ms step_avg:117.56ms
step:746/2245 train_time:87703ms step_avg:117.56ms
step:747/2245 train_time:87819ms step_avg:117.56ms
step:748/2245 train_time:87940ms step_avg:117.57ms
step:749/2245 train_time:88056ms step_avg:117.56ms
step:750/2245 train_time:88178ms step_avg:117.57ms
step:750/2245 val_loss:3.6733 train_time:88244ms step_avg:117.66ms
step:751/2245 train_time:88294ms step_avg:117.57ms
step:752/2245 train_time:88415ms step_avg:117.57ms
step:753/2245 train_time:88530ms step_avg:117.57ms
step:754/2245 train_time:88652ms step_avg:117.58ms
step:755/2245 train_time:88767ms step_avg:117.57ms
step:756/2245 train_time:88889ms step_avg:117.58ms
step:757/2245 train_time:89004ms step_avg:117.57ms
step:758/2245 train_time:89126ms step_avg:117.58ms
step:759/2245 train_time:89241ms step_avg:117.58ms
step:760/2245 train_time:89363ms step_avg:117.58ms
step:761/2245 train_time:89479ms step_avg:117.58ms
step:762/2245 train_time:89601ms step_avg:117.59ms
step:763/2245 train_time:89716ms step_avg:117.58ms
step:764/2245 train_time:89838ms step_avg:117.59ms
step:765/2245 train_time:89954ms step_avg:117.59ms
step:766/2245 train_time:90077ms step_avg:117.59ms
step:767/2245 train_time:90192ms step_avg:117.59ms
step:768/2245 train_time:90314ms step_avg:117.60ms
step:769/2245 train_time:90429ms step_avg:117.59ms
step:770/2245 train_time:90551ms step_avg:117.60ms
step:771/2245 train_time:90667ms step_avg:117.60ms
step:772/2245 train_time:90789ms step_avg:117.60ms
step:773/2245 train_time:90905ms step_avg:117.60ms
step:774/2245 train_time:91026ms step_avg:117.60ms
step:775/2245 train_time:91142ms step_avg:117.60ms
step:776/2245 train_time:91264ms step_avg:117.61ms
step:777/2245 train_time:91379ms step_avg:117.61ms
step:778/2245 train_time:91501ms step_avg:117.61ms
step:779/2245 train_time:91617ms step_avg:117.61ms
step:780/2245 train_time:91739ms step_avg:117.61ms
step:781/2245 train_time:91855ms step_avg:117.61ms
step:782/2245 train_time:91977ms step_avg:117.62ms
step:783/2245 train_time:92093ms step_avg:117.62ms
step:784/2245 train_time:92214ms step_avg:117.62ms
step:785/2245 train_time:92330ms step_avg:117.62ms
step:786/2245 train_time:92452ms step_avg:117.62ms
step:787/2245 train_time:92567ms step_avg:117.62ms
step:788/2245 train_time:92689ms step_avg:117.63ms
step:789/2245 train_time:92805ms step_avg:117.62ms
step:790/2245 train_time:92926ms step_avg:117.63ms
step:791/2245 train_time:93042ms step_avg:117.63ms
step:792/2245 train_time:93164ms step_avg:117.63ms
step:793/2245 train_time:93280ms step_avg:117.63ms
step:794/2245 train_time:93402ms step_avg:117.63ms
step:795/2245 train_time:93518ms step_avg:117.63ms
step:796/2245 train_time:93640ms step_avg:117.64ms
step:797/2245 train_time:93757ms step_avg:117.64ms
step:798/2245 train_time:93879ms step_avg:117.64ms
step:799/2245 train_time:93995ms step_avg:117.64ms
step:800/2245 train_time:94117ms step_avg:117.65ms
step:801/2245 train_time:94233ms step_avg:117.64ms
step:802/2245 train_time:94355ms step_avg:117.65ms
step:803/2245 train_time:94470ms step_avg:117.65ms
step:804/2245 train_time:94592ms step_avg:117.65ms
step:805/2245 train_time:94707ms step_avg:117.65ms
step:806/2245 train_time:94829ms step_avg:117.65ms
step:807/2245 train_time:94945ms step_avg:117.65ms
step:808/2245 train_time:95067ms step_avg:117.66ms
step:809/2245 train_time:95184ms step_avg:117.66ms
step:810/2245 train_time:95306ms step_avg:117.66ms
step:811/2245 train_time:95421ms step_avg:117.66ms
step:812/2245 train_time:95543ms step_avg:117.66ms
step:813/2245 train_time:95659ms step_avg:117.66ms
step:814/2245 train_time:95781ms step_avg:117.67ms
step:815/2245 train_time:95897ms step_avg:117.66ms
step:816/2245 train_time:96019ms step_avg:117.67ms
step:817/2245 train_time:96135ms step_avg:117.67ms
step:818/2245 train_time:96257ms step_avg:117.67ms
step:819/2245 train_time:96372ms step_avg:117.67ms
step:820/2245 train_time:96494ms step_avg:117.68ms
step:821/2245 train_time:96610ms step_avg:117.67ms
step:822/2245 train_time:96732ms step_avg:117.68ms
step:823/2245 train_time:96847ms step_avg:117.68ms
step:824/2245 train_time:96969ms step_avg:117.68ms
step:825/2245 train_time:97084ms step_avg:117.68ms
step:826/2245 train_time:97207ms step_avg:117.68ms
step:827/2245 train_time:97322ms step_avg:117.68ms
step:828/2245 train_time:97445ms step_avg:117.69ms
step:829/2245 train_time:97560ms step_avg:117.68ms
step:830/2245 train_time:97683ms step_avg:117.69ms
step:831/2245 train_time:97798ms step_avg:117.69ms
step:832/2245 train_time:97920ms step_avg:117.69ms
step:833/2245 train_time:98036ms step_avg:117.69ms
step:834/2245 train_time:98158ms step_avg:117.70ms
step:835/2245 train_time:98274ms step_avg:117.69ms
step:836/2245 train_time:98395ms step_avg:117.70ms
step:837/2245 train_time:98511ms step_avg:117.70ms
step:838/2245 train_time:98633ms step_avg:117.70ms
step:839/2245 train_time:98749ms step_avg:117.70ms
step:840/2245 train_time:98870ms step_avg:117.70ms
step:841/2245 train_time:98986ms step_avg:117.70ms
step:842/2245 train_time:99108ms step_avg:117.70ms
step:843/2245 train_time:99224ms step_avg:117.70ms
step:844/2245 train_time:99345ms step_avg:117.71ms
step:845/2245 train_time:99461ms step_avg:117.71ms
step:846/2245 train_time:99584ms step_avg:117.71ms
step:847/2245 train_time:99700ms step_avg:117.71ms
step:848/2245 train_time:99822ms step_avg:117.71ms
step:849/2245 train_time:99938ms step_avg:117.71ms
step:850/2245 train_time:100060ms step_avg:117.72ms
step:851/2245 train_time:100176ms step_avg:117.72ms
step:852/2245 train_time:100298ms step_avg:117.72ms
step:853/2245 train_time:100414ms step_avg:117.72ms
step:854/2245 train_time:100536ms step_avg:117.72ms
step:855/2245 train_time:100652ms step_avg:117.72ms
step:856/2245 train_time:100773ms step_avg:117.73ms
step:857/2245 train_time:100888ms step_avg:117.72ms
step:858/2245 train_time:101011ms step_avg:117.73ms
step:859/2245 train_time:101126ms step_avg:117.73ms
step:860/2245 train_time:101248ms step_avg:117.73ms
step:861/2245 train_time:101364ms step_avg:117.73ms
step:862/2245 train_time:101485ms step_avg:117.73ms
step:863/2245 train_time:101601ms step_avg:117.73ms
step:864/2245 train_time:101723ms step_avg:117.73ms
step:865/2245 train_time:101838ms step_avg:117.73ms
step:866/2245 train_time:101961ms step_avg:117.74ms
step:867/2245 train_time:102077ms step_avg:117.74ms
step:868/2245 train_time:102199ms step_avg:117.74ms
step:869/2245 train_time:102315ms step_avg:117.74ms
step:870/2245 train_time:102437ms step_avg:117.74ms
step:871/2245 train_time:102552ms step_avg:117.74ms
step:872/2245 train_time:102674ms step_avg:117.74ms
step:873/2245 train_time:102789ms step_avg:117.74ms
step:874/2245 train_time:102911ms step_avg:117.75ms
step:875/2245 train_time:103027ms step_avg:117.74ms
step:876/2245 train_time:103149ms step_avg:117.75ms
step:877/2245 train_time:103265ms step_avg:117.75ms
step:878/2245 train_time:103387ms step_avg:117.75ms
step:879/2245 train_time:103502ms step_avg:117.75ms
step:880/2245 train_time:103624ms step_avg:117.76ms
step:881/2245 train_time:103741ms step_avg:117.75ms
step:882/2245 train_time:103862ms step_avg:117.76ms
step:883/2245 train_time:103978ms step_avg:117.75ms
step:884/2245 train_time:104100ms step_avg:117.76ms
step:885/2245 train_time:104216ms step_avg:117.76ms
step:886/2245 train_time:104338ms step_avg:117.76ms
step:887/2245 train_time:104454ms step_avg:117.76ms
step:888/2245 train_time:104576ms step_avg:117.77ms
step:889/2245 train_time:104691ms step_avg:117.76ms
step:890/2245 train_time:104813ms step_avg:117.77ms
step:891/2245 train_time:104929ms step_avg:117.76ms
step:892/2245 train_time:105050ms step_avg:117.77ms
step:893/2245 train_time:105166ms step_avg:117.77ms
step:894/2245 train_time:105288ms step_avg:117.77ms
step:895/2245 train_time:105404ms step_avg:117.77ms
step:896/2245 train_time:105527ms step_avg:117.78ms
step:897/2245 train_time:105642ms step_avg:117.77ms
step:898/2245 train_time:105764ms step_avg:117.78ms
step:899/2245 train_time:105880ms step_avg:117.78ms
step:900/2245 train_time:106002ms step_avg:117.78ms
step:901/2245 train_time:106118ms step_avg:117.78ms
step:902/2245 train_time:106240ms step_avg:117.78ms
step:903/2245 train_time:106357ms step_avg:117.78ms
step:904/2245 train_time:106479ms step_avg:117.79ms
step:905/2245 train_time:106594ms step_avg:117.78ms
step:906/2245 train_time:106716ms step_avg:117.79ms
step:907/2245 train_time:106832ms step_avg:117.79ms
step:908/2245 train_time:106953ms step_avg:117.79ms
step:909/2245 train_time:107069ms step_avg:117.79ms
step:910/2245 train_time:107191ms step_avg:117.79ms
step:911/2245 train_time:107307ms step_avg:117.79ms
step:912/2245 train_time:107429ms step_avg:117.79ms
step:913/2245 train_time:107544ms step_avg:117.79ms
step:914/2245 train_time:107666ms step_avg:117.80ms
step:915/2245 train_time:107782ms step_avg:117.79ms
step:916/2245 train_time:107903ms step_avg:117.80ms
step:917/2245 train_time:108019ms step_avg:117.80ms
step:918/2245 train_time:108141ms step_avg:117.80ms
step:919/2245 train_time:108257ms step_avg:117.80ms
step:920/2245 train_time:108380ms step_avg:117.80ms
step:921/2245 train_time:108496ms step_avg:117.80ms
step:922/2245 train_time:108618ms step_avg:117.81ms
step:923/2245 train_time:108734ms step_avg:117.80ms
step:924/2245 train_time:108855ms step_avg:117.81ms
step:925/2245 train_time:108971ms step_avg:117.81ms
step:926/2245 train_time:109093ms step_avg:117.81ms
step:927/2245 train_time:109208ms step_avg:117.81ms
step:928/2245 train_time:109330ms step_avg:117.81ms
step:929/2245 train_time:109446ms step_avg:117.81ms
step:930/2245 train_time:109568ms step_avg:117.81ms
step:931/2245 train_time:109683ms step_avg:117.81ms
step:932/2245 train_time:109805ms step_avg:117.82ms
step:933/2245 train_time:109920ms step_avg:117.81ms
step:934/2245 train_time:110042ms step_avg:117.82ms
step:935/2245 train_time:110157ms step_avg:117.82ms
step:936/2245 train_time:110279ms step_avg:117.82ms
step:937/2245 train_time:110395ms step_avg:117.82ms
step:938/2245 train_time:110517ms step_avg:117.82ms
step:939/2245 train_time:110632ms step_avg:117.82ms
step:940/2245 train_time:110754ms step_avg:117.82ms
step:941/2245 train_time:110869ms step_avg:117.82ms
step:942/2245 train_time:110992ms step_avg:117.83ms
step:943/2245 train_time:111107ms step_avg:117.82ms
step:944/2245 train_time:111229ms step_avg:117.83ms
step:945/2245 train_time:111345ms step_avg:117.83ms
step:946/2245 train_time:111467ms step_avg:117.83ms
step:947/2245 train_time:111583ms step_avg:117.83ms
step:948/2245 train_time:111705ms step_avg:117.83ms
step:949/2245 train_time:111821ms step_avg:117.83ms
step:950/2245 train_time:111943ms step_avg:117.83ms
step:951/2245 train_time:112059ms step_avg:117.83ms
step:952/2245 train_time:112181ms step_avg:117.84ms
step:953/2245 train_time:112297ms step_avg:117.83ms
step:954/2245 train_time:112419ms step_avg:117.84ms
step:955/2245 train_time:112535ms step_avg:117.84ms
step:956/2245 train_time:112657ms step_avg:117.84ms
step:957/2245 train_time:112773ms step_avg:117.84ms
step:958/2245 train_time:112895ms step_avg:117.84ms
step:959/2245 train_time:113011ms step_avg:117.84ms
step:960/2245 train_time:113133ms step_avg:117.85ms
step:961/2245 train_time:113248ms step_avg:117.84ms
step:962/2245 train_time:113370ms step_avg:117.85ms
step:963/2245 train_time:113485ms step_avg:117.85ms
step:964/2245 train_time:113608ms step_avg:117.85ms
step:965/2245 train_time:113723ms step_avg:117.85ms
step:966/2245 train_time:113845ms step_avg:117.85ms
step:967/2245 train_time:113960ms step_avg:117.85ms
step:968/2245 train_time:114082ms step_avg:117.85ms
step:969/2245 train_time:114198ms step_avg:117.85ms
step:970/2245 train_time:114320ms step_avg:117.86ms
step:971/2245 train_time:114437ms step_avg:117.85ms
step:972/2245 train_time:114560ms step_avg:117.86ms
step:973/2245 train_time:114676ms step_avg:117.86ms
step:974/2245 train_time:114798ms step_avg:117.86ms
step:975/2245 train_time:114913ms step_avg:117.86ms
step:976/2245 train_time:115034ms step_avg:117.86ms
step:977/2245 train_time:115150ms step_avg:117.86ms
step:978/2245 train_time:115272ms step_avg:117.86ms
step:979/2245 train_time:115387ms step_avg:117.86ms
step:980/2245 train_time:115509ms step_avg:117.87ms
step:981/2245 train_time:115625ms step_avg:117.86ms
step:982/2245 train_time:115747ms step_avg:117.87ms
step:983/2245 train_time:115862ms step_avg:117.87ms
step:984/2245 train_time:115985ms step_avg:117.87ms
step:985/2245 train_time:116100ms step_avg:117.87ms
step:986/2245 train_time:116222ms step_avg:117.87ms
step:987/2245 train_time:116338ms step_avg:117.87ms
step:988/2245 train_time:116461ms step_avg:117.88ms
step:989/2245 train_time:116577ms step_avg:117.87ms
step:990/2245 train_time:116699ms step_avg:117.88ms
step:991/2245 train_time:116815ms step_avg:117.88ms
step:992/2245 train_time:116937ms step_avg:117.88ms
step:993/2245 train_time:117052ms step_avg:117.88ms
step:994/2245 train_time:117173ms step_avg:117.88ms
step:995/2245 train_time:117289ms step_avg:117.88ms
step:996/2245 train_time:117411ms step_avg:117.88ms
step:997/2245 train_time:117527ms step_avg:117.88ms
step:998/2245 train_time:117649ms step_avg:117.89ms
step:999/2245 train_time:117765ms step_avg:117.88ms
step:1000/2245 train_time:117887ms step_avg:117.89ms
step:1000/2245 val_loss:3.5937 train_time:117953ms step_avg:117.95ms
step:1001/2245 train_time:118004ms step_avg:117.89ms
step:1002/2245 train_time:118125ms step_avg:117.89ms
step:1003/2245 train_time:118240ms step_avg:117.89ms
step:1004/2245 train_time:118363ms step_avg:117.89ms
step:1005/2245 train_time:118478ms step_avg:117.89ms
step:1006/2245 train_time:118600ms step_avg:117.89ms
step:1007/2245 train_time:118716ms step_avg:117.89ms
step:1008/2245 train_time:118838ms step_avg:117.89ms
step:1009/2245 train_time:118954ms step_avg:117.89ms
step:1010/2245 train_time:119076ms step_avg:117.90ms
step:1011/2245 train_time:119191ms step_avg:117.89ms
step:1012/2245 train_time:119314ms step_avg:117.90ms
step:1013/2245 train_time:119430ms step_avg:117.90ms
step:1014/2245 train_time:119552ms step_avg:117.90ms
step:1015/2245 train_time:119668ms step_avg:117.90ms
step:1016/2245 train_time:119790ms step_avg:117.90ms
step:1017/2245 train_time:119905ms step_avg:117.90ms
step:1018/2245 train_time:120027ms step_avg:117.90ms
step:1019/2245 train_time:120143ms step_avg:117.90ms
step:1020/2245 train_time:120264ms step_avg:117.91ms
step:1021/2245 train_time:120380ms step_avg:117.90ms
step:1022/2245 train_time:120501ms step_avg:117.91ms
step:1023/2245 train_time:120617ms step_avg:117.91ms
step:1024/2245 train_time:120739ms step_avg:117.91ms
step:1025/2245 train_time:120855ms step_avg:117.91ms
step:1026/2245 train_time:120977ms step_avg:117.91ms
step:1027/2245 train_time:121093ms step_avg:117.91ms
step:1028/2245 train_time:121215ms step_avg:117.91ms
step:1029/2245 train_time:121331ms step_avg:117.91ms
step:1030/2245 train_time:121453ms step_avg:117.92ms
step:1031/2245 train_time:121569ms step_avg:117.91ms
step:1032/2245 train_time:121691ms step_avg:117.92ms
step:1033/2245 train_time:121807ms step_avg:117.92ms
step:1034/2245 train_time:121929ms step_avg:117.92ms
step:1035/2245 train_time:122044ms step_avg:117.92ms
step:1036/2245 train_time:122166ms step_avg:117.92ms
step:1037/2245 train_time:122282ms step_avg:117.92ms
step:1038/2245 train_time:122404ms step_avg:117.92ms
step:1039/2245 train_time:122519ms step_avg:117.92ms
step:1040/2245 train_time:122641ms step_avg:117.92ms
step:1041/2245 train_time:122757ms step_avg:117.92ms
step:1042/2245 train_time:122879ms step_avg:117.93ms
step:1043/2245 train_time:122994ms step_avg:117.92ms
step:1044/2245 train_time:123117ms step_avg:117.93ms
step:1045/2245 train_time:123233ms step_avg:117.93ms
step:1046/2245 train_time:123355ms step_avg:117.93ms
step:1047/2245 train_time:123472ms step_avg:117.93ms
step:1048/2245 train_time:123595ms step_avg:117.93ms
step:1049/2245 train_time:123710ms step_avg:117.93ms
step:1050/2245 train_time:123833ms step_avg:117.94ms
step:1051/2245 train_time:123948ms step_avg:117.93ms
step:1052/2245 train_time:124070ms step_avg:117.94ms
step:1053/2245 train_time:124185ms step_avg:117.93ms
step:1054/2245 train_time:124307ms step_avg:117.94ms
step:1055/2245 train_time:124423ms step_avg:117.94ms
step:1056/2245 train_time:124544ms step_avg:117.94ms
step:1057/2245 train_time:124661ms step_avg:117.94ms
step:1058/2245 train_time:124782ms step_avg:117.94ms
step:1059/2245 train_time:124898ms step_avg:117.94ms
step:1060/2245 train_time:125020ms step_avg:117.94ms
step:1061/2245 train_time:125135ms step_avg:117.94ms
step:1062/2245 train_time:125258ms step_avg:117.94ms
step:1063/2245 train_time:125373ms step_avg:117.94ms
step:1064/2245 train_time:125495ms step_avg:117.95ms
step:1065/2245 train_time:125611ms step_avg:117.95ms
step:1066/2245 train_time:125735ms step_avg:117.95ms
step:1067/2245 train_time:125850ms step_avg:117.95ms
step:1068/2245 train_time:125973ms step_avg:117.95ms
step:1069/2245 train_time:126088ms step_avg:117.95ms
step:1070/2245 train_time:126209ms step_avg:117.95ms
step:1071/2245 train_time:126325ms step_avg:117.95ms
step:1072/2245 train_time:126447ms step_avg:117.95ms
step:1073/2245 train_time:126563ms step_avg:117.95ms
step:1074/2245 train_time:126685ms step_avg:117.96ms
step:1075/2245 train_time:126800ms step_avg:117.95ms
step:1076/2245 train_time:126922ms step_avg:117.96ms
step:1077/2245 train_time:127038ms step_avg:117.96ms
step:1078/2245 train_time:127160ms step_avg:117.96ms
step:1079/2245 train_time:127275ms step_avg:117.96ms
step:1080/2245 train_time:127397ms step_avg:117.96ms
step:1081/2245 train_time:127513ms step_avg:117.96ms
step:1082/2245 train_time:127635ms step_avg:117.96ms
step:1083/2245 train_time:127751ms step_avg:117.96ms
step:1084/2245 train_time:127874ms step_avg:117.97ms
step:1085/2245 train_time:127990ms step_avg:117.96ms
step:1086/2245 train_time:128113ms step_avg:117.97ms
step:1087/2245 train_time:128228ms step_avg:117.96ms
step:1088/2245 train_time:128349ms step_avg:117.97ms
step:1089/2245 train_time:128465ms step_avg:117.97ms
step:1090/2245 train_time:128587ms step_avg:117.97ms
step:1091/2245 train_time:128702ms step_avg:117.97ms
step:1092/2245 train_time:128825ms step_avg:117.97ms
step:1093/2245 train_time:128940ms step_avg:117.97ms
step:1094/2245 train_time:129063ms step_avg:117.97ms
step:1095/2245 train_time:129178ms step_avg:117.97ms
step:1096/2245 train_time:129300ms step_avg:117.97ms
step:1097/2245 train_time:129416ms step_avg:117.97ms
step:1098/2245 train_time:129538ms step_avg:117.98ms
step:1099/2245 train_time:129654ms step_avg:117.97ms
step:1100/2245 train_time:129776ms step_avg:117.98ms
step:1101/2245 train_time:129893ms step_avg:117.98ms
step:1102/2245 train_time:130015ms step_avg:117.98ms
step:1103/2245 train_time:130131ms step_avg:117.98ms
step:1104/2245 train_time:130253ms step_avg:117.98ms
step:1105/2245 train_time:130369ms step_avg:117.98ms
step:1106/2245 train_time:130491ms step_avg:117.98ms
step:1107/2245 train_time:130607ms step_avg:117.98ms
step:1108/2245 train_time:130729ms step_avg:117.99ms
step:1109/2245 train_time:130845ms step_avg:117.98ms
step:1110/2245 train_time:130967ms step_avg:117.99ms
step:1111/2245 train_time:131082ms step_avg:117.99ms
step:1112/2245 train_time:131204ms step_avg:117.99ms
step:1113/2245 train_time:131320ms step_avg:117.99ms
step:1114/2245 train_time:131442ms step_avg:117.99ms
step:1115/2245 train_time:131557ms step_avg:117.99ms
step:1116/2245 train_time:131680ms step_avg:117.99ms
step:1117/2245 train_time:131795ms step_avg:117.99ms
step:1118/2245 train_time:131917ms step_avg:117.99ms
step:1119/2245 train_time:132033ms step_avg:117.99ms
step:1120/2245 train_time:132156ms step_avg:118.00ms
step:1121/2245 train_time:132272ms step_avg:117.99ms
step:1122/2245 train_time:132394ms step_avg:118.00ms
step:1123/2245 train_time:132510ms step_avg:118.00ms
step:1124/2245 train_time:132633ms step_avg:118.00ms
step:1125/2245 train_time:132747ms step_avg:118.00ms
step:1126/2245 train_time:132869ms step_avg:118.00ms
step:1127/2245 train_time:132985ms step_avg:118.00ms
step:1128/2245 train_time:133107ms step_avg:118.00ms
step:1129/2245 train_time:133222ms step_avg:118.00ms
step:1130/2245 train_time:133344ms step_avg:118.00ms
step:1131/2245 train_time:133460ms step_avg:118.00ms
step:1132/2245 train_time:133582ms step_avg:118.01ms
step:1133/2245 train_time:133697ms step_avg:118.00ms
step:1134/2245 train_time:133819ms step_avg:118.01ms
step:1135/2245 train_time:133935ms step_avg:118.00ms
step:1136/2245 train_time:134057ms step_avg:118.01ms
step:1137/2245 train_time:134173ms step_avg:118.01ms
step:1138/2245 train_time:134295ms step_avg:118.01ms
step:1139/2245 train_time:134412ms step_avg:118.01ms
step:1140/2245 train_time:134534ms step_avg:118.01ms
step:1141/2245 train_time:134650ms step_avg:118.01ms
step:1142/2245 train_time:134773ms step_avg:118.01ms
step:1143/2245 train_time:134888ms step_avg:118.01ms
step:1144/2245 train_time:135010ms step_avg:118.02ms
step:1145/2245 train_time:135126ms step_avg:118.01ms
step:1146/2245 train_time:135247ms step_avg:118.02ms
step:1147/2245 train_time:135363ms step_avg:118.01ms
step:1148/2245 train_time:135485ms step_avg:118.02ms
step:1149/2245 train_time:135601ms step_avg:118.02ms
step:1150/2245 train_time:135723ms step_avg:118.02ms
step:1151/2245 train_time:135839ms step_avg:118.02ms
step:1152/2245 train_time:135962ms step_avg:118.02ms
step:1153/2245 train_time:136077ms step_avg:118.02ms
step:1154/2245 train_time:136200ms step_avg:118.02ms
step:1155/2245 train_time:136315ms step_avg:118.02ms
step:1156/2245 train_time:136437ms step_avg:118.03ms
step:1157/2245 train_time:136553ms step_avg:118.02ms
step:1158/2245 train_time:136675ms step_avg:118.03ms
step:1159/2245 train_time:136791ms step_avg:118.03ms
step:1160/2245 train_time:136913ms step_avg:118.03ms
step:1161/2245 train_time:137030ms step_avg:118.03ms
step:1162/2245 train_time:137152ms step_avg:118.03ms
step:1163/2245 train_time:137267ms step_avg:118.03ms
step:1164/2245 train_time:137389ms step_avg:118.03ms
step:1165/2245 train_time:137504ms step_avg:118.03ms
step:1166/2245 train_time:137626ms step_avg:118.03ms
step:1167/2245 train_time:137742ms step_avg:118.03ms
step:1168/2245 train_time:137864ms step_avg:118.03ms
step:1169/2245 train_time:137980ms step_avg:118.03ms
step:1170/2245 train_time:138102ms step_avg:118.04ms
step:1171/2245 train_time:138217ms step_avg:118.03ms
step:1172/2245 train_time:138339ms step_avg:118.04ms
step:1173/2245 train_time:138455ms step_avg:118.04ms
step:1174/2245 train_time:138577ms step_avg:118.04ms
step:1175/2245 train_time:138694ms step_avg:118.04ms
step:1176/2245 train_time:138816ms step_avg:118.04ms
step:1177/2245 train_time:138932ms step_avg:118.04ms
step:1178/2245 train_time:139055ms step_avg:118.04ms
step:1179/2245 train_time:139170ms step_avg:118.04ms
step:1180/2245 train_time:139292ms step_avg:118.04ms
step:1181/2245 train_time:139408ms step_avg:118.04ms
step:1182/2245 train_time:139530ms step_avg:118.05ms
step:1183/2245 train_time:139645ms step_avg:118.04ms
step:1184/2245 train_time:139767ms step_avg:118.05ms
step:1185/2245 train_time:139883ms step_avg:118.04ms
step:1186/2245 train_time:140005ms step_avg:118.05ms
step:1187/2245 train_time:140120ms step_avg:118.05ms
step:1188/2245 train_time:140243ms step_avg:118.05ms
step:1189/2245 train_time:140358ms step_avg:118.05ms
step:1190/2245 train_time:140480ms step_avg:118.05ms
step:1191/2245 train_time:140597ms step_avg:118.05ms
step:1192/2245 train_time:140718ms step_avg:118.05ms
step:1193/2245 train_time:140834ms step_avg:118.05ms
step:1194/2245 train_time:140956ms step_avg:118.05ms
step:1195/2245 train_time:141072ms step_avg:118.05ms
step:1196/2245 train_time:141194ms step_avg:118.05ms
step:1197/2245 train_time:141310ms step_avg:118.05ms
step:1198/2245 train_time:141432ms step_avg:118.06ms
step:1199/2245 train_time:141548ms step_avg:118.06ms
step:1200/2245 train_time:141670ms step_avg:118.06ms
step:1201/2245 train_time:141785ms step_avg:118.06ms
step:1202/2245 train_time:141907ms step_avg:118.06ms
step:1203/2245 train_time:142023ms step_avg:118.06ms
step:1204/2245 train_time:142144ms step_avg:118.06ms
step:1205/2245 train_time:142260ms step_avg:118.06ms
step:1206/2245 train_time:142382ms step_avg:118.06ms
step:1207/2245 train_time:142498ms step_avg:118.06ms
step:1208/2245 train_time:142620ms step_avg:118.06ms
step:1209/2245 train_time:142736ms step_avg:118.06ms
step:1210/2245 train_time:142858ms step_avg:118.06ms
step:1211/2245 train_time:142974ms step_avg:118.06ms
step:1212/2245 train_time:143095ms step_avg:118.07ms
step:1213/2245 train_time:143211ms step_avg:118.06ms
step:1214/2245 train_time:143333ms step_avg:118.07ms
step:1215/2245 train_time:143450ms step_avg:118.07ms
step:1216/2245 train_time:143571ms step_avg:118.07ms
step:1217/2245 train_time:143687ms step_avg:118.07ms
step:1218/2245 train_time:143809ms step_avg:118.07ms
step:1219/2245 train_time:143925ms step_avg:118.07ms
step:1220/2245 train_time:144046ms step_avg:118.07ms
step:1221/2245 train_time:144162ms step_avg:118.07ms
step:1222/2245 train_time:144284ms step_avg:118.07ms
step:1223/2245 train_time:144399ms step_avg:118.07ms
step:1224/2245 train_time:144522ms step_avg:118.07ms
step:1225/2245 train_time:144637ms step_avg:118.07ms
step:1226/2245 train_time:144759ms step_avg:118.07ms
step:1227/2245 train_time:144875ms step_avg:118.07ms
step:1228/2245 train_time:144997ms step_avg:118.08ms
step:1229/2245 train_time:145113ms step_avg:118.07ms
step:1230/2245 train_time:145235ms step_avg:118.08ms
step:1231/2245 train_time:145352ms step_avg:118.08ms
step:1232/2245 train_time:145474ms step_avg:118.08ms
step:1233/2245 train_time:145591ms step_avg:118.08ms
step:1234/2245 train_time:145713ms step_avg:118.08ms
step:1235/2245 train_time:145828ms step_avg:118.08ms
step:1236/2245 train_time:145949ms step_avg:118.08ms
step:1237/2245 train_time:146065ms step_avg:118.08ms
step:1238/2245 train_time:146187ms step_avg:118.08ms
step:1239/2245 train_time:146302ms step_avg:118.08ms
step:1240/2245 train_time:146425ms step_avg:118.08ms
step:1241/2245 train_time:146540ms step_avg:118.08ms
step:1242/2245 train_time:146663ms step_avg:118.09ms
step:1243/2245 train_time:146779ms step_avg:118.08ms
step:1244/2245 train_time:146900ms step_avg:118.09ms
step:1245/2245 train_time:147016ms step_avg:118.09ms
step:1246/2245 train_time:147139ms step_avg:118.09ms
step:1247/2245 train_time:147255ms step_avg:118.09ms
step:1248/2245 train_time:147377ms step_avg:118.09ms
step:1249/2245 train_time:147493ms step_avg:118.09ms
step:1250/2245 train_time:147616ms step_avg:118.09ms
step:1250/2245 val_loss:3.5254 train_time:147682ms step_avg:118.15ms
step:1251/2245 train_time:147733ms step_avg:118.09ms
step:1252/2245 train_time:147854ms step_avg:118.09ms
step:1253/2245 train_time:147970ms step_avg:118.09ms
step:1254/2245 train_time:148091ms step_avg:118.10ms
step:1255/2245 train_time:148207ms step_avg:118.09ms
step:1256/2245 train_time:148329ms step_avg:118.10ms
step:1257/2245 train_time:148445ms step_avg:118.09ms
step:1258/2245 train_time:148567ms step_avg:118.10ms
step:1259/2245 train_time:148683ms step_avg:118.10ms
step:1260/2245 train_time:148805ms step_avg:118.10ms
step:1261/2245 train_time:148921ms step_avg:118.10ms
step:1262/2245 train_time:149043ms step_avg:118.10ms
step:1263/2245 train_time:149159ms step_avg:118.10ms
step:1264/2245 train_time:149280ms step_avg:118.10ms
step:1265/2245 train_time:149396ms step_avg:118.10ms
step:1266/2245 train_time:149517ms step_avg:118.10ms
step:1267/2245 train_time:149633ms step_avg:118.10ms
step:1268/2245 train_time:149755ms step_avg:118.10ms
step:1269/2245 train_time:149870ms step_avg:118.10ms
step:1270/2245 train_time:149993ms step_avg:118.10ms
step:1271/2245 train_time:150109ms step_avg:118.10ms
step:1272/2245 train_time:150231ms step_avg:118.11ms
step:1273/2245 train_time:150346ms step_avg:118.10ms
step:1274/2245 train_time:150469ms step_avg:118.11ms
step:1275/2245 train_time:150584ms step_avg:118.11ms
step:1276/2245 train_time:150708ms step_avg:118.11ms
step:1277/2245 train_time:150824ms step_avg:118.11ms
step:1278/2245 train_time:150946ms step_avg:118.11ms
step:1279/2245 train_time:151062ms step_avg:118.11ms
step:1280/2245 train_time:151183ms step_avg:118.11ms
step:1281/2245 train_time:151299ms step_avg:118.11ms
step:1282/2245 train_time:151421ms step_avg:118.11ms
step:1283/2245 train_time:151537ms step_avg:118.11ms
step:1284/2245 train_time:151660ms step_avg:118.12ms
step:1285/2245 train_time:151775ms step_avg:118.11ms
step:1286/2245 train_time:151897ms step_avg:118.12ms
step:1287/2245 train_time:152013ms step_avg:118.11ms
step:1288/2245 train_time:152135ms step_avg:118.12ms
step:1289/2245 train_time:152251ms step_avg:118.12ms
step:1290/2245 train_time:152373ms step_avg:118.12ms
step:1291/2245 train_time:152488ms step_avg:118.12ms
step:1292/2245 train_time:152610ms step_avg:118.12ms
step:1293/2245 train_time:152727ms step_avg:118.12ms
step:1294/2245 train_time:152849ms step_avg:118.12ms
step:1295/2245 train_time:152964ms step_avg:118.12ms
step:1296/2245 train_time:153087ms step_avg:118.12ms
step:1297/2245 train_time:153203ms step_avg:118.12ms
step:1298/2245 train_time:153325ms step_avg:118.12ms
step:1299/2245 train_time:153441ms step_avg:118.12ms
step:1300/2245 train_time:153563ms step_avg:118.13ms
step:1301/2245 train_time:153678ms step_avg:118.12ms
step:1302/2245 train_time:153801ms step_avg:118.13ms
step:1303/2245 train_time:153916ms step_avg:118.12ms
step:1304/2245 train_time:154039ms step_avg:118.13ms
step:1305/2245 train_time:154154ms step_avg:118.13ms
step:1306/2245 train_time:154276ms step_avg:118.13ms
step:1307/2245 train_time:154392ms step_avg:118.13ms
step:1308/2245 train_time:154514ms step_avg:118.13ms
step:1309/2245 train_time:154629ms step_avg:118.13ms
step:1310/2245 train_time:154752ms step_avg:118.13ms
step:1311/2245 train_time:154867ms step_avg:118.13ms
step:1312/2245 train_time:154990ms step_avg:118.13ms
step:1313/2245 train_time:155106ms step_avg:118.13ms
step:1314/2245 train_time:155228ms step_avg:118.13ms
step:1315/2245 train_time:155344ms step_avg:118.13ms
step:1316/2245 train_time:155466ms step_avg:118.14ms
step:1317/2245 train_time:155582ms step_avg:118.13ms
step:1318/2245 train_time:155704ms step_avg:118.14ms
step:1319/2245 train_time:155819ms step_avg:118.13ms
step:1320/2245 train_time:155941ms step_avg:118.14ms
step:1321/2245 train_time:156056ms step_avg:118.13ms
step:1322/2245 train_time:156178ms step_avg:118.14ms
step:1323/2245 train_time:156293ms step_avg:118.14ms
step:1324/2245 train_time:156416ms step_avg:118.14ms
step:1325/2245 train_time:156532ms step_avg:118.14ms
step:1326/2245 train_time:156654ms step_avg:118.14ms
step:1327/2245 train_time:156770ms step_avg:118.14ms
step:1328/2245 train_time:156892ms step_avg:118.14ms
step:1329/2245 train_time:157008ms step_avg:118.14ms
step:1330/2245 train_time:157130ms step_avg:118.14ms
step:1331/2245 train_time:157246ms step_avg:118.14ms
step:1332/2245 train_time:157368ms step_avg:118.14ms
step:1333/2245 train_time:157484ms step_avg:118.14ms
step:1334/2245 train_time:157607ms step_avg:118.15ms
step:1335/2245 train_time:157723ms step_avg:118.14ms
step:1336/2245 train_time:157845ms step_avg:118.15ms
step:1337/2245 train_time:157960ms step_avg:118.15ms
step:1338/2245 train_time:158082ms step_avg:118.15ms
step:1339/2245 train_time:158198ms step_avg:118.15ms
step:1340/2245 train_time:158320ms step_avg:118.15ms
step:1341/2245 train_time:158435ms step_avg:118.15ms
step:1342/2245 train_time:158557ms step_avg:118.15ms
step:1343/2245 train_time:158673ms step_avg:118.15ms
step:1344/2245 train_time:158795ms step_avg:118.15ms
step:1345/2245 train_time:158910ms step_avg:118.15ms
step:1346/2245 train_time:159032ms step_avg:118.15ms
step:1347/2245 train_time:159148ms step_avg:118.15ms
step:1348/2245 train_time:159270ms step_avg:118.15ms
step:1349/2245 train_time:159386ms step_avg:118.15ms
step:1350/2245 train_time:159508ms step_avg:118.15ms
step:1351/2245 train_time:159624ms step_avg:118.15ms
step:1352/2245 train_time:159746ms step_avg:118.16ms
step:1353/2245 train_time:159862ms step_avg:118.15ms
step:1354/2245 train_time:159983ms step_avg:118.16ms
step:1355/2245 train_time:160099ms step_avg:118.15ms
step:1356/2245 train_time:160221ms step_avg:118.16ms
step:1357/2245 train_time:160336ms step_avg:118.16ms
step:1358/2245 train_time:160459ms step_avg:118.16ms
step:1359/2245 train_time:160575ms step_avg:118.16ms
step:1360/2245 train_time:160696ms step_avg:118.16ms
step:1361/2245 train_time:160812ms step_avg:118.16ms
step:1362/2245 train_time:160934ms step_avg:118.16ms
step:1363/2245 train_time:161050ms step_avg:118.16ms
step:1364/2245 train_time:161172ms step_avg:118.16ms
step:1365/2245 train_time:161288ms step_avg:118.16ms
step:1366/2245 train_time:161410ms step_avg:118.16ms
step:1367/2245 train_time:161526ms step_avg:118.16ms
step:1368/2245 train_time:161648ms step_avg:118.16ms
step:1369/2245 train_time:161764ms step_avg:118.16ms
step:1370/2245 train_time:161887ms step_avg:118.17ms
step:1371/2245 train_time:162002ms step_avg:118.16ms
step:1372/2245 train_time:162125ms step_avg:118.17ms
step:1373/2245 train_time:162240ms step_avg:118.16ms
step:1374/2245 train_time:162362ms step_avg:118.17ms
step:1375/2245 train_time:162477ms step_avg:118.17ms
step:1376/2245 train_time:162599ms step_avg:118.17ms
step:1377/2245 train_time:162715ms step_avg:118.17ms
step:1378/2245 train_time:162837ms step_avg:118.17ms
step:1379/2245 train_time:162952ms step_avg:118.17ms
step:1380/2245 train_time:163074ms step_avg:118.17ms
step:1381/2245 train_time:163190ms step_avg:118.17ms
step:1382/2245 train_time:163312ms step_avg:118.17ms
step:1383/2245 train_time:163428ms step_avg:118.17ms
step:1384/2245 train_time:163549ms step_avg:118.17ms
step:1385/2245 train_time:163665ms step_avg:118.17ms
step:1386/2245 train_time:163788ms step_avg:118.17ms
step:1387/2245 train_time:163904ms step_avg:118.17ms
step:1388/2245 train_time:164026ms step_avg:118.17ms
step:1389/2245 train_time:164141ms step_avg:118.17ms
step:1390/2245 train_time:164263ms step_avg:118.17ms
step:1391/2245 train_time:164379ms step_avg:118.17ms
step:1392/2245 train_time:164501ms step_avg:118.18ms
step:1393/2245 train_time:164617ms step_avg:118.17ms
step:1394/2245 train_time:164739ms step_avg:118.18ms
step:1395/2245 train_time:164854ms step_avg:118.18ms
step:1396/2245 train_time:164976ms step_avg:118.18ms
step:1397/2245 train_time:165092ms step_avg:118.18ms
step:1398/2245 train_time:165215ms step_avg:118.18ms
step:1399/2245 train_time:165330ms step_avg:118.18ms
step:1400/2245 train_time:165452ms step_avg:118.18ms
step:1401/2245 train_time:165567ms step_avg:118.18ms
step:1402/2245 train_time:165689ms step_avg:118.18ms
step:1403/2245 train_time:165805ms step_avg:118.18ms
step:1404/2245 train_time:165928ms step_avg:118.18ms
step:1405/2245 train_time:166044ms step_avg:118.18ms
step:1406/2245 train_time:166166ms step_avg:118.18ms
step:1407/2245 train_time:166282ms step_avg:118.18ms
step:1408/2245 train_time:166404ms step_avg:118.18ms
step:1409/2245 train_time:166519ms step_avg:118.18ms
step:1410/2245 train_time:166641ms step_avg:118.19ms
step:1411/2245 train_time:166757ms step_avg:118.18ms
step:1412/2245 train_time:166878ms step_avg:118.19ms
step:1413/2245 train_time:166995ms step_avg:118.18ms
step:1414/2245 train_time:167116ms step_avg:118.19ms
step:1415/2245 train_time:167232ms step_avg:118.19ms
step:1416/2245 train_time:167354ms step_avg:118.19ms
step:1417/2245 train_time:167470ms step_avg:118.19ms
step:1418/2245 train_time:167592ms step_avg:118.19ms
step:1419/2245 train_time:167708ms step_avg:118.19ms
step:1420/2245 train_time:167830ms step_avg:118.19ms
step:1421/2245 train_time:167946ms step_avg:118.19ms
step:1422/2245 train_time:168068ms step_avg:118.19ms
step:1423/2245 train_time:168185ms step_avg:118.19ms
step:1424/2245 train_time:168307ms step_avg:118.19ms
step:1425/2245 train_time:168422ms step_avg:118.19ms
step:1426/2245 train_time:168544ms step_avg:118.19ms
step:1427/2245 train_time:168660ms step_avg:118.19ms
step:1428/2245 train_time:168781ms step_avg:118.19ms
step:1429/2245 train_time:168897ms step_avg:118.19ms
step:1430/2245 train_time:169019ms step_avg:118.19ms
step:1431/2245 train_time:169134ms step_avg:118.19ms
step:1432/2245 train_time:169257ms step_avg:118.20ms
step:1433/2245 train_time:169373ms step_avg:118.19ms
step:1434/2245 train_time:169495ms step_avg:118.20ms
step:1435/2245 train_time:169610ms step_avg:118.20ms
step:1436/2245 train_time:169732ms step_avg:118.20ms
step:1437/2245 train_time:169848ms step_avg:118.20ms
step:1438/2245 train_time:169970ms step_avg:118.20ms
step:1439/2245 train_time:170086ms step_avg:118.20ms
step:1440/2245 train_time:170208ms step_avg:118.20ms
step:1441/2245 train_time:170324ms step_avg:118.20ms
step:1442/2245 train_time:170446ms step_avg:118.20ms
step:1443/2245 train_time:170561ms step_avg:118.20ms
step:1444/2245 train_time:170683ms step_avg:118.20ms
step:1445/2245 train_time:170799ms step_avg:118.20ms
step:1446/2245 train_time:170921ms step_avg:118.20ms
step:1447/2245 train_time:171036ms step_avg:118.20ms
step:1448/2245 train_time:171159ms step_avg:118.20ms
step:1449/2245 train_time:171274ms step_avg:118.20ms
step:1450/2245 train_time:171396ms step_avg:118.20ms
step:1451/2245 train_time:171512ms step_avg:118.20ms
step:1452/2245 train_time:171633ms step_avg:118.20ms
step:1453/2245 train_time:171749ms step_avg:118.20ms
step:1454/2245 train_time:171871ms step_avg:118.21ms
step:1455/2245 train_time:171988ms step_avg:118.20ms
step:1456/2245 train_time:172110ms step_avg:118.21ms
step:1457/2245 train_time:172226ms step_avg:118.21ms
step:1458/2245 train_time:172348ms step_avg:118.21ms
step:1459/2245 train_time:172465ms step_avg:118.21ms
step:1460/2245 train_time:172586ms step_avg:118.21ms
step:1461/2245 train_time:172702ms step_avg:118.21ms
step:1462/2245 train_time:172824ms step_avg:118.21ms
step:1463/2245 train_time:172940ms step_avg:118.21ms
step:1464/2245 train_time:173062ms step_avg:118.21ms
step:1465/2245 train_time:173178ms step_avg:118.21ms
step:1466/2245 train_time:173300ms step_avg:118.21ms
step:1467/2245 train_time:173415ms step_avg:118.21ms
step:1468/2245 train_time:173537ms step_avg:118.21ms
step:1469/2245 train_time:173652ms step_avg:118.21ms
step:1470/2245 train_time:173774ms step_avg:118.21ms
step:1471/2245 train_time:173891ms step_avg:118.21ms
step:1472/2245 train_time:174014ms step_avg:118.22ms
step:1473/2245 train_time:174130ms step_avg:118.21ms
step:1474/2245 train_time:174254ms step_avg:118.22ms
step:1475/2245 train_time:174370ms step_avg:118.22ms
step:1476/2245 train_time:174493ms step_avg:118.22ms
step:1477/2245 train_time:174610ms step_avg:118.22ms
step:1478/2245 train_time:174733ms step_avg:118.22ms
step:1479/2245 train_time:174850ms step_avg:118.22ms
step:1480/2245 train_time:174973ms step_avg:118.22ms
step:1481/2245 train_time:175090ms step_avg:118.22ms
step:1482/2245 train_time:175213ms step_avg:118.23ms
step:1483/2245 train_time:175329ms step_avg:118.23ms
step:1484/2245 train_time:175453ms step_avg:118.23ms
step:1485/2245 train_time:175569ms step_avg:118.23ms
step:1486/2245 train_time:175692ms step_avg:118.23ms
step:1487/2245 train_time:175809ms step_avg:118.23ms
step:1488/2245 train_time:175933ms step_avg:118.23ms
step:1489/2245 train_time:176049ms step_avg:118.23ms
step:1490/2245 train_time:176171ms step_avg:118.24ms
step:1491/2245 train_time:176288ms step_avg:118.23ms
step:1492/2245 train_time:176410ms step_avg:118.24ms
step:1493/2245 train_time:176528ms step_avg:118.24ms
step:1494/2245 train_time:176650ms step_avg:118.24ms
step:1495/2245 train_time:176767ms step_avg:118.24ms
step:1496/2245 train_time:176890ms step_avg:118.24ms
step:1497/2245 train_time:177008ms step_avg:118.24ms
step:1498/2245 train_time:177131ms step_avg:118.25ms
step:1499/2245 train_time:177248ms step_avg:118.24ms
step:1500/2245 train_time:177371ms step_avg:118.25ms
step:1500/2245 val_loss:3.4450 train_time:177438ms step_avg:118.29ms
step:1501/2245 train_time:177489ms step_avg:118.25ms
step:1502/2245 train_time:177610ms step_avg:118.25ms
step:1503/2245 train_time:177726ms step_avg:118.25ms
step:1504/2245 train_time:177849ms step_avg:118.25ms
step:1505/2245 train_time:177965ms step_avg:118.25ms
step:1506/2245 train_time:178089ms step_avg:118.25ms
step:1507/2245 train_time:178205ms step_avg:118.25ms
step:1508/2245 train_time:178327ms step_avg:118.25ms
step:1509/2245 train_time:178444ms step_avg:118.25ms
step:1510/2245 train_time:178567ms step_avg:118.26ms
step:1511/2245 train_time:178683ms step_avg:118.26ms
step:1512/2245 train_time:178806ms step_avg:118.26ms
step:1513/2245 train_time:178923ms step_avg:118.26ms
step:1514/2245 train_time:179046ms step_avg:118.26ms
step:1515/2245 train_time:179162ms step_avg:118.26ms
step:1516/2245 train_time:179285ms step_avg:118.26ms
step:1517/2245 train_time:179401ms step_avg:118.26ms
step:1518/2245 train_time:179524ms step_avg:118.26ms
step:1519/2245 train_time:179641ms step_avg:118.26ms
step:1520/2245 train_time:179764ms step_avg:118.27ms
step:1521/2245 train_time:179881ms step_avg:118.27ms
step:1522/2245 train_time:180005ms step_avg:118.27ms
step:1523/2245 train_time:180121ms step_avg:118.27ms
step:1524/2245 train_time:180244ms step_avg:118.27ms
step:1525/2245 train_time:180361ms step_avg:118.27ms
step:1526/2245 train_time:180484ms step_avg:118.27ms
step:1527/2245 train_time:180601ms step_avg:118.27ms
step:1528/2245 train_time:180723ms step_avg:118.27ms
step:1529/2245 train_time:180840ms step_avg:118.27ms
step:1530/2245 train_time:180963ms step_avg:118.28ms
step:1531/2245 train_time:181079ms step_avg:118.28ms
step:1532/2245 train_time:181202ms step_avg:118.28ms
step:1533/2245 train_time:181320ms step_avg:118.28ms
step:1534/2245 train_time:181443ms step_avg:118.28ms
step:1535/2245 train_time:181559ms step_avg:118.28ms
step:1536/2245 train_time:181682ms step_avg:118.28ms
step:1537/2245 train_time:181799ms step_avg:118.28ms
step:1538/2245 train_time:181922ms step_avg:118.28ms
step:1539/2245 train_time:182039ms step_avg:118.28ms
step:1540/2245 train_time:182162ms step_avg:118.29ms
step:1541/2245 train_time:182279ms step_avg:118.29ms
step:1542/2245 train_time:182403ms step_avg:118.29ms
step:1543/2245 train_time:182520ms step_avg:118.29ms
step:1544/2245 train_time:182643ms step_avg:118.29ms
step:1545/2245 train_time:182760ms step_avg:118.29ms
step:1546/2245 train_time:182884ms step_avg:118.29ms
step:1547/2245 train_time:183001ms step_avg:118.29ms
step:1548/2245 train_time:183123ms step_avg:118.30ms
step:1549/2245 train_time:183240ms step_avg:118.30ms
step:1550/2245 train_time:183364ms step_avg:118.30ms
step:1551/2245 train_time:183481ms step_avg:118.30ms
step:1552/2245 train_time:183604ms step_avg:118.30ms
step:1553/2245 train_time:183721ms step_avg:118.30ms
step:1554/2245 train_time:183844ms step_avg:118.30ms
step:1555/2245 train_time:183960ms step_avg:118.30ms
step:1556/2245 train_time:184083ms step_avg:118.31ms
step:1557/2245 train_time:184200ms step_avg:118.30ms
step:1558/2245 train_time:184323ms step_avg:118.31ms
step:1559/2245 train_time:184440ms step_avg:118.31ms
step:1560/2245 train_time:184562ms step_avg:118.31ms
step:1561/2245 train_time:184679ms step_avg:118.31ms
step:1562/2245 train_time:184802ms step_avg:118.31ms
step:1563/2245 train_time:184919ms step_avg:118.31ms
step:1564/2245 train_time:185042ms step_avg:118.31ms
step:1565/2245 train_time:185159ms step_avg:118.31ms
step:1566/2245 train_time:185282ms step_avg:118.32ms
step:1567/2245 train_time:185399ms step_avg:118.31ms
step:1568/2245 train_time:185522ms step_avg:118.32ms
step:1569/2245 train_time:185638ms step_avg:118.32ms
step:1570/2245 train_time:185761ms step_avg:118.32ms
step:1571/2245 train_time:185879ms step_avg:118.32ms
step:1572/2245 train_time:186001ms step_avg:118.32ms
step:1573/2245 train_time:186118ms step_avg:118.32ms
step:1574/2245 train_time:186242ms step_avg:118.32ms
step:1575/2245 train_time:186358ms step_avg:118.32ms
step:1576/2245 train_time:186482ms step_avg:118.33ms
step:1577/2245 train_time:186599ms step_avg:118.33ms
step:1578/2245 train_time:186722ms step_avg:118.33ms
step:1579/2245 train_time:186839ms step_avg:118.33ms
step:1580/2245 train_time:186962ms step_avg:118.33ms
step:1581/2245 train_time:187079ms step_avg:118.33ms
step:1582/2245 train_time:187202ms step_avg:118.33ms
step:1583/2245 train_time:187319ms step_avg:118.33ms
step:1584/2245 train_time:187442ms step_avg:118.33ms
step:1585/2245 train_time:187559ms step_avg:118.33ms
step:1586/2245 train_time:187682ms step_avg:118.34ms
step:1587/2245 train_time:187798ms step_avg:118.34ms
step:1588/2245 train_time:187922ms step_avg:118.34ms
step:1589/2245 train_time:188039ms step_avg:118.34ms
step:1590/2245 train_time:188162ms step_avg:118.34ms
step:1591/2245 train_time:188279ms step_avg:118.34ms
step:1592/2245 train_time:188402ms step_avg:118.34ms
step:1593/2245 train_time:188518ms step_avg:118.34ms
step:1594/2245 train_time:188642ms step_avg:118.35ms
step:1595/2245 train_time:188759ms step_avg:118.34ms
step:1596/2245 train_time:188882ms step_avg:118.35ms
step:1597/2245 train_time:188998ms step_avg:118.35ms
step:1598/2245 train_time:189121ms step_avg:118.35ms
step:1599/2245 train_time:189238ms step_avg:118.35ms
step:1600/2245 train_time:189362ms step_avg:118.35ms
step:1601/2245 train_time:189480ms step_avg:118.35ms
step:1602/2245 train_time:189603ms step_avg:118.35ms
step:1603/2245 train_time:189720ms step_avg:118.35ms
step:1604/2245 train_time:189843ms step_avg:118.36ms
step:1605/2245 train_time:189960ms step_avg:118.35ms
step:1606/2245 train_time:190083ms step_avg:118.36ms
step:1607/2245 train_time:190200ms step_avg:118.36ms
step:1608/2245 train_time:190323ms step_avg:118.36ms
step:1609/2245 train_time:190439ms step_avg:118.36ms
step:1610/2245 train_time:190563ms step_avg:118.36ms
step:1611/2245 train_time:190680ms step_avg:118.36ms
step:1612/2245 train_time:190803ms step_avg:118.36ms
step:1613/2245 train_time:190920ms step_avg:118.36ms
step:1614/2245 train_time:191043ms step_avg:118.37ms
step:1615/2245 train_time:191159ms step_avg:118.36ms
step:1616/2245 train_time:191283ms step_avg:118.37ms
step:1617/2245 train_time:191399ms step_avg:118.37ms
step:1618/2245 train_time:191522ms step_avg:118.37ms
step:1619/2245 train_time:191639ms step_avg:118.37ms
step:1620/2245 train_time:191761ms step_avg:118.37ms
step:1621/2245 train_time:191878ms step_avg:118.37ms
step:1622/2245 train_time:192001ms step_avg:118.37ms
step:1623/2245 train_time:192118ms step_avg:118.37ms
step:1624/2245 train_time:192240ms step_avg:118.37ms
step:1625/2245 train_time:192358ms step_avg:118.37ms
step:1626/2245 train_time:192481ms step_avg:118.38ms
step:1627/2245 train_time:192599ms step_avg:118.38ms
step:1628/2245 train_time:192722ms step_avg:118.38ms
step:1629/2245 train_time:192838ms step_avg:118.38ms
step:1630/2245 train_time:192961ms step_avg:118.38ms
step:1631/2245 train_time:193079ms step_avg:118.38ms
step:1632/2245 train_time:193202ms step_avg:118.38ms
step:1633/2245 train_time:193319ms step_avg:118.38ms
step:1634/2245 train_time:193443ms step_avg:118.39ms
step:1635/2245 train_time:193560ms step_avg:118.39ms
step:1636/2245 train_time:193683ms step_avg:118.39ms
step:1637/2245 train_time:193800ms step_avg:118.39ms
step:1638/2245 train_time:193923ms step_avg:118.39ms
step:1639/2245 train_time:194039ms step_avg:118.39ms
step:1640/2245 train_time:194162ms step_avg:118.39ms
step:1641/2245 train_time:194279ms step_avg:118.39ms
step:1642/2245 train_time:194402ms step_avg:118.39ms
step:1643/2245 train_time:194519ms step_avg:118.39ms
step:1644/2245 train_time:194642ms step_avg:118.40ms
step:1645/2245 train_time:194759ms step_avg:118.39ms
step:1646/2245 train_time:194882ms step_avg:118.40ms
step:1647/2245 train_time:194999ms step_avg:118.40ms
step:1648/2245 train_time:195122ms step_avg:118.40ms
step:1649/2245 train_time:195239ms step_avg:118.40ms
step:1650/2245 train_time:195362ms step_avg:118.40ms
step:1651/2245 train_time:195479ms step_avg:118.40ms
step:1652/2245 train_time:195602ms step_avg:118.40ms
step:1653/2245 train_time:195719ms step_avg:118.40ms
step:1654/2245 train_time:195842ms step_avg:118.41ms
step:1655/2245 train_time:195959ms step_avg:118.40ms
step:1656/2245 train_time:196082ms step_avg:118.41ms
step:1657/2245 train_time:196198ms step_avg:118.41ms
step:1658/2245 train_time:196321ms step_avg:118.41ms
step:1659/2245 train_time:196437ms step_avg:118.41ms
step:1660/2245 train_time:196560ms step_avg:118.41ms
step:1661/2245 train_time:196677ms step_avg:118.41ms
step:1662/2245 train_time:196801ms step_avg:118.41ms
step:1663/2245 train_time:196918ms step_avg:118.41ms
step:1664/2245 train_time:197041ms step_avg:118.41ms
step:1665/2245 train_time:197158ms step_avg:118.41ms
step:1666/2245 train_time:197282ms step_avg:118.42ms
step:1667/2245 train_time:197398ms step_avg:118.42ms
step:1668/2245 train_time:197521ms step_avg:118.42ms
step:1669/2245 train_time:197639ms step_avg:118.42ms
step:1670/2245 train_time:197761ms step_avg:118.42ms
step:1671/2245 train_time:197879ms step_avg:118.42ms
step:1672/2245 train_time:198002ms step_avg:118.42ms
step:1673/2245 train_time:198119ms step_avg:118.42ms
step:1674/2245 train_time:198243ms step_avg:118.42ms
step:1675/2245 train_time:198359ms step_avg:118.42ms
step:1676/2245 train_time:198482ms step_avg:118.43ms
step:1677/2245 train_time:198599ms step_avg:118.43ms
step:1678/2245 train_time:198722ms step_avg:118.43ms
step:1679/2245 train_time:198840ms step_avg:118.43ms
step:1680/2245 train_time:198963ms step_avg:118.43ms
step:1681/2245 train_time:199081ms step_avg:118.43ms
step:1682/2245 train_time:199204ms step_avg:118.43ms
step:1683/2245 train_time:199320ms step_avg:118.43ms
step:1684/2245 train_time:199443ms step_avg:118.43ms
step:1685/2245 train_time:199560ms step_avg:118.43ms
step:1686/2245 train_time:199683ms step_avg:118.44ms
step:1687/2245 train_time:199800ms step_avg:118.44ms
step:1688/2245 train_time:199924ms step_avg:118.44ms
step:1689/2245 train_time:200041ms step_avg:118.44ms
step:1690/2245 train_time:200164ms step_avg:118.44ms
step:1691/2245 train_time:200281ms step_avg:118.44ms
step:1692/2245 train_time:200404ms step_avg:118.44ms
step:1693/2245 train_time:200520ms step_avg:118.44ms
step:1694/2245 train_time:200643ms step_avg:118.44ms
step:1695/2245 train_time:200760ms step_avg:118.44ms
step:1696/2245 train_time:200883ms step_avg:118.45ms
step:1697/2245 train_time:201000ms step_avg:118.44ms
step:1698/2245 train_time:201124ms step_avg:118.45ms
step:1699/2245 train_time:201240ms step_avg:118.45ms
step:1700/2245 train_time:201364ms step_avg:118.45ms
step:1701/2245 train_time:201480ms step_avg:118.45ms
step:1702/2245 train_time:201603ms step_avg:118.45ms
step:1703/2245 train_time:201720ms step_avg:118.45ms
step:1704/2245 train_time:201843ms step_avg:118.45ms
step:1705/2245 train_time:201959ms step_avg:118.45ms
step:1706/2245 train_time:202083ms step_avg:118.45ms
step:1707/2245 train_time:202199ms step_avg:118.45ms
step:1708/2245 train_time:202323ms step_avg:118.46ms
step:1709/2245 train_time:202440ms step_avg:118.46ms
step:1710/2245 train_time:202563ms step_avg:118.46ms
step:1711/2245 train_time:202680ms step_avg:118.46ms
step:1712/2245 train_time:202803ms step_avg:118.46ms
step:1713/2245 train_time:202920ms step_avg:118.46ms
step:1714/2245 train_time:203043ms step_avg:118.46ms
step:1715/2245 train_time:203160ms step_avg:118.46ms
step:1716/2245 train_time:203284ms step_avg:118.46ms
step:1717/2245 train_time:203400ms step_avg:118.46ms
step:1718/2245 train_time:203522ms step_avg:118.46ms
step:1719/2245 train_time:203639ms step_avg:118.46ms
step:1720/2245 train_time:203762ms step_avg:118.47ms
step:1721/2245 train_time:203879ms step_avg:118.47ms
step:1722/2245 train_time:204002ms step_avg:118.47ms
step:1723/2245 train_time:204119ms step_avg:118.47ms
step:1724/2245 train_time:204242ms step_avg:118.47ms
step:1725/2245 train_time:204359ms step_avg:118.47ms
step:1726/2245 train_time:204482ms step_avg:118.47ms
step:1727/2245 train_time:204600ms step_avg:118.47ms
step:1728/2245 train_time:204724ms step_avg:118.47ms
step:1729/2245 train_time:204841ms step_avg:118.47ms
step:1730/2245 train_time:204964ms step_avg:118.48ms
step:1731/2245 train_time:205081ms step_avg:118.48ms
step:1732/2245 train_time:205204ms step_avg:118.48ms
step:1733/2245 train_time:205321ms step_avg:118.48ms
step:1734/2245 train_time:205444ms step_avg:118.48ms
step:1735/2245 train_time:205561ms step_avg:118.48ms
step:1736/2245 train_time:205684ms step_avg:118.48ms
step:1737/2245 train_time:205801ms step_avg:118.48ms
step:1738/2245 train_time:205924ms step_avg:118.48ms
step:1739/2245 train_time:206040ms step_avg:118.48ms
step:1740/2245 train_time:206163ms step_avg:118.48ms
step:1741/2245 train_time:206280ms step_avg:118.48ms
step:1742/2245 train_time:206403ms step_avg:118.49ms
step:1743/2245 train_time:206520ms step_avg:118.49ms
step:1744/2245 train_time:206643ms step_avg:118.49ms
step:1745/2245 train_time:206760ms step_avg:118.49ms
step:1746/2245 train_time:206883ms step_avg:118.49ms
step:1747/2245 train_time:207000ms step_avg:118.49ms
step:1748/2245 train_time:207123ms step_avg:118.49ms
step:1749/2245 train_time:207240ms step_avg:118.49ms
step:1750/2245 train_time:207363ms step_avg:118.49ms
step:1750/2245 val_loss:3.3817 train_time:207429ms step_avg:118.53ms
step:1751/2245 train_time:207480ms step_avg:118.49ms
step:1752/2245 train_time:207602ms step_avg:118.49ms
step:1753/2245 train_time:207718ms step_avg:118.49ms
step:1754/2245 train_time:207841ms step_avg:118.50ms
step:1755/2245 train_time:207958ms step_avg:118.49ms
step:1756/2245 train_time:208081ms step_avg:118.50ms
step:1757/2245 train_time:208197ms step_avg:118.50ms
step:1758/2245 train_time:208320ms step_avg:118.50ms
step:1759/2245 train_time:208436ms step_avg:118.50ms
step:1760/2245 train_time:208560ms step_avg:118.50ms
step:1761/2245 train_time:208677ms step_avg:118.50ms
step:1762/2245 train_time:208799ms step_avg:118.50ms
step:1763/2245 train_time:208916ms step_avg:118.50ms
step:1764/2245 train_time:209039ms step_avg:118.50ms
step:1765/2245 train_time:209155ms step_avg:118.50ms
step:1766/2245 train_time:209279ms step_avg:118.50ms
step:1767/2245 train_time:209395ms step_avg:118.50ms
step:1768/2245 train_time:209517ms step_avg:118.51ms
step:1769/2245 train_time:209634ms step_avg:118.50ms
step:1770/2245 train_time:209756ms step_avg:118.51ms
step:1771/2245 train_time:209873ms step_avg:118.51ms
step:1772/2245 train_time:209995ms step_avg:118.51ms
step:1773/2245 train_time:210111ms step_avg:118.51ms
step:1774/2245 train_time:210234ms step_avg:118.51ms
step:1775/2245 train_time:210350ms step_avg:118.51ms
step:1776/2245 train_time:210472ms step_avg:118.51ms
step:1777/2245 train_time:210589ms step_avg:118.51ms
step:1778/2245 train_time:210712ms step_avg:118.51ms
step:1779/2245 train_time:210829ms step_avg:118.51ms
step:1780/2245 train_time:210952ms step_avg:118.51ms
step:1781/2245 train_time:211068ms step_avg:118.51ms
step:1782/2245 train_time:211191ms step_avg:118.51ms
step:1783/2245 train_time:211308ms step_avg:118.51ms
step:1784/2245 train_time:211431ms step_avg:118.52ms
step:1785/2245 train_time:211548ms step_avg:118.51ms
step:1786/2245 train_time:211671ms step_avg:118.52ms
step:1787/2245 train_time:211788ms step_avg:118.52ms
step:1788/2245 train_time:211911ms step_avg:118.52ms
step:1789/2245 train_time:212027ms step_avg:118.52ms
step:1790/2245 train_time:212150ms step_avg:118.52ms
step:1791/2245 train_time:212266ms step_avg:118.52ms
step:1792/2245 train_time:212390ms step_avg:118.52ms
step:1793/2245 train_time:212505ms step_avg:118.52ms
step:1794/2245 train_time:212629ms step_avg:118.52ms
step:1795/2245 train_time:212746ms step_avg:118.52ms
step:1796/2245 train_time:212868ms step_avg:118.52ms
step:1797/2245 train_time:212984ms step_avg:118.52ms
step:1798/2245 train_time:213108ms step_avg:118.52ms
step:1799/2245 train_time:213224ms step_avg:118.52ms
step:1800/2245 train_time:213347ms step_avg:118.53ms
step:1801/2245 train_time:213464ms step_avg:118.53ms
step:1802/2245 train_time:213587ms step_avg:118.53ms
step:1803/2245 train_time:213704ms step_avg:118.53ms
step:1804/2245 train_time:213827ms step_avg:118.53ms
step:1805/2245 train_time:213943ms step_avg:118.53ms
step:1806/2245 train_time:214066ms step_avg:118.53ms
step:1807/2245 train_time:214182ms step_avg:118.53ms
step:1808/2245 train_time:214305ms step_avg:118.53ms
step:1809/2245 train_time:214422ms step_avg:118.53ms
step:1810/2245 train_time:214545ms step_avg:118.53ms
step:1811/2245 train_time:214662ms step_avg:118.53ms
step:1812/2245 train_time:214785ms step_avg:118.54ms
step:1813/2245 train_time:214902ms step_avg:118.53ms
step:1814/2245 train_time:215025ms step_avg:118.54ms
step:1815/2245 train_time:215142ms step_avg:118.54ms
step:1816/2245 train_time:215265ms step_avg:118.54ms
step:1817/2245 train_time:215381ms step_avg:118.54ms
step:1818/2245 train_time:215504ms step_avg:118.54ms
step:1819/2245 train_time:215621ms step_avg:118.54ms
step:1820/2245 train_time:215744ms step_avg:118.54ms
step:1821/2245 train_time:215861ms step_avg:118.54ms
step:1822/2245 train_time:215984ms step_avg:118.54ms
step:1823/2245 train_time:216100ms step_avg:118.54ms
step:1824/2245 train_time:216223ms step_avg:118.54ms
step:1825/2245 train_time:216340ms step_avg:118.54ms
step:1826/2245 train_time:216463ms step_avg:118.54ms
step:1827/2245 train_time:216579ms step_avg:118.54ms
step:1828/2245 train_time:216702ms step_avg:118.55ms
step:1829/2245 train_time:216819ms step_avg:118.54ms
step:1830/2245 train_time:216942ms step_avg:118.55ms
step:1831/2245 train_time:217058ms step_avg:118.55ms
step:1832/2245 train_time:217182ms step_avg:118.55ms
step:1833/2245 train_time:217299ms step_avg:118.55ms
step:1834/2245 train_time:217422ms step_avg:118.55ms
step:1835/2245 train_time:217539ms step_avg:118.55ms
step:1836/2245 train_time:217662ms step_avg:118.55ms
step:1837/2245 train_time:217779ms step_avg:118.55ms
step:1838/2245 train_time:217903ms step_avg:118.55ms
step:1839/2245 train_time:218020ms step_avg:118.55ms
step:1840/2245 train_time:218143ms step_avg:118.56ms
step:1841/2245 train_time:218260ms step_avg:118.56ms
step:1842/2245 train_time:218383ms step_avg:118.56ms
step:1843/2245 train_time:218500ms step_avg:118.56ms
step:1844/2245 train_time:218623ms step_avg:118.56ms
step:1845/2245 train_time:218739ms step_avg:118.56ms
step:1846/2245 train_time:218862ms step_avg:118.56ms
step:1847/2245 train_time:218979ms step_avg:118.56ms
step:1848/2245 train_time:219102ms step_avg:118.56ms
step:1849/2245 train_time:219218ms step_avg:118.56ms
step:1850/2245 train_time:219341ms step_avg:118.56ms
step:1851/2245 train_time:219459ms step_avg:118.56ms
step:1852/2245 train_time:219582ms step_avg:118.56ms
step:1853/2245 train_time:219698ms step_avg:118.56ms
step:1854/2245 train_time:219821ms step_avg:118.57ms
step:1855/2245 train_time:219938ms step_avg:118.57ms
step:1856/2245 train_time:220062ms step_avg:118.57ms
step:1857/2245 train_time:220179ms step_avg:118.57ms
step:1858/2245 train_time:220302ms step_avg:118.57ms
step:1859/2245 train_time:220419ms step_avg:118.57ms
step:1860/2245 train_time:220543ms step_avg:118.57ms
step:1861/2245 train_time:220660ms step_avg:118.57ms
step:1862/2245 train_time:220783ms step_avg:118.57ms
step:1863/2245 train_time:220900ms step_avg:118.57ms
step:1864/2245 train_time:221023ms step_avg:118.57ms
step:1865/2245 train_time:221140ms step_avg:118.57ms
step:1866/2245 train_time:221262ms step_avg:118.58ms
step:1867/2245 train_time:221380ms step_avg:118.58ms
step:1868/2245 train_time:221503ms step_avg:118.58ms
step:1869/2245 train_time:221620ms step_avg:118.58ms
step:1870/2245 train_time:221743ms step_avg:118.58ms
step:1871/2245 train_time:221859ms step_avg:118.58ms
step:1872/2245 train_time:221982ms step_avg:118.58ms
step:1873/2245 train_time:222098ms step_avg:118.58ms
step:1874/2245 train_time:222222ms step_avg:118.58ms
step:1875/2245 train_time:222339ms step_avg:118.58ms
step:1876/2245 train_time:222461ms step_avg:118.58ms
step:1877/2245 train_time:222578ms step_avg:118.58ms
step:1878/2245 train_time:222701ms step_avg:118.58ms
step:1879/2245 train_time:222818ms step_avg:118.58ms
step:1880/2245 train_time:222942ms step_avg:118.59ms
step:1881/2245 train_time:223059ms step_avg:118.59ms
step:1882/2245 train_time:223182ms step_avg:118.59ms
step:1883/2245 train_time:223299ms step_avg:118.59ms
step:1884/2245 train_time:223422ms step_avg:118.59ms
step:1885/2245 train_time:223540ms step_avg:118.59ms
step:1886/2245 train_time:223662ms step_avg:118.59ms
step:1887/2245 train_time:223779ms step_avg:118.59ms
step:1888/2245 train_time:223902ms step_avg:118.59ms
step:1889/2245 train_time:224018ms step_avg:118.59ms
step:1890/2245 train_time:224142ms step_avg:118.59ms
step:1891/2245 train_time:224259ms step_avg:118.59ms
step:1892/2245 train_time:224383ms step_avg:118.60ms
step:1893/2245 train_time:224500ms step_avg:118.59ms
step:1894/2245 train_time:224623ms step_avg:118.60ms
step:1895/2245 train_time:224740ms step_avg:118.60ms
step:1896/2245 train_time:224863ms step_avg:118.60ms
step:1897/2245 train_time:224980ms step_avg:118.60ms
step:1898/2245 train_time:225103ms step_avg:118.60ms
step:1899/2245 train_time:225219ms step_avg:118.60ms
step:1900/2245 train_time:225343ms step_avg:118.60ms
step:1901/2245 train_time:225459ms step_avg:118.60ms
step:1902/2245 train_time:225582ms step_avg:118.60ms
step:1903/2245 train_time:225699ms step_avg:118.60ms
step:1904/2245 train_time:225822ms step_avg:118.60ms
step:1905/2245 train_time:225939ms step_avg:118.60ms
step:1906/2245 train_time:226063ms step_avg:118.61ms
step:1907/2245 train_time:226179ms step_avg:118.60ms
step:1908/2245 train_time:226302ms step_avg:118.61ms
step:1909/2245 train_time:226420ms step_avg:118.61ms
step:1910/2245 train_time:226542ms step_avg:118.61ms
step:1911/2245 train_time:226659ms step_avg:118.61ms
step:1912/2245 train_time:226783ms step_avg:118.61ms
step:1913/2245 train_time:226898ms step_avg:118.61ms
step:1914/2245 train_time:227023ms step_avg:118.61ms
step:1915/2245 train_time:227139ms step_avg:118.61ms
step:1916/2245 train_time:227262ms step_avg:118.61ms
step:1917/2245 train_time:227379ms step_avg:118.61ms
step:1918/2245 train_time:227501ms step_avg:118.61ms
step:1919/2245 train_time:227619ms step_avg:118.61ms
step:1920/2245 train_time:227741ms step_avg:118.62ms
step:1921/2245 train_time:227858ms step_avg:118.61ms
step:1922/2245 train_time:227983ms step_avg:118.62ms
step:1923/2245 train_time:228100ms step_avg:118.62ms
step:1924/2245 train_time:228223ms step_avg:118.62ms
step:1925/2245 train_time:228341ms step_avg:118.62ms
step:1926/2245 train_time:228463ms step_avg:118.62ms
step:1927/2245 train_time:228580ms step_avg:118.62ms
step:1928/2245 train_time:228703ms step_avg:118.62ms
step:1929/2245 train_time:228820ms step_avg:118.62ms
step:1930/2245 train_time:228943ms step_avg:118.62ms
step:1931/2245 train_time:229060ms step_avg:118.62ms
step:1932/2245 train_time:229182ms step_avg:118.62ms
step:1933/2245 train_time:229300ms step_avg:118.62ms
step:1934/2245 train_time:229423ms step_avg:118.63ms
step:1935/2245 train_time:229540ms step_avg:118.63ms
step:1936/2245 train_time:229663ms step_avg:118.63ms
step:1937/2245 train_time:229780ms step_avg:118.63ms
step:1938/2245 train_time:229903ms step_avg:118.63ms
step:1939/2245 train_time:230020ms step_avg:118.63ms
step:1940/2245 train_time:230144ms step_avg:118.63ms
step:1941/2245 train_time:230261ms step_avg:118.63ms
step:1942/2245 train_time:230383ms step_avg:118.63ms
step:1943/2245 train_time:230500ms step_avg:118.63ms
step:1944/2245 train_time:230623ms step_avg:118.63ms
step:1945/2245 train_time:230740ms step_avg:118.63ms
step:1946/2245 train_time:230863ms step_avg:118.63ms
step:1947/2245 train_time:230979ms step_avg:118.63ms
step:1948/2245 train_time:231103ms step_avg:118.64ms
step:1949/2245 train_time:231220ms step_avg:118.64ms
step:1950/2245 train_time:231344ms step_avg:118.64ms
step:1951/2245 train_time:231460ms step_avg:118.64ms
step:1952/2245 train_time:231584ms step_avg:118.64ms
step:1953/2245 train_time:231700ms step_avg:118.64ms
step:1954/2245 train_time:231823ms step_avg:118.64ms
step:1955/2245 train_time:231940ms step_avg:118.64ms
step:1956/2245 train_time:232063ms step_avg:118.64ms
step:1957/2245 train_time:232180ms step_avg:118.64ms
step:1958/2245 train_time:232303ms step_avg:118.64ms
step:1959/2245 train_time:232420ms step_avg:118.64ms
step:1960/2245 train_time:232544ms step_avg:118.64ms
step:1961/2245 train_time:232661ms step_avg:118.64ms
step:1962/2245 train_time:232783ms step_avg:118.65ms
step:1963/2245 train_time:232900ms step_avg:118.64ms
step:1964/2245 train_time:233023ms step_avg:118.65ms
step:1965/2245 train_time:233139ms step_avg:118.65ms
step:1966/2245 train_time:233262ms step_avg:118.65ms
step:1967/2245 train_time:233380ms step_avg:118.65ms
step:1968/2245 train_time:233504ms step_avg:118.65ms
step:1969/2245 train_time:233620ms step_avg:118.65ms
step:1970/2245 train_time:233743ms step_avg:118.65ms
step:1971/2245 train_time:233859ms step_avg:118.65ms
step:1972/2245 train_time:233982ms step_avg:118.65ms
step:1973/2245 train_time:234099ms step_avg:118.65ms
step:1974/2245 train_time:234222ms step_avg:118.65ms
step:1975/2245 train_time:234339ms step_avg:118.65ms
step:1976/2245 train_time:234462ms step_avg:118.66ms
step:1977/2245 train_time:234579ms step_avg:118.65ms
step:1978/2245 train_time:234702ms step_avg:118.66ms
step:1979/2245 train_time:234818ms step_avg:118.66ms
step:1980/2245 train_time:234942ms step_avg:118.66ms
step:1981/2245 train_time:235058ms step_avg:118.66ms
step:1982/2245 train_time:235181ms step_avg:118.66ms
step:1983/2245 train_time:235298ms step_avg:118.66ms
step:1984/2245 train_time:235421ms step_avg:118.66ms
step:1985/2245 train_time:235539ms step_avg:118.66ms
step:1986/2245 train_time:235662ms step_avg:118.66ms
step:1987/2245 train_time:235779ms step_avg:118.66ms
step:1988/2245 train_time:235902ms step_avg:118.66ms
step:1989/2245 train_time:236019ms step_avg:118.66ms
step:1990/2245 train_time:236143ms step_avg:118.66ms
step:1991/2245 train_time:236260ms step_avg:118.66ms
step:1992/2245 train_time:236383ms step_avg:118.67ms
step:1993/2245 train_time:236500ms step_avg:118.67ms
step:1994/2245 train_time:236623ms step_avg:118.67ms
step:1995/2245 train_time:236740ms step_avg:118.67ms
step:1996/2245 train_time:236862ms step_avg:118.67ms
step:1997/2245 train_time:236980ms step_avg:118.67ms
step:1998/2245 train_time:237103ms step_avg:118.67ms
step:1999/2245 train_time:237220ms step_avg:118.67ms
step:2000/2245 train_time:237343ms step_avg:118.67ms
step:2000/2245 val_loss:3.3254 train_time:237410ms step_avg:118.70ms
step:2001/2245 train_time:237461ms step_avg:118.67ms
step:2002/2245 train_time:237582ms step_avg:118.67ms
step:2003/2245 train_time:237699ms step_avg:118.67ms
step:2004/2245 train_time:237822ms step_avg:118.67ms
step:2005/2245 train_time:237939ms step_avg:118.67ms
step:2006/2245 train_time:238063ms step_avg:118.68ms
step:2007/2245 train_time:238179ms step_avg:118.67ms
step:2008/2245 train_time:238303ms step_avg:118.68ms
step:2009/2245 train_time:238420ms step_avg:118.68ms
step:2010/2245 train_time:238545ms step_avg:118.68ms
step:2011/2245 train_time:238662ms step_avg:118.68ms
step:2012/2245 train_time:238784ms step_avg:118.68ms
step:2013/2245 train_time:238901ms step_avg:118.68ms
step:2014/2245 train_time:239024ms step_avg:118.68ms
step:2015/2245 train_time:239141ms step_avg:118.68ms
step:2016/2245 train_time:239264ms step_avg:118.68ms
step:2017/2245 train_time:239381ms step_avg:118.68ms
step:2018/2245 train_time:239504ms step_avg:118.68ms
step:2019/2245 train_time:239621ms step_avg:118.68ms
step:2020/2245 train_time:239745ms step_avg:118.69ms
step:2021/2245 train_time:239862ms step_avg:118.68ms
step:2022/2245 train_time:239984ms step_avg:118.69ms
step:2023/2245 train_time:240102ms step_avg:118.69ms
step:2024/2245 train_time:240224ms step_avg:118.69ms
step:2025/2245 train_time:240342ms step_avg:118.69ms
step:2026/2245 train_time:240465ms step_avg:118.69ms
step:2027/2245 train_time:240582ms step_avg:118.69ms
step:2028/2245 train_time:240704ms step_avg:118.69ms
step:2029/2245 train_time:240821ms step_avg:118.69ms
step:2030/2245 train_time:240944ms step_avg:118.69ms
step:2031/2245 train_time:241060ms step_avg:118.69ms
step:2032/2245 train_time:241183ms step_avg:118.69ms
step:2033/2245 train_time:241300ms step_avg:118.69ms
step:2034/2245 train_time:241424ms step_avg:118.69ms
step:2035/2245 train_time:241540ms step_avg:118.69ms
step:2036/2245 train_time:241664ms step_avg:118.70ms
step:2037/2245 train_time:241780ms step_avg:118.69ms
step:2038/2245 train_time:241904ms step_avg:118.70ms
step:2039/2245 train_time:242020ms step_avg:118.70ms
step:2040/2245 train_time:242143ms step_avg:118.70ms
step:2041/2245 train_time:242260ms step_avg:118.70ms
step:2042/2245 train_time:242383ms step_avg:118.70ms
step:2043/2245 train_time:242500ms step_avg:118.70ms
step:2044/2245 train_time:242624ms step_avg:118.70ms
step:2045/2245 train_time:242741ms step_avg:118.70ms
step:2046/2245 train_time:242865ms step_avg:118.70ms
step:2047/2245 train_time:242982ms step_avg:118.70ms
step:2048/2245 train_time:243106ms step_avg:118.70ms
step:2049/2245 train_time:243222ms step_avg:118.70ms
step:2050/2245 train_time:243345ms step_avg:118.71ms
step:2051/2245 train_time:243462ms step_avg:118.70ms
step:2052/2245 train_time:243585ms step_avg:118.71ms
step:2053/2245 train_time:243701ms step_avg:118.70ms
step:2054/2245 train_time:243825ms step_avg:118.71ms
step:2055/2245 train_time:243942ms step_avg:118.71ms
step:2056/2245 train_time:244064ms step_avg:118.71ms
step:2057/2245 train_time:244181ms step_avg:118.71ms
step:2058/2245 train_time:244304ms step_avg:118.71ms
step:2059/2245 train_time:244421ms step_avg:118.71ms
step:2060/2245 train_time:244544ms step_avg:118.71ms
step:2061/2245 train_time:244662ms step_avg:118.71ms
step:2062/2245 train_time:244784ms step_avg:118.71ms
step:2063/2245 train_time:244901ms step_avg:118.71ms
step:2064/2245 train_time:245023ms step_avg:118.71ms
step:2065/2245 train_time:245141ms step_avg:118.71ms
step:2066/2245 train_time:245264ms step_avg:118.71ms
step:2067/2245 train_time:245380ms step_avg:118.71ms
step:2068/2245 train_time:245504ms step_avg:118.72ms
step:2069/2245 train_time:245620ms step_avg:118.71ms
step:2070/2245 train_time:245743ms step_avg:118.72ms
step:2071/2245 train_time:245860ms step_avg:118.72ms
step:2072/2245 train_time:245983ms step_avg:118.72ms
step:2073/2245 train_time:246100ms step_avg:118.72ms
step:2074/2245 train_time:246222ms step_avg:118.72ms
step:2075/2245 train_time:246339ms step_avg:118.72ms
step:2076/2245 train_time:246462ms step_avg:118.72ms
step:2077/2245 train_time:246579ms step_avg:118.72ms
step:2078/2245 train_time:246702ms step_avg:118.72ms
step:2079/2245 train_time:246820ms step_avg:118.72ms
step:2080/2245 train_time:246943ms step_avg:118.72ms
step:2081/2245 train_time:247060ms step_avg:118.72ms
step:2082/2245 train_time:247183ms step_avg:118.72ms
step:2083/2245 train_time:247300ms step_avg:118.72ms
step:2084/2245 train_time:247424ms step_avg:118.73ms
step:2085/2245 train_time:247540ms step_avg:118.72ms
step:2086/2245 train_time:247664ms step_avg:118.73ms
step:2087/2245 train_time:247780ms step_avg:118.73ms
step:2088/2245 train_time:247904ms step_avg:118.73ms
step:2089/2245 train_time:248021ms step_avg:118.73ms
step:2090/2245 train_time:248144ms step_avg:118.73ms
step:2091/2245 train_time:248261ms step_avg:118.73ms
step:2092/2245 train_time:248383ms step_avg:118.73ms
step:2093/2245 train_time:248500ms step_avg:118.73ms
step:2094/2245 train_time:248624ms step_avg:118.73ms
step:2095/2245 train_time:248741ms step_avg:118.73ms
step:2096/2245 train_time:248863ms step_avg:118.73ms
step:2097/2245 train_time:248981ms step_avg:118.73ms
step:2098/2245 train_time:249105ms step_avg:118.73ms
step:2099/2245 train_time:249221ms step_avg:118.73ms
step:2100/2245 train_time:249346ms step_avg:118.74ms
step:2101/2245 train_time:249462ms step_avg:118.73ms
step:2102/2245 train_time:249585ms step_avg:118.74ms
step:2103/2245 train_time:249702ms step_avg:118.74ms
step:2104/2245 train_time:249825ms step_avg:118.74ms
step:2105/2245 train_time:249942ms step_avg:118.74ms
step:2106/2245 train_time:250064ms step_avg:118.74ms
step:2107/2245 train_time:250181ms step_avg:118.74ms
step:2108/2245 train_time:250304ms step_avg:118.74ms
step:2109/2245 train_time:250422ms step_avg:118.74ms
step:2110/2245 train_time:250545ms step_avg:118.74ms
step:2111/2245 train_time:250661ms step_avg:118.74ms
step:2112/2245 train_time:250785ms step_avg:118.74ms
step:2113/2245 train_time:250902ms step_avg:118.74ms
step:2114/2245 train_time:251025ms step_avg:118.74ms
step:2115/2245 train_time:251141ms step_avg:118.74ms
step:2116/2245 train_time:251264ms step_avg:118.74ms
step:2117/2245 train_time:251380ms step_avg:118.74ms
step:2118/2245 train_time:251504ms step_avg:118.75ms
step:2119/2245 train_time:251620ms step_avg:118.74ms
step:2120/2245 train_time:251743ms step_avg:118.75ms
step:2121/2245 train_time:251860ms step_avg:118.75ms
step:2122/2245 train_time:251983ms step_avg:118.75ms
step:2123/2245 train_time:252101ms step_avg:118.75ms
step:2124/2245 train_time:252224ms step_avg:118.75ms
step:2125/2245 train_time:252340ms step_avg:118.75ms
step:2126/2245 train_time:252464ms step_avg:118.75ms
step:2127/2245 train_time:252580ms step_avg:118.75ms
step:2128/2245 train_time:252704ms step_avg:118.75ms
step:2129/2245 train_time:252821ms step_avg:118.75ms
step:2130/2245 train_time:252945ms step_avg:118.75ms
step:2131/2245 train_time:253062ms step_avg:118.75ms
step:2132/2245 train_time:253185ms step_avg:118.75ms
step:2133/2245 train_time:253302ms step_avg:118.75ms
step:2134/2245 train_time:253424ms step_avg:118.76ms
step:2135/2245 train_time:253541ms step_avg:118.75ms
step:2136/2245 train_time:253664ms step_avg:118.76ms
step:2137/2245 train_time:253780ms step_avg:118.76ms
step:2138/2245 train_time:253903ms step_avg:118.76ms
step:2139/2245 train_time:254020ms step_avg:118.76ms
step:2140/2245 train_time:254143ms step_avg:118.76ms
step:2141/2245 train_time:254260ms step_avg:118.76ms
step:2142/2245 train_time:254383ms step_avg:118.76ms
step:2143/2245 train_time:254500ms step_avg:118.76ms
step:2144/2245 train_time:254624ms step_avg:118.76ms
step:2145/2245 train_time:254740ms step_avg:118.76ms
step:2146/2245 train_time:254863ms step_avg:118.76ms
step:2147/2245 train_time:254979ms step_avg:118.76ms
step:2148/2245 train_time:255103ms step_avg:118.76ms
step:2149/2245 train_time:255220ms step_avg:118.76ms
step:2150/2245 train_time:255343ms step_avg:118.76ms
step:2151/2245 train_time:255460ms step_avg:118.76ms
step:2152/2245 train_time:255583ms step_avg:118.77ms
step:2153/2245 train_time:255701ms step_avg:118.76ms
step:2154/2245 train_time:255825ms step_avg:118.77ms
step:2155/2245 train_time:255941ms step_avg:118.77ms
step:2156/2245 train_time:256065ms step_avg:118.77ms
step:2157/2245 train_time:256182ms step_avg:118.77ms
step:2158/2245 train_time:256305ms step_avg:118.77ms
step:2159/2245 train_time:256422ms step_avg:118.77ms
step:2160/2245 train_time:256545ms step_avg:118.77ms
step:2161/2245 train_time:256662ms step_avg:118.77ms
step:2162/2245 train_time:256784ms step_avg:118.77ms
step:2163/2245 train_time:256901ms step_avg:118.77ms
step:2164/2245 train_time:257025ms step_avg:118.77ms
step:2165/2245 train_time:257142ms step_avg:118.77ms
step:2166/2245 train_time:257264ms step_avg:118.77ms
step:2167/2245 train_time:257381ms step_avg:118.77ms
step:2168/2245 train_time:257504ms step_avg:118.77ms
step:2169/2245 train_time:257620ms step_avg:118.77ms
step:2170/2245 train_time:257743ms step_avg:118.78ms
step:2171/2245 train_time:257860ms step_avg:118.77ms
step:2172/2245 train_time:257983ms step_avg:118.78ms
step:2173/2245 train_time:258100ms step_avg:118.78ms
step:2174/2245 train_time:258224ms step_avg:118.78ms
step:2175/2245 train_time:258341ms step_avg:118.78ms
step:2176/2245 train_time:258464ms step_avg:118.78ms
step:2177/2245 train_time:258581ms step_avg:118.78ms
step:2178/2245 train_time:258703ms step_avg:118.78ms
step:2179/2245 train_time:258820ms step_avg:118.78ms
step:2180/2245 train_time:258944ms step_avg:118.78ms
step:2181/2245 train_time:259061ms step_avg:118.78ms
step:2182/2245 train_time:259184ms step_avg:118.78ms
step:2183/2245 train_time:259301ms step_avg:118.78ms
step:2184/2245 train_time:259424ms step_avg:118.78ms
step:2185/2245 train_time:259542ms step_avg:118.78ms
step:2186/2245 train_time:259665ms step_avg:118.79ms
step:2187/2245 train_time:259781ms step_avg:118.78ms
step:2188/2245 train_time:259904ms step_avg:118.79ms
step:2189/2245 train_time:260021ms step_avg:118.79ms
step:2190/2245 train_time:260144ms step_avg:118.79ms
step:2191/2245 train_time:260261ms step_avg:118.79ms
step:2192/2245 train_time:260384ms step_avg:118.79ms
step:2193/2245 train_time:260501ms step_avg:118.79ms
step:2194/2245 train_time:260625ms step_avg:118.79ms
step:2195/2245 train_time:260741ms step_avg:118.79ms
step:2196/2245 train_time:260865ms step_avg:118.79ms
step:2197/2245 train_time:260981ms step_avg:118.79ms
step:2198/2245 train_time:261104ms step_avg:118.79ms
step:2199/2245 train_time:261222ms step_avg:118.79ms
step:2200/2245 train_time:261345ms step_avg:118.79ms
step:2201/2245 train_time:261461ms step_avg:118.79ms
step:2202/2245 train_time:261585ms step_avg:118.79ms
step:2203/2245 train_time:261702ms step_avg:118.79ms
step:2204/2245 train_time:261825ms step_avg:118.80ms
step:2205/2245 train_time:261942ms step_avg:118.79ms
step:2206/2245 train_time:262065ms step_avg:118.80ms
step:2207/2245 train_time:262182ms step_avg:118.80ms
step:2208/2245 train_time:262305ms step_avg:118.80ms
step:2209/2245 train_time:262423ms step_avg:118.80ms
step:2210/2245 train_time:262547ms step_avg:118.80ms
step:2211/2245 train_time:262663ms step_avg:118.80ms
step:2212/2245 train_time:262788ms step_avg:118.80ms
step:2213/2245 train_time:262905ms step_avg:118.80ms
step:2214/2245 train_time:263028ms step_avg:118.80ms
step:2215/2245 train_time:263145ms step_avg:118.80ms
step:2216/2245 train_time:263268ms step_avg:118.80ms
step:2217/2245 train_time:263385ms step_avg:118.80ms
step:2218/2245 train_time:263509ms step_avg:118.80ms
step:2219/2245 train_time:263625ms step_avg:118.80ms
step:2220/2245 train_time:263749ms step_avg:118.81ms
step:2221/2245 train_time:263865ms step_avg:118.80ms
step:2222/2245 train_time:263988ms step_avg:118.81ms
step:2223/2245 train_time:264106ms step_avg:118.81ms
step:2224/2245 train_time:264229ms step_avg:118.81ms
step:2225/2245 train_time:264346ms step_avg:118.81ms
step:2226/2245 train_time:264470ms step_avg:118.81ms
step:2227/2245 train_time:264587ms step_avg:118.81ms
step:2228/2245 train_time:264710ms step_avg:118.81ms
step:2229/2245 train_time:264827ms step_avg:118.81ms
step:2230/2245 train_time:264950ms step_avg:118.81ms
step:2231/2245 train_time:265067ms step_avg:118.81ms
step:2232/2245 train_time:265191ms step_avg:118.81ms
step:2233/2245 train_time:265308ms step_avg:118.81ms
step:2234/2245 train_time:265431ms step_avg:118.81ms
step:2235/2245 train_time:265548ms step_avg:118.81ms
step:2236/2245 train_time:265670ms step_avg:118.82ms
step:2237/2245 train_time:265787ms step_avg:118.81ms
step:2238/2245 train_time:265911ms step_avg:118.82ms
step:2239/2245 train_time:266028ms step_avg:118.82ms
step:2240/2245 train_time:266151ms step_avg:118.82ms
step:2241/2245 train_time:266268ms step_avg:118.82ms
step:2242/2245 train_time:266391ms step_avg:118.82ms
step:2243/2245 train_time:266508ms step_avg:118.82ms
step:2244/2245 train_time:266631ms step_avg:118.82ms
step:2245/2245 train_time:266747ms step_avg:118.82ms
step:2245/2245 val_loss:3.2795 train_time:266816ms step_avg:118.85ms
peak memory allocated: 29039 MiB reserved: 44436 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 10:10:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   42C    P0            116W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   42C    P0            118W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   42C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3341519      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3341520      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3341521      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3341522      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3341520      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3341521      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3341522      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:164ms step_avg:163.59ms
step:2/2245 train_time:218ms step_avg:108.95ms
step:3/2245 train_time:327ms step_avg:108.98ms
step:4/2245 train_time:445ms step_avg:111.29ms
step:5/2245 train_time:561ms step_avg:112.14ms
step:6/2245 train_time:683ms step_avg:113.88ms
step:7/2245 train_time:799ms step_avg:114.17ms
step:8/2245 train_time:922ms step_avg:115.22ms
step:9/2245 train_time:1038ms step_avg:115.30ms
step:10/2245 train_time:1161ms step_avg:116.10ms
step:11/2245 train_time:1278ms step_avg:116.15ms
step:12/2245 train_time:1400ms step_avg:116.71ms
step:13/2245 train_time:1517ms step_avg:116.69ms
step:14/2245 train_time:1640ms step_avg:117.16ms
step:15/2245 train_time:1757ms step_avg:117.12ms
step:16/2245 train_time:1880ms step_avg:117.51ms
step:17/2245 train_time:1997ms step_avg:117.47ms
step:18/2245 train_time:2120ms step_avg:117.77ms
step:19/2245 train_time:2237ms step_avg:117.71ms
step:20/2245 train_time:2360ms step_avg:118.00ms
step:21/2245 train_time:2476ms step_avg:117.90ms
step:22/2245 train_time:2599ms step_avg:118.13ms
step:23/2245 train_time:2715ms step_avg:118.06ms
step:24/2245 train_time:2838ms step_avg:118.27ms
step:25/2245 train_time:2955ms step_avg:118.19ms
step:26/2245 train_time:3078ms step_avg:118.37ms
step:27/2245 train_time:3195ms step_avg:118.32ms
step:28/2245 train_time:3318ms step_avg:118.49ms
step:29/2245 train_time:3434ms step_avg:118.40ms
step:30/2245 train_time:3556ms step_avg:118.55ms
step:31/2245 train_time:3673ms step_avg:118.48ms
step:32/2245 train_time:3796ms step_avg:118.63ms
step:33/2245 train_time:3913ms step_avg:118.58ms
step:34/2245 train_time:4035ms step_avg:118.68ms
step:35/2245 train_time:4152ms step_avg:118.62ms
step:36/2245 train_time:4274ms step_avg:118.73ms
step:37/2245 train_time:4391ms step_avg:118.68ms
step:38/2245 train_time:4514ms step_avg:118.79ms
step:39/2245 train_time:4630ms step_avg:118.73ms
step:40/2245 train_time:4753ms step_avg:118.82ms
step:41/2245 train_time:4869ms step_avg:118.76ms
step:42/2245 train_time:4991ms step_avg:118.83ms
step:43/2245 train_time:5107ms step_avg:118.76ms
step:44/2245 train_time:5229ms step_avg:118.83ms
step:45/2245 train_time:5344ms step_avg:118.76ms
step:46/2245 train_time:5466ms step_avg:118.84ms
step:47/2245 train_time:5583ms step_avg:118.78ms
step:48/2245 train_time:5704ms step_avg:118.84ms
step:49/2245 train_time:5820ms step_avg:118.78ms
step:50/2245 train_time:5942ms step_avg:118.84ms
step:51/2245 train_time:6057ms step_avg:118.77ms
step:52/2245 train_time:6179ms step_avg:118.83ms
step:53/2245 train_time:6294ms step_avg:118.76ms
step:54/2245 train_time:6416ms step_avg:118.82ms
step:55/2245 train_time:6532ms step_avg:118.76ms
step:56/2245 train_time:6654ms step_avg:118.82ms
step:57/2245 train_time:6769ms step_avg:118.76ms
step:58/2245 train_time:6891ms step_avg:118.81ms
step:59/2245 train_time:7006ms step_avg:118.75ms
step:60/2245 train_time:7128ms step_avg:118.80ms
step:61/2245 train_time:7244ms step_avg:118.75ms
step:62/2245 train_time:7365ms step_avg:118.79ms
step:63/2245 train_time:7480ms step_avg:118.74ms
step:64/2245 train_time:7602ms step_avg:118.78ms
step:65/2245 train_time:7717ms step_avg:118.73ms
step:66/2245 train_time:7839ms step_avg:118.78ms
step:67/2245 train_time:7955ms step_avg:118.73ms
step:68/2245 train_time:8076ms step_avg:118.77ms
step:69/2245 train_time:8192ms step_avg:118.72ms
step:70/2245 train_time:8314ms step_avg:118.77ms
step:71/2245 train_time:8429ms step_avg:118.71ms
step:72/2245 train_time:8550ms step_avg:118.75ms
step:73/2245 train_time:8666ms step_avg:118.71ms
step:74/2245 train_time:8787ms step_avg:118.74ms
step:75/2245 train_time:8902ms step_avg:118.70ms
step:76/2245 train_time:9024ms step_avg:118.73ms
step:77/2245 train_time:9139ms step_avg:118.69ms
step:78/2245 train_time:9261ms step_avg:118.73ms
step:79/2245 train_time:9376ms step_avg:118.68ms
step:80/2245 train_time:9497ms step_avg:118.72ms
step:81/2245 train_time:9613ms step_avg:118.68ms
step:82/2245 train_time:9735ms step_avg:118.72ms
step:83/2245 train_time:9849ms step_avg:118.67ms
step:84/2245 train_time:9971ms step_avg:118.70ms
step:85/2245 train_time:10086ms step_avg:118.66ms
step:86/2245 train_time:10208ms step_avg:118.69ms
step:87/2245 train_time:10323ms step_avg:118.65ms
step:88/2245 train_time:10444ms step_avg:118.68ms
step:89/2245 train_time:10559ms step_avg:118.64ms
step:90/2245 train_time:10680ms step_avg:118.67ms
step:91/2245 train_time:10795ms step_avg:118.63ms
step:92/2245 train_time:10917ms step_avg:118.66ms
step:93/2245 train_time:11032ms step_avg:118.62ms
step:94/2245 train_time:11153ms step_avg:118.65ms
step:95/2245 train_time:11268ms step_avg:118.61ms
step:96/2245 train_time:11390ms step_avg:118.64ms
step:97/2245 train_time:11504ms step_avg:118.60ms
step:98/2245 train_time:11626ms step_avg:118.63ms
step:99/2245 train_time:11741ms step_avg:118.59ms
step:100/2245 train_time:11862ms step_avg:118.62ms
step:101/2245 train_time:11977ms step_avg:118.58ms
step:102/2245 train_time:12098ms step_avg:118.61ms
step:103/2245 train_time:12213ms step_avg:118.57ms
step:104/2245 train_time:12334ms step_avg:118.60ms
step:105/2245 train_time:12449ms step_avg:118.56ms
step:106/2245 train_time:12570ms step_avg:118.59ms
step:107/2245 train_time:12685ms step_avg:118.55ms
step:108/2245 train_time:12806ms step_avg:118.57ms
step:109/2245 train_time:12921ms step_avg:118.54ms
step:110/2245 train_time:13042ms step_avg:118.56ms
step:111/2245 train_time:13156ms step_avg:118.52ms
step:112/2245 train_time:13277ms step_avg:118.55ms
step:113/2245 train_time:13392ms step_avg:118.52ms
step:114/2245 train_time:13514ms step_avg:118.54ms
step:115/2245 train_time:13628ms step_avg:118.51ms
step:116/2245 train_time:13749ms step_avg:118.53ms
step:117/2245 train_time:13864ms step_avg:118.50ms
step:118/2245 train_time:13985ms step_avg:118.52ms
step:119/2245 train_time:14100ms step_avg:118.49ms
step:120/2245 train_time:14221ms step_avg:118.51ms
step:121/2245 train_time:14335ms step_avg:118.47ms
step:122/2245 train_time:14456ms step_avg:118.49ms
step:123/2245 train_time:14571ms step_avg:118.46ms
step:124/2245 train_time:14692ms step_avg:118.48ms
step:125/2245 train_time:14807ms step_avg:118.45ms
step:126/2245 train_time:14927ms step_avg:118.47ms
step:127/2245 train_time:15042ms step_avg:118.44ms
step:128/2245 train_time:15163ms step_avg:118.46ms
step:129/2245 train_time:15278ms step_avg:118.44ms
step:130/2245 train_time:15399ms step_avg:118.45ms
step:131/2245 train_time:15514ms step_avg:118.42ms
step:132/2245 train_time:15635ms step_avg:118.44ms
step:133/2245 train_time:15749ms step_avg:118.41ms
step:134/2245 train_time:15870ms step_avg:118.43ms
step:135/2245 train_time:15985ms step_avg:118.41ms
step:136/2245 train_time:16106ms step_avg:118.43ms
step:137/2245 train_time:16221ms step_avg:118.40ms
step:138/2245 train_time:16341ms step_avg:118.42ms
step:139/2245 train_time:16456ms step_avg:118.39ms
step:140/2245 train_time:16577ms step_avg:118.41ms
step:141/2245 train_time:16692ms step_avg:118.38ms
step:142/2245 train_time:16813ms step_avg:118.40ms
step:143/2245 train_time:16927ms step_avg:118.37ms
step:144/2245 train_time:17048ms step_avg:118.39ms
step:145/2245 train_time:17163ms step_avg:118.36ms
step:146/2245 train_time:17284ms step_avg:118.38ms
step:147/2245 train_time:17398ms step_avg:118.36ms
step:148/2245 train_time:17519ms step_avg:118.37ms
step:149/2245 train_time:17634ms step_avg:118.35ms
step:150/2245 train_time:17755ms step_avg:118.37ms
step:151/2245 train_time:17869ms step_avg:118.34ms
step:152/2245 train_time:17990ms step_avg:118.36ms
step:153/2245 train_time:18105ms step_avg:118.33ms
step:154/2245 train_time:18225ms step_avg:118.35ms
step:155/2245 train_time:18340ms step_avg:118.33ms
step:156/2245 train_time:18461ms step_avg:118.34ms
step:157/2245 train_time:18575ms step_avg:118.31ms
step:158/2245 train_time:18696ms step_avg:118.33ms
step:159/2245 train_time:18811ms step_avg:118.31ms
step:160/2245 train_time:18932ms step_avg:118.32ms
step:161/2245 train_time:19046ms step_avg:118.30ms
step:162/2245 train_time:19167ms step_avg:118.31ms
step:163/2245 train_time:19281ms step_avg:118.29ms
step:164/2245 train_time:19402ms step_avg:118.31ms
step:165/2245 train_time:19516ms step_avg:118.28ms
step:166/2245 train_time:19637ms step_avg:118.30ms
step:167/2245 train_time:19752ms step_avg:118.28ms
step:168/2245 train_time:19873ms step_avg:118.29ms
step:169/2245 train_time:19987ms step_avg:118.27ms
step:170/2245 train_time:20108ms step_avg:118.28ms
step:171/2245 train_time:20222ms step_avg:118.26ms
step:172/2245 train_time:20343ms step_avg:118.27ms
step:173/2245 train_time:20457ms step_avg:118.25ms
step:174/2245 train_time:20578ms step_avg:118.26ms
step:175/2245 train_time:20692ms step_avg:118.24ms
step:176/2245 train_time:20813ms step_avg:118.26ms
step:177/2245 train_time:20927ms step_avg:118.23ms
step:178/2245 train_time:21048ms step_avg:118.25ms
step:179/2245 train_time:21162ms step_avg:118.22ms
step:180/2245 train_time:21282ms step_avg:118.24ms
step:181/2245 train_time:21397ms step_avg:118.21ms
step:182/2245 train_time:21517ms step_avg:118.23ms
step:183/2245 train_time:21632ms step_avg:118.21ms
step:184/2245 train_time:21752ms step_avg:118.22ms
step:185/2245 train_time:21866ms step_avg:118.20ms
step:186/2245 train_time:21987ms step_avg:118.21ms
step:187/2245 train_time:22101ms step_avg:118.19ms
step:188/2245 train_time:22222ms step_avg:118.20ms
step:189/2245 train_time:22336ms step_avg:118.18ms
step:190/2245 train_time:22457ms step_avg:118.19ms
step:191/2245 train_time:22571ms step_avg:118.18ms
step:192/2245 train_time:22692ms step_avg:118.19ms
step:193/2245 train_time:22807ms step_avg:118.17ms
step:194/2245 train_time:22927ms step_avg:118.18ms
step:195/2245 train_time:23042ms step_avg:118.16ms
step:196/2245 train_time:23162ms step_avg:118.17ms
step:197/2245 train_time:23276ms step_avg:118.15ms
step:198/2245 train_time:23397ms step_avg:118.16ms
step:199/2245 train_time:23511ms step_avg:118.15ms
step:200/2245 train_time:23632ms step_avg:118.16ms
step:201/2245 train_time:23746ms step_avg:118.14ms
step:202/2245 train_time:23867ms step_avg:118.15ms
step:203/2245 train_time:23981ms step_avg:118.13ms
step:204/2245 train_time:24101ms step_avg:118.14ms
step:205/2245 train_time:24216ms step_avg:118.13ms
step:206/2245 train_time:24337ms step_avg:118.14ms
step:207/2245 train_time:24451ms step_avg:118.12ms
step:208/2245 train_time:24572ms step_avg:118.13ms
step:209/2245 train_time:24686ms step_avg:118.12ms
step:210/2245 train_time:24807ms step_avg:118.13ms
step:211/2245 train_time:24921ms step_avg:118.11ms
step:212/2245 train_time:25041ms step_avg:118.12ms
step:213/2245 train_time:25156ms step_avg:118.10ms
step:214/2245 train_time:25276ms step_avg:118.11ms
step:215/2245 train_time:25391ms step_avg:118.10ms
step:216/2245 train_time:25511ms step_avg:118.11ms
step:217/2245 train_time:25625ms step_avg:118.09ms
step:218/2245 train_time:25746ms step_avg:118.10ms
step:219/2245 train_time:25860ms step_avg:118.08ms
step:220/2245 train_time:25980ms step_avg:118.09ms
step:221/2245 train_time:26095ms step_avg:118.08ms
step:222/2245 train_time:26216ms step_avg:118.09ms
step:223/2245 train_time:26330ms step_avg:118.07ms
step:224/2245 train_time:26451ms step_avg:118.08ms
step:225/2245 train_time:26565ms step_avg:118.07ms
step:226/2245 train_time:26685ms step_avg:118.08ms
step:227/2245 train_time:26800ms step_avg:118.06ms
step:228/2245 train_time:26920ms step_avg:118.07ms
step:229/2245 train_time:27034ms step_avg:118.05ms
step:230/2245 train_time:27155ms step_avg:118.06ms
step:231/2245 train_time:27269ms step_avg:118.05ms
step:232/2245 train_time:27389ms step_avg:118.06ms
step:233/2245 train_time:27503ms step_avg:118.04ms
step:234/2245 train_time:27624ms step_avg:118.05ms
step:235/2245 train_time:27739ms step_avg:118.04ms
step:236/2245 train_time:27859ms step_avg:118.05ms
step:237/2245 train_time:27973ms step_avg:118.03ms
step:238/2245 train_time:28094ms step_avg:118.04ms
step:239/2245 train_time:28208ms step_avg:118.03ms
step:240/2245 train_time:28328ms step_avg:118.03ms
step:241/2245 train_time:28442ms step_avg:118.02ms
step:242/2245 train_time:28563ms step_avg:118.03ms
step:243/2245 train_time:28677ms step_avg:118.01ms
step:244/2245 train_time:28797ms step_avg:118.02ms
step:245/2245 train_time:28912ms step_avg:118.01ms
step:246/2245 train_time:29032ms step_avg:118.02ms
step:247/2245 train_time:29146ms step_avg:118.00ms
step:248/2245 train_time:29267ms step_avg:118.01ms
step:249/2245 train_time:29382ms step_avg:118.00ms
step:250/2245 train_time:29502ms step_avg:118.01ms
step:250/2245 val_loss:4.0977 train_time:29567ms step_avg:118.27ms
step:251/2245 train_time:29617ms step_avg:118.00ms
step:252/2245 train_time:29737ms step_avg:118.00ms
step:253/2245 train_time:29851ms step_avg:117.99ms
step:254/2245 train_time:29971ms step_avg:118.00ms
step:255/2245 train_time:30085ms step_avg:117.98ms
step:256/2245 train_time:30206ms step_avg:117.99ms
step:257/2245 train_time:30320ms step_avg:117.98ms
step:258/2245 train_time:30441ms step_avg:117.99ms
step:259/2245 train_time:30555ms step_avg:117.97ms
step:260/2245 train_time:30675ms step_avg:117.98ms
step:261/2245 train_time:30789ms step_avg:117.96ms
step:262/2245 train_time:30909ms step_avg:117.97ms
step:263/2245 train_time:31024ms step_avg:117.96ms
step:264/2245 train_time:31144ms step_avg:117.97ms
step:265/2245 train_time:31258ms step_avg:117.96ms
step:266/2245 train_time:31378ms step_avg:117.96ms
step:267/2245 train_time:31493ms step_avg:117.95ms
step:268/2245 train_time:31613ms step_avg:117.96ms
step:269/2245 train_time:31727ms step_avg:117.94ms
step:270/2245 train_time:31847ms step_avg:117.95ms
step:271/2245 train_time:31961ms step_avg:117.94ms
step:272/2245 train_time:32082ms step_avg:117.95ms
step:273/2245 train_time:32196ms step_avg:117.94ms
step:274/2245 train_time:32317ms step_avg:117.94ms
step:275/2245 train_time:32431ms step_avg:117.93ms
step:276/2245 train_time:32551ms step_avg:117.94ms
step:277/2245 train_time:32666ms step_avg:117.93ms
step:278/2245 train_time:32786ms step_avg:117.94ms
step:279/2245 train_time:32900ms step_avg:117.92ms
step:280/2245 train_time:33021ms step_avg:117.93ms
step:281/2245 train_time:33135ms step_avg:117.92ms
step:282/2245 train_time:33255ms step_avg:117.93ms
step:283/2245 train_time:33370ms step_avg:117.91ms
step:284/2245 train_time:33490ms step_avg:117.92ms
step:285/2245 train_time:33604ms step_avg:117.91ms
step:286/2245 train_time:33725ms step_avg:117.92ms
step:287/2245 train_time:33838ms step_avg:117.90ms
step:288/2245 train_time:33959ms step_avg:117.91ms
step:289/2245 train_time:34073ms step_avg:117.90ms
step:290/2245 train_time:34194ms step_avg:117.91ms
step:291/2245 train_time:34308ms step_avg:117.90ms
step:292/2245 train_time:34428ms step_avg:117.90ms
step:293/2245 train_time:34542ms step_avg:117.89ms
step:294/2245 train_time:34662ms step_avg:117.90ms
step:295/2245 train_time:34777ms step_avg:117.89ms
step:296/2245 train_time:34897ms step_avg:117.90ms
step:297/2245 train_time:35011ms step_avg:117.88ms
step:298/2245 train_time:35132ms step_avg:117.89ms
step:299/2245 train_time:35246ms step_avg:117.88ms
step:300/2245 train_time:35366ms step_avg:117.89ms
step:301/2245 train_time:35481ms step_avg:117.88ms
step:302/2245 train_time:35601ms step_avg:117.89ms
step:303/2245 train_time:35716ms step_avg:117.87ms
step:304/2245 train_time:35837ms step_avg:117.88ms
step:305/2245 train_time:35950ms step_avg:117.87ms
step:306/2245 train_time:36071ms step_avg:117.88ms
step:307/2245 train_time:36185ms step_avg:117.87ms
step:308/2245 train_time:36306ms step_avg:117.88ms
step:309/2245 train_time:36420ms step_avg:117.86ms
step:310/2245 train_time:36540ms step_avg:117.87ms
step:311/2245 train_time:36654ms step_avg:117.86ms
step:312/2245 train_time:36775ms step_avg:117.87ms
step:313/2245 train_time:36889ms step_avg:117.86ms
step:314/2245 train_time:37009ms step_avg:117.86ms
step:315/2245 train_time:37123ms step_avg:117.85ms
step:316/2245 train_time:37244ms step_avg:117.86ms
step:317/2245 train_time:37358ms step_avg:117.85ms
step:318/2245 train_time:37478ms step_avg:117.86ms
step:319/2245 train_time:37592ms step_avg:117.84ms
step:320/2245 train_time:37713ms step_avg:117.85ms
step:321/2245 train_time:37827ms step_avg:117.84ms
step:322/2245 train_time:37947ms step_avg:117.85ms
step:323/2245 train_time:38061ms step_avg:117.84ms
step:324/2245 train_time:38182ms step_avg:117.84ms
step:325/2245 train_time:38296ms step_avg:117.83ms
step:326/2245 train_time:38417ms step_avg:117.84ms
step:327/2245 train_time:38530ms step_avg:117.83ms
step:328/2245 train_time:38651ms step_avg:117.84ms
step:329/2245 train_time:38765ms step_avg:117.83ms
step:330/2245 train_time:38886ms step_avg:117.84ms
step:331/2245 train_time:39000ms step_avg:117.82ms
step:332/2245 train_time:39120ms step_avg:117.83ms
step:333/2245 train_time:39234ms step_avg:117.82ms
step:334/2245 train_time:39355ms step_avg:117.83ms
step:335/2245 train_time:39469ms step_avg:117.82ms
step:336/2245 train_time:39589ms step_avg:117.82ms
step:337/2245 train_time:39704ms step_avg:117.81ms
step:338/2245 train_time:39825ms step_avg:117.82ms
step:339/2245 train_time:39939ms step_avg:117.81ms
step:340/2245 train_time:40059ms step_avg:117.82ms
step:341/2245 train_time:40173ms step_avg:117.81ms
step:342/2245 train_time:40294ms step_avg:117.82ms
step:343/2245 train_time:40407ms step_avg:117.81ms
step:344/2245 train_time:40528ms step_avg:117.81ms
step:345/2245 train_time:40642ms step_avg:117.80ms
step:346/2245 train_time:40762ms step_avg:117.81ms
step:347/2245 train_time:40877ms step_avg:117.80ms
step:348/2245 train_time:40997ms step_avg:117.81ms
step:349/2245 train_time:41111ms step_avg:117.80ms
step:350/2245 train_time:41231ms step_avg:117.80ms
step:351/2245 train_time:41345ms step_avg:117.79ms
step:352/2245 train_time:41466ms step_avg:117.80ms
step:353/2245 train_time:41580ms step_avg:117.79ms
step:354/2245 train_time:41701ms step_avg:117.80ms
step:355/2245 train_time:41815ms step_avg:117.79ms
step:356/2245 train_time:41935ms step_avg:117.79ms
step:357/2245 train_time:42049ms step_avg:117.78ms
step:358/2245 train_time:42169ms step_avg:117.79ms
step:359/2245 train_time:42284ms step_avg:117.78ms
step:360/2245 train_time:42404ms step_avg:117.79ms
step:361/2245 train_time:42518ms step_avg:117.78ms
step:362/2245 train_time:42639ms step_avg:117.79ms
step:363/2245 train_time:42753ms step_avg:117.78ms
step:364/2245 train_time:42873ms step_avg:117.78ms
step:365/2245 train_time:42987ms step_avg:117.77ms
step:366/2245 train_time:43108ms step_avg:117.78ms
step:367/2245 train_time:43222ms step_avg:117.77ms
step:368/2245 train_time:43343ms step_avg:117.78ms
step:369/2245 train_time:43457ms step_avg:117.77ms
step:370/2245 train_time:43578ms step_avg:117.78ms
step:371/2245 train_time:43692ms step_avg:117.77ms
step:372/2245 train_time:43812ms step_avg:117.77ms
step:373/2245 train_time:43926ms step_avg:117.76ms
step:374/2245 train_time:44046ms step_avg:117.77ms
step:375/2245 train_time:44160ms step_avg:117.76ms
step:376/2245 train_time:44281ms step_avg:117.77ms
step:377/2245 train_time:44395ms step_avg:117.76ms
step:378/2245 train_time:44516ms step_avg:117.77ms
step:379/2245 train_time:44630ms step_avg:117.76ms
step:380/2245 train_time:44750ms step_avg:117.76ms
step:381/2245 train_time:44864ms step_avg:117.75ms
step:382/2245 train_time:44984ms step_avg:117.76ms
step:383/2245 train_time:45099ms step_avg:117.75ms
step:384/2245 train_time:45219ms step_avg:117.76ms
step:385/2245 train_time:45334ms step_avg:117.75ms
step:386/2245 train_time:45454ms step_avg:117.76ms
step:387/2245 train_time:45568ms step_avg:117.75ms
step:388/2245 train_time:45688ms step_avg:117.75ms
step:389/2245 train_time:45803ms step_avg:117.74ms
step:390/2245 train_time:45923ms step_avg:117.75ms
step:391/2245 train_time:46037ms step_avg:117.74ms
step:392/2245 train_time:46158ms step_avg:117.75ms
step:393/2245 train_time:46272ms step_avg:117.74ms
step:394/2245 train_time:46392ms step_avg:117.75ms
step:395/2245 train_time:46506ms step_avg:117.74ms
step:396/2245 train_time:46627ms step_avg:117.74ms
step:397/2245 train_time:46740ms step_avg:117.73ms
step:398/2245 train_time:46860ms step_avg:117.74ms
step:399/2245 train_time:46975ms step_avg:117.73ms
step:400/2245 train_time:47096ms step_avg:117.74ms
step:401/2245 train_time:47210ms step_avg:117.73ms
step:402/2245 train_time:47330ms step_avg:117.74ms
step:403/2245 train_time:47444ms step_avg:117.73ms
step:404/2245 train_time:47565ms step_avg:117.73ms
step:405/2245 train_time:47678ms step_avg:117.72ms
step:406/2245 train_time:47799ms step_avg:117.73ms
step:407/2245 train_time:47913ms step_avg:117.72ms
step:408/2245 train_time:48033ms step_avg:117.73ms
step:409/2245 train_time:48147ms step_avg:117.72ms
step:410/2245 train_time:48267ms step_avg:117.73ms
step:411/2245 train_time:48381ms step_avg:117.72ms
step:412/2245 train_time:48502ms step_avg:117.72ms
step:413/2245 train_time:48616ms step_avg:117.71ms
step:414/2245 train_time:48736ms step_avg:117.72ms
step:415/2245 train_time:48851ms step_avg:117.71ms
step:416/2245 train_time:48971ms step_avg:117.72ms
step:417/2245 train_time:49085ms step_avg:117.71ms
step:418/2245 train_time:49205ms step_avg:117.72ms
step:419/2245 train_time:49320ms step_avg:117.71ms
step:420/2245 train_time:49440ms step_avg:117.71ms
step:421/2245 train_time:49554ms step_avg:117.71ms
step:422/2245 train_time:49675ms step_avg:117.71ms
step:423/2245 train_time:49789ms step_avg:117.70ms
step:424/2245 train_time:49909ms step_avg:117.71ms
step:425/2245 train_time:50023ms step_avg:117.70ms
step:426/2245 train_time:50143ms step_avg:117.71ms
step:427/2245 train_time:50257ms step_avg:117.70ms
step:428/2245 train_time:50378ms step_avg:117.70ms
step:429/2245 train_time:50492ms step_avg:117.70ms
step:430/2245 train_time:50612ms step_avg:117.70ms
step:431/2245 train_time:50726ms step_avg:117.69ms
step:432/2245 train_time:50847ms step_avg:117.70ms
step:433/2245 train_time:50961ms step_avg:117.69ms
step:434/2245 train_time:51081ms step_avg:117.70ms
step:435/2245 train_time:51195ms step_avg:117.69ms
step:436/2245 train_time:51315ms step_avg:117.70ms
step:437/2245 train_time:51429ms step_avg:117.69ms
step:438/2245 train_time:51549ms step_avg:117.69ms
step:439/2245 train_time:51664ms step_avg:117.68ms
step:440/2245 train_time:51784ms step_avg:117.69ms
step:441/2245 train_time:51897ms step_avg:117.68ms
step:442/2245 train_time:52018ms step_avg:117.69ms
step:443/2245 train_time:52132ms step_avg:117.68ms
step:444/2245 train_time:52252ms step_avg:117.69ms
step:445/2245 train_time:52366ms step_avg:117.68ms
step:446/2245 train_time:52487ms step_avg:117.68ms
step:447/2245 train_time:52601ms step_avg:117.68ms
step:448/2245 train_time:52721ms step_avg:117.68ms
step:449/2245 train_time:52836ms step_avg:117.67ms
step:450/2245 train_time:52956ms step_avg:117.68ms
step:451/2245 train_time:53070ms step_avg:117.67ms
step:452/2245 train_time:53190ms step_avg:117.68ms
step:453/2245 train_time:53304ms step_avg:117.67ms
step:454/2245 train_time:53424ms step_avg:117.68ms
step:455/2245 train_time:53539ms step_avg:117.67ms
step:456/2245 train_time:53659ms step_avg:117.67ms
step:457/2245 train_time:53773ms step_avg:117.66ms
step:458/2245 train_time:53893ms step_avg:117.67ms
step:459/2245 train_time:54007ms step_avg:117.66ms
step:460/2245 train_time:54127ms step_avg:117.67ms
step:461/2245 train_time:54242ms step_avg:117.66ms
step:462/2245 train_time:54363ms step_avg:117.67ms
step:463/2245 train_time:54477ms step_avg:117.66ms
step:464/2245 train_time:54598ms step_avg:117.67ms
step:465/2245 train_time:54711ms step_avg:117.66ms
step:466/2245 train_time:54831ms step_avg:117.66ms
step:467/2245 train_time:54945ms step_avg:117.66ms
step:468/2245 train_time:55066ms step_avg:117.66ms
step:469/2245 train_time:55180ms step_avg:117.65ms
step:470/2245 train_time:55301ms step_avg:117.66ms
step:471/2245 train_time:55415ms step_avg:117.65ms
step:472/2245 train_time:55535ms step_avg:117.66ms
step:473/2245 train_time:55649ms step_avg:117.65ms
step:474/2245 train_time:55769ms step_avg:117.66ms
step:475/2245 train_time:55884ms step_avg:117.65ms
step:476/2245 train_time:56004ms step_avg:117.66ms
step:477/2245 train_time:56118ms step_avg:117.65ms
step:478/2245 train_time:56238ms step_avg:117.65ms
step:479/2245 train_time:56352ms step_avg:117.65ms
step:480/2245 train_time:56473ms step_avg:117.65ms
step:481/2245 train_time:56587ms step_avg:117.64ms
step:482/2245 train_time:56707ms step_avg:117.65ms
step:483/2245 train_time:56821ms step_avg:117.64ms
step:484/2245 train_time:56941ms step_avg:117.65ms
step:485/2245 train_time:57055ms step_avg:117.64ms
step:486/2245 train_time:57176ms step_avg:117.65ms
step:487/2245 train_time:57290ms step_avg:117.64ms
step:488/2245 train_time:57410ms step_avg:117.64ms
step:489/2245 train_time:57524ms step_avg:117.64ms
step:490/2245 train_time:57644ms step_avg:117.64ms
step:491/2245 train_time:57758ms step_avg:117.63ms
step:492/2245 train_time:57878ms step_avg:117.64ms
step:493/2245 train_time:57992ms step_avg:117.63ms
step:494/2245 train_time:58113ms step_avg:117.64ms
step:495/2245 train_time:58226ms step_avg:117.63ms
step:496/2245 train_time:58347ms step_avg:117.64ms
step:497/2245 train_time:58462ms step_avg:117.63ms
step:498/2245 train_time:58582ms step_avg:117.63ms
step:499/2245 train_time:58696ms step_avg:117.63ms
step:500/2245 train_time:58817ms step_avg:117.63ms
step:500/2245 val_loss:3.8274 train_time:58882ms step_avg:117.76ms
step:501/2245 train_time:58931ms step_avg:117.63ms
step:502/2245 train_time:59051ms step_avg:117.63ms
step:503/2245 train_time:59165ms step_avg:117.62ms
step:504/2245 train_time:59286ms step_avg:117.63ms
step:505/2245 train_time:59400ms step_avg:117.62ms
step:506/2245 train_time:59520ms step_avg:117.63ms
step:507/2245 train_time:59634ms step_avg:117.62ms
step:508/2245 train_time:59755ms step_avg:117.63ms
step:509/2245 train_time:59869ms step_avg:117.62ms
step:510/2245 train_time:59989ms step_avg:117.63ms
step:511/2245 train_time:60103ms step_avg:117.62ms
step:512/2245 train_time:60223ms step_avg:117.62ms
step:513/2245 train_time:60337ms step_avg:117.62ms
step:514/2245 train_time:60457ms step_avg:117.62ms
step:515/2245 train_time:60572ms step_avg:117.62ms
step:516/2245 train_time:60692ms step_avg:117.62ms
step:517/2245 train_time:60806ms step_avg:117.61ms
step:518/2245 train_time:60927ms step_avg:117.62ms
step:519/2245 train_time:61041ms step_avg:117.61ms
step:520/2245 train_time:61162ms step_avg:117.62ms
step:521/2245 train_time:61276ms step_avg:117.61ms
step:522/2245 train_time:61396ms step_avg:117.62ms
step:523/2245 train_time:61510ms step_avg:117.61ms
step:524/2245 train_time:61630ms step_avg:117.62ms
step:525/2245 train_time:61744ms step_avg:117.61ms
step:526/2245 train_time:61865ms step_avg:117.61ms
step:527/2245 train_time:61979ms step_avg:117.61ms
step:528/2245 train_time:62099ms step_avg:117.61ms
step:529/2245 train_time:62214ms step_avg:117.61ms
step:530/2245 train_time:62334ms step_avg:117.61ms
step:531/2245 train_time:62448ms step_avg:117.60ms
step:532/2245 train_time:62568ms step_avg:117.61ms
step:533/2245 train_time:62682ms step_avg:117.60ms
step:534/2245 train_time:62802ms step_avg:117.61ms
step:535/2245 train_time:62916ms step_avg:117.60ms
step:536/2245 train_time:63036ms step_avg:117.61ms
step:537/2245 train_time:63150ms step_avg:117.60ms
step:538/2245 train_time:63271ms step_avg:117.60ms
step:539/2245 train_time:63385ms step_avg:117.60ms
step:540/2245 train_time:63506ms step_avg:117.60ms
step:541/2245 train_time:63620ms step_avg:117.60ms
step:542/2245 train_time:63740ms step_avg:117.60ms
step:543/2245 train_time:63854ms step_avg:117.59ms
step:544/2245 train_time:63974ms step_avg:117.60ms
step:545/2245 train_time:64089ms step_avg:117.59ms
step:546/2245 train_time:64209ms step_avg:117.60ms
step:547/2245 train_time:64323ms step_avg:117.59ms
step:548/2245 train_time:64443ms step_avg:117.60ms
step:549/2245 train_time:64557ms step_avg:117.59ms
step:550/2245 train_time:64677ms step_avg:117.59ms
step:551/2245 train_time:64791ms step_avg:117.59ms
step:552/2245 train_time:64911ms step_avg:117.59ms
step:553/2245 train_time:65025ms step_avg:117.59ms
step:554/2245 train_time:65146ms step_avg:117.59ms
step:555/2245 train_time:65260ms step_avg:117.59ms
step:556/2245 train_time:65381ms step_avg:117.59ms
step:557/2245 train_time:65495ms step_avg:117.59ms
step:558/2245 train_time:65615ms step_avg:117.59ms
step:559/2245 train_time:65729ms step_avg:117.58ms
step:560/2245 train_time:65850ms step_avg:117.59ms
step:561/2245 train_time:65964ms step_avg:117.58ms
step:562/2245 train_time:66084ms step_avg:117.59ms
step:563/2245 train_time:66198ms step_avg:117.58ms
step:564/2245 train_time:66319ms step_avg:117.59ms
step:565/2245 train_time:66433ms step_avg:117.58ms
step:566/2245 train_time:66553ms step_avg:117.58ms
step:567/2245 train_time:66667ms step_avg:117.58ms
step:568/2245 train_time:66788ms step_avg:117.58ms
step:569/2245 train_time:66902ms step_avg:117.58ms
step:570/2245 train_time:67022ms step_avg:117.58ms
step:571/2245 train_time:67136ms step_avg:117.58ms
step:572/2245 train_time:67257ms step_avg:117.58ms
step:573/2245 train_time:67371ms step_avg:117.58ms
step:574/2245 train_time:67492ms step_avg:117.58ms
step:575/2245 train_time:67605ms step_avg:117.57ms
step:576/2245 train_time:67726ms step_avg:117.58ms
step:577/2245 train_time:67840ms step_avg:117.57ms
step:578/2245 train_time:67961ms step_avg:117.58ms
step:579/2245 train_time:68075ms step_avg:117.57ms
step:580/2245 train_time:68195ms step_avg:117.58ms
step:581/2245 train_time:68309ms step_avg:117.57ms
step:582/2245 train_time:68429ms step_avg:117.58ms
step:583/2245 train_time:68543ms step_avg:117.57ms
step:584/2245 train_time:68664ms step_avg:117.57ms
step:585/2245 train_time:68778ms step_avg:117.57ms
step:586/2245 train_time:68898ms step_avg:117.57ms
step:587/2245 train_time:69012ms step_avg:117.57ms
step:588/2245 train_time:69133ms step_avg:117.57ms
step:589/2245 train_time:69247ms step_avg:117.57ms
step:590/2245 train_time:69367ms step_avg:117.57ms
step:591/2245 train_time:69481ms step_avg:117.57ms
step:592/2245 train_time:69602ms step_avg:117.57ms
step:593/2245 train_time:69716ms step_avg:117.56ms
step:594/2245 train_time:69836ms step_avg:117.57ms
step:595/2245 train_time:69951ms step_avg:117.56ms
step:596/2245 train_time:70071ms step_avg:117.57ms
step:597/2245 train_time:70185ms step_avg:117.56ms
step:598/2245 train_time:70305ms step_avg:117.57ms
step:599/2245 train_time:70420ms step_avg:117.56ms
step:600/2245 train_time:70540ms step_avg:117.57ms
step:601/2245 train_time:70655ms step_avg:117.56ms
step:602/2245 train_time:70774ms step_avg:117.57ms
step:603/2245 train_time:70889ms step_avg:117.56ms
step:604/2245 train_time:71009ms step_avg:117.56ms
step:605/2245 train_time:71123ms step_avg:117.56ms
step:606/2245 train_time:71243ms step_avg:117.56ms
step:607/2245 train_time:71357ms step_avg:117.56ms
step:608/2245 train_time:71478ms step_avg:117.56ms
step:609/2245 train_time:71592ms step_avg:117.56ms
step:610/2245 train_time:71712ms step_avg:117.56ms
step:611/2245 train_time:71826ms step_avg:117.56ms
step:612/2245 train_time:71947ms step_avg:117.56ms
step:613/2245 train_time:72061ms step_avg:117.55ms
step:614/2245 train_time:72182ms step_avg:117.56ms
step:615/2245 train_time:72296ms step_avg:117.55ms
step:616/2245 train_time:72417ms step_avg:117.56ms
step:617/2245 train_time:72531ms step_avg:117.55ms
step:618/2245 train_time:72651ms step_avg:117.56ms
step:619/2245 train_time:72765ms step_avg:117.55ms
step:620/2245 train_time:72886ms step_avg:117.56ms
step:621/2245 train_time:73000ms step_avg:117.55ms
step:622/2245 train_time:73120ms step_avg:117.56ms
step:623/2245 train_time:73234ms step_avg:117.55ms
step:624/2245 train_time:73355ms step_avg:117.56ms
step:625/2245 train_time:73469ms step_avg:117.55ms
step:626/2245 train_time:73589ms step_avg:117.55ms
step:627/2245 train_time:73703ms step_avg:117.55ms
step:628/2245 train_time:73824ms step_avg:117.55ms
step:629/2245 train_time:73938ms step_avg:117.55ms
step:630/2245 train_time:74058ms step_avg:117.55ms
step:631/2245 train_time:74173ms step_avg:117.55ms
step:632/2245 train_time:74293ms step_avg:117.55ms
step:633/2245 train_time:74407ms step_avg:117.55ms
step:634/2245 train_time:74527ms step_avg:117.55ms
step:635/2245 train_time:74642ms step_avg:117.55ms
step:636/2245 train_time:74762ms step_avg:117.55ms
step:637/2245 train_time:74876ms step_avg:117.55ms
step:638/2245 train_time:74997ms step_avg:117.55ms
step:639/2245 train_time:75111ms step_avg:117.55ms
step:640/2245 train_time:75232ms step_avg:117.55ms
step:641/2245 train_time:75346ms step_avg:117.54ms
step:642/2245 train_time:75466ms step_avg:117.55ms
step:643/2245 train_time:75580ms step_avg:117.54ms
step:644/2245 train_time:75701ms step_avg:117.55ms
step:645/2245 train_time:75815ms step_avg:117.54ms
step:646/2245 train_time:75935ms step_avg:117.55ms
step:647/2245 train_time:76049ms step_avg:117.54ms
step:648/2245 train_time:76170ms step_avg:117.55ms
step:649/2245 train_time:76284ms step_avg:117.54ms
step:650/2245 train_time:76404ms step_avg:117.54ms
step:651/2245 train_time:76518ms step_avg:117.54ms
step:652/2245 train_time:76639ms step_avg:117.54ms
step:653/2245 train_time:76752ms step_avg:117.54ms
step:654/2245 train_time:76872ms step_avg:117.54ms
step:655/2245 train_time:76986ms step_avg:117.54ms
step:656/2245 train_time:77107ms step_avg:117.54ms
step:657/2245 train_time:77221ms step_avg:117.54ms
step:658/2245 train_time:77341ms step_avg:117.54ms
step:659/2245 train_time:77455ms step_avg:117.53ms
step:660/2245 train_time:77576ms step_avg:117.54ms
step:661/2245 train_time:77690ms step_avg:117.53ms
step:662/2245 train_time:77810ms step_avg:117.54ms
step:663/2245 train_time:77924ms step_avg:117.53ms
step:664/2245 train_time:78045ms step_avg:117.54ms
step:665/2245 train_time:78159ms step_avg:117.53ms
step:666/2245 train_time:78279ms step_avg:117.54ms
step:667/2245 train_time:78393ms step_avg:117.53ms
step:668/2245 train_time:78514ms step_avg:117.54ms
step:669/2245 train_time:78628ms step_avg:117.53ms
step:670/2245 train_time:78748ms step_avg:117.53ms
step:671/2245 train_time:78862ms step_avg:117.53ms
step:672/2245 train_time:78983ms step_avg:117.53ms
step:673/2245 train_time:79097ms step_avg:117.53ms
step:674/2245 train_time:79217ms step_avg:117.53ms
step:675/2245 train_time:79331ms step_avg:117.53ms
step:676/2245 train_time:79451ms step_avg:117.53ms
step:677/2245 train_time:79565ms step_avg:117.53ms
step:678/2245 train_time:79686ms step_avg:117.53ms
step:679/2245 train_time:79800ms step_avg:117.53ms
step:680/2245 train_time:79921ms step_avg:117.53ms
step:681/2245 train_time:80035ms step_avg:117.53ms
step:682/2245 train_time:80155ms step_avg:117.53ms
step:683/2245 train_time:80269ms step_avg:117.52ms
step:684/2245 train_time:80389ms step_avg:117.53ms
step:685/2245 train_time:80503ms step_avg:117.52ms
step:686/2245 train_time:80624ms step_avg:117.53ms
step:687/2245 train_time:80738ms step_avg:117.52ms
step:688/2245 train_time:80858ms step_avg:117.53ms
step:689/2245 train_time:80973ms step_avg:117.52ms
step:690/2245 train_time:81094ms step_avg:117.53ms
step:691/2245 train_time:81208ms step_avg:117.52ms
step:692/2245 train_time:81328ms step_avg:117.53ms
step:693/2245 train_time:81442ms step_avg:117.52ms
step:694/2245 train_time:81562ms step_avg:117.52ms
step:695/2245 train_time:81676ms step_avg:117.52ms
step:696/2245 train_time:81796ms step_avg:117.52ms
step:697/2245 train_time:81911ms step_avg:117.52ms
step:698/2245 train_time:82031ms step_avg:117.52ms
step:699/2245 train_time:82145ms step_avg:117.52ms
step:700/2245 train_time:82265ms step_avg:117.52ms
step:701/2245 train_time:82380ms step_avg:117.52ms
step:702/2245 train_time:82500ms step_avg:117.52ms
step:703/2245 train_time:82614ms step_avg:117.52ms
step:704/2245 train_time:82734ms step_avg:117.52ms
step:705/2245 train_time:82849ms step_avg:117.52ms
step:706/2245 train_time:82969ms step_avg:117.52ms
step:707/2245 train_time:83083ms step_avg:117.51ms
step:708/2245 train_time:83204ms step_avg:117.52ms
step:709/2245 train_time:83318ms step_avg:117.51ms
step:710/2245 train_time:83438ms step_avg:117.52ms
step:711/2245 train_time:83552ms step_avg:117.51ms
step:712/2245 train_time:83672ms step_avg:117.52ms
step:713/2245 train_time:83786ms step_avg:117.51ms
step:714/2245 train_time:83906ms step_avg:117.52ms
step:715/2245 train_time:84021ms step_avg:117.51ms
step:716/2245 train_time:84141ms step_avg:117.52ms
step:717/2245 train_time:84256ms step_avg:117.51ms
step:718/2245 train_time:84377ms step_avg:117.52ms
step:719/2245 train_time:84491ms step_avg:117.51ms
step:720/2245 train_time:84611ms step_avg:117.52ms
step:721/2245 train_time:84725ms step_avg:117.51ms
step:722/2245 train_time:84845ms step_avg:117.51ms
step:723/2245 train_time:84959ms step_avg:117.51ms
step:724/2245 train_time:85080ms step_avg:117.51ms
step:725/2245 train_time:85194ms step_avg:117.51ms
step:726/2245 train_time:85314ms step_avg:117.51ms
step:727/2245 train_time:85428ms step_avg:117.51ms
step:728/2245 train_time:85548ms step_avg:117.51ms
step:729/2245 train_time:85663ms step_avg:117.51ms
step:730/2245 train_time:85783ms step_avg:117.51ms
step:731/2245 train_time:85897ms step_avg:117.51ms
step:732/2245 train_time:86018ms step_avg:117.51ms
step:733/2245 train_time:86133ms step_avg:117.51ms
step:734/2245 train_time:86253ms step_avg:117.51ms
step:735/2245 train_time:86367ms step_avg:117.51ms
step:736/2245 train_time:86488ms step_avg:117.51ms
step:737/2245 train_time:86603ms step_avg:117.51ms
step:738/2245 train_time:86725ms step_avg:117.51ms
step:739/2245 train_time:86841ms step_avg:117.51ms
step:740/2245 train_time:86962ms step_avg:117.52ms
step:741/2245 train_time:87078ms step_avg:117.51ms
step:742/2245 train_time:87199ms step_avg:117.52ms
step:743/2245 train_time:87315ms step_avg:117.52ms
step:744/2245 train_time:87437ms step_avg:117.52ms
step:745/2245 train_time:87553ms step_avg:117.52ms
step:746/2245 train_time:87676ms step_avg:117.53ms
step:747/2245 train_time:87792ms step_avg:117.53ms
step:748/2245 train_time:87914ms step_avg:117.53ms
step:749/2245 train_time:88030ms step_avg:117.53ms
step:750/2245 train_time:88152ms step_avg:117.54ms
step:750/2245 val_loss:3.6714 train_time:88217ms step_avg:117.62ms
step:751/2245 train_time:88267ms step_avg:117.53ms
step:752/2245 train_time:88388ms step_avg:117.54ms
step:753/2245 train_time:88504ms step_avg:117.53ms
step:754/2245 train_time:88625ms step_avg:117.54ms
step:755/2245 train_time:88741ms step_avg:117.54ms
step:756/2245 train_time:88863ms step_avg:117.54ms
step:757/2245 train_time:88978ms step_avg:117.54ms
step:758/2245 train_time:89101ms step_avg:117.55ms
step:759/2245 train_time:89216ms step_avg:117.54ms
step:760/2245 train_time:89338ms step_avg:117.55ms
step:761/2245 train_time:89453ms step_avg:117.55ms
step:762/2245 train_time:89576ms step_avg:117.55ms
step:763/2245 train_time:89691ms step_avg:117.55ms
step:764/2245 train_time:89813ms step_avg:117.56ms
step:765/2245 train_time:89929ms step_avg:117.55ms
step:766/2245 train_time:90052ms step_avg:117.56ms
step:767/2245 train_time:90169ms step_avg:117.56ms
step:768/2245 train_time:90291ms step_avg:117.57ms
step:769/2245 train_time:90406ms step_avg:117.56ms
step:770/2245 train_time:90528ms step_avg:117.57ms
step:771/2245 train_time:90643ms step_avg:117.57ms
step:772/2245 train_time:90765ms step_avg:117.57ms
step:773/2245 train_time:90881ms step_avg:117.57ms
step:774/2245 train_time:91002ms step_avg:117.57ms
step:775/2245 train_time:91118ms step_avg:117.57ms
step:776/2245 train_time:91240ms step_avg:117.58ms
step:777/2245 train_time:91356ms step_avg:117.57ms
step:778/2245 train_time:91478ms step_avg:117.58ms
step:779/2245 train_time:91593ms step_avg:117.58ms
step:780/2245 train_time:91716ms step_avg:117.58ms
step:781/2245 train_time:91831ms step_avg:117.58ms
step:782/2245 train_time:91953ms step_avg:117.59ms
step:783/2245 train_time:92069ms step_avg:117.58ms
step:784/2245 train_time:92191ms step_avg:117.59ms
step:785/2245 train_time:92306ms step_avg:117.59ms
step:786/2245 train_time:92429ms step_avg:117.59ms
step:787/2245 train_time:92544ms step_avg:117.59ms
step:788/2245 train_time:92665ms step_avg:117.60ms
step:789/2245 train_time:92781ms step_avg:117.59ms
step:790/2245 train_time:92903ms step_avg:117.60ms
step:791/2245 train_time:93018ms step_avg:117.60ms
step:792/2245 train_time:93140ms step_avg:117.60ms
step:793/2245 train_time:93255ms step_avg:117.60ms
step:794/2245 train_time:93377ms step_avg:117.60ms
step:795/2245 train_time:93493ms step_avg:117.60ms
step:796/2245 train_time:93615ms step_avg:117.61ms
step:797/2245 train_time:93731ms step_avg:117.60ms
step:798/2245 train_time:93853ms step_avg:117.61ms
step:799/2245 train_time:93969ms step_avg:117.61ms
step:800/2245 train_time:94092ms step_avg:117.61ms
step:801/2245 train_time:94208ms step_avg:117.61ms
step:802/2245 train_time:94330ms step_avg:117.62ms
step:803/2245 train_time:94445ms step_avg:117.62ms
step:804/2245 train_time:94567ms step_avg:117.62ms
step:805/2245 train_time:94682ms step_avg:117.62ms
step:806/2245 train_time:94804ms step_avg:117.62ms
step:807/2245 train_time:94920ms step_avg:117.62ms
step:808/2245 train_time:95042ms step_avg:117.63ms
step:809/2245 train_time:95157ms step_avg:117.62ms
step:810/2245 train_time:95279ms step_avg:117.63ms
step:811/2245 train_time:95394ms step_avg:117.63ms
step:812/2245 train_time:95517ms step_avg:117.63ms
step:813/2245 train_time:95633ms step_avg:117.63ms
step:814/2245 train_time:95756ms step_avg:117.64ms
step:815/2245 train_time:95871ms step_avg:117.63ms
step:816/2245 train_time:95994ms step_avg:117.64ms
step:817/2245 train_time:96109ms step_avg:117.64ms
step:818/2245 train_time:96231ms step_avg:117.64ms
step:819/2245 train_time:96347ms step_avg:117.64ms
step:820/2245 train_time:96469ms step_avg:117.65ms
step:821/2245 train_time:96585ms step_avg:117.64ms
step:822/2245 train_time:96707ms step_avg:117.65ms
step:823/2245 train_time:96822ms step_avg:117.65ms
step:824/2245 train_time:96944ms step_avg:117.65ms
step:825/2245 train_time:97060ms step_avg:117.65ms
step:826/2245 train_time:97182ms step_avg:117.65ms
step:827/2245 train_time:97298ms step_avg:117.65ms
step:828/2245 train_time:97420ms step_avg:117.66ms
step:829/2245 train_time:97535ms step_avg:117.65ms
step:830/2245 train_time:97657ms step_avg:117.66ms
step:831/2245 train_time:97773ms step_avg:117.66ms
step:832/2245 train_time:97895ms step_avg:117.66ms
step:833/2245 train_time:98011ms step_avg:117.66ms
step:834/2245 train_time:98132ms step_avg:117.66ms
step:835/2245 train_time:98248ms step_avg:117.66ms
step:836/2245 train_time:98370ms step_avg:117.67ms
step:837/2245 train_time:98487ms step_avg:117.67ms
step:838/2245 train_time:98609ms step_avg:117.67ms
step:839/2245 train_time:98724ms step_avg:117.67ms
step:840/2245 train_time:98846ms step_avg:117.67ms
step:841/2245 train_time:98961ms step_avg:117.67ms
step:842/2245 train_time:99083ms step_avg:117.68ms
step:843/2245 train_time:99199ms step_avg:117.67ms
step:844/2245 train_time:99320ms step_avg:117.68ms
step:845/2245 train_time:99436ms step_avg:117.68ms
step:846/2245 train_time:99557ms step_avg:117.68ms
step:847/2245 train_time:99673ms step_avg:117.68ms
step:848/2245 train_time:99796ms step_avg:117.68ms
step:849/2245 train_time:99912ms step_avg:117.68ms
step:850/2245 train_time:100034ms step_avg:117.69ms
step:851/2245 train_time:100150ms step_avg:117.69ms
step:852/2245 train_time:100273ms step_avg:117.69ms
step:853/2245 train_time:100389ms step_avg:117.69ms
step:854/2245 train_time:100512ms step_avg:117.70ms
step:855/2245 train_time:100627ms step_avg:117.69ms
step:856/2245 train_time:100749ms step_avg:117.70ms
step:857/2245 train_time:100864ms step_avg:117.69ms
step:858/2245 train_time:100986ms step_avg:117.70ms
step:859/2245 train_time:101102ms step_avg:117.70ms
step:860/2245 train_time:101224ms step_avg:117.70ms
step:861/2245 train_time:101340ms step_avg:117.70ms
step:862/2245 train_time:101461ms step_avg:117.70ms
step:863/2245 train_time:101577ms step_avg:117.70ms
step:864/2245 train_time:101699ms step_avg:117.71ms
step:865/2245 train_time:101814ms step_avg:117.70ms
step:866/2245 train_time:101936ms step_avg:117.71ms
step:867/2245 train_time:102052ms step_avg:117.71ms
step:868/2245 train_time:102174ms step_avg:117.71ms
step:869/2245 train_time:102290ms step_avg:117.71ms
step:870/2245 train_time:102412ms step_avg:117.72ms
step:871/2245 train_time:102528ms step_avg:117.71ms
step:872/2245 train_time:102650ms step_avg:117.72ms
step:873/2245 train_time:102767ms step_avg:117.72ms
step:874/2245 train_time:102888ms step_avg:117.72ms
step:875/2245 train_time:103004ms step_avg:117.72ms
step:876/2245 train_time:103126ms step_avg:117.72ms
step:877/2245 train_time:103242ms step_avg:117.72ms
step:878/2245 train_time:103363ms step_avg:117.73ms
step:879/2245 train_time:103479ms step_avg:117.72ms
step:880/2245 train_time:103601ms step_avg:117.73ms
step:881/2245 train_time:103716ms step_avg:117.73ms
step:882/2245 train_time:103838ms step_avg:117.73ms
step:883/2245 train_time:103953ms step_avg:117.73ms
step:884/2245 train_time:104076ms step_avg:117.73ms
step:885/2245 train_time:104192ms step_avg:117.73ms
step:886/2245 train_time:104314ms step_avg:117.74ms
step:887/2245 train_time:104430ms step_avg:117.73ms
step:888/2245 train_time:104552ms step_avg:117.74ms
step:889/2245 train_time:104669ms step_avg:117.74ms
step:890/2245 train_time:104791ms step_avg:117.74ms
step:891/2245 train_time:104906ms step_avg:117.74ms
step:892/2245 train_time:105028ms step_avg:117.74ms
step:893/2245 train_time:105143ms step_avg:117.74ms
step:894/2245 train_time:105265ms step_avg:117.75ms
step:895/2245 train_time:105381ms step_avg:117.74ms
step:896/2245 train_time:105503ms step_avg:117.75ms
step:897/2245 train_time:105618ms step_avg:117.75ms
step:898/2245 train_time:105740ms step_avg:117.75ms
step:899/2245 train_time:105855ms step_avg:117.75ms
step:900/2245 train_time:105977ms step_avg:117.75ms
step:901/2245 train_time:106093ms step_avg:117.75ms
step:902/2245 train_time:106215ms step_avg:117.75ms
step:903/2245 train_time:106331ms step_avg:117.75ms
step:904/2245 train_time:106453ms step_avg:117.76ms
step:905/2245 train_time:106569ms step_avg:117.76ms
step:906/2245 train_time:106691ms step_avg:117.76ms
step:907/2245 train_time:106807ms step_avg:117.76ms
step:908/2245 train_time:106929ms step_avg:117.76ms
step:909/2245 train_time:107044ms step_avg:117.76ms
step:910/2245 train_time:107166ms step_avg:117.76ms
step:911/2245 train_time:107281ms step_avg:117.76ms
step:912/2245 train_time:107403ms step_avg:117.77ms
step:913/2245 train_time:107518ms step_avg:117.76ms
step:914/2245 train_time:107640ms step_avg:117.77ms
step:915/2245 train_time:107756ms step_avg:117.77ms
step:916/2245 train_time:107878ms step_avg:117.77ms
step:917/2245 train_time:107994ms step_avg:117.77ms
step:918/2245 train_time:108116ms step_avg:117.77ms
step:919/2245 train_time:108232ms step_avg:117.77ms
step:920/2245 train_time:108354ms step_avg:117.78ms
step:921/2245 train_time:108470ms step_avg:117.77ms
step:922/2245 train_time:108591ms step_avg:117.78ms
step:923/2245 train_time:108707ms step_avg:117.78ms
step:924/2245 train_time:108829ms step_avg:117.78ms
step:925/2245 train_time:108945ms step_avg:117.78ms
step:926/2245 train_time:109067ms step_avg:117.78ms
step:927/2245 train_time:109182ms step_avg:117.78ms
step:928/2245 train_time:109304ms step_avg:117.78ms
step:929/2245 train_time:109419ms step_avg:117.78ms
step:930/2245 train_time:109541ms step_avg:117.79ms
step:931/2245 train_time:109657ms step_avg:117.78ms
step:932/2245 train_time:109778ms step_avg:117.79ms
step:933/2245 train_time:109894ms step_avg:117.79ms
step:934/2245 train_time:110016ms step_avg:117.79ms
step:935/2245 train_time:110132ms step_avg:117.79ms
step:936/2245 train_time:110254ms step_avg:117.79ms
step:937/2245 train_time:110369ms step_avg:117.79ms
step:938/2245 train_time:110491ms step_avg:117.79ms
step:939/2245 train_time:110607ms step_avg:117.79ms
step:940/2245 train_time:110729ms step_avg:117.80ms
step:941/2245 train_time:110844ms step_avg:117.79ms
step:942/2245 train_time:110966ms step_avg:117.80ms
step:943/2245 train_time:111081ms step_avg:117.80ms
step:944/2245 train_time:111204ms step_avg:117.80ms
step:945/2245 train_time:111319ms step_avg:117.80ms
step:946/2245 train_time:111441ms step_avg:117.80ms
step:947/2245 train_time:111557ms step_avg:117.80ms
step:948/2245 train_time:111678ms step_avg:117.80ms
step:949/2245 train_time:111794ms step_avg:117.80ms
step:950/2245 train_time:111916ms step_avg:117.81ms
step:951/2245 train_time:112032ms step_avg:117.80ms
step:952/2245 train_time:112154ms step_avg:117.81ms
step:953/2245 train_time:112270ms step_avg:117.81ms
step:954/2245 train_time:112392ms step_avg:117.81ms
step:955/2245 train_time:112508ms step_avg:117.81ms
step:956/2245 train_time:112631ms step_avg:117.81ms
step:957/2245 train_time:112746ms step_avg:117.81ms
step:958/2245 train_time:112868ms step_avg:117.82ms
step:959/2245 train_time:112984ms step_avg:117.81ms
step:960/2245 train_time:113106ms step_avg:117.82ms
step:961/2245 train_time:113221ms step_avg:117.82ms
step:962/2245 train_time:113343ms step_avg:117.82ms
step:963/2245 train_time:113458ms step_avg:117.82ms
step:964/2245 train_time:113580ms step_avg:117.82ms
step:965/2245 train_time:113696ms step_avg:117.82ms
step:966/2245 train_time:113818ms step_avg:117.82ms
step:967/2245 train_time:113933ms step_avg:117.82ms
step:968/2245 train_time:114055ms step_avg:117.83ms
step:969/2245 train_time:114171ms step_avg:117.82ms
step:970/2245 train_time:114293ms step_avg:117.83ms
step:971/2245 train_time:114409ms step_avg:117.83ms
step:972/2245 train_time:114531ms step_avg:117.83ms
step:973/2245 train_time:114647ms step_avg:117.83ms
step:974/2245 train_time:114769ms step_avg:117.83ms
step:975/2245 train_time:114884ms step_avg:117.83ms
step:976/2245 train_time:115006ms step_avg:117.83ms
step:977/2245 train_time:115121ms step_avg:117.83ms
step:978/2245 train_time:115243ms step_avg:117.84ms
step:979/2245 train_time:115359ms step_avg:117.83ms
step:980/2245 train_time:115481ms step_avg:117.84ms
step:981/2245 train_time:115596ms step_avg:117.84ms
step:982/2245 train_time:115719ms step_avg:117.84ms
step:983/2245 train_time:115835ms step_avg:117.84ms
step:984/2245 train_time:115957ms step_avg:117.84ms
step:985/2245 train_time:116073ms step_avg:117.84ms
step:986/2245 train_time:116195ms step_avg:117.85ms
step:987/2245 train_time:116311ms step_avg:117.84ms
step:988/2245 train_time:116433ms step_avg:117.85ms
step:989/2245 train_time:116549ms step_avg:117.85ms
step:990/2245 train_time:116672ms step_avg:117.85ms
step:991/2245 train_time:116787ms step_avg:117.85ms
step:992/2245 train_time:116909ms step_avg:117.85ms
step:993/2245 train_time:117025ms step_avg:117.85ms
step:994/2245 train_time:117147ms step_avg:117.85ms
step:995/2245 train_time:117262ms step_avg:117.85ms
step:996/2245 train_time:117384ms step_avg:117.86ms
step:997/2245 train_time:117500ms step_avg:117.85ms
step:998/2245 train_time:117622ms step_avg:117.86ms
step:999/2245 train_time:117738ms step_avg:117.86ms
step:1000/2245 train_time:117860ms step_avg:117.86ms
step:1000/2245 val_loss:3.5932 train_time:117925ms step_avg:117.93ms
step:1001/2245 train_time:117976ms step_avg:117.86ms
step:1002/2245 train_time:118097ms step_avg:117.86ms
step:1003/2245 train_time:118213ms step_avg:117.86ms
step:1004/2245 train_time:118335ms step_avg:117.86ms
step:1005/2245 train_time:118451ms step_avg:117.86ms
step:1006/2245 train_time:118573ms step_avg:117.87ms
step:1007/2245 train_time:118688ms step_avg:117.86ms
step:1008/2245 train_time:118810ms step_avg:117.87ms
step:1009/2245 train_time:118926ms step_avg:117.86ms
step:1010/2245 train_time:119048ms step_avg:117.87ms
step:1011/2245 train_time:119163ms step_avg:117.87ms
step:1012/2245 train_time:119286ms step_avg:117.87ms
step:1013/2245 train_time:119401ms step_avg:117.87ms
step:1014/2245 train_time:119523ms step_avg:117.87ms
step:1015/2245 train_time:119639ms step_avg:117.87ms
step:1016/2245 train_time:119761ms step_avg:117.87ms
step:1017/2245 train_time:119876ms step_avg:117.87ms
step:1018/2245 train_time:119998ms step_avg:117.88ms
step:1019/2245 train_time:120113ms step_avg:117.87ms
step:1020/2245 train_time:120235ms step_avg:117.88ms
step:1021/2245 train_time:120351ms step_avg:117.88ms
step:1022/2245 train_time:120473ms step_avg:117.88ms
step:1023/2245 train_time:120588ms step_avg:117.88ms
step:1024/2245 train_time:120710ms step_avg:117.88ms
step:1025/2245 train_time:120826ms step_avg:117.88ms
step:1026/2245 train_time:120948ms step_avg:117.88ms
step:1027/2245 train_time:121063ms step_avg:117.88ms
step:1028/2245 train_time:121185ms step_avg:117.88ms
step:1029/2245 train_time:121300ms step_avg:117.88ms
step:1030/2245 train_time:121423ms step_avg:117.89ms
step:1031/2245 train_time:121539ms step_avg:117.88ms
step:1032/2245 train_time:121661ms step_avg:117.89ms
step:1033/2245 train_time:121776ms step_avg:117.89ms
step:1034/2245 train_time:121898ms step_avg:117.89ms
step:1035/2245 train_time:122014ms step_avg:117.89ms
step:1036/2245 train_time:122136ms step_avg:117.89ms
step:1037/2245 train_time:122252ms step_avg:117.89ms
step:1038/2245 train_time:122374ms step_avg:117.89ms
step:1039/2245 train_time:122490ms step_avg:117.89ms
step:1040/2245 train_time:122611ms step_avg:117.90ms
step:1041/2245 train_time:122727ms step_avg:117.89ms
step:1042/2245 train_time:122850ms step_avg:117.90ms
step:1043/2245 train_time:122965ms step_avg:117.90ms
step:1044/2245 train_time:123087ms step_avg:117.90ms
step:1045/2245 train_time:123204ms step_avg:117.90ms
step:1046/2245 train_time:123326ms step_avg:117.90ms
step:1047/2245 train_time:123443ms step_avg:117.90ms
step:1048/2245 train_time:123565ms step_avg:117.91ms
step:1049/2245 train_time:123680ms step_avg:117.90ms
step:1050/2245 train_time:123802ms step_avg:117.91ms
step:1051/2245 train_time:123918ms step_avg:117.90ms
step:1052/2245 train_time:124039ms step_avg:117.91ms
step:1053/2245 train_time:124154ms step_avg:117.91ms
step:1054/2245 train_time:124276ms step_avg:117.91ms
step:1055/2245 train_time:124392ms step_avg:117.91ms
step:1056/2245 train_time:124514ms step_avg:117.91ms
step:1057/2245 train_time:124630ms step_avg:117.91ms
step:1058/2245 train_time:124752ms step_avg:117.91ms
step:1059/2245 train_time:124867ms step_avg:117.91ms
step:1060/2245 train_time:124990ms step_avg:117.92ms
step:1061/2245 train_time:125105ms step_avg:117.91ms
step:1062/2245 train_time:125228ms step_avg:117.92ms
step:1063/2245 train_time:125344ms step_avg:117.92ms
step:1064/2245 train_time:125466ms step_avg:117.92ms
step:1065/2245 train_time:125582ms step_avg:117.92ms
step:1066/2245 train_time:125704ms step_avg:117.92ms
step:1067/2245 train_time:125819ms step_avg:117.92ms
step:1068/2245 train_time:125941ms step_avg:117.92ms
step:1069/2245 train_time:126057ms step_avg:117.92ms
step:1070/2245 train_time:126179ms step_avg:117.92ms
step:1071/2245 train_time:126294ms step_avg:117.92ms
step:1072/2245 train_time:126416ms step_avg:117.93ms
step:1073/2245 train_time:126532ms step_avg:117.92ms
step:1074/2245 train_time:126654ms step_avg:117.93ms
step:1075/2245 train_time:126769ms step_avg:117.92ms
step:1076/2245 train_time:126891ms step_avg:117.93ms
step:1077/2245 train_time:127006ms step_avg:117.93ms
step:1078/2245 train_time:127129ms step_avg:117.93ms
step:1079/2245 train_time:127245ms step_avg:117.93ms
step:1080/2245 train_time:127367ms step_avg:117.93ms
step:1081/2245 train_time:127483ms step_avg:117.93ms
step:1082/2245 train_time:127605ms step_avg:117.93ms
step:1083/2245 train_time:127721ms step_avg:117.93ms
step:1084/2245 train_time:127843ms step_avg:117.94ms
step:1085/2245 train_time:127958ms step_avg:117.93ms
step:1086/2245 train_time:128080ms step_avg:117.94ms
step:1087/2245 train_time:128196ms step_avg:117.94ms
step:1088/2245 train_time:128317ms step_avg:117.94ms
step:1089/2245 train_time:128433ms step_avg:117.94ms
step:1090/2245 train_time:128555ms step_avg:117.94ms
step:1091/2245 train_time:128671ms step_avg:117.94ms
step:1092/2245 train_time:128793ms step_avg:117.94ms
step:1093/2245 train_time:128908ms step_avg:117.94ms
step:1094/2245 train_time:129031ms step_avg:117.94ms
step:1095/2245 train_time:129146ms step_avg:117.94ms
step:1096/2245 train_time:129269ms step_avg:117.95ms
step:1097/2245 train_time:129384ms step_avg:117.94ms
step:1098/2245 train_time:129507ms step_avg:117.95ms
step:1099/2245 train_time:129622ms step_avg:117.95ms
step:1100/2245 train_time:129745ms step_avg:117.95ms
step:1101/2245 train_time:129861ms step_avg:117.95ms
step:1102/2245 train_time:129984ms step_avg:117.95ms
step:1103/2245 train_time:130099ms step_avg:117.95ms
step:1104/2245 train_time:130221ms step_avg:117.95ms
step:1105/2245 train_time:130336ms step_avg:117.95ms
step:1106/2245 train_time:130458ms step_avg:117.95ms
step:1107/2245 train_time:130574ms step_avg:117.95ms
step:1108/2245 train_time:130695ms step_avg:117.96ms
step:1109/2245 train_time:130811ms step_avg:117.95ms
step:1110/2245 train_time:130934ms step_avg:117.96ms
step:1111/2245 train_time:131049ms step_avg:117.96ms
step:1112/2245 train_time:131171ms step_avg:117.96ms
step:1113/2245 train_time:131286ms step_avg:117.96ms
step:1114/2245 train_time:131408ms step_avg:117.96ms
step:1115/2245 train_time:131524ms step_avg:117.96ms
step:1116/2245 train_time:131646ms step_avg:117.96ms
step:1117/2245 train_time:131762ms step_avg:117.96ms
step:1118/2245 train_time:131884ms step_avg:117.96ms
step:1119/2245 train_time:132000ms step_avg:117.96ms
step:1120/2245 train_time:132122ms step_avg:117.97ms
step:1121/2245 train_time:132238ms step_avg:117.96ms
step:1122/2245 train_time:132360ms step_avg:117.97ms
step:1123/2245 train_time:132475ms step_avg:117.97ms
step:1124/2245 train_time:132597ms step_avg:117.97ms
step:1125/2245 train_time:132713ms step_avg:117.97ms
step:1126/2245 train_time:132834ms step_avg:117.97ms
step:1127/2245 train_time:132949ms step_avg:117.97ms
step:1128/2245 train_time:133072ms step_avg:117.97ms
step:1129/2245 train_time:133187ms step_avg:117.97ms
step:1130/2245 train_time:133309ms step_avg:117.97ms
step:1131/2245 train_time:133424ms step_avg:117.97ms
step:1132/2245 train_time:133547ms step_avg:117.97ms
step:1133/2245 train_time:133663ms step_avg:117.97ms
step:1134/2245 train_time:133785ms step_avg:117.98ms
step:1135/2245 train_time:133901ms step_avg:117.97ms
step:1136/2245 train_time:134024ms step_avg:117.98ms
step:1137/2245 train_time:134140ms step_avg:117.98ms
step:1138/2245 train_time:134261ms step_avg:117.98ms
step:1139/2245 train_time:134377ms step_avg:117.98ms
step:1140/2245 train_time:134499ms step_avg:117.98ms
step:1141/2245 train_time:134614ms step_avg:117.98ms
step:1142/2245 train_time:134736ms step_avg:117.98ms
step:1143/2245 train_time:134852ms step_avg:117.98ms
step:1144/2245 train_time:134974ms step_avg:117.98ms
step:1145/2245 train_time:135089ms step_avg:117.98ms
step:1146/2245 train_time:135211ms step_avg:117.98ms
step:1147/2245 train_time:135327ms step_avg:117.98ms
step:1148/2245 train_time:135450ms step_avg:117.99ms
step:1149/2245 train_time:135566ms step_avg:117.99ms
step:1150/2245 train_time:135687ms step_avg:117.99ms
step:1151/2245 train_time:135804ms step_avg:117.99ms
step:1152/2245 train_time:135926ms step_avg:117.99ms
step:1153/2245 train_time:136042ms step_avg:117.99ms
step:1154/2245 train_time:136164ms step_avg:117.99ms
step:1155/2245 train_time:136280ms step_avg:117.99ms
step:1156/2245 train_time:136402ms step_avg:117.99ms
step:1157/2245 train_time:136518ms step_avg:117.99ms
step:1158/2245 train_time:136639ms step_avg:118.00ms
step:1159/2245 train_time:136755ms step_avg:117.99ms
step:1160/2245 train_time:136877ms step_avg:118.00ms
step:1161/2245 train_time:136992ms step_avg:118.00ms
step:1162/2245 train_time:137114ms step_avg:118.00ms
step:1163/2245 train_time:137230ms step_avg:118.00ms
step:1164/2245 train_time:137352ms step_avg:118.00ms
step:1165/2245 train_time:137467ms step_avg:118.00ms
step:1166/2245 train_time:137589ms step_avg:118.00ms
step:1167/2245 train_time:137705ms step_avg:118.00ms
step:1168/2245 train_time:137827ms step_avg:118.00ms
step:1169/2245 train_time:137943ms step_avg:118.00ms
step:1170/2245 train_time:138066ms step_avg:118.01ms
step:1171/2245 train_time:138182ms step_avg:118.00ms
step:1172/2245 train_time:138305ms step_avg:118.01ms
step:1173/2245 train_time:138421ms step_avg:118.01ms
step:1174/2245 train_time:138543ms step_avg:118.01ms
step:1175/2245 train_time:138659ms step_avg:118.01ms
step:1176/2245 train_time:138780ms step_avg:118.01ms
step:1177/2245 train_time:138896ms step_avg:118.01ms
step:1178/2245 train_time:139018ms step_avg:118.01ms
step:1179/2245 train_time:139133ms step_avg:118.01ms
step:1180/2245 train_time:139254ms step_avg:118.01ms
step:1181/2245 train_time:139371ms step_avg:118.01ms
step:1182/2245 train_time:139492ms step_avg:118.01ms
step:1183/2245 train_time:139608ms step_avg:118.01ms
step:1184/2245 train_time:139730ms step_avg:118.02ms
step:1185/2245 train_time:139846ms step_avg:118.01ms
step:1186/2245 train_time:139968ms step_avg:118.02ms
step:1187/2245 train_time:140083ms step_avg:118.01ms
step:1188/2245 train_time:140206ms step_avg:118.02ms
step:1189/2245 train_time:140322ms step_avg:118.02ms
step:1190/2245 train_time:140444ms step_avg:118.02ms
step:1191/2245 train_time:140561ms step_avg:118.02ms
step:1192/2245 train_time:140683ms step_avg:118.02ms
step:1193/2245 train_time:140798ms step_avg:118.02ms
step:1194/2245 train_time:140919ms step_avg:118.02ms
step:1195/2245 train_time:141035ms step_avg:118.02ms
step:1196/2245 train_time:141157ms step_avg:118.02ms
step:1197/2245 train_time:141272ms step_avg:118.02ms
step:1198/2245 train_time:141394ms step_avg:118.02ms
step:1199/2245 train_time:141509ms step_avg:118.02ms
step:1200/2245 train_time:141631ms step_avg:118.03ms
step:1201/2245 train_time:141747ms step_avg:118.02ms
step:1202/2245 train_time:141869ms step_avg:118.03ms
step:1203/2245 train_time:141985ms step_avg:118.03ms
step:1204/2245 train_time:142107ms step_avg:118.03ms
step:1205/2245 train_time:142223ms step_avg:118.03ms
step:1206/2245 train_time:142345ms step_avg:118.03ms
step:1207/2245 train_time:142461ms step_avg:118.03ms
step:1208/2245 train_time:142582ms step_avg:118.03ms
step:1209/2245 train_time:142698ms step_avg:118.03ms
step:1210/2245 train_time:142820ms step_avg:118.03ms
step:1211/2245 train_time:142936ms step_avg:118.03ms
step:1212/2245 train_time:143057ms step_avg:118.03ms
step:1213/2245 train_time:143173ms step_avg:118.03ms
step:1214/2245 train_time:143295ms step_avg:118.04ms
step:1215/2245 train_time:143410ms step_avg:118.03ms
step:1216/2245 train_time:143532ms step_avg:118.04ms
step:1217/2245 train_time:143648ms step_avg:118.03ms
step:1218/2245 train_time:143769ms step_avg:118.04ms
step:1219/2245 train_time:143885ms step_avg:118.04ms
step:1220/2245 train_time:144007ms step_avg:118.04ms
step:1221/2245 train_time:144122ms step_avg:118.04ms
step:1222/2245 train_time:144245ms step_avg:118.04ms
step:1223/2245 train_time:144360ms step_avg:118.04ms
step:1224/2245 train_time:144483ms step_avg:118.04ms
step:1225/2245 train_time:144598ms step_avg:118.04ms
step:1226/2245 train_time:144720ms step_avg:118.04ms
step:1227/2245 train_time:144836ms step_avg:118.04ms
step:1228/2245 train_time:144957ms step_avg:118.04ms
step:1229/2245 train_time:145073ms step_avg:118.04ms
step:1230/2245 train_time:145195ms step_avg:118.04ms
step:1231/2245 train_time:145311ms step_avg:118.04ms
step:1232/2245 train_time:145433ms step_avg:118.05ms
step:1233/2245 train_time:145548ms step_avg:118.04ms
step:1234/2245 train_time:145670ms step_avg:118.05ms
step:1235/2245 train_time:145786ms step_avg:118.05ms
step:1236/2245 train_time:145908ms step_avg:118.05ms
step:1237/2245 train_time:146024ms step_avg:118.05ms
step:1238/2245 train_time:146146ms step_avg:118.05ms
step:1239/2245 train_time:146262ms step_avg:118.05ms
step:1240/2245 train_time:146385ms step_avg:118.05ms
step:1241/2245 train_time:146500ms step_avg:118.05ms
step:1242/2245 train_time:146623ms step_avg:118.05ms
step:1243/2245 train_time:146738ms step_avg:118.05ms
step:1244/2245 train_time:146860ms step_avg:118.05ms
step:1245/2245 train_time:146976ms step_avg:118.05ms
step:1246/2245 train_time:147098ms step_avg:118.06ms
step:1247/2245 train_time:147213ms step_avg:118.05ms
step:1248/2245 train_time:147335ms step_avg:118.06ms
step:1249/2245 train_time:147451ms step_avg:118.06ms
step:1250/2245 train_time:147573ms step_avg:118.06ms
step:1250/2245 val_loss:3.5242 train_time:147638ms step_avg:118.11ms
step:1251/2245 train_time:147691ms step_avg:118.06ms
step:1252/2245 train_time:147812ms step_avg:118.06ms
step:1253/2245 train_time:147927ms step_avg:118.06ms
step:1254/2245 train_time:148050ms step_avg:118.06ms
step:1255/2245 train_time:148165ms step_avg:118.06ms
step:1256/2245 train_time:148287ms step_avg:118.06ms
step:1257/2245 train_time:148402ms step_avg:118.06ms
step:1258/2245 train_time:148524ms step_avg:118.06ms
step:1259/2245 train_time:148639ms step_avg:118.06ms
step:1260/2245 train_time:148761ms step_avg:118.06ms
step:1261/2245 train_time:148877ms step_avg:118.06ms
step:1262/2245 train_time:149000ms step_avg:118.07ms
step:1263/2245 train_time:149117ms step_avg:118.07ms
step:1264/2245 train_time:149239ms step_avg:118.07ms
step:1265/2245 train_time:149355ms step_avg:118.07ms
step:1266/2245 train_time:149477ms step_avg:118.07ms
step:1267/2245 train_time:149593ms step_avg:118.07ms
step:1268/2245 train_time:149715ms step_avg:118.07ms
step:1269/2245 train_time:149830ms step_avg:118.07ms
step:1270/2245 train_time:149952ms step_avg:118.07ms
step:1271/2245 train_time:150068ms step_avg:118.07ms
step:1272/2245 train_time:150190ms step_avg:118.07ms
step:1273/2245 train_time:150306ms step_avg:118.07ms
step:1274/2245 train_time:150428ms step_avg:118.08ms
step:1275/2245 train_time:150543ms step_avg:118.07ms
step:1276/2245 train_time:150665ms step_avg:118.08ms
step:1277/2245 train_time:150780ms step_avg:118.07ms
step:1278/2245 train_time:150902ms step_avg:118.08ms
step:1279/2245 train_time:151018ms step_avg:118.08ms
step:1280/2245 train_time:151140ms step_avg:118.08ms
step:1281/2245 train_time:151256ms step_avg:118.08ms
step:1282/2245 train_time:151378ms step_avg:118.08ms
step:1283/2245 train_time:151495ms step_avg:118.08ms
step:1284/2245 train_time:151617ms step_avg:118.08ms
step:1285/2245 train_time:151732ms step_avg:118.08ms
step:1286/2245 train_time:151854ms step_avg:118.08ms
step:1287/2245 train_time:151970ms step_avg:118.08ms
step:1288/2245 train_time:152092ms step_avg:118.08ms
step:1289/2245 train_time:152207ms step_avg:118.08ms
step:1290/2245 train_time:152329ms step_avg:118.08ms
step:1291/2245 train_time:152445ms step_avg:118.08ms
step:1292/2245 train_time:152566ms step_avg:118.09ms
step:1293/2245 train_time:152682ms step_avg:118.08ms
step:1294/2245 train_time:152804ms step_avg:118.09ms
step:1295/2245 train_time:152919ms step_avg:118.08ms
step:1296/2245 train_time:153041ms step_avg:118.09ms
step:1297/2245 train_time:153157ms step_avg:118.09ms
step:1298/2245 train_time:153278ms step_avg:118.09ms
step:1299/2245 train_time:153394ms step_avg:118.09ms
step:1300/2245 train_time:153516ms step_avg:118.09ms
step:1301/2245 train_time:153632ms step_avg:118.09ms
step:1302/2245 train_time:153754ms step_avg:118.09ms
step:1303/2245 train_time:153869ms step_avg:118.09ms
step:1304/2245 train_time:153992ms step_avg:118.09ms
step:1305/2245 train_time:154107ms step_avg:118.09ms
step:1306/2245 train_time:154229ms step_avg:118.09ms
step:1307/2245 train_time:154345ms step_avg:118.09ms
step:1308/2245 train_time:154466ms step_avg:118.09ms
step:1309/2245 train_time:154582ms step_avg:118.09ms
step:1310/2245 train_time:154704ms step_avg:118.09ms
step:1311/2245 train_time:154820ms step_avg:118.09ms
step:1312/2245 train_time:154942ms step_avg:118.10ms
step:1313/2245 train_time:155057ms step_avg:118.09ms
step:1314/2245 train_time:155180ms step_avg:118.10ms
step:1315/2245 train_time:155296ms step_avg:118.10ms
step:1316/2245 train_time:155419ms step_avg:118.10ms
step:1317/2245 train_time:155535ms step_avg:118.10ms
step:1318/2245 train_time:155657ms step_avg:118.10ms
step:1319/2245 train_time:155773ms step_avg:118.10ms
step:1320/2245 train_time:155895ms step_avg:118.10ms
step:1321/2245 train_time:156011ms step_avg:118.10ms
step:1322/2245 train_time:156132ms step_avg:118.10ms
step:1323/2245 train_time:156248ms step_avg:118.10ms
step:1324/2245 train_time:156370ms step_avg:118.10ms
step:1325/2245 train_time:156485ms step_avg:118.10ms
step:1326/2245 train_time:156607ms step_avg:118.11ms
step:1327/2245 train_time:156723ms step_avg:118.10ms
step:1328/2245 train_time:156845ms step_avg:118.11ms
step:1329/2245 train_time:156960ms step_avg:118.10ms
step:1330/2245 train_time:157083ms step_avg:118.11ms
step:1331/2245 train_time:157199ms step_avg:118.11ms
step:1332/2245 train_time:157321ms step_avg:118.11ms
step:1333/2245 train_time:157436ms step_avg:118.11ms
step:1334/2245 train_time:157558ms step_avg:118.11ms
step:1335/2245 train_time:157675ms step_avg:118.11ms
step:1336/2245 train_time:157797ms step_avg:118.11ms
step:1337/2245 train_time:157913ms step_avg:118.11ms
step:1338/2245 train_time:158036ms step_avg:118.11ms
step:1339/2245 train_time:158151ms step_avg:118.11ms
step:1340/2245 train_time:158273ms step_avg:118.11ms
step:1341/2245 train_time:158388ms step_avg:118.11ms
step:1342/2245 train_time:158510ms step_avg:118.11ms
step:1343/2245 train_time:158625ms step_avg:118.11ms
step:1344/2245 train_time:158747ms step_avg:118.12ms
step:1345/2245 train_time:158862ms step_avg:118.11ms
step:1346/2245 train_time:158984ms step_avg:118.12ms
step:1347/2245 train_time:159100ms step_avg:118.11ms
step:1348/2245 train_time:159221ms step_avg:118.12ms
step:1349/2245 train_time:159337ms step_avg:118.11ms
step:1350/2245 train_time:159460ms step_avg:118.12ms
step:1351/2245 train_time:159576ms step_avg:118.12ms
step:1352/2245 train_time:159698ms step_avg:118.12ms
step:1353/2245 train_time:159814ms step_avg:118.12ms
step:1354/2245 train_time:159936ms step_avg:118.12ms
step:1355/2245 train_time:160051ms step_avg:118.12ms
step:1356/2245 train_time:160173ms step_avg:118.12ms
step:1357/2245 train_time:160288ms step_avg:118.12ms
step:1358/2245 train_time:160410ms step_avg:118.12ms
step:1359/2245 train_time:160526ms step_avg:118.12ms
step:1360/2245 train_time:160648ms step_avg:118.12ms
step:1361/2245 train_time:160763ms step_avg:118.12ms
step:1362/2245 train_time:160885ms step_avg:118.12ms
step:1363/2245 train_time:161001ms step_avg:118.12ms
step:1364/2245 train_time:161123ms step_avg:118.13ms
step:1365/2245 train_time:161239ms step_avg:118.12ms
step:1366/2245 train_time:161360ms step_avg:118.13ms
step:1367/2245 train_time:161477ms step_avg:118.12ms
step:1368/2245 train_time:161599ms step_avg:118.13ms
step:1369/2245 train_time:161716ms step_avg:118.13ms
step:1370/2245 train_time:161838ms step_avg:118.13ms
step:1371/2245 train_time:161954ms step_avg:118.13ms
step:1372/2245 train_time:162077ms step_avg:118.13ms
step:1373/2245 train_time:162192ms step_avg:118.13ms
step:1374/2245 train_time:162314ms step_avg:118.13ms
step:1375/2245 train_time:162430ms step_avg:118.13ms
step:1376/2245 train_time:162551ms step_avg:118.13ms
step:1377/2245 train_time:162667ms step_avg:118.13ms
step:1378/2245 train_time:162790ms step_avg:118.13ms
step:1379/2245 train_time:162905ms step_avg:118.13ms
step:1380/2245 train_time:163027ms step_avg:118.14ms
step:1381/2245 train_time:163143ms step_avg:118.13ms
step:1382/2245 train_time:163265ms step_avg:118.14ms
step:1383/2245 train_time:163380ms step_avg:118.13ms
step:1384/2245 train_time:163502ms step_avg:118.14ms
step:1385/2245 train_time:163618ms step_avg:118.14ms
step:1386/2245 train_time:163740ms step_avg:118.14ms
step:1387/2245 train_time:163856ms step_avg:118.14ms
step:1388/2245 train_time:163978ms step_avg:118.14ms
step:1389/2245 train_time:164094ms step_avg:118.14ms
step:1390/2245 train_time:164217ms step_avg:118.14ms
step:1391/2245 train_time:164333ms step_avg:118.14ms
step:1392/2245 train_time:164455ms step_avg:118.14ms
step:1393/2245 train_time:164570ms step_avg:118.14ms
step:1394/2245 train_time:164692ms step_avg:118.14ms
step:1395/2245 train_time:164808ms step_avg:118.14ms
step:1396/2245 train_time:164930ms step_avg:118.14ms
step:1397/2245 train_time:165046ms step_avg:118.14ms
step:1398/2245 train_time:165167ms step_avg:118.15ms
step:1399/2245 train_time:165283ms step_avg:118.14ms
step:1400/2245 train_time:165405ms step_avg:118.15ms
step:1401/2245 train_time:165521ms step_avg:118.14ms
step:1402/2245 train_time:165643ms step_avg:118.15ms
step:1403/2245 train_time:165759ms step_avg:118.15ms
step:1404/2245 train_time:165881ms step_avg:118.15ms
step:1405/2245 train_time:165997ms step_avg:118.15ms
step:1406/2245 train_time:166119ms step_avg:118.15ms
step:1407/2245 train_time:166235ms step_avg:118.15ms
step:1408/2245 train_time:166357ms step_avg:118.15ms
step:1409/2245 train_time:166473ms step_avg:118.15ms
step:1410/2245 train_time:166595ms step_avg:118.15ms
step:1411/2245 train_time:166711ms step_avg:118.15ms
step:1412/2245 train_time:166832ms step_avg:118.15ms
step:1413/2245 train_time:166949ms step_avg:118.15ms
step:1414/2245 train_time:167071ms step_avg:118.15ms
step:1415/2245 train_time:167186ms step_avg:118.15ms
step:1416/2245 train_time:167308ms step_avg:118.16ms
step:1417/2245 train_time:167424ms step_avg:118.15ms
step:1418/2245 train_time:167545ms step_avg:118.16ms
step:1419/2245 train_time:167661ms step_avg:118.15ms
step:1420/2245 train_time:167783ms step_avg:118.16ms
step:1421/2245 train_time:167899ms step_avg:118.16ms
step:1422/2245 train_time:168021ms step_avg:118.16ms
step:1423/2245 train_time:168137ms step_avg:118.16ms
step:1424/2245 train_time:168259ms step_avg:118.16ms
step:1425/2245 train_time:168375ms step_avg:118.16ms
step:1426/2245 train_time:168497ms step_avg:118.16ms
step:1427/2245 train_time:168612ms step_avg:118.16ms
step:1428/2245 train_time:168734ms step_avg:118.16ms
step:1429/2245 train_time:168850ms step_avg:118.16ms
step:1430/2245 train_time:168972ms step_avg:118.16ms
step:1431/2245 train_time:169088ms step_avg:118.16ms
step:1432/2245 train_time:169210ms step_avg:118.16ms
step:1433/2245 train_time:169326ms step_avg:118.16ms
step:1434/2245 train_time:169447ms step_avg:118.16ms
step:1435/2245 train_time:169563ms step_avg:118.16ms
step:1436/2245 train_time:169685ms step_avg:118.16ms
step:1437/2245 train_time:169800ms step_avg:118.16ms
step:1438/2245 train_time:169923ms step_avg:118.17ms
step:1439/2245 train_time:170038ms step_avg:118.16ms
step:1440/2245 train_time:170161ms step_avg:118.17ms
step:1441/2245 train_time:170277ms step_avg:118.17ms
step:1442/2245 train_time:170399ms step_avg:118.17ms
step:1443/2245 train_time:170515ms step_avg:118.17ms
step:1444/2245 train_time:170637ms step_avg:118.17ms
step:1445/2245 train_time:170753ms step_avg:118.17ms
step:1446/2245 train_time:170875ms step_avg:118.17ms
step:1447/2245 train_time:170990ms step_avg:118.17ms
step:1448/2245 train_time:171112ms step_avg:118.17ms
step:1449/2245 train_time:171227ms step_avg:118.17ms
step:1450/2245 train_time:171349ms step_avg:118.17ms
step:1451/2245 train_time:171465ms step_avg:118.17ms
step:1452/2245 train_time:171587ms step_avg:118.17ms
step:1453/2245 train_time:171702ms step_avg:118.17ms
step:1454/2245 train_time:171824ms step_avg:118.17ms
step:1455/2245 train_time:171940ms step_avg:118.17ms
step:1456/2245 train_time:172062ms step_avg:118.17ms
step:1457/2245 train_time:172178ms step_avg:118.17ms
step:1458/2245 train_time:172301ms step_avg:118.18ms
step:1459/2245 train_time:172417ms step_avg:118.17ms
step:1460/2245 train_time:172539ms step_avg:118.18ms
step:1461/2245 train_time:172655ms step_avg:118.18ms
step:1462/2245 train_time:172778ms step_avg:118.18ms
step:1463/2245 train_time:172893ms step_avg:118.18ms
step:1464/2245 train_time:173015ms step_avg:118.18ms
step:1465/2245 train_time:173130ms step_avg:118.18ms
step:1466/2245 train_time:173253ms step_avg:118.18ms
step:1467/2245 train_time:173368ms step_avg:118.18ms
step:1468/2245 train_time:173490ms step_avg:118.18ms
step:1469/2245 train_time:173606ms step_avg:118.18ms
step:1470/2245 train_time:173728ms step_avg:118.18ms
step:1471/2245 train_time:173844ms step_avg:118.18ms
step:1472/2245 train_time:173967ms step_avg:118.18ms
step:1473/2245 train_time:174084ms step_avg:118.18ms
step:1474/2245 train_time:174207ms step_avg:118.19ms
step:1475/2245 train_time:174323ms step_avg:118.19ms
step:1476/2245 train_time:174446ms step_avg:118.19ms
step:1477/2245 train_time:174563ms step_avg:118.19ms
step:1478/2245 train_time:174686ms step_avg:118.19ms
step:1479/2245 train_time:174802ms step_avg:118.19ms
step:1480/2245 train_time:174925ms step_avg:118.19ms
step:1481/2245 train_time:175042ms step_avg:118.19ms
step:1482/2245 train_time:175165ms step_avg:118.19ms
step:1483/2245 train_time:175281ms step_avg:118.19ms
step:1484/2245 train_time:175404ms step_avg:118.20ms
step:1485/2245 train_time:175521ms step_avg:118.20ms
step:1486/2245 train_time:175644ms step_avg:118.20ms
step:1487/2245 train_time:175761ms step_avg:118.20ms
step:1488/2245 train_time:175884ms step_avg:118.20ms
step:1489/2245 train_time:176001ms step_avg:118.20ms
step:1490/2245 train_time:176124ms step_avg:118.20ms
step:1491/2245 train_time:176241ms step_avg:118.20ms
step:1492/2245 train_time:176364ms step_avg:118.21ms
step:1493/2245 train_time:176481ms step_avg:118.21ms
step:1494/2245 train_time:176604ms step_avg:118.21ms
step:1495/2245 train_time:176720ms step_avg:118.21ms
step:1496/2245 train_time:176843ms step_avg:118.21ms
step:1497/2245 train_time:176961ms step_avg:118.21ms
step:1498/2245 train_time:177084ms step_avg:118.21ms
step:1499/2245 train_time:177201ms step_avg:118.21ms
step:1500/2245 train_time:177324ms step_avg:118.22ms
step:1500/2245 val_loss:3.4441 train_time:177391ms step_avg:118.26ms
step:1501/2245 train_time:177441ms step_avg:118.22ms
step:1502/2245 train_time:177563ms step_avg:118.22ms
step:1503/2245 train_time:177679ms step_avg:118.22ms
step:1504/2245 train_time:177802ms step_avg:118.22ms
step:1505/2245 train_time:177918ms step_avg:118.22ms
step:1506/2245 train_time:178041ms step_avg:118.22ms
step:1507/2245 train_time:178158ms step_avg:118.22ms
step:1508/2245 train_time:178280ms step_avg:118.22ms
step:1509/2245 train_time:178396ms step_avg:118.22ms
step:1510/2245 train_time:178519ms step_avg:118.22ms
step:1511/2245 train_time:178636ms step_avg:118.22ms
step:1512/2245 train_time:178758ms step_avg:118.23ms
step:1513/2245 train_time:178875ms step_avg:118.23ms
step:1514/2245 train_time:178998ms step_avg:118.23ms
step:1515/2245 train_time:179114ms step_avg:118.23ms
step:1516/2245 train_time:179238ms step_avg:118.23ms
step:1517/2245 train_time:179354ms step_avg:118.23ms
step:1518/2245 train_time:179477ms step_avg:118.23ms
step:1519/2245 train_time:179594ms step_avg:118.23ms
step:1520/2245 train_time:179717ms step_avg:118.23ms
step:1521/2245 train_time:179834ms step_avg:118.23ms
step:1522/2245 train_time:179957ms step_avg:118.24ms
step:1523/2245 train_time:180073ms step_avg:118.24ms
step:1524/2245 train_time:180196ms step_avg:118.24ms
step:1525/2245 train_time:180313ms step_avg:118.24ms
step:1526/2245 train_time:180436ms step_avg:118.24ms
step:1527/2245 train_time:180552ms step_avg:118.24ms
step:1528/2245 train_time:180675ms step_avg:118.24ms
step:1529/2245 train_time:180791ms step_avg:118.24ms
step:1530/2245 train_time:180914ms step_avg:118.24ms
step:1531/2245 train_time:181031ms step_avg:118.24ms
step:1532/2245 train_time:181154ms step_avg:118.25ms
step:1533/2245 train_time:181271ms step_avg:118.25ms
step:1534/2245 train_time:181394ms step_avg:118.25ms
step:1535/2245 train_time:181511ms step_avg:118.25ms
step:1536/2245 train_time:181634ms step_avg:118.25ms
step:1537/2245 train_time:181750ms step_avg:118.25ms
step:1538/2245 train_time:181874ms step_avg:118.25ms
step:1539/2245 train_time:181990ms step_avg:118.25ms
step:1540/2245 train_time:182113ms step_avg:118.26ms
step:1541/2245 train_time:182230ms step_avg:118.25ms
step:1542/2245 train_time:182353ms step_avg:118.26ms
step:1543/2245 train_time:182470ms step_avg:118.26ms
step:1544/2245 train_time:182592ms step_avg:118.26ms
step:1545/2245 train_time:182709ms step_avg:118.26ms
step:1546/2245 train_time:182832ms step_avg:118.26ms
step:1547/2245 train_time:182949ms step_avg:118.26ms
step:1548/2245 train_time:183072ms step_avg:118.26ms
step:1549/2245 train_time:183188ms step_avg:118.26ms
step:1550/2245 train_time:183311ms step_avg:118.27ms
step:1551/2245 train_time:183428ms step_avg:118.26ms
step:1552/2245 train_time:183551ms step_avg:118.27ms
step:1553/2245 train_time:183668ms step_avg:118.27ms
step:1554/2245 train_time:183792ms step_avg:118.27ms
step:1555/2245 train_time:183908ms step_avg:118.27ms
step:1556/2245 train_time:184031ms step_avg:118.27ms
step:1557/2245 train_time:184147ms step_avg:118.27ms
step:1558/2245 train_time:184272ms step_avg:118.27ms
step:1559/2245 train_time:184388ms step_avg:118.27ms
step:1560/2245 train_time:184510ms step_avg:118.28ms
step:1561/2245 train_time:184627ms step_avg:118.27ms
step:1562/2245 train_time:184750ms step_avg:118.28ms
step:1563/2245 train_time:184867ms step_avg:118.28ms
step:1564/2245 train_time:184990ms step_avg:118.28ms
step:1565/2245 train_time:185107ms step_avg:118.28ms
step:1566/2245 train_time:185231ms step_avg:118.28ms
step:1567/2245 train_time:185347ms step_avg:118.28ms
step:1568/2245 train_time:185471ms step_avg:118.29ms
step:1569/2245 train_time:185587ms step_avg:118.28ms
step:1570/2245 train_time:185710ms step_avg:118.29ms
step:1571/2245 train_time:185827ms step_avg:118.29ms
step:1572/2245 train_time:185949ms step_avg:118.29ms
step:1573/2245 train_time:186066ms step_avg:118.29ms
step:1574/2245 train_time:186189ms step_avg:118.29ms
step:1575/2245 train_time:186306ms step_avg:118.29ms
step:1576/2245 train_time:186429ms step_avg:118.29ms
step:1577/2245 train_time:186546ms step_avg:118.29ms
step:1578/2245 train_time:186669ms step_avg:118.29ms
step:1579/2245 train_time:186786ms step_avg:118.29ms
step:1580/2245 train_time:186909ms step_avg:118.30ms
step:1581/2245 train_time:187026ms step_avg:118.30ms
step:1582/2245 train_time:187148ms step_avg:118.30ms
step:1583/2245 train_time:187266ms step_avg:118.30ms
step:1584/2245 train_time:187389ms step_avg:118.30ms
step:1585/2245 train_time:187505ms step_avg:118.30ms
step:1586/2245 train_time:187627ms step_avg:118.30ms
step:1587/2245 train_time:187743ms step_avg:118.30ms
step:1588/2245 train_time:187867ms step_avg:118.30ms
step:1589/2245 train_time:187983ms step_avg:118.30ms
step:1590/2245 train_time:188106ms step_avg:118.31ms
step:1591/2245 train_time:188222ms step_avg:118.30ms
step:1592/2245 train_time:188345ms step_avg:118.31ms
step:1593/2245 train_time:188461ms step_avg:118.31ms
step:1594/2245 train_time:188584ms step_avg:118.31ms
step:1595/2245 train_time:188700ms step_avg:118.31ms
step:1596/2245 train_time:188823ms step_avg:118.31ms
step:1597/2245 train_time:188940ms step_avg:118.31ms
step:1598/2245 train_time:189062ms step_avg:118.31ms
step:1599/2245 train_time:189178ms step_avg:118.31ms
step:1600/2245 train_time:189301ms step_avg:118.31ms
step:1601/2245 train_time:189418ms step_avg:118.31ms
step:1602/2245 train_time:189542ms step_avg:118.32ms
step:1603/2245 train_time:189658ms step_avg:118.31ms
step:1604/2245 train_time:189780ms step_avg:118.32ms
step:1605/2245 train_time:189897ms step_avg:118.32ms
step:1606/2245 train_time:190020ms step_avg:118.32ms
step:1607/2245 train_time:190136ms step_avg:118.32ms
step:1608/2245 train_time:190259ms step_avg:118.32ms
step:1609/2245 train_time:190375ms step_avg:118.32ms
step:1610/2245 train_time:190498ms step_avg:118.32ms
step:1611/2245 train_time:190614ms step_avg:118.32ms
step:1612/2245 train_time:190737ms step_avg:118.32ms
step:1613/2245 train_time:190853ms step_avg:118.32ms
step:1614/2245 train_time:190976ms step_avg:118.32ms
step:1615/2245 train_time:191092ms step_avg:118.32ms
step:1616/2245 train_time:191216ms step_avg:118.33ms
step:1617/2245 train_time:191333ms step_avg:118.33ms
step:1618/2245 train_time:191455ms step_avg:118.33ms
step:1619/2245 train_time:191572ms step_avg:118.33ms
step:1620/2245 train_time:191695ms step_avg:118.33ms
step:1621/2245 train_time:191811ms step_avg:118.33ms
step:1622/2245 train_time:191935ms step_avg:118.33ms
step:1623/2245 train_time:192051ms step_avg:118.33ms
step:1624/2245 train_time:192174ms step_avg:118.33ms
step:1625/2245 train_time:192291ms step_avg:118.33ms
step:1626/2245 train_time:192414ms step_avg:118.34ms
step:1627/2245 train_time:192531ms step_avg:118.34ms
step:1628/2245 train_time:192654ms step_avg:118.34ms
step:1629/2245 train_time:192771ms step_avg:118.34ms
step:1630/2245 train_time:192894ms step_avg:118.34ms
step:1631/2245 train_time:193011ms step_avg:118.34ms
step:1632/2245 train_time:193134ms step_avg:118.34ms
step:1633/2245 train_time:193250ms step_avg:118.34ms
step:1634/2245 train_time:193374ms step_avg:118.34ms
step:1635/2245 train_time:193491ms step_avg:118.34ms
step:1636/2245 train_time:193614ms step_avg:118.35ms
step:1637/2245 train_time:193730ms step_avg:118.34ms
step:1638/2245 train_time:193853ms step_avg:118.35ms
step:1639/2245 train_time:193970ms step_avg:118.35ms
step:1640/2245 train_time:194093ms step_avg:118.35ms
step:1641/2245 train_time:194210ms step_avg:118.35ms
step:1642/2245 train_time:194332ms step_avg:118.35ms
step:1643/2245 train_time:194450ms step_avg:118.35ms
step:1644/2245 train_time:194573ms step_avg:118.35ms
step:1645/2245 train_time:194690ms step_avg:118.35ms
step:1646/2245 train_time:194812ms step_avg:118.36ms
step:1647/2245 train_time:194929ms step_avg:118.35ms
step:1648/2245 train_time:195052ms step_avg:118.36ms
step:1649/2245 train_time:195169ms step_avg:118.36ms
step:1650/2245 train_time:195293ms step_avg:118.36ms
step:1651/2245 train_time:195409ms step_avg:118.36ms
step:1652/2245 train_time:195533ms step_avg:118.36ms
step:1653/2245 train_time:195649ms step_avg:118.36ms
step:1654/2245 train_time:195772ms step_avg:118.36ms
step:1655/2245 train_time:195889ms step_avg:118.36ms
step:1656/2245 train_time:196012ms step_avg:118.36ms
step:1657/2245 train_time:196128ms step_avg:118.36ms
step:1658/2245 train_time:196252ms step_avg:118.37ms
step:1659/2245 train_time:196368ms step_avg:118.37ms
step:1660/2245 train_time:196491ms step_avg:118.37ms
step:1661/2245 train_time:196608ms step_avg:118.37ms
step:1662/2245 train_time:196731ms step_avg:118.37ms
step:1663/2245 train_time:196848ms step_avg:118.37ms
step:1664/2245 train_time:196970ms step_avg:118.37ms
step:1665/2245 train_time:197087ms step_avg:118.37ms
step:1666/2245 train_time:197210ms step_avg:118.37ms
step:1667/2245 train_time:197327ms step_avg:118.37ms
step:1668/2245 train_time:197449ms step_avg:118.37ms
step:1669/2245 train_time:197566ms step_avg:118.37ms
step:1670/2245 train_time:197689ms step_avg:118.38ms
step:1671/2245 train_time:197807ms step_avg:118.38ms
step:1672/2245 train_time:197930ms step_avg:118.38ms
step:1673/2245 train_time:198047ms step_avg:118.38ms
step:1674/2245 train_time:198170ms step_avg:118.38ms
step:1675/2245 train_time:198286ms step_avg:118.38ms
step:1676/2245 train_time:198409ms step_avg:118.38ms
step:1677/2245 train_time:198526ms step_avg:118.38ms
step:1678/2245 train_time:198649ms step_avg:118.38ms
step:1679/2245 train_time:198766ms step_avg:118.38ms
step:1680/2245 train_time:198889ms step_avg:118.39ms
step:1681/2245 train_time:199006ms step_avg:118.39ms
step:1682/2245 train_time:199130ms step_avg:118.39ms
step:1683/2245 train_time:199246ms step_avg:118.39ms
step:1684/2245 train_time:199369ms step_avg:118.39ms
step:1685/2245 train_time:199486ms step_avg:118.39ms
step:1686/2245 train_time:199609ms step_avg:118.39ms
step:1687/2245 train_time:199726ms step_avg:118.39ms
step:1688/2245 train_time:199849ms step_avg:118.39ms
step:1689/2245 train_time:199966ms step_avg:118.39ms
step:1690/2245 train_time:200088ms step_avg:118.40ms
step:1691/2245 train_time:200205ms step_avg:118.39ms
step:1692/2245 train_time:200328ms step_avg:118.40ms
step:1693/2245 train_time:200445ms step_avg:118.40ms
step:1694/2245 train_time:200568ms step_avg:118.40ms
step:1695/2245 train_time:200684ms step_avg:118.40ms
step:1696/2245 train_time:200807ms step_avg:118.40ms
step:1697/2245 train_time:200923ms step_avg:118.40ms
step:1698/2245 train_time:201046ms step_avg:118.40ms
step:1699/2245 train_time:201163ms step_avg:118.40ms
step:1700/2245 train_time:201286ms step_avg:118.40ms
step:1701/2245 train_time:201402ms step_avg:118.40ms
step:1702/2245 train_time:201525ms step_avg:118.40ms
step:1703/2245 train_time:201641ms step_avg:118.40ms
step:1704/2245 train_time:201764ms step_avg:118.41ms
step:1705/2245 train_time:201880ms step_avg:118.40ms
step:1706/2245 train_time:202003ms step_avg:118.41ms
step:1707/2245 train_time:202119ms step_avg:118.41ms
step:1708/2245 train_time:202242ms step_avg:118.41ms
step:1709/2245 train_time:202359ms step_avg:118.41ms
step:1710/2245 train_time:202482ms step_avg:118.41ms
step:1711/2245 train_time:202598ms step_avg:118.41ms
step:1712/2245 train_time:202721ms step_avg:118.41ms
step:1713/2245 train_time:202837ms step_avg:118.41ms
step:1714/2245 train_time:202960ms step_avg:118.41ms
step:1715/2245 train_time:203077ms step_avg:118.41ms
step:1716/2245 train_time:203200ms step_avg:118.41ms
step:1717/2245 train_time:203317ms step_avg:118.41ms
step:1718/2245 train_time:203439ms step_avg:118.42ms
step:1719/2245 train_time:203556ms step_avg:118.42ms
step:1720/2245 train_time:203679ms step_avg:118.42ms
step:1721/2245 train_time:203795ms step_avg:118.42ms
step:1722/2245 train_time:203917ms step_avg:118.42ms
step:1723/2245 train_time:204034ms step_avg:118.42ms
step:1724/2245 train_time:204157ms step_avg:118.42ms
step:1725/2245 train_time:204274ms step_avg:118.42ms
step:1726/2245 train_time:204396ms step_avg:118.42ms
step:1727/2245 train_time:204513ms step_avg:118.42ms
step:1728/2245 train_time:204636ms step_avg:118.42ms
step:1729/2245 train_time:204752ms step_avg:118.42ms
step:1730/2245 train_time:204876ms step_avg:118.43ms
step:1731/2245 train_time:204992ms step_avg:118.42ms
step:1732/2245 train_time:205115ms step_avg:118.43ms
step:1733/2245 train_time:205232ms step_avg:118.43ms
step:1734/2245 train_time:205355ms step_avg:118.43ms
step:1735/2245 train_time:205472ms step_avg:118.43ms
step:1736/2245 train_time:205595ms step_avg:118.43ms
step:1737/2245 train_time:205711ms step_avg:118.43ms
step:1738/2245 train_time:205835ms step_avg:118.43ms
step:1739/2245 train_time:205951ms step_avg:118.43ms
step:1740/2245 train_time:206074ms step_avg:118.43ms
step:1741/2245 train_time:206191ms step_avg:118.43ms
step:1742/2245 train_time:206314ms step_avg:118.44ms
step:1743/2245 train_time:206431ms step_avg:118.43ms
step:1744/2245 train_time:206554ms step_avg:118.44ms
step:1745/2245 train_time:206671ms step_avg:118.44ms
step:1746/2245 train_time:206795ms step_avg:118.44ms
step:1747/2245 train_time:206911ms step_avg:118.44ms
step:1748/2245 train_time:207034ms step_avg:118.44ms
step:1749/2245 train_time:207151ms step_avg:118.44ms
step:1750/2245 train_time:207274ms step_avg:118.44ms
step:1750/2245 val_loss:3.3792 train_time:207341ms step_avg:118.48ms
step:1751/2245 train_time:207392ms step_avg:118.44ms
step:1752/2245 train_time:207515ms step_avg:118.44ms
step:1753/2245 train_time:207631ms step_avg:118.44ms
step:1754/2245 train_time:207753ms step_avg:118.45ms
step:1755/2245 train_time:207870ms step_avg:118.44ms
step:1756/2245 train_time:207992ms step_avg:118.45ms
step:1757/2245 train_time:208108ms step_avg:118.45ms
step:1758/2245 train_time:208231ms step_avg:118.45ms
step:1759/2245 train_time:208348ms step_avg:118.45ms
step:1760/2245 train_time:208473ms step_avg:118.45ms
step:1761/2245 train_time:208590ms step_avg:118.45ms
step:1762/2245 train_time:208712ms step_avg:118.45ms
step:1763/2245 train_time:208829ms step_avg:118.45ms
step:1764/2245 train_time:208952ms step_avg:118.45ms
step:1765/2245 train_time:209069ms step_avg:118.45ms
step:1766/2245 train_time:209192ms step_avg:118.46ms
step:1767/2245 train_time:209309ms step_avg:118.45ms
step:1768/2245 train_time:209432ms step_avg:118.46ms
step:1769/2245 train_time:209550ms step_avg:118.46ms
step:1770/2245 train_time:209673ms step_avg:118.46ms
step:1771/2245 train_time:209790ms step_avg:118.46ms
step:1772/2245 train_time:209912ms step_avg:118.46ms
step:1773/2245 train_time:210028ms step_avg:118.46ms
step:1774/2245 train_time:210151ms step_avg:118.46ms
step:1775/2245 train_time:210268ms step_avg:118.46ms
step:1776/2245 train_time:210391ms step_avg:118.46ms
step:1777/2245 train_time:210507ms step_avg:118.46ms
step:1778/2245 train_time:210632ms step_avg:118.47ms
step:1779/2245 train_time:210748ms step_avg:118.46ms
step:1780/2245 train_time:210871ms step_avg:118.47ms
step:1781/2245 train_time:210988ms step_avg:118.47ms
step:1782/2245 train_time:211111ms step_avg:118.47ms
step:1783/2245 train_time:211227ms step_avg:118.47ms
step:1784/2245 train_time:211350ms step_avg:118.47ms
step:1785/2245 train_time:211467ms step_avg:118.47ms
step:1786/2245 train_time:211590ms step_avg:118.47ms
step:1787/2245 train_time:211706ms step_avg:118.47ms
step:1788/2245 train_time:211831ms step_avg:118.47ms
step:1789/2245 train_time:211947ms step_avg:118.47ms
step:1790/2245 train_time:212070ms step_avg:118.48ms
step:1791/2245 train_time:212187ms step_avg:118.47ms
step:1792/2245 train_time:212310ms step_avg:118.48ms
step:1793/2245 train_time:212427ms step_avg:118.48ms
step:1794/2245 train_time:212550ms step_avg:118.48ms
step:1795/2245 train_time:212666ms step_avg:118.48ms
step:1796/2245 train_time:212789ms step_avg:118.48ms
step:1797/2245 train_time:212905ms step_avg:118.48ms
step:1798/2245 train_time:213029ms step_avg:118.48ms
step:1799/2245 train_time:213145ms step_avg:118.48ms
step:1800/2245 train_time:213267ms step_avg:118.48ms
step:1801/2245 train_time:213385ms step_avg:118.48ms
step:1802/2245 train_time:213507ms step_avg:118.48ms
step:1803/2245 train_time:213624ms step_avg:118.48ms
step:1804/2245 train_time:213747ms step_avg:118.48ms
step:1805/2245 train_time:213863ms step_avg:118.48ms
step:1806/2245 train_time:213986ms step_avg:118.49ms
step:1807/2245 train_time:214103ms step_avg:118.49ms
step:1808/2245 train_time:214226ms step_avg:118.49ms
step:1809/2245 train_time:214343ms step_avg:118.49ms
step:1810/2245 train_time:214466ms step_avg:118.49ms
step:1811/2245 train_time:214583ms step_avg:118.49ms
step:1812/2245 train_time:214707ms step_avg:118.49ms
step:1813/2245 train_time:214823ms step_avg:118.49ms
step:1814/2245 train_time:214947ms step_avg:118.49ms
step:1815/2245 train_time:215064ms step_avg:118.49ms
step:1816/2245 train_time:215187ms step_avg:118.50ms
step:1817/2245 train_time:215304ms step_avg:118.49ms
step:1818/2245 train_time:215427ms step_avg:118.50ms
step:1819/2245 train_time:215545ms step_avg:118.50ms
step:1820/2245 train_time:215668ms step_avg:118.50ms
step:1821/2245 train_time:215784ms step_avg:118.50ms
step:1822/2245 train_time:215907ms step_avg:118.50ms
step:1823/2245 train_time:216023ms step_avg:118.50ms
step:1824/2245 train_time:216146ms step_avg:118.50ms
step:1825/2245 train_time:216262ms step_avg:118.50ms
step:1826/2245 train_time:216385ms step_avg:118.50ms
step:1827/2245 train_time:216501ms step_avg:118.50ms
step:1828/2245 train_time:216624ms step_avg:118.50ms
step:1829/2245 train_time:216740ms step_avg:118.50ms
step:1830/2245 train_time:216863ms step_avg:118.50ms
step:1831/2245 train_time:216979ms step_avg:118.50ms
step:1832/2245 train_time:217102ms step_avg:118.51ms
step:1833/2245 train_time:217219ms step_avg:118.50ms
step:1834/2245 train_time:217341ms step_avg:118.51ms
step:1835/2245 train_time:217457ms step_avg:118.51ms
step:1836/2245 train_time:217581ms step_avg:118.51ms
step:1837/2245 train_time:217697ms step_avg:118.51ms
step:1838/2245 train_time:217821ms step_avg:118.51ms
step:1839/2245 train_time:217937ms step_avg:118.51ms
step:1840/2245 train_time:218060ms step_avg:118.51ms
step:1841/2245 train_time:218176ms step_avg:118.51ms
step:1842/2245 train_time:218300ms step_avg:118.51ms
step:1843/2245 train_time:218416ms step_avg:118.51ms
step:1844/2245 train_time:218539ms step_avg:118.51ms
step:1845/2245 train_time:218655ms step_avg:118.51ms
step:1846/2245 train_time:218778ms step_avg:118.51ms
step:1847/2245 train_time:218895ms step_avg:118.51ms
step:1848/2245 train_time:219017ms step_avg:118.52ms
step:1849/2245 train_time:219133ms step_avg:118.51ms
step:1850/2245 train_time:219256ms step_avg:118.52ms
step:1851/2245 train_time:219373ms step_avg:118.52ms
step:1852/2245 train_time:219495ms step_avg:118.52ms
step:1853/2245 train_time:219612ms step_avg:118.52ms
step:1854/2245 train_time:219735ms step_avg:118.52ms
step:1855/2245 train_time:219851ms step_avg:118.52ms
step:1856/2245 train_time:219974ms step_avg:118.52ms
step:1857/2245 train_time:220091ms step_avg:118.52ms
step:1858/2245 train_time:220213ms step_avg:118.52ms
step:1859/2245 train_time:220330ms step_avg:118.52ms
step:1860/2245 train_time:220453ms step_avg:118.52ms
step:1861/2245 train_time:220570ms step_avg:118.52ms
step:1862/2245 train_time:220693ms step_avg:118.52ms
step:1863/2245 train_time:220810ms step_avg:118.52ms
step:1864/2245 train_time:220934ms step_avg:118.53ms
step:1865/2245 train_time:221050ms step_avg:118.53ms
step:1866/2245 train_time:221172ms step_avg:118.53ms
step:1867/2245 train_time:221289ms step_avg:118.53ms
step:1868/2245 train_time:221412ms step_avg:118.53ms
step:1869/2245 train_time:221530ms step_avg:118.53ms
step:1870/2245 train_time:221652ms step_avg:118.53ms
step:1871/2245 train_time:221768ms step_avg:118.53ms
step:1872/2245 train_time:221891ms step_avg:118.53ms
step:1873/2245 train_time:222008ms step_avg:118.53ms
step:1874/2245 train_time:222132ms step_avg:118.53ms
step:1875/2245 train_time:222249ms step_avg:118.53ms
step:1876/2245 train_time:222371ms step_avg:118.53ms
step:1877/2245 train_time:222488ms step_avg:118.53ms
step:1878/2245 train_time:222611ms step_avg:118.54ms
step:1879/2245 train_time:222728ms step_avg:118.54ms
step:1880/2245 train_time:222851ms step_avg:118.54ms
step:1881/2245 train_time:222967ms step_avg:118.54ms
step:1882/2245 train_time:223091ms step_avg:118.54ms
step:1883/2245 train_time:223207ms step_avg:118.54ms
step:1884/2245 train_time:223330ms step_avg:118.54ms
step:1885/2245 train_time:223447ms step_avg:118.54ms
step:1886/2245 train_time:223569ms step_avg:118.54ms
step:1887/2245 train_time:223685ms step_avg:118.54ms
step:1888/2245 train_time:223809ms step_avg:118.54ms
step:1889/2245 train_time:223924ms step_avg:118.54ms
step:1890/2245 train_time:224048ms step_avg:118.54ms
step:1891/2245 train_time:224165ms step_avg:118.54ms
step:1892/2245 train_time:224288ms step_avg:118.55ms
step:1893/2245 train_time:224406ms step_avg:118.54ms
step:1894/2245 train_time:224528ms step_avg:118.55ms
step:1895/2245 train_time:224645ms step_avg:118.55ms
step:1896/2245 train_time:224768ms step_avg:118.55ms
step:1897/2245 train_time:224885ms step_avg:118.55ms
step:1898/2245 train_time:225008ms step_avg:118.55ms
step:1899/2245 train_time:225124ms step_avg:118.55ms
step:1900/2245 train_time:225247ms step_avg:118.55ms
step:1901/2245 train_time:225364ms step_avg:118.55ms
step:1902/2245 train_time:225488ms step_avg:118.55ms
step:1903/2245 train_time:225604ms step_avg:118.55ms
step:1904/2245 train_time:225728ms step_avg:118.55ms
step:1905/2245 train_time:225845ms step_avg:118.55ms
step:1906/2245 train_time:225968ms step_avg:118.56ms
step:1907/2245 train_time:226085ms step_avg:118.56ms
step:1908/2245 train_time:226208ms step_avg:118.56ms
step:1909/2245 train_time:226325ms step_avg:118.56ms
step:1910/2245 train_time:226448ms step_avg:118.56ms
step:1911/2245 train_time:226565ms step_avg:118.56ms
step:1912/2245 train_time:226688ms step_avg:118.56ms
step:1913/2245 train_time:226803ms step_avg:118.56ms
step:1914/2245 train_time:226928ms step_avg:118.56ms
step:1915/2245 train_time:227044ms step_avg:118.56ms
step:1916/2245 train_time:227167ms step_avg:118.56ms
step:1917/2245 train_time:227284ms step_avg:118.56ms
step:1918/2245 train_time:227407ms step_avg:118.56ms
step:1919/2245 train_time:227524ms step_avg:118.56ms
step:1920/2245 train_time:227646ms step_avg:118.57ms
step:1921/2245 train_time:227763ms step_avg:118.56ms
step:1922/2245 train_time:227887ms step_avg:118.57ms
step:1923/2245 train_time:228004ms step_avg:118.57ms
step:1924/2245 train_time:228127ms step_avg:118.57ms
step:1925/2245 train_time:228244ms step_avg:118.57ms
step:1926/2245 train_time:228367ms step_avg:118.57ms
step:1927/2245 train_time:228484ms step_avg:118.57ms
step:1928/2245 train_time:228608ms step_avg:118.57ms
step:1929/2245 train_time:228725ms step_avg:118.57ms
step:1930/2245 train_time:228848ms step_avg:118.57ms
step:1931/2245 train_time:228965ms step_avg:118.57ms
step:1932/2245 train_time:229088ms step_avg:118.58ms
step:1933/2245 train_time:229204ms step_avg:118.57ms
step:1934/2245 train_time:229327ms step_avg:118.58ms
step:1935/2245 train_time:229444ms step_avg:118.58ms
step:1936/2245 train_time:229567ms step_avg:118.58ms
step:1937/2245 train_time:229684ms step_avg:118.58ms
step:1938/2245 train_time:229808ms step_avg:118.58ms
step:1939/2245 train_time:229925ms step_avg:118.58ms
step:1940/2245 train_time:230048ms step_avg:118.58ms
step:1941/2245 train_time:230165ms step_avg:118.58ms
step:1942/2245 train_time:230287ms step_avg:118.58ms
step:1943/2245 train_time:230405ms step_avg:118.58ms
step:1944/2245 train_time:230527ms step_avg:118.58ms
step:1945/2245 train_time:230644ms step_avg:118.58ms
step:1946/2245 train_time:230766ms step_avg:118.59ms
step:1947/2245 train_time:230884ms step_avg:118.58ms
step:1948/2245 train_time:231007ms step_avg:118.59ms
step:1949/2245 train_time:231124ms step_avg:118.59ms
step:1950/2245 train_time:231247ms step_avg:118.59ms
step:1951/2245 train_time:231364ms step_avg:118.59ms
step:1952/2245 train_time:231487ms step_avg:118.59ms
step:1953/2245 train_time:231603ms step_avg:118.59ms
step:1954/2245 train_time:231726ms step_avg:118.59ms
step:1955/2245 train_time:231842ms step_avg:118.59ms
step:1956/2245 train_time:231965ms step_avg:118.59ms
step:1957/2245 train_time:232081ms step_avg:118.59ms
step:1958/2245 train_time:232204ms step_avg:118.59ms
step:1959/2245 train_time:232320ms step_avg:118.59ms
step:1960/2245 train_time:232444ms step_avg:118.59ms
step:1961/2245 train_time:232560ms step_avg:118.59ms
step:1962/2245 train_time:232683ms step_avg:118.59ms
step:1963/2245 train_time:232799ms step_avg:118.59ms
step:1964/2245 train_time:232922ms step_avg:118.60ms
step:1965/2245 train_time:233038ms step_avg:118.59ms
step:1966/2245 train_time:233160ms step_avg:118.60ms
step:1967/2245 train_time:233277ms step_avg:118.60ms
step:1968/2245 train_time:233400ms step_avg:118.60ms
step:1969/2245 train_time:233516ms step_avg:118.60ms
step:1970/2245 train_time:233639ms step_avg:118.60ms
step:1971/2245 train_time:233755ms step_avg:118.60ms
step:1972/2245 train_time:233878ms step_avg:118.60ms
step:1973/2245 train_time:233995ms step_avg:118.60ms
step:1974/2245 train_time:234117ms step_avg:118.60ms
step:1975/2245 train_time:234234ms step_avg:118.60ms
step:1976/2245 train_time:234358ms step_avg:118.60ms
step:1977/2245 train_time:234474ms step_avg:118.60ms
step:1978/2245 train_time:234597ms step_avg:118.60ms
step:1979/2245 train_time:234713ms step_avg:118.60ms
step:1980/2245 train_time:234836ms step_avg:118.60ms
step:1981/2245 train_time:234952ms step_avg:118.60ms
step:1982/2245 train_time:235075ms step_avg:118.60ms
step:1983/2245 train_time:235191ms step_avg:118.60ms
step:1984/2245 train_time:235314ms step_avg:118.61ms
step:1985/2245 train_time:235431ms step_avg:118.60ms
step:1986/2245 train_time:235554ms step_avg:118.61ms
step:1987/2245 train_time:235671ms step_avg:118.61ms
step:1988/2245 train_time:235794ms step_avg:118.61ms
step:1989/2245 train_time:235911ms step_avg:118.61ms
step:1990/2245 train_time:236034ms step_avg:118.61ms
step:1991/2245 train_time:236150ms step_avg:118.61ms
step:1992/2245 train_time:236273ms step_avg:118.61ms
step:1993/2245 train_time:236390ms step_avg:118.61ms
step:1994/2245 train_time:236513ms step_avg:118.61ms
step:1995/2245 train_time:236630ms step_avg:118.61ms
step:1996/2245 train_time:236753ms step_avg:118.61ms
step:1997/2245 train_time:236870ms step_avg:118.61ms
step:1998/2245 train_time:236993ms step_avg:118.61ms
step:1999/2245 train_time:237110ms step_avg:118.61ms
step:2000/2245 train_time:237232ms step_avg:118.62ms
step:2000/2245 val_loss:3.3240 train_time:237299ms step_avg:118.65ms
step:2001/2245 train_time:237350ms step_avg:118.62ms
step:2002/2245 train_time:237471ms step_avg:118.62ms
step:2003/2245 train_time:237588ms step_avg:118.62ms
step:2004/2245 train_time:237711ms step_avg:118.62ms
step:2005/2245 train_time:237828ms step_avg:118.62ms
step:2006/2245 train_time:237951ms step_avg:118.62ms
step:2007/2245 train_time:238067ms step_avg:118.62ms
step:2008/2245 train_time:238190ms step_avg:118.62ms
step:2009/2245 train_time:238307ms step_avg:118.62ms
step:2010/2245 train_time:238431ms step_avg:118.62ms
step:2011/2245 train_time:238548ms step_avg:118.62ms
step:2012/2245 train_time:238671ms step_avg:118.62ms
step:2013/2245 train_time:238788ms step_avg:118.62ms
step:2014/2245 train_time:238911ms step_avg:118.62ms
step:2015/2245 train_time:239027ms step_avg:118.62ms
step:2016/2245 train_time:239149ms step_avg:118.63ms
step:2017/2245 train_time:239266ms step_avg:118.62ms
step:2018/2245 train_time:239390ms step_avg:118.63ms
step:2019/2245 train_time:239506ms step_avg:118.63ms
step:2020/2245 train_time:239630ms step_avg:118.63ms
step:2021/2245 train_time:239746ms step_avg:118.63ms
step:2022/2245 train_time:239869ms step_avg:118.63ms
step:2023/2245 train_time:239986ms step_avg:118.63ms
step:2024/2245 train_time:240108ms step_avg:118.63ms
step:2025/2245 train_time:240225ms step_avg:118.63ms
step:2026/2245 train_time:240348ms step_avg:118.63ms
step:2027/2245 train_time:240466ms step_avg:118.63ms
step:2028/2245 train_time:240589ms step_avg:118.63ms
step:2029/2245 train_time:240705ms step_avg:118.63ms
step:2030/2245 train_time:240829ms step_avg:118.63ms
step:2031/2245 train_time:240945ms step_avg:118.63ms
step:2032/2245 train_time:241068ms step_avg:118.64ms
step:2033/2245 train_time:241185ms step_avg:118.63ms
step:2034/2245 train_time:241308ms step_avg:118.64ms
step:2035/2245 train_time:241425ms step_avg:118.64ms
step:2036/2245 train_time:241548ms step_avg:118.64ms
step:2037/2245 train_time:241665ms step_avg:118.64ms
step:2038/2245 train_time:241788ms step_avg:118.64ms
step:2039/2245 train_time:241905ms step_avg:118.64ms
step:2040/2245 train_time:242028ms step_avg:118.64ms
step:2041/2245 train_time:242146ms step_avg:118.64ms
step:2042/2245 train_time:242269ms step_avg:118.64ms
step:2043/2245 train_time:242386ms step_avg:118.64ms
step:2044/2245 train_time:242509ms step_avg:118.64ms
step:2045/2245 train_time:242625ms step_avg:118.64ms
step:2046/2245 train_time:242750ms step_avg:118.65ms
step:2047/2245 train_time:242866ms step_avg:118.65ms
step:2048/2245 train_time:242990ms step_avg:118.65ms
step:2049/2245 train_time:243106ms step_avg:118.65ms
step:2050/2245 train_time:243229ms step_avg:118.65ms
step:2051/2245 train_time:243346ms step_avg:118.65ms
step:2052/2245 train_time:243469ms step_avg:118.65ms
step:2053/2245 train_time:243585ms step_avg:118.65ms
step:2054/2245 train_time:243709ms step_avg:118.65ms
step:2055/2245 train_time:243825ms step_avg:118.65ms
step:2056/2245 train_time:243948ms step_avg:118.65ms
step:2057/2245 train_time:244065ms step_avg:118.65ms
step:2058/2245 train_time:244188ms step_avg:118.65ms
step:2059/2245 train_time:244305ms step_avg:118.65ms
step:2060/2245 train_time:244429ms step_avg:118.65ms
step:2061/2245 train_time:244546ms step_avg:118.65ms
step:2062/2245 train_time:244668ms step_avg:118.66ms
step:2063/2245 train_time:244785ms step_avg:118.65ms
step:2064/2245 train_time:244908ms step_avg:118.66ms
step:2065/2245 train_time:245026ms step_avg:118.66ms
step:2066/2245 train_time:245149ms step_avg:118.66ms
step:2067/2245 train_time:245265ms step_avg:118.66ms
step:2068/2245 train_time:245388ms step_avg:118.66ms
step:2069/2245 train_time:245505ms step_avg:118.66ms
step:2070/2245 train_time:245629ms step_avg:118.66ms
step:2071/2245 train_time:245745ms step_avg:118.66ms
step:2072/2245 train_time:245868ms step_avg:118.66ms
step:2073/2245 train_time:245985ms step_avg:118.66ms
step:2074/2245 train_time:246107ms step_avg:118.66ms
step:2075/2245 train_time:246224ms step_avg:118.66ms
step:2076/2245 train_time:246347ms step_avg:118.66ms
step:2077/2245 train_time:246463ms step_avg:118.66ms
step:2078/2245 train_time:246586ms step_avg:118.66ms
step:2079/2245 train_time:246703ms step_avg:118.66ms
step:2080/2245 train_time:246826ms step_avg:118.67ms
step:2081/2245 train_time:246943ms step_avg:118.67ms
step:2082/2245 train_time:247066ms step_avg:118.67ms
step:2083/2245 train_time:247182ms step_avg:118.67ms
step:2084/2245 train_time:247306ms step_avg:118.67ms
step:2085/2245 train_time:247422ms step_avg:118.67ms
step:2086/2245 train_time:247545ms step_avg:118.67ms
step:2087/2245 train_time:247661ms step_avg:118.67ms
step:2088/2245 train_time:247784ms step_avg:118.67ms
step:2089/2245 train_time:247901ms step_avg:118.67ms
step:2090/2245 train_time:248024ms step_avg:118.67ms
step:2091/2245 train_time:248140ms step_avg:118.67ms
step:2092/2245 train_time:248263ms step_avg:118.67ms
step:2093/2245 train_time:248379ms step_avg:118.67ms
step:2094/2245 train_time:248502ms step_avg:118.67ms
step:2095/2245 train_time:248619ms step_avg:118.67ms
step:2096/2245 train_time:248742ms step_avg:118.67ms
step:2097/2245 train_time:248858ms step_avg:118.67ms
step:2098/2245 train_time:248982ms step_avg:118.68ms
step:2099/2245 train_time:249097ms step_avg:118.67ms
step:2100/2245 train_time:249221ms step_avg:118.68ms
step:2101/2245 train_time:249337ms step_avg:118.68ms
step:2102/2245 train_time:249460ms step_avg:118.68ms
step:2103/2245 train_time:249577ms step_avg:118.68ms
step:2104/2245 train_time:249699ms step_avg:118.68ms
step:2105/2245 train_time:249816ms step_avg:118.68ms
step:2106/2245 train_time:249939ms step_avg:118.68ms
step:2107/2245 train_time:250055ms step_avg:118.68ms
step:2108/2245 train_time:250178ms step_avg:118.68ms
step:2109/2245 train_time:250295ms step_avg:118.68ms
step:2110/2245 train_time:250418ms step_avg:118.68ms
step:2111/2245 train_time:250535ms step_avg:118.68ms
step:2112/2245 train_time:250658ms step_avg:118.68ms
step:2113/2245 train_time:250775ms step_avg:118.68ms
step:2114/2245 train_time:250898ms step_avg:118.68ms
step:2115/2245 train_time:251014ms step_avg:118.68ms
step:2116/2245 train_time:251137ms step_avg:118.68ms
step:2117/2245 train_time:251253ms step_avg:118.68ms
step:2118/2245 train_time:251376ms step_avg:118.69ms
step:2119/2245 train_time:251492ms step_avg:118.68ms
step:2120/2245 train_time:251616ms step_avg:118.69ms
step:2121/2245 train_time:251732ms step_avg:118.69ms
step:2122/2245 train_time:251855ms step_avg:118.69ms
step:2123/2245 train_time:251972ms step_avg:118.69ms
step:2124/2245 train_time:252096ms step_avg:118.69ms
step:2125/2245 train_time:252213ms step_avg:118.69ms
step:2126/2245 train_time:252336ms step_avg:118.69ms
step:2127/2245 train_time:252452ms step_avg:118.69ms
step:2128/2245 train_time:252575ms step_avg:118.69ms
step:2129/2245 train_time:252692ms step_avg:118.69ms
step:2130/2245 train_time:252815ms step_avg:118.69ms
step:2131/2245 train_time:252932ms step_avg:118.69ms
step:2132/2245 train_time:253055ms step_avg:118.69ms
step:2133/2245 train_time:253172ms step_avg:118.69ms
step:2134/2245 train_time:253295ms step_avg:118.70ms
step:2135/2245 train_time:253413ms step_avg:118.69ms
step:2136/2245 train_time:253535ms step_avg:118.70ms
step:2137/2245 train_time:253652ms step_avg:118.70ms
step:2138/2245 train_time:253774ms step_avg:118.70ms
step:2139/2245 train_time:253891ms step_avg:118.70ms
step:2140/2245 train_time:254014ms step_avg:118.70ms
step:2141/2245 train_time:254130ms step_avg:118.70ms
step:2142/2245 train_time:254253ms step_avg:118.70ms
step:2143/2245 train_time:254369ms step_avg:118.70ms
step:2144/2245 train_time:254493ms step_avg:118.70ms
step:2145/2245 train_time:254609ms step_avg:118.70ms
step:2146/2245 train_time:254733ms step_avg:118.70ms
step:2147/2245 train_time:254849ms step_avg:118.70ms
step:2148/2245 train_time:254972ms step_avg:118.70ms
step:2149/2245 train_time:255089ms step_avg:118.70ms
step:2150/2245 train_time:255212ms step_avg:118.70ms
step:2151/2245 train_time:255329ms step_avg:118.70ms
step:2152/2245 train_time:255453ms step_avg:118.70ms
step:2153/2245 train_time:255569ms step_avg:118.70ms
step:2154/2245 train_time:255693ms step_avg:118.71ms
step:2155/2245 train_time:255810ms step_avg:118.71ms
step:2156/2245 train_time:255933ms step_avg:118.71ms
step:2157/2245 train_time:256049ms step_avg:118.71ms
step:2158/2245 train_time:256172ms step_avg:118.71ms
step:2159/2245 train_time:256289ms step_avg:118.71ms
step:2160/2245 train_time:256413ms step_avg:118.71ms
step:2161/2245 train_time:256529ms step_avg:118.71ms
step:2162/2245 train_time:256652ms step_avg:118.71ms
step:2163/2245 train_time:256769ms step_avg:118.71ms
step:2164/2245 train_time:256892ms step_avg:118.71ms
step:2165/2245 train_time:257008ms step_avg:118.71ms
step:2166/2245 train_time:257132ms step_avg:118.71ms
step:2167/2245 train_time:257248ms step_avg:118.71ms
step:2168/2245 train_time:257372ms step_avg:118.71ms
step:2169/2245 train_time:257488ms step_avg:118.71ms
step:2170/2245 train_time:257611ms step_avg:118.71ms
step:2171/2245 train_time:257728ms step_avg:118.71ms
step:2172/2245 train_time:257851ms step_avg:118.72ms
step:2173/2245 train_time:257967ms step_avg:118.71ms
step:2174/2245 train_time:258091ms step_avg:118.72ms
step:2175/2245 train_time:258208ms step_avg:118.72ms
step:2176/2245 train_time:258331ms step_avg:118.72ms
step:2177/2245 train_time:258448ms step_avg:118.72ms
step:2178/2245 train_time:258571ms step_avg:118.72ms
step:2179/2245 train_time:258688ms step_avg:118.72ms
step:2180/2245 train_time:258811ms step_avg:118.72ms
step:2181/2245 train_time:258928ms step_avg:118.72ms
step:2182/2245 train_time:259051ms step_avg:118.72ms
step:2183/2245 train_time:259168ms step_avg:118.72ms
step:2184/2245 train_time:259292ms step_avg:118.72ms
step:2185/2245 train_time:259408ms step_avg:118.72ms
step:2186/2245 train_time:259531ms step_avg:118.72ms
step:2187/2245 train_time:259648ms step_avg:118.72ms
step:2188/2245 train_time:259772ms step_avg:118.73ms
step:2189/2245 train_time:259889ms step_avg:118.72ms
step:2190/2245 train_time:260012ms step_avg:118.73ms
step:2191/2245 train_time:260129ms step_avg:118.73ms
step:2192/2245 train_time:260252ms step_avg:118.73ms
step:2193/2245 train_time:260369ms step_avg:118.73ms
step:2194/2245 train_time:260492ms step_avg:118.73ms
step:2195/2245 train_time:260608ms step_avg:118.73ms
step:2196/2245 train_time:260731ms step_avg:118.73ms
step:2197/2245 train_time:260848ms step_avg:118.73ms
step:2198/2245 train_time:260971ms step_avg:118.73ms
step:2199/2245 train_time:261087ms step_avg:118.73ms
step:2200/2245 train_time:261211ms step_avg:118.73ms
step:2201/2245 train_time:261327ms step_avg:118.73ms
step:2202/2245 train_time:261450ms step_avg:118.73ms
step:2203/2245 train_time:261567ms step_avg:118.73ms
step:2204/2245 train_time:261691ms step_avg:118.73ms
step:2205/2245 train_time:261807ms step_avg:118.73ms
step:2206/2245 train_time:261931ms step_avg:118.74ms
step:2207/2245 train_time:262048ms step_avg:118.73ms
step:2208/2245 train_time:262172ms step_avg:118.74ms
step:2209/2245 train_time:262289ms step_avg:118.74ms
step:2210/2245 train_time:262412ms step_avg:118.74ms
step:2211/2245 train_time:262529ms step_avg:118.74ms
step:2212/2245 train_time:262653ms step_avg:118.74ms
step:2213/2245 train_time:262770ms step_avg:118.74ms
step:2214/2245 train_time:262894ms step_avg:118.74ms
step:2215/2245 train_time:263011ms step_avg:118.74ms
step:2216/2245 train_time:263134ms step_avg:118.74ms
step:2217/2245 train_time:263251ms step_avg:118.74ms
step:2218/2245 train_time:263374ms step_avg:118.74ms
step:2219/2245 train_time:263491ms step_avg:118.74ms
step:2220/2245 train_time:263615ms step_avg:118.75ms
step:2221/2245 train_time:263732ms step_avg:118.74ms
step:2222/2245 train_time:263855ms step_avg:118.75ms
step:2223/2245 train_time:263971ms step_avg:118.75ms
step:2224/2245 train_time:264096ms step_avg:118.75ms
step:2225/2245 train_time:264212ms step_avg:118.75ms
step:2226/2245 train_time:264336ms step_avg:118.75ms
step:2227/2245 train_time:264453ms step_avg:118.75ms
step:2228/2245 train_time:264576ms step_avg:118.75ms
step:2229/2245 train_time:264693ms step_avg:118.75ms
step:2230/2245 train_time:264816ms step_avg:118.75ms
step:2231/2245 train_time:264933ms step_avg:118.75ms
step:2232/2245 train_time:265057ms step_avg:118.75ms
step:2233/2245 train_time:265173ms step_avg:118.75ms
step:2234/2245 train_time:265296ms step_avg:118.75ms
step:2235/2245 train_time:265413ms step_avg:118.75ms
step:2236/2245 train_time:265536ms step_avg:118.75ms
step:2237/2245 train_time:265653ms step_avg:118.75ms
step:2238/2245 train_time:265776ms step_avg:118.76ms
step:2239/2245 train_time:265893ms step_avg:118.76ms
step:2240/2245 train_time:266016ms step_avg:118.76ms
step:2241/2245 train_time:266133ms step_avg:118.76ms
step:2242/2245 train_time:266257ms step_avg:118.76ms
step:2243/2245 train_time:266373ms step_avg:118.76ms
step:2244/2245 train_time:266496ms step_avg:118.76ms
step:2245/2245 train_time:266613ms step_avg:118.76ms
step:2245/2245 val_loss:3.2780 train_time:266682ms step_avg:118.79ms
peak memory allocated: 29050 MiB reserved: 44436 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 10:27:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   41C    P0            116W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   41C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   41C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   41C    P0            120W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3344917      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3344918      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3344919      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3344920      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3344918      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3344919      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3344920      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:168ms step_avg:168.25ms
step:2/2245 train_time:223ms step_avg:111.30ms
step:3/2245 train_time:332ms step_avg:110.60ms
step:4/2245 train_time:450ms step_avg:112.54ms
step:5/2245 train_time:566ms step_avg:113.13ms
step:6/2245 train_time:688ms step_avg:114.72ms
step:7/2245 train_time:804ms step_avg:114.89ms
step:8/2245 train_time:927ms step_avg:115.83ms
step:9/2245 train_time:1043ms step_avg:115.94ms
step:10/2245 train_time:1167ms step_avg:116.66ms
step:11/2245 train_time:1283ms step_avg:116.67ms
step:12/2245 train_time:1406ms step_avg:117.20ms
step:13/2245 train_time:1523ms step_avg:117.14ms
step:14/2245 train_time:1646ms step_avg:117.58ms
step:15/2245 train_time:1763ms step_avg:117.50ms
step:16/2245 train_time:1885ms step_avg:117.83ms
step:17/2245 train_time:2002ms step_avg:117.77ms
step:18/2245 train_time:2125ms step_avg:118.03ms
step:19/2245 train_time:2241ms step_avg:117.93ms
step:20/2245 train_time:2363ms step_avg:118.15ms
step:21/2245 train_time:2479ms step_avg:118.04ms
step:22/2245 train_time:2602ms step_avg:118.27ms
step:23/2245 train_time:2718ms step_avg:118.18ms
step:24/2245 train_time:2841ms step_avg:118.37ms
step:25/2245 train_time:2956ms step_avg:118.26ms
step:26/2245 train_time:3079ms step_avg:118.41ms
step:27/2245 train_time:3195ms step_avg:118.34ms
step:28/2245 train_time:3317ms step_avg:118.47ms
step:29/2245 train_time:3433ms step_avg:118.37ms
step:30/2245 train_time:3555ms step_avg:118.50ms
step:31/2245 train_time:3671ms step_avg:118.41ms
step:32/2245 train_time:3793ms step_avg:118.54ms
step:33/2245 train_time:3909ms step_avg:118.44ms
step:34/2245 train_time:4031ms step_avg:118.55ms
step:35/2245 train_time:4147ms step_avg:118.47ms
step:36/2245 train_time:4268ms step_avg:118.57ms
step:37/2245 train_time:4385ms step_avg:118.51ms
step:38/2245 train_time:4507ms step_avg:118.61ms
step:39/2245 train_time:4623ms step_avg:118.53ms
step:40/2245 train_time:4744ms step_avg:118.61ms
step:41/2245 train_time:4860ms step_avg:118.53ms
step:42/2245 train_time:4982ms step_avg:118.62ms
step:43/2245 train_time:5098ms step_avg:118.55ms
step:44/2245 train_time:5220ms step_avg:118.63ms
step:45/2245 train_time:5336ms step_avg:118.57ms
step:46/2245 train_time:5457ms step_avg:118.63ms
step:47/2245 train_time:5573ms step_avg:118.57ms
step:48/2245 train_time:5695ms step_avg:118.65ms
step:49/2245 train_time:5811ms step_avg:118.58ms
step:50/2245 train_time:5932ms step_avg:118.65ms
step:51/2245 train_time:6048ms step_avg:118.58ms
step:52/2245 train_time:6170ms step_avg:118.65ms
step:53/2245 train_time:6285ms step_avg:118.59ms
step:54/2245 train_time:6407ms step_avg:118.64ms
step:55/2245 train_time:6522ms step_avg:118.58ms
step:56/2245 train_time:6644ms step_avg:118.64ms
step:57/2245 train_time:6759ms step_avg:118.58ms
step:58/2245 train_time:6881ms step_avg:118.64ms
step:59/2245 train_time:6996ms step_avg:118.58ms
step:60/2245 train_time:7119ms step_avg:118.64ms
step:61/2245 train_time:7234ms step_avg:118.59ms
step:62/2245 train_time:7356ms step_avg:118.64ms
step:63/2245 train_time:7471ms step_avg:118.59ms
step:64/2245 train_time:7593ms step_avg:118.64ms
step:65/2245 train_time:7708ms step_avg:118.58ms
step:66/2245 train_time:7830ms step_avg:118.63ms
step:67/2245 train_time:7945ms step_avg:118.59ms
step:68/2245 train_time:8067ms step_avg:118.63ms
step:69/2245 train_time:8182ms step_avg:118.59ms
step:70/2245 train_time:8304ms step_avg:118.63ms
step:71/2245 train_time:8419ms step_avg:118.58ms
step:72/2245 train_time:8541ms step_avg:118.63ms
step:73/2245 train_time:8656ms step_avg:118.58ms
step:74/2245 train_time:8778ms step_avg:118.62ms
step:75/2245 train_time:8893ms step_avg:118.57ms
step:76/2245 train_time:9015ms step_avg:118.62ms
step:77/2245 train_time:9130ms step_avg:118.57ms
step:78/2245 train_time:9252ms step_avg:118.61ms
step:79/2245 train_time:9367ms step_avg:118.56ms
step:80/2245 train_time:9488ms step_avg:118.60ms
step:81/2245 train_time:9604ms step_avg:118.56ms
step:82/2245 train_time:9725ms step_avg:118.60ms
step:83/2245 train_time:9840ms step_avg:118.56ms
step:84/2245 train_time:9961ms step_avg:118.59ms
step:85/2245 train_time:10077ms step_avg:118.55ms
step:86/2245 train_time:10199ms step_avg:118.59ms
step:87/2245 train_time:10314ms step_avg:118.55ms
step:88/2245 train_time:10435ms step_avg:118.58ms
step:89/2245 train_time:10550ms step_avg:118.54ms
step:90/2245 train_time:10671ms step_avg:118.57ms
step:91/2245 train_time:10786ms step_avg:118.53ms
step:92/2245 train_time:10907ms step_avg:118.56ms
step:93/2245 train_time:11023ms step_avg:118.52ms
step:94/2245 train_time:11144ms step_avg:118.55ms
step:95/2245 train_time:11259ms step_avg:118.51ms
step:96/2245 train_time:11380ms step_avg:118.54ms
step:97/2245 train_time:11496ms step_avg:118.51ms
step:98/2245 train_time:11617ms step_avg:118.54ms
step:99/2245 train_time:11732ms step_avg:118.50ms
step:100/2245 train_time:11853ms step_avg:118.53ms
step:101/2245 train_time:11968ms step_avg:118.49ms
step:102/2245 train_time:12089ms step_avg:118.52ms
step:103/2245 train_time:12204ms step_avg:118.49ms
step:104/2245 train_time:12325ms step_avg:118.51ms
step:105/2245 train_time:12441ms step_avg:118.48ms
step:106/2245 train_time:12562ms step_avg:118.51ms
step:107/2245 train_time:12677ms step_avg:118.48ms
step:108/2245 train_time:12798ms step_avg:118.50ms
step:109/2245 train_time:12913ms step_avg:118.47ms
step:110/2245 train_time:13034ms step_avg:118.49ms
step:111/2245 train_time:13149ms step_avg:118.46ms
step:112/2245 train_time:13270ms step_avg:118.48ms
step:113/2245 train_time:13385ms step_avg:118.45ms
step:114/2245 train_time:13506ms step_avg:118.48ms
step:115/2245 train_time:13621ms step_avg:118.45ms
step:116/2245 train_time:13743ms step_avg:118.47ms
step:117/2245 train_time:13857ms step_avg:118.44ms
step:118/2245 train_time:13979ms step_avg:118.46ms
step:119/2245 train_time:14094ms step_avg:118.44ms
step:120/2245 train_time:14214ms step_avg:118.45ms
step:121/2245 train_time:14330ms step_avg:118.43ms
step:122/2245 train_time:14451ms step_avg:118.45ms
step:123/2245 train_time:14565ms step_avg:118.42ms
step:124/2245 train_time:14686ms step_avg:118.44ms
step:125/2245 train_time:14801ms step_avg:118.41ms
step:126/2245 train_time:14922ms step_avg:118.43ms
step:127/2245 train_time:15037ms step_avg:118.40ms
step:128/2245 train_time:15158ms step_avg:118.42ms
step:129/2245 train_time:15272ms step_avg:118.39ms
step:130/2245 train_time:15394ms step_avg:118.41ms
step:131/2245 train_time:15508ms step_avg:118.38ms
step:132/2245 train_time:15629ms step_avg:118.40ms
step:133/2245 train_time:15743ms step_avg:118.37ms
step:134/2245 train_time:15864ms step_avg:118.39ms
step:135/2245 train_time:15979ms step_avg:118.37ms
step:136/2245 train_time:16100ms step_avg:118.39ms
step:137/2245 train_time:16215ms step_avg:118.36ms
step:138/2245 train_time:16336ms step_avg:118.38ms
step:139/2245 train_time:16450ms step_avg:118.35ms
step:140/2245 train_time:16572ms step_avg:118.37ms
step:141/2245 train_time:16686ms step_avg:118.34ms
step:142/2245 train_time:16807ms step_avg:118.36ms
step:143/2245 train_time:16921ms step_avg:118.33ms
step:144/2245 train_time:17043ms step_avg:118.35ms
step:145/2245 train_time:17157ms step_avg:118.33ms
step:146/2245 train_time:17278ms step_avg:118.35ms
step:147/2245 train_time:17393ms step_avg:118.32ms
step:148/2245 train_time:17514ms step_avg:118.34ms
step:149/2245 train_time:17628ms step_avg:118.31ms
step:150/2245 train_time:17749ms step_avg:118.33ms
step:151/2245 train_time:17864ms step_avg:118.31ms
step:152/2245 train_time:17985ms step_avg:118.32ms
step:153/2245 train_time:18099ms step_avg:118.30ms
step:154/2245 train_time:18220ms step_avg:118.31ms
step:155/2245 train_time:18335ms step_avg:118.29ms
step:156/2245 train_time:18456ms step_avg:118.31ms
step:157/2245 train_time:18570ms step_avg:118.28ms
step:158/2245 train_time:18691ms step_avg:118.30ms
step:159/2245 train_time:18806ms step_avg:118.27ms
step:160/2245 train_time:18926ms step_avg:118.29ms
step:161/2245 train_time:19041ms step_avg:118.27ms
step:162/2245 train_time:19162ms step_avg:118.28ms
step:163/2245 train_time:19276ms step_avg:118.26ms
step:164/2245 train_time:19397ms step_avg:118.27ms
step:165/2245 train_time:19511ms step_avg:118.25ms
step:166/2245 train_time:19632ms step_avg:118.27ms
step:167/2245 train_time:19747ms step_avg:118.24ms
step:168/2245 train_time:19868ms step_avg:118.26ms
step:169/2245 train_time:19982ms step_avg:118.24ms
step:170/2245 train_time:20103ms step_avg:118.25ms
step:171/2245 train_time:20217ms step_avg:118.23ms
step:172/2245 train_time:20338ms step_avg:118.24ms
step:173/2245 train_time:20452ms step_avg:118.22ms
step:174/2245 train_time:20572ms step_avg:118.23ms
step:175/2245 train_time:20687ms step_avg:118.21ms
step:176/2245 train_time:20808ms step_avg:118.23ms
step:177/2245 train_time:20923ms step_avg:118.21ms
step:178/2245 train_time:21043ms step_avg:118.22ms
step:179/2245 train_time:21157ms step_avg:118.20ms
step:180/2245 train_time:21278ms step_avg:118.21ms
step:181/2245 train_time:21393ms step_avg:118.19ms
step:182/2245 train_time:21513ms step_avg:118.21ms
step:183/2245 train_time:21628ms step_avg:118.19ms
step:184/2245 train_time:21749ms step_avg:118.20ms
step:185/2245 train_time:21863ms step_avg:118.18ms
step:186/2245 train_time:21983ms step_avg:118.19ms
step:187/2245 train_time:22098ms step_avg:118.17ms
step:188/2245 train_time:22218ms step_avg:118.18ms
step:189/2245 train_time:22333ms step_avg:118.16ms
step:190/2245 train_time:22453ms step_avg:118.18ms
step:191/2245 train_time:22567ms step_avg:118.15ms
step:192/2245 train_time:22688ms step_avg:118.17ms
step:193/2245 train_time:22803ms step_avg:118.15ms
step:194/2245 train_time:22923ms step_avg:118.16ms
step:195/2245 train_time:23038ms step_avg:118.14ms
step:196/2245 train_time:23158ms step_avg:118.15ms
step:197/2245 train_time:23273ms step_avg:118.14ms
step:198/2245 train_time:23394ms step_avg:118.15ms
step:199/2245 train_time:23508ms step_avg:118.13ms
step:200/2245 train_time:23628ms step_avg:118.14ms
step:201/2245 train_time:23743ms step_avg:118.12ms
step:202/2245 train_time:23864ms step_avg:118.14ms
step:203/2245 train_time:23978ms step_avg:118.12ms
step:204/2245 train_time:24099ms step_avg:118.13ms
step:205/2245 train_time:24213ms step_avg:118.11ms
step:206/2245 train_time:24334ms step_avg:118.13ms
step:207/2245 train_time:24448ms step_avg:118.11ms
step:208/2245 train_time:24569ms step_avg:118.12ms
step:209/2245 train_time:24683ms step_avg:118.10ms
step:210/2245 train_time:24804ms step_avg:118.11ms
step:211/2245 train_time:24918ms step_avg:118.09ms
step:212/2245 train_time:25038ms step_avg:118.11ms
step:213/2245 train_time:25153ms step_avg:118.09ms
step:214/2245 train_time:25273ms step_avg:118.10ms
step:215/2245 train_time:25388ms step_avg:118.08ms
step:216/2245 train_time:25508ms step_avg:118.09ms
step:217/2245 train_time:25622ms step_avg:118.07ms
step:218/2245 train_time:25743ms step_avg:118.09ms
step:219/2245 train_time:25857ms step_avg:118.07ms
step:220/2245 train_time:25977ms step_avg:118.08ms
step:221/2245 train_time:26092ms step_avg:118.06ms
step:222/2245 train_time:26212ms step_avg:118.07ms
step:223/2245 train_time:26327ms step_avg:118.06ms
step:224/2245 train_time:26447ms step_avg:118.07ms
step:225/2245 train_time:26561ms step_avg:118.05ms
step:226/2245 train_time:26682ms step_avg:118.06ms
step:227/2245 train_time:26796ms step_avg:118.05ms
step:228/2245 train_time:26917ms step_avg:118.06ms
step:229/2245 train_time:27031ms step_avg:118.04ms
step:230/2245 train_time:27152ms step_avg:118.05ms
step:231/2245 train_time:27266ms step_avg:118.03ms
step:232/2245 train_time:27386ms step_avg:118.04ms
step:233/2245 train_time:27500ms step_avg:118.03ms
step:234/2245 train_time:27621ms step_avg:118.04ms
step:235/2245 train_time:27735ms step_avg:118.02ms
step:236/2245 train_time:27855ms step_avg:118.03ms
step:237/2245 train_time:27970ms step_avg:118.02ms
step:238/2245 train_time:28090ms step_avg:118.03ms
step:239/2245 train_time:28205ms step_avg:118.01ms
step:240/2245 train_time:28325ms step_avg:118.02ms
step:241/2245 train_time:28439ms step_avg:118.00ms
step:242/2245 train_time:28559ms step_avg:118.01ms
step:243/2245 train_time:28673ms step_avg:118.00ms
step:244/2245 train_time:28794ms step_avg:118.01ms
step:245/2245 train_time:28908ms step_avg:117.99ms
step:246/2245 train_time:29029ms step_avg:118.00ms
step:247/2245 train_time:29143ms step_avg:117.99ms
step:248/2245 train_time:29263ms step_avg:118.00ms
step:249/2245 train_time:29377ms step_avg:117.98ms
step:250/2245 train_time:29498ms step_avg:117.99ms
step:250/2245 val_loss:4.1199 train_time:29563ms step_avg:118.25ms
step:251/2245 train_time:29613ms step_avg:117.98ms
step:252/2245 train_time:29733ms step_avg:117.99ms
step:253/2245 train_time:29848ms step_avg:117.98ms
step:254/2245 train_time:29968ms step_avg:117.98ms
step:255/2245 train_time:30082ms step_avg:117.97ms
step:256/2245 train_time:30202ms step_avg:117.98ms
step:257/2245 train_time:30317ms step_avg:117.96ms
step:258/2245 train_time:30437ms step_avg:117.97ms
step:259/2245 train_time:30551ms step_avg:117.96ms
step:260/2245 train_time:30672ms step_avg:117.97ms
step:261/2245 train_time:30786ms step_avg:117.95ms
step:262/2245 train_time:30907ms step_avg:117.96ms
step:263/2245 train_time:31021ms step_avg:117.95ms
step:264/2245 train_time:31141ms step_avg:117.96ms
step:265/2245 train_time:31255ms step_avg:117.94ms
step:266/2245 train_time:31376ms step_avg:117.95ms
step:267/2245 train_time:31490ms step_avg:117.94ms
step:268/2245 train_time:31610ms step_avg:117.95ms
step:269/2245 train_time:31724ms step_avg:117.93ms
step:270/2245 train_time:31845ms step_avg:117.94ms
step:271/2245 train_time:31959ms step_avg:117.93ms
step:272/2245 train_time:32079ms step_avg:117.94ms
step:273/2245 train_time:32193ms step_avg:117.92ms
step:274/2245 train_time:32313ms step_avg:117.93ms
step:275/2245 train_time:32427ms step_avg:117.92ms
step:276/2245 train_time:32548ms step_avg:117.93ms
step:277/2245 train_time:32662ms step_avg:117.91ms
step:278/2245 train_time:32783ms step_avg:117.92ms
step:279/2245 train_time:32897ms step_avg:117.91ms
step:280/2245 train_time:33017ms step_avg:117.92ms
step:281/2245 train_time:33131ms step_avg:117.90ms
step:282/2245 train_time:33251ms step_avg:117.91ms
step:283/2245 train_time:33365ms step_avg:117.90ms
step:284/2245 train_time:33486ms step_avg:117.91ms
step:285/2245 train_time:33600ms step_avg:117.89ms
step:286/2245 train_time:33720ms step_avg:117.90ms
step:287/2245 train_time:33834ms step_avg:117.89ms
step:288/2245 train_time:33954ms step_avg:117.90ms
step:289/2245 train_time:34069ms step_avg:117.88ms
step:290/2245 train_time:34189ms step_avg:117.89ms
step:291/2245 train_time:34303ms step_avg:117.88ms
step:292/2245 train_time:34424ms step_avg:117.89ms
step:293/2245 train_time:34538ms step_avg:117.88ms
step:294/2245 train_time:34659ms step_avg:117.89ms
step:295/2245 train_time:34773ms step_avg:117.87ms
step:296/2245 train_time:34893ms step_avg:117.88ms
step:297/2245 train_time:35007ms step_avg:117.87ms
step:298/2245 train_time:35127ms step_avg:117.88ms
step:299/2245 train_time:35241ms step_avg:117.86ms
step:300/2245 train_time:35362ms step_avg:117.87ms
step:301/2245 train_time:35476ms step_avg:117.86ms
step:302/2245 train_time:35596ms step_avg:117.87ms
step:303/2245 train_time:35710ms step_avg:117.85ms
step:304/2245 train_time:35830ms step_avg:117.86ms
step:305/2245 train_time:35944ms step_avg:117.85ms
step:306/2245 train_time:36065ms step_avg:117.86ms
step:307/2245 train_time:36179ms step_avg:117.85ms
step:308/2245 train_time:36299ms step_avg:117.85ms
step:309/2245 train_time:36413ms step_avg:117.84ms
step:310/2245 train_time:36533ms step_avg:117.85ms
step:311/2245 train_time:36647ms step_avg:117.84ms
step:312/2245 train_time:36768ms step_avg:117.85ms
step:313/2245 train_time:36882ms step_avg:117.83ms
step:314/2245 train_time:37002ms step_avg:117.84ms
step:315/2245 train_time:37117ms step_avg:117.83ms
step:316/2245 train_time:37237ms step_avg:117.84ms
step:317/2245 train_time:37352ms step_avg:117.83ms
step:318/2245 train_time:37472ms step_avg:117.84ms
step:319/2245 train_time:37586ms step_avg:117.82ms
step:320/2245 train_time:37706ms step_avg:117.83ms
step:321/2245 train_time:37821ms step_avg:117.82ms
step:322/2245 train_time:37941ms step_avg:117.83ms
step:323/2245 train_time:38055ms step_avg:117.82ms
step:324/2245 train_time:38175ms step_avg:117.82ms
step:325/2245 train_time:38289ms step_avg:117.81ms
step:326/2245 train_time:38409ms step_avg:117.82ms
step:327/2245 train_time:38523ms step_avg:117.81ms
step:328/2245 train_time:38644ms step_avg:117.82ms
step:329/2245 train_time:38758ms step_avg:117.80ms
step:330/2245 train_time:38878ms step_avg:117.81ms
step:331/2245 train_time:38992ms step_avg:117.80ms
step:332/2245 train_time:39113ms step_avg:117.81ms
step:333/2245 train_time:39227ms step_avg:117.80ms
step:334/2245 train_time:39347ms step_avg:117.81ms
step:335/2245 train_time:39461ms step_avg:117.80ms
step:336/2245 train_time:39582ms step_avg:117.80ms
step:337/2245 train_time:39696ms step_avg:117.79ms
step:338/2245 train_time:39817ms step_avg:117.80ms
step:339/2245 train_time:39931ms step_avg:117.79ms
step:340/2245 train_time:40051ms step_avg:117.80ms
step:341/2245 train_time:40165ms step_avg:117.79ms
step:342/2245 train_time:40285ms step_avg:117.79ms
step:343/2245 train_time:40399ms step_avg:117.78ms
step:344/2245 train_time:40520ms step_avg:117.79ms
step:345/2245 train_time:40634ms step_avg:117.78ms
step:346/2245 train_time:40754ms step_avg:117.79ms
step:347/2245 train_time:40868ms step_avg:117.78ms
step:348/2245 train_time:40989ms step_avg:117.78ms
step:349/2245 train_time:41103ms step_avg:117.77ms
step:350/2245 train_time:41223ms step_avg:117.78ms
step:351/2245 train_time:41337ms step_avg:117.77ms
step:352/2245 train_time:41457ms step_avg:117.78ms
step:353/2245 train_time:41571ms step_avg:117.76ms
step:354/2245 train_time:41691ms step_avg:117.77ms
step:355/2245 train_time:41805ms step_avg:117.76ms
step:356/2245 train_time:41926ms step_avg:117.77ms
step:357/2245 train_time:42040ms step_avg:117.76ms
step:358/2245 train_time:42160ms step_avg:117.77ms
step:359/2245 train_time:42274ms step_avg:117.76ms
step:360/2245 train_time:42394ms step_avg:117.76ms
step:361/2245 train_time:42508ms step_avg:117.75ms
step:362/2245 train_time:42629ms step_avg:117.76ms
step:363/2245 train_time:42743ms step_avg:117.75ms
step:364/2245 train_time:42864ms step_avg:117.76ms
step:365/2245 train_time:42978ms step_avg:117.75ms
step:366/2245 train_time:43098ms step_avg:117.75ms
step:367/2245 train_time:43212ms step_avg:117.74ms
step:368/2245 train_time:43332ms step_avg:117.75ms
step:369/2245 train_time:43447ms step_avg:117.74ms
step:370/2245 train_time:43567ms step_avg:117.75ms
step:371/2245 train_time:43681ms step_avg:117.74ms
step:372/2245 train_time:43801ms step_avg:117.75ms
step:373/2245 train_time:43915ms step_avg:117.74ms
step:374/2245 train_time:44036ms step_avg:117.74ms
step:375/2245 train_time:44150ms step_avg:117.73ms
step:376/2245 train_time:44270ms step_avg:117.74ms
step:377/2245 train_time:44384ms step_avg:117.73ms
step:378/2245 train_time:44505ms step_avg:117.74ms
step:379/2245 train_time:44619ms step_avg:117.73ms
step:380/2245 train_time:44739ms step_avg:117.73ms
step:381/2245 train_time:44853ms step_avg:117.72ms
step:382/2245 train_time:44973ms step_avg:117.73ms
step:383/2245 train_time:45087ms step_avg:117.72ms
step:384/2245 train_time:45208ms step_avg:117.73ms
step:385/2245 train_time:45322ms step_avg:117.72ms
step:386/2245 train_time:45442ms step_avg:117.73ms
step:387/2245 train_time:45556ms step_avg:117.72ms
step:388/2245 train_time:45677ms step_avg:117.72ms
step:389/2245 train_time:45790ms step_avg:117.71ms
step:390/2245 train_time:45911ms step_avg:117.72ms
step:391/2245 train_time:46025ms step_avg:117.71ms
step:392/2245 train_time:46146ms step_avg:117.72ms
step:393/2245 train_time:46260ms step_avg:117.71ms
step:394/2245 train_time:46380ms step_avg:117.72ms
step:395/2245 train_time:46494ms step_avg:117.71ms
step:396/2245 train_time:46615ms step_avg:117.71ms
step:397/2245 train_time:46729ms step_avg:117.70ms
step:398/2245 train_time:46849ms step_avg:117.71ms
step:399/2245 train_time:46963ms step_avg:117.70ms
step:400/2245 train_time:47084ms step_avg:117.71ms
step:401/2245 train_time:47197ms step_avg:117.70ms
step:402/2245 train_time:47317ms step_avg:117.71ms
step:403/2245 train_time:47432ms step_avg:117.70ms
step:404/2245 train_time:47552ms step_avg:117.70ms
step:405/2245 train_time:47666ms step_avg:117.69ms
step:406/2245 train_time:47787ms step_avg:117.70ms
step:407/2245 train_time:47901ms step_avg:117.69ms
step:408/2245 train_time:48021ms step_avg:117.70ms
step:409/2245 train_time:48136ms step_avg:117.69ms
step:410/2245 train_time:48256ms step_avg:117.70ms
step:411/2245 train_time:48369ms step_avg:117.69ms
step:412/2245 train_time:48490ms step_avg:117.69ms
step:413/2245 train_time:48604ms step_avg:117.69ms
step:414/2245 train_time:48724ms step_avg:117.69ms
step:415/2245 train_time:48839ms step_avg:117.68ms
step:416/2245 train_time:48959ms step_avg:117.69ms
step:417/2245 train_time:49073ms step_avg:117.68ms
step:418/2245 train_time:49193ms step_avg:117.69ms
step:419/2245 train_time:49307ms step_avg:117.68ms
step:420/2245 train_time:49427ms step_avg:117.68ms
step:421/2245 train_time:49542ms step_avg:117.68ms
step:422/2245 train_time:49662ms step_avg:117.68ms
step:423/2245 train_time:49776ms step_avg:117.67ms
step:424/2245 train_time:49897ms step_avg:117.68ms
step:425/2245 train_time:50011ms step_avg:117.67ms
step:426/2245 train_time:50131ms step_avg:117.68ms
step:427/2245 train_time:50246ms step_avg:117.67ms
step:428/2245 train_time:50366ms step_avg:117.68ms
step:429/2245 train_time:50480ms step_avg:117.67ms
step:430/2245 train_time:50600ms step_avg:117.67ms
step:431/2245 train_time:50714ms step_avg:117.67ms
step:432/2245 train_time:50834ms step_avg:117.67ms
step:433/2245 train_time:50948ms step_avg:117.66ms
step:434/2245 train_time:51069ms step_avg:117.67ms
step:435/2245 train_time:51182ms step_avg:117.66ms
step:436/2245 train_time:51303ms step_avg:117.67ms
step:437/2245 train_time:51418ms step_avg:117.66ms
step:438/2245 train_time:51538ms step_avg:117.67ms
step:439/2245 train_time:51652ms step_avg:117.66ms
step:440/2245 train_time:51772ms step_avg:117.66ms
step:441/2245 train_time:51886ms step_avg:117.66ms
step:442/2245 train_time:52007ms step_avg:117.66ms
step:443/2245 train_time:52121ms step_avg:117.65ms
step:444/2245 train_time:52241ms step_avg:117.66ms
step:445/2245 train_time:52354ms step_avg:117.65ms
step:446/2245 train_time:52475ms step_avg:117.66ms
step:447/2245 train_time:52589ms step_avg:117.65ms
step:448/2245 train_time:52709ms step_avg:117.65ms
step:449/2245 train_time:52823ms step_avg:117.65ms
step:450/2245 train_time:52943ms step_avg:117.65ms
step:451/2245 train_time:53057ms step_avg:117.64ms
step:452/2245 train_time:53177ms step_avg:117.65ms
step:453/2245 train_time:53291ms step_avg:117.64ms
step:454/2245 train_time:53412ms step_avg:117.65ms
step:455/2245 train_time:53526ms step_avg:117.64ms
step:456/2245 train_time:53646ms step_avg:117.65ms
step:457/2245 train_time:53760ms step_avg:117.64ms
step:458/2245 train_time:53881ms step_avg:117.64ms
step:459/2245 train_time:53995ms step_avg:117.64ms
step:460/2245 train_time:54115ms step_avg:117.64ms
step:461/2245 train_time:54229ms step_avg:117.63ms
step:462/2245 train_time:54350ms step_avg:117.64ms
step:463/2245 train_time:54464ms step_avg:117.63ms
step:464/2245 train_time:54584ms step_avg:117.64ms
step:465/2245 train_time:54699ms step_avg:117.63ms
step:466/2245 train_time:54819ms step_avg:117.64ms
step:467/2245 train_time:54933ms step_avg:117.63ms
step:468/2245 train_time:55053ms step_avg:117.64ms
step:469/2245 train_time:55167ms step_avg:117.63ms
step:470/2245 train_time:55288ms step_avg:117.63ms
step:471/2245 train_time:55402ms step_avg:117.63ms
step:472/2245 train_time:55522ms step_avg:117.63ms
step:473/2245 train_time:55636ms step_avg:117.62ms
step:474/2245 train_time:55757ms step_avg:117.63ms
step:475/2245 train_time:55871ms step_avg:117.62ms
step:476/2245 train_time:55991ms step_avg:117.63ms
step:477/2245 train_time:56105ms step_avg:117.62ms
step:478/2245 train_time:56225ms step_avg:117.63ms
step:479/2245 train_time:56340ms step_avg:117.62ms
step:480/2245 train_time:56460ms step_avg:117.63ms
step:481/2245 train_time:56574ms step_avg:117.62ms
step:482/2245 train_time:56694ms step_avg:117.62ms
step:483/2245 train_time:56808ms step_avg:117.62ms
step:484/2245 train_time:56929ms step_avg:117.62ms
step:485/2245 train_time:57043ms step_avg:117.62ms
step:486/2245 train_time:57164ms step_avg:117.62ms
step:487/2245 train_time:57278ms step_avg:117.61ms
step:488/2245 train_time:57399ms step_avg:117.62ms
step:489/2245 train_time:57513ms step_avg:117.61ms
step:490/2245 train_time:57633ms step_avg:117.62ms
step:491/2245 train_time:57747ms step_avg:117.61ms
step:492/2245 train_time:57868ms step_avg:117.62ms
step:493/2245 train_time:57982ms step_avg:117.61ms
step:494/2245 train_time:58103ms step_avg:117.62ms
step:495/2245 train_time:58216ms step_avg:117.61ms
step:496/2245 train_time:58337ms step_avg:117.61ms
step:497/2245 train_time:58451ms step_avg:117.61ms
step:498/2245 train_time:58571ms step_avg:117.61ms
step:499/2245 train_time:58685ms step_avg:117.61ms
step:500/2245 train_time:58806ms step_avg:117.61ms
step:500/2245 val_loss:3.8319 train_time:58871ms step_avg:117.74ms
step:501/2245 train_time:58921ms step_avg:117.61ms
step:502/2245 train_time:59041ms step_avg:117.61ms
step:503/2245 train_time:59155ms step_avg:117.61ms
step:504/2245 train_time:59275ms step_avg:117.61ms
step:505/2245 train_time:59390ms step_avg:117.60ms
step:506/2245 train_time:59510ms step_avg:117.61ms
step:507/2245 train_time:59624ms step_avg:117.60ms
step:508/2245 train_time:59745ms step_avg:117.61ms
step:509/2245 train_time:59859ms step_avg:117.60ms
step:510/2245 train_time:59979ms step_avg:117.61ms
step:511/2245 train_time:60093ms step_avg:117.60ms
step:512/2245 train_time:60214ms step_avg:117.61ms
step:513/2245 train_time:60328ms step_avg:117.60ms
step:514/2245 train_time:60448ms step_avg:117.60ms
step:515/2245 train_time:60562ms step_avg:117.60ms
step:516/2245 train_time:60682ms step_avg:117.60ms
step:517/2245 train_time:60797ms step_avg:117.60ms
step:518/2245 train_time:60917ms step_avg:117.60ms
step:519/2245 train_time:61031ms step_avg:117.59ms
step:520/2245 train_time:61151ms step_avg:117.60ms
step:521/2245 train_time:61266ms step_avg:117.59ms
step:522/2245 train_time:61386ms step_avg:117.60ms
step:523/2245 train_time:61500ms step_avg:117.59ms
step:524/2245 train_time:61620ms step_avg:117.60ms
step:525/2245 train_time:61734ms step_avg:117.59ms
step:526/2245 train_time:61855ms step_avg:117.59ms
step:527/2245 train_time:61969ms step_avg:117.59ms
step:528/2245 train_time:62089ms step_avg:117.59ms
step:529/2245 train_time:62204ms step_avg:117.59ms
step:530/2245 train_time:62324ms step_avg:117.59ms
step:531/2245 train_time:62438ms step_avg:117.59ms
step:532/2245 train_time:62559ms step_avg:117.59ms
step:533/2245 train_time:62672ms step_avg:117.58ms
step:534/2245 train_time:62793ms step_avg:117.59ms
step:535/2245 train_time:62907ms step_avg:117.58ms
step:536/2245 train_time:63027ms step_avg:117.59ms
step:537/2245 train_time:63141ms step_avg:117.58ms
step:538/2245 train_time:63261ms step_avg:117.59ms
step:539/2245 train_time:63375ms step_avg:117.58ms
step:540/2245 train_time:63496ms step_avg:117.58ms
step:541/2245 train_time:63610ms step_avg:117.58ms
step:542/2245 train_time:63730ms step_avg:117.58ms
step:543/2245 train_time:63844ms step_avg:117.58ms
step:544/2245 train_time:63964ms step_avg:117.58ms
step:545/2245 train_time:64078ms step_avg:117.57ms
step:546/2245 train_time:64199ms step_avg:117.58ms
step:547/2245 train_time:64313ms step_avg:117.57ms
step:548/2245 train_time:64433ms step_avg:117.58ms
step:549/2245 train_time:64547ms step_avg:117.57ms
step:550/2245 train_time:64667ms step_avg:117.58ms
step:551/2245 train_time:64782ms step_avg:117.57ms
step:552/2245 train_time:64902ms step_avg:117.58ms
step:553/2245 train_time:65016ms step_avg:117.57ms
step:554/2245 train_time:65136ms step_avg:117.57ms
step:555/2245 train_time:65250ms step_avg:117.57ms
step:556/2245 train_time:65370ms step_avg:117.57ms
step:557/2245 train_time:65484ms step_avg:117.57ms
step:558/2245 train_time:65605ms step_avg:117.57ms
step:559/2245 train_time:65719ms step_avg:117.56ms
step:560/2245 train_time:65839ms step_avg:117.57ms
step:561/2245 train_time:65953ms step_avg:117.56ms
step:562/2245 train_time:66073ms step_avg:117.57ms
step:563/2245 train_time:66188ms step_avg:117.56ms
step:564/2245 train_time:66308ms step_avg:117.57ms
step:565/2245 train_time:66422ms step_avg:117.56ms
step:566/2245 train_time:66543ms step_avg:117.57ms
step:567/2245 train_time:66657ms step_avg:117.56ms
step:568/2245 train_time:66778ms step_avg:117.57ms
step:569/2245 train_time:66893ms step_avg:117.56ms
step:570/2245 train_time:67013ms step_avg:117.57ms
step:571/2245 train_time:67128ms step_avg:117.56ms
step:572/2245 train_time:67248ms step_avg:117.57ms
step:573/2245 train_time:67362ms step_avg:117.56ms
step:574/2245 train_time:67483ms step_avg:117.57ms
step:575/2245 train_time:67598ms step_avg:117.56ms
step:576/2245 train_time:67718ms step_avg:117.57ms
step:577/2245 train_time:67832ms step_avg:117.56ms
step:578/2245 train_time:67953ms step_avg:117.57ms
step:579/2245 train_time:68067ms step_avg:117.56ms
step:580/2245 train_time:68187ms step_avg:117.56ms
step:581/2245 train_time:68301ms step_avg:117.56ms
step:582/2245 train_time:68422ms step_avg:117.56ms
step:583/2245 train_time:68536ms step_avg:117.56ms
step:584/2245 train_time:68656ms step_avg:117.56ms
step:585/2245 train_time:68770ms step_avg:117.56ms
step:586/2245 train_time:68891ms step_avg:117.56ms
step:587/2245 train_time:69005ms step_avg:117.56ms
step:588/2245 train_time:69125ms step_avg:117.56ms
step:589/2245 train_time:69239ms step_avg:117.55ms
step:590/2245 train_time:69360ms step_avg:117.56ms
step:591/2245 train_time:69474ms step_avg:117.55ms
step:592/2245 train_time:69595ms step_avg:117.56ms
step:593/2245 train_time:69709ms step_avg:117.55ms
step:594/2245 train_time:69829ms step_avg:117.56ms
step:595/2245 train_time:69943ms step_avg:117.55ms
step:596/2245 train_time:70064ms step_avg:117.56ms
step:597/2245 train_time:70178ms step_avg:117.55ms
step:598/2245 train_time:70298ms step_avg:117.56ms
step:599/2245 train_time:70412ms step_avg:117.55ms
step:600/2245 train_time:70533ms step_avg:117.55ms
step:601/2245 train_time:70647ms step_avg:117.55ms
step:602/2245 train_time:70767ms step_avg:117.55ms
step:603/2245 train_time:70881ms step_avg:117.55ms
step:604/2245 train_time:71002ms step_avg:117.55ms
step:605/2245 train_time:71116ms step_avg:117.55ms
step:606/2245 train_time:71236ms step_avg:117.55ms
step:607/2245 train_time:71350ms step_avg:117.55ms
step:608/2245 train_time:71471ms step_avg:117.55ms
step:609/2245 train_time:71585ms step_avg:117.54ms
step:610/2245 train_time:71705ms step_avg:117.55ms
step:611/2245 train_time:71819ms step_avg:117.54ms
step:612/2245 train_time:71940ms step_avg:117.55ms
step:613/2245 train_time:72054ms step_avg:117.54ms
step:614/2245 train_time:72174ms step_avg:117.55ms
step:615/2245 train_time:72288ms step_avg:117.54ms
step:616/2245 train_time:72409ms step_avg:117.55ms
step:617/2245 train_time:72523ms step_avg:117.54ms
step:618/2245 train_time:72643ms step_avg:117.55ms
step:619/2245 train_time:72758ms step_avg:117.54ms
step:620/2245 train_time:72878ms step_avg:117.55ms
step:621/2245 train_time:72992ms step_avg:117.54ms
step:622/2245 train_time:73113ms step_avg:117.54ms
step:623/2245 train_time:73227ms step_avg:117.54ms
step:624/2245 train_time:73347ms step_avg:117.54ms
step:625/2245 train_time:73461ms step_avg:117.54ms
step:626/2245 train_time:73581ms step_avg:117.54ms
step:627/2245 train_time:73696ms step_avg:117.54ms
step:628/2245 train_time:73816ms step_avg:117.54ms
step:629/2245 train_time:73930ms step_avg:117.54ms
step:630/2245 train_time:74050ms step_avg:117.54ms
step:631/2245 train_time:74165ms step_avg:117.53ms
step:632/2245 train_time:74285ms step_avg:117.54ms
step:633/2245 train_time:74399ms step_avg:117.53ms
step:634/2245 train_time:74520ms step_avg:117.54ms
step:635/2245 train_time:74634ms step_avg:117.53ms
step:636/2245 train_time:74755ms step_avg:117.54ms
step:637/2245 train_time:74869ms step_avg:117.53ms
step:638/2245 train_time:74989ms step_avg:117.54ms
step:639/2245 train_time:75103ms step_avg:117.53ms
step:640/2245 train_time:75224ms step_avg:117.54ms
step:641/2245 train_time:75338ms step_avg:117.53ms
step:642/2245 train_time:75458ms step_avg:117.54ms
step:643/2245 train_time:75572ms step_avg:117.53ms
step:644/2245 train_time:75692ms step_avg:117.53ms
step:645/2245 train_time:75807ms step_avg:117.53ms
step:646/2245 train_time:75927ms step_avg:117.53ms
step:647/2245 train_time:76041ms step_avg:117.53ms
step:648/2245 train_time:76162ms step_avg:117.53ms
step:649/2245 train_time:76276ms step_avg:117.53ms
step:650/2245 train_time:76397ms step_avg:117.53ms
step:651/2245 train_time:76511ms step_avg:117.53ms
step:652/2245 train_time:76631ms step_avg:117.53ms
step:653/2245 train_time:76745ms step_avg:117.53ms
step:654/2245 train_time:76865ms step_avg:117.53ms
step:655/2245 train_time:76980ms step_avg:117.53ms
step:656/2245 train_time:77101ms step_avg:117.53ms
step:657/2245 train_time:77215ms step_avg:117.53ms
step:658/2245 train_time:77335ms step_avg:117.53ms
step:659/2245 train_time:77449ms step_avg:117.53ms
step:660/2245 train_time:77570ms step_avg:117.53ms
step:661/2245 train_time:77684ms step_avg:117.52ms
step:662/2245 train_time:77804ms step_avg:117.53ms
step:663/2245 train_time:77919ms step_avg:117.52ms
step:664/2245 train_time:78039ms step_avg:117.53ms
step:665/2245 train_time:78154ms step_avg:117.52ms
step:666/2245 train_time:78274ms step_avg:117.53ms
step:667/2245 train_time:78388ms step_avg:117.52ms
step:668/2245 train_time:78509ms step_avg:117.53ms
step:669/2245 train_time:78622ms step_avg:117.52ms
step:670/2245 train_time:78743ms step_avg:117.53ms
step:671/2245 train_time:78857ms step_avg:117.52ms
step:672/2245 train_time:78977ms step_avg:117.53ms
step:673/2245 train_time:79092ms step_avg:117.52ms
step:674/2245 train_time:79212ms step_avg:117.53ms
step:675/2245 train_time:79326ms step_avg:117.52ms
step:676/2245 train_time:79447ms step_avg:117.52ms
step:677/2245 train_time:79561ms step_avg:117.52ms
step:678/2245 train_time:79681ms step_avg:117.52ms
step:679/2245 train_time:79796ms step_avg:117.52ms
step:680/2245 train_time:79916ms step_avg:117.52ms
step:681/2245 train_time:80031ms step_avg:117.52ms
step:682/2245 train_time:80151ms step_avg:117.52ms
step:683/2245 train_time:80265ms step_avg:117.52ms
step:684/2245 train_time:80386ms step_avg:117.52ms
step:685/2245 train_time:80500ms step_avg:117.52ms
step:686/2245 train_time:80621ms step_avg:117.52ms
step:687/2245 train_time:80735ms step_avg:117.52ms
step:688/2245 train_time:80855ms step_avg:117.52ms
step:689/2245 train_time:80969ms step_avg:117.52ms
step:690/2245 train_time:81089ms step_avg:117.52ms
step:691/2245 train_time:81204ms step_avg:117.52ms
step:692/2245 train_time:81324ms step_avg:117.52ms
step:693/2245 train_time:81438ms step_avg:117.52ms
step:694/2245 train_time:81558ms step_avg:117.52ms
step:695/2245 train_time:81673ms step_avg:117.51ms
step:696/2245 train_time:81793ms step_avg:117.52ms
step:697/2245 train_time:81907ms step_avg:117.51ms
step:698/2245 train_time:82028ms step_avg:117.52ms
step:699/2245 train_time:82142ms step_avg:117.51ms
step:700/2245 train_time:82262ms step_avg:117.52ms
step:701/2245 train_time:82376ms step_avg:117.51ms
step:702/2245 train_time:82497ms step_avg:117.52ms
step:703/2245 train_time:82611ms step_avg:117.51ms
step:704/2245 train_time:82732ms step_avg:117.52ms
step:705/2245 train_time:82846ms step_avg:117.51ms
step:706/2245 train_time:82966ms step_avg:117.52ms
step:707/2245 train_time:83080ms step_avg:117.51ms
step:708/2245 train_time:83201ms step_avg:117.52ms
step:709/2245 train_time:83315ms step_avg:117.51ms
step:710/2245 train_time:83435ms step_avg:117.51ms
step:711/2245 train_time:83549ms step_avg:117.51ms
step:712/2245 train_time:83670ms step_avg:117.51ms
step:713/2245 train_time:83784ms step_avg:117.51ms
step:714/2245 train_time:83905ms step_avg:117.51ms
step:715/2245 train_time:84019ms step_avg:117.51ms
step:716/2245 train_time:84140ms step_avg:117.51ms
step:717/2245 train_time:84254ms step_avg:117.51ms
step:718/2245 train_time:84374ms step_avg:117.51ms
step:719/2245 train_time:84488ms step_avg:117.51ms
step:720/2245 train_time:84609ms step_avg:117.51ms
step:721/2245 train_time:84723ms step_avg:117.51ms
step:722/2245 train_time:84844ms step_avg:117.51ms
step:723/2245 train_time:84958ms step_avg:117.51ms
step:724/2245 train_time:85078ms step_avg:117.51ms
step:725/2245 train_time:85192ms step_avg:117.51ms
step:726/2245 train_time:85313ms step_avg:117.51ms
step:727/2245 train_time:85427ms step_avg:117.51ms
step:728/2245 train_time:85547ms step_avg:117.51ms
step:729/2245 train_time:85661ms step_avg:117.51ms
step:730/2245 train_time:85782ms step_avg:117.51ms
step:731/2245 train_time:85896ms step_avg:117.50ms
step:732/2245 train_time:86017ms step_avg:117.51ms
step:733/2245 train_time:86130ms step_avg:117.50ms
step:734/2245 train_time:86251ms step_avg:117.51ms
step:735/2245 train_time:86365ms step_avg:117.50ms
step:736/2245 train_time:86487ms step_avg:117.51ms
step:737/2245 train_time:86602ms step_avg:117.51ms
step:738/2245 train_time:86724ms step_avg:117.51ms
step:739/2245 train_time:86840ms step_avg:117.51ms
step:740/2245 train_time:86961ms step_avg:117.52ms
step:741/2245 train_time:87077ms step_avg:117.51ms
step:742/2245 train_time:87199ms step_avg:117.52ms
step:743/2245 train_time:87315ms step_avg:117.52ms
step:744/2245 train_time:87437ms step_avg:117.52ms
step:745/2245 train_time:87553ms step_avg:117.52ms
step:746/2245 train_time:87675ms step_avg:117.53ms
step:747/2245 train_time:87791ms step_avg:117.52ms
step:748/2245 train_time:87913ms step_avg:117.53ms
step:749/2245 train_time:88029ms step_avg:117.53ms
step:750/2245 train_time:88151ms step_avg:117.53ms
step:750/2245 val_loss:3.6765 train_time:88217ms step_avg:117.62ms
step:751/2245 train_time:88268ms step_avg:117.53ms
step:752/2245 train_time:88389ms step_avg:117.54ms
step:753/2245 train_time:88504ms step_avg:117.54ms
step:754/2245 train_time:88626ms step_avg:117.54ms
step:755/2245 train_time:88742ms step_avg:117.54ms
step:756/2245 train_time:88864ms step_avg:117.55ms
step:757/2245 train_time:88979ms step_avg:117.54ms
step:758/2245 train_time:89101ms step_avg:117.55ms
step:759/2245 train_time:89217ms step_avg:117.55ms
step:760/2245 train_time:89339ms step_avg:117.55ms
step:761/2245 train_time:89454ms step_avg:117.55ms
step:762/2245 train_time:89576ms step_avg:117.55ms
step:763/2245 train_time:89692ms step_avg:117.55ms
step:764/2245 train_time:89814ms step_avg:117.56ms
step:765/2245 train_time:89931ms step_avg:117.56ms
step:766/2245 train_time:90053ms step_avg:117.56ms
step:767/2245 train_time:90169ms step_avg:117.56ms
step:768/2245 train_time:90291ms step_avg:117.57ms
step:769/2245 train_time:90406ms step_avg:117.56ms
step:770/2245 train_time:90528ms step_avg:117.57ms
step:771/2245 train_time:90643ms step_avg:117.57ms
step:772/2245 train_time:90766ms step_avg:117.57ms
step:773/2245 train_time:90881ms step_avg:117.57ms
step:774/2245 train_time:91003ms step_avg:117.57ms
step:775/2245 train_time:91118ms step_avg:117.57ms
step:776/2245 train_time:91241ms step_avg:117.58ms
step:777/2245 train_time:91356ms step_avg:117.58ms
step:778/2245 train_time:91478ms step_avg:117.58ms
step:779/2245 train_time:91594ms step_avg:117.58ms
step:780/2245 train_time:91716ms step_avg:117.58ms
step:781/2245 train_time:91832ms step_avg:117.58ms
step:782/2245 train_time:91954ms step_avg:117.59ms
step:783/2245 train_time:92070ms step_avg:117.59ms
step:784/2245 train_time:92192ms step_avg:117.59ms
step:785/2245 train_time:92307ms step_avg:117.59ms
step:786/2245 train_time:92429ms step_avg:117.59ms
step:787/2245 train_time:92545ms step_avg:117.59ms
step:788/2245 train_time:92666ms step_avg:117.60ms
step:789/2245 train_time:92782ms step_avg:117.59ms
step:790/2245 train_time:92905ms step_avg:117.60ms
step:791/2245 train_time:93020ms step_avg:117.60ms
step:792/2245 train_time:93142ms step_avg:117.60ms
step:793/2245 train_time:93258ms step_avg:117.60ms
step:794/2245 train_time:93381ms step_avg:117.61ms
step:795/2245 train_time:93497ms step_avg:117.61ms
step:796/2245 train_time:93619ms step_avg:117.61ms
step:797/2245 train_time:93735ms step_avg:117.61ms
step:798/2245 train_time:93857ms step_avg:117.61ms
step:799/2245 train_time:93973ms step_avg:117.61ms
step:800/2245 train_time:94095ms step_avg:117.62ms
step:801/2245 train_time:94211ms step_avg:117.62ms
step:802/2245 train_time:94334ms step_avg:117.62ms
step:803/2245 train_time:94449ms step_avg:117.62ms
step:804/2245 train_time:94570ms step_avg:117.62ms
step:805/2245 train_time:94686ms step_avg:117.62ms
step:806/2245 train_time:94808ms step_avg:117.63ms
step:807/2245 train_time:94924ms step_avg:117.63ms
step:808/2245 train_time:95046ms step_avg:117.63ms
step:809/2245 train_time:95161ms step_avg:117.63ms
step:810/2245 train_time:95283ms step_avg:117.63ms
step:811/2245 train_time:95399ms step_avg:117.63ms
step:812/2245 train_time:95521ms step_avg:117.64ms
step:813/2245 train_time:95637ms step_avg:117.63ms
step:814/2245 train_time:95759ms step_avg:117.64ms
step:815/2245 train_time:95875ms step_avg:117.64ms
step:816/2245 train_time:95998ms step_avg:117.64ms
step:817/2245 train_time:96114ms step_avg:117.64ms
step:818/2245 train_time:96237ms step_avg:117.65ms
step:819/2245 train_time:96353ms step_avg:117.65ms
step:820/2245 train_time:96475ms step_avg:117.65ms
step:821/2245 train_time:96591ms step_avg:117.65ms
step:822/2245 train_time:96713ms step_avg:117.66ms
step:823/2245 train_time:96828ms step_avg:117.65ms
step:824/2245 train_time:96950ms step_avg:117.66ms
step:825/2245 train_time:97065ms step_avg:117.65ms
step:826/2245 train_time:97187ms step_avg:117.66ms
step:827/2245 train_time:97303ms step_avg:117.66ms
step:828/2245 train_time:97425ms step_avg:117.66ms
step:829/2245 train_time:97541ms step_avg:117.66ms
step:830/2245 train_time:97663ms step_avg:117.67ms
step:831/2245 train_time:97778ms step_avg:117.66ms
step:832/2245 train_time:97900ms step_avg:117.67ms
step:833/2245 train_time:98016ms step_avg:117.67ms
step:834/2245 train_time:98138ms step_avg:117.67ms
step:835/2245 train_time:98254ms step_avg:117.67ms
step:836/2245 train_time:98376ms step_avg:117.67ms
step:837/2245 train_time:98492ms step_avg:117.67ms
step:838/2245 train_time:98614ms step_avg:117.68ms
step:839/2245 train_time:98730ms step_avg:117.68ms
step:840/2245 train_time:98852ms step_avg:117.68ms
step:841/2245 train_time:98968ms step_avg:117.68ms
step:842/2245 train_time:99090ms step_avg:117.68ms
step:843/2245 train_time:99206ms step_avg:117.68ms
step:844/2245 train_time:99328ms step_avg:117.69ms
step:845/2245 train_time:99443ms step_avg:117.68ms
step:846/2245 train_time:99565ms step_avg:117.69ms
step:847/2245 train_time:99681ms step_avg:117.69ms
step:848/2245 train_time:99803ms step_avg:117.69ms
step:849/2245 train_time:99918ms step_avg:117.69ms
step:850/2245 train_time:100040ms step_avg:117.69ms
step:851/2245 train_time:100156ms step_avg:117.69ms
step:852/2245 train_time:100278ms step_avg:117.70ms
step:853/2245 train_time:100394ms step_avg:117.70ms
step:854/2245 train_time:100517ms step_avg:117.70ms
step:855/2245 train_time:100632ms step_avg:117.70ms
step:856/2245 train_time:100755ms step_avg:117.70ms
step:857/2245 train_time:100870ms step_avg:117.70ms
step:858/2245 train_time:100993ms step_avg:117.71ms
step:859/2245 train_time:101108ms step_avg:117.70ms
step:860/2245 train_time:101230ms step_avg:117.71ms
step:861/2245 train_time:101346ms step_avg:117.71ms
step:862/2245 train_time:101467ms step_avg:117.71ms
step:863/2245 train_time:101584ms step_avg:117.71ms
step:864/2245 train_time:101706ms step_avg:117.71ms
step:865/2245 train_time:101821ms step_avg:117.71ms
step:866/2245 train_time:101944ms step_avg:117.72ms
step:867/2245 train_time:102059ms step_avg:117.72ms
step:868/2245 train_time:102182ms step_avg:117.72ms
step:869/2245 train_time:102298ms step_avg:117.72ms
step:870/2245 train_time:102420ms step_avg:117.72ms
step:871/2245 train_time:102536ms step_avg:117.72ms
step:872/2245 train_time:102658ms step_avg:117.73ms
step:873/2245 train_time:102774ms step_avg:117.72ms
step:874/2245 train_time:102896ms step_avg:117.73ms
step:875/2245 train_time:103012ms step_avg:117.73ms
step:876/2245 train_time:103134ms step_avg:117.73ms
step:877/2245 train_time:103250ms step_avg:117.73ms
step:878/2245 train_time:103372ms step_avg:117.74ms
step:879/2245 train_time:103487ms step_avg:117.73ms
step:880/2245 train_time:103610ms step_avg:117.74ms
step:881/2245 train_time:103725ms step_avg:117.74ms
step:882/2245 train_time:103847ms step_avg:117.74ms
step:883/2245 train_time:103962ms step_avg:117.74ms
step:884/2245 train_time:104085ms step_avg:117.74ms
step:885/2245 train_time:104200ms step_avg:117.74ms
step:886/2245 train_time:104322ms step_avg:117.75ms
step:887/2245 train_time:104438ms step_avg:117.74ms
step:888/2245 train_time:104561ms step_avg:117.75ms
step:889/2245 train_time:104677ms step_avg:117.75ms
step:890/2245 train_time:104799ms step_avg:117.75ms
step:891/2245 train_time:104915ms step_avg:117.75ms
step:892/2245 train_time:105038ms step_avg:117.76ms
step:893/2245 train_time:105153ms step_avg:117.75ms
step:894/2245 train_time:105276ms step_avg:117.76ms
step:895/2245 train_time:105392ms step_avg:117.76ms
step:896/2245 train_time:105514ms step_avg:117.76ms
step:897/2245 train_time:105630ms step_avg:117.76ms
step:898/2245 train_time:105751ms step_avg:117.76ms
step:899/2245 train_time:105867ms step_avg:117.76ms
step:900/2245 train_time:105989ms step_avg:117.77ms
step:901/2245 train_time:106104ms step_avg:117.76ms
step:902/2245 train_time:106226ms step_avg:117.77ms
step:903/2245 train_time:106342ms step_avg:117.77ms
step:904/2245 train_time:106464ms step_avg:117.77ms
step:905/2245 train_time:106579ms step_avg:117.77ms
step:906/2245 train_time:106701ms step_avg:117.77ms
step:907/2245 train_time:106817ms step_avg:117.77ms
step:908/2245 train_time:106939ms step_avg:117.77ms
step:909/2245 train_time:107056ms step_avg:117.77ms
step:910/2245 train_time:107177ms step_avg:117.78ms
step:911/2245 train_time:107293ms step_avg:117.78ms
step:912/2245 train_time:107416ms step_avg:117.78ms
step:913/2245 train_time:107532ms step_avg:117.78ms
step:914/2245 train_time:107654ms step_avg:117.78ms
step:915/2245 train_time:107770ms step_avg:117.78ms
step:916/2245 train_time:107892ms step_avg:117.79ms
step:917/2245 train_time:108007ms step_avg:117.78ms
step:918/2245 train_time:108129ms step_avg:117.79ms
step:919/2245 train_time:108244ms step_avg:117.78ms
step:920/2245 train_time:108366ms step_avg:117.79ms
step:921/2245 train_time:108482ms step_avg:117.79ms
step:922/2245 train_time:108604ms step_avg:117.79ms
step:923/2245 train_time:108719ms step_avg:117.79ms
step:924/2245 train_time:108841ms step_avg:117.79ms
step:925/2245 train_time:108957ms step_avg:117.79ms
step:926/2245 train_time:109079ms step_avg:117.80ms
step:927/2245 train_time:109195ms step_avg:117.79ms
step:928/2245 train_time:109317ms step_avg:117.80ms
step:929/2245 train_time:109433ms step_avg:117.80ms
step:930/2245 train_time:109555ms step_avg:117.80ms
step:931/2245 train_time:109671ms step_avg:117.80ms
step:932/2245 train_time:109793ms step_avg:117.80ms
step:933/2245 train_time:109909ms step_avg:117.80ms
step:934/2245 train_time:110031ms step_avg:117.81ms
step:935/2245 train_time:110147ms step_avg:117.80ms
step:936/2245 train_time:110268ms step_avg:117.81ms
step:937/2245 train_time:110383ms step_avg:117.81ms
step:938/2245 train_time:110506ms step_avg:117.81ms
step:939/2245 train_time:110621ms step_avg:117.81ms
step:940/2245 train_time:110743ms step_avg:117.81ms
step:941/2245 train_time:110859ms step_avg:117.81ms
step:942/2245 train_time:110981ms step_avg:117.81ms
step:943/2245 train_time:111097ms step_avg:117.81ms
step:944/2245 train_time:111219ms step_avg:117.82ms
step:945/2245 train_time:111335ms step_avg:117.81ms
step:946/2245 train_time:111456ms step_avg:117.82ms
step:947/2245 train_time:111573ms step_avg:117.82ms
step:948/2245 train_time:111694ms step_avg:117.82ms
step:949/2245 train_time:111810ms step_avg:117.82ms
step:950/2245 train_time:111932ms step_avg:117.82ms
step:951/2245 train_time:112048ms step_avg:117.82ms
step:952/2245 train_time:112169ms step_avg:117.83ms
step:953/2245 train_time:112285ms step_avg:117.82ms
step:954/2245 train_time:112407ms step_avg:117.83ms
step:955/2245 train_time:112523ms step_avg:117.82ms
step:956/2245 train_time:112645ms step_avg:117.83ms
step:957/2245 train_time:112761ms step_avg:117.83ms
step:958/2245 train_time:112883ms step_avg:117.83ms
step:959/2245 train_time:112999ms step_avg:117.83ms
step:960/2245 train_time:113121ms step_avg:117.83ms
step:961/2245 train_time:113237ms step_avg:117.83ms
step:962/2245 train_time:113359ms step_avg:117.84ms
step:963/2245 train_time:113474ms step_avg:117.83ms
step:964/2245 train_time:113597ms step_avg:117.84ms
step:965/2245 train_time:113713ms step_avg:117.84ms
step:966/2245 train_time:113834ms step_avg:117.84ms
step:967/2245 train_time:113950ms step_avg:117.84ms
step:968/2245 train_time:114072ms step_avg:117.84ms
step:969/2245 train_time:114187ms step_avg:117.84ms
step:970/2245 train_time:114309ms step_avg:117.84ms
step:971/2245 train_time:114425ms step_avg:117.84ms
step:972/2245 train_time:114547ms step_avg:117.85ms
step:973/2245 train_time:114662ms step_avg:117.84ms
step:974/2245 train_time:114784ms step_avg:117.85ms
step:975/2245 train_time:114900ms step_avg:117.85ms
step:976/2245 train_time:115021ms step_avg:117.85ms
step:977/2245 train_time:115137ms step_avg:117.85ms
step:978/2245 train_time:115259ms step_avg:117.85ms
step:979/2245 train_time:115375ms step_avg:117.85ms
step:980/2245 train_time:115497ms step_avg:117.85ms
step:981/2245 train_time:115613ms step_avg:117.85ms
step:982/2245 train_time:115736ms step_avg:117.86ms
step:983/2245 train_time:115852ms step_avg:117.86ms
step:984/2245 train_time:115974ms step_avg:117.86ms
step:985/2245 train_time:116090ms step_avg:117.86ms
step:986/2245 train_time:116211ms step_avg:117.86ms
step:987/2245 train_time:116327ms step_avg:117.86ms
step:988/2245 train_time:116449ms step_avg:117.86ms
step:989/2245 train_time:116565ms step_avg:117.86ms
step:990/2245 train_time:116687ms step_avg:117.87ms
step:991/2245 train_time:116802ms step_avg:117.86ms
step:992/2245 train_time:116924ms step_avg:117.87ms
step:993/2245 train_time:117040ms step_avg:117.86ms
step:994/2245 train_time:117162ms step_avg:117.87ms
step:995/2245 train_time:117277ms step_avg:117.87ms
step:996/2245 train_time:117400ms step_avg:117.87ms
step:997/2245 train_time:117515ms step_avg:117.87ms
step:998/2245 train_time:117638ms step_avg:117.87ms
step:999/2245 train_time:117753ms step_avg:117.87ms
step:1000/2245 train_time:117876ms step_avg:117.88ms
step:1000/2245 val_loss:3.5971 train_time:117943ms step_avg:117.94ms
step:1001/2245 train_time:117993ms step_avg:117.88ms
step:1002/2245 train_time:118114ms step_avg:117.88ms
step:1003/2245 train_time:118230ms step_avg:117.88ms
step:1004/2245 train_time:118352ms step_avg:117.88ms
step:1005/2245 train_time:118468ms step_avg:117.88ms
step:1006/2245 train_time:118590ms step_avg:117.88ms
step:1007/2245 train_time:118705ms step_avg:117.88ms
step:1008/2245 train_time:118827ms step_avg:117.88ms
step:1009/2245 train_time:118943ms step_avg:117.88ms
step:1010/2245 train_time:119065ms step_avg:117.89ms
step:1011/2245 train_time:119181ms step_avg:117.88ms
step:1012/2245 train_time:119303ms step_avg:117.89ms
step:1013/2245 train_time:119419ms step_avg:117.89ms
step:1014/2245 train_time:119541ms step_avg:117.89ms
step:1015/2245 train_time:119656ms step_avg:117.89ms
step:1016/2245 train_time:119779ms step_avg:117.89ms
step:1017/2245 train_time:119894ms step_avg:117.89ms
step:1018/2245 train_time:120016ms step_avg:117.89ms
step:1019/2245 train_time:120132ms step_avg:117.89ms
step:1020/2245 train_time:120254ms step_avg:117.90ms
step:1021/2245 train_time:120370ms step_avg:117.89ms
step:1022/2245 train_time:120491ms step_avg:117.90ms
step:1023/2245 train_time:120607ms step_avg:117.90ms
step:1024/2245 train_time:120729ms step_avg:117.90ms
step:1025/2245 train_time:120846ms step_avg:117.90ms
step:1026/2245 train_time:120968ms step_avg:117.90ms
step:1027/2245 train_time:121084ms step_avg:117.90ms
step:1028/2245 train_time:121205ms step_avg:117.90ms
step:1029/2245 train_time:121321ms step_avg:117.90ms
step:1030/2245 train_time:121443ms step_avg:117.91ms
step:1031/2245 train_time:121559ms step_avg:117.90ms
step:1032/2245 train_time:121681ms step_avg:117.91ms
step:1033/2245 train_time:121796ms step_avg:117.91ms
step:1034/2245 train_time:121918ms step_avg:117.91ms
step:1035/2245 train_time:122033ms step_avg:117.91ms
step:1036/2245 train_time:122156ms step_avg:117.91ms
step:1037/2245 train_time:122271ms step_avg:117.91ms
step:1038/2245 train_time:122393ms step_avg:117.91ms
step:1039/2245 train_time:122510ms step_avg:117.91ms
step:1040/2245 train_time:122632ms step_avg:117.92ms
step:1041/2245 train_time:122748ms step_avg:117.91ms
step:1042/2245 train_time:122871ms step_avg:117.92ms
step:1043/2245 train_time:122987ms step_avg:117.92ms
step:1044/2245 train_time:123109ms step_avg:117.92ms
step:1045/2245 train_time:123225ms step_avg:117.92ms
step:1046/2245 train_time:123347ms step_avg:117.92ms
step:1047/2245 train_time:123463ms step_avg:117.92ms
step:1048/2245 train_time:123585ms step_avg:117.92ms
step:1049/2245 train_time:123700ms step_avg:117.92ms
step:1050/2245 train_time:123823ms step_avg:117.93ms
step:1051/2245 train_time:123938ms step_avg:117.92ms
step:1052/2245 train_time:124060ms step_avg:117.93ms
step:1053/2245 train_time:124176ms step_avg:117.93ms
step:1054/2245 train_time:124298ms step_avg:117.93ms
step:1055/2245 train_time:124414ms step_avg:117.93ms
step:1056/2245 train_time:124535ms step_avg:117.93ms
step:1057/2245 train_time:124651ms step_avg:117.93ms
step:1058/2245 train_time:124773ms step_avg:117.93ms
step:1059/2245 train_time:124890ms step_avg:117.93ms
step:1060/2245 train_time:125012ms step_avg:117.94ms
step:1061/2245 train_time:125128ms step_avg:117.93ms
step:1062/2245 train_time:125251ms step_avg:117.94ms
step:1063/2245 train_time:125367ms step_avg:117.94ms
step:1064/2245 train_time:125489ms step_avg:117.94ms
step:1065/2245 train_time:125605ms step_avg:117.94ms
step:1066/2245 train_time:125727ms step_avg:117.94ms
step:1067/2245 train_time:125842ms step_avg:117.94ms
step:1068/2245 train_time:125964ms step_avg:117.94ms
step:1069/2245 train_time:126079ms step_avg:117.94ms
step:1070/2245 train_time:126201ms step_avg:117.95ms
step:1071/2245 train_time:126317ms step_avg:117.94ms
step:1072/2245 train_time:126438ms step_avg:117.95ms
step:1073/2245 train_time:126554ms step_avg:117.94ms
step:1074/2245 train_time:126676ms step_avg:117.95ms
step:1075/2245 train_time:126792ms step_avg:117.95ms
step:1076/2245 train_time:126914ms step_avg:117.95ms
step:1077/2245 train_time:127030ms step_avg:117.95ms
step:1078/2245 train_time:127151ms step_avg:117.95ms
step:1079/2245 train_time:127267ms step_avg:117.95ms
step:1080/2245 train_time:127389ms step_avg:117.95ms
step:1081/2245 train_time:127505ms step_avg:117.95ms
step:1082/2245 train_time:127627ms step_avg:117.95ms
step:1083/2245 train_time:127743ms step_avg:117.95ms
step:1084/2245 train_time:127865ms step_avg:117.96ms
step:1085/2245 train_time:127981ms step_avg:117.95ms
step:1086/2245 train_time:128103ms step_avg:117.96ms
step:1087/2245 train_time:128218ms step_avg:117.96ms
step:1088/2245 train_time:128340ms step_avg:117.96ms
step:1089/2245 train_time:128455ms step_avg:117.96ms
step:1090/2245 train_time:128577ms step_avg:117.96ms
step:1091/2245 train_time:128693ms step_avg:117.96ms
step:1092/2245 train_time:128815ms step_avg:117.96ms
step:1093/2245 train_time:128931ms step_avg:117.96ms
step:1094/2245 train_time:129053ms step_avg:117.96ms
step:1095/2245 train_time:129169ms step_avg:117.96ms
step:1096/2245 train_time:129292ms step_avg:117.97ms
step:1097/2245 train_time:129408ms step_avg:117.97ms
step:1098/2245 train_time:129531ms step_avg:117.97ms
step:1099/2245 train_time:129646ms step_avg:117.97ms
step:1100/2245 train_time:129769ms step_avg:117.97ms
step:1101/2245 train_time:129885ms step_avg:117.97ms
step:1102/2245 train_time:130007ms step_avg:117.97ms
step:1103/2245 train_time:130123ms step_avg:117.97ms
step:1104/2245 train_time:130244ms step_avg:117.97ms
step:1105/2245 train_time:130360ms step_avg:117.97ms
step:1106/2245 train_time:130482ms step_avg:117.98ms
step:1107/2245 train_time:130597ms step_avg:117.97ms
step:1108/2245 train_time:130719ms step_avg:117.98ms
step:1109/2245 train_time:130835ms step_avg:117.98ms
step:1110/2245 train_time:130958ms step_avg:117.98ms
step:1111/2245 train_time:131073ms step_avg:117.98ms
step:1112/2245 train_time:131194ms step_avg:117.98ms
step:1113/2245 train_time:131310ms step_avg:117.98ms
step:1114/2245 train_time:131432ms step_avg:117.98ms
step:1115/2245 train_time:131548ms step_avg:117.98ms
step:1116/2245 train_time:131670ms step_avg:117.98ms
step:1117/2245 train_time:131786ms step_avg:117.98ms
step:1118/2245 train_time:131909ms step_avg:117.99ms
step:1119/2245 train_time:132025ms step_avg:117.98ms
step:1120/2245 train_time:132147ms step_avg:117.99ms
step:1121/2245 train_time:132263ms step_avg:117.99ms
step:1122/2245 train_time:132385ms step_avg:117.99ms
step:1123/2245 train_time:132501ms step_avg:117.99ms
step:1124/2245 train_time:132623ms step_avg:117.99ms
step:1125/2245 train_time:132738ms step_avg:117.99ms
step:1126/2245 train_time:132860ms step_avg:117.99ms
step:1127/2245 train_time:132975ms step_avg:117.99ms
step:1128/2245 train_time:133097ms step_avg:117.99ms
step:1129/2245 train_time:133213ms step_avg:117.99ms
step:1130/2245 train_time:133335ms step_avg:118.00ms
step:1131/2245 train_time:133451ms step_avg:117.99ms
step:1132/2245 train_time:133574ms step_avg:118.00ms
step:1133/2245 train_time:133690ms step_avg:118.00ms
step:1134/2245 train_time:133812ms step_avg:118.00ms
step:1135/2245 train_time:133927ms step_avg:118.00ms
step:1136/2245 train_time:134050ms step_avg:118.00ms
step:1137/2245 train_time:134166ms step_avg:118.00ms
step:1138/2245 train_time:134288ms step_avg:118.00ms
step:1139/2245 train_time:134403ms step_avg:118.00ms
step:1140/2245 train_time:134526ms step_avg:118.01ms
step:1141/2245 train_time:134641ms step_avg:118.00ms
step:1142/2245 train_time:134763ms step_avg:118.01ms
step:1143/2245 train_time:134878ms step_avg:118.00ms
step:1144/2245 train_time:135001ms step_avg:118.01ms
step:1145/2245 train_time:135116ms step_avg:118.01ms
step:1146/2245 train_time:135238ms step_avg:118.01ms
step:1147/2245 train_time:135354ms step_avg:118.01ms
step:1148/2245 train_time:135476ms step_avg:118.01ms
step:1149/2245 train_time:135592ms step_avg:118.01ms
step:1150/2245 train_time:135714ms step_avg:118.01ms
step:1151/2245 train_time:135830ms step_avg:118.01ms
step:1152/2245 train_time:135952ms step_avg:118.01ms
step:1153/2245 train_time:136068ms step_avg:118.01ms
step:1154/2245 train_time:136191ms step_avg:118.02ms
step:1155/2245 train_time:136307ms step_avg:118.01ms
step:1156/2245 train_time:136429ms step_avg:118.02ms
step:1157/2245 train_time:136545ms step_avg:118.02ms
step:1158/2245 train_time:136667ms step_avg:118.02ms
step:1159/2245 train_time:136783ms step_avg:118.02ms
step:1160/2245 train_time:136905ms step_avg:118.02ms
step:1161/2245 train_time:137021ms step_avg:118.02ms
step:1162/2245 train_time:137142ms step_avg:118.02ms
step:1163/2245 train_time:137258ms step_avg:118.02ms
step:1164/2245 train_time:137380ms step_avg:118.02ms
step:1165/2245 train_time:137496ms step_avg:118.02ms
step:1166/2245 train_time:137618ms step_avg:118.03ms
step:1167/2245 train_time:137733ms step_avg:118.02ms
step:1168/2245 train_time:137855ms step_avg:118.03ms
step:1169/2245 train_time:137971ms step_avg:118.03ms
step:1170/2245 train_time:138093ms step_avg:118.03ms
step:1171/2245 train_time:138209ms step_avg:118.03ms
step:1172/2245 train_time:138331ms step_avg:118.03ms
step:1173/2245 train_time:138446ms step_avg:118.03ms
step:1174/2245 train_time:138569ms step_avg:118.03ms
step:1175/2245 train_time:138686ms step_avg:118.03ms
step:1176/2245 train_time:138807ms step_avg:118.03ms
step:1177/2245 train_time:138923ms step_avg:118.03ms
step:1178/2245 train_time:139045ms step_avg:118.03ms
step:1179/2245 train_time:139160ms step_avg:118.03ms
step:1180/2245 train_time:139282ms step_avg:118.04ms
step:1181/2245 train_time:139398ms step_avg:118.03ms
step:1182/2245 train_time:139519ms step_avg:118.04ms
step:1183/2245 train_time:139635ms step_avg:118.03ms
step:1184/2245 train_time:139757ms step_avg:118.04ms
step:1185/2245 train_time:139873ms step_avg:118.04ms
step:1186/2245 train_time:139994ms step_avg:118.04ms
step:1187/2245 train_time:140110ms step_avg:118.04ms
step:1188/2245 train_time:140232ms step_avg:118.04ms
step:1189/2245 train_time:140348ms step_avg:118.04ms
step:1190/2245 train_time:140470ms step_avg:118.04ms
step:1191/2245 train_time:140587ms step_avg:118.04ms
step:1192/2245 train_time:140709ms step_avg:118.04ms
step:1193/2245 train_time:140825ms step_avg:118.04ms
step:1194/2245 train_time:140947ms step_avg:118.05ms
step:1195/2245 train_time:141063ms step_avg:118.04ms
step:1196/2245 train_time:141185ms step_avg:118.05ms
step:1197/2245 train_time:141300ms step_avg:118.05ms
step:1198/2245 train_time:141422ms step_avg:118.05ms
step:1199/2245 train_time:141538ms step_avg:118.05ms
step:1200/2245 train_time:141659ms step_avg:118.05ms
step:1201/2245 train_time:141775ms step_avg:118.05ms
step:1202/2245 train_time:141897ms step_avg:118.05ms
step:1203/2245 train_time:142013ms step_avg:118.05ms
step:1204/2245 train_time:142134ms step_avg:118.05ms
step:1205/2245 train_time:142250ms step_avg:118.05ms
step:1206/2245 train_time:142372ms step_avg:118.05ms
step:1207/2245 train_time:142487ms step_avg:118.05ms
step:1208/2245 train_time:142610ms step_avg:118.05ms
step:1209/2245 train_time:142726ms step_avg:118.05ms
step:1210/2245 train_time:142848ms step_avg:118.06ms
step:1211/2245 train_time:142964ms step_avg:118.05ms
step:1212/2245 train_time:143086ms step_avg:118.06ms
step:1213/2245 train_time:143202ms step_avg:118.06ms
step:1214/2245 train_time:143324ms step_avg:118.06ms
step:1215/2245 train_time:143440ms step_avg:118.06ms
step:1216/2245 train_time:143561ms step_avg:118.06ms
step:1217/2245 train_time:143678ms step_avg:118.06ms
step:1218/2245 train_time:143800ms step_avg:118.06ms
step:1219/2245 train_time:143916ms step_avg:118.06ms
step:1220/2245 train_time:144038ms step_avg:118.06ms
step:1221/2245 train_time:144153ms step_avg:118.06ms
step:1222/2245 train_time:144276ms step_avg:118.07ms
step:1223/2245 train_time:144391ms step_avg:118.06ms
step:1224/2245 train_time:144513ms step_avg:118.07ms
step:1225/2245 train_time:144628ms step_avg:118.06ms
step:1226/2245 train_time:144750ms step_avg:118.07ms
step:1227/2245 train_time:144867ms step_avg:118.07ms
step:1228/2245 train_time:144989ms step_avg:118.07ms
step:1229/2245 train_time:145105ms step_avg:118.07ms
step:1230/2245 train_time:145227ms step_avg:118.07ms
step:1231/2245 train_time:145343ms step_avg:118.07ms
step:1232/2245 train_time:145465ms step_avg:118.07ms
step:1233/2245 train_time:145581ms step_avg:118.07ms
step:1234/2245 train_time:145703ms step_avg:118.07ms
step:1235/2245 train_time:145818ms step_avg:118.07ms
step:1236/2245 train_time:145940ms step_avg:118.07ms
step:1237/2245 train_time:146056ms step_avg:118.07ms
step:1238/2245 train_time:146177ms step_avg:118.08ms
step:1239/2245 train_time:146293ms step_avg:118.07ms
step:1240/2245 train_time:146416ms step_avg:118.08ms
step:1241/2245 train_time:146531ms step_avg:118.07ms
step:1242/2245 train_time:146653ms step_avg:118.08ms
step:1243/2245 train_time:146769ms step_avg:118.08ms
step:1244/2245 train_time:146891ms step_avg:118.08ms
step:1245/2245 train_time:147007ms step_avg:118.08ms
step:1246/2245 train_time:147129ms step_avg:118.08ms
step:1247/2245 train_time:147245ms step_avg:118.08ms
step:1248/2245 train_time:147367ms step_avg:118.08ms
step:1249/2245 train_time:147483ms step_avg:118.08ms
step:1250/2245 train_time:147605ms step_avg:118.08ms
step:1250/2245 val_loss:3.5279 train_time:147672ms step_avg:118.14ms
step:1251/2245 train_time:147723ms step_avg:118.08ms
step:1252/2245 train_time:147844ms step_avg:118.09ms
step:1253/2245 train_time:147960ms step_avg:118.08ms
step:1254/2245 train_time:148082ms step_avg:118.09ms
step:1255/2245 train_time:148197ms step_avg:118.09ms
step:1256/2245 train_time:148319ms step_avg:118.09ms
step:1257/2245 train_time:148434ms step_avg:118.09ms
step:1258/2245 train_time:148556ms step_avg:118.09ms
step:1259/2245 train_time:148671ms step_avg:118.09ms
step:1260/2245 train_time:148794ms step_avg:118.09ms
step:1261/2245 train_time:148909ms step_avg:118.09ms
step:1262/2245 train_time:149031ms step_avg:118.09ms
step:1263/2245 train_time:149147ms step_avg:118.09ms
step:1264/2245 train_time:149269ms step_avg:118.09ms
step:1265/2245 train_time:149385ms step_avg:118.09ms
step:1266/2245 train_time:149508ms step_avg:118.09ms
step:1267/2245 train_time:149623ms step_avg:118.09ms
step:1268/2245 train_time:149746ms step_avg:118.10ms
step:1269/2245 train_time:149862ms step_avg:118.09ms
step:1270/2245 train_time:149984ms step_avg:118.10ms
step:1271/2245 train_time:150100ms step_avg:118.10ms
step:1272/2245 train_time:150222ms step_avg:118.10ms
step:1273/2245 train_time:150338ms step_avg:118.10ms
step:1274/2245 train_time:150460ms step_avg:118.10ms
step:1275/2245 train_time:150575ms step_avg:118.10ms
step:1276/2245 train_time:150698ms step_avg:118.10ms
step:1277/2245 train_time:150813ms step_avg:118.10ms
step:1278/2245 train_time:150935ms step_avg:118.10ms
step:1279/2245 train_time:151051ms step_avg:118.10ms
step:1280/2245 train_time:151173ms step_avg:118.10ms
step:1281/2245 train_time:151288ms step_avg:118.10ms
step:1282/2245 train_time:151410ms step_avg:118.10ms
step:1283/2245 train_time:151526ms step_avg:118.10ms
step:1284/2245 train_time:151648ms step_avg:118.11ms
step:1285/2245 train_time:151764ms step_avg:118.10ms
step:1286/2245 train_time:151886ms step_avg:118.11ms
step:1287/2245 train_time:152003ms step_avg:118.11ms
step:1288/2245 train_time:152126ms step_avg:118.11ms
step:1289/2245 train_time:152242ms step_avg:118.11ms
step:1290/2245 train_time:152364ms step_avg:118.11ms
step:1291/2245 train_time:152480ms step_avg:118.11ms
step:1292/2245 train_time:152601ms step_avg:118.11ms
step:1293/2245 train_time:152717ms step_avg:118.11ms
step:1294/2245 train_time:152839ms step_avg:118.11ms
step:1295/2245 train_time:152954ms step_avg:118.11ms
step:1296/2245 train_time:153076ms step_avg:118.11ms
step:1297/2245 train_time:153192ms step_avg:118.11ms
step:1298/2245 train_time:153314ms step_avg:118.12ms
step:1299/2245 train_time:153429ms step_avg:118.11ms
step:1300/2245 train_time:153551ms step_avg:118.12ms
step:1301/2245 train_time:153667ms step_avg:118.11ms
step:1302/2245 train_time:153790ms step_avg:118.12ms
step:1303/2245 train_time:153905ms step_avg:118.12ms
step:1304/2245 train_time:154028ms step_avg:118.12ms
step:1305/2245 train_time:154143ms step_avg:118.12ms
step:1306/2245 train_time:154266ms step_avg:118.12ms
step:1307/2245 train_time:154382ms step_avg:118.12ms
step:1308/2245 train_time:154504ms step_avg:118.12ms
step:1309/2245 train_time:154620ms step_avg:118.12ms
step:1310/2245 train_time:154742ms step_avg:118.12ms
step:1311/2245 train_time:154857ms step_avg:118.12ms
step:1312/2245 train_time:154979ms step_avg:118.12ms
step:1313/2245 train_time:155095ms step_avg:118.12ms
step:1314/2245 train_time:155217ms step_avg:118.13ms
step:1315/2245 train_time:155332ms step_avg:118.12ms
step:1316/2245 train_time:155454ms step_avg:118.13ms
step:1317/2245 train_time:155570ms step_avg:118.12ms
step:1318/2245 train_time:155692ms step_avg:118.13ms
step:1319/2245 train_time:155808ms step_avg:118.13ms
step:1320/2245 train_time:155929ms step_avg:118.13ms
step:1321/2245 train_time:156045ms step_avg:118.13ms
step:1322/2245 train_time:156167ms step_avg:118.13ms
step:1323/2245 train_time:156283ms step_avg:118.13ms
step:1324/2245 train_time:156406ms step_avg:118.13ms
step:1325/2245 train_time:156521ms step_avg:118.13ms
step:1326/2245 train_time:156644ms step_avg:118.13ms
step:1327/2245 train_time:156760ms step_avg:118.13ms
step:1328/2245 train_time:156881ms step_avg:118.13ms
step:1329/2245 train_time:156997ms step_avg:118.13ms
step:1330/2245 train_time:157119ms step_avg:118.13ms
step:1331/2245 train_time:157235ms step_avg:118.13ms
step:1332/2245 train_time:157357ms step_avg:118.14ms
step:1333/2245 train_time:157473ms step_avg:118.13ms
step:1334/2245 train_time:157594ms step_avg:118.14ms
step:1335/2245 train_time:157710ms step_avg:118.13ms
step:1336/2245 train_time:157832ms step_avg:118.14ms
step:1337/2245 train_time:157948ms step_avg:118.14ms
step:1338/2245 train_time:158070ms step_avg:118.14ms
step:1339/2245 train_time:158187ms step_avg:118.14ms
step:1340/2245 train_time:158310ms step_avg:118.14ms
step:1341/2245 train_time:158425ms step_avg:118.14ms
step:1342/2245 train_time:158548ms step_avg:118.14ms
step:1343/2245 train_time:158663ms step_avg:118.14ms
step:1344/2245 train_time:158786ms step_avg:118.14ms
step:1345/2245 train_time:158901ms step_avg:118.14ms
step:1346/2245 train_time:159023ms step_avg:118.14ms
step:1347/2245 train_time:159139ms step_avg:118.14ms
step:1348/2245 train_time:159260ms step_avg:118.15ms
step:1349/2245 train_time:159376ms step_avg:118.14ms
step:1350/2245 train_time:159499ms step_avg:118.15ms
step:1351/2245 train_time:159615ms step_avg:118.15ms
step:1352/2245 train_time:159736ms step_avg:118.15ms
step:1353/2245 train_time:159852ms step_avg:118.15ms
step:1354/2245 train_time:159973ms step_avg:118.15ms
step:1355/2245 train_time:160089ms step_avg:118.15ms
step:1356/2245 train_time:160211ms step_avg:118.15ms
step:1357/2245 train_time:160326ms step_avg:118.15ms
step:1358/2245 train_time:160449ms step_avg:118.15ms
step:1359/2245 train_time:160565ms step_avg:118.15ms
step:1360/2245 train_time:160687ms step_avg:118.15ms
step:1361/2245 train_time:160802ms step_avg:118.15ms
step:1362/2245 train_time:160925ms step_avg:118.15ms
step:1363/2245 train_time:161040ms step_avg:118.15ms
step:1364/2245 train_time:161163ms step_avg:118.15ms
step:1365/2245 train_time:161279ms step_avg:118.15ms
step:1366/2245 train_time:161401ms step_avg:118.16ms
step:1367/2245 train_time:161516ms step_avg:118.15ms
step:1368/2245 train_time:161638ms step_avg:118.16ms
step:1369/2245 train_time:161754ms step_avg:118.15ms
step:1370/2245 train_time:161875ms step_avg:118.16ms
step:1371/2245 train_time:161991ms step_avg:118.16ms
step:1372/2245 train_time:162113ms step_avg:118.16ms
step:1373/2245 train_time:162229ms step_avg:118.16ms
step:1374/2245 train_time:162350ms step_avg:118.16ms
step:1375/2245 train_time:162466ms step_avg:118.16ms
step:1376/2245 train_time:162588ms step_avg:118.16ms
step:1377/2245 train_time:162704ms step_avg:118.16ms
step:1378/2245 train_time:162826ms step_avg:118.16ms
step:1379/2245 train_time:162942ms step_avg:118.16ms
step:1380/2245 train_time:163065ms step_avg:118.16ms
step:1381/2245 train_time:163180ms step_avg:118.16ms
step:1382/2245 train_time:163302ms step_avg:118.16ms
step:1383/2245 train_time:163417ms step_avg:118.16ms
step:1384/2245 train_time:163539ms step_avg:118.16ms
step:1385/2245 train_time:163655ms step_avg:118.16ms
step:1386/2245 train_time:163777ms step_avg:118.17ms
step:1387/2245 train_time:163893ms step_avg:118.16ms
step:1388/2245 train_time:164015ms step_avg:118.17ms
step:1389/2245 train_time:164130ms step_avg:118.16ms
step:1390/2245 train_time:164252ms step_avg:118.17ms
step:1391/2245 train_time:164368ms step_avg:118.17ms
step:1392/2245 train_time:164491ms step_avg:118.17ms
step:1393/2245 train_time:164606ms step_avg:118.17ms
step:1394/2245 train_time:164728ms step_avg:118.17ms
step:1395/2245 train_time:164843ms step_avg:118.17ms
step:1396/2245 train_time:164966ms step_avg:118.17ms
step:1397/2245 train_time:165083ms step_avg:118.17ms
step:1398/2245 train_time:165205ms step_avg:118.17ms
step:1399/2245 train_time:165321ms step_avg:118.17ms
step:1400/2245 train_time:165443ms step_avg:118.17ms
step:1401/2245 train_time:165559ms step_avg:118.17ms
step:1402/2245 train_time:165681ms step_avg:118.17ms
step:1403/2245 train_time:165796ms step_avg:118.17ms
step:1404/2245 train_time:165919ms step_avg:118.18ms
step:1405/2245 train_time:166034ms step_avg:118.17ms
step:1406/2245 train_time:166156ms step_avg:118.18ms
step:1407/2245 train_time:166272ms step_avg:118.17ms
step:1408/2245 train_time:166394ms step_avg:118.18ms
step:1409/2245 train_time:166509ms step_avg:118.18ms
step:1410/2245 train_time:166631ms step_avg:118.18ms
step:1411/2245 train_time:166747ms step_avg:118.18ms
step:1412/2245 train_time:166869ms step_avg:118.18ms
step:1413/2245 train_time:166986ms step_avg:118.18ms
step:1414/2245 train_time:167108ms step_avg:118.18ms
step:1415/2245 train_time:167223ms step_avg:118.18ms
step:1416/2245 train_time:167347ms step_avg:118.18ms
step:1417/2245 train_time:167462ms step_avg:118.18ms
step:1418/2245 train_time:167584ms step_avg:118.18ms
step:1419/2245 train_time:167700ms step_avg:118.18ms
step:1420/2245 train_time:167822ms step_avg:118.18ms
step:1421/2245 train_time:167937ms step_avg:118.18ms
step:1422/2245 train_time:168059ms step_avg:118.19ms
step:1423/2245 train_time:168175ms step_avg:118.18ms
step:1424/2245 train_time:168296ms step_avg:118.19ms
step:1425/2245 train_time:168412ms step_avg:118.18ms
step:1426/2245 train_time:168534ms step_avg:118.19ms
step:1427/2245 train_time:168649ms step_avg:118.18ms
step:1428/2245 train_time:168772ms step_avg:118.19ms
step:1429/2245 train_time:168887ms step_avg:118.19ms
step:1430/2245 train_time:169009ms step_avg:118.19ms
step:1431/2245 train_time:169125ms step_avg:118.19ms
step:1432/2245 train_time:169247ms step_avg:118.19ms
step:1433/2245 train_time:169364ms step_avg:118.19ms
step:1434/2245 train_time:169486ms step_avg:118.19ms
step:1435/2245 train_time:169601ms step_avg:118.19ms
step:1436/2245 train_time:169724ms step_avg:118.19ms
step:1437/2245 train_time:169840ms step_avg:118.19ms
step:1438/2245 train_time:169962ms step_avg:118.19ms
step:1439/2245 train_time:170078ms step_avg:118.19ms
step:1440/2245 train_time:170200ms step_avg:118.19ms
step:1441/2245 train_time:170315ms step_avg:118.19ms
step:1442/2245 train_time:170437ms step_avg:118.20ms
step:1443/2245 train_time:170553ms step_avg:118.19ms
step:1444/2245 train_time:170675ms step_avg:118.20ms
step:1445/2245 train_time:170790ms step_avg:118.19ms
step:1446/2245 train_time:170912ms step_avg:118.20ms
step:1447/2245 train_time:171028ms step_avg:118.19ms
step:1448/2245 train_time:171150ms step_avg:118.20ms
step:1449/2245 train_time:171265ms step_avg:118.20ms
step:1450/2245 train_time:171388ms step_avg:118.20ms
step:1451/2245 train_time:171504ms step_avg:118.20ms
step:1452/2245 train_time:171626ms step_avg:118.20ms
step:1453/2245 train_time:171743ms step_avg:118.20ms
step:1454/2245 train_time:171866ms step_avg:118.20ms
step:1455/2245 train_time:171982ms step_avg:118.20ms
step:1456/2245 train_time:172104ms step_avg:118.20ms
step:1457/2245 train_time:172219ms step_avg:118.20ms
step:1458/2245 train_time:172341ms step_avg:118.20ms
step:1459/2245 train_time:172457ms step_avg:118.20ms
step:1460/2245 train_time:172579ms step_avg:118.20ms
step:1461/2245 train_time:172694ms step_avg:118.20ms
step:1462/2245 train_time:172816ms step_avg:118.21ms
step:1463/2245 train_time:172932ms step_avg:118.20ms
step:1464/2245 train_time:173054ms step_avg:118.21ms
step:1465/2245 train_time:173170ms step_avg:118.20ms
step:1466/2245 train_time:173292ms step_avg:118.21ms
step:1467/2245 train_time:173407ms step_avg:118.21ms
step:1468/2245 train_time:173529ms step_avg:118.21ms
step:1469/2245 train_time:173645ms step_avg:118.21ms
step:1470/2245 train_time:173767ms step_avg:118.21ms
step:1471/2245 train_time:173883ms step_avg:118.21ms
step:1472/2245 train_time:174006ms step_avg:118.21ms
step:1473/2245 train_time:174124ms step_avg:118.21ms
step:1474/2245 train_time:174246ms step_avg:118.21ms
step:1475/2245 train_time:174363ms step_avg:118.21ms
step:1476/2245 train_time:174486ms step_avg:118.22ms
step:1477/2245 train_time:174603ms step_avg:118.21ms
step:1478/2245 train_time:174726ms step_avg:118.22ms
step:1479/2245 train_time:174843ms step_avg:118.22ms
step:1480/2245 train_time:174966ms step_avg:118.22ms
step:1481/2245 train_time:175083ms step_avg:118.22ms
step:1482/2245 train_time:175206ms step_avg:118.22ms
step:1483/2245 train_time:175323ms step_avg:118.22ms
step:1484/2245 train_time:175447ms step_avg:118.23ms
step:1485/2245 train_time:175564ms step_avg:118.22ms
step:1486/2245 train_time:175687ms step_avg:118.23ms
step:1487/2245 train_time:175804ms step_avg:118.23ms
step:1488/2245 train_time:175927ms step_avg:118.23ms
step:1489/2245 train_time:176044ms step_avg:118.23ms
step:1490/2245 train_time:176167ms step_avg:118.23ms
step:1491/2245 train_time:176284ms step_avg:118.23ms
step:1492/2245 train_time:176407ms step_avg:118.24ms
step:1493/2245 train_time:176524ms step_avg:118.23ms
step:1494/2245 train_time:176648ms step_avg:118.24ms
step:1495/2245 train_time:176764ms step_avg:118.24ms
step:1496/2245 train_time:176887ms step_avg:118.24ms
step:1497/2245 train_time:177004ms step_avg:118.24ms
step:1498/2245 train_time:177128ms step_avg:118.24ms
step:1499/2245 train_time:177244ms step_avg:118.24ms
step:1500/2245 train_time:177368ms step_avg:118.25ms
step:1500/2245 val_loss:3.4460 train_time:177434ms step_avg:118.29ms
step:1501/2245 train_time:177485ms step_avg:118.24ms
step:1502/2245 train_time:177607ms step_avg:118.25ms
step:1503/2245 train_time:177723ms step_avg:118.25ms
step:1504/2245 train_time:177845ms step_avg:118.25ms
step:1505/2245 train_time:177961ms step_avg:118.25ms
step:1506/2245 train_time:178085ms step_avg:118.25ms
step:1507/2245 train_time:178202ms step_avg:118.25ms
step:1508/2245 train_time:178324ms step_avg:118.25ms
step:1509/2245 train_time:178441ms step_avg:118.25ms
step:1510/2245 train_time:178565ms step_avg:118.25ms
step:1511/2245 train_time:178682ms step_avg:118.25ms
step:1512/2245 train_time:178805ms step_avg:118.26ms
step:1513/2245 train_time:178921ms step_avg:118.26ms
step:1514/2245 train_time:179044ms step_avg:118.26ms
step:1515/2245 train_time:179161ms step_avg:118.26ms
step:1516/2245 train_time:179284ms step_avg:118.26ms
step:1517/2245 train_time:179400ms step_avg:118.26ms
step:1518/2245 train_time:179523ms step_avg:118.26ms
step:1519/2245 train_time:179639ms step_avg:118.26ms
step:1520/2245 train_time:179763ms step_avg:118.26ms
step:1521/2245 train_time:179880ms step_avg:118.26ms
step:1522/2245 train_time:180003ms step_avg:118.27ms
step:1523/2245 train_time:180119ms step_avg:118.27ms
step:1524/2245 train_time:180242ms step_avg:118.27ms
step:1525/2245 train_time:180359ms step_avg:118.27ms
step:1526/2245 train_time:180482ms step_avg:118.27ms
step:1527/2245 train_time:180599ms step_avg:118.27ms
step:1528/2245 train_time:180722ms step_avg:118.27ms
step:1529/2245 train_time:180838ms step_avg:118.27ms
step:1530/2245 train_time:180962ms step_avg:118.28ms
step:1531/2245 train_time:181078ms step_avg:118.27ms
step:1532/2245 train_time:181201ms step_avg:118.28ms
step:1533/2245 train_time:181318ms step_avg:118.28ms
step:1534/2245 train_time:181442ms step_avg:118.28ms
step:1535/2245 train_time:181559ms step_avg:118.28ms
step:1536/2245 train_time:181682ms step_avg:118.28ms
step:1537/2245 train_time:181799ms step_avg:118.28ms
step:1538/2245 train_time:181923ms step_avg:118.29ms
step:1539/2245 train_time:182039ms step_avg:118.28ms
step:1540/2245 train_time:182162ms step_avg:118.29ms
step:1541/2245 train_time:182279ms step_avg:118.29ms
step:1542/2245 train_time:182401ms step_avg:118.29ms
step:1543/2245 train_time:182518ms step_avg:118.29ms
step:1544/2245 train_time:182642ms step_avg:118.29ms
step:1545/2245 train_time:182759ms step_avg:118.29ms
step:1546/2245 train_time:182882ms step_avg:118.29ms
step:1547/2245 train_time:182999ms step_avg:118.29ms
step:1548/2245 train_time:183122ms step_avg:118.30ms
step:1549/2245 train_time:183239ms step_avg:118.29ms
step:1550/2245 train_time:183363ms step_avg:118.30ms
step:1551/2245 train_time:183480ms step_avg:118.30ms
step:1552/2245 train_time:183602ms step_avg:118.30ms
step:1553/2245 train_time:183719ms step_avg:118.30ms
step:1554/2245 train_time:183842ms step_avg:118.30ms
step:1555/2245 train_time:183959ms step_avg:118.30ms
step:1556/2245 train_time:184083ms step_avg:118.31ms
step:1557/2245 train_time:184199ms step_avg:118.30ms
step:1558/2245 train_time:184323ms step_avg:118.31ms
step:1559/2245 train_time:184440ms step_avg:118.31ms
step:1560/2245 train_time:184562ms step_avg:118.31ms
step:1561/2245 train_time:184679ms step_avg:118.31ms
step:1562/2245 train_time:184802ms step_avg:118.31ms
step:1563/2245 train_time:184919ms step_avg:118.31ms
step:1564/2245 train_time:185042ms step_avg:118.31ms
step:1565/2245 train_time:185158ms step_avg:118.31ms
step:1566/2245 train_time:185282ms step_avg:118.32ms
step:1567/2245 train_time:185399ms step_avg:118.31ms
step:1568/2245 train_time:185522ms step_avg:118.32ms
step:1569/2245 train_time:185638ms step_avg:118.32ms
step:1570/2245 train_time:185761ms step_avg:118.32ms
step:1571/2245 train_time:185878ms step_avg:118.32ms
step:1572/2245 train_time:186001ms step_avg:118.32ms
step:1573/2245 train_time:186117ms step_avg:118.32ms
step:1574/2245 train_time:186241ms step_avg:118.32ms
step:1575/2245 train_time:186357ms step_avg:118.32ms
step:1576/2245 train_time:186481ms step_avg:118.33ms
step:1577/2245 train_time:186597ms step_avg:118.32ms
step:1578/2245 train_time:186721ms step_avg:118.33ms
step:1579/2245 train_time:186838ms step_avg:118.33ms
step:1580/2245 train_time:186962ms step_avg:118.33ms
step:1581/2245 train_time:187078ms step_avg:118.33ms
step:1582/2245 train_time:187201ms step_avg:118.33ms
step:1583/2245 train_time:187318ms step_avg:118.33ms
step:1584/2245 train_time:187441ms step_avg:118.33ms
step:1585/2245 train_time:187558ms step_avg:118.33ms
step:1586/2245 train_time:187681ms step_avg:118.34ms
step:1587/2245 train_time:187798ms step_avg:118.33ms
step:1588/2245 train_time:187922ms step_avg:118.34ms
step:1589/2245 train_time:188039ms step_avg:118.34ms
step:1590/2245 train_time:188162ms step_avg:118.34ms
step:1591/2245 train_time:188279ms step_avg:118.34ms
step:1592/2245 train_time:188402ms step_avg:118.34ms
step:1593/2245 train_time:188519ms step_avg:118.34ms
step:1594/2245 train_time:188643ms step_avg:118.35ms
step:1595/2245 train_time:188759ms step_avg:118.34ms
step:1596/2245 train_time:188883ms step_avg:118.35ms
step:1597/2245 train_time:188999ms step_avg:118.35ms
step:1598/2245 train_time:189121ms step_avg:118.35ms
step:1599/2245 train_time:189238ms step_avg:118.35ms
step:1600/2245 train_time:189361ms step_avg:118.35ms
step:1601/2245 train_time:189479ms step_avg:118.35ms
step:1602/2245 train_time:189602ms step_avg:118.35ms
step:1603/2245 train_time:189718ms step_avg:118.35ms
step:1604/2245 train_time:189842ms step_avg:118.36ms
step:1605/2245 train_time:189958ms step_avg:118.35ms
step:1606/2245 train_time:190082ms step_avg:118.36ms
step:1607/2245 train_time:190198ms step_avg:118.36ms
step:1608/2245 train_time:190322ms step_avg:118.36ms
step:1609/2245 train_time:190438ms step_avg:118.36ms
step:1610/2245 train_time:190561ms step_avg:118.36ms
step:1611/2245 train_time:190677ms step_avg:118.36ms
step:1612/2245 train_time:190800ms step_avg:118.36ms
step:1613/2245 train_time:190917ms step_avg:118.36ms
step:1614/2245 train_time:191040ms step_avg:118.36ms
step:1615/2245 train_time:191157ms step_avg:118.36ms
step:1616/2245 train_time:191281ms step_avg:118.37ms
step:1617/2245 train_time:191397ms step_avg:118.37ms
step:1618/2245 train_time:191520ms step_avg:118.37ms
step:1619/2245 train_time:191637ms step_avg:118.37ms
step:1620/2245 train_time:191760ms step_avg:118.37ms
step:1621/2245 train_time:191877ms step_avg:118.37ms
step:1622/2245 train_time:192000ms step_avg:118.37ms
step:1623/2245 train_time:192117ms step_avg:118.37ms
step:1624/2245 train_time:192240ms step_avg:118.37ms
step:1625/2245 train_time:192357ms step_avg:118.37ms
step:1626/2245 train_time:192480ms step_avg:118.38ms
step:1627/2245 train_time:192597ms step_avg:118.38ms
step:1628/2245 train_time:192720ms step_avg:118.38ms
step:1629/2245 train_time:192837ms step_avg:118.38ms
step:1630/2245 train_time:192960ms step_avg:118.38ms
step:1631/2245 train_time:193077ms step_avg:118.38ms
step:1632/2245 train_time:193200ms step_avg:118.38ms
step:1633/2245 train_time:193316ms step_avg:118.38ms
step:1634/2245 train_time:193440ms step_avg:118.38ms
step:1635/2245 train_time:193557ms step_avg:118.38ms
step:1636/2245 train_time:193680ms step_avg:118.39ms
step:1637/2245 train_time:193796ms step_avg:118.39ms
step:1638/2245 train_time:193920ms step_avg:118.39ms
step:1639/2245 train_time:194036ms step_avg:118.39ms
step:1640/2245 train_time:194159ms step_avg:118.39ms
step:1641/2245 train_time:194277ms step_avg:118.39ms
step:1642/2245 train_time:194399ms step_avg:118.39ms
step:1643/2245 train_time:194516ms step_avg:118.39ms
step:1644/2245 train_time:194639ms step_avg:118.39ms
step:1645/2245 train_time:194756ms step_avg:118.39ms
step:1646/2245 train_time:194879ms step_avg:118.40ms
step:1647/2245 train_time:194996ms step_avg:118.39ms
step:1648/2245 train_time:195119ms step_avg:118.40ms
step:1649/2245 train_time:195236ms step_avg:118.40ms
step:1650/2245 train_time:195360ms step_avg:118.40ms
step:1651/2245 train_time:195476ms step_avg:118.40ms
step:1652/2245 train_time:195600ms step_avg:118.40ms
step:1653/2245 train_time:195717ms step_avg:118.40ms
step:1654/2245 train_time:195840ms step_avg:118.40ms
step:1655/2245 train_time:195956ms step_avg:118.40ms
step:1656/2245 train_time:196079ms step_avg:118.41ms
step:1657/2245 train_time:196196ms step_avg:118.40ms
step:1658/2245 train_time:196319ms step_avg:118.41ms
step:1659/2245 train_time:196436ms step_avg:118.41ms
step:1660/2245 train_time:196559ms step_avg:118.41ms
step:1661/2245 train_time:196675ms step_avg:118.41ms
step:1662/2245 train_time:196799ms step_avg:118.41ms
step:1663/2245 train_time:196917ms step_avg:118.41ms
step:1664/2245 train_time:197039ms step_avg:118.41ms
step:1665/2245 train_time:197156ms step_avg:118.41ms
step:1666/2245 train_time:197280ms step_avg:118.42ms
step:1667/2245 train_time:197396ms step_avg:118.41ms
step:1668/2245 train_time:197519ms step_avg:118.42ms
step:1669/2245 train_time:197636ms step_avg:118.42ms
step:1670/2245 train_time:197759ms step_avg:118.42ms
step:1671/2245 train_time:197877ms step_avg:118.42ms
step:1672/2245 train_time:198001ms step_avg:118.42ms
step:1673/2245 train_time:198118ms step_avg:118.42ms
step:1674/2245 train_time:198241ms step_avg:118.42ms
step:1675/2245 train_time:198357ms step_avg:118.42ms
step:1676/2245 train_time:198481ms step_avg:118.43ms
step:1677/2245 train_time:198598ms step_avg:118.42ms
step:1678/2245 train_time:198721ms step_avg:118.43ms
step:1679/2245 train_time:198838ms step_avg:118.43ms
step:1680/2245 train_time:198961ms step_avg:118.43ms
step:1681/2245 train_time:199078ms step_avg:118.43ms
step:1682/2245 train_time:199202ms step_avg:118.43ms
step:1683/2245 train_time:199318ms step_avg:118.43ms
step:1684/2245 train_time:199442ms step_avg:118.43ms
step:1685/2245 train_time:199558ms step_avg:118.43ms
step:1686/2245 train_time:199682ms step_avg:118.44ms
step:1687/2245 train_time:199799ms step_avg:118.43ms
step:1688/2245 train_time:199922ms step_avg:118.44ms
step:1689/2245 train_time:200039ms step_avg:118.44ms
step:1690/2245 train_time:200161ms step_avg:118.44ms
step:1691/2245 train_time:200279ms step_avg:118.44ms
step:1692/2245 train_time:200402ms step_avg:118.44ms
step:1693/2245 train_time:200518ms step_avg:118.44ms
step:1694/2245 train_time:200642ms step_avg:118.44ms
step:1695/2245 train_time:200758ms step_avg:118.44ms
step:1696/2245 train_time:200882ms step_avg:118.44ms
step:1697/2245 train_time:200999ms step_avg:118.44ms
step:1698/2245 train_time:201122ms step_avg:118.45ms
step:1699/2245 train_time:201239ms step_avg:118.45ms
step:1700/2245 train_time:201362ms step_avg:118.45ms
step:1701/2245 train_time:201479ms step_avg:118.45ms
step:1702/2245 train_time:201603ms step_avg:118.45ms
step:1703/2245 train_time:201719ms step_avg:118.45ms
step:1704/2245 train_time:201842ms step_avg:118.45ms
step:1705/2245 train_time:201959ms step_avg:118.45ms
step:1706/2245 train_time:202082ms step_avg:118.45ms
step:1707/2245 train_time:202199ms step_avg:118.45ms
step:1708/2245 train_time:202322ms step_avg:118.46ms
step:1709/2245 train_time:202439ms step_avg:118.45ms
step:1710/2245 train_time:202563ms step_avg:118.46ms
step:1711/2245 train_time:202679ms step_avg:118.46ms
step:1712/2245 train_time:202802ms step_avg:118.46ms
step:1713/2245 train_time:202919ms step_avg:118.46ms
step:1714/2245 train_time:203043ms step_avg:118.46ms
step:1715/2245 train_time:203159ms step_avg:118.46ms
step:1716/2245 train_time:203283ms step_avg:118.46ms
step:1717/2245 train_time:203400ms step_avg:118.46ms
step:1718/2245 train_time:203522ms step_avg:118.46ms
step:1719/2245 train_time:203639ms step_avg:118.46ms
step:1720/2245 train_time:203762ms step_avg:118.47ms
step:1721/2245 train_time:203878ms step_avg:118.46ms
step:1722/2245 train_time:204002ms step_avg:118.47ms
step:1723/2245 train_time:204119ms step_avg:118.47ms
step:1724/2245 train_time:204242ms step_avg:118.47ms
step:1725/2245 train_time:204358ms step_avg:118.47ms
step:1726/2245 train_time:204481ms step_avg:118.47ms
step:1727/2245 train_time:204598ms step_avg:118.47ms
step:1728/2245 train_time:204722ms step_avg:118.47ms
step:1729/2245 train_time:204838ms step_avg:118.47ms
step:1730/2245 train_time:204962ms step_avg:118.48ms
step:1731/2245 train_time:205079ms step_avg:118.47ms
step:1732/2245 train_time:205202ms step_avg:118.48ms
step:1733/2245 train_time:205318ms step_avg:118.48ms
step:1734/2245 train_time:205442ms step_avg:118.48ms
step:1735/2245 train_time:205559ms step_avg:118.48ms
step:1736/2245 train_time:205683ms step_avg:118.48ms
step:1737/2245 train_time:205799ms step_avg:118.48ms
step:1738/2245 train_time:205923ms step_avg:118.48ms
step:1739/2245 train_time:206039ms step_avg:118.48ms
step:1740/2245 train_time:206162ms step_avg:118.48ms
step:1741/2245 train_time:206279ms step_avg:118.48ms
step:1742/2245 train_time:206402ms step_avg:118.49ms
step:1743/2245 train_time:206518ms step_avg:118.48ms
step:1744/2245 train_time:206641ms step_avg:118.49ms
step:1745/2245 train_time:206759ms step_avg:118.49ms
step:1746/2245 train_time:206882ms step_avg:118.49ms
step:1747/2245 train_time:206999ms step_avg:118.49ms
step:1748/2245 train_time:207122ms step_avg:118.49ms
step:1749/2245 train_time:207238ms step_avg:118.49ms
step:1750/2245 train_time:207361ms step_avg:118.49ms
step:1750/2245 val_loss:3.3825 train_time:207428ms step_avg:118.53ms
step:1751/2245 train_time:207479ms step_avg:118.49ms
step:1752/2245 train_time:207601ms step_avg:118.49ms
step:1753/2245 train_time:207717ms step_avg:118.49ms
step:1754/2245 train_time:207840ms step_avg:118.50ms
step:1755/2245 train_time:207957ms step_avg:118.49ms
step:1756/2245 train_time:208080ms step_avg:118.50ms
step:1757/2245 train_time:208196ms step_avg:118.50ms
step:1758/2245 train_time:208318ms step_avg:118.50ms
step:1759/2245 train_time:208435ms step_avg:118.50ms
step:1760/2245 train_time:208559ms step_avg:118.50ms
step:1761/2245 train_time:208675ms step_avg:118.50ms
step:1762/2245 train_time:208798ms step_avg:118.50ms
step:1763/2245 train_time:208914ms step_avg:118.50ms
step:1764/2245 train_time:209037ms step_avg:118.50ms
step:1765/2245 train_time:209154ms step_avg:118.50ms
step:1766/2245 train_time:209276ms step_avg:118.50ms
step:1767/2245 train_time:209393ms step_avg:118.50ms
step:1768/2245 train_time:209515ms step_avg:118.50ms
step:1769/2245 train_time:209632ms step_avg:118.50ms
step:1770/2245 train_time:209755ms step_avg:118.51ms
step:1771/2245 train_time:209872ms step_avg:118.50ms
step:1772/2245 train_time:209995ms step_avg:118.51ms
step:1773/2245 train_time:210111ms step_avg:118.51ms
step:1774/2245 train_time:210233ms step_avg:118.51ms
step:1775/2245 train_time:210350ms step_avg:118.51ms
step:1776/2245 train_time:210472ms step_avg:118.51ms
step:1777/2245 train_time:210589ms step_avg:118.51ms
step:1778/2245 train_time:210713ms step_avg:118.51ms
step:1779/2245 train_time:210829ms step_avg:118.51ms
step:1780/2245 train_time:210953ms step_avg:118.51ms
step:1781/2245 train_time:211069ms step_avg:118.51ms
step:1782/2245 train_time:211192ms step_avg:118.51ms
step:1783/2245 train_time:211308ms step_avg:118.51ms
step:1784/2245 train_time:211431ms step_avg:118.52ms
step:1785/2245 train_time:211548ms step_avg:118.51ms
step:1786/2245 train_time:211671ms step_avg:118.52ms
step:1787/2245 train_time:211788ms step_avg:118.52ms
step:1788/2245 train_time:211911ms step_avg:118.52ms
step:1789/2245 train_time:212027ms step_avg:118.52ms
step:1790/2245 train_time:212151ms step_avg:118.52ms
step:1791/2245 train_time:212267ms step_avg:118.52ms
step:1792/2245 train_time:212390ms step_avg:118.52ms
step:1793/2245 train_time:212506ms step_avg:118.52ms
step:1794/2245 train_time:212629ms step_avg:118.52ms
step:1795/2245 train_time:212745ms step_avg:118.52ms
step:1796/2245 train_time:212868ms step_avg:118.52ms
step:1797/2245 train_time:212984ms step_avg:118.52ms
step:1798/2245 train_time:213108ms step_avg:118.52ms
step:1799/2245 train_time:213224ms step_avg:118.52ms
step:1800/2245 train_time:213347ms step_avg:118.53ms
step:1801/2245 train_time:213464ms step_avg:118.53ms
step:1802/2245 train_time:213588ms step_avg:118.53ms
step:1803/2245 train_time:213704ms step_avg:118.53ms
step:1804/2245 train_time:213827ms step_avg:118.53ms
step:1805/2245 train_time:213944ms step_avg:118.53ms
step:1806/2245 train_time:214067ms step_avg:118.53ms
step:1807/2245 train_time:214184ms step_avg:118.53ms
step:1808/2245 train_time:214307ms step_avg:118.53ms
step:1809/2245 train_time:214424ms step_avg:118.53ms
step:1810/2245 train_time:214547ms step_avg:118.53ms
step:1811/2245 train_time:214664ms step_avg:118.53ms
step:1812/2245 train_time:214788ms step_avg:118.54ms
step:1813/2245 train_time:214905ms step_avg:118.54ms
step:1814/2245 train_time:215027ms step_avg:118.54ms
step:1815/2245 train_time:215145ms step_avg:118.54ms
step:1816/2245 train_time:215267ms step_avg:118.54ms
step:1817/2245 train_time:215384ms step_avg:118.54ms
step:1818/2245 train_time:215507ms step_avg:118.54ms
step:1819/2245 train_time:215624ms step_avg:118.54ms
step:1820/2245 train_time:215747ms step_avg:118.54ms
step:1821/2245 train_time:215864ms step_avg:118.54ms
step:1822/2245 train_time:215987ms step_avg:118.54ms
step:1823/2245 train_time:216104ms step_avg:118.54ms
step:1824/2245 train_time:216227ms step_avg:118.55ms
step:1825/2245 train_time:216343ms step_avg:118.54ms
step:1826/2245 train_time:216466ms step_avg:118.55ms
step:1827/2245 train_time:216582ms step_avg:118.55ms
step:1828/2245 train_time:216705ms step_avg:118.55ms
step:1829/2245 train_time:216822ms step_avg:118.55ms
step:1830/2245 train_time:216946ms step_avg:118.55ms
step:1831/2245 train_time:217062ms step_avg:118.55ms
step:1832/2245 train_time:217186ms step_avg:118.55ms
step:1833/2245 train_time:217303ms step_avg:118.55ms
step:1834/2245 train_time:217426ms step_avg:118.55ms
step:1835/2245 train_time:217542ms step_avg:118.55ms
step:1836/2245 train_time:217665ms step_avg:118.55ms
step:1837/2245 train_time:217782ms step_avg:118.55ms
step:1838/2245 train_time:217905ms step_avg:118.56ms
step:1839/2245 train_time:218022ms step_avg:118.55ms
step:1840/2245 train_time:218145ms step_avg:118.56ms
step:1841/2245 train_time:218262ms step_avg:118.56ms
step:1842/2245 train_time:218386ms step_avg:118.56ms
step:1843/2245 train_time:218502ms step_avg:118.56ms
step:1844/2245 train_time:218625ms step_avg:118.56ms
step:1845/2245 train_time:218742ms step_avg:118.56ms
step:1846/2245 train_time:218865ms step_avg:118.56ms
step:1847/2245 train_time:218982ms step_avg:118.56ms
step:1848/2245 train_time:219105ms step_avg:118.56ms
step:1849/2245 train_time:219221ms step_avg:118.56ms
step:1850/2245 train_time:219345ms step_avg:118.56ms
step:1851/2245 train_time:219462ms step_avg:118.56ms
step:1852/2245 train_time:219585ms step_avg:118.57ms
step:1853/2245 train_time:219701ms step_avg:118.57ms
step:1854/2245 train_time:219824ms step_avg:118.57ms
step:1855/2245 train_time:219941ms step_avg:118.57ms
step:1856/2245 train_time:220064ms step_avg:118.57ms
step:1857/2245 train_time:220181ms step_avg:118.57ms
step:1858/2245 train_time:220305ms step_avg:118.57ms
step:1859/2245 train_time:220421ms step_avg:118.57ms
step:1860/2245 train_time:220545ms step_avg:118.57ms
step:1861/2245 train_time:220662ms step_avg:118.57ms
step:1862/2245 train_time:220784ms step_avg:118.57ms
step:1863/2245 train_time:220901ms step_avg:118.57ms
step:1864/2245 train_time:221025ms step_avg:118.58ms
step:1865/2245 train_time:221141ms step_avg:118.57ms
step:1866/2245 train_time:221264ms step_avg:118.58ms
step:1867/2245 train_time:221381ms step_avg:118.58ms
step:1868/2245 train_time:221505ms step_avg:118.58ms
step:1869/2245 train_time:221621ms step_avg:118.58ms
step:1870/2245 train_time:221744ms step_avg:118.58ms
step:1871/2245 train_time:221861ms step_avg:118.58ms
step:1872/2245 train_time:221983ms step_avg:118.58ms
step:1873/2245 train_time:222100ms step_avg:118.58ms
step:1874/2245 train_time:222224ms step_avg:118.58ms
step:1875/2245 train_time:222340ms step_avg:118.58ms
step:1876/2245 train_time:222463ms step_avg:118.58ms
step:1877/2245 train_time:222580ms step_avg:118.58ms
step:1878/2245 train_time:222702ms step_avg:118.58ms
step:1879/2245 train_time:222819ms step_avg:118.58ms
step:1880/2245 train_time:222943ms step_avg:118.59ms
step:1881/2245 train_time:223058ms step_avg:118.59ms
step:1882/2245 train_time:223182ms step_avg:118.59ms
step:1883/2245 train_time:223298ms step_avg:118.59ms
step:1884/2245 train_time:223421ms step_avg:118.59ms
step:1885/2245 train_time:223538ms step_avg:118.59ms
step:1886/2245 train_time:223661ms step_avg:118.59ms
step:1887/2245 train_time:223778ms step_avg:118.59ms
step:1888/2245 train_time:223901ms step_avg:118.59ms
step:1889/2245 train_time:224017ms step_avg:118.59ms
step:1890/2245 train_time:224141ms step_avg:118.59ms
step:1891/2245 train_time:224258ms step_avg:118.59ms
step:1892/2245 train_time:224381ms step_avg:118.59ms
step:1893/2245 train_time:224499ms step_avg:118.59ms
step:1894/2245 train_time:224622ms step_avg:118.60ms
step:1895/2245 train_time:224739ms step_avg:118.60ms
step:1896/2245 train_time:224862ms step_avg:118.60ms
step:1897/2245 train_time:224978ms step_avg:118.60ms
step:1898/2245 train_time:225103ms step_avg:118.60ms
step:1899/2245 train_time:225219ms step_avg:118.60ms
step:1900/2245 train_time:225342ms step_avg:118.60ms
step:1901/2245 train_time:225459ms step_avg:118.60ms
step:1902/2245 train_time:225583ms step_avg:118.60ms
step:1903/2245 train_time:225700ms step_avg:118.60ms
step:1904/2245 train_time:225822ms step_avg:118.60ms
step:1905/2245 train_time:225940ms step_avg:118.60ms
step:1906/2245 train_time:226064ms step_avg:118.61ms
step:1907/2245 train_time:226180ms step_avg:118.61ms
step:1908/2245 train_time:226303ms step_avg:118.61ms
step:1909/2245 train_time:226420ms step_avg:118.61ms
step:1910/2245 train_time:226544ms step_avg:118.61ms
step:1911/2245 train_time:226660ms step_avg:118.61ms
step:1912/2245 train_time:226784ms step_avg:118.61ms
step:1913/2245 train_time:226901ms step_avg:118.61ms
step:1914/2245 train_time:227024ms step_avg:118.61ms
step:1915/2245 train_time:227141ms step_avg:118.61ms
step:1916/2245 train_time:227264ms step_avg:118.61ms
step:1917/2245 train_time:227381ms step_avg:118.61ms
step:1918/2245 train_time:227504ms step_avg:118.62ms
step:1919/2245 train_time:227620ms step_avg:118.61ms
step:1920/2245 train_time:227744ms step_avg:118.62ms
step:1921/2245 train_time:227861ms step_avg:118.62ms
step:1922/2245 train_time:227984ms step_avg:118.62ms
step:1923/2245 train_time:228100ms step_avg:118.62ms
step:1924/2245 train_time:228224ms step_avg:118.62ms
step:1925/2245 train_time:228341ms step_avg:118.62ms
step:1926/2245 train_time:228464ms step_avg:118.62ms
step:1927/2245 train_time:228581ms step_avg:118.62ms
step:1928/2245 train_time:228704ms step_avg:118.62ms
step:1929/2245 train_time:228820ms step_avg:118.62ms
step:1930/2245 train_time:228944ms step_avg:118.62ms
step:1931/2245 train_time:229060ms step_avg:118.62ms
step:1932/2245 train_time:229183ms step_avg:118.62ms
step:1933/2245 train_time:229300ms step_avg:118.62ms
step:1934/2245 train_time:229423ms step_avg:118.63ms
step:1935/2245 train_time:229540ms step_avg:118.63ms
step:1936/2245 train_time:229663ms step_avg:118.63ms
step:1937/2245 train_time:229780ms step_avg:118.63ms
step:1938/2245 train_time:229904ms step_avg:118.63ms
step:1939/2245 train_time:230020ms step_avg:118.63ms
step:1940/2245 train_time:230143ms step_avg:118.63ms
step:1941/2245 train_time:230260ms step_avg:118.63ms
step:1942/2245 train_time:230382ms step_avg:118.63ms
step:1943/2245 train_time:230499ms step_avg:118.63ms
step:1944/2245 train_time:230622ms step_avg:118.63ms
step:1945/2245 train_time:230739ms step_avg:118.63ms
step:1946/2245 train_time:230862ms step_avg:118.63ms
step:1947/2245 train_time:230979ms step_avg:118.63ms
step:1948/2245 train_time:231103ms step_avg:118.64ms
step:1949/2245 train_time:231220ms step_avg:118.64ms
step:1950/2245 train_time:231343ms step_avg:118.64ms
step:1951/2245 train_time:231460ms step_avg:118.64ms
step:1952/2245 train_time:231583ms step_avg:118.64ms
step:1953/2245 train_time:231700ms step_avg:118.64ms
step:1954/2245 train_time:231824ms step_avg:118.64ms
step:1955/2245 train_time:231941ms step_avg:118.64ms
step:1956/2245 train_time:232063ms step_avg:118.64ms
step:1957/2245 train_time:232180ms step_avg:118.64ms
step:1958/2245 train_time:232303ms step_avg:118.64ms
step:1959/2245 train_time:232419ms step_avg:118.64ms
step:1960/2245 train_time:232543ms step_avg:118.64ms
step:1961/2245 train_time:232660ms step_avg:118.64ms
step:1962/2245 train_time:232783ms step_avg:118.65ms
step:1963/2245 train_time:232899ms step_avg:118.64ms
step:1964/2245 train_time:233022ms step_avg:118.65ms
step:1965/2245 train_time:233138ms step_avg:118.65ms
step:1966/2245 train_time:233262ms step_avg:118.65ms
step:1967/2245 train_time:233380ms step_avg:118.65ms
step:1968/2245 train_time:233503ms step_avg:118.65ms
step:1969/2245 train_time:233619ms step_avg:118.65ms
step:1970/2245 train_time:233742ms step_avg:118.65ms
step:1971/2245 train_time:233859ms step_avg:118.65ms
step:1972/2245 train_time:233982ms step_avg:118.65ms
step:1973/2245 train_time:234099ms step_avg:118.65ms
step:1974/2245 train_time:234222ms step_avg:118.65ms
step:1975/2245 train_time:234338ms step_avg:118.65ms
step:1976/2245 train_time:234462ms step_avg:118.65ms
step:1977/2245 train_time:234578ms step_avg:118.65ms
step:1978/2245 train_time:234701ms step_avg:118.66ms
step:1979/2245 train_time:234817ms step_avg:118.65ms
step:1980/2245 train_time:234941ms step_avg:118.66ms
step:1981/2245 train_time:235057ms step_avg:118.66ms
step:1982/2245 train_time:235179ms step_avg:118.66ms
step:1983/2245 train_time:235297ms step_avg:118.66ms
step:1984/2245 train_time:235420ms step_avg:118.66ms
step:1985/2245 train_time:235537ms step_avg:118.66ms
step:1986/2245 train_time:235660ms step_avg:118.66ms
step:1987/2245 train_time:235777ms step_avg:118.66ms
step:1988/2245 train_time:235899ms step_avg:118.66ms
step:1989/2245 train_time:236015ms step_avg:118.66ms
step:1990/2245 train_time:236138ms step_avg:118.66ms
step:1991/2245 train_time:236255ms step_avg:118.66ms
step:1992/2245 train_time:236377ms step_avg:118.66ms
step:1993/2245 train_time:236495ms step_avg:118.66ms
step:1994/2245 train_time:236617ms step_avg:118.66ms
step:1995/2245 train_time:236734ms step_avg:118.66ms
step:1996/2245 train_time:236856ms step_avg:118.67ms
step:1997/2245 train_time:236973ms step_avg:118.66ms
step:1998/2245 train_time:237096ms step_avg:118.67ms
step:1999/2245 train_time:237213ms step_avg:118.67ms
step:2000/2245 train_time:237336ms step_avg:118.67ms
step:2000/2245 val_loss:3.3270 train_time:237402ms step_avg:118.70ms
step:2001/2245 train_time:237453ms step_avg:118.67ms
step:2002/2245 train_time:237575ms step_avg:118.67ms
step:2003/2245 train_time:237692ms step_avg:118.67ms
step:2004/2245 train_time:237815ms step_avg:118.67ms
step:2005/2245 train_time:237931ms step_avg:118.67ms
step:2006/2245 train_time:238055ms step_avg:118.67ms
step:2007/2245 train_time:238171ms step_avg:118.67ms
step:2008/2245 train_time:238294ms step_avg:118.67ms
step:2009/2245 train_time:238410ms step_avg:118.67ms
step:2010/2245 train_time:238534ms step_avg:118.67ms
step:2011/2245 train_time:238650ms step_avg:118.67ms
step:2012/2245 train_time:238773ms step_avg:118.67ms
step:2013/2245 train_time:238890ms step_avg:118.67ms
step:2014/2245 train_time:239013ms step_avg:118.68ms
step:2015/2245 train_time:239130ms step_avg:118.67ms
step:2016/2245 train_time:239252ms step_avg:118.68ms
step:2017/2245 train_time:239369ms step_avg:118.68ms
step:2018/2245 train_time:239492ms step_avg:118.68ms
step:2019/2245 train_time:239610ms step_avg:118.68ms
step:2020/2245 train_time:239733ms step_avg:118.68ms
step:2021/2245 train_time:239849ms step_avg:118.68ms
step:2022/2245 train_time:239972ms step_avg:118.68ms
step:2023/2245 train_time:240089ms step_avg:118.68ms
step:2024/2245 train_time:240212ms step_avg:118.68ms
step:2025/2245 train_time:240328ms step_avg:118.68ms
step:2026/2245 train_time:240451ms step_avg:118.68ms
step:2027/2245 train_time:240568ms step_avg:118.68ms
step:2028/2245 train_time:240691ms step_avg:118.68ms
step:2029/2245 train_time:240807ms step_avg:118.68ms
step:2030/2245 train_time:240930ms step_avg:118.68ms
step:2031/2245 train_time:241047ms step_avg:118.68ms
step:2032/2245 train_time:241170ms step_avg:118.69ms
step:2033/2245 train_time:241287ms step_avg:118.68ms
step:2034/2245 train_time:241410ms step_avg:118.69ms
step:2035/2245 train_time:241526ms step_avg:118.69ms
step:2036/2245 train_time:241650ms step_avg:118.69ms
step:2037/2245 train_time:241767ms step_avg:118.69ms
step:2038/2245 train_time:241890ms step_avg:118.69ms
step:2039/2245 train_time:242006ms step_avg:118.69ms
step:2040/2245 train_time:242129ms step_avg:118.69ms
step:2041/2245 train_time:242246ms step_avg:118.69ms
step:2042/2245 train_time:242369ms step_avg:118.69ms
step:2043/2245 train_time:242486ms step_avg:118.69ms
step:2044/2245 train_time:242609ms step_avg:118.69ms
step:2045/2245 train_time:242725ms step_avg:118.69ms
step:2046/2245 train_time:242849ms step_avg:118.69ms
step:2047/2245 train_time:242966ms step_avg:118.69ms
step:2048/2245 train_time:243089ms step_avg:118.70ms
step:2049/2245 train_time:243206ms step_avg:118.70ms
step:2050/2245 train_time:243329ms step_avg:118.70ms
step:2051/2245 train_time:243446ms step_avg:118.70ms
step:2052/2245 train_time:243569ms step_avg:118.70ms
step:2053/2245 train_time:243686ms step_avg:118.70ms
step:2054/2245 train_time:243810ms step_avg:118.70ms
step:2055/2245 train_time:243926ms step_avg:118.70ms
step:2056/2245 train_time:244049ms step_avg:118.70ms
step:2057/2245 train_time:244165ms step_avg:118.70ms
step:2058/2245 train_time:244289ms step_avg:118.70ms
step:2059/2245 train_time:244405ms step_avg:118.70ms
step:2060/2245 train_time:244528ms step_avg:118.70ms
step:2061/2245 train_time:244645ms step_avg:118.70ms
step:2062/2245 train_time:244768ms step_avg:118.70ms
step:2063/2245 train_time:244885ms step_avg:118.70ms
step:2064/2245 train_time:245008ms step_avg:118.71ms
step:2065/2245 train_time:245126ms step_avg:118.71ms
step:2066/2245 train_time:245249ms step_avg:118.71ms
step:2067/2245 train_time:245365ms step_avg:118.71ms
step:2068/2245 train_time:245489ms step_avg:118.71ms
step:2069/2245 train_time:245605ms step_avg:118.71ms
step:2070/2245 train_time:245728ms step_avg:118.71ms
step:2071/2245 train_time:245845ms step_avg:118.71ms
step:2072/2245 train_time:245968ms step_avg:118.71ms
step:2073/2245 train_time:246085ms step_avg:118.71ms
step:2074/2245 train_time:246209ms step_avg:118.71ms
step:2075/2245 train_time:246325ms step_avg:118.71ms
step:2076/2245 train_time:246448ms step_avg:118.71ms
step:2077/2245 train_time:246564ms step_avg:118.71ms
step:2078/2245 train_time:246687ms step_avg:118.71ms
step:2079/2245 train_time:246804ms step_avg:118.71ms
step:2080/2245 train_time:246927ms step_avg:118.72ms
step:2081/2245 train_time:247045ms step_avg:118.71ms
step:2082/2245 train_time:247169ms step_avg:118.72ms
step:2083/2245 train_time:247285ms step_avg:118.72ms
step:2084/2245 train_time:247409ms step_avg:118.72ms
step:2085/2245 train_time:247525ms step_avg:118.72ms
step:2086/2245 train_time:247649ms step_avg:118.72ms
step:2087/2245 train_time:247765ms step_avg:118.72ms
step:2088/2245 train_time:247888ms step_avg:118.72ms
step:2089/2245 train_time:248005ms step_avg:118.72ms
step:2090/2245 train_time:248128ms step_avg:118.72ms
step:2091/2245 train_time:248244ms step_avg:118.72ms
step:2092/2245 train_time:248367ms step_avg:118.72ms
step:2093/2245 train_time:248484ms step_avg:118.72ms
step:2094/2245 train_time:248608ms step_avg:118.72ms
step:2095/2245 train_time:248725ms step_avg:118.72ms
step:2096/2245 train_time:248848ms step_avg:118.73ms
step:2097/2245 train_time:248965ms step_avg:118.72ms
step:2098/2245 train_time:249089ms step_avg:118.73ms
step:2099/2245 train_time:249205ms step_avg:118.73ms
step:2100/2245 train_time:249329ms step_avg:118.73ms
step:2101/2245 train_time:249446ms step_avg:118.73ms
step:2102/2245 train_time:249569ms step_avg:118.73ms
step:2103/2245 train_time:249685ms step_avg:118.73ms
step:2104/2245 train_time:249808ms step_avg:118.73ms
step:2105/2245 train_time:249925ms step_avg:118.73ms
step:2106/2245 train_time:250049ms step_avg:118.73ms
step:2107/2245 train_time:250166ms step_avg:118.73ms
step:2108/2245 train_time:250289ms step_avg:118.73ms
step:2109/2245 train_time:250406ms step_avg:118.73ms
step:2110/2245 train_time:250529ms step_avg:118.73ms
step:2111/2245 train_time:250646ms step_avg:118.73ms
step:2112/2245 train_time:250769ms step_avg:118.74ms
step:2113/2245 train_time:250885ms step_avg:118.73ms
step:2114/2245 train_time:251009ms step_avg:118.74ms
step:2115/2245 train_time:251125ms step_avg:118.74ms
step:2116/2245 train_time:251248ms step_avg:118.74ms
step:2117/2245 train_time:251364ms step_avg:118.74ms
step:2118/2245 train_time:251488ms step_avg:118.74ms
step:2119/2245 train_time:251604ms step_avg:118.74ms
step:2120/2245 train_time:251727ms step_avg:118.74ms
step:2121/2245 train_time:251844ms step_avg:118.74ms
step:2122/2245 train_time:251967ms step_avg:118.74ms
step:2123/2245 train_time:252084ms step_avg:118.74ms
step:2124/2245 train_time:252207ms step_avg:118.74ms
step:2125/2245 train_time:252324ms step_avg:118.74ms
step:2126/2245 train_time:252448ms step_avg:118.74ms
step:2127/2245 train_time:252564ms step_avg:118.74ms
step:2128/2245 train_time:252687ms step_avg:118.74ms
step:2129/2245 train_time:252804ms step_avg:118.74ms
step:2130/2245 train_time:252927ms step_avg:118.75ms
step:2131/2245 train_time:253044ms step_avg:118.74ms
step:2132/2245 train_time:253167ms step_avg:118.75ms
step:2133/2245 train_time:253284ms step_avg:118.75ms
step:2134/2245 train_time:253407ms step_avg:118.75ms
step:2135/2245 train_time:253525ms step_avg:118.75ms
step:2136/2245 train_time:253648ms step_avg:118.75ms
step:2137/2245 train_time:253765ms step_avg:118.75ms
step:2138/2245 train_time:253887ms step_avg:118.75ms
step:2139/2245 train_time:254004ms step_avg:118.75ms
step:2140/2245 train_time:254127ms step_avg:118.75ms
step:2141/2245 train_time:254243ms step_avg:118.75ms
step:2142/2245 train_time:254366ms step_avg:118.75ms
step:2143/2245 train_time:254482ms step_avg:118.75ms
step:2144/2245 train_time:254606ms step_avg:118.75ms
step:2145/2245 train_time:254723ms step_avg:118.75ms
step:2146/2245 train_time:254845ms step_avg:118.75ms
step:2147/2245 train_time:254962ms step_avg:118.75ms
step:2148/2245 train_time:255085ms step_avg:118.75ms
step:2149/2245 train_time:255202ms step_avg:118.75ms
step:2150/2245 train_time:255325ms step_avg:118.76ms
step:2151/2245 train_time:255442ms step_avg:118.75ms
step:2152/2245 train_time:255564ms step_avg:118.76ms
step:2153/2245 train_time:255682ms step_avg:118.76ms
step:2154/2245 train_time:255806ms step_avg:118.76ms
step:2155/2245 train_time:255922ms step_avg:118.76ms
step:2156/2245 train_time:256045ms step_avg:118.76ms
step:2157/2245 train_time:256162ms step_avg:118.76ms
step:2158/2245 train_time:256285ms step_avg:118.76ms
step:2159/2245 train_time:256402ms step_avg:118.76ms
step:2160/2245 train_time:256525ms step_avg:118.76ms
step:2161/2245 train_time:256641ms step_avg:118.76ms
step:2162/2245 train_time:256764ms step_avg:118.76ms
step:2163/2245 train_time:256880ms step_avg:118.76ms
step:2164/2245 train_time:257003ms step_avg:118.76ms
step:2165/2245 train_time:257120ms step_avg:118.76ms
step:2166/2245 train_time:257242ms step_avg:118.76ms
step:2167/2245 train_time:257359ms step_avg:118.76ms
step:2168/2245 train_time:257482ms step_avg:118.76ms
step:2169/2245 train_time:257597ms step_avg:118.76ms
step:2170/2245 train_time:257720ms step_avg:118.77ms
step:2171/2245 train_time:257837ms step_avg:118.76ms
step:2172/2245 train_time:257960ms step_avg:118.77ms
step:2173/2245 train_time:258076ms step_avg:118.76ms
step:2174/2245 train_time:258199ms step_avg:118.77ms
step:2175/2245 train_time:258316ms step_avg:118.77ms
step:2176/2245 train_time:258438ms step_avg:118.77ms
step:2177/2245 train_time:258554ms step_avg:118.77ms
step:2178/2245 train_time:258677ms step_avg:118.77ms
step:2179/2245 train_time:258793ms step_avg:118.77ms
step:2180/2245 train_time:258917ms step_avg:118.77ms
step:2181/2245 train_time:259033ms step_avg:118.77ms
step:2182/2245 train_time:259156ms step_avg:118.77ms
step:2183/2245 train_time:259273ms step_avg:118.77ms
step:2184/2245 train_time:259396ms step_avg:118.77ms
step:2185/2245 train_time:259513ms step_avg:118.77ms
step:2186/2245 train_time:259636ms step_avg:118.77ms
step:2187/2245 train_time:259752ms step_avg:118.77ms
step:2188/2245 train_time:259875ms step_avg:118.77ms
step:2189/2245 train_time:259992ms step_avg:118.77ms
step:2190/2245 train_time:260115ms step_avg:118.77ms
step:2191/2245 train_time:260231ms step_avg:118.77ms
step:2192/2245 train_time:260354ms step_avg:118.77ms
step:2193/2245 train_time:260470ms step_avg:118.77ms
step:2194/2245 train_time:260594ms step_avg:118.78ms
step:2195/2245 train_time:260710ms step_avg:118.77ms
step:2196/2245 train_time:260833ms step_avg:118.78ms
step:2197/2245 train_time:260949ms step_avg:118.78ms
step:2198/2245 train_time:261072ms step_avg:118.78ms
step:2199/2245 train_time:261189ms step_avg:118.78ms
step:2200/2245 train_time:261312ms step_avg:118.78ms
step:2201/2245 train_time:261428ms step_avg:118.78ms
step:2202/2245 train_time:261551ms step_avg:118.78ms
step:2203/2245 train_time:261668ms step_avg:118.78ms
step:2204/2245 train_time:261792ms step_avg:118.78ms
step:2205/2245 train_time:261909ms step_avg:118.78ms
step:2206/2245 train_time:262032ms step_avg:118.78ms
step:2207/2245 train_time:262149ms step_avg:118.78ms
step:2208/2245 train_time:262273ms step_avg:118.78ms
step:2209/2245 train_time:262390ms step_avg:118.78ms
step:2210/2245 train_time:262514ms step_avg:118.78ms
step:2211/2245 train_time:262630ms step_avg:118.78ms
step:2212/2245 train_time:262754ms step_avg:118.79ms
step:2213/2245 train_time:262871ms step_avg:118.79ms
step:2214/2245 train_time:262995ms step_avg:118.79ms
step:2215/2245 train_time:263111ms step_avg:118.79ms
step:2216/2245 train_time:263235ms step_avg:118.79ms
step:2217/2245 train_time:263351ms step_avg:118.79ms
step:2218/2245 train_time:263474ms step_avg:118.79ms
step:2219/2245 train_time:263591ms step_avg:118.79ms
step:2220/2245 train_time:263715ms step_avg:118.79ms
step:2221/2245 train_time:263831ms step_avg:118.79ms
step:2222/2245 train_time:263954ms step_avg:118.79ms
step:2223/2245 train_time:264071ms step_avg:118.79ms
step:2224/2245 train_time:264194ms step_avg:118.79ms
step:2225/2245 train_time:264310ms step_avg:118.79ms
step:2226/2245 train_time:264435ms step_avg:118.79ms
step:2227/2245 train_time:264551ms step_avg:118.79ms
step:2228/2245 train_time:264675ms step_avg:118.79ms
step:2229/2245 train_time:264792ms step_avg:118.79ms
step:2230/2245 train_time:264915ms step_avg:118.80ms
step:2231/2245 train_time:265032ms step_avg:118.79ms
step:2232/2245 train_time:265155ms step_avg:118.80ms
step:2233/2245 train_time:265272ms step_avg:118.80ms
step:2234/2245 train_time:265394ms step_avg:118.80ms
step:2235/2245 train_time:265511ms step_avg:118.80ms
step:2236/2245 train_time:265634ms step_avg:118.80ms
step:2237/2245 train_time:265751ms step_avg:118.80ms
step:2238/2245 train_time:265874ms step_avg:118.80ms
step:2239/2245 train_time:265991ms step_avg:118.80ms
step:2240/2245 train_time:266115ms step_avg:118.80ms
step:2241/2245 train_time:266231ms step_avg:118.80ms
step:2242/2245 train_time:266354ms step_avg:118.80ms
step:2243/2245 train_time:266471ms step_avg:118.80ms
step:2244/2245 train_time:266594ms step_avg:118.80ms
step:2245/2245 train_time:266711ms step_avg:118.80ms
step:2245/2245 val_loss:3.2814 train_time:266779ms step_avg:118.83ms
peak memory allocated: 29050 MiB reserved: 44436 MiB

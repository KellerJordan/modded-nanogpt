import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 09:59:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   42C    P0            116W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   42C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   42C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3339287      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3339288      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3339289      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3339290      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3339288      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3339289      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3339290      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:163ms step_avg:163.12ms
step:2/2245 train_time:217ms step_avg:108.59ms
step:3/2245 train_time:326ms step_avg:108.67ms
step:4/2245 train_time:444ms step_avg:111.06ms
step:5/2245 train_time:560ms step_avg:111.98ms
step:6/2245 train_time:683ms step_avg:113.76ms
step:7/2245 train_time:799ms step_avg:114.09ms
step:8/2245 train_time:921ms step_avg:115.15ms
step:9/2245 train_time:1038ms step_avg:115.32ms
step:10/2245 train_time:1161ms step_avg:116.15ms
step:11/2245 train_time:1278ms step_avg:116.21ms
step:12/2245 train_time:1401ms step_avg:116.79ms
step:13/2245 train_time:1519ms step_avg:116.82ms
step:14/2245 train_time:1642ms step_avg:117.30ms
step:15/2245 train_time:1759ms step_avg:117.26ms
step:16/2245 train_time:1882ms step_avg:117.60ms
step:17/2245 train_time:1998ms step_avg:117.54ms
step:18/2245 train_time:2121ms step_avg:117.85ms
step:19/2245 train_time:2238ms step_avg:117.79ms
step:20/2245 train_time:2361ms step_avg:118.04ms
step:21/2245 train_time:2477ms step_avg:117.97ms
step:22/2245 train_time:2601ms step_avg:118.21ms
step:23/2245 train_time:2718ms step_avg:118.15ms
step:24/2245 train_time:2841ms step_avg:118.37ms
step:25/2245 train_time:2957ms step_avg:118.30ms
step:26/2245 train_time:3080ms step_avg:118.46ms
step:27/2245 train_time:3197ms step_avg:118.40ms
step:28/2245 train_time:3320ms step_avg:118.56ms
step:29/2245 train_time:3436ms step_avg:118.50ms
step:30/2245 train_time:3559ms step_avg:118.62ms
step:31/2245 train_time:3676ms step_avg:118.57ms
step:32/2245 train_time:3799ms step_avg:118.71ms
step:33/2245 train_time:3916ms step_avg:118.66ms
step:34/2245 train_time:4039ms step_avg:118.79ms
step:35/2245 train_time:4155ms step_avg:118.71ms
step:36/2245 train_time:4277ms step_avg:118.81ms
step:37/2245 train_time:4394ms step_avg:118.75ms
step:38/2245 train_time:4516ms step_avg:118.84ms
step:39/2245 train_time:4631ms step_avg:118.75ms
step:40/2245 train_time:4753ms step_avg:118.84ms
step:41/2245 train_time:4869ms step_avg:118.75ms
step:42/2245 train_time:4991ms step_avg:118.83ms
step:43/2245 train_time:5106ms step_avg:118.75ms
step:44/2245 train_time:5229ms step_avg:118.83ms
step:45/2245 train_time:5344ms step_avg:118.76ms
step:46/2245 train_time:5466ms step_avg:118.82ms
step:47/2245 train_time:5581ms step_avg:118.75ms
step:48/2245 train_time:5703ms step_avg:118.81ms
step:49/2245 train_time:5818ms step_avg:118.74ms
step:50/2245 train_time:5940ms step_avg:118.80ms
step:51/2245 train_time:6056ms step_avg:118.74ms
step:52/2245 train_time:6178ms step_avg:118.81ms
step:53/2245 train_time:6293ms step_avg:118.74ms
step:54/2245 train_time:6415ms step_avg:118.80ms
step:55/2245 train_time:6531ms step_avg:118.74ms
step:56/2245 train_time:6653ms step_avg:118.80ms
step:57/2245 train_time:6768ms step_avg:118.74ms
step:58/2245 train_time:6890ms step_avg:118.80ms
step:59/2245 train_time:7006ms step_avg:118.74ms
step:60/2245 train_time:7128ms step_avg:118.80ms
step:61/2245 train_time:7243ms step_avg:118.74ms
step:62/2245 train_time:7365ms step_avg:118.79ms
step:63/2245 train_time:7480ms step_avg:118.73ms
step:64/2245 train_time:7602ms step_avg:118.77ms
step:65/2245 train_time:7717ms step_avg:118.72ms
step:66/2245 train_time:7839ms step_avg:118.77ms
step:67/2245 train_time:7955ms step_avg:118.72ms
step:68/2245 train_time:8076ms step_avg:118.76ms
step:69/2245 train_time:8191ms step_avg:118.71ms
step:70/2245 train_time:8313ms step_avg:118.75ms
step:71/2245 train_time:8428ms step_avg:118.70ms
step:72/2245 train_time:8549ms step_avg:118.74ms
step:73/2245 train_time:8665ms step_avg:118.70ms
step:74/2245 train_time:8786ms step_avg:118.73ms
step:75/2245 train_time:8901ms step_avg:118.69ms
step:76/2245 train_time:9023ms step_avg:118.72ms
step:77/2245 train_time:9138ms step_avg:118.68ms
step:78/2245 train_time:9260ms step_avg:118.72ms
step:79/2245 train_time:9375ms step_avg:118.67ms
step:80/2245 train_time:9496ms step_avg:118.70ms
step:81/2245 train_time:9611ms step_avg:118.66ms
step:82/2245 train_time:9733ms step_avg:118.69ms
step:83/2245 train_time:9848ms step_avg:118.65ms
step:84/2245 train_time:9969ms step_avg:118.68ms
step:85/2245 train_time:10085ms step_avg:118.65ms
step:86/2245 train_time:10206ms step_avg:118.68ms
step:87/2245 train_time:10321ms step_avg:118.64ms
step:88/2245 train_time:10443ms step_avg:118.67ms
step:89/2245 train_time:10558ms step_avg:118.63ms
step:90/2245 train_time:10679ms step_avg:118.65ms
step:91/2245 train_time:10794ms step_avg:118.61ms
step:92/2245 train_time:10915ms step_avg:118.64ms
step:93/2245 train_time:11030ms step_avg:118.60ms
step:94/2245 train_time:11151ms step_avg:118.63ms
step:95/2245 train_time:11266ms step_avg:118.59ms
step:96/2245 train_time:11388ms step_avg:118.62ms
step:97/2245 train_time:11502ms step_avg:118.58ms
step:98/2245 train_time:11624ms step_avg:118.61ms
step:99/2245 train_time:11739ms step_avg:118.57ms
step:100/2245 train_time:11860ms step_avg:118.60ms
step:101/2245 train_time:11975ms step_avg:118.57ms
step:102/2245 train_time:12096ms step_avg:118.59ms
step:103/2245 train_time:12211ms step_avg:118.55ms
step:104/2245 train_time:12332ms step_avg:118.58ms
step:105/2245 train_time:12447ms step_avg:118.54ms
step:106/2245 train_time:12568ms step_avg:118.57ms
step:107/2245 train_time:12683ms step_avg:118.53ms
step:108/2245 train_time:12804ms step_avg:118.56ms
step:109/2245 train_time:12919ms step_avg:118.52ms
step:110/2245 train_time:13040ms step_avg:118.55ms
step:111/2245 train_time:13155ms step_avg:118.51ms
step:112/2245 train_time:13276ms step_avg:118.54ms
step:113/2245 train_time:13390ms step_avg:118.50ms
step:114/2245 train_time:13511ms step_avg:118.52ms
step:115/2245 train_time:13626ms step_avg:118.49ms
step:116/2245 train_time:13747ms step_avg:118.51ms
step:117/2245 train_time:13862ms step_avg:118.48ms
step:118/2245 train_time:13983ms step_avg:118.50ms
step:119/2245 train_time:14098ms step_avg:118.47ms
step:120/2245 train_time:14219ms step_avg:118.49ms
step:121/2245 train_time:14334ms step_avg:118.46ms
step:122/2245 train_time:14455ms step_avg:118.48ms
step:123/2245 train_time:14569ms step_avg:118.45ms
step:124/2245 train_time:14690ms step_avg:118.47ms
step:125/2245 train_time:14805ms step_avg:118.44ms
step:126/2245 train_time:14926ms step_avg:118.46ms
step:127/2245 train_time:15041ms step_avg:118.43ms
step:128/2245 train_time:15162ms step_avg:118.45ms
step:129/2245 train_time:15276ms step_avg:118.42ms
step:130/2245 train_time:15397ms step_avg:118.44ms
step:131/2245 train_time:15512ms step_avg:118.41ms
step:132/2245 train_time:15632ms step_avg:118.43ms
step:133/2245 train_time:15747ms step_avg:118.40ms
step:134/2245 train_time:15868ms step_avg:118.42ms
step:135/2245 train_time:15983ms step_avg:118.39ms
step:136/2245 train_time:16104ms step_avg:118.41ms
step:137/2245 train_time:16219ms step_avg:118.38ms
step:138/2245 train_time:16339ms step_avg:118.40ms
step:139/2245 train_time:16454ms step_avg:118.37ms
step:140/2245 train_time:16575ms step_avg:118.39ms
step:141/2245 train_time:16689ms step_avg:118.36ms
step:142/2245 train_time:16810ms step_avg:118.38ms
step:143/2245 train_time:16925ms step_avg:118.35ms
step:144/2245 train_time:17045ms step_avg:118.37ms
step:145/2245 train_time:17160ms step_avg:118.34ms
step:146/2245 train_time:17281ms step_avg:118.36ms
step:147/2245 train_time:17396ms step_avg:118.34ms
step:148/2245 train_time:17516ms step_avg:118.35ms
step:149/2245 train_time:17631ms step_avg:118.33ms
step:150/2245 train_time:17752ms step_avg:118.34ms
step:151/2245 train_time:17866ms step_avg:118.32ms
step:152/2245 train_time:17987ms step_avg:118.34ms
step:153/2245 train_time:18101ms step_avg:118.31ms
step:154/2245 train_time:18222ms step_avg:118.32ms
step:155/2245 train_time:18337ms step_avg:118.30ms
step:156/2245 train_time:18458ms step_avg:118.32ms
step:157/2245 train_time:18572ms step_avg:118.29ms
step:158/2245 train_time:18693ms step_avg:118.31ms
step:159/2245 train_time:18807ms step_avg:118.28ms
step:160/2245 train_time:18928ms step_avg:118.30ms
step:161/2245 train_time:19043ms step_avg:118.28ms
step:162/2245 train_time:19163ms step_avg:118.29ms
step:163/2245 train_time:19278ms step_avg:118.27ms
step:164/2245 train_time:19398ms step_avg:118.28ms
step:165/2245 train_time:19513ms step_avg:118.26ms
step:166/2245 train_time:19633ms step_avg:118.27ms
step:167/2245 train_time:19748ms step_avg:118.25ms
step:168/2245 train_time:19869ms step_avg:118.27ms
step:169/2245 train_time:19983ms step_avg:118.24ms
step:170/2245 train_time:20104ms step_avg:118.26ms
step:171/2245 train_time:20218ms step_avg:118.23ms
step:172/2245 train_time:20339ms step_avg:118.25ms
step:173/2245 train_time:20454ms step_avg:118.23ms
step:174/2245 train_time:20574ms step_avg:118.24ms
step:175/2245 train_time:20688ms step_avg:118.22ms
step:176/2245 train_time:20809ms step_avg:118.23ms
step:177/2245 train_time:20923ms step_avg:118.21ms
step:178/2245 train_time:21044ms step_avg:118.23ms
step:179/2245 train_time:21158ms step_avg:118.20ms
step:180/2245 train_time:21279ms step_avg:118.22ms
step:181/2245 train_time:21393ms step_avg:118.19ms
step:182/2245 train_time:21514ms step_avg:118.21ms
step:183/2245 train_time:21628ms step_avg:118.19ms
step:184/2245 train_time:21749ms step_avg:118.20ms
step:185/2245 train_time:21863ms step_avg:118.18ms
step:186/2245 train_time:21984ms step_avg:118.19ms
step:187/2245 train_time:22098ms step_avg:118.17ms
step:188/2245 train_time:22218ms step_avg:118.18ms
step:189/2245 train_time:22333ms step_avg:118.16ms
step:190/2245 train_time:22454ms step_avg:118.18ms
step:191/2245 train_time:22568ms step_avg:118.16ms
step:192/2245 train_time:22688ms step_avg:118.17ms
step:193/2245 train_time:22803ms step_avg:118.15ms
step:194/2245 train_time:22923ms step_avg:118.16ms
step:195/2245 train_time:23037ms step_avg:118.14ms
step:196/2245 train_time:23158ms step_avg:118.15ms
step:197/2245 train_time:23272ms step_avg:118.13ms
step:198/2245 train_time:23392ms step_avg:118.14ms
step:199/2245 train_time:23507ms step_avg:118.12ms
step:200/2245 train_time:23627ms step_avg:118.14ms
step:201/2245 train_time:23742ms step_avg:118.12ms
step:202/2245 train_time:23862ms step_avg:118.13ms
step:203/2245 train_time:23976ms step_avg:118.11ms
step:204/2245 train_time:24097ms step_avg:118.12ms
step:205/2245 train_time:24210ms step_avg:118.10ms
step:206/2245 train_time:24331ms step_avg:118.11ms
step:207/2245 train_time:24445ms step_avg:118.09ms
step:208/2245 train_time:24566ms step_avg:118.11ms
step:209/2245 train_time:24680ms step_avg:118.09ms
step:210/2245 train_time:24801ms step_avg:118.10ms
step:211/2245 train_time:24915ms step_avg:118.08ms
step:212/2245 train_time:25036ms step_avg:118.09ms
step:213/2245 train_time:25150ms step_avg:118.08ms
step:214/2245 train_time:25270ms step_avg:118.09ms
step:215/2245 train_time:25385ms step_avg:118.07ms
step:216/2245 train_time:25505ms step_avg:118.08ms
step:217/2245 train_time:25619ms step_avg:118.06ms
step:218/2245 train_time:25740ms step_avg:118.07ms
step:219/2245 train_time:25854ms step_avg:118.06ms
step:220/2245 train_time:25975ms step_avg:118.07ms
step:221/2245 train_time:26089ms step_avg:118.05ms
step:222/2245 train_time:26210ms step_avg:118.06ms
step:223/2245 train_time:26324ms step_avg:118.04ms
step:224/2245 train_time:26444ms step_avg:118.06ms
step:225/2245 train_time:26559ms step_avg:118.04ms
step:226/2245 train_time:26679ms step_avg:118.05ms
step:227/2245 train_time:26794ms step_avg:118.03ms
step:228/2245 train_time:26914ms step_avg:118.04ms
step:229/2245 train_time:27028ms step_avg:118.03ms
step:230/2245 train_time:27149ms step_avg:118.04ms
step:231/2245 train_time:27263ms step_avg:118.02ms
step:232/2245 train_time:27383ms step_avg:118.03ms
step:233/2245 train_time:27498ms step_avg:118.02ms
step:234/2245 train_time:27618ms step_avg:118.03ms
step:235/2245 train_time:27732ms step_avg:118.01ms
step:236/2245 train_time:27853ms step_avg:118.02ms
step:237/2245 train_time:27967ms step_avg:118.00ms
step:238/2245 train_time:28087ms step_avg:118.01ms
step:239/2245 train_time:28201ms step_avg:118.00ms
step:240/2245 train_time:28322ms step_avg:118.01ms
step:241/2245 train_time:28437ms step_avg:117.99ms
step:242/2245 train_time:28557ms step_avg:118.00ms
step:243/2245 train_time:28670ms step_avg:117.99ms
step:244/2245 train_time:28791ms step_avg:118.00ms
step:245/2245 train_time:28906ms step_avg:117.98ms
step:246/2245 train_time:29026ms step_avg:117.99ms
step:247/2245 train_time:29141ms step_avg:117.98ms
step:248/2245 train_time:29261ms step_avg:117.99ms
step:249/2245 train_time:29375ms step_avg:117.97ms
step:250/2245 train_time:29495ms step_avg:117.98ms
step:250/2245 val_loss:4.1164 train_time:29561ms step_avg:118.24ms
step:251/2245 train_time:29611ms step_avg:117.97ms
step:252/2245 train_time:29731ms step_avg:117.98ms
step:253/2245 train_time:29845ms step_avg:117.96ms
step:254/2245 train_time:29965ms step_avg:117.97ms
step:255/2245 train_time:30080ms step_avg:117.96ms
step:256/2245 train_time:30201ms step_avg:117.97ms
step:257/2245 train_time:30314ms step_avg:117.96ms
step:258/2245 train_time:30435ms step_avg:117.96ms
step:259/2245 train_time:30549ms step_avg:117.95ms
step:260/2245 train_time:30669ms step_avg:117.96ms
step:261/2245 train_time:30783ms step_avg:117.94ms
step:262/2245 train_time:30904ms step_avg:117.95ms
step:263/2245 train_time:31018ms step_avg:117.94ms
step:264/2245 train_time:31139ms step_avg:117.95ms
step:265/2245 train_time:31253ms step_avg:117.94ms
step:266/2245 train_time:31374ms step_avg:117.95ms
step:267/2245 train_time:31488ms step_avg:117.93ms
step:268/2245 train_time:31608ms step_avg:117.94ms
step:269/2245 train_time:31722ms step_avg:117.93ms
step:270/2245 train_time:31843ms step_avg:117.94ms
step:271/2245 train_time:31957ms step_avg:117.92ms
step:272/2245 train_time:32078ms step_avg:117.93ms
step:273/2245 train_time:32192ms step_avg:117.92ms
step:274/2245 train_time:32312ms step_avg:117.93ms
step:275/2245 train_time:32426ms step_avg:117.91ms
step:276/2245 train_time:32547ms step_avg:117.92ms
step:277/2245 train_time:32661ms step_avg:117.91ms
step:278/2245 train_time:32781ms step_avg:117.92ms
step:279/2245 train_time:32896ms step_avg:117.91ms
step:280/2245 train_time:33016ms step_avg:117.92ms
step:281/2245 train_time:33130ms step_avg:117.90ms
step:282/2245 train_time:33250ms step_avg:117.91ms
step:283/2245 train_time:33364ms step_avg:117.89ms
step:284/2245 train_time:33484ms step_avg:117.90ms
step:285/2245 train_time:33598ms step_avg:117.89ms
step:286/2245 train_time:33719ms step_avg:117.90ms
step:287/2245 train_time:33833ms step_avg:117.88ms
step:288/2245 train_time:33953ms step_avg:117.89ms
step:289/2245 train_time:34067ms step_avg:117.88ms
step:290/2245 train_time:34187ms step_avg:117.89ms
step:291/2245 train_time:34301ms step_avg:117.87ms
step:292/2245 train_time:34422ms step_avg:117.88ms
step:293/2245 train_time:34536ms step_avg:117.87ms
step:294/2245 train_time:34656ms step_avg:117.88ms
step:295/2245 train_time:34771ms step_avg:117.87ms
step:296/2245 train_time:34891ms step_avg:117.87ms
step:297/2245 train_time:35005ms step_avg:117.86ms
step:298/2245 train_time:35125ms step_avg:117.87ms
step:299/2245 train_time:35239ms step_avg:117.86ms
step:300/2245 train_time:35359ms step_avg:117.86ms
step:301/2245 train_time:35474ms step_avg:117.85ms
step:302/2245 train_time:35594ms step_avg:117.86ms
step:303/2245 train_time:35708ms step_avg:117.85ms
step:304/2245 train_time:35828ms step_avg:117.86ms
step:305/2245 train_time:35942ms step_avg:117.84ms
step:306/2245 train_time:36063ms step_avg:117.85ms
step:307/2245 train_time:36177ms step_avg:117.84ms
step:308/2245 train_time:36297ms step_avg:117.85ms
step:309/2245 train_time:36411ms step_avg:117.83ms
step:310/2245 train_time:36531ms step_avg:117.84ms
step:311/2245 train_time:36645ms step_avg:117.83ms
step:312/2245 train_time:36766ms step_avg:117.84ms
step:313/2245 train_time:36880ms step_avg:117.83ms
step:314/2245 train_time:37000ms step_avg:117.83ms
step:315/2245 train_time:37114ms step_avg:117.82ms
step:316/2245 train_time:37234ms step_avg:117.83ms
step:317/2245 train_time:37348ms step_avg:117.82ms
step:318/2245 train_time:37468ms step_avg:117.82ms
step:319/2245 train_time:37582ms step_avg:117.81ms
step:320/2245 train_time:37702ms step_avg:117.82ms
step:321/2245 train_time:37816ms step_avg:117.81ms
step:322/2245 train_time:37936ms step_avg:117.81ms
step:323/2245 train_time:38051ms step_avg:117.80ms
step:324/2245 train_time:38171ms step_avg:117.81ms
step:325/2245 train_time:38285ms step_avg:117.80ms
step:326/2245 train_time:38405ms step_avg:117.81ms
step:327/2245 train_time:38519ms step_avg:117.80ms
step:328/2245 train_time:38640ms step_avg:117.80ms
step:329/2245 train_time:38754ms step_avg:117.79ms
step:330/2245 train_time:38874ms step_avg:117.80ms
step:331/2245 train_time:38988ms step_avg:117.79ms
step:332/2245 train_time:39108ms step_avg:117.80ms
step:333/2245 train_time:39222ms step_avg:117.78ms
step:334/2245 train_time:39343ms step_avg:117.79ms
step:335/2245 train_time:39458ms step_avg:117.78ms
step:336/2245 train_time:39578ms step_avg:117.79ms
step:337/2245 train_time:39691ms step_avg:117.78ms
step:338/2245 train_time:39812ms step_avg:117.79ms
step:339/2245 train_time:39926ms step_avg:117.77ms
step:340/2245 train_time:40046ms step_avg:117.78ms
step:341/2245 train_time:40160ms step_avg:117.77ms
step:342/2245 train_time:40280ms step_avg:117.78ms
step:343/2245 train_time:40395ms step_avg:117.77ms
step:344/2245 train_time:40515ms step_avg:117.78ms
step:345/2245 train_time:40629ms step_avg:117.77ms
step:346/2245 train_time:40749ms step_avg:117.77ms
step:347/2245 train_time:40863ms step_avg:117.76ms
step:348/2245 train_time:40984ms step_avg:117.77ms
step:349/2245 train_time:41098ms step_avg:117.76ms
step:350/2245 train_time:41218ms step_avg:117.77ms
step:351/2245 train_time:41332ms step_avg:117.76ms
step:352/2245 train_time:41452ms step_avg:117.76ms
step:353/2245 train_time:41566ms step_avg:117.75ms
step:354/2245 train_time:41687ms step_avg:117.76ms
step:355/2245 train_time:41801ms step_avg:117.75ms
step:356/2245 train_time:41921ms step_avg:117.76ms
step:357/2245 train_time:42036ms step_avg:117.75ms
step:358/2245 train_time:42156ms step_avg:117.75ms
step:359/2245 train_time:42271ms step_avg:117.75ms
step:360/2245 train_time:42391ms step_avg:117.75ms
step:361/2245 train_time:42505ms step_avg:117.74ms
step:362/2245 train_time:42625ms step_avg:117.75ms
step:363/2245 train_time:42739ms step_avg:117.74ms
step:364/2245 train_time:42859ms step_avg:117.75ms
step:365/2245 train_time:42974ms step_avg:117.74ms
step:366/2245 train_time:43094ms step_avg:117.74ms
step:367/2245 train_time:43208ms step_avg:117.73ms
step:368/2245 train_time:43328ms step_avg:117.74ms
step:369/2245 train_time:43442ms step_avg:117.73ms
step:370/2245 train_time:43563ms step_avg:117.74ms
step:371/2245 train_time:43677ms step_avg:117.73ms
step:372/2245 train_time:43797ms step_avg:117.73ms
step:373/2245 train_time:43912ms step_avg:117.73ms
step:374/2245 train_time:44032ms step_avg:117.73ms
step:375/2245 train_time:44145ms step_avg:117.72ms
step:376/2245 train_time:44266ms step_avg:117.73ms
step:377/2245 train_time:44380ms step_avg:117.72ms
step:378/2245 train_time:44500ms step_avg:117.73ms
step:379/2245 train_time:44615ms step_avg:117.72ms
step:380/2245 train_time:44735ms step_avg:117.72ms
step:381/2245 train_time:44849ms step_avg:117.71ms
step:382/2245 train_time:44969ms step_avg:117.72ms
step:383/2245 train_time:45083ms step_avg:117.71ms
step:384/2245 train_time:45204ms step_avg:117.72ms
step:385/2245 train_time:45317ms step_avg:117.71ms
step:386/2245 train_time:45438ms step_avg:117.71ms
step:387/2245 train_time:45552ms step_avg:117.71ms
step:388/2245 train_time:45672ms step_avg:117.71ms
step:389/2245 train_time:45786ms step_avg:117.70ms
step:390/2245 train_time:45907ms step_avg:117.71ms
step:391/2245 train_time:46021ms step_avg:117.70ms
step:392/2245 train_time:46141ms step_avg:117.71ms
step:393/2245 train_time:46255ms step_avg:117.70ms
step:394/2245 train_time:46375ms step_avg:117.70ms
step:395/2245 train_time:46489ms step_avg:117.69ms
step:396/2245 train_time:46609ms step_avg:117.70ms
step:397/2245 train_time:46724ms step_avg:117.69ms
step:398/2245 train_time:46844ms step_avg:117.70ms
step:399/2245 train_time:46959ms step_avg:117.69ms
step:400/2245 train_time:47079ms step_avg:117.70ms
step:401/2245 train_time:47193ms step_avg:117.69ms
step:402/2245 train_time:47313ms step_avg:117.69ms
step:403/2245 train_time:47427ms step_avg:117.68ms
step:404/2245 train_time:47547ms step_avg:117.69ms
step:405/2245 train_time:47662ms step_avg:117.68ms
step:406/2245 train_time:47782ms step_avg:117.69ms
step:407/2245 train_time:47897ms step_avg:117.68ms
step:408/2245 train_time:48017ms step_avg:117.69ms
step:409/2245 train_time:48132ms step_avg:117.68ms
step:410/2245 train_time:48252ms step_avg:117.69ms
step:411/2245 train_time:48366ms step_avg:117.68ms
step:412/2245 train_time:48487ms step_avg:117.69ms
step:413/2245 train_time:48601ms step_avg:117.68ms
step:414/2245 train_time:48721ms step_avg:117.68ms
step:415/2245 train_time:48835ms step_avg:117.67ms
step:416/2245 train_time:48955ms step_avg:117.68ms
step:417/2245 train_time:49069ms step_avg:117.67ms
step:418/2245 train_time:49189ms step_avg:117.68ms
step:419/2245 train_time:49304ms step_avg:117.67ms
step:420/2245 train_time:49424ms step_avg:117.68ms
step:421/2245 train_time:49538ms step_avg:117.67ms
step:422/2245 train_time:49658ms step_avg:117.67ms
step:423/2245 train_time:49772ms step_avg:117.67ms
step:424/2245 train_time:49893ms step_avg:117.67ms
step:425/2245 train_time:50007ms step_avg:117.66ms
step:426/2245 train_time:50127ms step_avg:117.67ms
step:427/2245 train_time:50242ms step_avg:117.66ms
step:428/2245 train_time:50362ms step_avg:117.67ms
step:429/2245 train_time:50476ms step_avg:117.66ms
step:430/2245 train_time:50596ms step_avg:117.67ms
step:431/2245 train_time:50710ms step_avg:117.66ms
step:432/2245 train_time:50830ms step_avg:117.66ms
step:433/2245 train_time:50945ms step_avg:117.66ms
step:434/2245 train_time:51065ms step_avg:117.66ms
step:435/2245 train_time:51178ms step_avg:117.65ms
step:436/2245 train_time:51299ms step_avg:117.66ms
step:437/2245 train_time:51413ms step_avg:117.65ms
step:438/2245 train_time:51533ms step_avg:117.66ms
step:439/2245 train_time:51647ms step_avg:117.65ms
step:440/2245 train_time:51767ms step_avg:117.65ms
step:441/2245 train_time:51882ms step_avg:117.65ms
step:442/2245 train_time:52002ms step_avg:117.65ms
step:443/2245 train_time:52116ms step_avg:117.64ms
step:444/2245 train_time:52236ms step_avg:117.65ms
step:445/2245 train_time:52350ms step_avg:117.64ms
step:446/2245 train_time:52470ms step_avg:117.65ms
step:447/2245 train_time:52585ms step_avg:117.64ms
step:448/2245 train_time:52705ms step_avg:117.64ms
step:449/2245 train_time:52819ms step_avg:117.64ms
step:450/2245 train_time:52939ms step_avg:117.64ms
step:451/2245 train_time:53053ms step_avg:117.63ms
step:452/2245 train_time:53173ms step_avg:117.64ms
step:453/2245 train_time:53288ms step_avg:117.63ms
step:454/2245 train_time:53408ms step_avg:117.64ms
step:455/2245 train_time:53522ms step_avg:117.63ms
step:456/2245 train_time:53642ms step_avg:117.64ms
step:457/2245 train_time:53756ms step_avg:117.63ms
step:458/2245 train_time:53877ms step_avg:117.63ms
step:459/2245 train_time:53991ms step_avg:117.63ms
step:460/2245 train_time:54111ms step_avg:117.63ms
step:461/2245 train_time:54225ms step_avg:117.62ms
step:462/2245 train_time:54345ms step_avg:117.63ms
step:463/2245 train_time:54460ms step_avg:117.62ms
step:464/2245 train_time:54580ms step_avg:117.63ms
step:465/2245 train_time:54694ms step_avg:117.62ms
step:466/2245 train_time:54815ms step_avg:117.63ms
step:467/2245 train_time:54929ms step_avg:117.62ms
step:468/2245 train_time:55050ms step_avg:117.63ms
step:469/2245 train_time:55164ms step_avg:117.62ms
step:470/2245 train_time:55284ms step_avg:117.63ms
step:471/2245 train_time:55398ms step_avg:117.62ms
step:472/2245 train_time:55518ms step_avg:117.62ms
step:473/2245 train_time:55632ms step_avg:117.62ms
step:474/2245 train_time:55753ms step_avg:117.62ms
step:475/2245 train_time:55867ms step_avg:117.61ms
step:476/2245 train_time:55987ms step_avg:117.62ms
step:477/2245 train_time:56101ms step_avg:117.61ms
step:478/2245 train_time:56221ms step_avg:117.62ms
step:479/2245 train_time:56336ms step_avg:117.61ms
step:480/2245 train_time:56456ms step_avg:117.62ms
step:481/2245 train_time:56570ms step_avg:117.61ms
step:482/2245 train_time:56690ms step_avg:117.61ms
step:483/2245 train_time:56804ms step_avg:117.61ms
step:484/2245 train_time:56924ms step_avg:117.61ms
step:485/2245 train_time:57038ms step_avg:117.60ms
step:486/2245 train_time:57158ms step_avg:117.61ms
step:487/2245 train_time:57272ms step_avg:117.60ms
step:488/2245 train_time:57392ms step_avg:117.61ms
step:489/2245 train_time:57506ms step_avg:117.60ms
step:490/2245 train_time:57626ms step_avg:117.60ms
step:491/2245 train_time:57741ms step_avg:117.60ms
step:492/2245 train_time:57861ms step_avg:117.60ms
step:493/2245 train_time:57975ms step_avg:117.60ms
step:494/2245 train_time:58096ms step_avg:117.60ms
step:495/2245 train_time:58209ms step_avg:117.59ms
step:496/2245 train_time:58329ms step_avg:117.60ms
step:497/2245 train_time:58443ms step_avg:117.59ms
step:498/2245 train_time:58564ms step_avg:117.60ms
step:499/2245 train_time:58678ms step_avg:117.59ms
step:500/2245 train_time:58798ms step_avg:117.60ms
step:500/2245 val_loss:3.8372 train_time:58863ms step_avg:117.73ms
step:501/2245 train_time:58913ms step_avg:117.59ms
step:502/2245 train_time:59033ms step_avg:117.60ms
step:503/2245 train_time:59146ms step_avg:117.59ms
step:504/2245 train_time:59266ms step_avg:117.59ms
step:505/2245 train_time:59381ms step_avg:117.59ms
step:506/2245 train_time:59501ms step_avg:117.59ms
step:507/2245 train_time:59616ms step_avg:117.59ms
step:508/2245 train_time:59736ms step_avg:117.59ms
step:509/2245 train_time:59851ms step_avg:117.59ms
step:510/2245 train_time:59971ms step_avg:117.59ms
step:511/2245 train_time:60085ms step_avg:117.58ms
step:512/2245 train_time:60205ms step_avg:117.59ms
step:513/2245 train_time:60320ms step_avg:117.58ms
step:514/2245 train_time:60441ms step_avg:117.59ms
step:515/2245 train_time:60555ms step_avg:117.58ms
step:516/2245 train_time:60675ms step_avg:117.59ms
step:517/2245 train_time:60789ms step_avg:117.58ms
step:518/2245 train_time:60909ms step_avg:117.59ms
step:519/2245 train_time:61024ms step_avg:117.58ms
step:520/2245 train_time:61144ms step_avg:117.58ms
step:521/2245 train_time:61258ms step_avg:117.58ms
step:522/2245 train_time:61379ms step_avg:117.58ms
step:523/2245 train_time:61493ms step_avg:117.58ms
step:524/2245 train_time:61613ms step_avg:117.58ms
step:525/2245 train_time:61727ms step_avg:117.58ms
step:526/2245 train_time:61847ms step_avg:117.58ms
step:527/2245 train_time:61962ms step_avg:117.57ms
step:528/2245 train_time:62082ms step_avg:117.58ms
step:529/2245 train_time:62196ms step_avg:117.57ms
step:530/2245 train_time:62316ms step_avg:117.58ms
step:531/2245 train_time:62431ms step_avg:117.57ms
step:532/2245 train_time:62551ms step_avg:117.58ms
step:533/2245 train_time:62665ms step_avg:117.57ms
step:534/2245 train_time:62785ms step_avg:117.58ms
step:535/2245 train_time:62900ms step_avg:117.57ms
step:536/2245 train_time:63020ms step_avg:117.57ms
step:537/2245 train_time:63134ms step_avg:117.57ms
step:538/2245 train_time:63255ms step_avg:117.57ms
step:539/2245 train_time:63369ms step_avg:117.57ms
step:540/2245 train_time:63489ms step_avg:117.57ms
step:541/2245 train_time:63603ms step_avg:117.57ms
step:542/2245 train_time:63723ms step_avg:117.57ms
step:543/2245 train_time:63837ms step_avg:117.56ms
step:544/2245 train_time:63957ms step_avg:117.57ms
step:545/2245 train_time:64072ms step_avg:117.56ms
step:546/2245 train_time:64192ms step_avg:117.57ms
step:547/2245 train_time:64306ms step_avg:117.56ms
step:548/2245 train_time:64427ms step_avg:117.57ms
step:549/2245 train_time:64540ms step_avg:117.56ms
step:550/2245 train_time:64661ms step_avg:117.56ms
step:551/2245 train_time:64775ms step_avg:117.56ms
step:552/2245 train_time:64895ms step_avg:117.56ms
step:553/2245 train_time:65009ms step_avg:117.56ms
step:554/2245 train_time:65129ms step_avg:117.56ms
step:555/2245 train_time:65243ms step_avg:117.56ms
step:556/2245 train_time:65364ms step_avg:117.56ms
step:557/2245 train_time:65478ms step_avg:117.55ms
step:558/2245 train_time:65598ms step_avg:117.56ms
step:559/2245 train_time:65712ms step_avg:117.55ms
step:560/2245 train_time:65832ms step_avg:117.56ms
step:561/2245 train_time:65946ms step_avg:117.55ms
step:562/2245 train_time:66066ms step_avg:117.56ms
step:563/2245 train_time:66181ms step_avg:117.55ms
step:564/2245 train_time:66301ms step_avg:117.55ms
step:565/2245 train_time:66415ms step_avg:117.55ms
step:566/2245 train_time:66535ms step_avg:117.55ms
step:567/2245 train_time:66649ms step_avg:117.55ms
step:568/2245 train_time:66770ms step_avg:117.55ms
step:569/2245 train_time:66884ms step_avg:117.55ms
step:570/2245 train_time:67004ms step_avg:117.55ms
step:571/2245 train_time:67118ms step_avg:117.55ms
step:572/2245 train_time:67239ms step_avg:117.55ms
step:573/2245 train_time:67353ms step_avg:117.54ms
step:574/2245 train_time:67474ms step_avg:117.55ms
step:575/2245 train_time:67588ms step_avg:117.54ms
step:576/2245 train_time:67708ms step_avg:117.55ms
step:577/2245 train_time:67822ms step_avg:117.54ms
step:578/2245 train_time:67942ms step_avg:117.55ms
step:579/2245 train_time:68057ms step_avg:117.54ms
step:580/2245 train_time:68177ms step_avg:117.55ms
step:581/2245 train_time:68291ms step_avg:117.54ms
step:582/2245 train_time:68412ms step_avg:117.55ms
step:583/2245 train_time:68525ms step_avg:117.54ms
step:584/2245 train_time:68646ms step_avg:117.54ms
step:585/2245 train_time:68760ms step_avg:117.54ms
step:586/2245 train_time:68880ms step_avg:117.54ms
step:587/2245 train_time:68995ms step_avg:117.54ms
step:588/2245 train_time:69115ms step_avg:117.54ms
step:589/2245 train_time:69229ms step_avg:117.54ms
step:590/2245 train_time:69350ms step_avg:117.54ms
step:591/2245 train_time:69463ms step_avg:117.54ms
step:592/2245 train_time:69584ms step_avg:117.54ms
step:593/2245 train_time:69699ms step_avg:117.54ms
step:594/2245 train_time:69819ms step_avg:117.54ms
step:595/2245 train_time:69933ms step_avg:117.53ms
step:596/2245 train_time:70053ms step_avg:117.54ms
step:597/2245 train_time:70168ms step_avg:117.53ms
step:598/2245 train_time:70288ms step_avg:117.54ms
step:599/2245 train_time:70401ms step_avg:117.53ms
step:600/2245 train_time:70522ms step_avg:117.54ms
step:601/2245 train_time:70636ms step_avg:117.53ms
step:602/2245 train_time:70756ms step_avg:117.54ms
step:603/2245 train_time:70870ms step_avg:117.53ms
step:604/2245 train_time:70991ms step_avg:117.53ms
step:605/2245 train_time:71104ms step_avg:117.53ms
step:606/2245 train_time:71225ms step_avg:117.53ms
step:607/2245 train_time:71339ms step_avg:117.53ms
step:608/2245 train_time:71460ms step_avg:117.53ms
step:609/2245 train_time:71573ms step_avg:117.53ms
step:610/2245 train_time:71694ms step_avg:117.53ms
step:611/2245 train_time:71808ms step_avg:117.53ms
step:612/2245 train_time:71928ms step_avg:117.53ms
step:613/2245 train_time:72043ms step_avg:117.53ms
step:614/2245 train_time:72164ms step_avg:117.53ms
step:615/2245 train_time:72277ms step_avg:117.52ms
step:616/2245 train_time:72398ms step_avg:117.53ms
step:617/2245 train_time:72512ms step_avg:117.52ms
step:618/2245 train_time:72632ms step_avg:117.53ms
step:619/2245 train_time:72746ms step_avg:117.52ms
step:620/2245 train_time:72867ms step_avg:117.53ms
step:621/2245 train_time:72981ms step_avg:117.52ms
step:622/2245 train_time:73101ms step_avg:117.53ms
step:623/2245 train_time:73215ms step_avg:117.52ms
step:624/2245 train_time:73335ms step_avg:117.52ms
step:625/2245 train_time:73449ms step_avg:117.52ms
step:626/2245 train_time:73569ms step_avg:117.52ms
step:627/2245 train_time:73683ms step_avg:117.52ms
step:628/2245 train_time:73804ms step_avg:117.52ms
step:629/2245 train_time:73918ms step_avg:117.52ms
step:630/2245 train_time:74038ms step_avg:117.52ms
step:631/2245 train_time:74153ms step_avg:117.52ms
step:632/2245 train_time:74273ms step_avg:117.52ms
step:633/2245 train_time:74387ms step_avg:117.52ms
step:634/2245 train_time:74508ms step_avg:117.52ms
step:635/2245 train_time:74622ms step_avg:117.51ms
step:636/2245 train_time:74742ms step_avg:117.52ms
step:637/2245 train_time:74856ms step_avg:117.51ms
step:638/2245 train_time:74977ms step_avg:117.52ms
step:639/2245 train_time:75091ms step_avg:117.51ms
step:640/2245 train_time:75212ms step_avg:117.52ms
step:641/2245 train_time:75325ms step_avg:117.51ms
step:642/2245 train_time:75445ms step_avg:117.52ms
step:643/2245 train_time:75560ms step_avg:117.51ms
step:644/2245 train_time:75681ms step_avg:117.52ms
step:645/2245 train_time:75795ms step_avg:117.51ms
step:646/2245 train_time:75915ms step_avg:117.52ms
step:647/2245 train_time:76029ms step_avg:117.51ms
step:648/2245 train_time:76149ms step_avg:117.51ms
step:649/2245 train_time:76263ms step_avg:117.51ms
step:650/2245 train_time:76384ms step_avg:117.51ms
step:651/2245 train_time:76498ms step_avg:117.51ms
step:652/2245 train_time:76618ms step_avg:117.51ms
step:653/2245 train_time:76732ms step_avg:117.51ms
step:654/2245 train_time:76852ms step_avg:117.51ms
step:655/2245 train_time:76966ms step_avg:117.51ms
step:656/2245 train_time:77087ms step_avg:117.51ms
step:657/2245 train_time:77201ms step_avg:117.51ms
step:658/2245 train_time:77321ms step_avg:117.51ms
step:659/2245 train_time:77435ms step_avg:117.50ms
step:660/2245 train_time:77556ms step_avg:117.51ms
step:661/2245 train_time:77670ms step_avg:117.50ms
step:662/2245 train_time:77791ms step_avg:117.51ms
step:663/2245 train_time:77905ms step_avg:117.50ms
step:664/2245 train_time:78025ms step_avg:117.51ms
step:665/2245 train_time:78139ms step_avg:117.50ms
step:666/2245 train_time:78260ms step_avg:117.51ms
step:667/2245 train_time:78374ms step_avg:117.50ms
step:668/2245 train_time:78494ms step_avg:117.51ms
step:669/2245 train_time:78609ms step_avg:117.50ms
step:670/2245 train_time:78729ms step_avg:117.51ms
step:671/2245 train_time:78843ms step_avg:117.50ms
step:672/2245 train_time:78963ms step_avg:117.50ms
step:673/2245 train_time:79078ms step_avg:117.50ms
step:674/2245 train_time:79198ms step_avg:117.50ms
step:675/2245 train_time:79312ms step_avg:117.50ms
step:676/2245 train_time:79432ms step_avg:117.50ms
step:677/2245 train_time:79546ms step_avg:117.50ms
step:678/2245 train_time:79666ms step_avg:117.50ms
step:679/2245 train_time:79780ms step_avg:117.50ms
step:680/2245 train_time:79901ms step_avg:117.50ms
step:681/2245 train_time:80015ms step_avg:117.50ms
step:682/2245 train_time:80135ms step_avg:117.50ms
step:683/2245 train_time:80249ms step_avg:117.50ms
step:684/2245 train_time:80370ms step_avg:117.50ms
step:685/2245 train_time:80484ms step_avg:117.49ms
step:686/2245 train_time:80604ms step_avg:117.50ms
step:687/2245 train_time:80718ms step_avg:117.49ms
step:688/2245 train_time:80838ms step_avg:117.50ms
step:689/2245 train_time:80952ms step_avg:117.49ms
step:690/2245 train_time:81073ms step_avg:117.50ms
step:691/2245 train_time:81187ms step_avg:117.49ms
step:692/2245 train_time:81308ms step_avg:117.50ms
step:693/2245 train_time:81422ms step_avg:117.49ms
step:694/2245 train_time:81542ms step_avg:117.50ms
step:695/2245 train_time:81656ms step_avg:117.49ms
step:696/2245 train_time:81777ms step_avg:117.50ms
step:697/2245 train_time:81891ms step_avg:117.49ms
step:698/2245 train_time:82011ms step_avg:117.49ms
step:699/2245 train_time:82125ms step_avg:117.49ms
step:700/2245 train_time:82245ms step_avg:117.49ms
step:701/2245 train_time:82359ms step_avg:117.49ms
step:702/2245 train_time:82480ms step_avg:117.49ms
step:703/2245 train_time:82594ms step_avg:117.49ms
step:704/2245 train_time:82714ms step_avg:117.49ms
step:705/2245 train_time:82828ms step_avg:117.49ms
step:706/2245 train_time:82949ms step_avg:117.49ms
step:707/2245 train_time:83063ms step_avg:117.49ms
step:708/2245 train_time:83183ms step_avg:117.49ms
step:709/2245 train_time:83298ms step_avg:117.49ms
step:710/2245 train_time:83418ms step_avg:117.49ms
step:711/2245 train_time:83532ms step_avg:117.49ms
step:712/2245 train_time:83652ms step_avg:117.49ms
step:713/2245 train_time:83766ms step_avg:117.48ms
step:714/2245 train_time:83886ms step_avg:117.49ms
step:715/2245 train_time:84000ms step_avg:117.48ms
step:716/2245 train_time:84121ms step_avg:117.49ms
step:717/2245 train_time:84235ms step_avg:117.48ms
step:718/2245 train_time:84355ms step_avg:117.49ms
step:719/2245 train_time:84470ms step_avg:117.48ms
step:720/2245 train_time:84590ms step_avg:117.49ms
step:721/2245 train_time:84704ms step_avg:117.48ms
step:722/2245 train_time:84824ms step_avg:117.49ms
step:723/2245 train_time:84939ms step_avg:117.48ms
step:724/2245 train_time:85060ms step_avg:117.49ms
step:725/2245 train_time:85174ms step_avg:117.48ms
step:726/2245 train_time:85294ms step_avg:117.48ms
step:727/2245 train_time:85408ms step_avg:117.48ms
step:728/2245 train_time:85529ms step_avg:117.48ms
step:729/2245 train_time:85643ms step_avg:117.48ms
step:730/2245 train_time:85764ms step_avg:117.48ms
step:731/2245 train_time:85877ms step_avg:117.48ms
step:732/2245 train_time:85998ms step_avg:117.48ms
step:733/2245 train_time:86112ms step_avg:117.48ms
step:734/2245 train_time:86232ms step_avg:117.48ms
step:735/2245 train_time:86346ms step_avg:117.48ms
step:736/2245 train_time:86468ms step_avg:117.48ms
step:737/2245 train_time:86583ms step_avg:117.48ms
step:738/2245 train_time:86705ms step_avg:117.49ms
step:739/2245 train_time:86820ms step_avg:117.48ms
step:740/2245 train_time:86942ms step_avg:117.49ms
step:741/2245 train_time:87058ms step_avg:117.49ms
step:742/2245 train_time:87179ms step_avg:117.49ms
step:743/2245 train_time:87295ms step_avg:117.49ms
step:744/2245 train_time:87417ms step_avg:117.50ms
step:745/2245 train_time:87532ms step_avg:117.49ms
step:746/2245 train_time:87655ms step_avg:117.50ms
step:747/2245 train_time:87771ms step_avg:117.50ms
step:748/2245 train_time:87893ms step_avg:117.50ms
step:749/2245 train_time:88009ms step_avg:117.50ms
step:750/2245 train_time:88132ms step_avg:117.51ms
step:750/2245 val_loss:3.6765 train_time:88197ms step_avg:117.60ms
step:751/2245 train_time:88248ms step_avg:117.51ms
step:752/2245 train_time:88369ms step_avg:117.51ms
step:753/2245 train_time:88484ms step_avg:117.51ms
step:754/2245 train_time:88606ms step_avg:117.51ms
step:755/2245 train_time:88722ms step_avg:117.51ms
step:756/2245 train_time:88843ms step_avg:117.52ms
step:757/2245 train_time:88958ms step_avg:117.51ms
step:758/2245 train_time:89080ms step_avg:117.52ms
step:759/2245 train_time:89196ms step_avg:117.52ms
step:760/2245 train_time:89318ms step_avg:117.52ms
step:761/2245 train_time:89433ms step_avg:117.52ms
step:762/2245 train_time:89555ms step_avg:117.53ms
step:763/2245 train_time:89670ms step_avg:117.52ms
step:764/2245 train_time:89792ms step_avg:117.53ms
step:765/2245 train_time:89908ms step_avg:117.53ms
step:766/2245 train_time:90031ms step_avg:117.53ms
step:767/2245 train_time:90146ms step_avg:117.53ms
step:768/2245 train_time:90268ms step_avg:117.54ms
step:769/2245 train_time:90384ms step_avg:117.53ms
step:770/2245 train_time:90506ms step_avg:117.54ms
step:771/2245 train_time:90621ms step_avg:117.54ms
step:772/2245 train_time:90743ms step_avg:117.54ms
step:773/2245 train_time:90859ms step_avg:117.54ms
step:774/2245 train_time:90980ms step_avg:117.55ms
step:775/2245 train_time:91096ms step_avg:117.54ms
step:776/2245 train_time:91218ms step_avg:117.55ms
step:777/2245 train_time:91334ms step_avg:117.55ms
step:778/2245 train_time:91456ms step_avg:117.55ms
step:779/2245 train_time:91571ms step_avg:117.55ms
step:780/2245 train_time:91693ms step_avg:117.56ms
step:781/2245 train_time:91809ms step_avg:117.55ms
step:782/2245 train_time:91931ms step_avg:117.56ms
step:783/2245 train_time:92047ms step_avg:117.56ms
step:784/2245 train_time:92169ms step_avg:117.56ms
step:785/2245 train_time:92285ms step_avg:117.56ms
step:786/2245 train_time:92407ms step_avg:117.57ms
step:787/2245 train_time:92522ms step_avg:117.56ms
step:788/2245 train_time:92643ms step_avg:117.57ms
step:789/2245 train_time:92759ms step_avg:117.57ms
step:790/2245 train_time:92881ms step_avg:117.57ms
step:791/2245 train_time:92996ms step_avg:117.57ms
step:792/2245 train_time:93117ms step_avg:117.57ms
step:793/2245 train_time:93233ms step_avg:117.57ms
step:794/2245 train_time:93355ms step_avg:117.58ms
step:795/2245 train_time:93471ms step_avg:117.57ms
step:796/2245 train_time:93593ms step_avg:117.58ms
step:797/2245 train_time:93709ms step_avg:117.58ms
step:798/2245 train_time:93831ms step_avg:117.58ms
step:799/2245 train_time:93947ms step_avg:117.58ms
step:800/2245 train_time:94069ms step_avg:117.59ms
step:801/2245 train_time:94184ms step_avg:117.58ms
step:802/2245 train_time:94306ms step_avg:117.59ms
step:803/2245 train_time:94421ms step_avg:117.59ms
step:804/2245 train_time:94543ms step_avg:117.59ms
step:805/2245 train_time:94659ms step_avg:117.59ms
step:806/2245 train_time:94781ms step_avg:117.59ms
step:807/2245 train_time:94897ms step_avg:117.59ms
step:808/2245 train_time:95019ms step_avg:117.60ms
step:809/2245 train_time:95134ms step_avg:117.59ms
step:810/2245 train_time:95256ms step_avg:117.60ms
step:811/2245 train_time:95372ms step_avg:117.60ms
step:812/2245 train_time:95496ms step_avg:117.61ms
step:813/2245 train_time:95611ms step_avg:117.60ms
step:814/2245 train_time:95734ms step_avg:117.61ms
step:815/2245 train_time:95849ms step_avg:117.61ms
step:816/2245 train_time:95971ms step_avg:117.61ms
step:817/2245 train_time:96086ms step_avg:117.61ms
step:818/2245 train_time:96208ms step_avg:117.61ms
step:819/2245 train_time:96324ms step_avg:117.61ms
step:820/2245 train_time:96446ms step_avg:117.62ms
step:821/2245 train_time:96561ms step_avg:117.61ms
step:822/2245 train_time:96683ms step_avg:117.62ms
step:823/2245 train_time:96799ms step_avg:117.62ms
step:824/2245 train_time:96921ms step_avg:117.62ms
step:825/2245 train_time:97036ms step_avg:117.62ms
step:826/2245 train_time:97159ms step_avg:117.63ms
step:827/2245 train_time:97274ms step_avg:117.62ms
step:828/2245 train_time:97396ms step_avg:117.63ms
step:829/2245 train_time:97512ms step_avg:117.63ms
step:830/2245 train_time:97634ms step_avg:117.63ms
step:831/2245 train_time:97750ms step_avg:117.63ms
step:832/2245 train_time:97872ms step_avg:117.64ms
step:833/2245 train_time:97987ms step_avg:117.63ms
step:834/2245 train_time:98109ms step_avg:117.64ms
step:835/2245 train_time:98225ms step_avg:117.63ms
step:836/2245 train_time:98347ms step_avg:117.64ms
step:837/2245 train_time:98463ms step_avg:117.64ms
step:838/2245 train_time:98585ms step_avg:117.64ms
step:839/2245 train_time:98700ms step_avg:117.64ms
step:840/2245 train_time:98823ms step_avg:117.65ms
step:841/2245 train_time:98938ms step_avg:117.64ms
step:842/2245 train_time:99060ms step_avg:117.65ms
step:843/2245 train_time:99176ms step_avg:117.65ms
step:844/2245 train_time:99297ms step_avg:117.65ms
step:845/2245 train_time:99413ms step_avg:117.65ms
step:846/2245 train_time:99535ms step_avg:117.65ms
step:847/2245 train_time:99651ms step_avg:117.65ms
step:848/2245 train_time:99773ms step_avg:117.66ms
step:849/2245 train_time:99889ms step_avg:117.65ms
step:850/2245 train_time:100011ms step_avg:117.66ms
step:851/2245 train_time:100126ms step_avg:117.66ms
step:852/2245 train_time:100247ms step_avg:117.66ms
step:853/2245 train_time:100363ms step_avg:117.66ms
step:854/2245 train_time:100485ms step_avg:117.66ms
step:855/2245 train_time:100600ms step_avg:117.66ms
step:856/2245 train_time:100722ms step_avg:117.67ms
step:857/2245 train_time:100837ms step_avg:117.66ms
step:858/2245 train_time:100959ms step_avg:117.67ms
step:859/2245 train_time:101075ms step_avg:117.67ms
step:860/2245 train_time:101197ms step_avg:117.67ms
step:861/2245 train_time:101313ms step_avg:117.67ms
step:862/2245 train_time:101435ms step_avg:117.67ms
step:863/2245 train_time:101551ms step_avg:117.67ms
step:864/2245 train_time:101673ms step_avg:117.68ms
step:865/2245 train_time:101788ms step_avg:117.67ms
step:866/2245 train_time:101910ms step_avg:117.68ms
step:867/2245 train_time:102026ms step_avg:117.68ms
step:868/2245 train_time:102147ms step_avg:117.68ms
step:869/2245 train_time:102263ms step_avg:117.68ms
step:870/2245 train_time:102385ms step_avg:117.68ms
step:871/2245 train_time:102500ms step_avg:117.68ms
step:872/2245 train_time:102622ms step_avg:117.69ms
step:873/2245 train_time:102738ms step_avg:117.68ms
step:874/2245 train_time:102860ms step_avg:117.69ms
step:875/2245 train_time:102975ms step_avg:117.69ms
step:876/2245 train_time:103097ms step_avg:117.69ms
step:877/2245 train_time:103214ms step_avg:117.69ms
step:878/2245 train_time:103336ms step_avg:117.69ms
step:879/2245 train_time:103452ms step_avg:117.69ms
step:880/2245 train_time:103575ms step_avg:117.70ms
step:881/2245 train_time:103691ms step_avg:117.70ms
step:882/2245 train_time:103812ms step_avg:117.70ms
step:883/2245 train_time:103927ms step_avg:117.70ms
step:884/2245 train_time:104049ms step_avg:117.70ms
step:885/2245 train_time:104165ms step_avg:117.70ms
step:886/2245 train_time:104287ms step_avg:117.71ms
step:887/2245 train_time:104402ms step_avg:117.70ms
step:888/2245 train_time:104524ms step_avg:117.71ms
step:889/2245 train_time:104639ms step_avg:117.70ms
step:890/2245 train_time:104761ms step_avg:117.71ms
step:891/2245 train_time:104877ms step_avg:117.71ms
step:892/2245 train_time:104999ms step_avg:117.71ms
step:893/2245 train_time:105115ms step_avg:117.71ms
step:894/2245 train_time:105237ms step_avg:117.71ms
step:895/2245 train_time:105353ms step_avg:117.71ms
step:896/2245 train_time:105475ms step_avg:117.72ms
step:897/2245 train_time:105591ms step_avg:117.72ms
step:898/2245 train_time:105712ms step_avg:117.72ms
step:899/2245 train_time:105829ms step_avg:117.72ms
step:900/2245 train_time:105951ms step_avg:117.72ms
step:901/2245 train_time:106066ms step_avg:117.72ms
step:902/2245 train_time:106188ms step_avg:117.73ms
step:903/2245 train_time:106304ms step_avg:117.72ms
step:904/2245 train_time:106426ms step_avg:117.73ms
step:905/2245 train_time:106541ms step_avg:117.73ms
step:906/2245 train_time:106663ms step_avg:117.73ms
step:907/2245 train_time:106778ms step_avg:117.73ms
step:908/2245 train_time:106900ms step_avg:117.73ms
step:909/2245 train_time:107016ms step_avg:117.73ms
step:910/2245 train_time:107138ms step_avg:117.73ms
step:911/2245 train_time:107254ms step_avg:117.73ms
step:912/2245 train_time:107375ms step_avg:117.74ms
step:913/2245 train_time:107491ms step_avg:117.73ms
step:914/2245 train_time:107613ms step_avg:117.74ms
step:915/2245 train_time:107729ms step_avg:117.74ms
step:916/2245 train_time:107851ms step_avg:117.74ms
step:917/2245 train_time:107967ms step_avg:117.74ms
step:918/2245 train_time:108088ms step_avg:117.74ms
step:919/2245 train_time:108204ms step_avg:117.74ms
step:920/2245 train_time:108325ms step_avg:117.74ms
step:921/2245 train_time:108441ms step_avg:117.74ms
step:922/2245 train_time:108563ms step_avg:117.75ms
step:923/2245 train_time:108678ms step_avg:117.74ms
step:924/2245 train_time:108800ms step_avg:117.75ms
step:925/2245 train_time:108915ms step_avg:117.75ms
step:926/2245 train_time:109038ms step_avg:117.75ms
step:927/2245 train_time:109154ms step_avg:117.75ms
step:928/2245 train_time:109275ms step_avg:117.75ms
step:929/2245 train_time:109391ms step_avg:117.75ms
step:930/2245 train_time:109513ms step_avg:117.76ms
step:931/2245 train_time:109629ms step_avg:117.75ms
step:932/2245 train_time:109751ms step_avg:117.76ms
step:933/2245 train_time:109866ms step_avg:117.76ms
step:934/2245 train_time:109988ms step_avg:117.76ms
step:935/2245 train_time:110104ms step_avg:117.76ms
step:936/2245 train_time:110225ms step_avg:117.76ms
step:937/2245 train_time:110341ms step_avg:117.76ms
step:938/2245 train_time:110463ms step_avg:117.76ms
step:939/2245 train_time:110578ms step_avg:117.76ms
step:940/2245 train_time:110700ms step_avg:117.77ms
step:941/2245 train_time:110815ms step_avg:117.76ms
step:942/2245 train_time:110937ms step_avg:117.77ms
step:943/2245 train_time:111053ms step_avg:117.77ms
step:944/2245 train_time:111175ms step_avg:117.77ms
step:945/2245 train_time:111291ms step_avg:117.77ms
step:946/2245 train_time:111413ms step_avg:117.77ms
step:947/2245 train_time:111529ms step_avg:117.77ms
step:948/2245 train_time:111650ms step_avg:117.77ms
step:949/2245 train_time:111766ms step_avg:117.77ms
step:950/2245 train_time:111887ms step_avg:117.78ms
step:951/2245 train_time:112003ms step_avg:117.77ms
step:952/2245 train_time:112124ms step_avg:117.78ms
step:953/2245 train_time:112240ms step_avg:117.78ms
step:954/2245 train_time:112362ms step_avg:117.78ms
step:955/2245 train_time:112477ms step_avg:117.78ms
step:956/2245 train_time:112600ms step_avg:117.78ms
step:957/2245 train_time:112715ms step_avg:117.78ms
step:958/2245 train_time:112838ms step_avg:117.79ms
step:959/2245 train_time:112954ms step_avg:117.78ms
step:960/2245 train_time:113076ms step_avg:117.79ms
step:961/2245 train_time:113192ms step_avg:117.79ms
step:962/2245 train_time:113314ms step_avg:117.79ms
step:963/2245 train_time:113430ms step_avg:117.79ms
step:964/2245 train_time:113552ms step_avg:117.79ms
step:965/2245 train_time:113667ms step_avg:117.79ms
step:966/2245 train_time:113788ms step_avg:117.79ms
step:967/2245 train_time:113904ms step_avg:117.79ms
step:968/2245 train_time:114026ms step_avg:117.80ms
step:969/2245 train_time:114141ms step_avg:117.79ms
step:970/2245 train_time:114263ms step_avg:117.80ms
step:971/2245 train_time:114379ms step_avg:117.79ms
step:972/2245 train_time:114501ms step_avg:117.80ms
step:973/2245 train_time:114616ms step_avg:117.80ms
step:974/2245 train_time:114738ms step_avg:117.80ms
step:975/2245 train_time:114854ms step_avg:117.80ms
step:976/2245 train_time:114976ms step_avg:117.80ms
step:977/2245 train_time:115092ms step_avg:117.80ms
step:978/2245 train_time:115214ms step_avg:117.81ms
step:979/2245 train_time:115330ms step_avg:117.80ms
step:980/2245 train_time:115452ms step_avg:117.81ms
step:981/2245 train_time:115568ms step_avg:117.81ms
step:982/2245 train_time:115690ms step_avg:117.81ms
step:983/2245 train_time:115805ms step_avg:117.81ms
step:984/2245 train_time:115927ms step_avg:117.81ms
step:985/2245 train_time:116043ms step_avg:117.81ms
step:986/2245 train_time:116164ms step_avg:117.81ms
step:987/2245 train_time:116280ms step_avg:117.81ms
step:988/2245 train_time:116402ms step_avg:117.82ms
step:989/2245 train_time:116517ms step_avg:117.81ms
step:990/2245 train_time:116639ms step_avg:117.82ms
step:991/2245 train_time:116754ms step_avg:117.81ms
step:992/2245 train_time:116876ms step_avg:117.82ms
step:993/2245 train_time:116992ms step_avg:117.82ms
step:994/2245 train_time:117114ms step_avg:117.82ms
step:995/2245 train_time:117230ms step_avg:117.82ms
step:996/2245 train_time:117353ms step_avg:117.82ms
step:997/2245 train_time:117469ms step_avg:117.82ms
step:998/2245 train_time:117592ms step_avg:117.83ms
step:999/2245 train_time:117707ms step_avg:117.82ms
step:1000/2245 train_time:117829ms step_avg:117.83ms
step:1000/2245 val_loss:3.6122 train_time:117895ms step_avg:117.89ms
step:1001/2245 train_time:117945ms step_avg:117.83ms
step:1002/2245 train_time:118066ms step_avg:117.83ms
step:1003/2245 train_time:118181ms step_avg:117.83ms
step:1004/2245 train_time:118303ms step_avg:117.83ms
step:1005/2245 train_time:118418ms step_avg:117.83ms
step:1006/2245 train_time:118540ms step_avg:117.83ms
step:1007/2245 train_time:118656ms step_avg:117.83ms
step:1008/2245 train_time:118778ms step_avg:117.84ms
step:1009/2245 train_time:118893ms step_avg:117.83ms
step:1010/2245 train_time:119015ms step_avg:117.84ms
step:1011/2245 train_time:119131ms step_avg:117.83ms
step:1012/2245 train_time:119253ms step_avg:117.84ms
step:1013/2245 train_time:119369ms step_avg:117.84ms
step:1014/2245 train_time:119491ms step_avg:117.84ms
step:1015/2245 train_time:119606ms step_avg:117.84ms
step:1016/2245 train_time:119729ms step_avg:117.84ms
step:1017/2245 train_time:119845ms step_avg:117.84ms
step:1018/2245 train_time:119967ms step_avg:117.85ms
step:1019/2245 train_time:120082ms step_avg:117.84ms
step:1020/2245 train_time:120204ms step_avg:117.85ms
step:1021/2245 train_time:120319ms step_avg:117.84ms
step:1022/2245 train_time:120441ms step_avg:117.85ms
step:1023/2245 train_time:120556ms step_avg:117.85ms
step:1024/2245 train_time:120678ms step_avg:117.85ms
step:1025/2245 train_time:120794ms step_avg:117.85ms
step:1026/2245 train_time:120915ms step_avg:117.85ms
step:1027/2245 train_time:121031ms step_avg:117.85ms
step:1028/2245 train_time:121153ms step_avg:117.85ms
step:1029/2245 train_time:121268ms step_avg:117.85ms
step:1030/2245 train_time:121390ms step_avg:117.85ms
step:1031/2245 train_time:121507ms step_avg:117.85ms
step:1032/2245 train_time:121629ms step_avg:117.86ms
step:1033/2245 train_time:121745ms step_avg:117.86ms
step:1034/2245 train_time:121867ms step_avg:117.86ms
step:1035/2245 train_time:121983ms step_avg:117.86ms
step:1036/2245 train_time:122105ms step_avg:117.86ms
step:1037/2245 train_time:122220ms step_avg:117.86ms
step:1038/2245 train_time:122342ms step_avg:117.86ms
step:1039/2245 train_time:122458ms step_avg:117.86ms
step:1040/2245 train_time:122580ms step_avg:117.87ms
step:1041/2245 train_time:122695ms step_avg:117.86ms
step:1042/2245 train_time:122817ms step_avg:117.87ms
step:1043/2245 train_time:122932ms step_avg:117.86ms
step:1044/2245 train_time:123054ms step_avg:117.87ms
step:1045/2245 train_time:123170ms step_avg:117.87ms
step:1046/2245 train_time:123292ms step_avg:117.87ms
step:1047/2245 train_time:123408ms step_avg:117.87ms
step:1048/2245 train_time:123530ms step_avg:117.87ms
step:1049/2245 train_time:123646ms step_avg:117.87ms
step:1050/2245 train_time:123768ms step_avg:117.87ms
step:1051/2245 train_time:123884ms step_avg:117.87ms
step:1052/2245 train_time:124005ms step_avg:117.88ms
step:1053/2245 train_time:124121ms step_avg:117.87ms
step:1054/2245 train_time:124242ms step_avg:117.88ms
step:1055/2245 train_time:124358ms step_avg:117.87ms
step:1056/2245 train_time:124479ms step_avg:117.88ms
step:1057/2245 train_time:124595ms step_avg:117.88ms
step:1058/2245 train_time:124716ms step_avg:117.88ms
step:1059/2245 train_time:124832ms step_avg:117.88ms
step:1060/2245 train_time:124954ms step_avg:117.88ms
step:1061/2245 train_time:125069ms step_avg:117.88ms
step:1062/2245 train_time:125192ms step_avg:117.88ms
step:1063/2245 train_time:125307ms step_avg:117.88ms
step:1064/2245 train_time:125428ms step_avg:117.88ms
step:1065/2245 train_time:125544ms step_avg:117.88ms
step:1066/2245 train_time:125667ms step_avg:117.89ms
step:1067/2245 train_time:125782ms step_avg:117.88ms
step:1068/2245 train_time:125904ms step_avg:117.89ms
step:1069/2245 train_time:126019ms step_avg:117.88ms
step:1070/2245 train_time:126141ms step_avg:117.89ms
step:1071/2245 train_time:126256ms step_avg:117.89ms
step:1072/2245 train_time:126377ms step_avg:117.89ms
step:1073/2245 train_time:126493ms step_avg:117.89ms
step:1074/2245 train_time:126615ms step_avg:117.89ms
step:1075/2245 train_time:126730ms step_avg:117.89ms
step:1076/2245 train_time:126852ms step_avg:117.89ms
step:1077/2245 train_time:126967ms step_avg:117.89ms
step:1078/2245 train_time:127089ms step_avg:117.89ms
step:1079/2245 train_time:127205ms step_avg:117.89ms
step:1080/2245 train_time:127327ms step_avg:117.89ms
step:1081/2245 train_time:127443ms step_avg:117.89ms
step:1082/2245 train_time:127565ms step_avg:117.90ms
step:1083/2245 train_time:127680ms step_avg:117.90ms
step:1084/2245 train_time:127802ms step_avg:117.90ms
step:1085/2245 train_time:127917ms step_avg:117.90ms
step:1086/2245 train_time:128039ms step_avg:117.90ms
step:1087/2245 train_time:128155ms step_avg:117.90ms
step:1088/2245 train_time:128277ms step_avg:117.90ms
step:1089/2245 train_time:128392ms step_avg:117.90ms
step:1090/2245 train_time:128514ms step_avg:117.90ms
step:1091/2245 train_time:128630ms step_avg:117.90ms
step:1092/2245 train_time:128752ms step_avg:117.90ms
step:1093/2245 train_time:128868ms step_avg:117.90ms
step:1094/2245 train_time:128990ms step_avg:117.91ms
step:1095/2245 train_time:129106ms step_avg:117.90ms
step:1096/2245 train_time:129229ms step_avg:117.91ms
step:1097/2245 train_time:129345ms step_avg:117.91ms
step:1098/2245 train_time:129468ms step_avg:117.91ms
step:1099/2245 train_time:129584ms step_avg:117.91ms
step:1100/2245 train_time:129705ms step_avg:117.91ms
step:1101/2245 train_time:129821ms step_avg:117.91ms
step:1102/2245 train_time:129943ms step_avg:117.92ms
step:1103/2245 train_time:130058ms step_avg:117.91ms
step:1104/2245 train_time:130180ms step_avg:117.92ms
step:1105/2245 train_time:130296ms step_avg:117.91ms
step:1106/2245 train_time:130418ms step_avg:117.92ms
step:1107/2245 train_time:130533ms step_avg:117.92ms
step:1108/2245 train_time:130655ms step_avg:117.92ms
step:1109/2245 train_time:130771ms step_avg:117.92ms
step:1110/2245 train_time:130893ms step_avg:117.92ms
step:1111/2245 train_time:131009ms step_avg:117.92ms
step:1112/2245 train_time:131130ms step_avg:117.92ms
step:1113/2245 train_time:131246ms step_avg:117.92ms
step:1114/2245 train_time:131368ms step_avg:117.92ms
step:1115/2245 train_time:131484ms step_avg:117.92ms
step:1116/2245 train_time:131606ms step_avg:117.93ms
step:1117/2245 train_time:131722ms step_avg:117.92ms
step:1118/2245 train_time:131844ms step_avg:117.93ms
step:1119/2245 train_time:131959ms step_avg:117.93ms
step:1120/2245 train_time:132081ms step_avg:117.93ms
step:1121/2245 train_time:132197ms step_avg:117.93ms
step:1122/2245 train_time:132318ms step_avg:117.93ms
step:1123/2245 train_time:132434ms step_avg:117.93ms
step:1124/2245 train_time:132556ms step_avg:117.93ms
step:1125/2245 train_time:132671ms step_avg:117.93ms
step:1126/2245 train_time:132793ms step_avg:117.93ms
step:1127/2245 train_time:132908ms step_avg:117.93ms
step:1128/2245 train_time:133031ms step_avg:117.94ms
step:1129/2245 train_time:133146ms step_avg:117.93ms
step:1130/2245 train_time:133269ms step_avg:117.94ms
step:1131/2245 train_time:133385ms step_avg:117.94ms
step:1132/2245 train_time:133507ms step_avg:117.94ms
step:1133/2245 train_time:133624ms step_avg:117.94ms
step:1134/2245 train_time:133745ms step_avg:117.94ms
step:1135/2245 train_time:133860ms step_avg:117.94ms
step:1136/2245 train_time:133982ms step_avg:117.94ms
step:1137/2245 train_time:134097ms step_avg:117.94ms
step:1138/2245 train_time:134219ms step_avg:117.94ms
step:1139/2245 train_time:134335ms step_avg:117.94ms
step:1140/2245 train_time:134457ms step_avg:117.94ms
step:1141/2245 train_time:134572ms step_avg:117.94ms
step:1142/2245 train_time:134694ms step_avg:117.95ms
step:1143/2245 train_time:134809ms step_avg:117.94ms
step:1144/2245 train_time:134932ms step_avg:117.95ms
step:1145/2245 train_time:135047ms step_avg:117.95ms
step:1146/2245 train_time:135169ms step_avg:117.95ms
step:1147/2245 train_time:135285ms step_avg:117.95ms
step:1148/2245 train_time:135408ms step_avg:117.95ms
step:1149/2245 train_time:135523ms step_avg:117.95ms
step:1150/2245 train_time:135645ms step_avg:117.95ms
step:1151/2245 train_time:135761ms step_avg:117.95ms
step:1152/2245 train_time:135883ms step_avg:117.95ms
step:1153/2245 train_time:135999ms step_avg:117.95ms
step:1154/2245 train_time:136120ms step_avg:117.96ms
step:1155/2245 train_time:136236ms step_avg:117.95ms
step:1156/2245 train_time:136358ms step_avg:117.96ms
step:1157/2245 train_time:136473ms step_avg:117.95ms
step:1158/2245 train_time:136595ms step_avg:117.96ms
step:1159/2245 train_time:136710ms step_avg:117.96ms
step:1160/2245 train_time:136832ms step_avg:117.96ms
step:1161/2245 train_time:136948ms step_avg:117.96ms
step:1162/2245 train_time:137070ms step_avg:117.96ms
step:1163/2245 train_time:137186ms step_avg:117.96ms
step:1164/2245 train_time:137308ms step_avg:117.96ms
step:1165/2245 train_time:137424ms step_avg:117.96ms
step:1166/2245 train_time:137546ms step_avg:117.96ms
step:1167/2245 train_time:137662ms step_avg:117.96ms
step:1168/2245 train_time:137784ms step_avg:117.97ms
step:1169/2245 train_time:137900ms step_avg:117.96ms
step:1170/2245 train_time:138022ms step_avg:117.97ms
step:1171/2245 train_time:138136ms step_avg:117.96ms
step:1172/2245 train_time:138258ms step_avg:117.97ms
step:1173/2245 train_time:138374ms step_avg:117.97ms
step:1174/2245 train_time:138496ms step_avg:117.97ms
step:1175/2245 train_time:138611ms step_avg:117.97ms
step:1176/2245 train_time:138733ms step_avg:117.97ms
step:1177/2245 train_time:138849ms step_avg:117.97ms
step:1178/2245 train_time:138971ms step_avg:117.97ms
step:1179/2245 train_time:139086ms step_avg:117.97ms
step:1180/2245 train_time:139208ms step_avg:117.97ms
step:1181/2245 train_time:139324ms step_avg:117.97ms
step:1182/2245 train_time:139447ms step_avg:117.98ms
step:1183/2245 train_time:139562ms step_avg:117.97ms
step:1184/2245 train_time:139684ms step_avg:117.98ms
step:1185/2245 train_time:139799ms step_avg:117.97ms
step:1186/2245 train_time:139921ms step_avg:117.98ms
step:1187/2245 train_time:140036ms step_avg:117.98ms
step:1188/2245 train_time:140159ms step_avg:117.98ms
step:1189/2245 train_time:140275ms step_avg:117.98ms
step:1190/2245 train_time:140397ms step_avg:117.98ms
step:1191/2245 train_time:140513ms step_avg:117.98ms
step:1192/2245 train_time:140635ms step_avg:117.98ms
step:1193/2245 train_time:140750ms step_avg:117.98ms
step:1194/2245 train_time:140872ms step_avg:117.98ms
step:1195/2245 train_time:140987ms step_avg:117.98ms
step:1196/2245 train_time:141109ms step_avg:117.98ms
step:1197/2245 train_time:141225ms step_avg:117.98ms
step:1198/2245 train_time:141347ms step_avg:117.99ms
step:1199/2245 train_time:141462ms step_avg:117.98ms
step:1200/2245 train_time:141584ms step_avg:117.99ms
step:1201/2245 train_time:141699ms step_avg:117.98ms
step:1202/2245 train_time:141821ms step_avg:117.99ms
step:1203/2245 train_time:141937ms step_avg:117.99ms
step:1204/2245 train_time:142058ms step_avg:117.99ms
step:1205/2245 train_time:142174ms step_avg:117.99ms
step:1206/2245 train_time:142296ms step_avg:117.99ms
step:1207/2245 train_time:142411ms step_avg:117.99ms
step:1208/2245 train_time:142533ms step_avg:117.99ms
step:1209/2245 train_time:142649ms step_avg:117.99ms
step:1210/2245 train_time:142771ms step_avg:117.99ms
step:1211/2245 train_time:142887ms step_avg:117.99ms
step:1212/2245 train_time:143009ms step_avg:117.99ms
step:1213/2245 train_time:143125ms step_avg:117.99ms
step:1214/2245 train_time:143247ms step_avg:118.00ms
step:1215/2245 train_time:143363ms step_avg:117.99ms
step:1216/2245 train_time:143485ms step_avg:118.00ms
step:1217/2245 train_time:143600ms step_avg:118.00ms
step:1218/2245 train_time:143722ms step_avg:118.00ms
step:1219/2245 train_time:143838ms step_avg:118.00ms
step:1220/2245 train_time:143959ms step_avg:118.00ms
step:1221/2245 train_time:144075ms step_avg:118.00ms
step:1222/2245 train_time:144197ms step_avg:118.00ms
step:1223/2245 train_time:144312ms step_avg:118.00ms
step:1224/2245 train_time:144435ms step_avg:118.00ms
step:1225/2245 train_time:144550ms step_avg:118.00ms
step:1226/2245 train_time:144672ms step_avg:118.00ms
step:1227/2245 train_time:144788ms step_avg:118.00ms
step:1228/2245 train_time:144909ms step_avg:118.00ms
step:1229/2245 train_time:145026ms step_avg:118.00ms
step:1230/2245 train_time:145148ms step_avg:118.01ms
step:1231/2245 train_time:145264ms step_avg:118.00ms
step:1232/2245 train_time:145386ms step_avg:118.01ms
step:1233/2245 train_time:145501ms step_avg:118.01ms
step:1234/2245 train_time:145623ms step_avg:118.01ms
step:1235/2245 train_time:145738ms step_avg:118.01ms
step:1236/2245 train_time:145860ms step_avg:118.01ms
step:1237/2245 train_time:145976ms step_avg:118.01ms
step:1238/2245 train_time:146098ms step_avg:118.01ms
step:1239/2245 train_time:146213ms step_avg:118.01ms
step:1240/2245 train_time:146335ms step_avg:118.01ms
step:1241/2245 train_time:146450ms step_avg:118.01ms
step:1242/2245 train_time:146573ms step_avg:118.01ms
step:1243/2245 train_time:146688ms step_avg:118.01ms
step:1244/2245 train_time:146810ms step_avg:118.01ms
step:1245/2245 train_time:146926ms step_avg:118.01ms
step:1246/2245 train_time:147049ms step_avg:118.02ms
step:1247/2245 train_time:147165ms step_avg:118.01ms
step:1248/2245 train_time:147286ms step_avg:118.02ms
step:1249/2245 train_time:147402ms step_avg:118.02ms
step:1250/2245 train_time:147524ms step_avg:118.02ms
step:1250/2245 val_loss:3.5280 train_time:147590ms step_avg:118.07ms
step:1251/2245 train_time:147640ms step_avg:118.02ms
step:1252/2245 train_time:147762ms step_avg:118.02ms
step:1253/2245 train_time:147877ms step_avg:118.02ms
step:1254/2245 train_time:147999ms step_avg:118.02ms
step:1255/2245 train_time:148115ms step_avg:118.02ms
step:1256/2245 train_time:148236ms step_avg:118.02ms
step:1257/2245 train_time:148352ms step_avg:118.02ms
step:1258/2245 train_time:148473ms step_avg:118.02ms
step:1259/2245 train_time:148589ms step_avg:118.02ms
step:1260/2245 train_time:148711ms step_avg:118.02ms
step:1261/2245 train_time:148827ms step_avg:118.02ms
step:1262/2245 train_time:148949ms step_avg:118.03ms
step:1263/2245 train_time:149065ms step_avg:118.02ms
step:1264/2245 train_time:149187ms step_avg:118.03ms
step:1265/2245 train_time:149303ms step_avg:118.03ms
step:1266/2245 train_time:149425ms step_avg:118.03ms
step:1267/2245 train_time:149541ms step_avg:118.03ms
step:1268/2245 train_time:149663ms step_avg:118.03ms
step:1269/2245 train_time:149778ms step_avg:118.03ms
step:1270/2245 train_time:149900ms step_avg:118.03ms
step:1271/2245 train_time:150015ms step_avg:118.03ms
step:1272/2245 train_time:150137ms step_avg:118.03ms
step:1273/2245 train_time:150253ms step_avg:118.03ms
step:1274/2245 train_time:150375ms step_avg:118.03ms
step:1275/2245 train_time:150490ms step_avg:118.03ms
step:1276/2245 train_time:150613ms step_avg:118.04ms
step:1277/2245 train_time:150728ms step_avg:118.03ms
step:1278/2245 train_time:150850ms step_avg:118.04ms
step:1279/2245 train_time:150966ms step_avg:118.03ms
step:1280/2245 train_time:151088ms step_avg:118.04ms
step:1281/2245 train_time:151204ms step_avg:118.04ms
step:1282/2245 train_time:151326ms step_avg:118.04ms
step:1283/2245 train_time:151442ms step_avg:118.04ms
step:1284/2245 train_time:151565ms step_avg:118.04ms
step:1285/2245 train_time:151681ms step_avg:118.04ms
step:1286/2245 train_time:151803ms step_avg:118.04ms
step:1287/2245 train_time:151919ms step_avg:118.04ms
step:1288/2245 train_time:152041ms step_avg:118.04ms
step:1289/2245 train_time:152156ms step_avg:118.04ms
step:1290/2245 train_time:152278ms step_avg:118.04ms
step:1291/2245 train_time:152393ms step_avg:118.04ms
step:1292/2245 train_time:152515ms step_avg:118.05ms
step:1293/2245 train_time:152630ms step_avg:118.04ms
step:1294/2245 train_time:152752ms step_avg:118.05ms
step:1295/2245 train_time:152867ms step_avg:118.04ms
step:1296/2245 train_time:152989ms step_avg:118.05ms
step:1297/2245 train_time:153105ms step_avg:118.05ms
step:1298/2245 train_time:153226ms step_avg:118.05ms
step:1299/2245 train_time:153342ms step_avg:118.05ms
step:1300/2245 train_time:153464ms step_avg:118.05ms
step:1301/2245 train_time:153581ms step_avg:118.05ms
step:1302/2245 train_time:153704ms step_avg:118.05ms
step:1303/2245 train_time:153819ms step_avg:118.05ms
step:1304/2245 train_time:153942ms step_avg:118.05ms
step:1305/2245 train_time:154057ms step_avg:118.05ms
step:1306/2245 train_time:154178ms step_avg:118.05ms
step:1307/2245 train_time:154294ms step_avg:118.05ms
step:1308/2245 train_time:154416ms step_avg:118.05ms
step:1309/2245 train_time:154531ms step_avg:118.05ms
step:1310/2245 train_time:154653ms step_avg:118.06ms
step:1311/2245 train_time:154768ms step_avg:118.05ms
step:1312/2245 train_time:154891ms step_avg:118.06ms
step:1313/2245 train_time:155006ms step_avg:118.05ms
step:1314/2245 train_time:155128ms step_avg:118.06ms
step:1315/2245 train_time:155243ms step_avg:118.06ms
step:1316/2245 train_time:155365ms step_avg:118.06ms
step:1317/2245 train_time:155480ms step_avg:118.06ms
step:1318/2245 train_time:155602ms step_avg:118.06ms
step:1319/2245 train_time:155718ms step_avg:118.06ms
step:1320/2245 train_time:155840ms step_avg:118.06ms
step:1321/2245 train_time:155956ms step_avg:118.06ms
step:1322/2245 train_time:156078ms step_avg:118.06ms
step:1323/2245 train_time:156193ms step_avg:118.06ms
step:1324/2245 train_time:156315ms step_avg:118.06ms
step:1325/2245 train_time:156430ms step_avg:118.06ms
step:1326/2245 train_time:156552ms step_avg:118.06ms
step:1327/2245 train_time:156668ms step_avg:118.06ms
step:1328/2245 train_time:156790ms step_avg:118.06ms
step:1329/2245 train_time:156905ms step_avg:118.06ms
step:1330/2245 train_time:157028ms step_avg:118.07ms
step:1331/2245 train_time:157143ms step_avg:118.06ms
step:1332/2245 train_time:157265ms step_avg:118.07ms
step:1333/2245 train_time:157381ms step_avg:118.07ms
step:1334/2245 train_time:157503ms step_avg:118.07ms
step:1335/2245 train_time:157620ms step_avg:118.07ms
step:1336/2245 train_time:157742ms step_avg:118.07ms
step:1337/2245 train_time:157857ms step_avg:118.07ms
step:1338/2245 train_time:157979ms step_avg:118.07ms
step:1339/2245 train_time:158095ms step_avg:118.07ms
step:1340/2245 train_time:158216ms step_avg:118.07ms
step:1341/2245 train_time:158332ms step_avg:118.07ms
step:1342/2245 train_time:158454ms step_avg:118.07ms
step:1343/2245 train_time:158569ms step_avg:118.07ms
step:1344/2245 train_time:158691ms step_avg:118.07ms
step:1345/2245 train_time:158806ms step_avg:118.07ms
step:1346/2245 train_time:158928ms step_avg:118.07ms
step:1347/2245 train_time:159045ms step_avg:118.07ms
step:1348/2245 train_time:159166ms step_avg:118.08ms
step:1349/2245 train_time:159281ms step_avg:118.07ms
step:1350/2245 train_time:159404ms step_avg:118.08ms
step:1351/2245 train_time:159519ms step_avg:118.08ms
step:1352/2245 train_time:159641ms step_avg:118.08ms
step:1353/2245 train_time:159757ms step_avg:118.08ms
step:1354/2245 train_time:159878ms step_avg:118.08ms
step:1355/2245 train_time:159994ms step_avg:118.08ms
step:1356/2245 train_time:160116ms step_avg:118.08ms
step:1357/2245 train_time:160231ms step_avg:118.08ms
step:1358/2245 train_time:160353ms step_avg:118.08ms
step:1359/2245 train_time:160468ms step_avg:118.08ms
step:1360/2245 train_time:160590ms step_avg:118.08ms
step:1361/2245 train_time:160706ms step_avg:118.08ms
step:1362/2245 train_time:160828ms step_avg:118.08ms
step:1363/2245 train_time:160944ms step_avg:118.08ms
step:1364/2245 train_time:161066ms step_avg:118.08ms
step:1365/2245 train_time:161182ms step_avg:118.08ms
step:1366/2245 train_time:161304ms step_avg:118.08ms
step:1367/2245 train_time:161421ms step_avg:118.08ms
step:1368/2245 train_time:161543ms step_avg:118.09ms
step:1369/2245 train_time:161659ms step_avg:118.09ms
step:1370/2245 train_time:161781ms step_avg:118.09ms
step:1371/2245 train_time:161896ms step_avg:118.09ms
step:1372/2245 train_time:162018ms step_avg:118.09ms
step:1373/2245 train_time:162134ms step_avg:118.09ms
step:1374/2245 train_time:162255ms step_avg:118.09ms
step:1375/2245 train_time:162371ms step_avg:118.09ms
step:1376/2245 train_time:162493ms step_avg:118.09ms
step:1377/2245 train_time:162608ms step_avg:118.09ms
step:1378/2245 train_time:162730ms step_avg:118.09ms
step:1379/2245 train_time:162845ms step_avg:118.09ms
step:1380/2245 train_time:162968ms step_avg:118.09ms
step:1381/2245 train_time:163084ms step_avg:118.09ms
step:1382/2245 train_time:163205ms step_avg:118.09ms
step:1383/2245 train_time:163322ms step_avg:118.09ms
step:1384/2245 train_time:163444ms step_avg:118.10ms
step:1385/2245 train_time:163560ms step_avg:118.09ms
step:1386/2245 train_time:163682ms step_avg:118.10ms
step:1387/2245 train_time:163797ms step_avg:118.09ms
step:1388/2245 train_time:163919ms step_avg:118.10ms
step:1389/2245 train_time:164034ms step_avg:118.09ms
step:1390/2245 train_time:164156ms step_avg:118.10ms
step:1391/2245 train_time:164272ms step_avg:118.10ms
step:1392/2245 train_time:164394ms step_avg:118.10ms
step:1393/2245 train_time:164510ms step_avg:118.10ms
step:1394/2245 train_time:164632ms step_avg:118.10ms
step:1395/2245 train_time:164747ms step_avg:118.10ms
step:1396/2245 train_time:164869ms step_avg:118.10ms
step:1397/2245 train_time:164985ms step_avg:118.10ms
step:1398/2245 train_time:165107ms step_avg:118.10ms
step:1399/2245 train_time:165223ms step_avg:118.10ms
step:1400/2245 train_time:165345ms step_avg:118.10ms
step:1401/2245 train_time:165461ms step_avg:118.10ms
step:1402/2245 train_time:165583ms step_avg:118.11ms
step:1403/2245 train_time:165699ms step_avg:118.10ms
step:1404/2245 train_time:165821ms step_avg:118.11ms
step:1405/2245 train_time:165937ms step_avg:118.10ms
step:1406/2245 train_time:166058ms step_avg:118.11ms
step:1407/2245 train_time:166174ms step_avg:118.10ms
step:1408/2245 train_time:166296ms step_avg:118.11ms
step:1409/2245 train_time:166411ms step_avg:118.11ms
step:1410/2245 train_time:166533ms step_avg:118.11ms
step:1411/2245 train_time:166649ms step_avg:118.11ms
step:1412/2245 train_time:166770ms step_avg:118.11ms
step:1413/2245 train_time:166886ms step_avg:118.11ms
step:1414/2245 train_time:167008ms step_avg:118.11ms
step:1415/2245 train_time:167124ms step_avg:118.11ms
step:1416/2245 train_time:167246ms step_avg:118.11ms
step:1417/2245 train_time:167362ms step_avg:118.11ms
step:1418/2245 train_time:167484ms step_avg:118.11ms
step:1419/2245 train_time:167601ms step_avg:118.11ms
step:1420/2245 train_time:167723ms step_avg:118.11ms
step:1421/2245 train_time:167839ms step_avg:118.11ms
step:1422/2245 train_time:167960ms step_avg:118.12ms
step:1423/2245 train_time:168076ms step_avg:118.11ms
step:1424/2245 train_time:168197ms step_avg:118.12ms
step:1425/2245 train_time:168312ms step_avg:118.11ms
step:1426/2245 train_time:168434ms step_avg:118.12ms
step:1427/2245 train_time:168549ms step_avg:118.11ms
step:1428/2245 train_time:168672ms step_avg:118.12ms
step:1429/2245 train_time:168787ms step_avg:118.12ms
step:1430/2245 train_time:168908ms step_avg:118.12ms
step:1431/2245 train_time:169025ms step_avg:118.12ms
step:1432/2245 train_time:169147ms step_avg:118.12ms
step:1433/2245 train_time:169263ms step_avg:118.12ms
step:1434/2245 train_time:169385ms step_avg:118.12ms
step:1435/2245 train_time:169501ms step_avg:118.12ms
step:1436/2245 train_time:169624ms step_avg:118.12ms
step:1437/2245 train_time:169740ms step_avg:118.12ms
step:1438/2245 train_time:169862ms step_avg:118.12ms
step:1439/2245 train_time:169977ms step_avg:118.12ms
step:1440/2245 train_time:170098ms step_avg:118.12ms
step:1441/2245 train_time:170214ms step_avg:118.12ms
step:1442/2245 train_time:170336ms step_avg:118.12ms
step:1443/2245 train_time:170451ms step_avg:118.12ms
step:1444/2245 train_time:170573ms step_avg:118.13ms
step:1445/2245 train_time:170688ms step_avg:118.12ms
step:1446/2245 train_time:170810ms step_avg:118.13ms
step:1447/2245 train_time:170926ms step_avg:118.12ms
step:1448/2245 train_time:171048ms step_avg:118.13ms
step:1449/2245 train_time:171163ms step_avg:118.13ms
step:1450/2245 train_time:171285ms step_avg:118.13ms
step:1451/2245 train_time:171402ms step_avg:118.13ms
step:1452/2245 train_time:171524ms step_avg:118.13ms
step:1453/2245 train_time:171640ms step_avg:118.13ms
step:1454/2245 train_time:171763ms step_avg:118.13ms
step:1455/2245 train_time:171879ms step_avg:118.13ms
step:1456/2245 train_time:172002ms step_avg:118.13ms
step:1457/2245 train_time:172116ms step_avg:118.13ms
step:1458/2245 train_time:172238ms step_avg:118.13ms
step:1459/2245 train_time:172354ms step_avg:118.13ms
step:1460/2245 train_time:172476ms step_avg:118.13ms
step:1461/2245 train_time:172591ms step_avg:118.13ms
step:1462/2245 train_time:172714ms step_avg:118.14ms
step:1463/2245 train_time:172829ms step_avg:118.13ms
step:1464/2245 train_time:172951ms step_avg:118.14ms
step:1465/2245 train_time:173067ms step_avg:118.13ms
step:1466/2245 train_time:173189ms step_avg:118.14ms
step:1467/2245 train_time:173305ms step_avg:118.14ms
step:1468/2245 train_time:173427ms step_avg:118.14ms
step:1469/2245 train_time:173542ms step_avg:118.14ms
step:1470/2245 train_time:173664ms step_avg:118.14ms
step:1471/2245 train_time:173781ms step_avg:118.14ms
step:1472/2245 train_time:173904ms step_avg:118.14ms
step:1473/2245 train_time:174021ms step_avg:118.14ms
step:1474/2245 train_time:174143ms step_avg:118.14ms
step:1475/2245 train_time:174259ms step_avg:118.14ms
step:1476/2245 train_time:174382ms step_avg:118.14ms
step:1477/2245 train_time:174498ms step_avg:118.14ms
step:1478/2245 train_time:174621ms step_avg:118.15ms
step:1479/2245 train_time:174738ms step_avg:118.15ms
step:1480/2245 train_time:174860ms step_avg:118.15ms
step:1481/2245 train_time:174977ms step_avg:118.15ms
step:1482/2245 train_time:175100ms step_avg:118.15ms
step:1483/2245 train_time:175216ms step_avg:118.15ms
step:1484/2245 train_time:175339ms step_avg:118.15ms
step:1485/2245 train_time:175455ms step_avg:118.15ms
step:1486/2245 train_time:175577ms step_avg:118.15ms
step:1487/2245 train_time:175694ms step_avg:118.15ms
step:1488/2245 train_time:175817ms step_avg:118.16ms
step:1489/2245 train_time:175933ms step_avg:118.16ms
step:1490/2245 train_time:176056ms step_avg:118.16ms
step:1491/2245 train_time:176173ms step_avg:118.16ms
step:1492/2245 train_time:176295ms step_avg:118.16ms
step:1493/2245 train_time:176412ms step_avg:118.16ms
step:1494/2245 train_time:176534ms step_avg:118.16ms
step:1495/2245 train_time:176650ms step_avg:118.16ms
step:1496/2245 train_time:176773ms step_avg:118.16ms
step:1497/2245 train_time:176890ms step_avg:118.16ms
step:1498/2245 train_time:177013ms step_avg:118.17ms
step:1499/2245 train_time:177130ms step_avg:118.17ms
step:1500/2245 train_time:177253ms step_avg:118.17ms
step:1500/2245 val_loss:3.4457 train_time:177320ms step_avg:118.21ms
step:1501/2245 train_time:177371ms step_avg:118.17ms
step:1502/2245 train_time:177493ms step_avg:118.17ms
step:1503/2245 train_time:177609ms step_avg:118.17ms
step:1504/2245 train_time:177731ms step_avg:118.17ms
step:1505/2245 train_time:177847ms step_avg:118.17ms
step:1506/2245 train_time:177971ms step_avg:118.17ms
step:1507/2245 train_time:178087ms step_avg:118.17ms
step:1508/2245 train_time:178210ms step_avg:118.18ms
step:1509/2245 train_time:178327ms step_avg:118.18ms
step:1510/2245 train_time:178451ms step_avg:118.18ms
step:1511/2245 train_time:178568ms step_avg:118.18ms
step:1512/2245 train_time:178690ms step_avg:118.18ms
step:1513/2245 train_time:178807ms step_avg:118.18ms
step:1514/2245 train_time:178930ms step_avg:118.18ms
step:1515/2245 train_time:179046ms step_avg:118.18ms
step:1516/2245 train_time:179170ms step_avg:118.19ms
step:1517/2245 train_time:179286ms step_avg:118.18ms
step:1518/2245 train_time:179409ms step_avg:118.19ms
step:1519/2245 train_time:179526ms step_avg:118.19ms
step:1520/2245 train_time:179650ms step_avg:118.19ms
step:1521/2245 train_time:179766ms step_avg:118.19ms
step:1522/2245 train_time:179889ms step_avg:118.19ms
step:1523/2245 train_time:180005ms step_avg:118.19ms
step:1524/2245 train_time:180128ms step_avg:118.19ms
step:1525/2245 train_time:180244ms step_avg:118.19ms
step:1526/2245 train_time:180367ms step_avg:118.20ms
step:1527/2245 train_time:180484ms step_avg:118.20ms
step:1528/2245 train_time:180606ms step_avg:118.20ms
step:1529/2245 train_time:180723ms step_avg:118.20ms
step:1530/2245 train_time:180846ms step_avg:118.20ms
step:1531/2245 train_time:180962ms step_avg:118.20ms
step:1532/2245 train_time:181085ms step_avg:118.20ms
step:1533/2245 train_time:181201ms step_avg:118.20ms
step:1534/2245 train_time:181324ms step_avg:118.20ms
step:1535/2245 train_time:181440ms step_avg:118.20ms
step:1536/2245 train_time:181563ms step_avg:118.21ms
step:1537/2245 train_time:181679ms step_avg:118.20ms
step:1538/2245 train_time:181802ms step_avg:118.21ms
step:1539/2245 train_time:181918ms step_avg:118.21ms
step:1540/2245 train_time:182041ms step_avg:118.21ms
step:1541/2245 train_time:182157ms step_avg:118.21ms
step:1542/2245 train_time:182280ms step_avg:118.21ms
step:1543/2245 train_time:182396ms step_avg:118.21ms
step:1544/2245 train_time:182519ms step_avg:118.21ms
step:1545/2245 train_time:182636ms step_avg:118.21ms
step:1546/2245 train_time:182759ms step_avg:118.21ms
step:1547/2245 train_time:182875ms step_avg:118.21ms
step:1548/2245 train_time:182998ms step_avg:118.22ms
step:1549/2245 train_time:183115ms step_avg:118.21ms
step:1550/2245 train_time:183238ms step_avg:118.22ms
step:1551/2245 train_time:183354ms step_avg:118.22ms
step:1552/2245 train_time:183477ms step_avg:118.22ms
step:1553/2245 train_time:183593ms step_avg:118.22ms
step:1554/2245 train_time:183716ms step_avg:118.22ms
step:1555/2245 train_time:183832ms step_avg:118.22ms
step:1556/2245 train_time:183955ms step_avg:118.22ms
step:1557/2245 train_time:184071ms step_avg:118.22ms
step:1558/2245 train_time:184195ms step_avg:118.23ms
step:1559/2245 train_time:184311ms step_avg:118.22ms
step:1560/2245 train_time:184434ms step_avg:118.23ms
step:1561/2245 train_time:184551ms step_avg:118.23ms
step:1562/2245 train_time:184674ms step_avg:118.23ms
step:1563/2245 train_time:184791ms step_avg:118.23ms
step:1564/2245 train_time:184914ms step_avg:118.23ms
step:1565/2245 train_time:185030ms step_avg:118.23ms
step:1566/2245 train_time:185153ms step_avg:118.23ms
step:1567/2245 train_time:185270ms step_avg:118.23ms
step:1568/2245 train_time:185393ms step_avg:118.24ms
step:1569/2245 train_time:185510ms step_avg:118.23ms
step:1570/2245 train_time:185633ms step_avg:118.24ms
step:1571/2245 train_time:185750ms step_avg:118.24ms
step:1572/2245 train_time:185872ms step_avg:118.24ms
step:1573/2245 train_time:185989ms step_avg:118.24ms
step:1574/2245 train_time:186112ms step_avg:118.24ms
step:1575/2245 train_time:186228ms step_avg:118.24ms
step:1576/2245 train_time:186352ms step_avg:118.24ms
step:1577/2245 train_time:186468ms step_avg:118.24ms
step:1578/2245 train_time:186592ms step_avg:118.25ms
step:1579/2245 train_time:186708ms step_avg:118.24ms
step:1580/2245 train_time:186832ms step_avg:118.25ms
step:1581/2245 train_time:186949ms step_avg:118.25ms
step:1582/2245 train_time:187071ms step_avg:118.25ms
step:1583/2245 train_time:187189ms step_avg:118.25ms
step:1584/2245 train_time:187312ms step_avg:118.25ms
step:1585/2245 train_time:187428ms step_avg:118.25ms
step:1586/2245 train_time:187551ms step_avg:118.25ms
step:1587/2245 train_time:187667ms step_avg:118.25ms
step:1588/2245 train_time:187791ms step_avg:118.26ms
step:1589/2245 train_time:187907ms step_avg:118.26ms
step:1590/2245 train_time:188030ms step_avg:118.26ms
step:1591/2245 train_time:188148ms step_avg:118.26ms
step:1592/2245 train_time:188271ms step_avg:118.26ms
step:1593/2245 train_time:188387ms step_avg:118.26ms
step:1594/2245 train_time:188512ms step_avg:118.26ms
step:1595/2245 train_time:188628ms step_avg:118.26ms
step:1596/2245 train_time:188751ms step_avg:118.27ms
step:1597/2245 train_time:188868ms step_avg:118.26ms
step:1598/2245 train_time:188990ms step_avg:118.27ms
step:1599/2245 train_time:189106ms step_avg:118.27ms
step:1600/2245 train_time:189230ms step_avg:118.27ms
step:1601/2245 train_time:189347ms step_avg:118.27ms
step:1602/2245 train_time:189470ms step_avg:118.27ms
step:1603/2245 train_time:189587ms step_avg:118.27ms
step:1604/2245 train_time:189711ms step_avg:118.27ms
step:1605/2245 train_time:189827ms step_avg:118.27ms
step:1606/2245 train_time:189950ms step_avg:118.28ms
step:1607/2245 train_time:190067ms step_avg:118.27ms
step:1608/2245 train_time:190191ms step_avg:118.28ms
step:1609/2245 train_time:190307ms step_avg:118.28ms
step:1610/2245 train_time:190430ms step_avg:118.28ms
step:1611/2245 train_time:190546ms step_avg:118.28ms
step:1612/2245 train_time:190669ms step_avg:118.28ms
step:1613/2245 train_time:190785ms step_avg:118.28ms
step:1614/2245 train_time:190909ms step_avg:118.28ms
step:1615/2245 train_time:191026ms step_avg:118.28ms
step:1616/2245 train_time:191149ms step_avg:118.29ms
step:1617/2245 train_time:191266ms step_avg:118.28ms
step:1618/2245 train_time:191388ms step_avg:118.29ms
step:1619/2245 train_time:191506ms step_avg:118.29ms
step:1620/2245 train_time:191628ms step_avg:118.29ms
step:1621/2245 train_time:191744ms step_avg:118.29ms
step:1622/2245 train_time:191867ms step_avg:118.29ms
step:1623/2245 train_time:191985ms step_avg:118.29ms
step:1624/2245 train_time:192108ms step_avg:118.29ms
step:1625/2245 train_time:192225ms step_avg:118.29ms
step:1626/2245 train_time:192348ms step_avg:118.30ms
step:1627/2245 train_time:192465ms step_avg:118.29ms
step:1628/2245 train_time:192588ms step_avg:118.30ms
step:1629/2245 train_time:192704ms step_avg:118.30ms
step:1630/2245 train_time:192827ms step_avg:118.30ms
step:1631/2245 train_time:192943ms step_avg:118.30ms
step:1632/2245 train_time:193065ms step_avg:118.30ms
step:1633/2245 train_time:193182ms step_avg:118.30ms
step:1634/2245 train_time:193306ms step_avg:118.30ms
step:1635/2245 train_time:193422ms step_avg:118.30ms
step:1636/2245 train_time:193545ms step_avg:118.30ms
step:1637/2245 train_time:193661ms step_avg:118.30ms
step:1638/2245 train_time:193784ms step_avg:118.31ms
step:1639/2245 train_time:193900ms step_avg:118.30ms
step:1640/2245 train_time:194023ms step_avg:118.31ms
step:1641/2245 train_time:194139ms step_avg:118.31ms
step:1642/2245 train_time:194262ms step_avg:118.31ms
step:1643/2245 train_time:194378ms step_avg:118.31ms
step:1644/2245 train_time:194501ms step_avg:118.31ms
step:1645/2245 train_time:194618ms step_avg:118.31ms
step:1646/2245 train_time:194740ms step_avg:118.31ms
step:1647/2245 train_time:194857ms step_avg:118.31ms
step:1648/2245 train_time:194979ms step_avg:118.31ms
step:1649/2245 train_time:195096ms step_avg:118.31ms
step:1650/2245 train_time:195219ms step_avg:118.31ms
step:1651/2245 train_time:195335ms step_avg:118.31ms
step:1652/2245 train_time:195458ms step_avg:118.32ms
step:1653/2245 train_time:195575ms step_avg:118.32ms
step:1654/2245 train_time:195698ms step_avg:118.32ms
step:1655/2245 train_time:195814ms step_avg:118.32ms
step:1656/2245 train_time:195936ms step_avg:118.32ms
step:1657/2245 train_time:196052ms step_avg:118.32ms
step:1658/2245 train_time:196176ms step_avg:118.32ms
step:1659/2245 train_time:196292ms step_avg:118.32ms
step:1660/2245 train_time:196415ms step_avg:118.32ms
step:1661/2245 train_time:196531ms step_avg:118.32ms
step:1662/2245 train_time:196654ms step_avg:118.32ms
step:1663/2245 train_time:196771ms step_avg:118.32ms
step:1664/2245 train_time:196893ms step_avg:118.33ms
step:1665/2245 train_time:197010ms step_avg:118.32ms
step:1666/2245 train_time:197133ms step_avg:118.33ms
step:1667/2245 train_time:197250ms step_avg:118.33ms
step:1668/2245 train_time:197373ms step_avg:118.33ms
step:1669/2245 train_time:197490ms step_avg:118.33ms
step:1670/2245 train_time:197612ms step_avg:118.33ms
step:1671/2245 train_time:197729ms step_avg:118.33ms
step:1672/2245 train_time:197852ms step_avg:118.33ms
step:1673/2245 train_time:197969ms step_avg:118.33ms
step:1674/2245 train_time:198092ms step_avg:118.33ms
step:1675/2245 train_time:198208ms step_avg:118.33ms
step:1676/2245 train_time:198331ms step_avg:118.34ms
step:1677/2245 train_time:198448ms step_avg:118.33ms
step:1678/2245 train_time:198570ms step_avg:118.34ms
step:1679/2245 train_time:198687ms step_avg:118.34ms
step:1680/2245 train_time:198810ms step_avg:118.34ms
step:1681/2245 train_time:198928ms step_avg:118.34ms
step:1682/2245 train_time:199051ms step_avg:118.34ms
step:1683/2245 train_time:199167ms step_avg:118.34ms
step:1684/2245 train_time:199291ms step_avg:118.34ms
step:1685/2245 train_time:199408ms step_avg:118.34ms
step:1686/2245 train_time:199532ms step_avg:118.35ms
step:1687/2245 train_time:199648ms step_avg:118.35ms
step:1688/2245 train_time:199772ms step_avg:118.35ms
step:1689/2245 train_time:199889ms step_avg:118.35ms
step:1690/2245 train_time:200012ms step_avg:118.35ms
step:1691/2245 train_time:200129ms step_avg:118.35ms
step:1692/2245 train_time:200251ms step_avg:118.35ms
step:1693/2245 train_time:200368ms step_avg:118.35ms
step:1694/2245 train_time:200491ms step_avg:118.35ms
step:1695/2245 train_time:200607ms step_avg:118.35ms
step:1696/2245 train_time:200730ms step_avg:118.36ms
step:1697/2245 train_time:200847ms step_avg:118.35ms
step:1698/2245 train_time:200970ms step_avg:118.36ms
step:1699/2245 train_time:201087ms step_avg:118.36ms
step:1700/2245 train_time:201210ms step_avg:118.36ms
step:1701/2245 train_time:201326ms step_avg:118.36ms
step:1702/2245 train_time:201450ms step_avg:118.36ms
step:1703/2245 train_time:201566ms step_avg:118.36ms
step:1704/2245 train_time:201689ms step_avg:118.36ms
step:1705/2245 train_time:201806ms step_avg:118.36ms
step:1706/2245 train_time:201930ms step_avg:118.36ms
step:1707/2245 train_time:202046ms step_avg:118.36ms
step:1708/2245 train_time:202170ms step_avg:118.37ms
step:1709/2245 train_time:202286ms step_avg:118.37ms
step:1710/2245 train_time:202409ms step_avg:118.37ms
step:1711/2245 train_time:202526ms step_avg:118.37ms
step:1712/2245 train_time:202650ms step_avg:118.37ms
step:1713/2245 train_time:202767ms step_avg:118.37ms
step:1714/2245 train_time:202890ms step_avg:118.37ms
step:1715/2245 train_time:203007ms step_avg:118.37ms
step:1716/2245 train_time:203131ms step_avg:118.37ms
step:1717/2245 train_time:203248ms step_avg:118.37ms
step:1718/2245 train_time:203370ms step_avg:118.38ms
step:1719/2245 train_time:203487ms step_avg:118.38ms
step:1720/2245 train_time:203610ms step_avg:118.38ms
step:1721/2245 train_time:203726ms step_avg:118.38ms
step:1722/2245 train_time:203850ms step_avg:118.38ms
step:1723/2245 train_time:203967ms step_avg:118.38ms
step:1724/2245 train_time:204091ms step_avg:118.38ms
step:1725/2245 train_time:204208ms step_avg:118.38ms
step:1726/2245 train_time:204330ms step_avg:118.38ms
step:1727/2245 train_time:204447ms step_avg:118.38ms
step:1728/2245 train_time:204570ms step_avg:118.39ms
step:1729/2245 train_time:204686ms step_avg:118.38ms
step:1730/2245 train_time:204810ms step_avg:118.39ms
step:1731/2245 train_time:204926ms step_avg:118.39ms
step:1732/2245 train_time:205050ms step_avg:118.39ms
step:1733/2245 train_time:205167ms step_avg:118.39ms
step:1734/2245 train_time:205290ms step_avg:118.39ms
step:1735/2245 train_time:205407ms step_avg:118.39ms
step:1736/2245 train_time:205530ms step_avg:118.39ms
step:1737/2245 train_time:205647ms step_avg:118.39ms
step:1738/2245 train_time:205770ms step_avg:118.39ms
step:1739/2245 train_time:205886ms step_avg:118.39ms
step:1740/2245 train_time:206010ms step_avg:118.40ms
step:1741/2245 train_time:206126ms step_avg:118.40ms
step:1742/2245 train_time:206249ms step_avg:118.40ms
step:1743/2245 train_time:206366ms step_avg:118.40ms
step:1744/2245 train_time:206490ms step_avg:118.40ms
step:1745/2245 train_time:206607ms step_avg:118.40ms
step:1746/2245 train_time:206730ms step_avg:118.40ms
step:1747/2245 train_time:206847ms step_avg:118.40ms
step:1748/2245 train_time:206970ms step_avg:118.40ms
step:1749/2245 train_time:207087ms step_avg:118.40ms
step:1750/2245 train_time:207211ms step_avg:118.41ms
step:1750/2245 val_loss:3.3813 train_time:207277ms step_avg:118.44ms
step:1751/2245 train_time:207328ms step_avg:118.41ms
step:1752/2245 train_time:207450ms step_avg:118.41ms
step:1753/2245 train_time:207566ms step_avg:118.41ms
step:1754/2245 train_time:207689ms step_avg:118.41ms
step:1755/2245 train_time:207806ms step_avg:118.41ms
step:1756/2245 train_time:207928ms step_avg:118.41ms
step:1757/2245 train_time:208044ms step_avg:118.41ms
step:1758/2245 train_time:208167ms step_avg:118.41ms
step:1759/2245 train_time:208283ms step_avg:118.41ms
step:1760/2245 train_time:208406ms step_avg:118.41ms
step:1761/2245 train_time:208522ms step_avg:118.41ms
step:1762/2245 train_time:208645ms step_avg:118.41ms
step:1763/2245 train_time:208761ms step_avg:118.41ms
step:1764/2245 train_time:208884ms step_avg:118.42ms
step:1765/2245 train_time:209001ms step_avg:118.41ms
step:1766/2245 train_time:209124ms step_avg:118.42ms
step:1767/2245 train_time:209240ms step_avg:118.42ms
step:1768/2245 train_time:209363ms step_avg:118.42ms
step:1769/2245 train_time:209479ms step_avg:118.42ms
step:1770/2245 train_time:209601ms step_avg:118.42ms
step:1771/2245 train_time:209718ms step_avg:118.42ms
step:1772/2245 train_time:209841ms step_avg:118.42ms
step:1773/2245 train_time:209957ms step_avg:118.42ms
step:1774/2245 train_time:210080ms step_avg:118.42ms
step:1775/2245 train_time:210196ms step_avg:118.42ms
step:1776/2245 train_time:210319ms step_avg:118.42ms
step:1777/2245 train_time:210436ms step_avg:118.42ms
step:1778/2245 train_time:210560ms step_avg:118.43ms
step:1779/2245 train_time:210676ms step_avg:118.42ms
step:1780/2245 train_time:210799ms step_avg:118.43ms
step:1781/2245 train_time:210915ms step_avg:118.43ms
step:1782/2245 train_time:211039ms step_avg:118.43ms
step:1783/2245 train_time:211155ms step_avg:118.43ms
step:1784/2245 train_time:211278ms step_avg:118.43ms
step:1785/2245 train_time:211394ms step_avg:118.43ms
step:1786/2245 train_time:211517ms step_avg:118.43ms
step:1787/2245 train_time:211633ms step_avg:118.43ms
step:1788/2245 train_time:211757ms step_avg:118.43ms
step:1789/2245 train_time:211873ms step_avg:118.43ms
step:1790/2245 train_time:211997ms step_avg:118.43ms
step:1791/2245 train_time:212114ms step_avg:118.43ms
step:1792/2245 train_time:212237ms step_avg:118.44ms
step:1793/2245 train_time:212354ms step_avg:118.43ms
step:1794/2245 train_time:212478ms step_avg:118.44ms
step:1795/2245 train_time:212593ms step_avg:118.44ms
step:1796/2245 train_time:212716ms step_avg:118.44ms
step:1797/2245 train_time:212832ms step_avg:118.44ms
step:1798/2245 train_time:212956ms step_avg:118.44ms
step:1799/2245 train_time:213073ms step_avg:118.44ms
step:1800/2245 train_time:213195ms step_avg:118.44ms
step:1801/2245 train_time:213313ms step_avg:118.44ms
step:1802/2245 train_time:213435ms step_avg:118.44ms
step:1803/2245 train_time:213552ms step_avg:118.44ms
step:1804/2245 train_time:213675ms step_avg:118.45ms
step:1805/2245 train_time:213791ms step_avg:118.44ms
step:1806/2245 train_time:213914ms step_avg:118.45ms
step:1807/2245 train_time:214031ms step_avg:118.45ms
step:1808/2245 train_time:214154ms step_avg:118.45ms
step:1809/2245 train_time:214271ms step_avg:118.45ms
step:1810/2245 train_time:214395ms step_avg:118.45ms
step:1811/2245 train_time:214512ms step_avg:118.45ms
step:1812/2245 train_time:214636ms step_avg:118.45ms
step:1813/2245 train_time:214752ms step_avg:118.45ms
step:1814/2245 train_time:214875ms step_avg:118.45ms
step:1815/2245 train_time:214992ms step_avg:118.45ms
step:1816/2245 train_time:215115ms step_avg:118.46ms
step:1817/2245 train_time:215232ms step_avg:118.45ms
step:1818/2245 train_time:215355ms step_avg:118.46ms
step:1819/2245 train_time:215471ms step_avg:118.46ms
step:1820/2245 train_time:215595ms step_avg:118.46ms
step:1821/2245 train_time:215712ms step_avg:118.46ms
step:1822/2245 train_time:215835ms step_avg:118.46ms
step:1823/2245 train_time:215951ms step_avg:118.46ms
step:1824/2245 train_time:216074ms step_avg:118.46ms
step:1825/2245 train_time:216190ms step_avg:118.46ms
step:1826/2245 train_time:216312ms step_avg:118.46ms
step:1827/2245 train_time:216428ms step_avg:118.46ms
step:1828/2245 train_time:216551ms step_avg:118.46ms
step:1829/2245 train_time:216669ms step_avg:118.46ms
step:1830/2245 train_time:216792ms step_avg:118.47ms
step:1831/2245 train_time:216908ms step_avg:118.46ms
step:1832/2245 train_time:217031ms step_avg:118.47ms
step:1833/2245 train_time:217148ms step_avg:118.47ms
step:1834/2245 train_time:217271ms step_avg:118.47ms
step:1835/2245 train_time:217387ms step_avg:118.47ms
step:1836/2245 train_time:217511ms step_avg:118.47ms
step:1837/2245 train_time:217628ms step_avg:118.47ms
step:1838/2245 train_time:217751ms step_avg:118.47ms
step:1839/2245 train_time:217867ms step_avg:118.47ms
step:1840/2245 train_time:217990ms step_avg:118.47ms
step:1841/2245 train_time:218107ms step_avg:118.47ms
step:1842/2245 train_time:218230ms step_avg:118.47ms
step:1843/2245 train_time:218347ms step_avg:118.47ms
step:1844/2245 train_time:218469ms step_avg:118.48ms
step:1845/2245 train_time:218586ms step_avg:118.47ms
step:1846/2245 train_time:218708ms step_avg:118.48ms
step:1847/2245 train_time:218825ms step_avg:118.48ms
step:1848/2245 train_time:218948ms step_avg:118.48ms
step:1849/2245 train_time:219064ms step_avg:118.48ms
step:1850/2245 train_time:219187ms step_avg:118.48ms
step:1851/2245 train_time:219303ms step_avg:118.48ms
step:1852/2245 train_time:219425ms step_avg:118.48ms
step:1853/2245 train_time:219542ms step_avg:118.48ms
step:1854/2245 train_time:219665ms step_avg:118.48ms
step:1855/2245 train_time:219782ms step_avg:118.48ms
step:1856/2245 train_time:219905ms step_avg:118.48ms
step:1857/2245 train_time:220021ms step_avg:118.48ms
step:1858/2245 train_time:220144ms step_avg:118.48ms
step:1859/2245 train_time:220261ms step_avg:118.48ms
step:1860/2245 train_time:220384ms step_avg:118.49ms
step:1861/2245 train_time:220500ms step_avg:118.48ms
step:1862/2245 train_time:220623ms step_avg:118.49ms
step:1863/2245 train_time:220740ms step_avg:118.49ms
step:1864/2245 train_time:220862ms step_avg:118.49ms
step:1865/2245 train_time:220979ms step_avg:118.49ms
step:1866/2245 train_time:221101ms step_avg:118.49ms
step:1867/2245 train_time:221218ms step_avg:118.49ms
step:1868/2245 train_time:221341ms step_avg:118.49ms
step:1869/2245 train_time:221458ms step_avg:118.49ms
step:1870/2245 train_time:221580ms step_avg:118.49ms
step:1871/2245 train_time:221697ms step_avg:118.49ms
step:1872/2245 train_time:221820ms step_avg:118.49ms
step:1873/2245 train_time:221936ms step_avg:118.49ms
step:1874/2245 train_time:222060ms step_avg:118.50ms
step:1875/2245 train_time:222176ms step_avg:118.49ms
step:1876/2245 train_time:222299ms step_avg:118.50ms
step:1877/2245 train_time:222416ms step_avg:118.50ms
step:1878/2245 train_time:222538ms step_avg:118.50ms
step:1879/2245 train_time:222655ms step_avg:118.50ms
step:1880/2245 train_time:222778ms step_avg:118.50ms
step:1881/2245 train_time:222894ms step_avg:118.50ms
step:1882/2245 train_time:223017ms step_avg:118.50ms
step:1883/2245 train_time:223134ms step_avg:118.50ms
step:1884/2245 train_time:223257ms step_avg:118.50ms
step:1885/2245 train_time:223374ms step_avg:118.50ms
step:1886/2245 train_time:223498ms step_avg:118.50ms
step:1887/2245 train_time:223614ms step_avg:118.50ms
step:1888/2245 train_time:223737ms step_avg:118.50ms
step:1889/2245 train_time:223853ms step_avg:118.50ms
step:1890/2245 train_time:223977ms step_avg:118.51ms
step:1891/2245 train_time:224094ms step_avg:118.51ms
step:1892/2245 train_time:224217ms step_avg:118.51ms
step:1893/2245 train_time:224334ms step_avg:118.51ms
step:1894/2245 train_time:224457ms step_avg:118.51ms
step:1895/2245 train_time:224574ms step_avg:118.51ms
step:1896/2245 train_time:224697ms step_avg:118.51ms
step:1897/2245 train_time:224813ms step_avg:118.51ms
step:1898/2245 train_time:224936ms step_avg:118.51ms
step:1899/2245 train_time:225052ms step_avg:118.51ms
step:1900/2245 train_time:225175ms step_avg:118.51ms
step:1901/2245 train_time:225291ms step_avg:118.51ms
step:1902/2245 train_time:225415ms step_avg:118.51ms
step:1903/2245 train_time:225531ms step_avg:118.51ms
step:1904/2245 train_time:225655ms step_avg:118.52ms
step:1905/2245 train_time:225772ms step_avg:118.52ms
step:1906/2245 train_time:225895ms step_avg:118.52ms
step:1907/2245 train_time:226012ms step_avg:118.52ms
step:1908/2245 train_time:226135ms step_avg:118.52ms
step:1909/2245 train_time:226252ms step_avg:118.52ms
step:1910/2245 train_time:226374ms step_avg:118.52ms
step:1911/2245 train_time:226491ms step_avg:118.52ms
step:1912/2245 train_time:226614ms step_avg:118.52ms
step:1913/2245 train_time:226730ms step_avg:118.52ms
step:1914/2245 train_time:226854ms step_avg:118.52ms
step:1915/2245 train_time:226970ms step_avg:118.52ms
step:1916/2245 train_time:227094ms step_avg:118.52ms
step:1917/2245 train_time:227210ms step_avg:118.52ms
step:1918/2245 train_time:227333ms step_avg:118.53ms
step:1919/2245 train_time:227450ms step_avg:118.53ms
step:1920/2245 train_time:227573ms step_avg:118.53ms
step:1921/2245 train_time:227690ms step_avg:118.53ms
step:1922/2245 train_time:227814ms step_avg:118.53ms
step:1923/2245 train_time:227931ms step_avg:118.53ms
step:1924/2245 train_time:228054ms step_avg:118.53ms
step:1925/2245 train_time:228172ms step_avg:118.53ms
step:1926/2245 train_time:228295ms step_avg:118.53ms
step:1927/2245 train_time:228412ms step_avg:118.53ms
step:1928/2245 train_time:228535ms step_avg:118.53ms
step:1929/2245 train_time:228652ms step_avg:118.53ms
step:1930/2245 train_time:228775ms step_avg:118.54ms
step:1931/2245 train_time:228892ms step_avg:118.54ms
step:1932/2245 train_time:229015ms step_avg:118.54ms
step:1933/2245 train_time:229132ms step_avg:118.54ms
step:1934/2245 train_time:229255ms step_avg:118.54ms
step:1935/2245 train_time:229372ms step_avg:118.54ms
step:1936/2245 train_time:229496ms step_avg:118.54ms
step:1937/2245 train_time:229612ms step_avg:118.54ms
step:1938/2245 train_time:229735ms step_avg:118.54ms
step:1939/2245 train_time:229852ms step_avg:118.54ms
step:1940/2245 train_time:229975ms step_avg:118.54ms
step:1941/2245 train_time:230092ms step_avg:118.54ms
step:1942/2245 train_time:230215ms step_avg:118.55ms
step:1943/2245 train_time:230332ms step_avg:118.54ms
step:1944/2245 train_time:230455ms step_avg:118.55ms
step:1945/2245 train_time:230572ms step_avg:118.55ms
step:1946/2245 train_time:230695ms step_avg:118.55ms
step:1947/2245 train_time:230811ms step_avg:118.55ms
step:1948/2245 train_time:230934ms step_avg:118.55ms
step:1949/2245 train_time:231051ms step_avg:118.55ms
step:1950/2245 train_time:231174ms step_avg:118.55ms
step:1951/2245 train_time:231291ms step_avg:118.55ms
step:1952/2245 train_time:231414ms step_avg:118.55ms
step:1953/2245 train_time:231530ms step_avg:118.55ms
step:1954/2245 train_time:231653ms step_avg:118.55ms
step:1955/2245 train_time:231770ms step_avg:118.55ms
step:1956/2245 train_time:231892ms step_avg:118.55ms
step:1957/2245 train_time:232009ms step_avg:118.55ms
step:1958/2245 train_time:232132ms step_avg:118.56ms
step:1959/2245 train_time:232249ms step_avg:118.55ms
step:1960/2245 train_time:232372ms step_avg:118.56ms
step:1961/2245 train_time:232489ms step_avg:118.56ms
step:1962/2245 train_time:232612ms step_avg:118.56ms
step:1963/2245 train_time:232729ms step_avg:118.56ms
step:1964/2245 train_time:232852ms step_avg:118.56ms
step:1965/2245 train_time:232969ms step_avg:118.56ms
step:1966/2245 train_time:233092ms step_avg:118.56ms
step:1967/2245 train_time:233209ms step_avg:118.56ms
step:1968/2245 train_time:233333ms step_avg:118.56ms
step:1969/2245 train_time:233449ms step_avg:118.56ms
step:1970/2245 train_time:233572ms step_avg:118.56ms
step:1971/2245 train_time:233689ms step_avg:118.56ms
step:1972/2245 train_time:233812ms step_avg:118.57ms
step:1973/2245 train_time:233929ms step_avg:118.57ms
step:1974/2245 train_time:234053ms step_avg:118.57ms
step:1975/2245 train_time:234169ms step_avg:118.57ms
step:1976/2245 train_time:234292ms step_avg:118.57ms
step:1977/2245 train_time:234408ms step_avg:118.57ms
step:1978/2245 train_time:234531ms step_avg:118.57ms
step:1979/2245 train_time:234648ms step_avg:118.57ms
step:1980/2245 train_time:234771ms step_avg:118.57ms
step:1981/2245 train_time:234888ms step_avg:118.57ms
step:1982/2245 train_time:235010ms step_avg:118.57ms
step:1983/2245 train_time:235128ms step_avg:118.57ms
step:1984/2245 train_time:235251ms step_avg:118.57ms
step:1985/2245 train_time:235368ms step_avg:118.57ms
step:1986/2245 train_time:235491ms step_avg:118.58ms
step:1987/2245 train_time:235608ms step_avg:118.57ms
step:1988/2245 train_time:235731ms step_avg:118.58ms
step:1989/2245 train_time:235848ms step_avg:118.58ms
step:1990/2245 train_time:235971ms step_avg:118.58ms
step:1991/2245 train_time:236089ms step_avg:118.58ms
step:1992/2245 train_time:236212ms step_avg:118.58ms
step:1993/2245 train_time:236330ms step_avg:118.58ms
step:1994/2245 train_time:236453ms step_avg:118.58ms
step:1995/2245 train_time:236570ms step_avg:118.58ms
step:1996/2245 train_time:236693ms step_avg:118.58ms
step:1997/2245 train_time:236810ms step_avg:118.58ms
step:1998/2245 train_time:236933ms step_avg:118.58ms
step:1999/2245 train_time:237051ms step_avg:118.58ms
step:2000/2245 train_time:237174ms step_avg:118.59ms
step:2000/2245 val_loss:3.3267 train_time:237240ms step_avg:118.62ms
step:2001/2245 train_time:237291ms step_avg:118.59ms
step:2002/2245 train_time:237413ms step_avg:118.59ms
step:2003/2245 train_time:237529ms step_avg:118.59ms
step:2004/2245 train_time:237652ms step_avg:118.59ms
step:2005/2245 train_time:237768ms step_avg:118.59ms
step:2006/2245 train_time:237891ms step_avg:118.59ms
step:2007/2245 train_time:238008ms step_avg:118.59ms
step:2008/2245 train_time:238130ms step_avg:118.59ms
step:2009/2245 train_time:238247ms step_avg:118.59ms
step:2010/2245 train_time:238372ms step_avg:118.59ms
step:2011/2245 train_time:238488ms step_avg:118.59ms
step:2012/2245 train_time:238611ms step_avg:118.59ms
step:2013/2245 train_time:238728ms step_avg:118.59ms
step:2014/2245 train_time:238850ms step_avg:118.60ms
step:2015/2245 train_time:238967ms step_avg:118.59ms
step:2016/2245 train_time:239089ms step_avg:118.60ms
step:2017/2245 train_time:239205ms step_avg:118.59ms
step:2018/2245 train_time:239328ms step_avg:118.60ms
step:2019/2245 train_time:239444ms step_avg:118.60ms
step:2020/2245 train_time:239567ms step_avg:118.60ms
step:2021/2245 train_time:239684ms step_avg:118.60ms
step:2022/2245 train_time:239807ms step_avg:118.60ms
step:2023/2245 train_time:239924ms step_avg:118.60ms
step:2024/2245 train_time:240046ms step_avg:118.60ms
step:2025/2245 train_time:240163ms step_avg:118.60ms
step:2026/2245 train_time:240285ms step_avg:118.60ms
step:2027/2245 train_time:240402ms step_avg:118.60ms
step:2028/2245 train_time:240525ms step_avg:118.60ms
step:2029/2245 train_time:240641ms step_avg:118.60ms
step:2030/2245 train_time:240763ms step_avg:118.60ms
step:2031/2245 train_time:240880ms step_avg:118.60ms
step:2032/2245 train_time:241003ms step_avg:118.60ms
step:2033/2245 train_time:241119ms step_avg:118.60ms
step:2034/2245 train_time:241242ms step_avg:118.60ms
step:2035/2245 train_time:241359ms step_avg:118.60ms
step:2036/2245 train_time:241482ms step_avg:118.61ms
step:2037/2245 train_time:241599ms step_avg:118.61ms
step:2038/2245 train_time:241722ms step_avg:118.61ms
step:2039/2245 train_time:241838ms step_avg:118.61ms
step:2040/2245 train_time:241960ms step_avg:118.61ms
step:2041/2245 train_time:242078ms step_avg:118.61ms
step:2042/2245 train_time:242200ms step_avg:118.61ms
step:2043/2245 train_time:242317ms step_avg:118.61ms
step:2044/2245 train_time:242440ms step_avg:118.61ms
step:2045/2245 train_time:242556ms step_avg:118.61ms
step:2046/2245 train_time:242679ms step_avg:118.61ms
step:2047/2245 train_time:242796ms step_avg:118.61ms
step:2048/2245 train_time:242920ms step_avg:118.61ms
step:2049/2245 train_time:243036ms step_avg:118.61ms
step:2050/2245 train_time:243160ms step_avg:118.61ms
step:2051/2245 train_time:243276ms step_avg:118.61ms
step:2052/2245 train_time:243400ms step_avg:118.62ms
step:2053/2245 train_time:243516ms step_avg:118.61ms
step:2054/2245 train_time:243640ms step_avg:118.62ms
step:2055/2245 train_time:243757ms step_avg:118.62ms
step:2056/2245 train_time:243880ms step_avg:118.62ms
step:2057/2245 train_time:243997ms step_avg:118.62ms
step:2058/2245 train_time:244120ms step_avg:118.62ms
step:2059/2245 train_time:244237ms step_avg:118.62ms
step:2060/2245 train_time:244360ms step_avg:118.62ms
step:2061/2245 train_time:244477ms step_avg:118.62ms
step:2062/2245 train_time:244599ms step_avg:118.62ms
step:2063/2245 train_time:244716ms step_avg:118.62ms
step:2064/2245 train_time:244839ms step_avg:118.62ms
step:2065/2245 train_time:244955ms step_avg:118.62ms
step:2066/2245 train_time:245078ms step_avg:118.62ms
step:2067/2245 train_time:245194ms step_avg:118.62ms
step:2068/2245 train_time:245318ms step_avg:118.63ms
step:2069/2245 train_time:245434ms step_avg:118.62ms
step:2070/2245 train_time:245558ms step_avg:118.63ms
step:2071/2245 train_time:245674ms step_avg:118.63ms
step:2072/2245 train_time:245797ms step_avg:118.63ms
step:2073/2245 train_time:245914ms step_avg:118.63ms
step:2074/2245 train_time:246037ms step_avg:118.63ms
step:2075/2245 train_time:246154ms step_avg:118.63ms
step:2076/2245 train_time:246277ms step_avg:118.63ms
step:2077/2245 train_time:246394ms step_avg:118.63ms
step:2078/2245 train_time:246516ms step_avg:118.63ms
step:2079/2245 train_time:246633ms step_avg:118.63ms
step:2080/2245 train_time:246756ms step_avg:118.63ms
step:2081/2245 train_time:246873ms step_avg:118.63ms
step:2082/2245 train_time:246996ms step_avg:118.63ms
step:2083/2245 train_time:247113ms step_avg:118.63ms
step:2084/2245 train_time:247236ms step_avg:118.64ms
step:2085/2245 train_time:247352ms step_avg:118.63ms
step:2086/2245 train_time:247476ms step_avg:118.64ms
step:2087/2245 train_time:247592ms step_avg:118.64ms
step:2088/2245 train_time:247715ms step_avg:118.64ms
step:2089/2245 train_time:247832ms step_avg:118.64ms
step:2090/2245 train_time:247955ms step_avg:118.64ms
step:2091/2245 train_time:248071ms step_avg:118.64ms
step:2092/2245 train_time:248193ms step_avg:118.64ms
step:2093/2245 train_time:248311ms step_avg:118.64ms
step:2094/2245 train_time:248435ms step_avg:118.64ms
step:2095/2245 train_time:248551ms step_avg:118.64ms
step:2096/2245 train_time:248675ms step_avg:118.64ms
step:2097/2245 train_time:248792ms step_avg:118.64ms
step:2098/2245 train_time:248916ms step_avg:118.64ms
step:2099/2245 train_time:249032ms step_avg:118.64ms
step:2100/2245 train_time:249156ms step_avg:118.65ms
step:2101/2245 train_time:249272ms step_avg:118.64ms
step:2102/2245 train_time:249395ms step_avg:118.65ms
step:2103/2245 train_time:249512ms step_avg:118.65ms
step:2104/2245 train_time:249635ms step_avg:118.65ms
step:2105/2245 train_time:249752ms step_avg:118.65ms
step:2106/2245 train_time:249876ms step_avg:118.65ms
step:2107/2245 train_time:249992ms step_avg:118.65ms
step:2108/2245 train_time:250116ms step_avg:118.65ms
step:2109/2245 train_time:250232ms step_avg:118.65ms
step:2110/2245 train_time:250355ms step_avg:118.65ms
step:2111/2245 train_time:250472ms step_avg:118.65ms
step:2112/2245 train_time:250595ms step_avg:118.65ms
step:2113/2245 train_time:250712ms step_avg:118.65ms
step:2114/2245 train_time:250835ms step_avg:118.65ms
step:2115/2245 train_time:250952ms step_avg:118.65ms
step:2116/2245 train_time:251074ms step_avg:118.66ms
step:2117/2245 train_time:251191ms step_avg:118.65ms
step:2118/2245 train_time:251314ms step_avg:118.66ms
step:2119/2245 train_time:251431ms step_avg:118.66ms
step:2120/2245 train_time:251554ms step_avg:118.66ms
step:2121/2245 train_time:251671ms step_avg:118.66ms
step:2122/2245 train_time:251794ms step_avg:118.66ms
step:2123/2245 train_time:251912ms step_avg:118.66ms
step:2124/2245 train_time:252035ms step_avg:118.66ms
step:2125/2245 train_time:252152ms step_avg:118.66ms
step:2126/2245 train_time:252275ms step_avg:118.66ms
step:2127/2245 train_time:252391ms step_avg:118.66ms
step:2128/2245 train_time:252515ms step_avg:118.66ms
step:2129/2245 train_time:252632ms step_avg:118.66ms
step:2130/2245 train_time:252756ms step_avg:118.66ms
step:2131/2245 train_time:252872ms step_avg:118.66ms
step:2132/2245 train_time:252996ms step_avg:118.67ms
step:2133/2245 train_time:253113ms step_avg:118.67ms
step:2134/2245 train_time:253236ms step_avg:118.67ms
step:2135/2245 train_time:253353ms step_avg:118.67ms
step:2136/2245 train_time:253476ms step_avg:118.67ms
step:2137/2245 train_time:253592ms step_avg:118.67ms
step:2138/2245 train_time:253715ms step_avg:118.67ms
step:2139/2245 train_time:253831ms step_avg:118.67ms
step:2140/2245 train_time:253954ms step_avg:118.67ms
step:2141/2245 train_time:254071ms step_avg:118.67ms
step:2142/2245 train_time:254194ms step_avg:118.67ms
step:2143/2245 train_time:254311ms step_avg:118.67ms
step:2144/2245 train_time:254434ms step_avg:118.67ms
step:2145/2245 train_time:254551ms step_avg:118.67ms
step:2146/2245 train_time:254674ms step_avg:118.67ms
step:2147/2245 train_time:254791ms step_avg:118.67ms
step:2148/2245 train_time:254914ms step_avg:118.68ms
step:2149/2245 train_time:255032ms step_avg:118.67ms
step:2150/2245 train_time:255155ms step_avg:118.68ms
step:2151/2245 train_time:255272ms step_avg:118.68ms
step:2152/2245 train_time:255395ms step_avg:118.68ms
step:2153/2245 train_time:255512ms step_avg:118.68ms
step:2154/2245 train_time:255636ms step_avg:118.68ms
step:2155/2245 train_time:255752ms step_avg:118.68ms
step:2156/2245 train_time:255875ms step_avg:118.68ms
step:2157/2245 train_time:255992ms step_avg:118.68ms
step:2158/2245 train_time:256115ms step_avg:118.68ms
step:2159/2245 train_time:256232ms step_avg:118.68ms
step:2160/2245 train_time:256356ms step_avg:118.68ms
step:2161/2245 train_time:256473ms step_avg:118.68ms
step:2162/2245 train_time:256596ms step_avg:118.68ms
step:2163/2245 train_time:256713ms step_avg:118.68ms
step:2164/2245 train_time:256836ms step_avg:118.69ms
step:2165/2245 train_time:256953ms step_avg:118.69ms
step:2166/2245 train_time:257076ms step_avg:118.69ms
step:2167/2245 train_time:257192ms step_avg:118.69ms
step:2168/2245 train_time:257316ms step_avg:118.69ms
step:2169/2245 train_time:257432ms step_avg:118.69ms
step:2170/2245 train_time:257556ms step_avg:118.69ms
step:2171/2245 train_time:257672ms step_avg:118.69ms
step:2172/2245 train_time:257796ms step_avg:118.69ms
step:2173/2245 train_time:257913ms step_avg:118.69ms
step:2174/2245 train_time:258036ms step_avg:118.69ms
step:2175/2245 train_time:258154ms step_avg:118.69ms
step:2176/2245 train_time:258276ms step_avg:118.69ms
step:2177/2245 train_time:258392ms step_avg:118.69ms
step:2178/2245 train_time:258515ms step_avg:118.69ms
step:2179/2245 train_time:258631ms step_avg:118.69ms
step:2180/2245 train_time:258755ms step_avg:118.69ms
step:2181/2245 train_time:258872ms step_avg:118.69ms
step:2182/2245 train_time:258995ms step_avg:118.70ms
step:2183/2245 train_time:259112ms step_avg:118.70ms
step:2184/2245 train_time:259236ms step_avg:118.70ms
step:2185/2245 train_time:259353ms step_avg:118.70ms
step:2186/2245 train_time:259476ms step_avg:118.70ms
step:2187/2245 train_time:259592ms step_avg:118.70ms
step:2188/2245 train_time:259715ms step_avg:118.70ms
step:2189/2245 train_time:259833ms step_avg:118.70ms
step:2190/2245 train_time:259956ms step_avg:118.70ms
step:2191/2245 train_time:260072ms step_avg:118.70ms
step:2192/2245 train_time:260196ms step_avg:118.70ms
step:2193/2245 train_time:260313ms step_avg:118.70ms
step:2194/2245 train_time:260436ms step_avg:118.70ms
step:2195/2245 train_time:260552ms step_avg:118.70ms
step:2196/2245 train_time:260675ms step_avg:118.70ms
step:2197/2245 train_time:260791ms step_avg:118.70ms
step:2198/2245 train_time:260914ms step_avg:118.71ms
step:2199/2245 train_time:261031ms step_avg:118.70ms
step:2200/2245 train_time:261154ms step_avg:118.71ms
step:2201/2245 train_time:261271ms step_avg:118.71ms
step:2202/2245 train_time:261394ms step_avg:118.71ms
step:2203/2245 train_time:261512ms step_avg:118.71ms
step:2204/2245 train_time:261635ms step_avg:118.71ms
step:2205/2245 train_time:261752ms step_avg:118.71ms
step:2206/2245 train_time:261875ms step_avg:118.71ms
step:2207/2245 train_time:261992ms step_avg:118.71ms
step:2208/2245 train_time:262115ms step_avg:118.71ms
step:2209/2245 train_time:262233ms step_avg:118.71ms
step:2210/2245 train_time:262356ms step_avg:118.71ms
step:2211/2245 train_time:262473ms step_avg:118.71ms
step:2212/2245 train_time:262596ms step_avg:118.71ms
step:2213/2245 train_time:262714ms step_avg:118.71ms
step:2214/2245 train_time:262837ms step_avg:118.72ms
step:2215/2245 train_time:262954ms step_avg:118.72ms
step:2216/2245 train_time:263077ms step_avg:118.72ms
step:2217/2245 train_time:263195ms step_avg:118.72ms
step:2218/2245 train_time:263318ms step_avg:118.72ms
step:2219/2245 train_time:263435ms step_avg:118.72ms
step:2220/2245 train_time:263559ms step_avg:118.72ms
step:2221/2245 train_time:263676ms step_avg:118.72ms
step:2222/2245 train_time:263799ms step_avg:118.72ms
step:2223/2245 train_time:263916ms step_avg:118.72ms
step:2224/2245 train_time:264039ms step_avg:118.72ms
step:2225/2245 train_time:264155ms step_avg:118.72ms
step:2226/2245 train_time:264279ms step_avg:118.72ms
step:2227/2245 train_time:264396ms step_avg:118.72ms
step:2228/2245 train_time:264520ms step_avg:118.73ms
step:2229/2245 train_time:264637ms step_avg:118.72ms
step:2230/2245 train_time:264760ms step_avg:118.73ms
step:2231/2245 train_time:264877ms step_avg:118.73ms
step:2232/2245 train_time:265000ms step_avg:118.73ms
step:2233/2245 train_time:265117ms step_avg:118.73ms
step:2234/2245 train_time:265240ms step_avg:118.73ms
step:2235/2245 train_time:265357ms step_avg:118.73ms
step:2236/2245 train_time:265480ms step_avg:118.73ms
step:2237/2245 train_time:265597ms step_avg:118.73ms
step:2238/2245 train_time:265720ms step_avg:118.73ms
step:2239/2245 train_time:265837ms step_avg:118.73ms
step:2240/2245 train_time:265960ms step_avg:118.73ms
step:2241/2245 train_time:266077ms step_avg:118.73ms
step:2242/2245 train_time:266201ms step_avg:118.73ms
step:2243/2245 train_time:266318ms step_avg:118.73ms
step:2244/2245 train_time:266441ms step_avg:118.73ms
step:2245/2245 train_time:266558ms step_avg:118.73ms
step:2245/2245 val_loss:3.2807 train_time:266627ms step_avg:118.76ms
peak memory allocated: 29050 MiB reserved: 44436 MiB

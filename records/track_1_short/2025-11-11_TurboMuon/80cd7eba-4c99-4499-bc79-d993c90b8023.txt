import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 09:47:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   42C    P0            117W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   42C    P0            118W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   42C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3337074      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3337075      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3337076      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3337077      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3337075      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3337076      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3337077      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:161ms step_avg:161.14ms
step:2/2245 train_time:215ms step_avg:107.46ms
step:3/2245 train_time:324ms step_avg:108.05ms
step:4/2245 train_time:442ms step_avg:110.60ms
step:5/2245 train_time:557ms step_avg:111.50ms
step:6/2245 train_time:680ms step_avg:113.40ms
step:7/2245 train_time:797ms step_avg:113.84ms
step:8/2245 train_time:920ms step_avg:114.99ms
step:9/2245 train_time:1036ms step_avg:115.15ms
step:10/2245 train_time:1160ms step_avg:115.97ms
step:11/2245 train_time:1276ms step_avg:116.03ms
step:12/2245 train_time:1400ms step_avg:116.68ms
step:13/2245 train_time:1517ms step_avg:116.70ms
step:14/2245 train_time:1640ms step_avg:117.17ms
step:15/2245 train_time:1757ms step_avg:117.12ms
step:16/2245 train_time:1880ms step_avg:117.48ms
step:17/2245 train_time:1996ms step_avg:117.44ms
step:18/2245 train_time:2120ms step_avg:117.78ms
step:19/2245 train_time:2236ms step_avg:117.69ms
step:20/2245 train_time:2358ms step_avg:117.92ms
step:21/2245 train_time:2474ms step_avg:117.83ms
step:22/2245 train_time:2597ms step_avg:118.03ms
step:23/2245 train_time:2713ms step_avg:117.95ms
step:24/2245 train_time:2835ms step_avg:118.14ms
step:25/2245 train_time:2951ms step_avg:118.04ms
step:26/2245 train_time:3073ms step_avg:118.20ms
step:27/2245 train_time:3189ms step_avg:118.10ms
step:28/2245 train_time:3311ms step_avg:118.25ms
step:29/2245 train_time:3427ms step_avg:118.17ms
step:30/2245 train_time:3549ms step_avg:118.30ms
step:31/2245 train_time:3665ms step_avg:118.21ms
step:32/2245 train_time:3787ms step_avg:118.33ms
step:33/2245 train_time:3902ms step_avg:118.24ms
step:34/2245 train_time:4024ms step_avg:118.35ms
step:35/2245 train_time:4139ms step_avg:118.27ms
step:36/2245 train_time:4261ms step_avg:118.36ms
step:37/2245 train_time:4377ms step_avg:118.29ms
step:38/2245 train_time:4499ms step_avg:118.39ms
step:39/2245 train_time:4614ms step_avg:118.31ms
step:40/2245 train_time:4736ms step_avg:118.40ms
step:41/2245 train_time:4852ms step_avg:118.35ms
step:42/2245 train_time:4974ms step_avg:118.43ms
step:43/2245 train_time:5089ms step_avg:118.36ms
step:44/2245 train_time:5212ms step_avg:118.44ms
step:45/2245 train_time:5327ms step_avg:118.38ms
step:46/2245 train_time:5449ms step_avg:118.46ms
step:47/2245 train_time:5564ms step_avg:118.39ms
step:48/2245 train_time:5686ms step_avg:118.47ms
step:49/2245 train_time:5802ms step_avg:118.41ms
step:50/2245 train_time:5924ms step_avg:118.48ms
step:51/2245 train_time:6039ms step_avg:118.41ms
step:52/2245 train_time:6161ms step_avg:118.48ms
step:53/2245 train_time:6276ms step_avg:118.42ms
step:54/2245 train_time:6398ms step_avg:118.48ms
step:55/2245 train_time:6513ms step_avg:118.42ms
step:56/2245 train_time:6635ms step_avg:118.48ms
step:57/2245 train_time:6750ms step_avg:118.42ms
step:58/2245 train_time:6872ms step_avg:118.48ms
step:59/2245 train_time:6987ms step_avg:118.43ms
step:60/2245 train_time:7109ms step_avg:118.48ms
step:61/2245 train_time:7224ms step_avg:118.43ms
step:62/2245 train_time:7346ms step_avg:118.49ms
step:63/2245 train_time:7461ms step_avg:118.43ms
step:64/2245 train_time:7583ms step_avg:118.49ms
step:65/2245 train_time:7698ms step_avg:118.43ms
step:66/2245 train_time:7820ms step_avg:118.48ms
step:67/2245 train_time:7935ms step_avg:118.44ms
step:68/2245 train_time:8057ms step_avg:118.48ms
step:69/2245 train_time:8172ms step_avg:118.44ms
step:70/2245 train_time:8294ms step_avg:118.48ms
step:71/2245 train_time:8409ms step_avg:118.43ms
step:72/2245 train_time:8530ms step_avg:118.47ms
step:73/2245 train_time:8645ms step_avg:118.43ms
step:74/2245 train_time:8767ms step_avg:118.47ms
step:75/2245 train_time:8882ms step_avg:118.43ms
step:76/2245 train_time:9003ms step_avg:118.47ms
step:77/2245 train_time:9119ms step_avg:118.43ms
step:78/2245 train_time:9240ms step_avg:118.47ms
step:79/2245 train_time:9355ms step_avg:118.42ms
step:80/2245 train_time:9477ms step_avg:118.46ms
step:81/2245 train_time:9592ms step_avg:118.42ms
step:82/2245 train_time:9714ms step_avg:118.46ms
step:83/2245 train_time:9829ms step_avg:118.42ms
step:84/2245 train_time:9950ms step_avg:118.45ms
step:85/2245 train_time:10065ms step_avg:118.41ms
step:86/2245 train_time:10186ms step_avg:118.45ms
step:87/2245 train_time:10301ms step_avg:118.41ms
step:88/2245 train_time:10423ms step_avg:118.44ms
step:89/2245 train_time:10538ms step_avg:118.40ms
step:90/2245 train_time:10658ms step_avg:118.43ms
step:91/2245 train_time:10774ms step_avg:118.39ms
step:92/2245 train_time:10895ms step_avg:118.43ms
step:93/2245 train_time:11010ms step_avg:118.39ms
step:94/2245 train_time:11131ms step_avg:118.41ms
step:95/2245 train_time:11246ms step_avg:118.38ms
step:96/2245 train_time:11367ms step_avg:118.41ms
step:97/2245 train_time:11482ms step_avg:118.38ms
step:98/2245 train_time:11604ms step_avg:118.40ms
step:99/2245 train_time:11718ms step_avg:118.37ms
step:100/2245 train_time:11840ms step_avg:118.40ms
step:101/2245 train_time:11954ms step_avg:118.36ms
step:102/2245 train_time:12076ms step_avg:118.39ms
step:103/2245 train_time:12190ms step_avg:118.35ms
step:104/2245 train_time:12312ms step_avg:118.38ms
step:105/2245 train_time:12427ms step_avg:118.35ms
step:106/2245 train_time:12548ms step_avg:118.37ms
step:107/2245 train_time:12662ms step_avg:118.34ms
step:108/2245 train_time:12784ms step_avg:118.37ms
step:109/2245 train_time:12898ms step_avg:118.33ms
step:110/2245 train_time:13019ms step_avg:118.36ms
step:111/2245 train_time:13134ms step_avg:118.33ms
step:112/2245 train_time:13255ms step_avg:118.35ms
step:113/2245 train_time:13370ms step_avg:118.32ms
step:114/2245 train_time:13491ms step_avg:118.34ms
step:115/2245 train_time:13606ms step_avg:118.31ms
step:116/2245 train_time:13727ms step_avg:118.34ms
step:117/2245 train_time:13842ms step_avg:118.30ms
step:118/2245 train_time:13962ms step_avg:118.32ms
step:119/2245 train_time:14077ms step_avg:118.29ms
step:120/2245 train_time:14198ms step_avg:118.32ms
step:121/2245 train_time:14313ms step_avg:118.29ms
step:122/2245 train_time:14434ms step_avg:118.31ms
step:123/2245 train_time:14548ms step_avg:118.28ms
step:124/2245 train_time:14669ms step_avg:118.30ms
step:125/2245 train_time:14784ms step_avg:118.27ms
step:126/2245 train_time:14905ms step_avg:118.29ms
step:127/2245 train_time:15019ms step_avg:118.26ms
step:128/2245 train_time:15140ms step_avg:118.28ms
step:129/2245 train_time:15255ms step_avg:118.25ms
step:130/2245 train_time:15375ms step_avg:118.27ms
step:131/2245 train_time:15490ms step_avg:118.24ms
step:132/2245 train_time:15611ms step_avg:118.26ms
step:133/2245 train_time:15726ms step_avg:118.24ms
step:134/2245 train_time:15847ms step_avg:118.26ms
step:135/2245 train_time:15961ms step_avg:118.23ms
step:136/2245 train_time:16082ms step_avg:118.25ms
step:137/2245 train_time:16197ms step_avg:118.23ms
step:138/2245 train_time:16318ms step_avg:118.25ms
step:139/2245 train_time:16432ms step_avg:118.22ms
step:140/2245 train_time:16554ms step_avg:118.24ms
step:141/2245 train_time:16668ms step_avg:118.21ms
step:142/2245 train_time:16789ms step_avg:118.23ms
step:143/2245 train_time:16903ms step_avg:118.21ms
step:144/2245 train_time:17024ms step_avg:118.22ms
step:145/2245 train_time:17138ms step_avg:118.20ms
step:146/2245 train_time:17259ms step_avg:118.21ms
step:147/2245 train_time:17374ms step_avg:118.19ms
step:148/2245 train_time:17495ms step_avg:118.21ms
step:149/2245 train_time:17609ms step_avg:118.18ms
step:150/2245 train_time:17731ms step_avg:118.20ms
step:151/2245 train_time:17845ms step_avg:118.18ms
step:152/2245 train_time:17966ms step_avg:118.20ms
step:153/2245 train_time:18080ms step_avg:118.17ms
step:154/2245 train_time:18201ms step_avg:118.19ms
step:155/2245 train_time:18316ms step_avg:118.17ms
step:156/2245 train_time:18436ms step_avg:118.18ms
step:157/2245 train_time:18551ms step_avg:118.16ms
step:158/2245 train_time:18671ms step_avg:118.17ms
step:159/2245 train_time:18786ms step_avg:118.15ms
step:160/2245 train_time:18907ms step_avg:118.17ms
step:161/2245 train_time:19021ms step_avg:118.14ms
step:162/2245 train_time:19142ms step_avg:118.16ms
step:163/2245 train_time:19256ms step_avg:118.14ms
step:164/2245 train_time:19377ms step_avg:118.15ms
step:165/2245 train_time:19491ms step_avg:118.13ms
step:166/2245 train_time:19612ms step_avg:118.14ms
step:167/2245 train_time:19727ms step_avg:118.12ms
step:168/2245 train_time:19847ms step_avg:118.14ms
step:169/2245 train_time:19961ms step_avg:118.12ms
step:170/2245 train_time:20082ms step_avg:118.13ms
step:171/2245 train_time:20196ms step_avg:118.11ms
step:172/2245 train_time:20317ms step_avg:118.12ms
step:173/2245 train_time:20431ms step_avg:118.10ms
step:174/2245 train_time:20552ms step_avg:118.11ms
step:175/2245 train_time:20666ms step_avg:118.09ms
step:176/2245 train_time:20787ms step_avg:118.11ms
step:177/2245 train_time:20901ms step_avg:118.08ms
step:178/2245 train_time:21021ms step_avg:118.10ms
step:179/2245 train_time:21136ms step_avg:118.08ms
step:180/2245 train_time:21256ms step_avg:118.09ms
step:181/2245 train_time:21371ms step_avg:118.07ms
step:182/2245 train_time:21491ms step_avg:118.08ms
step:183/2245 train_time:21605ms step_avg:118.06ms
step:184/2245 train_time:21726ms step_avg:118.08ms
step:185/2245 train_time:21840ms step_avg:118.06ms
step:186/2245 train_time:21961ms step_avg:118.07ms
step:187/2245 train_time:22075ms step_avg:118.05ms
step:188/2245 train_time:22196ms step_avg:118.06ms
step:189/2245 train_time:22310ms step_avg:118.04ms
step:190/2245 train_time:22431ms step_avg:118.06ms
step:191/2245 train_time:22545ms step_avg:118.04ms
step:192/2245 train_time:22665ms step_avg:118.05ms
step:193/2245 train_time:22780ms step_avg:118.03ms
step:194/2245 train_time:22900ms step_avg:118.04ms
step:195/2245 train_time:23015ms step_avg:118.02ms
step:196/2245 train_time:23135ms step_avg:118.04ms
step:197/2245 train_time:23250ms step_avg:118.02ms
step:198/2245 train_time:23370ms step_avg:118.03ms
step:199/2245 train_time:23485ms step_avg:118.01ms
step:200/2245 train_time:23605ms step_avg:118.03ms
step:201/2245 train_time:23719ms step_avg:118.01ms
step:202/2245 train_time:23839ms step_avg:118.02ms
step:203/2245 train_time:23954ms step_avg:118.00ms
step:204/2245 train_time:24074ms step_avg:118.01ms
step:205/2245 train_time:24189ms step_avg:117.99ms
step:206/2245 train_time:24309ms step_avg:118.00ms
step:207/2245 train_time:24423ms step_avg:117.99ms
step:208/2245 train_time:24544ms step_avg:118.00ms
step:209/2245 train_time:24658ms step_avg:117.98ms
step:210/2245 train_time:24778ms step_avg:117.99ms
step:211/2245 train_time:24892ms step_avg:117.97ms
step:212/2245 train_time:25013ms step_avg:117.99ms
step:213/2245 train_time:25127ms step_avg:117.97ms
step:214/2245 train_time:25248ms step_avg:117.98ms
step:215/2245 train_time:25362ms step_avg:117.96ms
step:216/2245 train_time:25482ms step_avg:117.97ms
step:217/2245 train_time:25596ms step_avg:117.95ms
step:218/2245 train_time:25717ms step_avg:117.97ms
step:219/2245 train_time:25831ms step_avg:117.95ms
step:220/2245 train_time:25952ms step_avg:117.96ms
step:221/2245 train_time:26065ms step_avg:117.94ms
step:222/2245 train_time:26186ms step_avg:117.95ms
step:223/2245 train_time:26300ms step_avg:117.94ms
step:224/2245 train_time:26421ms step_avg:117.95ms
step:225/2245 train_time:26535ms step_avg:117.93ms
step:226/2245 train_time:26655ms step_avg:117.94ms
step:227/2245 train_time:26769ms step_avg:117.93ms
step:228/2245 train_time:26890ms step_avg:117.94ms
step:229/2245 train_time:27005ms step_avg:117.92ms
step:230/2245 train_time:27125ms step_avg:117.93ms
step:231/2245 train_time:27239ms step_avg:117.92ms
step:232/2245 train_time:27359ms step_avg:117.93ms
step:233/2245 train_time:27473ms step_avg:117.91ms
step:234/2245 train_time:27594ms step_avg:117.92ms
step:235/2245 train_time:27708ms step_avg:117.91ms
step:236/2245 train_time:27828ms step_avg:117.92ms
step:237/2245 train_time:27943ms step_avg:117.90ms
step:238/2245 train_time:28064ms step_avg:117.91ms
step:239/2245 train_time:28178ms step_avg:117.90ms
step:240/2245 train_time:28298ms step_avg:117.91ms
step:241/2245 train_time:28412ms step_avg:117.89ms
step:242/2245 train_time:28532ms step_avg:117.90ms
step:243/2245 train_time:28646ms step_avg:117.89ms
step:244/2245 train_time:28766ms step_avg:117.90ms
step:245/2245 train_time:28881ms step_avg:117.88ms
step:246/2245 train_time:29001ms step_avg:117.89ms
step:247/2245 train_time:29115ms step_avg:117.88ms
step:248/2245 train_time:29235ms step_avg:117.89ms
step:249/2245 train_time:29350ms step_avg:117.87ms
step:250/2245 train_time:29470ms step_avg:117.88ms
step:250/2245 val_loss:4.0939 train_time:29536ms step_avg:118.14ms
step:251/2245 train_time:29586ms step_avg:117.87ms
step:252/2245 train_time:29705ms step_avg:117.88ms
step:253/2245 train_time:29819ms step_avg:117.86ms
step:254/2245 train_time:29940ms step_avg:117.87ms
step:255/2245 train_time:30054ms step_avg:117.86ms
step:256/2245 train_time:30174ms step_avg:117.87ms
step:257/2245 train_time:30288ms step_avg:117.85ms
step:258/2245 train_time:30409ms step_avg:117.86ms
step:259/2245 train_time:30523ms step_avg:117.85ms
step:260/2245 train_time:30643ms step_avg:117.86ms
step:261/2245 train_time:30757ms step_avg:117.84ms
step:262/2245 train_time:30878ms step_avg:117.85ms
step:263/2245 train_time:30992ms step_avg:117.84ms
step:264/2245 train_time:31112ms step_avg:117.85ms
step:265/2245 train_time:31226ms step_avg:117.83ms
step:266/2245 train_time:31347ms step_avg:117.84ms
step:267/2245 train_time:31461ms step_avg:117.83ms
step:268/2245 train_time:31581ms step_avg:117.84ms
step:269/2245 train_time:31695ms step_avg:117.83ms
step:270/2245 train_time:31815ms step_avg:117.83ms
step:271/2245 train_time:31929ms step_avg:117.82ms
step:272/2245 train_time:32050ms step_avg:117.83ms
step:273/2245 train_time:32164ms step_avg:117.82ms
step:274/2245 train_time:32284ms step_avg:117.83ms
step:275/2245 train_time:32398ms step_avg:117.81ms
step:276/2245 train_time:32519ms step_avg:117.82ms
step:277/2245 train_time:32633ms step_avg:117.81ms
step:278/2245 train_time:32753ms step_avg:117.82ms
step:279/2245 train_time:32867ms step_avg:117.80ms
step:280/2245 train_time:32987ms step_avg:117.81ms
step:281/2245 train_time:33101ms step_avg:117.80ms
step:282/2245 train_time:33221ms step_avg:117.81ms
step:283/2245 train_time:33335ms step_avg:117.79ms
step:284/2245 train_time:33456ms step_avg:117.80ms
step:285/2245 train_time:33570ms step_avg:117.79ms
step:286/2245 train_time:33690ms step_avg:117.80ms
step:287/2245 train_time:33804ms step_avg:117.78ms
step:288/2245 train_time:33925ms step_avg:117.79ms
step:289/2245 train_time:34039ms step_avg:117.78ms
step:290/2245 train_time:34159ms step_avg:117.79ms
step:291/2245 train_time:34274ms step_avg:117.78ms
step:292/2245 train_time:34394ms step_avg:117.79ms
step:293/2245 train_time:34508ms step_avg:117.77ms
step:294/2245 train_time:34628ms step_avg:117.78ms
step:295/2245 train_time:34742ms step_avg:117.77ms
step:296/2245 train_time:34862ms step_avg:117.78ms
step:297/2245 train_time:34976ms step_avg:117.77ms
step:298/2245 train_time:35097ms step_avg:117.77ms
step:299/2245 train_time:35211ms step_avg:117.76ms
step:300/2245 train_time:35332ms step_avg:117.77ms
step:301/2245 train_time:35446ms step_avg:117.76ms
step:302/2245 train_time:35566ms step_avg:117.77ms
step:303/2245 train_time:35680ms step_avg:117.76ms
step:304/2245 train_time:35800ms step_avg:117.76ms
step:305/2245 train_time:35914ms step_avg:117.75ms
step:306/2245 train_time:36035ms step_avg:117.76ms
step:307/2245 train_time:36149ms step_avg:117.75ms
step:308/2245 train_time:36270ms step_avg:117.76ms
step:309/2245 train_time:36384ms step_avg:117.75ms
step:310/2245 train_time:36504ms step_avg:117.76ms
step:311/2245 train_time:36618ms step_avg:117.74ms
step:312/2245 train_time:36739ms step_avg:117.75ms
step:313/2245 train_time:36853ms step_avg:117.74ms
step:314/2245 train_time:36973ms step_avg:117.75ms
step:315/2245 train_time:37087ms step_avg:117.74ms
step:316/2245 train_time:37208ms step_avg:117.75ms
step:317/2245 train_time:37322ms step_avg:117.73ms
step:318/2245 train_time:37442ms step_avg:117.74ms
step:319/2245 train_time:37556ms step_avg:117.73ms
step:320/2245 train_time:37677ms step_avg:117.74ms
step:321/2245 train_time:37791ms step_avg:117.73ms
step:322/2245 train_time:37911ms step_avg:117.74ms
step:323/2245 train_time:38025ms step_avg:117.72ms
step:324/2245 train_time:38145ms step_avg:117.73ms
step:325/2245 train_time:38259ms step_avg:117.72ms
step:326/2245 train_time:38379ms step_avg:117.73ms
step:327/2245 train_time:38493ms step_avg:117.72ms
step:328/2245 train_time:38614ms step_avg:117.72ms
step:329/2245 train_time:38728ms step_avg:117.72ms
step:330/2245 train_time:38849ms step_avg:117.72ms
step:331/2245 train_time:38963ms step_avg:117.71ms
step:332/2245 train_time:39083ms step_avg:117.72ms
step:333/2245 train_time:39197ms step_avg:117.71ms
step:334/2245 train_time:39317ms step_avg:117.72ms
step:335/2245 train_time:39432ms step_avg:117.71ms
step:336/2245 train_time:39552ms step_avg:117.71ms
step:337/2245 train_time:39666ms step_avg:117.70ms
step:338/2245 train_time:39786ms step_avg:117.71ms
step:339/2245 train_time:39900ms step_avg:117.70ms
step:340/2245 train_time:40020ms step_avg:117.71ms
step:341/2245 train_time:40134ms step_avg:117.70ms
step:342/2245 train_time:40255ms step_avg:117.70ms
step:343/2245 train_time:40368ms step_avg:117.69ms
step:344/2245 train_time:40489ms step_avg:117.70ms
step:345/2245 train_time:40603ms step_avg:117.69ms
step:346/2245 train_time:40723ms step_avg:117.70ms
step:347/2245 train_time:40837ms step_avg:117.69ms
step:348/2245 train_time:40957ms step_avg:117.69ms
step:349/2245 train_time:41071ms step_avg:117.68ms
step:350/2245 train_time:41191ms step_avg:117.69ms
step:351/2245 train_time:41305ms step_avg:117.68ms
step:352/2245 train_time:41425ms step_avg:117.69ms
step:353/2245 train_time:41539ms step_avg:117.67ms
step:354/2245 train_time:41660ms step_avg:117.68ms
step:355/2245 train_time:41774ms step_avg:117.67ms
step:356/2245 train_time:41894ms step_avg:117.68ms
step:357/2245 train_time:42008ms step_avg:117.67ms
step:358/2245 train_time:42128ms step_avg:117.68ms
step:359/2245 train_time:42242ms step_avg:117.66ms
step:360/2245 train_time:42362ms step_avg:117.67ms
step:361/2245 train_time:42476ms step_avg:117.66ms
step:362/2245 train_time:42596ms step_avg:117.67ms
step:363/2245 train_time:42710ms step_avg:117.66ms
step:364/2245 train_time:42831ms step_avg:117.67ms
step:365/2245 train_time:42945ms step_avg:117.66ms
step:366/2245 train_time:43065ms step_avg:117.66ms
step:367/2245 train_time:43179ms step_avg:117.65ms
step:368/2245 train_time:43299ms step_avg:117.66ms
step:369/2245 train_time:43413ms step_avg:117.65ms
step:370/2245 train_time:43533ms step_avg:117.66ms
step:371/2245 train_time:43647ms step_avg:117.65ms
step:372/2245 train_time:43768ms step_avg:117.65ms
step:373/2245 train_time:43882ms step_avg:117.65ms
step:374/2245 train_time:44002ms step_avg:117.65ms
step:375/2245 train_time:44116ms step_avg:117.64ms
step:376/2245 train_time:44236ms step_avg:117.65ms
step:377/2245 train_time:44350ms step_avg:117.64ms
step:378/2245 train_time:44471ms step_avg:117.65ms
step:379/2245 train_time:44584ms step_avg:117.64ms
step:380/2245 train_time:44704ms step_avg:117.64ms
step:381/2245 train_time:44818ms step_avg:117.63ms
step:382/2245 train_time:44938ms step_avg:117.64ms
step:383/2245 train_time:45052ms step_avg:117.63ms
step:384/2245 train_time:45173ms step_avg:117.64ms
step:385/2245 train_time:45287ms step_avg:117.63ms
step:386/2245 train_time:45407ms step_avg:117.63ms
step:387/2245 train_time:45521ms step_avg:117.63ms
step:388/2245 train_time:45641ms step_avg:117.63ms
step:389/2245 train_time:45755ms step_avg:117.62ms
step:390/2245 train_time:45876ms step_avg:117.63ms
step:391/2245 train_time:45990ms step_avg:117.62ms
step:392/2245 train_time:46110ms step_avg:117.63ms
step:393/2245 train_time:46224ms step_avg:117.62ms
step:394/2245 train_time:46344ms step_avg:117.63ms
step:395/2245 train_time:46458ms step_avg:117.62ms
step:396/2245 train_time:46579ms step_avg:117.62ms
step:397/2245 train_time:46693ms step_avg:117.61ms
step:398/2245 train_time:46813ms step_avg:117.62ms
step:399/2245 train_time:46927ms step_avg:117.61ms
step:400/2245 train_time:47047ms step_avg:117.62ms
step:401/2245 train_time:47161ms step_avg:117.61ms
step:402/2245 train_time:47282ms step_avg:117.62ms
step:403/2245 train_time:47395ms step_avg:117.61ms
step:404/2245 train_time:47516ms step_avg:117.61ms
step:405/2245 train_time:47630ms step_avg:117.60ms
step:406/2245 train_time:47750ms step_avg:117.61ms
step:407/2245 train_time:47864ms step_avg:117.60ms
step:408/2245 train_time:47984ms step_avg:117.61ms
step:409/2245 train_time:48098ms step_avg:117.60ms
step:410/2245 train_time:48218ms step_avg:117.61ms
step:411/2245 train_time:48333ms step_avg:117.60ms
step:412/2245 train_time:48453ms step_avg:117.60ms
step:413/2245 train_time:48567ms step_avg:117.60ms
step:414/2245 train_time:48687ms step_avg:117.60ms
step:415/2245 train_time:48801ms step_avg:117.59ms
step:416/2245 train_time:48921ms step_avg:117.60ms
step:417/2245 train_time:49036ms step_avg:117.59ms
step:418/2245 train_time:49156ms step_avg:117.60ms
step:419/2245 train_time:49270ms step_avg:117.59ms
step:420/2245 train_time:49390ms step_avg:117.60ms
step:421/2245 train_time:49504ms step_avg:117.59ms
step:422/2245 train_time:49624ms step_avg:117.59ms
step:423/2245 train_time:49738ms step_avg:117.58ms
step:424/2245 train_time:49859ms step_avg:117.59ms
step:425/2245 train_time:49973ms step_avg:117.58ms
step:426/2245 train_time:50093ms step_avg:117.59ms
step:427/2245 train_time:50207ms step_avg:117.58ms
step:428/2245 train_time:50327ms step_avg:117.59ms
step:429/2245 train_time:50441ms step_avg:117.58ms
step:430/2245 train_time:50561ms step_avg:117.58ms
step:431/2245 train_time:50675ms step_avg:117.58ms
step:432/2245 train_time:50795ms step_avg:117.58ms
step:433/2245 train_time:50909ms step_avg:117.57ms
step:434/2245 train_time:51030ms step_avg:117.58ms
step:435/2245 train_time:51144ms step_avg:117.57ms
step:436/2245 train_time:51264ms step_avg:117.58ms
step:437/2245 train_time:51378ms step_avg:117.57ms
step:438/2245 train_time:51498ms step_avg:117.58ms
step:439/2245 train_time:51612ms step_avg:117.57ms
step:440/2245 train_time:51732ms step_avg:117.57ms
step:441/2245 train_time:51846ms step_avg:117.57ms
step:442/2245 train_time:51966ms step_avg:117.57ms
step:443/2245 train_time:52080ms step_avg:117.56ms
step:444/2245 train_time:52201ms step_avg:117.57ms
step:445/2245 train_time:52315ms step_avg:117.56ms
step:446/2245 train_time:52435ms step_avg:117.57ms
step:447/2245 train_time:52549ms step_avg:117.56ms
step:448/2245 train_time:52669ms step_avg:117.56ms
step:449/2245 train_time:52783ms step_avg:117.56ms
step:450/2245 train_time:52904ms step_avg:117.56ms
step:451/2245 train_time:53017ms step_avg:117.55ms
step:452/2245 train_time:53138ms step_avg:117.56ms
step:453/2245 train_time:53252ms step_avg:117.55ms
step:454/2245 train_time:53372ms step_avg:117.56ms
step:455/2245 train_time:53486ms step_avg:117.55ms
step:456/2245 train_time:53606ms step_avg:117.56ms
step:457/2245 train_time:53720ms step_avg:117.55ms
step:458/2245 train_time:53840ms step_avg:117.55ms
step:459/2245 train_time:53954ms step_avg:117.55ms
step:460/2245 train_time:54074ms step_avg:117.55ms
step:461/2245 train_time:54188ms step_avg:117.54ms
step:462/2245 train_time:54308ms step_avg:117.55ms
step:463/2245 train_time:54422ms step_avg:117.54ms
step:464/2245 train_time:54542ms step_avg:117.55ms
step:465/2245 train_time:54656ms step_avg:117.54ms
step:466/2245 train_time:54777ms step_avg:117.55ms
step:467/2245 train_time:54891ms step_avg:117.54ms
step:468/2245 train_time:55011ms step_avg:117.54ms
step:469/2245 train_time:55124ms step_avg:117.54ms
step:470/2245 train_time:55244ms step_avg:117.54ms
step:471/2245 train_time:55358ms step_avg:117.53ms
step:472/2245 train_time:55479ms step_avg:117.54ms
step:473/2245 train_time:55592ms step_avg:117.53ms
step:474/2245 train_time:55713ms step_avg:117.54ms
step:475/2245 train_time:55827ms step_avg:117.53ms
step:476/2245 train_time:55947ms step_avg:117.54ms
step:477/2245 train_time:56061ms step_avg:117.53ms
step:478/2245 train_time:56181ms step_avg:117.53ms
step:479/2245 train_time:56296ms step_avg:117.53ms
step:480/2245 train_time:56416ms step_avg:117.53ms
step:481/2245 train_time:56530ms step_avg:117.53ms
step:482/2245 train_time:56650ms step_avg:117.53ms
step:483/2245 train_time:56765ms step_avg:117.53ms
step:484/2245 train_time:56885ms step_avg:117.53ms
step:485/2245 train_time:56999ms step_avg:117.52ms
step:486/2245 train_time:57119ms step_avg:117.53ms
step:487/2245 train_time:57233ms step_avg:117.52ms
step:488/2245 train_time:57353ms step_avg:117.53ms
step:489/2245 train_time:57467ms step_avg:117.52ms
step:490/2245 train_time:57588ms step_avg:117.53ms
step:491/2245 train_time:57701ms step_avg:117.52ms
step:492/2245 train_time:57821ms step_avg:117.52ms
step:493/2245 train_time:57935ms step_avg:117.52ms
step:494/2245 train_time:58056ms step_avg:117.52ms
step:495/2245 train_time:58170ms step_avg:117.52ms
step:496/2245 train_time:58290ms step_avg:117.52ms
step:497/2245 train_time:58404ms step_avg:117.51ms
step:498/2245 train_time:58524ms step_avg:117.52ms
step:499/2245 train_time:58638ms step_avg:117.51ms
step:500/2245 train_time:58758ms step_avg:117.52ms
step:500/2245 val_loss:3.8230 train_time:58823ms step_avg:117.65ms
step:501/2245 train_time:58873ms step_avg:117.51ms
step:502/2245 train_time:58993ms step_avg:117.52ms
step:503/2245 train_time:59107ms step_avg:117.51ms
step:504/2245 train_time:59227ms step_avg:117.51ms
step:505/2245 train_time:59341ms step_avg:117.51ms
step:506/2245 train_time:59461ms step_avg:117.51ms
step:507/2245 train_time:59575ms step_avg:117.51ms
step:508/2245 train_time:59695ms step_avg:117.51ms
step:509/2245 train_time:59809ms step_avg:117.50ms
step:510/2245 train_time:59930ms step_avg:117.51ms
step:511/2245 train_time:60044ms step_avg:117.50ms
step:512/2245 train_time:60164ms step_avg:117.51ms
step:513/2245 train_time:60278ms step_avg:117.50ms
step:514/2245 train_time:60398ms step_avg:117.51ms
step:515/2245 train_time:60512ms step_avg:117.50ms
step:516/2245 train_time:60633ms step_avg:117.50ms
step:517/2245 train_time:60747ms step_avg:117.50ms
step:518/2245 train_time:60867ms step_avg:117.50ms
step:519/2245 train_time:60981ms step_avg:117.50ms
step:520/2245 train_time:61101ms step_avg:117.50ms
step:521/2245 train_time:61215ms step_avg:117.50ms
step:522/2245 train_time:61335ms step_avg:117.50ms
step:523/2245 train_time:61449ms step_avg:117.49ms
step:524/2245 train_time:61570ms step_avg:117.50ms
step:525/2245 train_time:61683ms step_avg:117.49ms
step:526/2245 train_time:61804ms step_avg:117.50ms
step:527/2245 train_time:61918ms step_avg:117.49ms
step:528/2245 train_time:62038ms step_avg:117.50ms
step:529/2245 train_time:62152ms step_avg:117.49ms
step:530/2245 train_time:62273ms step_avg:117.50ms
step:531/2245 train_time:62386ms step_avg:117.49ms
step:532/2245 train_time:62507ms step_avg:117.49ms
step:533/2245 train_time:62622ms step_avg:117.49ms
step:534/2245 train_time:62742ms step_avg:117.49ms
step:535/2245 train_time:62855ms step_avg:117.49ms
step:536/2245 train_time:62976ms step_avg:117.49ms
step:537/2245 train_time:63089ms step_avg:117.48ms
step:538/2245 train_time:63210ms step_avg:117.49ms
step:539/2245 train_time:63324ms step_avg:117.48ms
step:540/2245 train_time:63444ms step_avg:117.49ms
step:541/2245 train_time:63558ms step_avg:117.48ms
step:542/2245 train_time:63678ms step_avg:117.49ms
step:543/2245 train_time:63792ms step_avg:117.48ms
step:544/2245 train_time:63912ms step_avg:117.49ms
step:545/2245 train_time:64026ms step_avg:117.48ms
step:546/2245 train_time:64147ms step_avg:117.49ms
step:547/2245 train_time:64261ms step_avg:117.48ms
step:548/2245 train_time:64381ms step_avg:117.48ms
step:549/2245 train_time:64495ms step_avg:117.48ms
step:550/2245 train_time:64615ms step_avg:117.48ms
step:551/2245 train_time:64730ms step_avg:117.48ms
step:552/2245 train_time:64850ms step_avg:117.48ms
step:553/2245 train_time:64964ms step_avg:117.48ms
step:554/2245 train_time:65085ms step_avg:117.48ms
step:555/2245 train_time:65198ms step_avg:117.47ms
step:556/2245 train_time:65319ms step_avg:117.48ms
step:557/2245 train_time:65433ms step_avg:117.47ms
step:558/2245 train_time:65553ms step_avg:117.48ms
step:559/2245 train_time:65667ms step_avg:117.47ms
step:560/2245 train_time:65787ms step_avg:117.48ms
step:561/2245 train_time:65902ms step_avg:117.47ms
step:562/2245 train_time:66022ms step_avg:117.48ms
step:563/2245 train_time:66137ms step_avg:117.47ms
step:564/2245 train_time:66257ms step_avg:117.48ms
step:565/2245 train_time:66371ms step_avg:117.47ms
step:566/2245 train_time:66491ms step_avg:117.48ms
step:567/2245 train_time:66605ms step_avg:117.47ms
step:568/2245 train_time:66726ms step_avg:117.47ms
step:569/2245 train_time:66840ms step_avg:117.47ms
step:570/2245 train_time:66959ms step_avg:117.47ms
step:571/2245 train_time:67073ms step_avg:117.47ms
step:572/2245 train_time:67193ms step_avg:117.47ms
step:573/2245 train_time:67307ms step_avg:117.46ms
step:574/2245 train_time:67427ms step_avg:117.47ms
step:575/2245 train_time:67542ms step_avg:117.46ms
step:576/2245 train_time:67662ms step_avg:117.47ms
step:577/2245 train_time:67777ms step_avg:117.46ms
step:578/2245 train_time:67897ms step_avg:117.47ms
step:579/2245 train_time:68011ms step_avg:117.46ms
step:580/2245 train_time:68131ms step_avg:117.47ms
step:581/2245 train_time:68245ms step_avg:117.46ms
step:582/2245 train_time:68365ms step_avg:117.47ms
step:583/2245 train_time:68480ms step_avg:117.46ms
step:584/2245 train_time:68600ms step_avg:117.47ms
step:585/2245 train_time:68714ms step_avg:117.46ms
step:586/2245 train_time:68834ms step_avg:117.46ms
step:587/2245 train_time:68948ms step_avg:117.46ms
step:588/2245 train_time:69069ms step_avg:117.46ms
step:589/2245 train_time:69183ms step_avg:117.46ms
step:590/2245 train_time:69303ms step_avg:117.46ms
step:591/2245 train_time:69417ms step_avg:117.46ms
step:592/2245 train_time:69537ms step_avg:117.46ms
step:593/2245 train_time:69651ms step_avg:117.46ms
step:594/2245 train_time:69771ms step_avg:117.46ms
step:595/2245 train_time:69885ms step_avg:117.45ms
step:596/2245 train_time:70006ms step_avg:117.46ms
step:597/2245 train_time:70120ms step_avg:117.45ms
step:598/2245 train_time:70240ms step_avg:117.46ms
step:599/2245 train_time:70354ms step_avg:117.45ms
step:600/2245 train_time:70474ms step_avg:117.46ms
step:601/2245 train_time:70588ms step_avg:117.45ms
step:602/2245 train_time:70709ms step_avg:117.46ms
step:603/2245 train_time:70823ms step_avg:117.45ms
step:604/2245 train_time:70943ms step_avg:117.46ms
step:605/2245 train_time:71057ms step_avg:117.45ms
step:606/2245 train_time:71177ms step_avg:117.45ms
step:607/2245 train_time:71291ms step_avg:117.45ms
step:608/2245 train_time:71411ms step_avg:117.45ms
step:609/2245 train_time:71525ms step_avg:117.45ms
step:610/2245 train_time:71645ms step_avg:117.45ms
step:611/2245 train_time:71759ms step_avg:117.45ms
step:612/2245 train_time:71879ms step_avg:117.45ms
step:613/2245 train_time:71994ms step_avg:117.44ms
step:614/2245 train_time:72114ms step_avg:117.45ms
step:615/2245 train_time:72229ms step_avg:117.44ms
step:616/2245 train_time:72349ms step_avg:117.45ms
step:617/2245 train_time:72463ms step_avg:117.44ms
step:618/2245 train_time:72584ms step_avg:117.45ms
step:619/2245 train_time:72697ms step_avg:117.44ms
step:620/2245 train_time:72818ms step_avg:117.45ms
step:621/2245 train_time:72932ms step_avg:117.44ms
step:622/2245 train_time:73052ms step_avg:117.45ms
step:623/2245 train_time:73166ms step_avg:117.44ms
step:624/2245 train_time:73287ms step_avg:117.45ms
step:625/2245 train_time:73401ms step_avg:117.44ms
step:626/2245 train_time:73521ms step_avg:117.44ms
step:627/2245 train_time:73635ms step_avg:117.44ms
step:628/2245 train_time:73755ms step_avg:117.44ms
step:629/2245 train_time:73869ms step_avg:117.44ms
step:630/2245 train_time:73989ms step_avg:117.44ms
step:631/2245 train_time:74103ms step_avg:117.44ms
step:632/2245 train_time:74224ms step_avg:117.44ms
step:633/2245 train_time:74337ms step_avg:117.44ms
step:634/2245 train_time:74458ms step_avg:117.44ms
step:635/2245 train_time:74572ms step_avg:117.44ms
step:636/2245 train_time:74692ms step_avg:117.44ms
step:637/2245 train_time:74806ms step_avg:117.43ms
step:638/2245 train_time:74926ms step_avg:117.44ms
step:639/2245 train_time:75040ms step_avg:117.43ms
step:640/2245 train_time:75160ms step_avg:117.44ms
step:641/2245 train_time:75274ms step_avg:117.43ms
step:642/2245 train_time:75394ms step_avg:117.44ms
step:643/2245 train_time:75508ms step_avg:117.43ms
step:644/2245 train_time:75629ms step_avg:117.44ms
step:645/2245 train_time:75743ms step_avg:117.43ms
step:646/2245 train_time:75863ms step_avg:117.43ms
step:647/2245 train_time:75977ms step_avg:117.43ms
step:648/2245 train_time:76097ms step_avg:117.43ms
step:649/2245 train_time:76211ms step_avg:117.43ms
step:650/2245 train_time:76331ms step_avg:117.43ms
step:651/2245 train_time:76445ms step_avg:117.43ms
step:652/2245 train_time:76565ms step_avg:117.43ms
step:653/2245 train_time:76679ms step_avg:117.43ms
step:654/2245 train_time:76800ms step_avg:117.43ms
step:655/2245 train_time:76914ms step_avg:117.43ms
step:656/2245 train_time:77034ms step_avg:117.43ms
step:657/2245 train_time:77149ms step_avg:117.43ms
step:658/2245 train_time:77269ms step_avg:117.43ms
step:659/2245 train_time:77383ms step_avg:117.43ms
step:660/2245 train_time:77503ms step_avg:117.43ms
step:661/2245 train_time:77617ms step_avg:117.42ms
step:662/2245 train_time:77737ms step_avg:117.43ms
step:663/2245 train_time:77851ms step_avg:117.42ms
step:664/2245 train_time:77972ms step_avg:117.43ms
step:665/2245 train_time:78086ms step_avg:117.42ms
step:666/2245 train_time:78206ms step_avg:117.43ms
step:667/2245 train_time:78320ms step_avg:117.42ms
step:668/2245 train_time:78441ms step_avg:117.43ms
step:669/2245 train_time:78555ms step_avg:117.42ms
step:670/2245 train_time:78675ms step_avg:117.43ms
step:671/2245 train_time:78789ms step_avg:117.42ms
step:672/2245 train_time:78910ms step_avg:117.43ms
step:673/2245 train_time:79024ms step_avg:117.42ms
step:674/2245 train_time:79144ms step_avg:117.43ms
step:675/2245 train_time:79259ms step_avg:117.42ms
step:676/2245 train_time:79379ms step_avg:117.42ms
step:677/2245 train_time:79493ms step_avg:117.42ms
step:678/2245 train_time:79613ms step_avg:117.42ms
step:679/2245 train_time:79727ms step_avg:117.42ms
step:680/2245 train_time:79847ms step_avg:117.42ms
step:681/2245 train_time:79961ms step_avg:117.42ms
step:682/2245 train_time:80081ms step_avg:117.42ms
step:683/2245 train_time:80195ms step_avg:117.42ms
step:684/2245 train_time:80315ms step_avg:117.42ms
step:685/2245 train_time:80430ms step_avg:117.42ms
step:686/2245 train_time:80550ms step_avg:117.42ms
step:687/2245 train_time:80664ms step_avg:117.41ms
step:688/2245 train_time:80784ms step_avg:117.42ms
step:689/2245 train_time:80898ms step_avg:117.41ms
step:690/2245 train_time:81018ms step_avg:117.42ms
step:691/2245 train_time:81132ms step_avg:117.41ms
step:692/2245 train_time:81253ms step_avg:117.42ms
step:693/2245 train_time:81366ms step_avg:117.41ms
step:694/2245 train_time:81487ms step_avg:117.42ms
step:695/2245 train_time:81601ms step_avg:117.41ms
step:696/2245 train_time:81721ms step_avg:117.42ms
step:697/2245 train_time:81835ms step_avg:117.41ms
step:698/2245 train_time:81955ms step_avg:117.41ms
step:699/2245 train_time:82070ms step_avg:117.41ms
step:700/2245 train_time:82190ms step_avg:117.41ms
step:701/2245 train_time:82304ms step_avg:117.41ms
step:702/2245 train_time:82424ms step_avg:117.41ms
step:703/2245 train_time:82538ms step_avg:117.41ms
step:704/2245 train_time:82659ms step_avg:117.41ms
step:705/2245 train_time:82773ms step_avg:117.41ms
step:706/2245 train_time:82893ms step_avg:117.41ms
step:707/2245 train_time:83007ms step_avg:117.41ms
step:708/2245 train_time:83128ms step_avg:117.41ms
step:709/2245 train_time:83242ms step_avg:117.41ms
step:710/2245 train_time:83363ms step_avg:117.41ms
step:711/2245 train_time:83477ms step_avg:117.41ms
step:712/2245 train_time:83597ms step_avg:117.41ms
step:713/2245 train_time:83711ms step_avg:117.41ms
step:714/2245 train_time:83832ms step_avg:117.41ms
step:715/2245 train_time:83946ms step_avg:117.41ms
step:716/2245 train_time:84066ms step_avg:117.41ms
step:717/2245 train_time:84181ms step_avg:117.41ms
step:718/2245 train_time:84301ms step_avg:117.41ms
step:719/2245 train_time:84415ms step_avg:117.41ms
step:720/2245 train_time:84535ms step_avg:117.41ms
step:721/2245 train_time:84649ms step_avg:117.41ms
step:722/2245 train_time:84770ms step_avg:117.41ms
step:723/2245 train_time:84884ms step_avg:117.41ms
step:724/2245 train_time:85004ms step_avg:117.41ms
step:725/2245 train_time:85118ms step_avg:117.40ms
step:726/2245 train_time:85238ms step_avg:117.41ms
step:727/2245 train_time:85352ms step_avg:117.40ms
step:728/2245 train_time:85473ms step_avg:117.41ms
step:729/2245 train_time:85587ms step_avg:117.40ms
step:730/2245 train_time:85707ms step_avg:117.41ms
step:731/2245 train_time:85821ms step_avg:117.40ms
step:732/2245 train_time:85941ms step_avg:117.41ms
step:733/2245 train_time:86055ms step_avg:117.40ms
step:734/2245 train_time:86175ms step_avg:117.40ms
step:735/2245 train_time:86289ms step_avg:117.40ms
step:736/2245 train_time:86410ms step_avg:117.41ms
step:737/2245 train_time:86526ms step_avg:117.40ms
step:738/2245 train_time:86648ms step_avg:117.41ms
step:739/2245 train_time:86763ms step_avg:117.41ms
step:740/2245 train_time:86884ms step_avg:117.41ms
step:741/2245 train_time:87000ms step_avg:117.41ms
step:742/2245 train_time:87121ms step_avg:117.41ms
step:743/2245 train_time:87237ms step_avg:117.41ms
step:744/2245 train_time:87359ms step_avg:117.42ms
step:745/2245 train_time:87474ms step_avg:117.42ms
step:746/2245 train_time:87596ms step_avg:117.42ms
step:747/2245 train_time:87712ms step_avg:117.42ms
step:748/2245 train_time:87833ms step_avg:117.42ms
step:749/2245 train_time:87949ms step_avg:117.42ms
step:750/2245 train_time:88071ms step_avg:117.43ms
step:750/2245 val_loss:3.6708 train_time:88137ms step_avg:117.52ms
step:751/2245 train_time:88187ms step_avg:117.43ms
step:752/2245 train_time:88309ms step_avg:117.43ms
step:753/2245 train_time:88424ms step_avg:117.43ms
step:754/2245 train_time:88546ms step_avg:117.43ms
step:755/2245 train_time:88661ms step_avg:117.43ms
step:756/2245 train_time:88783ms step_avg:117.44ms
step:757/2245 train_time:88898ms step_avg:117.43ms
step:758/2245 train_time:89020ms step_avg:117.44ms
step:759/2245 train_time:89136ms step_avg:117.44ms
step:760/2245 train_time:89258ms step_avg:117.44ms
step:761/2245 train_time:89373ms step_avg:117.44ms
step:762/2245 train_time:89495ms step_avg:117.45ms
step:763/2245 train_time:89610ms step_avg:117.44ms
step:764/2245 train_time:89732ms step_avg:117.45ms
step:765/2245 train_time:89848ms step_avg:117.45ms
step:766/2245 train_time:89971ms step_avg:117.46ms
step:767/2245 train_time:90087ms step_avg:117.45ms
step:768/2245 train_time:90209ms step_avg:117.46ms
step:769/2245 train_time:90324ms step_avg:117.46ms
step:770/2245 train_time:90446ms step_avg:117.46ms
step:771/2245 train_time:90561ms step_avg:117.46ms
step:772/2245 train_time:90684ms step_avg:117.47ms
step:773/2245 train_time:90799ms step_avg:117.46ms
step:774/2245 train_time:90920ms step_avg:117.47ms
step:775/2245 train_time:91036ms step_avg:117.47ms
step:776/2245 train_time:91158ms step_avg:117.47ms
step:777/2245 train_time:91274ms step_avg:117.47ms
step:778/2245 train_time:91395ms step_avg:117.47ms
step:779/2245 train_time:91511ms step_avg:117.47ms
step:780/2245 train_time:91633ms step_avg:117.48ms
step:781/2245 train_time:91749ms step_avg:117.48ms
step:782/2245 train_time:91872ms step_avg:117.48ms
step:783/2245 train_time:91988ms step_avg:117.48ms
step:784/2245 train_time:92110ms step_avg:117.49ms
step:785/2245 train_time:92226ms step_avg:117.49ms
step:786/2245 train_time:92348ms step_avg:117.49ms
step:787/2245 train_time:92463ms step_avg:117.49ms
step:788/2245 train_time:92585ms step_avg:117.49ms
step:789/2245 train_time:92700ms step_avg:117.49ms
step:790/2245 train_time:92822ms step_avg:117.50ms
step:791/2245 train_time:92937ms step_avg:117.49ms
step:792/2245 train_time:93059ms step_avg:117.50ms
step:793/2245 train_time:93175ms step_avg:117.50ms
step:794/2245 train_time:93297ms step_avg:117.50ms
step:795/2245 train_time:93412ms step_avg:117.50ms
step:796/2245 train_time:93534ms step_avg:117.51ms
step:797/2245 train_time:93650ms step_avg:117.50ms
step:798/2245 train_time:93773ms step_avg:117.51ms
step:799/2245 train_time:93889ms step_avg:117.51ms
step:800/2245 train_time:94011ms step_avg:117.51ms
step:801/2245 train_time:94126ms step_avg:117.51ms
step:802/2245 train_time:94248ms step_avg:117.52ms
step:803/2245 train_time:94364ms step_avg:117.51ms
step:804/2245 train_time:94486ms step_avg:117.52ms
step:805/2245 train_time:94601ms step_avg:117.52ms
step:806/2245 train_time:94723ms step_avg:117.52ms
step:807/2245 train_time:94838ms step_avg:117.52ms
step:808/2245 train_time:94960ms step_avg:117.52ms
step:809/2245 train_time:95075ms step_avg:117.52ms
step:810/2245 train_time:95198ms step_avg:117.53ms
step:811/2245 train_time:95313ms step_avg:117.53ms
step:812/2245 train_time:95435ms step_avg:117.53ms
step:813/2245 train_time:95552ms step_avg:117.53ms
step:814/2245 train_time:95674ms step_avg:117.54ms
step:815/2245 train_time:95789ms step_avg:117.53ms
step:816/2245 train_time:95911ms step_avg:117.54ms
step:817/2245 train_time:96027ms step_avg:117.54ms
step:818/2245 train_time:96149ms step_avg:117.54ms
step:819/2245 train_time:96265ms step_avg:117.54ms
step:820/2245 train_time:96387ms step_avg:117.55ms
step:821/2245 train_time:96502ms step_avg:117.54ms
step:822/2245 train_time:96625ms step_avg:117.55ms
step:823/2245 train_time:96740ms step_avg:117.54ms
step:824/2245 train_time:96861ms step_avg:117.55ms
step:825/2245 train_time:96977ms step_avg:117.55ms
step:826/2245 train_time:97100ms step_avg:117.55ms
step:827/2245 train_time:97215ms step_avg:117.55ms
step:828/2245 train_time:97337ms step_avg:117.56ms
step:829/2245 train_time:97453ms step_avg:117.55ms
step:830/2245 train_time:97575ms step_avg:117.56ms
step:831/2245 train_time:97690ms step_avg:117.56ms
step:832/2245 train_time:97813ms step_avg:117.56ms
step:833/2245 train_time:97928ms step_avg:117.56ms
step:834/2245 train_time:98050ms step_avg:117.57ms
step:835/2245 train_time:98165ms step_avg:117.56ms
step:836/2245 train_time:98287ms step_avg:117.57ms
step:837/2245 train_time:98403ms step_avg:117.57ms
step:838/2245 train_time:98525ms step_avg:117.57ms
step:839/2245 train_time:98640ms step_avg:117.57ms
step:840/2245 train_time:98762ms step_avg:117.57ms
step:841/2245 train_time:98877ms step_avg:117.57ms
step:842/2245 train_time:98999ms step_avg:117.58ms
step:843/2245 train_time:99116ms step_avg:117.57ms
step:844/2245 train_time:99237ms step_avg:117.58ms
step:845/2245 train_time:99353ms step_avg:117.58ms
step:846/2245 train_time:99474ms step_avg:117.58ms
step:847/2245 train_time:99590ms step_avg:117.58ms
step:848/2245 train_time:99712ms step_avg:117.58ms
step:849/2245 train_time:99827ms step_avg:117.58ms
step:850/2245 train_time:99949ms step_avg:117.59ms
step:851/2245 train_time:100064ms step_avg:117.58ms
step:852/2245 train_time:100186ms step_avg:117.59ms
step:853/2245 train_time:100303ms step_avg:117.59ms
step:854/2245 train_time:100425ms step_avg:117.59ms
step:855/2245 train_time:100540ms step_avg:117.59ms
step:856/2245 train_time:100662ms step_avg:117.60ms
step:857/2245 train_time:100777ms step_avg:117.59ms
step:858/2245 train_time:100900ms step_avg:117.60ms
step:859/2245 train_time:101016ms step_avg:117.60ms
step:860/2245 train_time:101139ms step_avg:117.60ms
step:861/2245 train_time:101255ms step_avg:117.60ms
step:862/2245 train_time:101377ms step_avg:117.61ms
step:863/2245 train_time:101492ms step_avg:117.60ms
step:864/2245 train_time:101614ms step_avg:117.61ms
step:865/2245 train_time:101729ms step_avg:117.61ms
step:866/2245 train_time:101852ms step_avg:117.61ms
step:867/2245 train_time:101968ms step_avg:117.61ms
step:868/2245 train_time:102089ms step_avg:117.61ms
step:869/2245 train_time:102205ms step_avg:117.61ms
step:870/2245 train_time:102327ms step_avg:117.62ms
step:871/2245 train_time:102442ms step_avg:117.61ms
step:872/2245 train_time:102564ms step_avg:117.62ms
step:873/2245 train_time:102680ms step_avg:117.62ms
step:874/2245 train_time:102802ms step_avg:117.62ms
step:875/2245 train_time:102917ms step_avg:117.62ms
step:876/2245 train_time:103039ms step_avg:117.62ms
step:877/2245 train_time:103155ms step_avg:117.62ms
step:878/2245 train_time:103277ms step_avg:117.63ms
step:879/2245 train_time:103392ms step_avg:117.62ms
step:880/2245 train_time:103514ms step_avg:117.63ms
step:881/2245 train_time:103630ms step_avg:117.63ms
step:882/2245 train_time:103752ms step_avg:117.63ms
step:883/2245 train_time:103867ms step_avg:117.63ms
step:884/2245 train_time:103988ms step_avg:117.63ms
step:885/2245 train_time:104104ms step_avg:117.63ms
step:886/2245 train_time:104226ms step_avg:117.64ms
step:887/2245 train_time:104342ms step_avg:117.63ms
step:888/2245 train_time:104464ms step_avg:117.64ms
step:889/2245 train_time:104579ms step_avg:117.64ms
step:890/2245 train_time:104701ms step_avg:117.64ms
step:891/2245 train_time:104817ms step_avg:117.64ms
step:892/2245 train_time:104939ms step_avg:117.64ms
step:893/2245 train_time:105055ms step_avg:117.64ms
step:894/2245 train_time:105177ms step_avg:117.65ms
step:895/2245 train_time:105292ms step_avg:117.65ms
step:896/2245 train_time:105414ms step_avg:117.65ms
step:897/2245 train_time:105529ms step_avg:117.65ms
step:898/2245 train_time:105651ms step_avg:117.65ms
step:899/2245 train_time:105767ms step_avg:117.65ms
step:900/2245 train_time:105889ms step_avg:117.65ms
step:901/2245 train_time:106005ms step_avg:117.65ms
step:902/2245 train_time:106127ms step_avg:117.66ms
step:903/2245 train_time:106242ms step_avg:117.65ms
step:904/2245 train_time:106363ms step_avg:117.66ms
step:905/2245 train_time:106479ms step_avg:117.66ms
step:906/2245 train_time:106601ms step_avg:117.66ms
step:907/2245 train_time:106716ms step_avg:117.66ms
step:908/2245 train_time:106838ms step_avg:117.66ms
step:909/2245 train_time:106954ms step_avg:117.66ms
step:910/2245 train_time:107075ms step_avg:117.67ms
step:911/2245 train_time:107191ms step_avg:117.66ms
step:912/2245 train_time:107314ms step_avg:117.67ms
step:913/2245 train_time:107429ms step_avg:117.67ms
step:914/2245 train_time:107552ms step_avg:117.67ms
step:915/2245 train_time:107668ms step_avg:117.67ms
step:916/2245 train_time:107789ms step_avg:117.67ms
step:917/2245 train_time:107905ms step_avg:117.67ms
step:918/2245 train_time:108027ms step_avg:117.68ms
step:919/2245 train_time:108142ms step_avg:117.67ms
step:920/2245 train_time:108265ms step_avg:117.68ms
step:921/2245 train_time:108380ms step_avg:117.68ms
step:922/2245 train_time:108501ms step_avg:117.68ms
step:923/2245 train_time:108617ms step_avg:117.68ms
step:924/2245 train_time:108739ms step_avg:117.68ms
step:925/2245 train_time:108854ms step_avg:117.68ms
step:926/2245 train_time:108976ms step_avg:117.68ms
step:927/2245 train_time:109092ms step_avg:117.68ms
step:928/2245 train_time:109214ms step_avg:117.69ms
step:929/2245 train_time:109329ms step_avg:117.69ms
step:930/2245 train_time:109451ms step_avg:117.69ms
step:931/2245 train_time:109567ms step_avg:117.69ms
step:932/2245 train_time:109688ms step_avg:117.69ms
step:933/2245 train_time:109804ms step_avg:117.69ms
step:934/2245 train_time:109926ms step_avg:117.69ms
step:935/2245 train_time:110041ms step_avg:117.69ms
step:936/2245 train_time:110163ms step_avg:117.70ms
step:937/2245 train_time:110278ms step_avg:117.69ms
step:938/2245 train_time:110400ms step_avg:117.70ms
step:939/2245 train_time:110516ms step_avg:117.69ms
step:940/2245 train_time:110638ms step_avg:117.70ms
step:941/2245 train_time:110753ms step_avg:117.70ms
step:942/2245 train_time:110875ms step_avg:117.70ms
step:943/2245 train_time:110991ms step_avg:117.70ms
step:944/2245 train_time:111113ms step_avg:117.70ms
step:945/2245 train_time:111229ms step_avg:117.70ms
step:946/2245 train_time:111351ms step_avg:117.71ms
step:947/2245 train_time:111467ms step_avg:117.71ms
step:948/2245 train_time:111588ms step_avg:117.71ms
step:949/2245 train_time:111703ms step_avg:117.71ms
step:950/2245 train_time:111825ms step_avg:117.71ms
step:951/2245 train_time:111941ms step_avg:117.71ms
step:952/2245 train_time:112063ms step_avg:117.71ms
step:953/2245 train_time:112178ms step_avg:117.71ms
step:954/2245 train_time:112300ms step_avg:117.71ms
step:955/2245 train_time:112415ms step_avg:117.71ms
step:956/2245 train_time:112537ms step_avg:117.72ms
step:957/2245 train_time:112653ms step_avg:117.71ms
step:958/2245 train_time:112775ms step_avg:117.72ms
step:959/2245 train_time:112891ms step_avg:117.72ms
step:960/2245 train_time:113013ms step_avg:117.72ms
step:961/2245 train_time:113128ms step_avg:117.72ms
step:962/2245 train_time:113251ms step_avg:117.72ms
step:963/2245 train_time:113366ms step_avg:117.72ms
step:964/2245 train_time:113488ms step_avg:117.73ms
step:965/2245 train_time:113604ms step_avg:117.72ms
step:966/2245 train_time:113725ms step_avg:117.73ms
step:967/2245 train_time:113841ms step_avg:117.73ms
step:968/2245 train_time:113963ms step_avg:117.73ms
step:969/2245 train_time:114078ms step_avg:117.73ms
step:970/2245 train_time:114200ms step_avg:117.73ms
step:971/2245 train_time:114315ms step_avg:117.73ms
step:972/2245 train_time:114437ms step_avg:117.73ms
step:973/2245 train_time:114553ms step_avg:117.73ms
step:974/2245 train_time:114675ms step_avg:117.74ms
step:975/2245 train_time:114790ms step_avg:117.73ms
step:976/2245 train_time:114913ms step_avg:117.74ms
step:977/2245 train_time:115028ms step_avg:117.74ms
step:978/2245 train_time:115150ms step_avg:117.74ms
step:979/2245 train_time:115266ms step_avg:117.74ms
step:980/2245 train_time:115388ms step_avg:117.74ms
step:981/2245 train_time:115503ms step_avg:117.74ms
step:982/2245 train_time:115626ms step_avg:117.75ms
step:983/2245 train_time:115741ms step_avg:117.74ms
step:984/2245 train_time:115863ms step_avg:117.75ms
step:985/2245 train_time:115979ms step_avg:117.74ms
step:986/2245 train_time:116101ms step_avg:117.75ms
step:987/2245 train_time:116216ms step_avg:117.75ms
step:988/2245 train_time:116339ms step_avg:117.75ms
step:989/2245 train_time:116454ms step_avg:117.75ms
step:990/2245 train_time:116576ms step_avg:117.75ms
step:991/2245 train_time:116692ms step_avg:117.75ms
step:992/2245 train_time:116814ms step_avg:117.76ms
step:993/2245 train_time:116930ms step_avg:117.75ms
step:994/2245 train_time:117052ms step_avg:117.76ms
step:995/2245 train_time:117168ms step_avg:117.76ms
step:996/2245 train_time:117290ms step_avg:117.76ms
step:997/2245 train_time:117406ms step_avg:117.76ms
step:998/2245 train_time:117528ms step_avg:117.76ms
step:999/2245 train_time:117643ms step_avg:117.76ms
step:1000/2245 train_time:117765ms step_avg:117.76ms
step:1000/2245 val_loss:3.5922 train_time:117830ms step_avg:117.83ms
step:1001/2245 train_time:117881ms step_avg:117.76ms
step:1002/2245 train_time:118002ms step_avg:117.77ms
step:1003/2245 train_time:118117ms step_avg:117.76ms
step:1004/2245 train_time:118240ms step_avg:117.77ms
step:1005/2245 train_time:118355ms step_avg:117.77ms
step:1006/2245 train_time:118476ms step_avg:117.77ms
step:1007/2245 train_time:118592ms step_avg:117.77ms
step:1008/2245 train_time:118713ms step_avg:117.77ms
step:1009/2245 train_time:118829ms step_avg:117.77ms
step:1010/2245 train_time:118951ms step_avg:117.77ms
step:1011/2245 train_time:119067ms step_avg:117.77ms
step:1012/2245 train_time:119189ms step_avg:117.78ms
step:1013/2245 train_time:119305ms step_avg:117.77ms
step:1014/2245 train_time:119427ms step_avg:117.78ms
step:1015/2245 train_time:119542ms step_avg:117.78ms
step:1016/2245 train_time:119664ms step_avg:117.78ms
step:1017/2245 train_time:119780ms step_avg:117.78ms
step:1018/2245 train_time:119902ms step_avg:117.78ms
step:1019/2245 train_time:120018ms step_avg:117.78ms
step:1020/2245 train_time:120140ms step_avg:117.78ms
step:1021/2245 train_time:120255ms step_avg:117.78ms
step:1022/2245 train_time:120377ms step_avg:117.79ms
step:1023/2245 train_time:120492ms step_avg:117.78ms
step:1024/2245 train_time:120614ms step_avg:117.79ms
step:1025/2245 train_time:120730ms step_avg:117.79ms
step:1026/2245 train_time:120852ms step_avg:117.79ms
step:1027/2245 train_time:120968ms step_avg:117.79ms
step:1028/2245 train_time:121091ms step_avg:117.79ms
step:1029/2245 train_time:121207ms step_avg:117.79ms
step:1030/2245 train_time:121329ms step_avg:117.79ms
step:1031/2245 train_time:121445ms step_avg:117.79ms
step:1032/2245 train_time:121567ms step_avg:117.80ms
step:1033/2245 train_time:121683ms step_avg:117.80ms
step:1034/2245 train_time:121805ms step_avg:117.80ms
step:1035/2245 train_time:121921ms step_avg:117.80ms
step:1036/2245 train_time:122044ms step_avg:117.80ms
step:1037/2245 train_time:122159ms step_avg:117.80ms
step:1038/2245 train_time:122281ms step_avg:117.80ms
step:1039/2245 train_time:122397ms step_avg:117.80ms
step:1040/2245 train_time:122519ms step_avg:117.81ms
step:1041/2245 train_time:122634ms step_avg:117.80ms
step:1042/2245 train_time:122756ms step_avg:117.81ms
step:1043/2245 train_time:122871ms step_avg:117.81ms
step:1044/2245 train_time:122994ms step_avg:117.81ms
step:1045/2245 train_time:123110ms step_avg:117.81ms
step:1046/2245 train_time:123232ms step_avg:117.81ms
step:1047/2245 train_time:123347ms step_avg:117.81ms
step:1048/2245 train_time:123469ms step_avg:117.81ms
step:1049/2245 train_time:123584ms step_avg:117.81ms
step:1050/2245 train_time:123707ms step_avg:117.82ms
step:1051/2245 train_time:123822ms step_avg:117.81ms
step:1052/2245 train_time:123945ms step_avg:117.82ms
step:1053/2245 train_time:124061ms step_avg:117.82ms
step:1054/2245 train_time:124183ms step_avg:117.82ms
step:1055/2245 train_time:124299ms step_avg:117.82ms
step:1056/2245 train_time:124420ms step_avg:117.82ms
step:1057/2245 train_time:124536ms step_avg:117.82ms
step:1058/2245 train_time:124657ms step_avg:117.82ms
step:1059/2245 train_time:124773ms step_avg:117.82ms
step:1060/2245 train_time:124894ms step_avg:117.82ms
step:1061/2245 train_time:125009ms step_avg:117.82ms
step:1062/2245 train_time:125133ms step_avg:117.83ms
step:1063/2245 train_time:125248ms step_avg:117.82ms
step:1064/2245 train_time:125370ms step_avg:117.83ms
step:1065/2245 train_time:125486ms step_avg:117.83ms
step:1066/2245 train_time:125607ms step_avg:117.83ms
step:1067/2245 train_time:125723ms step_avg:117.83ms
step:1068/2245 train_time:125846ms step_avg:117.83ms
step:1069/2245 train_time:125961ms step_avg:117.83ms
step:1070/2245 train_time:126083ms step_avg:117.83ms
step:1071/2245 train_time:126199ms step_avg:117.83ms
step:1072/2245 train_time:126320ms step_avg:117.84ms
step:1073/2245 train_time:126436ms step_avg:117.83ms
step:1074/2245 train_time:126557ms step_avg:117.84ms
step:1075/2245 train_time:126673ms step_avg:117.84ms
step:1076/2245 train_time:126795ms step_avg:117.84ms
step:1077/2245 train_time:126911ms step_avg:117.84ms
step:1078/2245 train_time:127033ms step_avg:117.84ms
step:1079/2245 train_time:127148ms step_avg:117.84ms
step:1080/2245 train_time:127270ms step_avg:117.84ms
step:1081/2245 train_time:127386ms step_avg:117.84ms
step:1082/2245 train_time:127508ms step_avg:117.84ms
step:1083/2245 train_time:127624ms step_avg:117.84ms
step:1084/2245 train_time:127746ms step_avg:117.85ms
step:1085/2245 train_time:127862ms step_avg:117.85ms
step:1086/2245 train_time:127985ms step_avg:117.85ms
step:1087/2245 train_time:128101ms step_avg:117.85ms
step:1088/2245 train_time:128223ms step_avg:117.85ms
step:1089/2245 train_time:128339ms step_avg:117.85ms
step:1090/2245 train_time:128461ms step_avg:117.85ms
step:1091/2245 train_time:128576ms step_avg:117.85ms
step:1092/2245 train_time:128698ms step_avg:117.86ms
step:1093/2245 train_time:128814ms step_avg:117.85ms
step:1094/2245 train_time:128936ms step_avg:117.86ms
step:1095/2245 train_time:129051ms step_avg:117.85ms
step:1096/2245 train_time:129174ms step_avg:117.86ms
step:1097/2245 train_time:129290ms step_avg:117.86ms
step:1098/2245 train_time:129412ms step_avg:117.86ms
step:1099/2245 train_time:129527ms step_avg:117.86ms
step:1100/2245 train_time:129649ms step_avg:117.86ms
step:1101/2245 train_time:129765ms step_avg:117.86ms
step:1102/2245 train_time:129887ms step_avg:117.86ms
step:1103/2245 train_time:130003ms step_avg:117.86ms
step:1104/2245 train_time:130124ms step_avg:117.87ms
step:1105/2245 train_time:130240ms step_avg:117.86ms
step:1106/2245 train_time:130362ms step_avg:117.87ms
step:1107/2245 train_time:130477ms step_avg:117.87ms
step:1108/2245 train_time:130599ms step_avg:117.87ms
step:1109/2245 train_time:130715ms step_avg:117.87ms
step:1110/2245 train_time:130837ms step_avg:117.87ms
step:1111/2245 train_time:130952ms step_avg:117.87ms
step:1112/2245 train_time:131074ms step_avg:117.87ms
step:1113/2245 train_time:131189ms step_avg:117.87ms
step:1114/2245 train_time:131311ms step_avg:117.87ms
step:1115/2245 train_time:131426ms step_avg:117.87ms
step:1116/2245 train_time:131548ms step_avg:117.87ms
step:1117/2245 train_time:131664ms step_avg:117.87ms
step:1118/2245 train_time:131786ms step_avg:117.88ms
step:1119/2245 train_time:131902ms step_avg:117.87ms
step:1120/2245 train_time:132024ms step_avg:117.88ms
step:1121/2245 train_time:132140ms step_avg:117.88ms
step:1122/2245 train_time:132262ms step_avg:117.88ms
step:1123/2245 train_time:132378ms step_avg:117.88ms
step:1124/2245 train_time:132500ms step_avg:117.88ms
step:1125/2245 train_time:132615ms step_avg:117.88ms
step:1126/2245 train_time:132737ms step_avg:117.88ms
step:1127/2245 train_time:132852ms step_avg:117.88ms
step:1128/2245 train_time:132975ms step_avg:117.89ms
step:1129/2245 train_time:133090ms step_avg:117.88ms
step:1130/2245 train_time:133212ms step_avg:117.89ms
step:1131/2245 train_time:133328ms step_avg:117.88ms
step:1132/2245 train_time:133450ms step_avg:117.89ms
step:1133/2245 train_time:133566ms step_avg:117.89ms
step:1134/2245 train_time:133687ms step_avg:117.89ms
step:1135/2245 train_time:133803ms step_avg:117.89ms
step:1136/2245 train_time:133925ms step_avg:117.89ms
step:1137/2245 train_time:134041ms step_avg:117.89ms
step:1138/2245 train_time:134163ms step_avg:117.89ms
step:1139/2245 train_time:134279ms step_avg:117.89ms
step:1140/2245 train_time:134402ms step_avg:117.90ms
step:1141/2245 train_time:134517ms step_avg:117.89ms
step:1142/2245 train_time:134639ms step_avg:117.90ms
step:1143/2245 train_time:134754ms step_avg:117.90ms
step:1144/2245 train_time:134876ms step_avg:117.90ms
step:1145/2245 train_time:134991ms step_avg:117.90ms
step:1146/2245 train_time:135113ms step_avg:117.90ms
step:1147/2245 train_time:135229ms step_avg:117.90ms
step:1148/2245 train_time:135351ms step_avg:117.90ms
step:1149/2245 train_time:135467ms step_avg:117.90ms
step:1150/2245 train_time:135588ms step_avg:117.90ms
step:1151/2245 train_time:135704ms step_avg:117.90ms
step:1152/2245 train_time:135826ms step_avg:117.90ms
step:1153/2245 train_time:135942ms step_avg:117.90ms
step:1154/2245 train_time:136065ms step_avg:117.91ms
step:1155/2245 train_time:136181ms step_avg:117.91ms
step:1156/2245 train_time:136303ms step_avg:117.91ms
step:1157/2245 train_time:136419ms step_avg:117.91ms
step:1158/2245 train_time:136540ms step_avg:117.91ms
step:1159/2245 train_time:136656ms step_avg:117.91ms
step:1160/2245 train_time:136777ms step_avg:117.91ms
step:1161/2245 train_time:136893ms step_avg:117.91ms
step:1162/2245 train_time:137014ms step_avg:117.91ms
step:1163/2245 train_time:137130ms step_avg:117.91ms
step:1164/2245 train_time:137252ms step_avg:117.91ms
step:1165/2245 train_time:137368ms step_avg:117.91ms
step:1166/2245 train_time:137490ms step_avg:117.92ms
step:1167/2245 train_time:137605ms step_avg:117.91ms
step:1168/2245 train_time:137728ms step_avg:117.92ms
step:1169/2245 train_time:137844ms step_avg:117.92ms
step:1170/2245 train_time:137966ms step_avg:117.92ms
step:1171/2245 train_time:138081ms step_avg:117.92ms
step:1172/2245 train_time:138204ms step_avg:117.92ms
step:1173/2245 train_time:138319ms step_avg:117.92ms
step:1174/2245 train_time:138441ms step_avg:117.92ms
step:1175/2245 train_time:138557ms step_avg:117.92ms
step:1176/2245 train_time:138679ms step_avg:117.92ms
step:1177/2245 train_time:138794ms step_avg:117.92ms
step:1178/2245 train_time:138916ms step_avg:117.93ms
step:1179/2245 train_time:139031ms step_avg:117.92ms
step:1180/2245 train_time:139153ms step_avg:117.93ms
step:1181/2245 train_time:139269ms step_avg:117.92ms
step:1182/2245 train_time:139391ms step_avg:117.93ms
step:1183/2245 train_time:139507ms step_avg:117.93ms
step:1184/2245 train_time:139629ms step_avg:117.93ms
step:1185/2245 train_time:139745ms step_avg:117.93ms
step:1186/2245 train_time:139867ms step_avg:117.93ms
step:1187/2245 train_time:139982ms step_avg:117.93ms
step:1188/2245 train_time:140105ms step_avg:117.93ms
step:1189/2245 train_time:140221ms step_avg:117.93ms
step:1190/2245 train_time:140344ms step_avg:117.94ms
step:1191/2245 train_time:140460ms step_avg:117.93ms
step:1192/2245 train_time:140582ms step_avg:117.94ms
step:1193/2245 train_time:140697ms step_avg:117.94ms
step:1194/2245 train_time:140819ms step_avg:117.94ms
step:1195/2245 train_time:140934ms step_avg:117.94ms
step:1196/2245 train_time:141056ms step_avg:117.94ms
step:1197/2245 train_time:141171ms step_avg:117.94ms
step:1198/2245 train_time:141293ms step_avg:117.94ms
step:1199/2245 train_time:141408ms step_avg:117.94ms
step:1200/2245 train_time:141530ms step_avg:117.94ms
step:1201/2245 train_time:141646ms step_avg:117.94ms
step:1202/2245 train_time:141768ms step_avg:117.94ms
step:1203/2245 train_time:141883ms step_avg:117.94ms
step:1204/2245 train_time:142005ms step_avg:117.94ms
step:1205/2245 train_time:142121ms step_avg:117.94ms
step:1206/2245 train_time:142244ms step_avg:117.95ms
step:1207/2245 train_time:142359ms step_avg:117.94ms
step:1208/2245 train_time:142481ms step_avg:117.95ms
step:1209/2245 train_time:142597ms step_avg:117.95ms
step:1210/2245 train_time:142719ms step_avg:117.95ms
step:1211/2245 train_time:142834ms step_avg:117.95ms
step:1212/2245 train_time:142956ms step_avg:117.95ms
step:1213/2245 train_time:143071ms step_avg:117.95ms
step:1214/2245 train_time:143193ms step_avg:117.95ms
step:1215/2245 train_time:143309ms step_avg:117.95ms
step:1216/2245 train_time:143431ms step_avg:117.95ms
step:1217/2245 train_time:143547ms step_avg:117.95ms
step:1218/2245 train_time:143669ms step_avg:117.95ms
step:1219/2245 train_time:143784ms step_avg:117.95ms
step:1220/2245 train_time:143907ms step_avg:117.96ms
step:1221/2245 train_time:144022ms step_avg:117.95ms
step:1222/2245 train_time:144144ms step_avg:117.96ms
step:1223/2245 train_time:144260ms step_avg:117.96ms
step:1224/2245 train_time:144382ms step_avg:117.96ms
step:1225/2245 train_time:144498ms step_avg:117.96ms
step:1226/2245 train_time:144620ms step_avg:117.96ms
step:1227/2245 train_time:144735ms step_avg:117.96ms
step:1228/2245 train_time:144857ms step_avg:117.96ms
step:1229/2245 train_time:144972ms step_avg:117.96ms
step:1230/2245 train_time:145094ms step_avg:117.96ms
step:1231/2245 train_time:145209ms step_avg:117.96ms
step:1232/2245 train_time:145331ms step_avg:117.96ms
step:1233/2245 train_time:145447ms step_avg:117.96ms
step:1234/2245 train_time:145569ms step_avg:117.97ms
step:1235/2245 train_time:145684ms step_avg:117.96ms
step:1236/2245 train_time:145807ms step_avg:117.97ms
step:1237/2245 train_time:145923ms step_avg:117.97ms
step:1238/2245 train_time:146045ms step_avg:117.97ms
step:1239/2245 train_time:146160ms step_avg:117.97ms
step:1240/2245 train_time:146282ms step_avg:117.97ms
step:1241/2245 train_time:146397ms step_avg:117.97ms
step:1242/2245 train_time:146519ms step_avg:117.97ms
step:1243/2245 train_time:146635ms step_avg:117.97ms
step:1244/2245 train_time:146757ms step_avg:117.97ms
step:1245/2245 train_time:146872ms step_avg:117.97ms
step:1246/2245 train_time:146995ms step_avg:117.97ms
step:1247/2245 train_time:147111ms step_avg:117.97ms
step:1248/2245 train_time:147233ms step_avg:117.97ms
step:1249/2245 train_time:147349ms step_avg:117.97ms
step:1250/2245 train_time:147471ms step_avg:117.98ms
step:1250/2245 val_loss:3.5232 train_time:147537ms step_avg:118.03ms
step:1251/2245 train_time:147588ms step_avg:117.98ms
step:1252/2245 train_time:147709ms step_avg:117.98ms
step:1253/2245 train_time:147825ms step_avg:117.98ms
step:1254/2245 train_time:147947ms step_avg:117.98ms
step:1255/2245 train_time:148062ms step_avg:117.98ms
step:1256/2245 train_time:148183ms step_avg:117.98ms
step:1257/2245 train_time:148299ms step_avg:117.98ms
step:1258/2245 train_time:148421ms step_avg:117.98ms
step:1259/2245 train_time:148536ms step_avg:117.98ms
step:1260/2245 train_time:148658ms step_avg:117.98ms
step:1261/2245 train_time:148774ms step_avg:117.98ms
step:1262/2245 train_time:148896ms step_avg:117.98ms
step:1263/2245 train_time:149012ms step_avg:117.98ms
step:1264/2245 train_time:149134ms step_avg:117.99ms
step:1265/2245 train_time:149249ms step_avg:117.98ms
step:1266/2245 train_time:149370ms step_avg:117.99ms
step:1267/2245 train_time:149486ms step_avg:117.98ms
step:1268/2245 train_time:149608ms step_avg:117.99ms
step:1269/2245 train_time:149723ms step_avg:117.99ms
step:1270/2245 train_time:149845ms step_avg:117.99ms
step:1271/2245 train_time:149961ms step_avg:117.99ms
step:1272/2245 train_time:150083ms step_avg:117.99ms
step:1273/2245 train_time:150199ms step_avg:117.99ms
step:1274/2245 train_time:150321ms step_avg:117.99ms
step:1275/2245 train_time:150436ms step_avg:117.99ms
step:1276/2245 train_time:150559ms step_avg:117.99ms
step:1277/2245 train_time:150674ms step_avg:117.99ms
step:1278/2245 train_time:150797ms step_avg:117.99ms
step:1279/2245 train_time:150912ms step_avg:117.99ms
step:1280/2245 train_time:151034ms step_avg:118.00ms
step:1281/2245 train_time:151150ms step_avg:117.99ms
step:1282/2245 train_time:151272ms step_avg:118.00ms
step:1283/2245 train_time:151388ms step_avg:118.00ms
step:1284/2245 train_time:151510ms step_avg:118.00ms
step:1285/2245 train_time:151625ms step_avg:118.00ms
step:1286/2245 train_time:151747ms step_avg:118.00ms
step:1287/2245 train_time:151863ms step_avg:118.00ms
step:1288/2245 train_time:151985ms step_avg:118.00ms
step:1289/2245 train_time:152100ms step_avg:118.00ms
step:1290/2245 train_time:152221ms step_avg:118.00ms
step:1291/2245 train_time:152337ms step_avg:118.00ms
step:1292/2245 train_time:152459ms step_avg:118.00ms
step:1293/2245 train_time:152575ms step_avg:118.00ms
step:1294/2245 train_time:152697ms step_avg:118.00ms
step:1295/2245 train_time:152813ms step_avg:118.00ms
step:1296/2245 train_time:152935ms step_avg:118.01ms
step:1297/2245 train_time:153051ms step_avg:118.00ms
step:1298/2245 train_time:153173ms step_avg:118.01ms
step:1299/2245 train_time:153288ms step_avg:118.00ms
step:1300/2245 train_time:153410ms step_avg:118.01ms
step:1301/2245 train_time:153525ms step_avg:118.01ms
step:1302/2245 train_time:153648ms step_avg:118.01ms
step:1303/2245 train_time:153763ms step_avg:118.01ms
step:1304/2245 train_time:153885ms step_avg:118.01ms
step:1305/2245 train_time:154001ms step_avg:118.01ms
step:1306/2245 train_time:154123ms step_avg:118.01ms
step:1307/2245 train_time:154239ms step_avg:118.01ms
step:1308/2245 train_time:154361ms step_avg:118.01ms
step:1309/2245 train_time:154477ms step_avg:118.01ms
step:1310/2245 train_time:154600ms step_avg:118.01ms
step:1311/2245 train_time:154715ms step_avg:118.01ms
step:1312/2245 train_time:154837ms step_avg:118.02ms
step:1313/2245 train_time:154953ms step_avg:118.01ms
step:1314/2245 train_time:155076ms step_avg:118.02ms
step:1315/2245 train_time:155192ms step_avg:118.02ms
step:1316/2245 train_time:155314ms step_avg:118.02ms
step:1317/2245 train_time:155429ms step_avg:118.02ms
step:1318/2245 train_time:155551ms step_avg:118.02ms
step:1319/2245 train_time:155666ms step_avg:118.02ms
step:1320/2245 train_time:155788ms step_avg:118.02ms
step:1321/2245 train_time:155904ms step_avg:118.02ms
step:1322/2245 train_time:156026ms step_avg:118.02ms
step:1323/2245 train_time:156141ms step_avg:118.02ms
step:1324/2245 train_time:156263ms step_avg:118.02ms
step:1325/2245 train_time:156379ms step_avg:118.02ms
step:1326/2245 train_time:156501ms step_avg:118.02ms
step:1327/2245 train_time:156617ms step_avg:118.02ms
step:1328/2245 train_time:156738ms step_avg:118.03ms
step:1329/2245 train_time:156854ms step_avg:118.02ms
step:1330/2245 train_time:156976ms step_avg:118.03ms
step:1331/2245 train_time:157093ms step_avg:118.03ms
step:1332/2245 train_time:157215ms step_avg:118.03ms
step:1333/2245 train_time:157331ms step_avg:118.03ms
step:1334/2245 train_time:157453ms step_avg:118.03ms
step:1335/2245 train_time:157568ms step_avg:118.03ms
step:1336/2245 train_time:157690ms step_avg:118.03ms
step:1337/2245 train_time:157806ms step_avg:118.03ms
step:1338/2245 train_time:157928ms step_avg:118.03ms
step:1339/2245 train_time:158043ms step_avg:118.03ms
step:1340/2245 train_time:158165ms step_avg:118.03ms
step:1341/2245 train_time:158280ms step_avg:118.03ms
step:1342/2245 train_time:158402ms step_avg:118.03ms
step:1343/2245 train_time:158518ms step_avg:118.03ms
step:1344/2245 train_time:158640ms step_avg:118.04ms
step:1345/2245 train_time:158755ms step_avg:118.03ms
step:1346/2245 train_time:158877ms step_avg:118.04ms
step:1347/2245 train_time:158993ms step_avg:118.03ms
step:1348/2245 train_time:159115ms step_avg:118.04ms
step:1349/2245 train_time:159231ms step_avg:118.04ms
step:1350/2245 train_time:159353ms step_avg:118.04ms
step:1351/2245 train_time:159469ms step_avg:118.04ms
step:1352/2245 train_time:159591ms step_avg:118.04ms
step:1353/2245 train_time:159706ms step_avg:118.04ms
step:1354/2245 train_time:159827ms step_avg:118.04ms
step:1355/2245 train_time:159943ms step_avg:118.04ms
step:1356/2245 train_time:160065ms step_avg:118.04ms
step:1357/2245 train_time:160181ms step_avg:118.04ms
step:1358/2245 train_time:160304ms step_avg:118.04ms
step:1359/2245 train_time:160419ms step_avg:118.04ms
step:1360/2245 train_time:160541ms step_avg:118.04ms
step:1361/2245 train_time:160656ms step_avg:118.04ms
step:1362/2245 train_time:160779ms step_avg:118.05ms
step:1363/2245 train_time:160894ms step_avg:118.04ms
step:1364/2245 train_time:161016ms step_avg:118.05ms
step:1365/2245 train_time:161133ms step_avg:118.05ms
step:1366/2245 train_time:161255ms step_avg:118.05ms
step:1367/2245 train_time:161370ms step_avg:118.05ms
step:1368/2245 train_time:161492ms step_avg:118.05ms
step:1369/2245 train_time:161608ms step_avg:118.05ms
step:1370/2245 train_time:161730ms step_avg:118.05ms
step:1371/2245 train_time:161845ms step_avg:118.05ms
step:1372/2245 train_time:161967ms step_avg:118.05ms
step:1373/2245 train_time:162083ms step_avg:118.05ms
step:1374/2245 train_time:162204ms step_avg:118.05ms
step:1375/2245 train_time:162321ms step_avg:118.05ms
step:1376/2245 train_time:162443ms step_avg:118.05ms
step:1377/2245 train_time:162558ms step_avg:118.05ms
step:1378/2245 train_time:162681ms step_avg:118.06ms
step:1379/2245 train_time:162796ms step_avg:118.05ms
step:1380/2245 train_time:162918ms step_avg:118.06ms
step:1381/2245 train_time:163034ms step_avg:118.05ms
step:1382/2245 train_time:163156ms step_avg:118.06ms
step:1383/2245 train_time:163272ms step_avg:118.06ms
step:1384/2245 train_time:163394ms step_avg:118.06ms
step:1385/2245 train_time:163510ms step_avg:118.06ms
step:1386/2245 train_time:163632ms step_avg:118.06ms
step:1387/2245 train_time:163747ms step_avg:118.06ms
step:1388/2245 train_time:163869ms step_avg:118.06ms
step:1389/2245 train_time:163984ms step_avg:118.06ms
step:1390/2245 train_time:164107ms step_avg:118.06ms
step:1391/2245 train_time:164223ms step_avg:118.06ms
step:1392/2245 train_time:164345ms step_avg:118.06ms
step:1393/2245 train_time:164461ms step_avg:118.06ms
step:1394/2245 train_time:164583ms step_avg:118.07ms
step:1395/2245 train_time:164698ms step_avg:118.06ms
step:1396/2245 train_time:164820ms step_avg:118.07ms
step:1397/2245 train_time:164937ms step_avg:118.06ms
step:1398/2245 train_time:165059ms step_avg:118.07ms
step:1399/2245 train_time:165174ms step_avg:118.07ms
step:1400/2245 train_time:165296ms step_avg:118.07ms
step:1401/2245 train_time:165412ms step_avg:118.07ms
step:1402/2245 train_time:165534ms step_avg:118.07ms
step:1403/2245 train_time:165649ms step_avg:118.07ms
step:1404/2245 train_time:165771ms step_avg:118.07ms
step:1405/2245 train_time:165886ms step_avg:118.07ms
step:1406/2245 train_time:166008ms step_avg:118.07ms
step:1407/2245 train_time:166123ms step_avg:118.07ms
step:1408/2245 train_time:166245ms step_avg:118.07ms
step:1409/2245 train_time:166360ms step_avg:118.07ms
step:1410/2245 train_time:166483ms step_avg:118.07ms
step:1411/2245 train_time:166598ms step_avg:118.07ms
step:1412/2245 train_time:166720ms step_avg:118.07ms
step:1413/2245 train_time:166836ms step_avg:118.07ms
step:1414/2245 train_time:166959ms step_avg:118.08ms
step:1415/2245 train_time:167074ms step_avg:118.07ms
step:1416/2245 train_time:167197ms step_avg:118.08ms
step:1417/2245 train_time:167313ms step_avg:118.08ms
step:1418/2245 train_time:167435ms step_avg:118.08ms
step:1419/2245 train_time:167551ms step_avg:118.08ms
step:1420/2245 train_time:167673ms step_avg:118.08ms
step:1421/2245 train_time:167789ms step_avg:118.08ms
step:1422/2245 train_time:167911ms step_avg:118.08ms
step:1423/2245 train_time:168026ms step_avg:118.08ms
step:1424/2245 train_time:168148ms step_avg:118.08ms
step:1425/2245 train_time:168263ms step_avg:118.08ms
step:1426/2245 train_time:168385ms step_avg:118.08ms
step:1427/2245 train_time:168500ms step_avg:118.08ms
step:1428/2245 train_time:168622ms step_avg:118.08ms
step:1429/2245 train_time:168738ms step_avg:118.08ms
step:1430/2245 train_time:168859ms step_avg:118.08ms
step:1431/2245 train_time:168975ms step_avg:118.08ms
step:1432/2245 train_time:169098ms step_avg:118.09ms
step:1433/2245 train_time:169214ms step_avg:118.08ms
step:1434/2245 train_time:169336ms step_avg:118.09ms
step:1435/2245 train_time:169452ms step_avg:118.09ms
step:1436/2245 train_time:169574ms step_avg:118.09ms
step:1437/2245 train_time:169690ms step_avg:118.09ms
step:1438/2245 train_time:169811ms step_avg:118.09ms
step:1439/2245 train_time:169927ms step_avg:118.09ms
step:1440/2245 train_time:170048ms step_avg:118.09ms
step:1441/2245 train_time:170164ms step_avg:118.09ms
step:1442/2245 train_time:170286ms step_avg:118.09ms
step:1443/2245 train_time:170401ms step_avg:118.09ms
step:1444/2245 train_time:170523ms step_avg:118.09ms
step:1445/2245 train_time:170638ms step_avg:118.09ms
step:1446/2245 train_time:170761ms step_avg:118.09ms
step:1447/2245 train_time:170877ms step_avg:118.09ms
step:1448/2245 train_time:170998ms step_avg:118.09ms
step:1449/2245 train_time:171114ms step_avg:118.09ms
step:1450/2245 train_time:171236ms step_avg:118.09ms
step:1451/2245 train_time:171352ms step_avg:118.09ms
step:1452/2245 train_time:171474ms step_avg:118.10ms
step:1453/2245 train_time:171590ms step_avg:118.09ms
step:1454/2245 train_time:171712ms step_avg:118.10ms
step:1455/2245 train_time:171828ms step_avg:118.09ms
step:1456/2245 train_time:171950ms step_avg:118.10ms
step:1457/2245 train_time:172065ms step_avg:118.10ms
step:1458/2245 train_time:172187ms step_avg:118.10ms
step:1459/2245 train_time:172303ms step_avg:118.10ms
step:1460/2245 train_time:172425ms step_avg:118.10ms
step:1461/2245 train_time:172540ms step_avg:118.10ms
step:1462/2245 train_time:172663ms step_avg:118.10ms
step:1463/2245 train_time:172779ms step_avg:118.10ms
step:1464/2245 train_time:172901ms step_avg:118.10ms
step:1465/2245 train_time:173017ms step_avg:118.10ms
step:1466/2245 train_time:173139ms step_avg:118.10ms
step:1467/2245 train_time:173254ms step_avg:118.10ms
step:1468/2245 train_time:173376ms step_avg:118.10ms
step:1469/2245 train_time:173493ms step_avg:118.10ms
step:1470/2245 train_time:173615ms step_avg:118.11ms
step:1471/2245 train_time:173732ms step_avg:118.10ms
step:1472/2245 train_time:173855ms step_avg:118.11ms
step:1473/2245 train_time:173971ms step_avg:118.11ms
step:1474/2245 train_time:174094ms step_avg:118.11ms
step:1475/2245 train_time:174210ms step_avg:118.11ms
step:1476/2245 train_time:174333ms step_avg:118.11ms
step:1477/2245 train_time:174449ms step_avg:118.11ms
step:1478/2245 train_time:174572ms step_avg:118.11ms
step:1479/2245 train_time:174688ms step_avg:118.11ms
step:1480/2245 train_time:174810ms step_avg:118.11ms
step:1481/2245 train_time:174927ms step_avg:118.11ms
step:1482/2245 train_time:175050ms step_avg:118.12ms
step:1483/2245 train_time:175166ms step_avg:118.12ms
step:1484/2245 train_time:175289ms step_avg:118.12ms
step:1485/2245 train_time:175406ms step_avg:118.12ms
step:1486/2245 train_time:175528ms step_avg:118.12ms
step:1487/2245 train_time:175645ms step_avg:118.12ms
step:1488/2245 train_time:175768ms step_avg:118.12ms
step:1489/2245 train_time:175885ms step_avg:118.12ms
step:1490/2245 train_time:176008ms step_avg:118.13ms
step:1491/2245 train_time:176124ms step_avg:118.12ms
step:1492/2245 train_time:176247ms step_avg:118.13ms
step:1493/2245 train_time:176364ms step_avg:118.13ms
step:1494/2245 train_time:176486ms step_avg:118.13ms
step:1495/2245 train_time:176603ms step_avg:118.13ms
step:1496/2245 train_time:176726ms step_avg:118.13ms
step:1497/2245 train_time:176843ms step_avg:118.13ms
step:1498/2245 train_time:176966ms step_avg:118.14ms
step:1499/2245 train_time:177083ms step_avg:118.13ms
step:1500/2245 train_time:177206ms step_avg:118.14ms
step:1500/2245 val_loss:3.4418 train_time:177272ms step_avg:118.18ms
step:1501/2245 train_time:177322ms step_avg:118.14ms
step:1502/2245 train_time:177444ms step_avg:118.14ms
step:1503/2245 train_time:177560ms step_avg:118.14ms
step:1504/2245 train_time:177683ms step_avg:118.14ms
step:1505/2245 train_time:177799ms step_avg:118.14ms
step:1506/2245 train_time:177922ms step_avg:118.14ms
step:1507/2245 train_time:178039ms step_avg:118.14ms
step:1508/2245 train_time:178161ms step_avg:118.14ms
step:1509/2245 train_time:178278ms step_avg:118.14ms
step:1510/2245 train_time:178401ms step_avg:118.15ms
step:1511/2245 train_time:178518ms step_avg:118.15ms
step:1512/2245 train_time:178640ms step_avg:118.15ms
step:1513/2245 train_time:178757ms step_avg:118.15ms
step:1514/2245 train_time:178880ms step_avg:118.15ms
step:1515/2245 train_time:178996ms step_avg:118.15ms
step:1516/2245 train_time:179119ms step_avg:118.15ms
step:1517/2245 train_time:179235ms step_avg:118.15ms
step:1518/2245 train_time:179358ms step_avg:118.15ms
step:1519/2245 train_time:179474ms step_avg:118.15ms
step:1520/2245 train_time:179597ms step_avg:118.16ms
step:1521/2245 train_time:179715ms step_avg:118.16ms
step:1522/2245 train_time:179838ms step_avg:118.16ms
step:1523/2245 train_time:179955ms step_avg:118.16ms
step:1524/2245 train_time:180077ms step_avg:118.16ms
step:1525/2245 train_time:180194ms step_avg:118.16ms
step:1526/2245 train_time:180317ms step_avg:118.16ms
step:1527/2245 train_time:180434ms step_avg:118.16ms
step:1528/2245 train_time:180556ms step_avg:118.17ms
step:1529/2245 train_time:180673ms step_avg:118.16ms
step:1530/2245 train_time:180796ms step_avg:118.17ms
step:1531/2245 train_time:180912ms step_avg:118.17ms
step:1532/2245 train_time:181035ms step_avg:118.17ms
step:1533/2245 train_time:181152ms step_avg:118.17ms
step:1534/2245 train_time:181275ms step_avg:118.17ms
step:1535/2245 train_time:181391ms step_avg:118.17ms
step:1536/2245 train_time:181514ms step_avg:118.17ms
step:1537/2245 train_time:181630ms step_avg:118.17ms
step:1538/2245 train_time:181753ms step_avg:118.17ms
step:1539/2245 train_time:181869ms step_avg:118.17ms
step:1540/2245 train_time:181991ms step_avg:118.18ms
step:1541/2245 train_time:182108ms step_avg:118.17ms
step:1542/2245 train_time:182230ms step_avg:118.18ms
step:1543/2245 train_time:182347ms step_avg:118.18ms
step:1544/2245 train_time:182469ms step_avg:118.18ms
step:1545/2245 train_time:182586ms step_avg:118.18ms
step:1546/2245 train_time:182709ms step_avg:118.18ms
step:1547/2245 train_time:182826ms step_avg:118.18ms
step:1548/2245 train_time:182949ms step_avg:118.18ms
step:1549/2245 train_time:183066ms step_avg:118.18ms
step:1550/2245 train_time:183188ms step_avg:118.19ms
step:1551/2245 train_time:183305ms step_avg:118.19ms
step:1552/2245 train_time:183428ms step_avg:118.19ms
step:1553/2245 train_time:183544ms step_avg:118.19ms
step:1554/2245 train_time:183668ms step_avg:118.19ms
step:1555/2245 train_time:183784ms step_avg:118.19ms
step:1556/2245 train_time:183907ms step_avg:118.19ms
step:1557/2245 train_time:184023ms step_avg:118.19ms
step:1558/2245 train_time:184147ms step_avg:118.19ms
step:1559/2245 train_time:184263ms step_avg:118.19ms
step:1560/2245 train_time:184385ms step_avg:118.20ms
step:1561/2245 train_time:184502ms step_avg:118.19ms
step:1562/2245 train_time:184625ms step_avg:118.20ms
step:1563/2245 train_time:184742ms step_avg:118.20ms
step:1564/2245 train_time:184865ms step_avg:118.20ms
step:1565/2245 train_time:184981ms step_avg:118.20ms
step:1566/2245 train_time:185104ms step_avg:118.20ms
step:1567/2245 train_time:185220ms step_avg:118.20ms
step:1568/2245 train_time:185344ms step_avg:118.20ms
step:1569/2245 train_time:185460ms step_avg:118.20ms
step:1570/2245 train_time:185583ms step_avg:118.21ms
step:1571/2245 train_time:185700ms step_avg:118.20ms
step:1572/2245 train_time:185822ms step_avg:118.21ms
step:1573/2245 train_time:185939ms step_avg:118.21ms
step:1574/2245 train_time:186062ms step_avg:118.21ms
step:1575/2245 train_time:186178ms step_avg:118.21ms
step:1576/2245 train_time:186302ms step_avg:118.21ms
step:1577/2245 train_time:186419ms step_avg:118.21ms
step:1578/2245 train_time:186542ms step_avg:118.21ms
step:1579/2245 train_time:186659ms step_avg:118.21ms
step:1580/2245 train_time:186782ms step_avg:118.22ms
step:1581/2245 train_time:186899ms step_avg:118.22ms
step:1582/2245 train_time:187021ms step_avg:118.22ms
step:1583/2245 train_time:187139ms step_avg:118.22ms
step:1584/2245 train_time:187262ms step_avg:118.22ms
step:1585/2245 train_time:187378ms step_avg:118.22ms
step:1586/2245 train_time:187501ms step_avg:118.22ms
step:1587/2245 train_time:187617ms step_avg:118.22ms
step:1588/2245 train_time:187740ms step_avg:118.22ms
step:1589/2245 train_time:187857ms step_avg:118.22ms
step:1590/2245 train_time:187979ms step_avg:118.23ms
step:1591/2245 train_time:188097ms step_avg:118.23ms
step:1592/2245 train_time:188220ms step_avg:118.23ms
step:1593/2245 train_time:188336ms step_avg:118.23ms
step:1594/2245 train_time:188460ms step_avg:118.23ms
step:1595/2245 train_time:188577ms step_avg:118.23ms
step:1596/2245 train_time:188700ms step_avg:118.23ms
step:1597/2245 train_time:188817ms step_avg:118.23ms
step:1598/2245 train_time:188940ms step_avg:118.24ms
step:1599/2245 train_time:189056ms step_avg:118.23ms
step:1600/2245 train_time:189179ms step_avg:118.24ms
step:1601/2245 train_time:189296ms step_avg:118.24ms
step:1602/2245 train_time:189419ms step_avg:118.24ms
step:1603/2245 train_time:189535ms step_avg:118.24ms
step:1604/2245 train_time:189658ms step_avg:118.24ms
step:1605/2245 train_time:189774ms step_avg:118.24ms
step:1606/2245 train_time:189897ms step_avg:118.24ms
step:1607/2245 train_time:190014ms step_avg:118.24ms
step:1608/2245 train_time:190138ms step_avg:118.24ms
step:1609/2245 train_time:190254ms step_avg:118.24ms
step:1610/2245 train_time:190377ms step_avg:118.25ms
step:1611/2245 train_time:190494ms step_avg:118.25ms
step:1612/2245 train_time:190616ms step_avg:118.25ms
step:1613/2245 train_time:190733ms step_avg:118.25ms
step:1614/2245 train_time:190856ms step_avg:118.25ms
step:1615/2245 train_time:190972ms step_avg:118.25ms
step:1616/2245 train_time:191096ms step_avg:118.25ms
step:1617/2245 train_time:191212ms step_avg:118.25ms
step:1618/2245 train_time:191334ms step_avg:118.25ms
step:1619/2245 train_time:191451ms step_avg:118.25ms
step:1620/2245 train_time:191573ms step_avg:118.25ms
step:1621/2245 train_time:191689ms step_avg:118.25ms
step:1622/2245 train_time:191812ms step_avg:118.26ms
step:1623/2245 train_time:191928ms step_avg:118.26ms
step:1624/2245 train_time:192051ms step_avg:118.26ms
step:1625/2245 train_time:192167ms step_avg:118.26ms
step:1626/2245 train_time:192290ms step_avg:118.26ms
step:1627/2245 train_time:192406ms step_avg:118.26ms
step:1628/2245 train_time:192529ms step_avg:118.26ms
step:1629/2245 train_time:192645ms step_avg:118.26ms
step:1630/2245 train_time:192769ms step_avg:118.26ms
step:1631/2245 train_time:192885ms step_avg:118.26ms
step:1632/2245 train_time:193008ms step_avg:118.26ms
step:1633/2245 train_time:193124ms step_avg:118.26ms
step:1634/2245 train_time:193248ms step_avg:118.27ms
step:1635/2245 train_time:193365ms step_avg:118.27ms
step:1636/2245 train_time:193488ms step_avg:118.27ms
step:1637/2245 train_time:193605ms step_avg:118.27ms
step:1638/2245 train_time:193728ms step_avg:118.27ms
step:1639/2245 train_time:193844ms step_avg:118.27ms
step:1640/2245 train_time:193967ms step_avg:118.27ms
step:1641/2245 train_time:194083ms step_avg:118.27ms
step:1642/2245 train_time:194206ms step_avg:118.27ms
step:1643/2245 train_time:194322ms step_avg:118.27ms
step:1644/2245 train_time:194445ms step_avg:118.28ms
step:1645/2245 train_time:194562ms step_avg:118.27ms
step:1646/2245 train_time:194684ms step_avg:118.28ms
step:1647/2245 train_time:194801ms step_avg:118.28ms
step:1648/2245 train_time:194925ms step_avg:118.28ms
step:1649/2245 train_time:195041ms step_avg:118.28ms
step:1650/2245 train_time:195164ms step_avg:118.28ms
step:1651/2245 train_time:195281ms step_avg:118.28ms
step:1652/2245 train_time:195404ms step_avg:118.28ms
step:1653/2245 train_time:195521ms step_avg:118.28ms
step:1654/2245 train_time:195644ms step_avg:118.29ms
step:1655/2245 train_time:195760ms step_avg:118.28ms
step:1656/2245 train_time:195882ms step_avg:118.29ms
step:1657/2245 train_time:195998ms step_avg:118.29ms
step:1658/2245 train_time:196122ms step_avg:118.29ms
step:1659/2245 train_time:196238ms step_avg:118.29ms
step:1660/2245 train_time:196361ms step_avg:118.29ms
step:1661/2245 train_time:196477ms step_avg:118.29ms
step:1662/2245 train_time:196600ms step_avg:118.29ms
step:1663/2245 train_time:196718ms step_avg:118.29ms
step:1664/2245 train_time:196840ms step_avg:118.29ms
step:1665/2245 train_time:196957ms step_avg:118.29ms
step:1666/2245 train_time:197080ms step_avg:118.30ms
step:1667/2245 train_time:197196ms step_avg:118.29ms
step:1668/2245 train_time:197319ms step_avg:118.30ms
step:1669/2245 train_time:197436ms step_avg:118.30ms
step:1670/2245 train_time:197559ms step_avg:118.30ms
step:1671/2245 train_time:197675ms step_avg:118.30ms
step:1672/2245 train_time:197798ms step_avg:118.30ms
step:1673/2245 train_time:197916ms step_avg:118.30ms
step:1674/2245 train_time:198039ms step_avg:118.30ms
step:1675/2245 train_time:198155ms step_avg:118.30ms
step:1676/2245 train_time:198278ms step_avg:118.30ms
step:1677/2245 train_time:198395ms step_avg:118.30ms
step:1678/2245 train_time:198518ms step_avg:118.31ms
step:1679/2245 train_time:198635ms step_avg:118.31ms
step:1680/2245 train_time:198759ms step_avg:118.31ms
step:1681/2245 train_time:198876ms step_avg:118.31ms
step:1682/2245 train_time:199000ms step_avg:118.31ms
step:1683/2245 train_time:199116ms step_avg:118.31ms
step:1684/2245 train_time:199239ms step_avg:118.31ms
step:1685/2245 train_time:199356ms step_avg:118.31ms
step:1686/2245 train_time:199479ms step_avg:118.31ms
step:1687/2245 train_time:199595ms step_avg:118.31ms
step:1688/2245 train_time:199718ms step_avg:118.32ms
step:1689/2245 train_time:199835ms step_avg:118.32ms
step:1690/2245 train_time:199958ms step_avg:118.32ms
step:1691/2245 train_time:200075ms step_avg:118.32ms
step:1692/2245 train_time:200198ms step_avg:118.32ms
step:1693/2245 train_time:200315ms step_avg:118.32ms
step:1694/2245 train_time:200438ms step_avg:118.32ms
step:1695/2245 train_time:200554ms step_avg:118.32ms
step:1696/2245 train_time:200677ms step_avg:118.32ms
step:1697/2245 train_time:200794ms step_avg:118.32ms
step:1698/2245 train_time:200917ms step_avg:118.33ms
step:1699/2245 train_time:201033ms step_avg:118.32ms
step:1700/2245 train_time:201157ms step_avg:118.33ms
step:1701/2245 train_time:201273ms step_avg:118.33ms
step:1702/2245 train_time:201396ms step_avg:118.33ms
step:1703/2245 train_time:201512ms step_avg:118.33ms
step:1704/2245 train_time:201635ms step_avg:118.33ms
step:1705/2245 train_time:201751ms step_avg:118.33ms
step:1706/2245 train_time:201875ms step_avg:118.33ms
step:1707/2245 train_time:201991ms step_avg:118.33ms
step:1708/2245 train_time:202114ms step_avg:118.33ms
step:1709/2245 train_time:202231ms step_avg:118.33ms
step:1710/2245 train_time:202354ms step_avg:118.34ms
step:1711/2245 train_time:202470ms step_avg:118.33ms
step:1712/2245 train_time:202593ms step_avg:118.34ms
step:1713/2245 train_time:202709ms step_avg:118.34ms
step:1714/2245 train_time:202832ms step_avg:118.34ms
step:1715/2245 train_time:202948ms step_avg:118.34ms
step:1716/2245 train_time:203072ms step_avg:118.34ms
step:1717/2245 train_time:203188ms step_avg:118.34ms
step:1718/2245 train_time:203311ms step_avg:118.34ms
step:1719/2245 train_time:203427ms step_avg:118.34ms
step:1720/2245 train_time:203550ms step_avg:118.34ms
step:1721/2245 train_time:203666ms step_avg:118.34ms
step:1722/2245 train_time:203789ms step_avg:118.34ms
step:1723/2245 train_time:203905ms step_avg:118.34ms
step:1724/2245 train_time:204028ms step_avg:118.35ms
step:1725/2245 train_time:204145ms step_avg:118.35ms
step:1726/2245 train_time:204269ms step_avg:118.35ms
step:1727/2245 train_time:204385ms step_avg:118.35ms
step:1728/2245 train_time:204508ms step_avg:118.35ms
step:1729/2245 train_time:204624ms step_avg:118.35ms
step:1730/2245 train_time:204748ms step_avg:118.35ms
step:1731/2245 train_time:204864ms step_avg:118.35ms
step:1732/2245 train_time:204987ms step_avg:118.35ms
step:1733/2245 train_time:205104ms step_avg:118.35ms
step:1734/2245 train_time:205227ms step_avg:118.35ms
step:1735/2245 train_time:205344ms step_avg:118.35ms
step:1736/2245 train_time:205467ms step_avg:118.36ms
step:1737/2245 train_time:205583ms step_avg:118.36ms
step:1738/2245 train_time:205706ms step_avg:118.36ms
step:1739/2245 train_time:205822ms step_avg:118.36ms
step:1740/2245 train_time:205945ms step_avg:118.36ms
step:1741/2245 train_time:206061ms step_avg:118.36ms
step:1742/2245 train_time:206184ms step_avg:118.36ms
step:1743/2245 train_time:206301ms step_avg:118.36ms
step:1744/2245 train_time:206424ms step_avg:118.36ms
step:1745/2245 train_time:206541ms step_avg:118.36ms
step:1746/2245 train_time:206664ms step_avg:118.36ms
step:1747/2245 train_time:206780ms step_avg:118.36ms
step:1748/2245 train_time:206903ms step_avg:118.37ms
step:1749/2245 train_time:207020ms step_avg:118.36ms
step:1750/2245 train_time:207144ms step_avg:118.37ms
step:1750/2245 val_loss:3.3783 train_time:207210ms step_avg:118.41ms
step:1751/2245 train_time:207261ms step_avg:118.37ms
step:1752/2245 train_time:207384ms step_avg:118.37ms
step:1753/2245 train_time:207499ms step_avg:118.37ms
step:1754/2245 train_time:207622ms step_avg:118.37ms
step:1755/2245 train_time:207739ms step_avg:118.37ms
step:1756/2245 train_time:207862ms step_avg:118.37ms
step:1757/2245 train_time:207978ms step_avg:118.37ms
step:1758/2245 train_time:208100ms step_avg:118.37ms
step:1759/2245 train_time:208217ms step_avg:118.37ms
step:1760/2245 train_time:208342ms step_avg:118.38ms
step:1761/2245 train_time:208458ms step_avg:118.37ms
step:1762/2245 train_time:208581ms step_avg:118.38ms
step:1763/2245 train_time:208698ms step_avg:118.38ms
step:1764/2245 train_time:208821ms step_avg:118.38ms
step:1765/2245 train_time:208937ms step_avg:118.38ms
step:1766/2245 train_time:209060ms step_avg:118.38ms
step:1767/2245 train_time:209176ms step_avg:118.38ms
step:1768/2245 train_time:209299ms step_avg:118.38ms
step:1769/2245 train_time:209416ms step_avg:118.38ms
step:1770/2245 train_time:209539ms step_avg:118.38ms
step:1771/2245 train_time:209656ms step_avg:118.38ms
step:1772/2245 train_time:209779ms step_avg:118.39ms
step:1773/2245 train_time:209895ms step_avg:118.38ms
step:1774/2245 train_time:210018ms step_avg:118.39ms
step:1775/2245 train_time:210134ms step_avg:118.39ms
step:1776/2245 train_time:210257ms step_avg:118.39ms
step:1777/2245 train_time:210374ms step_avg:118.39ms
step:1778/2245 train_time:210498ms step_avg:118.39ms
step:1779/2245 train_time:210616ms step_avg:118.39ms
step:1780/2245 train_time:210739ms step_avg:118.39ms
step:1781/2245 train_time:210856ms step_avg:118.39ms
step:1782/2245 train_time:210979ms step_avg:118.39ms
step:1783/2245 train_time:211096ms step_avg:118.39ms
step:1784/2245 train_time:211218ms step_avg:118.40ms
step:1785/2245 train_time:211335ms step_avg:118.40ms
step:1786/2245 train_time:211458ms step_avg:118.40ms
step:1787/2245 train_time:211575ms step_avg:118.40ms
step:1788/2245 train_time:211698ms step_avg:118.40ms
step:1789/2245 train_time:211815ms step_avg:118.40ms
step:1790/2245 train_time:211938ms step_avg:118.40ms
step:1791/2245 train_time:212055ms step_avg:118.40ms
step:1792/2245 train_time:212178ms step_avg:118.40ms
step:1793/2245 train_time:212295ms step_avg:118.40ms
step:1794/2245 train_time:212418ms step_avg:118.40ms
step:1795/2245 train_time:212534ms step_avg:118.40ms
step:1796/2245 train_time:212656ms step_avg:118.41ms
step:1797/2245 train_time:212774ms step_avg:118.40ms
step:1798/2245 train_time:212897ms step_avg:118.41ms
step:1799/2245 train_time:213014ms step_avg:118.41ms
step:1800/2245 train_time:213137ms step_avg:118.41ms
step:1801/2245 train_time:213254ms step_avg:118.41ms
step:1802/2245 train_time:213378ms step_avg:118.41ms
step:1803/2245 train_time:213495ms step_avg:118.41ms
step:1804/2245 train_time:213618ms step_avg:118.41ms
step:1805/2245 train_time:213734ms step_avg:118.41ms
step:1806/2245 train_time:213857ms step_avg:118.41ms
step:1807/2245 train_time:213974ms step_avg:118.41ms
step:1808/2245 train_time:214097ms step_avg:118.42ms
step:1809/2245 train_time:214214ms step_avg:118.42ms
step:1810/2245 train_time:214337ms step_avg:118.42ms
step:1811/2245 train_time:214455ms step_avg:118.42ms
step:1812/2245 train_time:214578ms step_avg:118.42ms
step:1813/2245 train_time:214695ms step_avg:118.42ms
step:1814/2245 train_time:214818ms step_avg:118.42ms
step:1815/2245 train_time:214935ms step_avg:118.42ms
step:1816/2245 train_time:215057ms step_avg:118.42ms
step:1817/2245 train_time:215174ms step_avg:118.42ms
step:1818/2245 train_time:215297ms step_avg:118.43ms
step:1819/2245 train_time:215414ms step_avg:118.42ms
step:1820/2245 train_time:215537ms step_avg:118.43ms
step:1821/2245 train_time:215654ms step_avg:118.43ms
step:1822/2245 train_time:215777ms step_avg:118.43ms
step:1823/2245 train_time:215894ms step_avg:118.43ms
step:1824/2245 train_time:216016ms step_avg:118.43ms
step:1825/2245 train_time:216132ms step_avg:118.43ms
step:1826/2245 train_time:216255ms step_avg:118.43ms
step:1827/2245 train_time:216370ms step_avg:118.43ms
step:1828/2245 train_time:216494ms step_avg:118.43ms
step:1829/2245 train_time:216610ms step_avg:118.43ms
step:1830/2245 train_time:216733ms step_avg:118.43ms
step:1831/2245 train_time:216849ms step_avg:118.43ms
step:1832/2245 train_time:216972ms step_avg:118.43ms
step:1833/2245 train_time:217088ms step_avg:118.43ms
step:1834/2245 train_time:217211ms step_avg:118.44ms
step:1835/2245 train_time:217327ms step_avg:118.43ms
step:1836/2245 train_time:217450ms step_avg:118.44ms
step:1837/2245 train_time:217567ms step_avg:118.44ms
step:1838/2245 train_time:217690ms step_avg:118.44ms
step:1839/2245 train_time:217806ms step_avg:118.44ms
step:1840/2245 train_time:217930ms step_avg:118.44ms
step:1841/2245 train_time:218046ms step_avg:118.44ms
step:1842/2245 train_time:218169ms step_avg:118.44ms
step:1843/2245 train_time:218285ms step_avg:118.44ms
step:1844/2245 train_time:218408ms step_avg:118.44ms
step:1845/2245 train_time:218525ms step_avg:118.44ms
step:1846/2245 train_time:218647ms step_avg:118.44ms
step:1847/2245 train_time:218764ms step_avg:118.44ms
step:1848/2245 train_time:218887ms step_avg:118.45ms
step:1849/2245 train_time:219003ms step_avg:118.44ms
step:1850/2245 train_time:219126ms step_avg:118.45ms
step:1851/2245 train_time:219242ms step_avg:118.45ms
step:1852/2245 train_time:219365ms step_avg:118.45ms
step:1853/2245 train_time:219481ms step_avg:118.45ms
step:1854/2245 train_time:219604ms step_avg:118.45ms
step:1855/2245 train_time:219720ms step_avg:118.45ms
step:1856/2245 train_time:219843ms step_avg:118.45ms
step:1857/2245 train_time:219959ms step_avg:118.45ms
step:1858/2245 train_time:220082ms step_avg:118.45ms
step:1859/2245 train_time:220199ms step_avg:118.45ms
step:1860/2245 train_time:220322ms step_avg:118.45ms
step:1861/2245 train_time:220439ms step_avg:118.45ms
step:1862/2245 train_time:220562ms step_avg:118.45ms
step:1863/2245 train_time:220679ms step_avg:118.45ms
step:1864/2245 train_time:220801ms step_avg:118.46ms
step:1865/2245 train_time:220918ms step_avg:118.45ms
step:1866/2245 train_time:221040ms step_avg:118.46ms
step:1867/2245 train_time:221157ms step_avg:118.46ms
step:1868/2245 train_time:221280ms step_avg:118.46ms
step:1869/2245 train_time:221397ms step_avg:118.46ms
step:1870/2245 train_time:221520ms step_avg:118.46ms
step:1871/2245 train_time:221636ms step_avg:118.46ms
step:1872/2245 train_time:221759ms step_avg:118.46ms
step:1873/2245 train_time:221876ms step_avg:118.46ms
step:1874/2245 train_time:222000ms step_avg:118.46ms
step:1875/2245 train_time:222116ms step_avg:118.46ms
step:1876/2245 train_time:222238ms step_avg:118.46ms
step:1877/2245 train_time:222355ms step_avg:118.46ms
step:1878/2245 train_time:222477ms step_avg:118.47ms
step:1879/2245 train_time:222595ms step_avg:118.46ms
step:1880/2245 train_time:222719ms step_avg:118.47ms
step:1881/2245 train_time:222835ms step_avg:118.47ms
step:1882/2245 train_time:222958ms step_avg:118.47ms
step:1883/2245 train_time:223075ms step_avg:118.47ms
step:1884/2245 train_time:223198ms step_avg:118.47ms
step:1885/2245 train_time:223315ms step_avg:118.47ms
step:1886/2245 train_time:223438ms step_avg:118.47ms
step:1887/2245 train_time:223555ms step_avg:118.47ms
step:1888/2245 train_time:223678ms step_avg:118.47ms
step:1889/2245 train_time:223794ms step_avg:118.47ms
step:1890/2245 train_time:223918ms step_avg:118.48ms
step:1891/2245 train_time:224035ms step_avg:118.47ms
step:1892/2245 train_time:224158ms step_avg:118.48ms
step:1893/2245 train_time:224276ms step_avg:118.48ms
step:1894/2245 train_time:224399ms step_avg:118.48ms
step:1895/2245 train_time:224515ms step_avg:118.48ms
step:1896/2245 train_time:224638ms step_avg:118.48ms
step:1897/2245 train_time:224754ms step_avg:118.48ms
step:1898/2245 train_time:224878ms step_avg:118.48ms
step:1899/2245 train_time:224994ms step_avg:118.48ms
step:1900/2245 train_time:225117ms step_avg:118.48ms
step:1901/2245 train_time:225234ms step_avg:118.48ms
step:1902/2245 train_time:225357ms step_avg:118.48ms
step:1903/2245 train_time:225474ms step_avg:118.48ms
step:1904/2245 train_time:225597ms step_avg:118.49ms
step:1905/2245 train_time:225714ms step_avg:118.49ms
step:1906/2245 train_time:225837ms step_avg:118.49ms
step:1907/2245 train_time:225954ms step_avg:118.49ms
step:1908/2245 train_time:226077ms step_avg:118.49ms
step:1909/2245 train_time:226193ms step_avg:118.49ms
step:1910/2245 train_time:226316ms step_avg:118.49ms
step:1911/2245 train_time:226432ms step_avg:118.49ms
step:1912/2245 train_time:226555ms step_avg:118.49ms
step:1913/2245 train_time:226671ms step_avg:118.49ms
step:1914/2245 train_time:226795ms step_avg:118.49ms
step:1915/2245 train_time:226911ms step_avg:118.49ms
step:1916/2245 train_time:227034ms step_avg:118.49ms
step:1917/2245 train_time:227151ms step_avg:118.49ms
step:1918/2245 train_time:227274ms step_avg:118.50ms
step:1919/2245 train_time:227390ms step_avg:118.49ms
step:1920/2245 train_time:227513ms step_avg:118.50ms
step:1921/2245 train_time:227629ms step_avg:118.50ms
step:1922/2245 train_time:227752ms step_avg:118.50ms
step:1923/2245 train_time:227869ms step_avg:118.50ms
step:1924/2245 train_time:227992ms step_avg:118.50ms
step:1925/2245 train_time:228108ms step_avg:118.50ms
step:1926/2245 train_time:228231ms step_avg:118.50ms
step:1927/2245 train_time:228347ms step_avg:118.50ms
step:1928/2245 train_time:228470ms step_avg:118.50ms
step:1929/2245 train_time:228587ms step_avg:118.50ms
step:1930/2245 train_time:228710ms step_avg:118.50ms
step:1931/2245 train_time:228826ms step_avg:118.50ms
step:1932/2245 train_time:228949ms step_avg:118.50ms
step:1933/2245 train_time:229066ms step_avg:118.50ms
step:1934/2245 train_time:229188ms step_avg:118.50ms
step:1935/2245 train_time:229305ms step_avg:118.50ms
step:1936/2245 train_time:229428ms step_avg:118.51ms
step:1937/2245 train_time:229545ms step_avg:118.51ms
step:1938/2245 train_time:229668ms step_avg:118.51ms
step:1939/2245 train_time:229784ms step_avg:118.51ms
step:1940/2245 train_time:229907ms step_avg:118.51ms
step:1941/2245 train_time:230024ms step_avg:118.51ms
step:1942/2245 train_time:230146ms step_avg:118.51ms
step:1943/2245 train_time:230264ms step_avg:118.51ms
step:1944/2245 train_time:230387ms step_avg:118.51ms
step:1945/2245 train_time:230503ms step_avg:118.51ms
step:1946/2245 train_time:230625ms step_avg:118.51ms
step:1947/2245 train_time:230742ms step_avg:118.51ms
step:1948/2245 train_time:230866ms step_avg:118.51ms
step:1949/2245 train_time:230982ms step_avg:118.51ms
step:1950/2245 train_time:231105ms step_avg:118.52ms
step:1951/2245 train_time:231223ms step_avg:118.51ms
step:1952/2245 train_time:231345ms step_avg:118.52ms
step:1953/2245 train_time:231462ms step_avg:118.52ms
step:1954/2245 train_time:231585ms step_avg:118.52ms
step:1955/2245 train_time:231701ms step_avg:118.52ms
step:1956/2245 train_time:231824ms step_avg:118.52ms
step:1957/2245 train_time:231940ms step_avg:118.52ms
step:1958/2245 train_time:232064ms step_avg:118.52ms
step:1959/2245 train_time:232180ms step_avg:118.52ms
step:1960/2245 train_time:232304ms step_avg:118.52ms
step:1961/2245 train_time:232420ms step_avg:118.52ms
step:1962/2245 train_time:232544ms step_avg:118.52ms
step:1963/2245 train_time:232660ms step_avg:118.52ms
step:1964/2245 train_time:232783ms step_avg:118.53ms
step:1965/2245 train_time:232900ms step_avg:118.52ms
step:1966/2245 train_time:233023ms step_avg:118.53ms
step:1967/2245 train_time:233139ms step_avg:118.53ms
step:1968/2245 train_time:233263ms step_avg:118.53ms
step:1969/2245 train_time:233379ms step_avg:118.53ms
step:1970/2245 train_time:233502ms step_avg:118.53ms
step:1971/2245 train_time:233618ms step_avg:118.53ms
step:1972/2245 train_time:233741ms step_avg:118.53ms
step:1973/2245 train_time:233858ms step_avg:118.53ms
step:1974/2245 train_time:233981ms step_avg:118.53ms
step:1975/2245 train_time:234098ms step_avg:118.53ms
step:1976/2245 train_time:234221ms step_avg:118.53ms
step:1977/2245 train_time:234337ms step_avg:118.53ms
step:1978/2245 train_time:234460ms step_avg:118.53ms
step:1979/2245 train_time:234577ms step_avg:118.53ms
step:1980/2245 train_time:234699ms step_avg:118.53ms
step:1981/2245 train_time:234816ms step_avg:118.53ms
step:1982/2245 train_time:234939ms step_avg:118.54ms
step:1983/2245 train_time:235056ms step_avg:118.54ms
step:1984/2245 train_time:235179ms step_avg:118.54ms
step:1985/2245 train_time:235296ms step_avg:118.54ms
step:1986/2245 train_time:235419ms step_avg:118.54ms
step:1987/2245 train_time:235535ms step_avg:118.54ms
step:1988/2245 train_time:235658ms step_avg:118.54ms
step:1989/2245 train_time:235775ms step_avg:118.54ms
step:1990/2245 train_time:235898ms step_avg:118.54ms
step:1991/2245 train_time:236015ms step_avg:118.54ms
step:1992/2245 train_time:236137ms step_avg:118.54ms
step:1993/2245 train_time:236255ms step_avg:118.54ms
step:1994/2245 train_time:236379ms step_avg:118.54ms
step:1995/2245 train_time:236496ms step_avg:118.54ms
step:1996/2245 train_time:236618ms step_avg:118.55ms
step:1997/2245 train_time:236736ms step_avg:118.55ms
step:1998/2245 train_time:236859ms step_avg:118.55ms
step:1999/2245 train_time:236976ms step_avg:118.55ms
step:2000/2245 train_time:237099ms step_avg:118.55ms
step:2000/2245 val_loss:3.3239 train_time:237165ms step_avg:118.58ms
step:2001/2245 train_time:237216ms step_avg:118.55ms
step:2002/2245 train_time:237338ms step_avg:118.55ms
step:2003/2245 train_time:237454ms step_avg:118.55ms
step:2004/2245 train_time:237577ms step_avg:118.55ms
step:2005/2245 train_time:237694ms step_avg:118.55ms
step:2006/2245 train_time:237817ms step_avg:118.55ms
step:2007/2245 train_time:237934ms step_avg:118.55ms
step:2008/2245 train_time:238056ms step_avg:118.55ms
step:2009/2245 train_time:238173ms step_avg:118.55ms
step:2010/2245 train_time:238297ms step_avg:118.56ms
step:2011/2245 train_time:238413ms step_avg:118.55ms
step:2012/2245 train_time:238536ms step_avg:118.56ms
step:2013/2245 train_time:238652ms step_avg:118.56ms
step:2014/2245 train_time:238775ms step_avg:118.56ms
step:2015/2245 train_time:238892ms step_avg:118.56ms
step:2016/2245 train_time:239015ms step_avg:118.56ms
step:2017/2245 train_time:239131ms step_avg:118.56ms
step:2018/2245 train_time:239254ms step_avg:118.56ms
step:2019/2245 train_time:239370ms step_avg:118.56ms
step:2020/2245 train_time:239493ms step_avg:118.56ms
step:2021/2245 train_time:239609ms step_avg:118.56ms
step:2022/2245 train_time:239732ms step_avg:118.56ms
step:2023/2245 train_time:239848ms step_avg:118.56ms
step:2024/2245 train_time:239971ms step_avg:118.56ms
step:2025/2245 train_time:240088ms step_avg:118.56ms
step:2026/2245 train_time:240210ms step_avg:118.56ms
step:2027/2245 train_time:240327ms step_avg:118.56ms
step:2028/2245 train_time:240450ms step_avg:118.57ms
step:2029/2245 train_time:240566ms step_avg:118.56ms
step:2030/2245 train_time:240688ms step_avg:118.57ms
step:2031/2245 train_time:240805ms step_avg:118.56ms
step:2032/2245 train_time:240928ms step_avg:118.57ms
step:2033/2245 train_time:241044ms step_avg:118.57ms
step:2034/2245 train_time:241168ms step_avg:118.57ms
step:2035/2245 train_time:241284ms step_avg:118.57ms
step:2036/2245 train_time:241407ms step_avg:118.57ms
step:2037/2245 train_time:241524ms step_avg:118.57ms
step:2038/2245 train_time:241647ms step_avg:118.57ms
step:2039/2245 train_time:241763ms step_avg:118.57ms
step:2040/2245 train_time:241885ms step_avg:118.57ms
step:2041/2245 train_time:242002ms step_avg:118.57ms
step:2042/2245 train_time:242125ms step_avg:118.57ms
step:2043/2245 train_time:242241ms step_avg:118.57ms
step:2044/2245 train_time:242365ms step_avg:118.57ms
step:2045/2245 train_time:242481ms step_avg:118.57ms
step:2046/2245 train_time:242605ms step_avg:118.58ms
step:2047/2245 train_time:242721ms step_avg:118.57ms
step:2048/2245 train_time:242844ms step_avg:118.58ms
step:2049/2245 train_time:242961ms step_avg:118.58ms
step:2050/2245 train_time:243084ms step_avg:118.58ms
step:2051/2245 train_time:243200ms step_avg:118.58ms
step:2052/2245 train_time:243323ms step_avg:118.58ms
step:2053/2245 train_time:243439ms step_avg:118.58ms
step:2054/2245 train_time:243563ms step_avg:118.58ms
step:2055/2245 train_time:243680ms step_avg:118.58ms
step:2056/2245 train_time:243802ms step_avg:118.58ms
step:2057/2245 train_time:243919ms step_avg:118.58ms
step:2058/2245 train_time:244042ms step_avg:118.58ms
step:2059/2245 train_time:244159ms step_avg:118.58ms
step:2060/2245 train_time:244281ms step_avg:118.58ms
step:2061/2245 train_time:244399ms step_avg:118.58ms
step:2062/2245 train_time:244521ms step_avg:118.58ms
step:2063/2245 train_time:244638ms step_avg:118.58ms
step:2064/2245 train_time:244761ms step_avg:118.59ms
step:2065/2245 train_time:244879ms step_avg:118.59ms
step:2066/2245 train_time:245001ms step_avg:118.59ms
step:2067/2245 train_time:245118ms step_avg:118.59ms
step:2068/2245 train_time:245241ms step_avg:118.59ms
step:2069/2245 train_time:245357ms step_avg:118.59ms
step:2070/2245 train_time:245480ms step_avg:118.59ms
step:2071/2245 train_time:245597ms step_avg:118.59ms
step:2072/2245 train_time:245720ms step_avg:118.59ms
step:2073/2245 train_time:245837ms step_avg:118.59ms
step:2074/2245 train_time:245960ms step_avg:118.59ms
step:2075/2245 train_time:246077ms step_avg:118.59ms
step:2076/2245 train_time:246199ms step_avg:118.59ms
step:2077/2245 train_time:246316ms step_avg:118.59ms
step:2078/2245 train_time:246438ms step_avg:118.59ms
step:2079/2245 train_time:246555ms step_avg:118.59ms
step:2080/2245 train_time:246678ms step_avg:118.60ms
step:2081/2245 train_time:246795ms step_avg:118.59ms
step:2082/2245 train_time:246919ms step_avg:118.60ms
step:2083/2245 train_time:247036ms step_avg:118.60ms
step:2084/2245 train_time:247159ms step_avg:118.60ms
step:2085/2245 train_time:247276ms step_avg:118.60ms
step:2086/2245 train_time:247399ms step_avg:118.60ms
step:2087/2245 train_time:247515ms step_avg:118.60ms
step:2088/2245 train_time:247639ms step_avg:118.60ms
step:2089/2245 train_time:247755ms step_avg:118.60ms
step:2090/2245 train_time:247879ms step_avg:118.60ms
step:2091/2245 train_time:247996ms step_avg:118.60ms
step:2092/2245 train_time:248119ms step_avg:118.60ms
step:2093/2245 train_time:248236ms step_avg:118.60ms
step:2094/2245 train_time:248359ms step_avg:118.61ms
step:2095/2245 train_time:248476ms step_avg:118.60ms
step:2096/2245 train_time:248599ms step_avg:118.61ms
step:2097/2245 train_time:248715ms step_avg:118.61ms
step:2098/2245 train_time:248839ms step_avg:118.61ms
step:2099/2245 train_time:248955ms step_avg:118.61ms
step:2100/2245 train_time:249079ms step_avg:118.61ms
step:2101/2245 train_time:249196ms step_avg:118.61ms
step:2102/2245 train_time:249319ms step_avg:118.61ms
step:2103/2245 train_time:249436ms step_avg:118.61ms
step:2104/2245 train_time:249558ms step_avg:118.61ms
step:2105/2245 train_time:249676ms step_avg:118.61ms
step:2106/2245 train_time:249798ms step_avg:118.61ms
step:2107/2245 train_time:249915ms step_avg:118.61ms
step:2108/2245 train_time:250038ms step_avg:118.61ms
step:2109/2245 train_time:250155ms step_avg:118.61ms
step:2110/2245 train_time:250279ms step_avg:118.62ms
step:2111/2245 train_time:250395ms step_avg:118.61ms
step:2112/2245 train_time:250518ms step_avg:118.62ms
step:2113/2245 train_time:250635ms step_avg:118.62ms
step:2114/2245 train_time:250759ms step_avg:118.62ms
step:2115/2245 train_time:250876ms step_avg:118.62ms
step:2116/2245 train_time:250998ms step_avg:118.62ms
step:2117/2245 train_time:251115ms step_avg:118.62ms
step:2118/2245 train_time:251239ms step_avg:118.62ms
step:2119/2245 train_time:251355ms step_avg:118.62ms
step:2120/2245 train_time:251478ms step_avg:118.62ms
step:2121/2245 train_time:251595ms step_avg:118.62ms
step:2122/2245 train_time:251718ms step_avg:118.62ms
step:2123/2245 train_time:251835ms step_avg:118.62ms
step:2124/2245 train_time:251958ms step_avg:118.62ms
step:2125/2245 train_time:252075ms step_avg:118.62ms
step:2126/2245 train_time:252199ms step_avg:118.63ms
step:2127/2245 train_time:252315ms step_avg:118.62ms
step:2128/2245 train_time:252439ms step_avg:118.63ms
step:2129/2245 train_time:252555ms step_avg:118.63ms
step:2130/2245 train_time:252678ms step_avg:118.63ms
step:2131/2245 train_time:252795ms step_avg:118.63ms
step:2132/2245 train_time:252919ms step_avg:118.63ms
step:2133/2245 train_time:253036ms step_avg:118.63ms
step:2134/2245 train_time:253159ms step_avg:118.63ms
step:2135/2245 train_time:253276ms step_avg:118.63ms
step:2136/2245 train_time:253399ms step_avg:118.63ms
step:2137/2245 train_time:253515ms step_avg:118.63ms
step:2138/2245 train_time:253637ms step_avg:118.63ms
step:2139/2245 train_time:253754ms step_avg:118.63ms
step:2140/2245 train_time:253877ms step_avg:118.63ms
step:2141/2245 train_time:253993ms step_avg:118.63ms
step:2142/2245 train_time:254116ms step_avg:118.63ms
step:2143/2245 train_time:254232ms step_avg:118.63ms
step:2144/2245 train_time:254356ms step_avg:118.64ms
step:2145/2245 train_time:254472ms step_avg:118.64ms
step:2146/2245 train_time:254596ms step_avg:118.64ms
step:2147/2245 train_time:254712ms step_avg:118.64ms
step:2148/2245 train_time:254834ms step_avg:118.64ms
step:2149/2245 train_time:254950ms step_avg:118.64ms
step:2150/2245 train_time:255074ms step_avg:118.64ms
step:2151/2245 train_time:255190ms step_avg:118.64ms
step:2152/2245 train_time:255313ms step_avg:118.64ms
step:2153/2245 train_time:255429ms step_avg:118.64ms
step:2154/2245 train_time:255552ms step_avg:118.64ms
step:2155/2245 train_time:255668ms step_avg:118.64ms
step:2156/2245 train_time:255791ms step_avg:118.64ms
step:2157/2245 train_time:255907ms step_avg:118.64ms
step:2158/2245 train_time:256030ms step_avg:118.64ms
step:2159/2245 train_time:256146ms step_avg:118.64ms
step:2160/2245 train_time:256269ms step_avg:118.64ms
step:2161/2245 train_time:256386ms step_avg:118.64ms
step:2162/2245 train_time:256508ms step_avg:118.64ms
step:2163/2245 train_time:256625ms step_avg:118.64ms
step:2164/2245 train_time:256748ms step_avg:118.65ms
step:2165/2245 train_time:256865ms step_avg:118.64ms
step:2166/2245 train_time:256988ms step_avg:118.65ms
step:2167/2245 train_time:257104ms step_avg:118.64ms
step:2168/2245 train_time:257227ms step_avg:118.65ms
step:2169/2245 train_time:257343ms step_avg:118.65ms
step:2170/2245 train_time:257467ms step_avg:118.65ms
step:2171/2245 train_time:257583ms step_avg:118.65ms
step:2172/2245 train_time:257706ms step_avg:118.65ms
step:2173/2245 train_time:257823ms step_avg:118.65ms
step:2174/2245 train_time:257946ms step_avg:118.65ms
step:2175/2245 train_time:258063ms step_avg:118.65ms
step:2176/2245 train_time:258185ms step_avg:118.65ms
step:2177/2245 train_time:258302ms step_avg:118.65ms
step:2178/2245 train_time:258425ms step_avg:118.65ms
step:2179/2245 train_time:258542ms step_avg:118.65ms
step:2180/2245 train_time:258666ms step_avg:118.65ms
step:2181/2245 train_time:258782ms step_avg:118.65ms
step:2182/2245 train_time:258905ms step_avg:118.65ms
step:2183/2245 train_time:259022ms step_avg:118.65ms
step:2184/2245 train_time:259145ms step_avg:118.66ms
step:2185/2245 train_time:259262ms step_avg:118.66ms
step:2186/2245 train_time:259385ms step_avg:118.66ms
step:2187/2245 train_time:259502ms step_avg:118.66ms
step:2188/2245 train_time:259626ms step_avg:118.66ms
step:2189/2245 train_time:259742ms step_avg:118.66ms
step:2190/2245 train_time:259865ms step_avg:118.66ms
step:2191/2245 train_time:259981ms step_avg:118.66ms
step:2192/2245 train_time:260105ms step_avg:118.66ms
step:2193/2245 train_time:260221ms step_avg:118.66ms
step:2194/2245 train_time:260344ms step_avg:118.66ms
step:2195/2245 train_time:260460ms step_avg:118.66ms
step:2196/2245 train_time:260583ms step_avg:118.66ms
step:2197/2245 train_time:260699ms step_avg:118.66ms
step:2198/2245 train_time:260823ms step_avg:118.66ms
step:2199/2245 train_time:260939ms step_avg:118.66ms
step:2200/2245 train_time:261063ms step_avg:118.66ms
step:2201/2245 train_time:261179ms step_avg:118.66ms
step:2202/2245 train_time:261302ms step_avg:118.67ms
step:2203/2245 train_time:261419ms step_avg:118.67ms
step:2204/2245 train_time:261542ms step_avg:118.67ms
step:2205/2245 train_time:261659ms step_avg:118.67ms
step:2206/2245 train_time:261782ms step_avg:118.67ms
step:2207/2245 train_time:261899ms step_avg:118.67ms
step:2208/2245 train_time:262023ms step_avg:118.67ms
step:2209/2245 train_time:262141ms step_avg:118.67ms
step:2210/2245 train_time:262264ms step_avg:118.67ms
step:2211/2245 train_time:262381ms step_avg:118.67ms
step:2212/2245 train_time:262505ms step_avg:118.67ms
step:2213/2245 train_time:262622ms step_avg:118.67ms
step:2214/2245 train_time:262745ms step_avg:118.67ms
step:2215/2245 train_time:262862ms step_avg:118.67ms
step:2216/2245 train_time:262986ms step_avg:118.68ms
step:2217/2245 train_time:263103ms step_avg:118.68ms
step:2218/2245 train_time:263226ms step_avg:118.68ms
step:2219/2245 train_time:263342ms step_avg:118.68ms
step:2220/2245 train_time:263466ms step_avg:118.68ms
step:2221/2245 train_time:263582ms step_avg:118.68ms
step:2222/2245 train_time:263705ms step_avg:118.68ms
step:2223/2245 train_time:263822ms step_avg:118.68ms
step:2224/2245 train_time:263946ms step_avg:118.68ms
step:2225/2245 train_time:264062ms step_avg:118.68ms
step:2226/2245 train_time:264186ms step_avg:118.68ms
step:2227/2245 train_time:264303ms step_avg:118.68ms
step:2228/2245 train_time:264426ms step_avg:118.68ms
step:2229/2245 train_time:264543ms step_avg:118.68ms
step:2230/2245 train_time:264667ms step_avg:118.68ms
step:2231/2245 train_time:264783ms step_avg:118.68ms
step:2232/2245 train_time:264906ms step_avg:118.69ms
step:2233/2245 train_time:265024ms step_avg:118.68ms
step:2234/2245 train_time:265146ms step_avg:118.69ms
step:2235/2245 train_time:265263ms step_avg:118.69ms
step:2236/2245 train_time:265386ms step_avg:118.69ms
step:2237/2245 train_time:265502ms step_avg:118.69ms
step:2238/2245 train_time:265626ms step_avg:118.69ms
step:2239/2245 train_time:265743ms step_avg:118.69ms
step:2240/2245 train_time:265866ms step_avg:118.69ms
step:2241/2245 train_time:265983ms step_avg:118.69ms
step:2242/2245 train_time:266107ms step_avg:118.69ms
step:2243/2245 train_time:266223ms step_avg:118.69ms
step:2244/2245 train_time:266347ms step_avg:118.69ms
step:2245/2245 train_time:266464ms step_avg:118.69ms
step:2245/2245 val_loss:3.2780 train_time:266532ms step_avg:118.72ms
peak memory allocated: 30321 MiB reserved: 44436 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 09:32:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   40C    P0            114W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   40C    P0            118W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   40C    P0            118W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   40C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3294491      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3294492      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3294493      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3294494      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3294492      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3294493      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3294494      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:166ms step_avg:166.16ms
step:2/2245 train_time:220ms step_avg:110.10ms
step:3/2245 train_time:329ms step_avg:109.71ms
step:4/2245 train_time:447ms step_avg:111.84ms
step:5/2245 train_time:563ms step_avg:112.54ms
step:6/2245 train_time:686ms step_avg:114.26ms
step:7/2245 train_time:801ms step_avg:114.47ms
step:8/2245 train_time:924ms step_avg:115.50ms
step:9/2245 train_time:1040ms step_avg:115.55ms
step:10/2245 train_time:1163ms step_avg:116.28ms
step:11/2245 train_time:1279ms step_avg:116.29ms
step:12/2245 train_time:1402ms step_avg:116.84ms
step:13/2245 train_time:1519ms step_avg:116.82ms
step:14/2245 train_time:1642ms step_avg:117.27ms
step:15/2245 train_time:1758ms step_avg:117.21ms
step:16/2245 train_time:1881ms step_avg:117.58ms
step:17/2245 train_time:1998ms step_avg:117.53ms
step:18/2245 train_time:2121ms step_avg:117.85ms
step:19/2245 train_time:2238ms step_avg:117.78ms
step:20/2245 train_time:2361ms step_avg:118.04ms
step:21/2245 train_time:2477ms step_avg:117.94ms
step:22/2245 train_time:2599ms step_avg:118.16ms
step:23/2245 train_time:2716ms step_avg:118.08ms
step:24/2245 train_time:2838ms step_avg:118.26ms
step:25/2245 train_time:2954ms step_avg:118.17ms
step:26/2245 train_time:3077ms step_avg:118.33ms
step:27/2245 train_time:3193ms step_avg:118.25ms
step:28/2245 train_time:3315ms step_avg:118.40ms
step:29/2245 train_time:3431ms step_avg:118.32ms
step:30/2245 train_time:3553ms step_avg:118.45ms
step:31/2245 train_time:3669ms step_avg:118.36ms
step:32/2245 train_time:3792ms step_avg:118.49ms
step:33/2245 train_time:3908ms step_avg:118.42ms
step:34/2245 train_time:4030ms step_avg:118.54ms
step:35/2245 train_time:4146ms step_avg:118.47ms
step:36/2245 train_time:4269ms step_avg:118.57ms
step:37/2245 train_time:4385ms step_avg:118.51ms
step:38/2245 train_time:4507ms step_avg:118.60ms
step:39/2245 train_time:4623ms step_avg:118.53ms
step:40/2245 train_time:4744ms step_avg:118.61ms
step:41/2245 train_time:4860ms step_avg:118.53ms
step:42/2245 train_time:4982ms step_avg:118.61ms
step:43/2245 train_time:5097ms step_avg:118.54ms
step:44/2245 train_time:5219ms step_avg:118.62ms
step:45/2245 train_time:5335ms step_avg:118.55ms
step:46/2245 train_time:5457ms step_avg:118.62ms
step:47/2245 train_time:5572ms step_avg:118.56ms
step:48/2245 train_time:5694ms step_avg:118.62ms
step:49/2245 train_time:5809ms step_avg:118.56ms
step:50/2245 train_time:5931ms step_avg:118.62ms
step:51/2245 train_time:6046ms step_avg:118.56ms
step:52/2245 train_time:6168ms step_avg:118.62ms
step:53/2245 train_time:6284ms step_avg:118.57ms
step:54/2245 train_time:6406ms step_avg:118.62ms
step:55/2245 train_time:6521ms step_avg:118.56ms
step:56/2245 train_time:6642ms step_avg:118.61ms
step:57/2245 train_time:6758ms step_avg:118.56ms
step:58/2245 train_time:6880ms step_avg:118.62ms
step:59/2245 train_time:6995ms step_avg:118.56ms
step:60/2245 train_time:7118ms step_avg:118.63ms
step:61/2245 train_time:7233ms step_avg:118.57ms
step:62/2245 train_time:7355ms step_avg:118.62ms
step:63/2245 train_time:7470ms step_avg:118.57ms
step:64/2245 train_time:7592ms step_avg:118.63ms
step:65/2245 train_time:7707ms step_avg:118.57ms
step:66/2245 train_time:7829ms step_avg:118.62ms
step:67/2245 train_time:7944ms step_avg:118.56ms
step:68/2245 train_time:8065ms step_avg:118.61ms
step:69/2245 train_time:8180ms step_avg:118.55ms
step:70/2245 train_time:8301ms step_avg:118.59ms
step:71/2245 train_time:8417ms step_avg:118.55ms
step:72/2245 train_time:8538ms step_avg:118.58ms
step:73/2245 train_time:8653ms step_avg:118.54ms
step:74/2245 train_time:8775ms step_avg:118.58ms
step:75/2245 train_time:8890ms step_avg:118.54ms
step:76/2245 train_time:9012ms step_avg:118.58ms
step:77/2245 train_time:9127ms step_avg:118.53ms
step:78/2245 train_time:9248ms step_avg:118.56ms
step:79/2245 train_time:9364ms step_avg:118.53ms
step:80/2245 train_time:9485ms step_avg:118.56ms
step:81/2245 train_time:9600ms step_avg:118.52ms
step:82/2245 train_time:9722ms step_avg:118.56ms
step:83/2245 train_time:9837ms step_avg:118.51ms
step:84/2245 train_time:9958ms step_avg:118.55ms
step:85/2245 train_time:10073ms step_avg:118.51ms
step:86/2245 train_time:10195ms step_avg:118.55ms
step:87/2245 train_time:10310ms step_avg:118.51ms
step:88/2245 train_time:10431ms step_avg:118.54ms
step:89/2245 train_time:10546ms step_avg:118.50ms
step:90/2245 train_time:10668ms step_avg:118.53ms
step:91/2245 train_time:10783ms step_avg:118.49ms
step:92/2245 train_time:10904ms step_avg:118.52ms
step:93/2245 train_time:11019ms step_avg:118.48ms
step:94/2245 train_time:11140ms step_avg:118.51ms
step:95/2245 train_time:11255ms step_avg:118.47ms
step:96/2245 train_time:11376ms step_avg:118.50ms
step:97/2245 train_time:11491ms step_avg:118.47ms
step:98/2245 train_time:11613ms step_avg:118.50ms
step:99/2245 train_time:11728ms step_avg:118.46ms
step:100/2245 train_time:11849ms step_avg:118.49ms
step:101/2245 train_time:11964ms step_avg:118.45ms
step:102/2245 train_time:12085ms step_avg:118.48ms
step:103/2245 train_time:12200ms step_avg:118.44ms
step:104/2245 train_time:12321ms step_avg:118.47ms
step:105/2245 train_time:12436ms step_avg:118.44ms
step:106/2245 train_time:12557ms step_avg:118.47ms
step:107/2245 train_time:12672ms step_avg:118.43ms
step:108/2245 train_time:12793ms step_avg:118.46ms
step:109/2245 train_time:12908ms step_avg:118.42ms
step:110/2245 train_time:13029ms step_avg:118.45ms
step:111/2245 train_time:13144ms step_avg:118.41ms
step:112/2245 train_time:13265ms step_avg:118.44ms
step:113/2245 train_time:13380ms step_avg:118.41ms
step:114/2245 train_time:13501ms step_avg:118.43ms
step:115/2245 train_time:13616ms step_avg:118.40ms
step:116/2245 train_time:13737ms step_avg:118.42ms
step:117/2245 train_time:13851ms step_avg:118.39ms
step:118/2245 train_time:13972ms step_avg:118.40ms
step:119/2245 train_time:14087ms step_avg:118.38ms
step:120/2245 train_time:14208ms step_avg:118.40ms
step:121/2245 train_time:14323ms step_avg:118.37ms
step:122/2245 train_time:14444ms step_avg:118.39ms
step:123/2245 train_time:14558ms step_avg:118.36ms
step:124/2245 train_time:14679ms step_avg:118.38ms
step:125/2245 train_time:14793ms step_avg:118.35ms
step:126/2245 train_time:14914ms step_avg:118.37ms
step:127/2245 train_time:15029ms step_avg:118.34ms
step:128/2245 train_time:15150ms step_avg:118.36ms
step:129/2245 train_time:15265ms step_avg:118.33ms
step:130/2245 train_time:15386ms step_avg:118.35ms
step:131/2245 train_time:15500ms step_avg:118.32ms
step:132/2245 train_time:15621ms step_avg:118.34ms
step:133/2245 train_time:15736ms step_avg:118.31ms
step:134/2245 train_time:15857ms step_avg:118.33ms
step:135/2245 train_time:15971ms step_avg:118.31ms
step:136/2245 train_time:16092ms step_avg:118.33ms
step:137/2245 train_time:16207ms step_avg:118.30ms
step:138/2245 train_time:16328ms step_avg:118.32ms
step:139/2245 train_time:16443ms step_avg:118.29ms
step:140/2245 train_time:16563ms step_avg:118.31ms
step:141/2245 train_time:16678ms step_avg:118.28ms
step:142/2245 train_time:16799ms step_avg:118.30ms
step:143/2245 train_time:16914ms step_avg:118.28ms
step:144/2245 train_time:17034ms step_avg:118.29ms
step:145/2245 train_time:17149ms step_avg:118.27ms
step:146/2245 train_time:17270ms step_avg:118.29ms
step:147/2245 train_time:17385ms step_avg:118.26ms
step:148/2245 train_time:17505ms step_avg:118.28ms
step:149/2245 train_time:17619ms step_avg:118.25ms
step:150/2245 train_time:17740ms step_avg:118.27ms
step:151/2245 train_time:17854ms step_avg:118.24ms
step:152/2245 train_time:17975ms step_avg:118.26ms
step:153/2245 train_time:18090ms step_avg:118.24ms
step:154/2245 train_time:18211ms step_avg:118.25ms
step:155/2245 train_time:18326ms step_avg:118.23ms
step:156/2245 train_time:18446ms step_avg:118.25ms
step:157/2245 train_time:18561ms step_avg:118.22ms
step:158/2245 train_time:18682ms step_avg:118.24ms
step:159/2245 train_time:18796ms step_avg:118.22ms
step:160/2245 train_time:18917ms step_avg:118.23ms
step:161/2245 train_time:19031ms step_avg:118.21ms
step:162/2245 train_time:19152ms step_avg:118.22ms
step:163/2245 train_time:19266ms step_avg:118.20ms
step:164/2245 train_time:19387ms step_avg:118.21ms
step:165/2245 train_time:19501ms step_avg:118.19ms
step:166/2245 train_time:19622ms step_avg:118.20ms
step:167/2245 train_time:19736ms step_avg:118.18ms
step:168/2245 train_time:19857ms step_avg:118.19ms
step:169/2245 train_time:19971ms step_avg:118.17ms
step:170/2245 train_time:20092ms step_avg:118.19ms
step:171/2245 train_time:20206ms step_avg:118.16ms
step:172/2245 train_time:20327ms step_avg:118.18ms
step:173/2245 train_time:20441ms step_avg:118.16ms
step:174/2245 train_time:20562ms step_avg:118.17ms
step:175/2245 train_time:20676ms step_avg:118.15ms
step:176/2245 train_time:20797ms step_avg:118.17ms
step:177/2245 train_time:20911ms step_avg:118.14ms
step:178/2245 train_time:21032ms step_avg:118.16ms
step:179/2245 train_time:21146ms step_avg:118.13ms
step:180/2245 train_time:21267ms step_avg:118.15ms
step:181/2245 train_time:21381ms step_avg:118.13ms
step:182/2245 train_time:21502ms step_avg:118.14ms
step:183/2245 train_time:21616ms step_avg:118.12ms
step:184/2245 train_time:21737ms step_avg:118.13ms
step:185/2245 train_time:21851ms step_avg:118.11ms
step:186/2245 train_time:21971ms step_avg:118.13ms
step:187/2245 train_time:22086ms step_avg:118.11ms
step:188/2245 train_time:22206ms step_avg:118.11ms
step:189/2245 train_time:22320ms step_avg:118.10ms
step:190/2245 train_time:22441ms step_avg:118.11ms
step:191/2245 train_time:22555ms step_avg:118.09ms
step:192/2245 train_time:22675ms step_avg:118.10ms
step:193/2245 train_time:22790ms step_avg:118.08ms
step:194/2245 train_time:22911ms step_avg:118.10ms
step:195/2245 train_time:23025ms step_avg:118.08ms
step:196/2245 train_time:23146ms step_avg:118.09ms
step:197/2245 train_time:23260ms step_avg:118.07ms
step:198/2245 train_time:23380ms step_avg:118.08ms
step:199/2245 train_time:23494ms step_avg:118.06ms
step:200/2245 train_time:23615ms step_avg:118.08ms
step:201/2245 train_time:23729ms step_avg:118.06ms
step:202/2245 train_time:23850ms step_avg:118.07ms
step:203/2245 train_time:23964ms step_avg:118.05ms
step:204/2245 train_time:24085ms step_avg:118.06ms
step:205/2245 train_time:24199ms step_avg:118.04ms
step:206/2245 train_time:24319ms step_avg:118.05ms
step:207/2245 train_time:24433ms step_avg:118.04ms
step:208/2245 train_time:24554ms step_avg:118.05ms
step:209/2245 train_time:24668ms step_avg:118.03ms
step:210/2245 train_time:24789ms step_avg:118.04ms
step:211/2245 train_time:24903ms step_avg:118.02ms
step:212/2245 train_time:25024ms step_avg:118.04ms
step:213/2245 train_time:25138ms step_avg:118.02ms
step:214/2245 train_time:25259ms step_avg:118.03ms
step:215/2245 train_time:25373ms step_avg:118.01ms
step:216/2245 train_time:25494ms step_avg:118.03ms
step:217/2245 train_time:25608ms step_avg:118.01ms
step:218/2245 train_time:25729ms step_avg:118.02ms
step:219/2245 train_time:25843ms step_avg:118.00ms
step:220/2245 train_time:25963ms step_avg:118.01ms
step:221/2245 train_time:26077ms step_avg:118.00ms
step:222/2245 train_time:26198ms step_avg:118.01ms
step:223/2245 train_time:26312ms step_avg:117.99ms
step:224/2245 train_time:26433ms step_avg:118.00ms
step:225/2245 train_time:26547ms step_avg:117.99ms
step:226/2245 train_time:26667ms step_avg:118.00ms
step:227/2245 train_time:26782ms step_avg:117.98ms
step:228/2245 train_time:26902ms step_avg:117.99ms
step:229/2245 train_time:27016ms step_avg:117.98ms
step:230/2245 train_time:27137ms step_avg:117.99ms
step:231/2245 train_time:27251ms step_avg:117.97ms
step:232/2245 train_time:27371ms step_avg:117.98ms
step:233/2245 train_time:27486ms step_avg:117.96ms
step:234/2245 train_time:27606ms step_avg:117.98ms
step:235/2245 train_time:27720ms step_avg:117.96ms
step:236/2245 train_time:27840ms step_avg:117.97ms
step:237/2245 train_time:27955ms step_avg:117.95ms
step:238/2245 train_time:28076ms step_avg:117.97ms
step:239/2245 train_time:28190ms step_avg:117.95ms
step:240/2245 train_time:28310ms step_avg:117.96ms
step:241/2245 train_time:28425ms step_avg:117.95ms
step:242/2245 train_time:28545ms step_avg:117.96ms
step:243/2245 train_time:28659ms step_avg:117.94ms
step:244/2245 train_time:28780ms step_avg:117.95ms
step:245/2245 train_time:28894ms step_avg:117.94ms
step:246/2245 train_time:29015ms step_avg:117.95ms
step:247/2245 train_time:29129ms step_avg:117.93ms
step:248/2245 train_time:29249ms step_avg:117.94ms
step:249/2245 train_time:29364ms step_avg:117.93ms
step:250/2245 train_time:29484ms step_avg:117.94ms
step:250/2245 val_loss:4.0976 train_time:29549ms step_avg:118.20ms
step:251/2245 train_time:29599ms step_avg:117.93ms
step:252/2245 train_time:29719ms step_avg:117.93ms
step:253/2245 train_time:29833ms step_avg:117.92ms
step:254/2245 train_time:29953ms step_avg:117.93ms
step:255/2245 train_time:30068ms step_avg:117.91ms
step:256/2245 train_time:30188ms step_avg:117.92ms
step:257/2245 train_time:30302ms step_avg:117.91ms
step:258/2245 train_time:30423ms step_avg:117.92ms
step:259/2245 train_time:30537ms step_avg:117.91ms
step:260/2245 train_time:30658ms step_avg:117.91ms
step:261/2245 train_time:30771ms step_avg:117.90ms
step:262/2245 train_time:30892ms step_avg:117.91ms
step:263/2245 train_time:31006ms step_avg:117.89ms
step:264/2245 train_time:31127ms step_avg:117.90ms
step:265/2245 train_time:31241ms step_avg:117.89ms
step:266/2245 train_time:31361ms step_avg:117.90ms
step:267/2245 train_time:31475ms step_avg:117.88ms
step:268/2245 train_time:31595ms step_avg:117.89ms
step:269/2245 train_time:31709ms step_avg:117.88ms
step:270/2245 train_time:31830ms step_avg:117.89ms
step:271/2245 train_time:31943ms step_avg:117.87ms
step:272/2245 train_time:32064ms step_avg:117.88ms
step:273/2245 train_time:32178ms step_avg:117.87ms
step:274/2245 train_time:32299ms step_avg:117.88ms
step:275/2245 train_time:32413ms step_avg:117.86ms
step:276/2245 train_time:32533ms step_avg:117.87ms
step:277/2245 train_time:32647ms step_avg:117.86ms
step:278/2245 train_time:32768ms step_avg:117.87ms
step:279/2245 train_time:32882ms step_avg:117.86ms
step:280/2245 train_time:33002ms step_avg:117.87ms
step:281/2245 train_time:33116ms step_avg:117.85ms
step:282/2245 train_time:33237ms step_avg:117.86ms
step:283/2245 train_time:33351ms step_avg:117.85ms
step:284/2245 train_time:33471ms step_avg:117.86ms
step:285/2245 train_time:33585ms step_avg:117.84ms
step:286/2245 train_time:33706ms step_avg:117.85ms
step:287/2245 train_time:33820ms step_avg:117.84ms
step:288/2245 train_time:33941ms step_avg:117.85ms
step:289/2245 train_time:34055ms step_avg:117.84ms
step:290/2245 train_time:34175ms step_avg:117.85ms
step:291/2245 train_time:34290ms step_avg:117.83ms
step:292/2245 train_time:34410ms step_avg:117.84ms
step:293/2245 train_time:34524ms step_avg:117.83ms
step:294/2245 train_time:34644ms step_avg:117.84ms
step:295/2245 train_time:34758ms step_avg:117.82ms
step:296/2245 train_time:34879ms step_avg:117.83ms
step:297/2245 train_time:34993ms step_avg:117.82ms
step:298/2245 train_time:35113ms step_avg:117.83ms
step:299/2245 train_time:35227ms step_avg:117.82ms
step:300/2245 train_time:35348ms step_avg:117.83ms
step:301/2245 train_time:35462ms step_avg:117.82ms
step:302/2245 train_time:35583ms step_avg:117.82ms
step:303/2245 train_time:35697ms step_avg:117.81ms
step:304/2245 train_time:35817ms step_avg:117.82ms
step:305/2245 train_time:35931ms step_avg:117.80ms
step:306/2245 train_time:36051ms step_avg:117.81ms
step:307/2245 train_time:36165ms step_avg:117.80ms
step:308/2245 train_time:36286ms step_avg:117.81ms
step:309/2245 train_time:36400ms step_avg:117.80ms
step:310/2245 train_time:36520ms step_avg:117.81ms
step:311/2245 train_time:36634ms step_avg:117.79ms
step:312/2245 train_time:36754ms step_avg:117.80ms
step:313/2245 train_time:36868ms step_avg:117.79ms
step:314/2245 train_time:36989ms step_avg:117.80ms
step:315/2245 train_time:37103ms step_avg:117.79ms
step:316/2245 train_time:37223ms step_avg:117.80ms
step:317/2245 train_time:37338ms step_avg:117.78ms
step:318/2245 train_time:37458ms step_avg:117.79ms
step:319/2245 train_time:37572ms step_avg:117.78ms
step:320/2245 train_time:37692ms step_avg:117.79ms
step:321/2245 train_time:37806ms step_avg:117.78ms
step:322/2245 train_time:37927ms step_avg:117.79ms
step:323/2245 train_time:38041ms step_avg:117.77ms
step:324/2245 train_time:38161ms step_avg:117.78ms
step:325/2245 train_time:38276ms step_avg:117.77ms
step:326/2245 train_time:38396ms step_avg:117.78ms
step:327/2245 train_time:38510ms step_avg:117.77ms
step:328/2245 train_time:38631ms step_avg:117.78ms
step:329/2245 train_time:38745ms step_avg:117.76ms
step:330/2245 train_time:38865ms step_avg:117.77ms
step:331/2245 train_time:38979ms step_avg:117.76ms
step:332/2245 train_time:39100ms step_avg:117.77ms
step:333/2245 train_time:39213ms step_avg:117.76ms
step:334/2245 train_time:39333ms step_avg:117.76ms
step:335/2245 train_time:39447ms step_avg:117.75ms
step:336/2245 train_time:39568ms step_avg:117.76ms
step:337/2245 train_time:39682ms step_avg:117.75ms
step:338/2245 train_time:39803ms step_avg:117.76ms
step:339/2245 train_time:39917ms step_avg:117.75ms
step:340/2245 train_time:40037ms step_avg:117.76ms
step:341/2245 train_time:40151ms step_avg:117.74ms
step:342/2245 train_time:40271ms step_avg:117.75ms
step:343/2245 train_time:40385ms step_avg:117.74ms
step:344/2245 train_time:40505ms step_avg:117.75ms
step:345/2245 train_time:40620ms step_avg:117.74ms
step:346/2245 train_time:40740ms step_avg:117.75ms
step:347/2245 train_time:40855ms step_avg:117.74ms
step:348/2245 train_time:40975ms step_avg:117.74ms
step:349/2245 train_time:41089ms step_avg:117.73ms
step:350/2245 train_time:41209ms step_avg:117.74ms
step:351/2245 train_time:41323ms step_avg:117.73ms
step:352/2245 train_time:41443ms step_avg:117.74ms
step:353/2245 train_time:41558ms step_avg:117.73ms
step:354/2245 train_time:41678ms step_avg:117.73ms
step:355/2245 train_time:41792ms step_avg:117.72ms
step:356/2245 train_time:41912ms step_avg:117.73ms
step:357/2245 train_time:42027ms step_avg:117.72ms
step:358/2245 train_time:42147ms step_avg:117.73ms
step:359/2245 train_time:42261ms step_avg:117.72ms
step:360/2245 train_time:42382ms step_avg:117.73ms
step:361/2245 train_time:42496ms step_avg:117.72ms
step:362/2245 train_time:42617ms step_avg:117.73ms
step:363/2245 train_time:42731ms step_avg:117.72ms
step:364/2245 train_time:42852ms step_avg:117.72ms
step:365/2245 train_time:42966ms step_avg:117.72ms
step:366/2245 train_time:43086ms step_avg:117.72ms
step:367/2245 train_time:43200ms step_avg:117.71ms
step:368/2245 train_time:43321ms step_avg:117.72ms
step:369/2245 train_time:43435ms step_avg:117.71ms
step:370/2245 train_time:43555ms step_avg:117.72ms
step:371/2245 train_time:43669ms step_avg:117.71ms
step:372/2245 train_time:43790ms step_avg:117.71ms
step:373/2245 train_time:43904ms step_avg:117.70ms
step:374/2245 train_time:44024ms step_avg:117.71ms
step:375/2245 train_time:44138ms step_avg:117.70ms
step:376/2245 train_time:44258ms step_avg:117.71ms
step:377/2245 train_time:44372ms step_avg:117.70ms
step:378/2245 train_time:44493ms step_avg:117.71ms
step:379/2245 train_time:44607ms step_avg:117.70ms
step:380/2245 train_time:44727ms step_avg:117.70ms
step:381/2245 train_time:44841ms step_avg:117.69ms
step:382/2245 train_time:44961ms step_avg:117.70ms
step:383/2245 train_time:45076ms step_avg:117.69ms
step:384/2245 train_time:45196ms step_avg:117.70ms
step:385/2245 train_time:45310ms step_avg:117.69ms
step:386/2245 train_time:45431ms step_avg:117.70ms
step:387/2245 train_time:45545ms step_avg:117.69ms
step:388/2245 train_time:45665ms step_avg:117.69ms
step:389/2245 train_time:45779ms step_avg:117.68ms
step:390/2245 train_time:45899ms step_avg:117.69ms
step:391/2245 train_time:46013ms step_avg:117.68ms
step:392/2245 train_time:46133ms step_avg:117.69ms
step:393/2245 train_time:46247ms step_avg:117.68ms
step:394/2245 train_time:46368ms step_avg:117.68ms
step:395/2245 train_time:46482ms step_avg:117.67ms
step:396/2245 train_time:46602ms step_avg:117.68ms
step:397/2245 train_time:46716ms step_avg:117.67ms
step:398/2245 train_time:46836ms step_avg:117.68ms
step:399/2245 train_time:46950ms step_avg:117.67ms
step:400/2245 train_time:47071ms step_avg:117.68ms
step:401/2245 train_time:47185ms step_avg:117.67ms
step:402/2245 train_time:47305ms step_avg:117.68ms
step:403/2245 train_time:47420ms step_avg:117.67ms
step:404/2245 train_time:47541ms step_avg:117.67ms
step:405/2245 train_time:47654ms step_avg:117.67ms
step:406/2245 train_time:47775ms step_avg:117.67ms
step:407/2245 train_time:47889ms step_avg:117.66ms
step:408/2245 train_time:48009ms step_avg:117.67ms
step:409/2245 train_time:48123ms step_avg:117.66ms
step:410/2245 train_time:48243ms step_avg:117.67ms
step:411/2245 train_time:48358ms step_avg:117.66ms
step:412/2245 train_time:48478ms step_avg:117.67ms
step:413/2245 train_time:48592ms step_avg:117.66ms
step:414/2245 train_time:48712ms step_avg:117.66ms
step:415/2245 train_time:48827ms step_avg:117.65ms
step:416/2245 train_time:48947ms step_avg:117.66ms
step:417/2245 train_time:49061ms step_avg:117.65ms
step:418/2245 train_time:49181ms step_avg:117.66ms
step:419/2245 train_time:49295ms step_avg:117.65ms
step:420/2245 train_time:49415ms step_avg:117.66ms
step:421/2245 train_time:49529ms step_avg:117.65ms
step:422/2245 train_time:49650ms step_avg:117.65ms
step:423/2245 train_time:49764ms step_avg:117.64ms
step:424/2245 train_time:49884ms step_avg:117.65ms
step:425/2245 train_time:49998ms step_avg:117.64ms
step:426/2245 train_time:50118ms step_avg:117.65ms
step:427/2245 train_time:50232ms step_avg:117.64ms
step:428/2245 train_time:50352ms step_avg:117.65ms
step:429/2245 train_time:50467ms step_avg:117.64ms
step:430/2245 train_time:50587ms step_avg:117.65ms
step:431/2245 train_time:50702ms step_avg:117.64ms
step:432/2245 train_time:50822ms step_avg:117.64ms
step:433/2245 train_time:50936ms step_avg:117.63ms
step:434/2245 train_time:51056ms step_avg:117.64ms
step:435/2245 train_time:51170ms step_avg:117.63ms
step:436/2245 train_time:51290ms step_avg:117.64ms
step:437/2245 train_time:51405ms step_avg:117.63ms
step:438/2245 train_time:51525ms step_avg:117.64ms
step:439/2245 train_time:51640ms step_avg:117.63ms
step:440/2245 train_time:51760ms step_avg:117.64ms
step:441/2245 train_time:51873ms step_avg:117.63ms
step:442/2245 train_time:51994ms step_avg:117.63ms
step:443/2245 train_time:52108ms step_avg:117.62ms
step:444/2245 train_time:52228ms step_avg:117.63ms
step:445/2245 train_time:52342ms step_avg:117.62ms
step:446/2245 train_time:52463ms step_avg:117.63ms
step:447/2245 train_time:52577ms step_avg:117.62ms
step:448/2245 train_time:52697ms step_avg:117.63ms
step:449/2245 train_time:52811ms step_avg:117.62ms
step:450/2245 train_time:52931ms step_avg:117.62ms
step:451/2245 train_time:53045ms step_avg:117.62ms
step:452/2245 train_time:53165ms step_avg:117.62ms
step:453/2245 train_time:53279ms step_avg:117.61ms
step:454/2245 train_time:53400ms step_avg:117.62ms
step:455/2245 train_time:53513ms step_avg:117.61ms
step:456/2245 train_time:53634ms step_avg:117.62ms
step:457/2245 train_time:53747ms step_avg:117.61ms
step:458/2245 train_time:53868ms step_avg:117.62ms
step:459/2245 train_time:53982ms step_avg:117.61ms
step:460/2245 train_time:54102ms step_avg:117.61ms
step:461/2245 train_time:54217ms step_avg:117.61ms
step:462/2245 train_time:54337ms step_avg:117.61ms
step:463/2245 train_time:54451ms step_avg:117.61ms
step:464/2245 train_time:54572ms step_avg:117.61ms
step:465/2245 train_time:54686ms step_avg:117.60ms
step:466/2245 train_time:54806ms step_avg:117.61ms
step:467/2245 train_time:54921ms step_avg:117.60ms
step:468/2245 train_time:55041ms step_avg:117.61ms
step:469/2245 train_time:55155ms step_avg:117.60ms
step:470/2245 train_time:55275ms step_avg:117.61ms
step:471/2245 train_time:55389ms step_avg:117.60ms
step:472/2245 train_time:55510ms step_avg:117.61ms
step:473/2245 train_time:55623ms step_avg:117.60ms
step:474/2245 train_time:55744ms step_avg:117.60ms
step:475/2245 train_time:55858ms step_avg:117.59ms
step:476/2245 train_time:55978ms step_avg:117.60ms
step:477/2245 train_time:56092ms step_avg:117.59ms
step:478/2245 train_time:56212ms step_avg:117.60ms
step:479/2245 train_time:56326ms step_avg:117.59ms
step:480/2245 train_time:56447ms step_avg:117.60ms
step:481/2245 train_time:56560ms step_avg:117.59ms
step:482/2245 train_time:56680ms step_avg:117.59ms
step:483/2245 train_time:56794ms step_avg:117.59ms
step:484/2245 train_time:56915ms step_avg:117.59ms
step:485/2245 train_time:57029ms step_avg:117.58ms
step:486/2245 train_time:57149ms step_avg:117.59ms
step:487/2245 train_time:57263ms step_avg:117.58ms
step:488/2245 train_time:57383ms step_avg:117.59ms
step:489/2245 train_time:57498ms step_avg:117.58ms
step:490/2245 train_time:57618ms step_avg:117.59ms
step:491/2245 train_time:57732ms step_avg:117.58ms
step:492/2245 train_time:57852ms step_avg:117.59ms
step:493/2245 train_time:57967ms step_avg:117.58ms
step:494/2245 train_time:58088ms step_avg:117.59ms
step:495/2245 train_time:58201ms step_avg:117.58ms
step:496/2245 train_time:58322ms step_avg:117.58ms
step:497/2245 train_time:58436ms step_avg:117.58ms
step:498/2245 train_time:58556ms step_avg:117.58ms
step:499/2245 train_time:58671ms step_avg:117.58ms
step:500/2245 train_time:58791ms step_avg:117.58ms
step:500/2245 val_loss:3.8264 train_time:58856ms step_avg:117.71ms
step:501/2245 train_time:58907ms step_avg:117.58ms
step:502/2245 train_time:59027ms step_avg:117.58ms
step:503/2245 train_time:59141ms step_avg:117.58ms
step:504/2245 train_time:59261ms step_avg:117.58ms
step:505/2245 train_time:59375ms step_avg:117.58ms
step:506/2245 train_time:59496ms step_avg:117.58ms
step:507/2245 train_time:59609ms step_avg:117.57ms
step:508/2245 train_time:59730ms step_avg:117.58ms
step:509/2245 train_time:59844ms step_avg:117.57ms
step:510/2245 train_time:59964ms step_avg:117.58ms
step:511/2245 train_time:60078ms step_avg:117.57ms
step:512/2245 train_time:60199ms step_avg:117.58ms
step:513/2245 train_time:60313ms step_avg:117.57ms
step:514/2245 train_time:60433ms step_avg:117.57ms
step:515/2245 train_time:60547ms step_avg:117.57ms
step:516/2245 train_time:60668ms step_avg:117.57ms
step:517/2245 train_time:60782ms step_avg:117.57ms
step:518/2245 train_time:60902ms step_avg:117.57ms
step:519/2245 train_time:61016ms step_avg:117.57ms
step:520/2245 train_time:61137ms step_avg:117.57ms
step:521/2245 train_time:61251ms step_avg:117.56ms
step:522/2245 train_time:61371ms step_avg:117.57ms
step:523/2245 train_time:61485ms step_avg:117.56ms
step:524/2245 train_time:61606ms step_avg:117.57ms
step:525/2245 train_time:61720ms step_avg:117.56ms
step:526/2245 train_time:61840ms step_avg:117.57ms
step:527/2245 train_time:61954ms step_avg:117.56ms
step:528/2245 train_time:62074ms step_avg:117.57ms
step:529/2245 train_time:62189ms step_avg:117.56ms
step:530/2245 train_time:62309ms step_avg:117.56ms
step:531/2245 train_time:62423ms step_avg:117.56ms
step:532/2245 train_time:62543ms step_avg:117.56ms
step:533/2245 train_time:62657ms step_avg:117.56ms
step:534/2245 train_time:62778ms step_avg:117.56ms
step:535/2245 train_time:62892ms step_avg:117.55ms
step:536/2245 train_time:63012ms step_avg:117.56ms
step:537/2245 train_time:63126ms step_avg:117.55ms
step:538/2245 train_time:63246ms step_avg:117.56ms
step:539/2245 train_time:63360ms step_avg:117.55ms
step:540/2245 train_time:63482ms step_avg:117.56ms
step:541/2245 train_time:63595ms step_avg:117.55ms
step:542/2245 train_time:63716ms step_avg:117.56ms
step:543/2245 train_time:63830ms step_avg:117.55ms
step:544/2245 train_time:63950ms step_avg:117.56ms
step:545/2245 train_time:64064ms step_avg:117.55ms
step:546/2245 train_time:64185ms step_avg:117.56ms
step:547/2245 train_time:64299ms step_avg:117.55ms
step:548/2245 train_time:64420ms step_avg:117.55ms
step:549/2245 train_time:64534ms step_avg:117.55ms
step:550/2245 train_time:64654ms step_avg:117.55ms
step:551/2245 train_time:64768ms step_avg:117.55ms
step:552/2245 train_time:64889ms step_avg:117.55ms
step:553/2245 train_time:65002ms step_avg:117.55ms
step:554/2245 train_time:65123ms step_avg:117.55ms
step:555/2245 train_time:65237ms step_avg:117.54ms
step:556/2245 train_time:65358ms step_avg:117.55ms
step:557/2245 train_time:65472ms step_avg:117.54ms
step:558/2245 train_time:65592ms step_avg:117.55ms
step:559/2245 train_time:65706ms step_avg:117.54ms
step:560/2245 train_time:65826ms step_avg:117.55ms
step:561/2245 train_time:65940ms step_avg:117.54ms
step:562/2245 train_time:66061ms step_avg:117.55ms
step:563/2245 train_time:66176ms step_avg:117.54ms
step:564/2245 train_time:66296ms step_avg:117.55ms
step:565/2245 train_time:66410ms step_avg:117.54ms
step:566/2245 train_time:66531ms step_avg:117.55ms
step:567/2245 train_time:66645ms step_avg:117.54ms
step:568/2245 train_time:66765ms step_avg:117.54ms
step:569/2245 train_time:66879ms step_avg:117.54ms
step:570/2245 train_time:67000ms step_avg:117.54ms
step:571/2245 train_time:67114ms step_avg:117.54ms
step:572/2245 train_time:67234ms step_avg:117.54ms
step:573/2245 train_time:67348ms step_avg:117.54ms
step:574/2245 train_time:67469ms step_avg:117.54ms
step:575/2245 train_time:67583ms step_avg:117.54ms
step:576/2245 train_time:67703ms step_avg:117.54ms
step:577/2245 train_time:67817ms step_avg:117.53ms
step:578/2245 train_time:67938ms step_avg:117.54ms
step:579/2245 train_time:68052ms step_avg:117.53ms
step:580/2245 train_time:68172ms step_avg:117.54ms
step:581/2245 train_time:68286ms step_avg:117.53ms
step:582/2245 train_time:68407ms step_avg:117.54ms
step:583/2245 train_time:68521ms step_avg:117.53ms
step:584/2245 train_time:68642ms step_avg:117.54ms
step:585/2245 train_time:68756ms step_avg:117.53ms
step:586/2245 train_time:68876ms step_avg:117.54ms
step:587/2245 train_time:68990ms step_avg:117.53ms
step:588/2245 train_time:69110ms step_avg:117.53ms
step:589/2245 train_time:69224ms step_avg:117.53ms
step:590/2245 train_time:69345ms step_avg:117.53ms
step:591/2245 train_time:69458ms step_avg:117.53ms
step:592/2245 train_time:69579ms step_avg:117.53ms
step:593/2245 train_time:69693ms step_avg:117.53ms
step:594/2245 train_time:69813ms step_avg:117.53ms
step:595/2245 train_time:69927ms step_avg:117.53ms
step:596/2245 train_time:70048ms step_avg:117.53ms
step:597/2245 train_time:70162ms step_avg:117.52ms
step:598/2245 train_time:70282ms step_avg:117.53ms
step:599/2245 train_time:70396ms step_avg:117.52ms
step:600/2245 train_time:70517ms step_avg:117.53ms
step:601/2245 train_time:70631ms step_avg:117.52ms
step:602/2245 train_time:70751ms step_avg:117.53ms
step:603/2245 train_time:70865ms step_avg:117.52ms
step:604/2245 train_time:70985ms step_avg:117.53ms
step:605/2245 train_time:71099ms step_avg:117.52ms
step:606/2245 train_time:71220ms step_avg:117.52ms
step:607/2245 train_time:71334ms step_avg:117.52ms
step:608/2245 train_time:71454ms step_avg:117.52ms
step:609/2245 train_time:71568ms step_avg:117.52ms
step:610/2245 train_time:71688ms step_avg:117.52ms
step:611/2245 train_time:71802ms step_avg:117.52ms
step:612/2245 train_time:71923ms step_avg:117.52ms
step:613/2245 train_time:72037ms step_avg:117.52ms
step:614/2245 train_time:72157ms step_avg:117.52ms
step:615/2245 train_time:72271ms step_avg:117.51ms
step:616/2245 train_time:72392ms step_avg:117.52ms
step:617/2245 train_time:72506ms step_avg:117.51ms
step:618/2245 train_time:72626ms step_avg:117.52ms
step:619/2245 train_time:72740ms step_avg:117.51ms
step:620/2245 train_time:72860ms step_avg:117.52ms
step:621/2245 train_time:72975ms step_avg:117.51ms
step:622/2245 train_time:73095ms step_avg:117.52ms
step:623/2245 train_time:73210ms step_avg:117.51ms
step:624/2245 train_time:73330ms step_avg:117.52ms
step:625/2245 train_time:73444ms step_avg:117.51ms
step:626/2245 train_time:73564ms step_avg:117.51ms
step:627/2245 train_time:73678ms step_avg:117.51ms
step:628/2245 train_time:73799ms step_avg:117.51ms
step:629/2245 train_time:73913ms step_avg:117.51ms
step:630/2245 train_time:74033ms step_avg:117.51ms
step:631/2245 train_time:74147ms step_avg:117.51ms
step:632/2245 train_time:74267ms step_avg:117.51ms
step:633/2245 train_time:74382ms step_avg:117.51ms
step:634/2245 train_time:74502ms step_avg:117.51ms
step:635/2245 train_time:74616ms step_avg:117.51ms
step:636/2245 train_time:74737ms step_avg:117.51ms
step:637/2245 train_time:74851ms step_avg:117.50ms
step:638/2245 train_time:74971ms step_avg:117.51ms
step:639/2245 train_time:75085ms step_avg:117.50ms
step:640/2245 train_time:75205ms step_avg:117.51ms
step:641/2245 train_time:75319ms step_avg:117.50ms
step:642/2245 train_time:75439ms step_avg:117.51ms
step:643/2245 train_time:75553ms step_avg:117.50ms
step:644/2245 train_time:75674ms step_avg:117.51ms
step:645/2245 train_time:75788ms step_avg:117.50ms
step:646/2245 train_time:75908ms step_avg:117.50ms
step:647/2245 train_time:76022ms step_avg:117.50ms
step:648/2245 train_time:76142ms step_avg:117.50ms
step:649/2245 train_time:76256ms step_avg:117.50ms
step:650/2245 train_time:76377ms step_avg:117.50ms
step:651/2245 train_time:76491ms step_avg:117.50ms
step:652/2245 train_time:76611ms step_avg:117.50ms
step:653/2245 train_time:76725ms step_avg:117.50ms
step:654/2245 train_time:76846ms step_avg:117.50ms
step:655/2245 train_time:76960ms step_avg:117.50ms
step:656/2245 train_time:77080ms step_avg:117.50ms
step:657/2245 train_time:77195ms step_avg:117.50ms
step:658/2245 train_time:77315ms step_avg:117.50ms
step:659/2245 train_time:77429ms step_avg:117.49ms
step:660/2245 train_time:77549ms step_avg:117.50ms
step:661/2245 train_time:77663ms step_avg:117.49ms
step:662/2245 train_time:77783ms step_avg:117.50ms
step:663/2245 train_time:77897ms step_avg:117.49ms
step:664/2245 train_time:78018ms step_avg:117.50ms
step:665/2245 train_time:78132ms step_avg:117.49ms
step:666/2245 train_time:78252ms step_avg:117.50ms
step:667/2245 train_time:78366ms step_avg:117.49ms
step:668/2245 train_time:78487ms step_avg:117.50ms
step:669/2245 train_time:78601ms step_avg:117.49ms
step:670/2245 train_time:78722ms step_avg:117.49ms
step:671/2245 train_time:78836ms step_avg:117.49ms
step:672/2245 train_time:78956ms step_avg:117.49ms
step:673/2245 train_time:79070ms step_avg:117.49ms
step:674/2245 train_time:79191ms step_avg:117.49ms
step:675/2245 train_time:79305ms step_avg:117.49ms
step:676/2245 train_time:79425ms step_avg:117.49ms
step:677/2245 train_time:79539ms step_avg:117.49ms
step:678/2245 train_time:79660ms step_avg:117.49ms
step:679/2245 train_time:79774ms step_avg:117.49ms
step:680/2245 train_time:79895ms step_avg:117.49ms
step:681/2245 train_time:80009ms step_avg:117.49ms
step:682/2245 train_time:80129ms step_avg:117.49ms
step:683/2245 train_time:80243ms step_avg:117.49ms
step:684/2245 train_time:80364ms step_avg:117.49ms
step:685/2245 train_time:80478ms step_avg:117.49ms
step:686/2245 train_time:80598ms step_avg:117.49ms
step:687/2245 train_time:80713ms step_avg:117.49ms
step:688/2245 train_time:80833ms step_avg:117.49ms
step:689/2245 train_time:80947ms step_avg:117.48ms
step:690/2245 train_time:81067ms step_avg:117.49ms
step:691/2245 train_time:81182ms step_avg:117.48ms
step:692/2245 train_time:81302ms step_avg:117.49ms
step:693/2245 train_time:81416ms step_avg:117.48ms
step:694/2245 train_time:81537ms step_avg:117.49ms
step:695/2245 train_time:81651ms step_avg:117.48ms
step:696/2245 train_time:81771ms step_avg:117.49ms
step:697/2245 train_time:81885ms step_avg:117.48ms
step:698/2245 train_time:82005ms step_avg:117.49ms
step:699/2245 train_time:82120ms step_avg:117.48ms
step:700/2245 train_time:82240ms step_avg:117.49ms
step:701/2245 train_time:82354ms step_avg:117.48ms
step:702/2245 train_time:82475ms step_avg:117.49ms
step:703/2245 train_time:82589ms step_avg:117.48ms
step:704/2245 train_time:82709ms step_avg:117.48ms
step:705/2245 train_time:82823ms step_avg:117.48ms
step:706/2245 train_time:82943ms step_avg:117.48ms
step:707/2245 train_time:83057ms step_avg:117.48ms
step:708/2245 train_time:83178ms step_avg:117.48ms
step:709/2245 train_time:83292ms step_avg:117.48ms
step:710/2245 train_time:83412ms step_avg:117.48ms
step:711/2245 train_time:83526ms step_avg:117.48ms
step:712/2245 train_time:83646ms step_avg:117.48ms
step:713/2245 train_time:83761ms step_avg:117.48ms
step:714/2245 train_time:83881ms step_avg:117.48ms
step:715/2245 train_time:83995ms step_avg:117.48ms
step:716/2245 train_time:84116ms step_avg:117.48ms
step:717/2245 train_time:84230ms step_avg:117.48ms
step:718/2245 train_time:84351ms step_avg:117.48ms
step:719/2245 train_time:84464ms step_avg:117.47ms
step:720/2245 train_time:84584ms step_avg:117.48ms
step:721/2245 train_time:84699ms step_avg:117.47ms
step:722/2245 train_time:84819ms step_avg:117.48ms
step:723/2245 train_time:84933ms step_avg:117.47ms
step:724/2245 train_time:85054ms step_avg:117.48ms
step:725/2245 train_time:85168ms step_avg:117.47ms
step:726/2245 train_time:85288ms step_avg:117.48ms
step:727/2245 train_time:85402ms step_avg:117.47ms
step:728/2245 train_time:85523ms step_avg:117.48ms
step:729/2245 train_time:85637ms step_avg:117.47ms
step:730/2245 train_time:85757ms step_avg:117.48ms
step:731/2245 train_time:85871ms step_avg:117.47ms
step:732/2245 train_time:85991ms step_avg:117.47ms
step:733/2245 train_time:86105ms step_avg:117.47ms
step:734/2245 train_time:86226ms step_avg:117.47ms
step:735/2245 train_time:86339ms step_avg:117.47ms
step:736/2245 train_time:86461ms step_avg:117.47ms
step:737/2245 train_time:86576ms step_avg:117.47ms
step:738/2245 train_time:86698ms step_avg:117.48ms
step:739/2245 train_time:86813ms step_avg:117.47ms
step:740/2245 train_time:86935ms step_avg:117.48ms
step:741/2245 train_time:87050ms step_avg:117.48ms
step:742/2245 train_time:87172ms step_avg:117.48ms
step:743/2245 train_time:87287ms step_avg:117.48ms
step:744/2245 train_time:87409ms step_avg:117.49ms
step:745/2245 train_time:87524ms step_avg:117.48ms
step:746/2245 train_time:87646ms step_avg:117.49ms
step:747/2245 train_time:87762ms step_avg:117.49ms
step:748/2245 train_time:87883ms step_avg:117.49ms
step:749/2245 train_time:87999ms step_avg:117.49ms
step:750/2245 train_time:88121ms step_avg:117.49ms
step:750/2245 val_loss:3.6722 train_time:88186ms step_avg:117.58ms
step:751/2245 train_time:88236ms step_avg:117.49ms
step:752/2245 train_time:88357ms step_avg:117.50ms
step:753/2245 train_time:88473ms step_avg:117.49ms
step:754/2245 train_time:88594ms step_avg:117.50ms
step:755/2245 train_time:88709ms step_avg:117.50ms
step:756/2245 train_time:88831ms step_avg:117.50ms
step:757/2245 train_time:88946ms step_avg:117.50ms
step:758/2245 train_time:89068ms step_avg:117.50ms
step:759/2245 train_time:89184ms step_avg:117.50ms
step:760/2245 train_time:89306ms step_avg:117.51ms
step:761/2245 train_time:89421ms step_avg:117.50ms
step:762/2245 train_time:89543ms step_avg:117.51ms
step:763/2245 train_time:89659ms step_avg:117.51ms
step:764/2245 train_time:89781ms step_avg:117.51ms
step:765/2245 train_time:89896ms step_avg:117.51ms
step:766/2245 train_time:90018ms step_avg:117.52ms
step:767/2245 train_time:90133ms step_avg:117.51ms
step:768/2245 train_time:90255ms step_avg:117.52ms
step:769/2245 train_time:90371ms step_avg:117.52ms
step:770/2245 train_time:90492ms step_avg:117.52ms
step:771/2245 train_time:90608ms step_avg:117.52ms
step:772/2245 train_time:90730ms step_avg:117.53ms
step:773/2245 train_time:90845ms step_avg:117.52ms
step:774/2245 train_time:90967ms step_avg:117.53ms
step:775/2245 train_time:91082ms step_avg:117.53ms
step:776/2245 train_time:91204ms step_avg:117.53ms
step:777/2245 train_time:91320ms step_avg:117.53ms
step:778/2245 train_time:91442ms step_avg:117.53ms
step:779/2245 train_time:91557ms step_avg:117.53ms
step:780/2245 train_time:91679ms step_avg:117.54ms
step:781/2245 train_time:91795ms step_avg:117.53ms
step:782/2245 train_time:91916ms step_avg:117.54ms
step:783/2245 train_time:92031ms step_avg:117.54ms
step:784/2245 train_time:92153ms step_avg:117.54ms
step:785/2245 train_time:92269ms step_avg:117.54ms
step:786/2245 train_time:92391ms step_avg:117.55ms
step:787/2245 train_time:92506ms step_avg:117.54ms
step:788/2245 train_time:92628ms step_avg:117.55ms
step:789/2245 train_time:92743ms step_avg:117.54ms
step:790/2245 train_time:92865ms step_avg:117.55ms
step:791/2245 train_time:92981ms step_avg:117.55ms
step:792/2245 train_time:93102ms step_avg:117.55ms
step:793/2245 train_time:93218ms step_avg:117.55ms
step:794/2245 train_time:93340ms step_avg:117.56ms
step:795/2245 train_time:93455ms step_avg:117.55ms
step:796/2245 train_time:93577ms step_avg:117.56ms
step:797/2245 train_time:93692ms step_avg:117.56ms
step:798/2245 train_time:93814ms step_avg:117.56ms
step:799/2245 train_time:93930ms step_avg:117.56ms
step:800/2245 train_time:94052ms step_avg:117.56ms
step:801/2245 train_time:94167ms step_avg:117.56ms
step:802/2245 train_time:94290ms step_avg:117.57ms
step:803/2245 train_time:94405ms step_avg:117.57ms
step:804/2245 train_time:94527ms step_avg:117.57ms
step:805/2245 train_time:94643ms step_avg:117.57ms
step:806/2245 train_time:94765ms step_avg:117.57ms
step:807/2245 train_time:94881ms step_avg:117.57ms
step:808/2245 train_time:95003ms step_avg:117.58ms
step:809/2245 train_time:95119ms step_avg:117.58ms
step:810/2245 train_time:95241ms step_avg:117.58ms
step:811/2245 train_time:95356ms step_avg:117.58ms
step:812/2245 train_time:95478ms step_avg:117.58ms
step:813/2245 train_time:95593ms step_avg:117.58ms
step:814/2245 train_time:95716ms step_avg:117.59ms
step:815/2245 train_time:95831ms step_avg:117.58ms
step:816/2245 train_time:95953ms step_avg:117.59ms
step:817/2245 train_time:96068ms step_avg:117.59ms
step:818/2245 train_time:96190ms step_avg:117.59ms
step:819/2245 train_time:96305ms step_avg:117.59ms
step:820/2245 train_time:96427ms step_avg:117.59ms
step:821/2245 train_time:96543ms step_avg:117.59ms
step:822/2245 train_time:96665ms step_avg:117.60ms
step:823/2245 train_time:96780ms step_avg:117.59ms
step:824/2245 train_time:96902ms step_avg:117.60ms
step:825/2245 train_time:97018ms step_avg:117.60ms
step:826/2245 train_time:97141ms step_avg:117.60ms
step:827/2245 train_time:97256ms step_avg:117.60ms
step:828/2245 train_time:97378ms step_avg:117.61ms
step:829/2245 train_time:97493ms step_avg:117.60ms
step:830/2245 train_time:97615ms step_avg:117.61ms
step:831/2245 train_time:97731ms step_avg:117.61ms
step:832/2245 train_time:97853ms step_avg:117.61ms
step:833/2245 train_time:97969ms step_avg:117.61ms
step:834/2245 train_time:98091ms step_avg:117.62ms
step:835/2245 train_time:98207ms step_avg:117.61ms
step:836/2245 train_time:98328ms step_avg:117.62ms
step:837/2245 train_time:98444ms step_avg:117.62ms
step:838/2245 train_time:98567ms step_avg:117.62ms
step:839/2245 train_time:98682ms step_avg:117.62ms
step:840/2245 train_time:98805ms step_avg:117.62ms
step:841/2245 train_time:98921ms step_avg:117.62ms
step:842/2245 train_time:99043ms step_avg:117.63ms
step:843/2245 train_time:99159ms step_avg:117.63ms
step:844/2245 train_time:99281ms step_avg:117.63ms
step:845/2245 train_time:99397ms step_avg:117.63ms
step:846/2245 train_time:99518ms step_avg:117.63ms
step:847/2245 train_time:99634ms step_avg:117.63ms
step:848/2245 train_time:99756ms step_avg:117.64ms
step:849/2245 train_time:99871ms step_avg:117.63ms
step:850/2245 train_time:99993ms step_avg:117.64ms
step:851/2245 train_time:100108ms step_avg:117.64ms
step:852/2245 train_time:100231ms step_avg:117.64ms
step:853/2245 train_time:100346ms step_avg:117.64ms
step:854/2245 train_time:100468ms step_avg:117.64ms
step:855/2245 train_time:100583ms step_avg:117.64ms
step:856/2245 train_time:100705ms step_avg:117.65ms
step:857/2245 train_time:100821ms step_avg:117.64ms
step:858/2245 train_time:100943ms step_avg:117.65ms
step:859/2245 train_time:101059ms step_avg:117.65ms
step:860/2245 train_time:101181ms step_avg:117.65ms
step:861/2245 train_time:101297ms step_avg:117.65ms
step:862/2245 train_time:101419ms step_avg:117.66ms
step:863/2245 train_time:101535ms step_avg:117.65ms
step:864/2245 train_time:101656ms step_avg:117.66ms
step:865/2245 train_time:101771ms step_avg:117.65ms
step:866/2245 train_time:101893ms step_avg:117.66ms
step:867/2245 train_time:102009ms step_avg:117.66ms
step:868/2245 train_time:102131ms step_avg:117.66ms
step:869/2245 train_time:102246ms step_avg:117.66ms
step:870/2245 train_time:102368ms step_avg:117.66ms
step:871/2245 train_time:102483ms step_avg:117.66ms
step:872/2245 train_time:102605ms step_avg:117.67ms
step:873/2245 train_time:102721ms step_avg:117.66ms
step:874/2245 train_time:102844ms step_avg:117.67ms
step:875/2245 train_time:102960ms step_avg:117.67ms
step:876/2245 train_time:103083ms step_avg:117.67ms
step:877/2245 train_time:103199ms step_avg:117.67ms
step:878/2245 train_time:103321ms step_avg:117.68ms
step:879/2245 train_time:103436ms step_avg:117.67ms
step:880/2245 train_time:103558ms step_avg:117.68ms
step:881/2245 train_time:103673ms step_avg:117.68ms
step:882/2245 train_time:103795ms step_avg:117.68ms
step:883/2245 train_time:103910ms step_avg:117.68ms
step:884/2245 train_time:104032ms step_avg:117.68ms
step:885/2245 train_time:104148ms step_avg:117.68ms
step:886/2245 train_time:104269ms step_avg:117.69ms
step:887/2245 train_time:104384ms step_avg:117.68ms
step:888/2245 train_time:104506ms step_avg:117.69ms
step:889/2245 train_time:104622ms step_avg:117.68ms
step:890/2245 train_time:104744ms step_avg:117.69ms
step:891/2245 train_time:104859ms step_avg:117.69ms
step:892/2245 train_time:104981ms step_avg:117.69ms
step:893/2245 train_time:105097ms step_avg:117.69ms
step:894/2245 train_time:105219ms step_avg:117.70ms
step:895/2245 train_time:105335ms step_avg:117.69ms
step:896/2245 train_time:105457ms step_avg:117.70ms
step:897/2245 train_time:105573ms step_avg:117.70ms
step:898/2245 train_time:105694ms step_avg:117.70ms
step:899/2245 train_time:105810ms step_avg:117.70ms
step:900/2245 train_time:105932ms step_avg:117.70ms
step:901/2245 train_time:106048ms step_avg:117.70ms
step:902/2245 train_time:106170ms step_avg:117.70ms
step:903/2245 train_time:106285ms step_avg:117.70ms
step:904/2245 train_time:106406ms step_avg:117.71ms
step:905/2245 train_time:106522ms step_avg:117.70ms
step:906/2245 train_time:106644ms step_avg:117.71ms
step:907/2245 train_time:106759ms step_avg:117.71ms
step:908/2245 train_time:106881ms step_avg:117.71ms
step:909/2245 train_time:106997ms step_avg:117.71ms
step:910/2245 train_time:107119ms step_avg:117.71ms
step:911/2245 train_time:107235ms step_avg:117.71ms
step:912/2245 train_time:107356ms step_avg:117.72ms
step:913/2245 train_time:107472ms step_avg:117.71ms
step:914/2245 train_time:107593ms step_avg:117.72ms
step:915/2245 train_time:107709ms step_avg:117.71ms
step:916/2245 train_time:107831ms step_avg:117.72ms
step:917/2245 train_time:107947ms step_avg:117.72ms
step:918/2245 train_time:108069ms step_avg:117.72ms
step:919/2245 train_time:108184ms step_avg:117.72ms
step:920/2245 train_time:108306ms step_avg:117.72ms
step:921/2245 train_time:108422ms step_avg:117.72ms
step:922/2245 train_time:108544ms step_avg:117.73ms
step:923/2245 train_time:108659ms step_avg:117.72ms
step:924/2245 train_time:108781ms step_avg:117.73ms
step:925/2245 train_time:108896ms step_avg:117.73ms
step:926/2245 train_time:109018ms step_avg:117.73ms
step:927/2245 train_time:109134ms step_avg:117.73ms
step:928/2245 train_time:109255ms step_avg:117.73ms
step:929/2245 train_time:109371ms step_avg:117.73ms
step:930/2245 train_time:109493ms step_avg:117.73ms
step:931/2245 train_time:109609ms step_avg:117.73ms
step:932/2245 train_time:109730ms step_avg:117.74ms
step:933/2245 train_time:109846ms step_avg:117.73ms
step:934/2245 train_time:109968ms step_avg:117.74ms
step:935/2245 train_time:110083ms step_avg:117.74ms
step:936/2245 train_time:110205ms step_avg:117.74ms
step:937/2245 train_time:110321ms step_avg:117.74ms
step:938/2245 train_time:110443ms step_avg:117.74ms
step:939/2245 train_time:110558ms step_avg:117.74ms
step:940/2245 train_time:110680ms step_avg:117.74ms
step:941/2245 train_time:110795ms step_avg:117.74ms
step:942/2245 train_time:110918ms step_avg:117.75ms
step:943/2245 train_time:111033ms step_avg:117.74ms
step:944/2245 train_time:111155ms step_avg:117.75ms
step:945/2245 train_time:111270ms step_avg:117.75ms
step:946/2245 train_time:111392ms step_avg:117.75ms
step:947/2245 train_time:111508ms step_avg:117.75ms
step:948/2245 train_time:111630ms step_avg:117.75ms
step:949/2245 train_time:111746ms step_avg:117.75ms
step:950/2245 train_time:111867ms step_avg:117.75ms
step:951/2245 train_time:111983ms step_avg:117.75ms
step:952/2245 train_time:112105ms step_avg:117.76ms
step:953/2245 train_time:112221ms step_avg:117.76ms
step:954/2245 train_time:112342ms step_avg:117.76ms
step:955/2245 train_time:112458ms step_avg:117.76ms
step:956/2245 train_time:112581ms step_avg:117.76ms
step:957/2245 train_time:112696ms step_avg:117.76ms
step:958/2245 train_time:112818ms step_avg:117.76ms
step:959/2245 train_time:112935ms step_avg:117.76ms
step:960/2245 train_time:113056ms step_avg:117.77ms
step:961/2245 train_time:113172ms step_avg:117.76ms
step:962/2245 train_time:113294ms step_avg:117.77ms
step:963/2245 train_time:113409ms step_avg:117.77ms
step:964/2245 train_time:113532ms step_avg:117.77ms
step:965/2245 train_time:113648ms step_avg:117.77ms
step:966/2245 train_time:113770ms step_avg:117.77ms
step:967/2245 train_time:113885ms step_avg:117.77ms
step:968/2245 train_time:114007ms step_avg:117.78ms
step:969/2245 train_time:114122ms step_avg:117.77ms
step:970/2245 train_time:114245ms step_avg:117.78ms
step:971/2245 train_time:114361ms step_avg:117.78ms
step:972/2245 train_time:114483ms step_avg:117.78ms
step:973/2245 train_time:114599ms step_avg:117.78ms
step:974/2245 train_time:114722ms step_avg:117.78ms
step:975/2245 train_time:114837ms step_avg:117.78ms
step:976/2245 train_time:114959ms step_avg:117.79ms
step:977/2245 train_time:115074ms step_avg:117.78ms
step:978/2245 train_time:115196ms step_avg:117.79ms
step:979/2245 train_time:115312ms step_avg:117.79ms
step:980/2245 train_time:115434ms step_avg:117.79ms
step:981/2245 train_time:115549ms step_avg:117.79ms
step:982/2245 train_time:115672ms step_avg:117.79ms
step:983/2245 train_time:115787ms step_avg:117.79ms
step:984/2245 train_time:115909ms step_avg:117.79ms
step:985/2245 train_time:116025ms step_avg:117.79ms
step:986/2245 train_time:116147ms step_avg:117.80ms
step:987/2245 train_time:116262ms step_avg:117.79ms
step:988/2245 train_time:116384ms step_avg:117.80ms
step:989/2245 train_time:116500ms step_avg:117.80ms
step:990/2245 train_time:116622ms step_avg:117.80ms
step:991/2245 train_time:116738ms step_avg:117.80ms
step:992/2245 train_time:116860ms step_avg:117.80ms
step:993/2245 train_time:116975ms step_avg:117.80ms
step:994/2245 train_time:117097ms step_avg:117.80ms
step:995/2245 train_time:117213ms step_avg:117.80ms
step:996/2245 train_time:117335ms step_avg:117.81ms
step:997/2245 train_time:117450ms step_avg:117.80ms
step:998/2245 train_time:117572ms step_avg:117.81ms
step:999/2245 train_time:117688ms step_avg:117.81ms
step:1000/2245 train_time:117810ms step_avg:117.81ms
step:1000/2245 val_loss:3.6203 train_time:117875ms step_avg:117.88ms
step:1001/2245 train_time:117926ms step_avg:117.81ms
step:1002/2245 train_time:118047ms step_avg:117.81ms
step:1003/2245 train_time:118163ms step_avg:117.81ms
step:1004/2245 train_time:118285ms step_avg:117.81ms
step:1005/2245 train_time:118401ms step_avg:117.81ms
step:1006/2245 train_time:118523ms step_avg:117.82ms
step:1007/2245 train_time:118638ms step_avg:117.81ms
step:1008/2245 train_time:118760ms step_avg:117.82ms
step:1009/2245 train_time:118876ms step_avg:117.82ms
step:1010/2245 train_time:118998ms step_avg:117.82ms
step:1011/2245 train_time:119114ms step_avg:117.82ms
step:1012/2245 train_time:119236ms step_avg:117.82ms
step:1013/2245 train_time:119352ms step_avg:117.82ms
step:1014/2245 train_time:119474ms step_avg:117.82ms
step:1015/2245 train_time:119590ms step_avg:117.82ms
step:1016/2245 train_time:119712ms step_avg:117.83ms
step:1017/2245 train_time:119828ms step_avg:117.82ms
step:1018/2245 train_time:119950ms step_avg:117.83ms
step:1019/2245 train_time:120066ms step_avg:117.83ms
step:1020/2245 train_time:120187ms step_avg:117.83ms
step:1021/2245 train_time:120303ms step_avg:117.83ms
step:1022/2245 train_time:120425ms step_avg:117.83ms
step:1023/2245 train_time:120541ms step_avg:117.83ms
step:1024/2245 train_time:120663ms step_avg:117.84ms
step:1025/2245 train_time:120779ms step_avg:117.83ms
step:1026/2245 train_time:120901ms step_avg:117.84ms
step:1027/2245 train_time:121017ms step_avg:117.84ms
step:1028/2245 train_time:121139ms step_avg:117.84ms
step:1029/2245 train_time:121255ms step_avg:117.84ms
step:1030/2245 train_time:121377ms step_avg:117.84ms
step:1031/2245 train_time:121493ms step_avg:117.84ms
step:1032/2245 train_time:121615ms step_avg:117.84ms
step:1033/2245 train_time:121731ms step_avg:117.84ms
step:1034/2245 train_time:121853ms step_avg:117.85ms
step:1035/2245 train_time:121968ms step_avg:117.84ms
step:1036/2245 train_time:122091ms step_avg:117.85ms
step:1037/2245 train_time:122206ms step_avg:117.85ms
step:1038/2245 train_time:122328ms step_avg:117.85ms
step:1039/2245 train_time:122444ms step_avg:117.85ms
step:1040/2245 train_time:122566ms step_avg:117.85ms
step:1041/2245 train_time:122681ms step_avg:117.85ms
step:1042/2245 train_time:122804ms step_avg:117.85ms
step:1043/2245 train_time:122919ms step_avg:117.85ms
step:1044/2245 train_time:123041ms step_avg:117.86ms
step:1045/2245 train_time:123157ms step_avg:117.85ms
step:1046/2245 train_time:123279ms step_avg:117.86ms
step:1047/2245 train_time:123395ms step_avg:117.86ms
step:1048/2245 train_time:123517ms step_avg:117.86ms
step:1049/2245 train_time:123633ms step_avg:117.86ms
step:1050/2245 train_time:123755ms step_avg:117.86ms
step:1051/2245 train_time:123870ms step_avg:117.86ms
step:1052/2245 train_time:123992ms step_avg:117.86ms
step:1053/2245 train_time:124108ms step_avg:117.86ms
step:1054/2245 train_time:124230ms step_avg:117.87ms
step:1055/2245 train_time:124345ms step_avg:117.86ms
step:1056/2245 train_time:124467ms step_avg:117.87ms
step:1057/2245 train_time:124583ms step_avg:117.86ms
step:1058/2245 train_time:124705ms step_avg:117.87ms
step:1059/2245 train_time:124821ms step_avg:117.87ms
step:1060/2245 train_time:124943ms step_avg:117.87ms
step:1061/2245 train_time:125058ms step_avg:117.87ms
step:1062/2245 train_time:125181ms step_avg:117.87ms
step:1063/2245 train_time:125296ms step_avg:117.87ms
step:1064/2245 train_time:125418ms step_avg:117.87ms
step:1065/2245 train_time:125534ms step_avg:117.87ms
step:1066/2245 train_time:125656ms step_avg:117.88ms
step:1067/2245 train_time:125771ms step_avg:117.87ms
step:1068/2245 train_time:125894ms step_avg:117.88ms
step:1069/2245 train_time:126010ms step_avg:117.88ms
step:1070/2245 train_time:126132ms step_avg:117.88ms
step:1071/2245 train_time:126247ms step_avg:117.88ms
step:1072/2245 train_time:126368ms step_avg:117.88ms
step:1073/2245 train_time:126484ms step_avg:117.88ms
step:1074/2245 train_time:126607ms step_avg:117.88ms
step:1075/2245 train_time:126722ms step_avg:117.88ms
step:1076/2245 train_time:126844ms step_avg:117.88ms
step:1077/2245 train_time:126959ms step_avg:117.88ms
step:1078/2245 train_time:127081ms step_avg:117.89ms
step:1079/2245 train_time:127197ms step_avg:117.88ms
step:1080/2245 train_time:127319ms step_avg:117.89ms
step:1081/2245 train_time:127435ms step_avg:117.89ms
step:1082/2245 train_time:127557ms step_avg:117.89ms
step:1083/2245 train_time:127673ms step_avg:117.89ms
step:1084/2245 train_time:127795ms step_avg:117.89ms
step:1085/2245 train_time:127911ms step_avg:117.89ms
step:1086/2245 train_time:128034ms step_avg:117.89ms
step:1087/2245 train_time:128149ms step_avg:117.89ms
step:1088/2245 train_time:128271ms step_avg:117.90ms
step:1089/2245 train_time:128387ms step_avg:117.89ms
step:1090/2245 train_time:128508ms step_avg:117.90ms
step:1091/2245 train_time:128624ms step_avg:117.90ms
step:1092/2245 train_time:128746ms step_avg:117.90ms
step:1093/2245 train_time:128862ms step_avg:117.90ms
step:1094/2245 train_time:128984ms step_avg:117.90ms
step:1095/2245 train_time:129099ms step_avg:117.90ms
step:1096/2245 train_time:129222ms step_avg:117.90ms
step:1097/2245 train_time:129337ms step_avg:117.90ms
step:1098/2245 train_time:129459ms step_avg:117.90ms
step:1099/2245 train_time:129575ms step_avg:117.90ms
step:1100/2245 train_time:129697ms step_avg:117.91ms
step:1101/2245 train_time:129813ms step_avg:117.90ms
step:1102/2245 train_time:129935ms step_avg:117.91ms
step:1103/2245 train_time:130051ms step_avg:117.91ms
step:1104/2245 train_time:130173ms step_avg:117.91ms
step:1105/2245 train_time:130289ms step_avg:117.91ms
step:1106/2245 train_time:130411ms step_avg:117.91ms
step:1107/2245 train_time:130527ms step_avg:117.91ms
step:1108/2245 train_time:130649ms step_avg:117.91ms
step:1109/2245 train_time:130764ms step_avg:117.91ms
step:1110/2245 train_time:130886ms step_avg:117.92ms
step:1111/2245 train_time:131002ms step_avg:117.91ms
step:1112/2245 train_time:131124ms step_avg:117.92ms
step:1113/2245 train_time:131239ms step_avg:117.91ms
step:1114/2245 train_time:131361ms step_avg:117.92ms
step:1115/2245 train_time:131477ms step_avg:117.92ms
step:1116/2245 train_time:131599ms step_avg:117.92ms
step:1117/2245 train_time:131714ms step_avg:117.92ms
step:1118/2245 train_time:131836ms step_avg:117.92ms
step:1119/2245 train_time:131952ms step_avg:117.92ms
step:1120/2245 train_time:132074ms step_avg:117.92ms
step:1121/2245 train_time:132190ms step_avg:117.92ms
step:1122/2245 train_time:132312ms step_avg:117.93ms
step:1123/2245 train_time:132427ms step_avg:117.92ms
step:1124/2245 train_time:132549ms step_avg:117.93ms
step:1125/2245 train_time:132665ms step_avg:117.92ms
step:1126/2245 train_time:132786ms step_avg:117.93ms
step:1127/2245 train_time:132902ms step_avg:117.93ms
step:1128/2245 train_time:133024ms step_avg:117.93ms
step:1129/2245 train_time:133140ms step_avg:117.93ms
step:1130/2245 train_time:133262ms step_avg:117.93ms
step:1131/2245 train_time:133378ms step_avg:117.93ms
step:1132/2245 train_time:133500ms step_avg:117.93ms
step:1133/2245 train_time:133616ms step_avg:117.93ms
step:1134/2245 train_time:133737ms step_avg:117.93ms
step:1135/2245 train_time:133853ms step_avg:117.93ms
step:1136/2245 train_time:133975ms step_avg:117.94ms
step:1137/2245 train_time:134090ms step_avg:117.93ms
step:1138/2245 train_time:134212ms step_avg:117.94ms
step:1139/2245 train_time:134328ms step_avg:117.94ms
step:1140/2245 train_time:134450ms step_avg:117.94ms
step:1141/2245 train_time:134565ms step_avg:117.94ms
step:1142/2245 train_time:134687ms step_avg:117.94ms
step:1143/2245 train_time:134803ms step_avg:117.94ms
step:1144/2245 train_time:134925ms step_avg:117.94ms
step:1145/2245 train_time:135040ms step_avg:117.94ms
step:1146/2245 train_time:135162ms step_avg:117.94ms
step:1147/2245 train_time:135278ms step_avg:117.94ms
step:1148/2245 train_time:135400ms step_avg:117.94ms
step:1149/2245 train_time:135515ms step_avg:117.94ms
step:1150/2245 train_time:135637ms step_avg:117.95ms
step:1151/2245 train_time:135753ms step_avg:117.94ms
step:1152/2245 train_time:135875ms step_avg:117.95ms
step:1153/2245 train_time:135992ms step_avg:117.95ms
step:1154/2245 train_time:136114ms step_avg:117.95ms
step:1155/2245 train_time:136230ms step_avg:117.95ms
step:1156/2245 train_time:136352ms step_avg:117.95ms
step:1157/2245 train_time:136467ms step_avg:117.95ms
step:1158/2245 train_time:136589ms step_avg:117.95ms
step:1159/2245 train_time:136705ms step_avg:117.95ms
step:1160/2245 train_time:136827ms step_avg:117.95ms
step:1161/2245 train_time:136942ms step_avg:117.95ms
step:1162/2245 train_time:137064ms step_avg:117.96ms
step:1163/2245 train_time:137180ms step_avg:117.95ms
step:1164/2245 train_time:137302ms step_avg:117.96ms
step:1165/2245 train_time:137418ms step_avg:117.96ms
step:1166/2245 train_time:137539ms step_avg:117.96ms
step:1167/2245 train_time:137655ms step_avg:117.96ms
step:1168/2245 train_time:137778ms step_avg:117.96ms
step:1169/2245 train_time:137893ms step_avg:117.96ms
step:1170/2245 train_time:138015ms step_avg:117.96ms
step:1171/2245 train_time:138131ms step_avg:117.96ms
step:1172/2245 train_time:138253ms step_avg:117.96ms
step:1173/2245 train_time:138369ms step_avg:117.96ms
step:1174/2245 train_time:138490ms step_avg:117.96ms
step:1175/2245 train_time:138606ms step_avg:117.96ms
step:1176/2245 train_time:138727ms step_avg:117.97ms
step:1177/2245 train_time:138843ms step_avg:117.96ms
step:1178/2245 train_time:138965ms step_avg:117.97ms
step:1179/2245 train_time:139080ms step_avg:117.96ms
step:1180/2245 train_time:139202ms step_avg:117.97ms
step:1181/2245 train_time:139318ms step_avg:117.97ms
step:1182/2245 train_time:139440ms step_avg:117.97ms
step:1183/2245 train_time:139555ms step_avg:117.97ms
step:1184/2245 train_time:139677ms step_avg:117.97ms
step:1185/2245 train_time:139793ms step_avg:117.97ms
step:1186/2245 train_time:139915ms step_avg:117.97ms
step:1187/2245 train_time:140031ms step_avg:117.97ms
step:1188/2245 train_time:140154ms step_avg:117.97ms
step:1189/2245 train_time:140270ms step_avg:117.97ms
step:1190/2245 train_time:140392ms step_avg:117.98ms
step:1191/2245 train_time:140509ms step_avg:117.98ms
step:1192/2245 train_time:140630ms step_avg:117.98ms
step:1193/2245 train_time:140746ms step_avg:117.98ms
step:1194/2245 train_time:140867ms step_avg:117.98ms
step:1195/2245 train_time:140982ms step_avg:117.98ms
step:1196/2245 train_time:141104ms step_avg:117.98ms
step:1197/2245 train_time:141220ms step_avg:117.98ms
step:1198/2245 train_time:141342ms step_avg:117.98ms
step:1199/2245 train_time:141458ms step_avg:117.98ms
step:1200/2245 train_time:141579ms step_avg:117.98ms
step:1201/2245 train_time:141696ms step_avg:117.98ms
step:1202/2245 train_time:141818ms step_avg:117.99ms
step:1203/2245 train_time:141934ms step_avg:117.98ms
step:1204/2245 train_time:142056ms step_avg:117.99ms
step:1205/2245 train_time:142172ms step_avg:117.98ms
step:1206/2245 train_time:142294ms step_avg:117.99ms
step:1207/2245 train_time:142410ms step_avg:117.99ms
step:1208/2245 train_time:142532ms step_avg:117.99ms
step:1209/2245 train_time:142647ms step_avg:117.99ms
step:1210/2245 train_time:142769ms step_avg:117.99ms
step:1211/2245 train_time:142885ms step_avg:117.99ms
step:1212/2245 train_time:143007ms step_avg:117.99ms
step:1213/2245 train_time:143122ms step_avg:117.99ms
step:1214/2245 train_time:143245ms step_avg:117.99ms
step:1215/2245 train_time:143361ms step_avg:117.99ms
step:1216/2245 train_time:143483ms step_avg:118.00ms
step:1217/2245 train_time:143599ms step_avg:117.99ms
step:1218/2245 train_time:143721ms step_avg:118.00ms
step:1219/2245 train_time:143837ms step_avg:118.00ms
step:1220/2245 train_time:143958ms step_avg:118.00ms
step:1221/2245 train_time:144074ms step_avg:118.00ms
step:1222/2245 train_time:144196ms step_avg:118.00ms
step:1223/2245 train_time:144312ms step_avg:118.00ms
step:1224/2245 train_time:144434ms step_avg:118.00ms
step:1225/2245 train_time:144550ms step_avg:118.00ms
step:1226/2245 train_time:144672ms step_avg:118.00ms
step:1227/2245 train_time:144788ms step_avg:118.00ms
step:1228/2245 train_time:144910ms step_avg:118.00ms
step:1229/2245 train_time:145025ms step_avg:118.00ms
step:1230/2245 train_time:145147ms step_avg:118.01ms
step:1231/2245 train_time:145263ms step_avg:118.00ms
step:1232/2245 train_time:145385ms step_avg:118.01ms
step:1233/2245 train_time:145500ms step_avg:118.01ms
step:1234/2245 train_time:145623ms step_avg:118.01ms
step:1235/2245 train_time:145738ms step_avg:118.01ms
step:1236/2245 train_time:145859ms step_avg:118.01ms
step:1237/2245 train_time:145975ms step_avg:118.01ms
step:1238/2245 train_time:146097ms step_avg:118.01ms
step:1239/2245 train_time:146212ms step_avg:118.01ms
step:1240/2245 train_time:146334ms step_avg:118.01ms
step:1241/2245 train_time:146449ms step_avg:118.01ms
step:1242/2245 train_time:146572ms step_avg:118.01ms
step:1243/2245 train_time:146688ms step_avg:118.01ms
step:1244/2245 train_time:146810ms step_avg:118.01ms
step:1245/2245 train_time:146926ms step_avg:118.01ms
step:1246/2245 train_time:147047ms step_avg:118.02ms
step:1247/2245 train_time:147163ms step_avg:118.01ms
step:1248/2245 train_time:147285ms step_avg:118.02ms
step:1249/2245 train_time:147400ms step_avg:118.01ms
step:1250/2245 train_time:147522ms step_avg:118.02ms
step:1250/2245 val_loss:3.5245 train_time:147588ms step_avg:118.07ms
step:1251/2245 train_time:147639ms step_avg:118.02ms
step:1252/2245 train_time:147760ms step_avg:118.02ms
step:1253/2245 train_time:147876ms step_avg:118.02ms
step:1254/2245 train_time:147998ms step_avg:118.02ms
step:1255/2245 train_time:148113ms step_avg:118.02ms
step:1256/2245 train_time:148235ms step_avg:118.02ms
step:1257/2245 train_time:148350ms step_avg:118.02ms
step:1258/2245 train_time:148472ms step_avg:118.02ms
step:1259/2245 train_time:148588ms step_avg:118.02ms
step:1260/2245 train_time:148710ms step_avg:118.02ms
step:1261/2245 train_time:148825ms step_avg:118.02ms
step:1262/2245 train_time:148947ms step_avg:118.02ms
step:1263/2245 train_time:149064ms step_avg:118.02ms
step:1264/2245 train_time:149186ms step_avg:118.03ms
step:1265/2245 train_time:149302ms step_avg:118.03ms
step:1266/2245 train_time:149424ms step_avg:118.03ms
step:1267/2245 train_time:149539ms step_avg:118.03ms
step:1268/2245 train_time:149661ms step_avg:118.03ms
step:1269/2245 train_time:149776ms step_avg:118.03ms
step:1270/2245 train_time:149898ms step_avg:118.03ms
step:1271/2245 train_time:150014ms step_avg:118.03ms
step:1272/2245 train_time:150136ms step_avg:118.03ms
step:1273/2245 train_time:150252ms step_avg:118.03ms
step:1274/2245 train_time:150374ms step_avg:118.03ms
step:1275/2245 train_time:150489ms step_avg:118.03ms
step:1276/2245 train_time:150612ms step_avg:118.03ms
step:1277/2245 train_time:150727ms step_avg:118.03ms
step:1278/2245 train_time:150849ms step_avg:118.04ms
step:1279/2245 train_time:150964ms step_avg:118.03ms
step:1280/2245 train_time:151086ms step_avg:118.04ms
step:1281/2245 train_time:151202ms step_avg:118.03ms
step:1282/2245 train_time:151324ms step_avg:118.04ms
step:1283/2245 train_time:151440ms step_avg:118.04ms
step:1284/2245 train_time:151563ms step_avg:118.04ms
step:1285/2245 train_time:151678ms step_avg:118.04ms
step:1286/2245 train_time:151799ms step_avg:118.04ms
step:1287/2245 train_time:151915ms step_avg:118.04ms
step:1288/2245 train_time:152037ms step_avg:118.04ms
step:1289/2245 train_time:152153ms step_avg:118.04ms
step:1290/2245 train_time:152275ms step_avg:118.04ms
step:1291/2245 train_time:152391ms step_avg:118.04ms
step:1292/2245 train_time:152513ms step_avg:118.04ms
step:1293/2245 train_time:152628ms step_avg:118.04ms
step:1294/2245 train_time:152750ms step_avg:118.04ms
step:1295/2245 train_time:152865ms step_avg:118.04ms
step:1296/2245 train_time:152987ms step_avg:118.05ms
step:1297/2245 train_time:153103ms step_avg:118.04ms
step:1298/2245 train_time:153225ms step_avg:118.05ms
step:1299/2245 train_time:153341ms step_avg:118.05ms
step:1300/2245 train_time:153463ms step_avg:118.05ms
step:1301/2245 train_time:153579ms step_avg:118.05ms
step:1302/2245 train_time:153702ms step_avg:118.05ms
step:1303/2245 train_time:153817ms step_avg:118.05ms
step:1304/2245 train_time:153939ms step_avg:118.05ms
step:1305/2245 train_time:154055ms step_avg:118.05ms
step:1306/2245 train_time:154177ms step_avg:118.05ms
step:1307/2245 train_time:154293ms step_avg:118.05ms
step:1308/2245 train_time:154415ms step_avg:118.05ms
step:1309/2245 train_time:154531ms step_avg:118.05ms
step:1310/2245 train_time:154654ms step_avg:118.06ms
step:1311/2245 train_time:154769ms step_avg:118.05ms
step:1312/2245 train_time:154891ms step_avg:118.06ms
step:1313/2245 train_time:155007ms step_avg:118.06ms
step:1314/2245 train_time:155129ms step_avg:118.06ms
step:1315/2245 train_time:155245ms step_avg:118.06ms
step:1316/2245 train_time:155367ms step_avg:118.06ms
step:1317/2245 train_time:155483ms step_avg:118.06ms
step:1318/2245 train_time:155606ms step_avg:118.06ms
step:1319/2245 train_time:155721ms step_avg:118.06ms
step:1320/2245 train_time:155843ms step_avg:118.06ms
step:1321/2245 train_time:155958ms step_avg:118.06ms
step:1322/2245 train_time:156081ms step_avg:118.06ms
step:1323/2245 train_time:156196ms step_avg:118.06ms
step:1324/2245 train_time:156318ms step_avg:118.06ms
step:1325/2245 train_time:156433ms step_avg:118.06ms
step:1326/2245 train_time:156555ms step_avg:118.07ms
step:1327/2245 train_time:156671ms step_avg:118.06ms
step:1328/2245 train_time:156793ms step_avg:118.07ms
step:1329/2245 train_time:156909ms step_avg:118.07ms
step:1330/2245 train_time:157031ms step_avg:118.07ms
step:1331/2245 train_time:157146ms step_avg:118.07ms
step:1332/2245 train_time:157268ms step_avg:118.07ms
step:1333/2245 train_time:157384ms step_avg:118.07ms
step:1334/2245 train_time:157506ms step_avg:118.07ms
step:1335/2245 train_time:157622ms step_avg:118.07ms
step:1336/2245 train_time:157744ms step_avg:118.07ms
step:1337/2245 train_time:157860ms step_avg:118.07ms
step:1338/2245 train_time:157982ms step_avg:118.07ms
step:1339/2245 train_time:158098ms step_avg:118.07ms
step:1340/2245 train_time:158219ms step_avg:118.07ms
step:1341/2245 train_time:158334ms step_avg:118.07ms
step:1342/2245 train_time:158456ms step_avg:118.07ms
step:1343/2245 train_time:158572ms step_avg:118.07ms
step:1344/2245 train_time:158695ms step_avg:118.08ms
step:1345/2245 train_time:158810ms step_avg:118.07ms
step:1346/2245 train_time:158932ms step_avg:118.08ms
step:1347/2245 train_time:159047ms step_avg:118.07ms
step:1348/2245 train_time:159168ms step_avg:118.08ms
step:1349/2245 train_time:159284ms step_avg:118.08ms
step:1350/2245 train_time:159406ms step_avg:118.08ms
step:1351/2245 train_time:159522ms step_avg:118.08ms
step:1352/2245 train_time:159644ms step_avg:118.08ms
step:1353/2245 train_time:159759ms step_avg:118.08ms
step:1354/2245 train_time:159881ms step_avg:118.08ms
step:1355/2245 train_time:159997ms step_avg:118.08ms
step:1356/2245 train_time:160118ms step_avg:118.08ms
step:1357/2245 train_time:160234ms step_avg:118.08ms
step:1358/2245 train_time:160356ms step_avg:118.08ms
step:1359/2245 train_time:160471ms step_avg:118.08ms
step:1360/2245 train_time:160594ms step_avg:118.08ms
step:1361/2245 train_time:160709ms step_avg:118.08ms
step:1362/2245 train_time:160832ms step_avg:118.08ms
step:1363/2245 train_time:160947ms step_avg:118.08ms
step:1364/2245 train_time:161069ms step_avg:118.09ms
step:1365/2245 train_time:161185ms step_avg:118.08ms
step:1366/2245 train_time:161307ms step_avg:118.09ms
step:1367/2245 train_time:161423ms step_avg:118.09ms
step:1368/2245 train_time:161545ms step_avg:118.09ms
step:1369/2245 train_time:161662ms step_avg:118.09ms
step:1370/2245 train_time:161784ms step_avg:118.09ms
step:1371/2245 train_time:161899ms step_avg:118.09ms
step:1372/2245 train_time:162021ms step_avg:118.09ms
step:1373/2245 train_time:162137ms step_avg:118.09ms
step:1374/2245 train_time:162258ms step_avg:118.09ms
step:1375/2245 train_time:162374ms step_avg:118.09ms
step:1376/2245 train_time:162496ms step_avg:118.09ms
step:1377/2245 train_time:162611ms step_avg:118.09ms
step:1378/2245 train_time:162734ms step_avg:118.09ms
step:1379/2245 train_time:162849ms step_avg:118.09ms
step:1380/2245 train_time:162971ms step_avg:118.10ms
step:1381/2245 train_time:163087ms step_avg:118.09ms
step:1382/2245 train_time:163209ms step_avg:118.10ms
step:1383/2245 train_time:163325ms step_avg:118.09ms
step:1384/2245 train_time:163447ms step_avg:118.10ms
step:1385/2245 train_time:163563ms step_avg:118.10ms
step:1386/2245 train_time:163685ms step_avg:118.10ms
step:1387/2245 train_time:163800ms step_avg:118.10ms
step:1388/2245 train_time:163922ms step_avg:118.10ms
step:1389/2245 train_time:164038ms step_avg:118.10ms
step:1390/2245 train_time:164159ms step_avg:118.10ms
step:1391/2245 train_time:164276ms step_avg:118.10ms
step:1392/2245 train_time:164397ms step_avg:118.10ms
step:1393/2245 train_time:164513ms step_avg:118.10ms
step:1394/2245 train_time:164635ms step_avg:118.10ms
step:1395/2245 train_time:164751ms step_avg:118.10ms
step:1396/2245 train_time:164873ms step_avg:118.10ms
step:1397/2245 train_time:164989ms step_avg:118.10ms
step:1398/2245 train_time:165111ms step_avg:118.11ms
step:1399/2245 train_time:165227ms step_avg:118.10ms
step:1400/2245 train_time:165348ms step_avg:118.11ms
step:1401/2245 train_time:165464ms step_avg:118.10ms
step:1402/2245 train_time:165586ms step_avg:118.11ms
step:1403/2245 train_time:165702ms step_avg:118.11ms
step:1404/2245 train_time:165824ms step_avg:118.11ms
step:1405/2245 train_time:165939ms step_avg:118.11ms
step:1406/2245 train_time:166061ms step_avg:118.11ms
step:1407/2245 train_time:166177ms step_avg:118.11ms
step:1408/2245 train_time:166298ms step_avg:118.11ms
step:1409/2245 train_time:166414ms step_avg:118.11ms
step:1410/2245 train_time:166536ms step_avg:118.11ms
step:1411/2245 train_time:166651ms step_avg:118.11ms
step:1412/2245 train_time:166774ms step_avg:118.11ms
step:1413/2245 train_time:166890ms step_avg:118.11ms
step:1414/2245 train_time:167011ms step_avg:118.11ms
step:1415/2245 train_time:167127ms step_avg:118.11ms
step:1416/2245 train_time:167249ms step_avg:118.11ms
step:1417/2245 train_time:167365ms step_avg:118.11ms
step:1418/2245 train_time:167487ms step_avg:118.12ms
step:1419/2245 train_time:167604ms step_avg:118.11ms
step:1420/2245 train_time:167726ms step_avg:118.12ms
step:1421/2245 train_time:167843ms step_avg:118.12ms
step:1422/2245 train_time:167965ms step_avg:118.12ms
step:1423/2245 train_time:168081ms step_avg:118.12ms
step:1424/2245 train_time:168203ms step_avg:118.12ms
step:1425/2245 train_time:168318ms step_avg:118.12ms
step:1426/2245 train_time:168440ms step_avg:118.12ms
step:1427/2245 train_time:168555ms step_avg:118.12ms
step:1428/2245 train_time:168678ms step_avg:118.12ms
step:1429/2245 train_time:168793ms step_avg:118.12ms
step:1430/2245 train_time:168915ms step_avg:118.12ms
step:1431/2245 train_time:169031ms step_avg:118.12ms
step:1432/2245 train_time:169153ms step_avg:118.12ms
step:1433/2245 train_time:169268ms step_avg:118.12ms
step:1434/2245 train_time:169390ms step_avg:118.12ms
step:1435/2245 train_time:169506ms step_avg:118.12ms
step:1436/2245 train_time:169629ms step_avg:118.13ms
step:1437/2245 train_time:169744ms step_avg:118.12ms
step:1438/2245 train_time:169867ms step_avg:118.13ms
step:1439/2245 train_time:169983ms step_avg:118.13ms
step:1440/2245 train_time:170105ms step_avg:118.13ms
step:1441/2245 train_time:170221ms step_avg:118.13ms
step:1442/2245 train_time:170343ms step_avg:118.13ms
step:1443/2245 train_time:170458ms step_avg:118.13ms
step:1444/2245 train_time:170580ms step_avg:118.13ms
step:1445/2245 train_time:170695ms step_avg:118.13ms
step:1446/2245 train_time:170817ms step_avg:118.13ms
step:1447/2245 train_time:170932ms step_avg:118.13ms
step:1448/2245 train_time:171054ms step_avg:118.13ms
step:1449/2245 train_time:171170ms step_avg:118.13ms
step:1450/2245 train_time:171292ms step_avg:118.13ms
step:1451/2245 train_time:171408ms step_avg:118.13ms
step:1452/2245 train_time:171529ms step_avg:118.13ms
step:1453/2245 train_time:171645ms step_avg:118.13ms
step:1454/2245 train_time:171768ms step_avg:118.13ms
step:1455/2245 train_time:171885ms step_avg:118.13ms
step:1456/2245 train_time:172007ms step_avg:118.14ms
step:1457/2245 train_time:172122ms step_avg:118.13ms
step:1458/2245 train_time:172245ms step_avg:118.14ms
step:1459/2245 train_time:172361ms step_avg:118.14ms
step:1460/2245 train_time:172483ms step_avg:118.14ms
step:1461/2245 train_time:172598ms step_avg:118.14ms
step:1462/2245 train_time:172720ms step_avg:118.14ms
step:1463/2245 train_time:172835ms step_avg:118.14ms
step:1464/2245 train_time:172958ms step_avg:118.14ms
step:1465/2245 train_time:173073ms step_avg:118.14ms
step:1466/2245 train_time:173195ms step_avg:118.14ms
step:1467/2245 train_time:173311ms step_avg:118.14ms
step:1468/2245 train_time:173432ms step_avg:118.14ms
step:1469/2245 train_time:173548ms step_avg:118.14ms
step:1470/2245 train_time:173670ms step_avg:118.14ms
step:1471/2245 train_time:173786ms step_avg:118.14ms
step:1472/2245 train_time:173909ms step_avg:118.14ms
step:1473/2245 train_time:174026ms step_avg:118.14ms
step:1474/2245 train_time:174148ms step_avg:118.15ms
step:1475/2245 train_time:174265ms step_avg:118.15ms
step:1476/2245 train_time:174388ms step_avg:118.15ms
step:1477/2245 train_time:174504ms step_avg:118.15ms
step:1478/2245 train_time:174627ms step_avg:118.15ms
step:1479/2245 train_time:174744ms step_avg:118.15ms
step:1480/2245 train_time:174867ms step_avg:118.15ms
step:1481/2245 train_time:174985ms step_avg:118.15ms
step:1482/2245 train_time:175109ms step_avg:118.16ms
step:1483/2245 train_time:175225ms step_avg:118.16ms
step:1484/2245 train_time:175348ms step_avg:118.16ms
step:1485/2245 train_time:175465ms step_avg:118.16ms
step:1486/2245 train_time:175587ms step_avg:118.16ms
step:1487/2245 train_time:175704ms step_avg:118.16ms
step:1488/2245 train_time:175827ms step_avg:118.16ms
step:1489/2245 train_time:175944ms step_avg:118.16ms
step:1490/2245 train_time:176067ms step_avg:118.17ms
step:1491/2245 train_time:176184ms step_avg:118.17ms
step:1492/2245 train_time:176307ms step_avg:118.17ms
step:1493/2245 train_time:176425ms step_avg:118.17ms
step:1494/2245 train_time:176547ms step_avg:118.17ms
step:1495/2245 train_time:176664ms step_avg:118.17ms
step:1496/2245 train_time:176787ms step_avg:118.17ms
step:1497/2245 train_time:176905ms step_avg:118.17ms
step:1498/2245 train_time:177028ms step_avg:118.18ms
step:1499/2245 train_time:177145ms step_avg:118.18ms
step:1500/2245 train_time:177268ms step_avg:118.18ms
step:1500/2245 val_loss:3.4423 train_time:177335ms step_avg:118.22ms
step:1501/2245 train_time:177386ms step_avg:118.18ms
step:1502/2245 train_time:177508ms step_avg:118.18ms
step:1503/2245 train_time:177623ms step_avg:118.18ms
step:1504/2245 train_time:177745ms step_avg:118.18ms
step:1505/2245 train_time:177861ms step_avg:118.18ms
step:1506/2245 train_time:177985ms step_avg:118.18ms
step:1507/2245 train_time:178101ms step_avg:118.18ms
step:1508/2245 train_time:178224ms step_avg:118.19ms
step:1509/2245 train_time:178340ms step_avg:118.18ms
step:1510/2245 train_time:178463ms step_avg:118.19ms
step:1511/2245 train_time:178580ms step_avg:118.19ms
step:1512/2245 train_time:178703ms step_avg:118.19ms
step:1513/2245 train_time:178820ms step_avg:118.19ms
step:1514/2245 train_time:178943ms step_avg:118.19ms
step:1515/2245 train_time:179059ms step_avg:118.19ms
step:1516/2245 train_time:179182ms step_avg:118.19ms
step:1517/2245 train_time:179298ms step_avg:118.19ms
step:1518/2245 train_time:179421ms step_avg:118.20ms
step:1519/2245 train_time:179537ms step_avg:118.19ms
step:1520/2245 train_time:179660ms step_avg:118.20ms
step:1521/2245 train_time:179776ms step_avg:118.20ms
step:1522/2245 train_time:179899ms step_avg:118.20ms
step:1523/2245 train_time:180014ms step_avg:118.20ms
step:1524/2245 train_time:180137ms step_avg:118.20ms
step:1525/2245 train_time:180254ms step_avg:118.20ms
step:1526/2245 train_time:180378ms step_avg:118.20ms
step:1527/2245 train_time:180494ms step_avg:118.20ms
step:1528/2245 train_time:180616ms step_avg:118.20ms
step:1529/2245 train_time:180733ms step_avg:118.20ms
step:1530/2245 train_time:180856ms step_avg:118.21ms
step:1531/2245 train_time:180972ms step_avg:118.21ms
step:1532/2245 train_time:181095ms step_avg:118.21ms
step:1533/2245 train_time:181211ms step_avg:118.21ms
step:1534/2245 train_time:181334ms step_avg:118.21ms
step:1535/2245 train_time:181451ms step_avg:118.21ms
step:1536/2245 train_time:181574ms step_avg:118.21ms
step:1537/2245 train_time:181690ms step_avg:118.21ms
step:1538/2245 train_time:181813ms step_avg:118.21ms
step:1539/2245 train_time:181930ms step_avg:118.21ms
step:1540/2245 train_time:182053ms step_avg:118.22ms
step:1541/2245 train_time:182169ms step_avg:118.22ms
step:1542/2245 train_time:182292ms step_avg:118.22ms
step:1543/2245 train_time:182409ms step_avg:118.22ms
step:1544/2245 train_time:182531ms step_avg:118.22ms
step:1545/2245 train_time:182648ms step_avg:118.22ms
step:1546/2245 train_time:182771ms step_avg:118.22ms
step:1547/2245 train_time:182887ms step_avg:118.22ms
step:1548/2245 train_time:183010ms step_avg:118.22ms
step:1549/2245 train_time:183127ms step_avg:118.22ms
step:1550/2245 train_time:183250ms step_avg:118.23ms
step:1551/2245 train_time:183366ms step_avg:118.22ms
step:1552/2245 train_time:183489ms step_avg:118.23ms
step:1553/2245 train_time:183606ms step_avg:118.23ms
step:1554/2245 train_time:183729ms step_avg:118.23ms
step:1555/2245 train_time:183845ms step_avg:118.23ms
step:1556/2245 train_time:183967ms step_avg:118.23ms
step:1557/2245 train_time:184084ms step_avg:118.23ms
step:1558/2245 train_time:184208ms step_avg:118.23ms
step:1559/2245 train_time:184324ms step_avg:118.23ms
step:1560/2245 train_time:184446ms step_avg:118.23ms
step:1561/2245 train_time:184564ms step_avg:118.23ms
step:1562/2245 train_time:184686ms step_avg:118.24ms
step:1563/2245 train_time:184804ms step_avg:118.24ms
step:1564/2245 train_time:184926ms step_avg:118.24ms
step:1565/2245 train_time:185044ms step_avg:118.24ms
step:1566/2245 train_time:185167ms step_avg:118.24ms
step:1567/2245 train_time:185283ms step_avg:118.24ms
step:1568/2245 train_time:185406ms step_avg:118.24ms
step:1569/2245 train_time:185522ms step_avg:118.24ms
step:1570/2245 train_time:185645ms step_avg:118.25ms
step:1571/2245 train_time:185762ms step_avg:118.24ms
step:1572/2245 train_time:185884ms step_avg:118.25ms
step:1573/2245 train_time:186001ms step_avg:118.25ms
step:1574/2245 train_time:186124ms step_avg:118.25ms
step:1575/2245 train_time:186240ms step_avg:118.25ms
step:1576/2245 train_time:186364ms step_avg:118.25ms
step:1577/2245 train_time:186481ms step_avg:118.25ms
step:1578/2245 train_time:186605ms step_avg:118.25ms
step:1579/2245 train_time:186721ms step_avg:118.25ms
step:1580/2245 train_time:186845ms step_avg:118.26ms
step:1581/2245 train_time:186963ms step_avg:118.26ms
step:1582/2245 train_time:187085ms step_avg:118.26ms
step:1583/2245 train_time:187203ms step_avg:118.26ms
step:1584/2245 train_time:187326ms step_avg:118.26ms
step:1585/2245 train_time:187442ms step_avg:118.26ms
step:1586/2245 train_time:187564ms step_avg:118.26ms
step:1587/2245 train_time:187681ms step_avg:118.26ms
step:1588/2245 train_time:187804ms step_avg:118.26ms
step:1589/2245 train_time:187920ms step_avg:118.26ms
step:1590/2245 train_time:188043ms step_avg:118.27ms
step:1591/2245 train_time:188160ms step_avg:118.27ms
step:1592/2245 train_time:188284ms step_avg:118.27ms
step:1593/2245 train_time:188400ms step_avg:118.27ms
step:1594/2245 train_time:188524ms step_avg:118.27ms
step:1595/2245 train_time:188641ms step_avg:118.27ms
step:1596/2245 train_time:188764ms step_avg:118.27ms
step:1597/2245 train_time:188880ms step_avg:118.27ms
step:1598/2245 train_time:189003ms step_avg:118.27ms
step:1599/2245 train_time:189119ms step_avg:118.27ms
step:1600/2245 train_time:189242ms step_avg:118.28ms
step:1601/2245 train_time:189359ms step_avg:118.28ms
step:1602/2245 train_time:189482ms step_avg:118.28ms
step:1603/2245 train_time:189598ms step_avg:118.28ms
step:1604/2245 train_time:189721ms step_avg:118.28ms
step:1605/2245 train_time:189837ms step_avg:118.28ms
step:1606/2245 train_time:189959ms step_avg:118.28ms
step:1607/2245 train_time:190076ms step_avg:118.28ms
step:1608/2245 train_time:190199ms step_avg:118.28ms
step:1609/2245 train_time:190315ms step_avg:118.28ms
step:1610/2245 train_time:190437ms step_avg:118.28ms
step:1611/2245 train_time:190554ms step_avg:118.28ms
step:1612/2245 train_time:190676ms step_avg:118.29ms
step:1613/2245 train_time:190793ms step_avg:118.28ms
step:1614/2245 train_time:190916ms step_avg:118.29ms
step:1615/2245 train_time:191032ms step_avg:118.29ms
step:1616/2245 train_time:191156ms step_avg:118.29ms
step:1617/2245 train_time:191272ms step_avg:118.29ms
step:1618/2245 train_time:191395ms step_avg:118.29ms
step:1619/2245 train_time:191511ms step_avg:118.29ms
step:1620/2245 train_time:191634ms step_avg:118.29ms
step:1621/2245 train_time:191750ms step_avg:118.29ms
step:1622/2245 train_time:191873ms step_avg:118.29ms
step:1623/2245 train_time:191990ms step_avg:118.29ms
step:1624/2245 train_time:192113ms step_avg:118.30ms
step:1625/2245 train_time:192229ms step_avg:118.29ms
step:1626/2245 train_time:192353ms step_avg:118.30ms
step:1627/2245 train_time:192469ms step_avg:118.30ms
step:1628/2245 train_time:192592ms step_avg:118.30ms
step:1629/2245 train_time:192708ms step_avg:118.30ms
step:1630/2245 train_time:192831ms step_avg:118.30ms
step:1631/2245 train_time:192947ms step_avg:118.30ms
step:1632/2245 train_time:193070ms step_avg:118.30ms
step:1633/2245 train_time:193187ms step_avg:118.30ms
step:1634/2245 train_time:193310ms step_avg:118.31ms
step:1635/2245 train_time:193427ms step_avg:118.30ms
step:1636/2245 train_time:193550ms step_avg:118.31ms
step:1637/2245 train_time:193666ms step_avg:118.31ms
step:1638/2245 train_time:193790ms step_avg:118.31ms
step:1639/2245 train_time:193906ms step_avg:118.31ms
step:1640/2245 train_time:194029ms step_avg:118.31ms
step:1641/2245 train_time:194145ms step_avg:118.31ms
step:1642/2245 train_time:194269ms step_avg:118.31ms
step:1643/2245 train_time:194386ms step_avg:118.31ms
step:1644/2245 train_time:194509ms step_avg:118.31ms
step:1645/2245 train_time:194626ms step_avg:118.31ms
step:1646/2245 train_time:194749ms step_avg:118.32ms
step:1647/2245 train_time:194865ms step_avg:118.32ms
step:1648/2245 train_time:194988ms step_avg:118.32ms
step:1649/2245 train_time:195105ms step_avg:118.32ms
step:1650/2245 train_time:195228ms step_avg:118.32ms
step:1651/2245 train_time:195344ms step_avg:118.32ms
step:1652/2245 train_time:195468ms step_avg:118.32ms
step:1653/2245 train_time:195585ms step_avg:118.32ms
step:1654/2245 train_time:195707ms step_avg:118.32ms
step:1655/2245 train_time:195824ms step_avg:118.32ms
step:1656/2245 train_time:195947ms step_avg:118.33ms
step:1657/2245 train_time:196063ms step_avg:118.32ms
step:1658/2245 train_time:196187ms step_avg:118.33ms
step:1659/2245 train_time:196304ms step_avg:118.33ms
step:1660/2245 train_time:196427ms step_avg:118.33ms
step:1661/2245 train_time:196543ms step_avg:118.33ms
step:1662/2245 train_time:196667ms step_avg:118.33ms
step:1663/2245 train_time:196784ms step_avg:118.33ms
step:1664/2245 train_time:196906ms step_avg:118.33ms
step:1665/2245 train_time:197023ms step_avg:118.33ms
step:1666/2245 train_time:197147ms step_avg:118.34ms
step:1667/2245 train_time:197263ms step_avg:118.33ms
step:1668/2245 train_time:197386ms step_avg:118.34ms
step:1669/2245 train_time:197503ms step_avg:118.34ms
step:1670/2245 train_time:197625ms step_avg:118.34ms
step:1671/2245 train_time:197743ms step_avg:118.34ms
step:1672/2245 train_time:197866ms step_avg:118.34ms
step:1673/2245 train_time:197984ms step_avg:118.34ms
step:1674/2245 train_time:198106ms step_avg:118.34ms
step:1675/2245 train_time:198223ms step_avg:118.34ms
step:1676/2245 train_time:198346ms step_avg:118.34ms
step:1677/2245 train_time:198462ms step_avg:118.34ms
step:1678/2245 train_time:198586ms step_avg:118.35ms
step:1679/2245 train_time:198703ms step_avg:118.35ms
step:1680/2245 train_time:198827ms step_avg:118.35ms
step:1681/2245 train_time:198944ms step_avg:118.35ms
step:1682/2245 train_time:199067ms step_avg:118.35ms
step:1683/2245 train_time:199183ms step_avg:118.35ms
step:1684/2245 train_time:199306ms step_avg:118.35ms
step:1685/2245 train_time:199422ms step_avg:118.35ms
step:1686/2245 train_time:199545ms step_avg:118.35ms
step:1687/2245 train_time:199662ms step_avg:118.35ms
step:1688/2245 train_time:199785ms step_avg:118.36ms
step:1689/2245 train_time:199903ms step_avg:118.36ms
step:1690/2245 train_time:200025ms step_avg:118.36ms
step:1691/2245 train_time:200143ms step_avg:118.36ms
step:1692/2245 train_time:200266ms step_avg:118.36ms
step:1693/2245 train_time:200383ms step_avg:118.36ms
step:1694/2245 train_time:200505ms step_avg:118.36ms
step:1695/2245 train_time:200622ms step_avg:118.36ms
step:1696/2245 train_time:200745ms step_avg:118.36ms
step:1697/2245 train_time:200861ms step_avg:118.36ms
step:1698/2245 train_time:200985ms step_avg:118.37ms
step:1699/2245 train_time:201101ms step_avg:118.36ms
step:1700/2245 train_time:201224ms step_avg:118.37ms
step:1701/2245 train_time:201340ms step_avg:118.37ms
step:1702/2245 train_time:201463ms step_avg:118.37ms
step:1703/2245 train_time:201580ms step_avg:118.37ms
step:1704/2245 train_time:201703ms step_avg:118.37ms
step:1705/2245 train_time:201820ms step_avg:118.37ms
step:1706/2245 train_time:201943ms step_avg:118.37ms
step:1707/2245 train_time:202060ms step_avg:118.37ms
step:1708/2245 train_time:202184ms step_avg:118.37ms
step:1709/2245 train_time:202300ms step_avg:118.37ms
step:1710/2245 train_time:202423ms step_avg:118.38ms
step:1711/2245 train_time:202540ms step_avg:118.38ms
step:1712/2245 train_time:202662ms step_avg:118.38ms
step:1713/2245 train_time:202779ms step_avg:118.38ms
step:1714/2245 train_time:202902ms step_avg:118.38ms
step:1715/2245 train_time:203019ms step_avg:118.38ms
step:1716/2245 train_time:203142ms step_avg:118.38ms
step:1717/2245 train_time:203259ms step_avg:118.38ms
step:1718/2245 train_time:203381ms step_avg:118.38ms
step:1719/2245 train_time:203498ms step_avg:118.38ms
step:1720/2245 train_time:203620ms step_avg:118.38ms
step:1721/2245 train_time:203737ms step_avg:118.38ms
step:1722/2245 train_time:203859ms step_avg:118.39ms
step:1723/2245 train_time:203975ms step_avg:118.38ms
step:1724/2245 train_time:204099ms step_avg:118.39ms
step:1725/2245 train_time:204215ms step_avg:118.39ms
step:1726/2245 train_time:204338ms step_avg:118.39ms
step:1727/2245 train_time:204454ms step_avg:118.39ms
step:1728/2245 train_time:204578ms step_avg:118.39ms
step:1729/2245 train_time:204694ms step_avg:118.39ms
step:1730/2245 train_time:204817ms step_avg:118.39ms
step:1731/2245 train_time:204933ms step_avg:118.39ms
step:1732/2245 train_time:205057ms step_avg:118.39ms
step:1733/2245 train_time:205173ms step_avg:118.39ms
step:1734/2245 train_time:205296ms step_avg:118.39ms
step:1735/2245 train_time:205413ms step_avg:118.39ms
step:1736/2245 train_time:205536ms step_avg:118.40ms
step:1737/2245 train_time:205653ms step_avg:118.40ms
step:1738/2245 train_time:205776ms step_avg:118.40ms
step:1739/2245 train_time:205892ms step_avg:118.40ms
step:1740/2245 train_time:206015ms step_avg:118.40ms
step:1741/2245 train_time:206132ms step_avg:118.40ms
step:1742/2245 train_time:206255ms step_avg:118.40ms
step:1743/2245 train_time:206371ms step_avg:118.40ms
step:1744/2245 train_time:206494ms step_avg:118.40ms
step:1745/2245 train_time:206610ms step_avg:118.40ms
step:1746/2245 train_time:206734ms step_avg:118.40ms
step:1747/2245 train_time:206850ms step_avg:118.40ms
step:1748/2245 train_time:206973ms step_avg:118.41ms
step:1749/2245 train_time:207090ms step_avg:118.40ms
step:1750/2245 train_time:207214ms step_avg:118.41ms
step:1750/2245 val_loss:3.3789 train_time:207280ms step_avg:118.45ms
step:1751/2245 train_time:207332ms step_avg:118.41ms
step:1752/2245 train_time:207454ms step_avg:118.41ms
step:1753/2245 train_time:207570ms step_avg:118.41ms
step:1754/2245 train_time:207693ms step_avg:118.41ms
step:1755/2245 train_time:207809ms step_avg:118.41ms
step:1756/2245 train_time:207932ms step_avg:118.41ms
step:1757/2245 train_time:208048ms step_avg:118.41ms
step:1758/2245 train_time:208171ms step_avg:118.41ms
step:1759/2245 train_time:208287ms step_avg:118.41ms
step:1760/2245 train_time:208411ms step_avg:118.42ms
step:1761/2245 train_time:208527ms step_avg:118.41ms
step:1762/2245 train_time:208650ms step_avg:118.42ms
step:1763/2245 train_time:208766ms step_avg:118.42ms
step:1764/2245 train_time:208889ms step_avg:118.42ms
step:1765/2245 train_time:209006ms step_avg:118.42ms
step:1766/2245 train_time:209129ms step_avg:118.42ms
step:1767/2245 train_time:209246ms step_avg:118.42ms
step:1768/2245 train_time:209369ms step_avg:118.42ms
step:1769/2245 train_time:209486ms step_avg:118.42ms
step:1770/2245 train_time:209608ms step_avg:118.42ms
step:1771/2245 train_time:209725ms step_avg:118.42ms
step:1772/2245 train_time:209848ms step_avg:118.42ms
step:1773/2245 train_time:209964ms step_avg:118.42ms
step:1774/2245 train_time:210087ms step_avg:118.43ms
step:1775/2245 train_time:210204ms step_avg:118.42ms
step:1776/2245 train_time:210327ms step_avg:118.43ms
step:1777/2245 train_time:210443ms step_avg:118.43ms
step:1778/2245 train_time:210568ms step_avg:118.43ms
step:1779/2245 train_time:210684ms step_avg:118.43ms
step:1780/2245 train_time:210808ms step_avg:118.43ms
step:1781/2245 train_time:210924ms step_avg:118.43ms
step:1782/2245 train_time:211048ms step_avg:118.43ms
step:1783/2245 train_time:211164ms step_avg:118.43ms
step:1784/2245 train_time:211287ms step_avg:118.43ms
step:1785/2245 train_time:211403ms step_avg:118.43ms
step:1786/2245 train_time:211527ms step_avg:118.44ms
step:1787/2245 train_time:211643ms step_avg:118.43ms
step:1788/2245 train_time:211766ms step_avg:118.44ms
step:1789/2245 train_time:211883ms step_avg:118.44ms
step:1790/2245 train_time:212006ms step_avg:118.44ms
step:1791/2245 train_time:212122ms step_avg:118.44ms
step:1792/2245 train_time:212245ms step_avg:118.44ms
step:1793/2245 train_time:212362ms step_avg:118.44ms
step:1794/2245 train_time:212485ms step_avg:118.44ms
step:1795/2245 train_time:212601ms step_avg:118.44ms
step:1796/2245 train_time:212724ms step_avg:118.44ms
step:1797/2245 train_time:212840ms step_avg:118.44ms
step:1798/2245 train_time:212963ms step_avg:118.44ms
step:1799/2245 train_time:213080ms step_avg:118.44ms
step:1800/2245 train_time:213203ms step_avg:118.45ms
step:1801/2245 train_time:213320ms step_avg:118.45ms
step:1802/2245 train_time:213443ms step_avg:118.45ms
step:1803/2245 train_time:213560ms step_avg:118.45ms
step:1804/2245 train_time:213683ms step_avg:118.45ms
step:1805/2245 train_time:213799ms step_avg:118.45ms
step:1806/2245 train_time:213922ms step_avg:118.45ms
step:1807/2245 train_time:214038ms step_avg:118.45ms
step:1808/2245 train_time:214162ms step_avg:118.45ms
step:1809/2245 train_time:214278ms step_avg:118.45ms
step:1810/2245 train_time:214401ms step_avg:118.45ms
step:1811/2245 train_time:214518ms step_avg:118.45ms
step:1812/2245 train_time:214642ms step_avg:118.46ms
step:1813/2245 train_time:214759ms step_avg:118.46ms
step:1814/2245 train_time:214882ms step_avg:118.46ms
step:1815/2245 train_time:214999ms step_avg:118.46ms
step:1816/2245 train_time:215122ms step_avg:118.46ms
step:1817/2245 train_time:215239ms step_avg:118.46ms
step:1818/2245 train_time:215362ms step_avg:118.46ms
step:1819/2245 train_time:215479ms step_avg:118.46ms
step:1820/2245 train_time:215601ms step_avg:118.46ms
step:1821/2245 train_time:215717ms step_avg:118.46ms
step:1822/2245 train_time:215840ms step_avg:118.46ms
step:1823/2245 train_time:215957ms step_avg:118.46ms
step:1824/2245 train_time:216080ms step_avg:118.46ms
step:1825/2245 train_time:216196ms step_avg:118.46ms
step:1826/2245 train_time:216319ms step_avg:118.47ms
step:1827/2245 train_time:216435ms step_avg:118.46ms
step:1828/2245 train_time:216559ms step_avg:118.47ms
step:1829/2245 train_time:216676ms step_avg:118.47ms
step:1830/2245 train_time:216799ms step_avg:118.47ms
step:1831/2245 train_time:216915ms step_avg:118.47ms
step:1832/2245 train_time:217039ms step_avg:118.47ms
step:1833/2245 train_time:217155ms step_avg:118.47ms
step:1834/2245 train_time:217278ms step_avg:118.47ms
step:1835/2245 train_time:217395ms step_avg:118.47ms
step:1836/2245 train_time:217518ms step_avg:118.47ms
step:1837/2245 train_time:217635ms step_avg:118.47ms
step:1838/2245 train_time:217759ms step_avg:118.48ms
step:1839/2245 train_time:217875ms step_avg:118.47ms
step:1840/2245 train_time:217998ms step_avg:118.48ms
step:1841/2245 train_time:218115ms step_avg:118.48ms
step:1842/2245 train_time:218239ms step_avg:118.48ms
step:1843/2245 train_time:218355ms step_avg:118.48ms
step:1844/2245 train_time:218478ms step_avg:118.48ms
step:1845/2245 train_time:218595ms step_avg:118.48ms
step:1846/2245 train_time:218717ms step_avg:118.48ms
step:1847/2245 train_time:218834ms step_avg:118.48ms
step:1848/2245 train_time:218957ms step_avg:118.48ms
step:1849/2245 train_time:219074ms step_avg:118.48ms
step:1850/2245 train_time:219198ms step_avg:118.49ms
step:1851/2245 train_time:219315ms step_avg:118.48ms
step:1852/2245 train_time:219438ms step_avg:118.49ms
step:1853/2245 train_time:219555ms step_avg:118.49ms
step:1854/2245 train_time:219678ms step_avg:118.49ms
step:1855/2245 train_time:219794ms step_avg:118.49ms
step:1856/2245 train_time:219917ms step_avg:118.49ms
step:1857/2245 train_time:220035ms step_avg:118.49ms
step:1858/2245 train_time:220158ms step_avg:118.49ms
step:1859/2245 train_time:220274ms step_avg:118.49ms
step:1860/2245 train_time:220398ms step_avg:118.49ms
step:1861/2245 train_time:220514ms step_avg:118.49ms
step:1862/2245 train_time:220637ms step_avg:118.49ms
step:1863/2245 train_time:220754ms step_avg:118.49ms
step:1864/2245 train_time:220877ms step_avg:118.50ms
step:1865/2245 train_time:220993ms step_avg:118.50ms
step:1866/2245 train_time:221116ms step_avg:118.50ms
step:1867/2245 train_time:221233ms step_avg:118.50ms
step:1868/2245 train_time:221356ms step_avg:118.50ms
step:1869/2245 train_time:221473ms step_avg:118.50ms
step:1870/2245 train_time:221596ms step_avg:118.50ms
step:1871/2245 train_time:221712ms step_avg:118.50ms
step:1872/2245 train_time:221835ms step_avg:118.50ms
step:1873/2245 train_time:221951ms step_avg:118.50ms
step:1874/2245 train_time:222075ms step_avg:118.50ms
step:1875/2245 train_time:222191ms step_avg:118.50ms
step:1876/2245 train_time:222313ms step_avg:118.50ms
step:1877/2245 train_time:222430ms step_avg:118.50ms
step:1878/2245 train_time:222553ms step_avg:118.51ms
step:1879/2245 train_time:222669ms step_avg:118.50ms
step:1880/2245 train_time:222792ms step_avg:118.51ms
step:1881/2245 train_time:222908ms step_avg:118.51ms
step:1882/2245 train_time:223031ms step_avg:118.51ms
step:1883/2245 train_time:223147ms step_avg:118.51ms
step:1884/2245 train_time:223270ms step_avg:118.51ms
step:1885/2245 train_time:223386ms step_avg:118.51ms
step:1886/2245 train_time:223509ms step_avg:118.51ms
step:1887/2245 train_time:223626ms step_avg:118.51ms
step:1888/2245 train_time:223749ms step_avg:118.51ms
step:1889/2245 train_time:223864ms step_avg:118.51ms
step:1890/2245 train_time:223988ms step_avg:118.51ms
step:1891/2245 train_time:224104ms step_avg:118.51ms
step:1892/2245 train_time:224227ms step_avg:118.51ms
step:1893/2245 train_time:224344ms step_avg:118.51ms
step:1894/2245 train_time:224467ms step_avg:118.51ms
step:1895/2245 train_time:224584ms step_avg:118.51ms
step:1896/2245 train_time:224707ms step_avg:118.52ms
step:1897/2245 train_time:224824ms step_avg:118.52ms
step:1898/2245 train_time:224947ms step_avg:118.52ms
step:1899/2245 train_time:225064ms step_avg:118.52ms
step:1900/2245 train_time:225187ms step_avg:118.52ms
step:1901/2245 train_time:225303ms step_avg:118.52ms
step:1902/2245 train_time:225427ms step_avg:118.52ms
step:1903/2245 train_time:225543ms step_avg:118.52ms
step:1904/2245 train_time:225667ms step_avg:118.52ms
step:1905/2245 train_time:225783ms step_avg:118.52ms
step:1906/2245 train_time:225906ms step_avg:118.52ms
step:1907/2245 train_time:226023ms step_avg:118.52ms
step:1908/2245 train_time:226146ms step_avg:118.53ms
step:1909/2245 train_time:226262ms step_avg:118.52ms
step:1910/2245 train_time:226384ms step_avg:118.53ms
step:1911/2245 train_time:226500ms step_avg:118.52ms
step:1912/2245 train_time:226624ms step_avg:118.53ms
step:1913/2245 train_time:226740ms step_avg:118.53ms
step:1914/2245 train_time:226864ms step_avg:118.53ms
step:1915/2245 train_time:226980ms step_avg:118.53ms
step:1916/2245 train_time:227103ms step_avg:118.53ms
step:1917/2245 train_time:227219ms step_avg:118.53ms
step:1918/2245 train_time:227342ms step_avg:118.53ms
step:1919/2245 train_time:227459ms step_avg:118.53ms
step:1920/2245 train_time:227582ms step_avg:118.53ms
step:1921/2245 train_time:227698ms step_avg:118.53ms
step:1922/2245 train_time:227822ms step_avg:118.53ms
step:1923/2245 train_time:227938ms step_avg:118.53ms
step:1924/2245 train_time:228062ms step_avg:118.54ms
step:1925/2245 train_time:228179ms step_avg:118.53ms
step:1926/2245 train_time:228302ms step_avg:118.54ms
step:1927/2245 train_time:228418ms step_avg:118.54ms
step:1928/2245 train_time:228542ms step_avg:118.54ms
step:1929/2245 train_time:228659ms step_avg:118.54ms
step:1930/2245 train_time:228782ms step_avg:118.54ms
step:1931/2245 train_time:228899ms step_avg:118.54ms
step:1932/2245 train_time:229022ms step_avg:118.54ms
step:1933/2245 train_time:229139ms step_avg:118.54ms
step:1934/2245 train_time:229262ms step_avg:118.54ms
step:1935/2245 train_time:229378ms step_avg:118.54ms
step:1936/2245 train_time:229502ms step_avg:118.54ms
step:1937/2245 train_time:229619ms step_avg:118.54ms
step:1938/2245 train_time:229741ms step_avg:118.55ms
step:1939/2245 train_time:229858ms step_avg:118.54ms
step:1940/2245 train_time:229981ms step_avg:118.55ms
step:1941/2245 train_time:230098ms step_avg:118.55ms
step:1942/2245 train_time:230220ms step_avg:118.55ms
step:1943/2245 train_time:230337ms step_avg:118.55ms
step:1944/2245 train_time:230460ms step_avg:118.55ms
step:1945/2245 train_time:230577ms step_avg:118.55ms
step:1946/2245 train_time:230700ms step_avg:118.55ms
step:1947/2245 train_time:230817ms step_avg:118.55ms
step:1948/2245 train_time:230940ms step_avg:118.55ms
step:1949/2245 train_time:231058ms step_avg:118.55ms
step:1950/2245 train_time:231180ms step_avg:118.55ms
step:1951/2245 train_time:231297ms step_avg:118.55ms
step:1952/2245 train_time:231420ms step_avg:118.56ms
step:1953/2245 train_time:231536ms step_avg:118.55ms
step:1954/2245 train_time:231659ms step_avg:118.56ms
step:1955/2245 train_time:231776ms step_avg:118.56ms
step:1956/2245 train_time:231898ms step_avg:118.56ms
step:1957/2245 train_time:232015ms step_avg:118.56ms
step:1958/2245 train_time:232138ms step_avg:118.56ms
step:1959/2245 train_time:232255ms step_avg:118.56ms
step:1960/2245 train_time:232379ms step_avg:118.56ms
step:1961/2245 train_time:232495ms step_avg:118.56ms
step:1962/2245 train_time:232618ms step_avg:118.56ms
step:1963/2245 train_time:232734ms step_avg:118.56ms
step:1964/2245 train_time:232858ms step_avg:118.56ms
step:1965/2245 train_time:232974ms step_avg:118.56ms
step:1966/2245 train_time:233098ms step_avg:118.56ms
step:1967/2245 train_time:233214ms step_avg:118.56ms
step:1968/2245 train_time:233338ms step_avg:118.57ms
step:1969/2245 train_time:233455ms step_avg:118.57ms
step:1970/2245 train_time:233577ms step_avg:118.57ms
step:1971/2245 train_time:233694ms step_avg:118.57ms
step:1972/2245 train_time:233816ms step_avg:118.57ms
step:1973/2245 train_time:233933ms step_avg:118.57ms
step:1974/2245 train_time:234057ms step_avg:118.57ms
step:1975/2245 train_time:234173ms step_avg:118.57ms
step:1976/2245 train_time:234297ms step_avg:118.57ms
step:1977/2245 train_time:234413ms step_avg:118.57ms
step:1978/2245 train_time:234536ms step_avg:118.57ms
step:1979/2245 train_time:234653ms step_avg:118.57ms
step:1980/2245 train_time:234776ms step_avg:118.57ms
step:1981/2245 train_time:234892ms step_avg:118.57ms
step:1982/2245 train_time:235015ms step_avg:118.57ms
step:1983/2245 train_time:235132ms step_avg:118.57ms
step:1984/2245 train_time:235255ms step_avg:118.58ms
step:1985/2245 train_time:235371ms step_avg:118.57ms
step:1986/2245 train_time:235494ms step_avg:118.58ms
step:1987/2245 train_time:235611ms step_avg:118.58ms
step:1988/2245 train_time:235733ms step_avg:118.58ms
step:1989/2245 train_time:235850ms step_avg:118.58ms
step:1990/2245 train_time:235972ms step_avg:118.58ms
step:1991/2245 train_time:236089ms step_avg:118.58ms
step:1992/2245 train_time:236212ms step_avg:118.58ms
step:1993/2245 train_time:236328ms step_avg:118.58ms
step:1994/2245 train_time:236451ms step_avg:118.58ms
step:1995/2245 train_time:236567ms step_avg:118.58ms
step:1996/2245 train_time:236690ms step_avg:118.58ms
step:1997/2245 train_time:236807ms step_avg:118.58ms
step:1998/2245 train_time:236930ms step_avg:118.58ms
step:1999/2245 train_time:237047ms step_avg:118.58ms
step:2000/2245 train_time:237169ms step_avg:118.58ms
step:2000/2245 val_loss:3.3240 train_time:237236ms step_avg:118.62ms
step:2001/2245 train_time:237287ms step_avg:118.58ms
step:2002/2245 train_time:237409ms step_avg:118.59ms
step:2003/2245 train_time:237525ms step_avg:118.58ms
step:2004/2245 train_time:237648ms step_avg:118.59ms
step:2005/2245 train_time:237764ms step_avg:118.59ms
step:2006/2245 train_time:237888ms step_avg:118.59ms
step:2007/2245 train_time:238004ms step_avg:118.59ms
step:2008/2245 train_time:238126ms step_avg:118.59ms
step:2009/2245 train_time:238243ms step_avg:118.59ms
step:2010/2245 train_time:238366ms step_avg:118.59ms
step:2011/2245 train_time:238483ms step_avg:118.59ms
step:2012/2245 train_time:238606ms step_avg:118.59ms
step:2013/2245 train_time:238722ms step_avg:118.59ms
step:2014/2245 train_time:238845ms step_avg:118.59ms
step:2015/2245 train_time:238961ms step_avg:118.59ms
step:2016/2245 train_time:239084ms step_avg:118.59ms
step:2017/2245 train_time:239201ms step_avg:118.59ms
step:2018/2245 train_time:239323ms step_avg:118.59ms
step:2019/2245 train_time:239440ms step_avg:118.59ms
step:2020/2245 train_time:239563ms step_avg:118.60ms
step:2021/2245 train_time:239679ms step_avg:118.59ms
step:2022/2245 train_time:239802ms step_avg:118.60ms
step:2023/2245 train_time:239920ms step_avg:118.60ms
step:2024/2245 train_time:240042ms step_avg:118.60ms
step:2025/2245 train_time:240160ms step_avg:118.60ms
step:2026/2245 train_time:240283ms step_avg:118.60ms
step:2027/2245 train_time:240400ms step_avg:118.60ms
step:2028/2245 train_time:240523ms step_avg:118.60ms
step:2029/2245 train_time:240639ms step_avg:118.60ms
step:2030/2245 train_time:240761ms step_avg:118.60ms
step:2031/2245 train_time:240878ms step_avg:118.60ms
step:2032/2245 train_time:241001ms step_avg:118.60ms
step:2033/2245 train_time:241118ms step_avg:118.60ms
step:2034/2245 train_time:241240ms step_avg:118.60ms
step:2035/2245 train_time:241357ms step_avg:118.60ms
step:2036/2245 train_time:241481ms step_avg:118.61ms
step:2037/2245 train_time:241598ms step_avg:118.60ms
step:2038/2245 train_time:241721ms step_avg:118.61ms
step:2039/2245 train_time:241837ms step_avg:118.61ms
step:2040/2245 train_time:241959ms step_avg:118.61ms
step:2041/2245 train_time:242077ms step_avg:118.61ms
step:2042/2245 train_time:242199ms step_avg:118.61ms
step:2043/2245 train_time:242316ms step_avg:118.61ms
step:2044/2245 train_time:242439ms step_avg:118.61ms
step:2045/2245 train_time:242555ms step_avg:118.61ms
step:2046/2245 train_time:242679ms step_avg:118.61ms
step:2047/2245 train_time:242795ms step_avg:118.61ms
step:2048/2245 train_time:242918ms step_avg:118.61ms
step:2049/2245 train_time:243035ms step_avg:118.61ms
step:2050/2245 train_time:243158ms step_avg:118.61ms
step:2051/2245 train_time:243275ms step_avg:118.61ms
step:2052/2245 train_time:243398ms step_avg:118.62ms
step:2053/2245 train_time:243514ms step_avg:118.61ms
step:2054/2245 train_time:243638ms step_avg:118.62ms
step:2055/2245 train_time:243754ms step_avg:118.62ms
step:2056/2245 train_time:243878ms step_avg:118.62ms
step:2057/2245 train_time:243994ms step_avg:118.62ms
step:2058/2245 train_time:244117ms step_avg:118.62ms
step:2059/2245 train_time:244234ms step_avg:118.62ms
step:2060/2245 train_time:244357ms step_avg:118.62ms
step:2061/2245 train_time:244473ms step_avg:118.62ms
step:2062/2245 train_time:244596ms step_avg:118.62ms
step:2063/2245 train_time:244712ms step_avg:118.62ms
step:2064/2245 train_time:244835ms step_avg:118.62ms
step:2065/2245 train_time:244953ms step_avg:118.62ms
step:2066/2245 train_time:245075ms step_avg:118.62ms
step:2067/2245 train_time:245192ms step_avg:118.62ms
step:2068/2245 train_time:245316ms step_avg:118.62ms
step:2069/2245 train_time:245432ms step_avg:118.62ms
step:2070/2245 train_time:245555ms step_avg:118.63ms
step:2071/2245 train_time:245672ms step_avg:118.62ms
step:2072/2245 train_time:245795ms step_avg:118.63ms
step:2073/2245 train_time:245911ms step_avg:118.63ms
step:2074/2245 train_time:246034ms step_avg:118.63ms
step:2075/2245 train_time:246151ms step_avg:118.63ms
step:2076/2245 train_time:246274ms step_avg:118.63ms
step:2077/2245 train_time:246391ms step_avg:118.63ms
step:2078/2245 train_time:246513ms step_avg:118.63ms
step:2079/2245 train_time:246630ms step_avg:118.63ms
step:2080/2245 train_time:246754ms step_avg:118.63ms
step:2081/2245 train_time:246871ms step_avg:118.63ms
step:2082/2245 train_time:246994ms step_avg:118.63ms
step:2083/2245 train_time:247110ms step_avg:118.63ms
step:2084/2245 train_time:247234ms step_avg:118.63ms
step:2085/2245 train_time:247350ms step_avg:118.63ms
step:2086/2245 train_time:247473ms step_avg:118.64ms
step:2087/2245 train_time:247590ms step_avg:118.63ms
step:2088/2245 train_time:247713ms step_avg:118.64ms
step:2089/2245 train_time:247829ms step_avg:118.64ms
step:2090/2245 train_time:247953ms step_avg:118.64ms
step:2091/2245 train_time:248070ms step_avg:118.64ms
step:2092/2245 train_time:248193ms step_avg:118.64ms
step:2093/2245 train_time:248310ms step_avg:118.64ms
step:2094/2245 train_time:248433ms step_avg:118.64ms
step:2095/2245 train_time:248550ms step_avg:118.64ms
step:2096/2245 train_time:248673ms step_avg:118.64ms
step:2097/2245 train_time:248789ms step_avg:118.64ms
step:2098/2245 train_time:248913ms step_avg:118.64ms
step:2099/2245 train_time:249029ms step_avg:118.64ms
step:2100/2245 train_time:249153ms step_avg:118.64ms
step:2101/2245 train_time:249269ms step_avg:118.64ms
step:2102/2245 train_time:249393ms step_avg:118.65ms
step:2103/2245 train_time:249509ms step_avg:118.64ms
step:2104/2245 train_time:249632ms step_avg:118.65ms
step:2105/2245 train_time:249749ms step_avg:118.65ms
step:2106/2245 train_time:249872ms step_avg:118.65ms
step:2107/2245 train_time:249989ms step_avg:118.65ms
step:2108/2245 train_time:250112ms step_avg:118.65ms
step:2109/2245 train_time:250229ms step_avg:118.65ms
step:2110/2245 train_time:250352ms step_avg:118.65ms
step:2111/2245 train_time:250469ms step_avg:118.65ms
step:2112/2245 train_time:250593ms step_avg:118.65ms
step:2113/2245 train_time:250710ms step_avg:118.65ms
step:2114/2245 train_time:250833ms step_avg:118.65ms
step:2115/2245 train_time:250950ms step_avg:118.65ms
step:2116/2245 train_time:251073ms step_avg:118.65ms
step:2117/2245 train_time:251190ms step_avg:118.65ms
step:2118/2245 train_time:251313ms step_avg:118.66ms
step:2119/2245 train_time:251430ms step_avg:118.65ms
step:2120/2245 train_time:251553ms step_avg:118.66ms
step:2121/2245 train_time:251670ms step_avg:118.66ms
step:2122/2245 train_time:251793ms step_avg:118.66ms
step:2123/2245 train_time:251910ms step_avg:118.66ms
step:2124/2245 train_time:252032ms step_avg:118.66ms
step:2125/2245 train_time:252150ms step_avg:118.66ms
step:2126/2245 train_time:252273ms step_avg:118.66ms
step:2127/2245 train_time:252389ms step_avg:118.66ms
step:2128/2245 train_time:252512ms step_avg:118.66ms
step:2129/2245 train_time:252629ms step_avg:118.66ms
step:2130/2245 train_time:252753ms step_avg:118.66ms
step:2131/2245 train_time:252870ms step_avg:118.66ms
step:2132/2245 train_time:252993ms step_avg:118.66ms
step:2133/2245 train_time:253110ms step_avg:118.66ms
step:2134/2245 train_time:253233ms step_avg:118.67ms
step:2135/2245 train_time:253350ms step_avg:118.67ms
step:2136/2245 train_time:253473ms step_avg:118.67ms
step:2137/2245 train_time:253589ms step_avg:118.67ms
step:2138/2245 train_time:253712ms step_avg:118.67ms
step:2139/2245 train_time:253828ms step_avg:118.67ms
step:2140/2245 train_time:253952ms step_avg:118.67ms
step:2141/2245 train_time:254068ms step_avg:118.67ms
step:2142/2245 train_time:254191ms step_avg:118.67ms
step:2143/2245 train_time:254308ms step_avg:118.67ms
step:2144/2245 train_time:254432ms step_avg:118.67ms
step:2145/2245 train_time:254549ms step_avg:118.67ms
step:2146/2245 train_time:254672ms step_avg:118.67ms
step:2147/2245 train_time:254788ms step_avg:118.67ms
step:2148/2245 train_time:254911ms step_avg:118.67ms
step:2149/2245 train_time:255028ms step_avg:118.67ms
step:2150/2245 train_time:255151ms step_avg:118.68ms
step:2151/2245 train_time:255268ms step_avg:118.67ms
step:2152/2245 train_time:255392ms step_avg:118.68ms
step:2153/2245 train_time:255509ms step_avg:118.68ms
step:2154/2245 train_time:255633ms step_avg:118.68ms
step:2155/2245 train_time:255749ms step_avg:118.68ms
step:2156/2245 train_time:255873ms step_avg:118.68ms
step:2157/2245 train_time:255990ms step_avg:118.68ms
step:2158/2245 train_time:256113ms step_avg:118.68ms
step:2159/2245 train_time:256229ms step_avg:118.68ms
step:2160/2245 train_time:256352ms step_avg:118.68ms
step:2161/2245 train_time:256469ms step_avg:118.68ms
step:2162/2245 train_time:256593ms step_avg:118.68ms
step:2163/2245 train_time:256710ms step_avg:118.68ms
step:2164/2245 train_time:256833ms step_avg:118.68ms
step:2165/2245 train_time:256949ms step_avg:118.68ms
step:2166/2245 train_time:257073ms step_avg:118.69ms
step:2167/2245 train_time:257189ms step_avg:118.68ms
step:2168/2245 train_time:257313ms step_avg:118.69ms
step:2169/2245 train_time:257429ms step_avg:118.69ms
step:2170/2245 train_time:257553ms step_avg:118.69ms
step:2171/2245 train_time:257669ms step_avg:118.69ms
step:2172/2245 train_time:257793ms step_avg:118.69ms
step:2173/2245 train_time:257910ms step_avg:118.69ms
step:2174/2245 train_time:258034ms step_avg:118.69ms
step:2175/2245 train_time:258151ms step_avg:118.69ms
step:2176/2245 train_time:258273ms step_avg:118.69ms
step:2177/2245 train_time:258391ms step_avg:118.69ms
step:2178/2245 train_time:258513ms step_avg:118.69ms
step:2179/2245 train_time:258629ms step_avg:118.69ms
step:2180/2245 train_time:258753ms step_avg:118.69ms
step:2181/2245 train_time:258869ms step_avg:118.69ms
step:2182/2245 train_time:258992ms step_avg:118.69ms
step:2183/2245 train_time:259109ms step_avg:118.69ms
step:2184/2245 train_time:259233ms step_avg:118.70ms
step:2185/2245 train_time:259350ms step_avg:118.70ms
step:2186/2245 train_time:259473ms step_avg:118.70ms
step:2187/2245 train_time:259590ms step_avg:118.70ms
step:2188/2245 train_time:259714ms step_avg:118.70ms
step:2189/2245 train_time:259831ms step_avg:118.70ms
step:2190/2245 train_time:259954ms step_avg:118.70ms
step:2191/2245 train_time:260071ms step_avg:118.70ms
step:2192/2245 train_time:260194ms step_avg:118.70ms
step:2193/2245 train_time:260311ms step_avg:118.70ms
step:2194/2245 train_time:260433ms step_avg:118.70ms
step:2195/2245 train_time:260549ms step_avg:118.70ms
step:2196/2245 train_time:260673ms step_avg:118.70ms
step:2197/2245 train_time:260789ms step_avg:118.70ms
step:2198/2245 train_time:260913ms step_avg:118.70ms
step:2199/2245 train_time:261029ms step_avg:118.70ms
step:2200/2245 train_time:261153ms step_avg:118.71ms
step:2201/2245 train_time:261269ms step_avg:118.70ms
step:2202/2245 train_time:261392ms step_avg:118.71ms
step:2203/2245 train_time:261510ms step_avg:118.71ms
step:2204/2245 train_time:261632ms step_avg:118.71ms
step:2205/2245 train_time:261749ms step_avg:118.71ms
step:2206/2245 train_time:261872ms step_avg:118.71ms
step:2207/2245 train_time:261990ms step_avg:118.71ms
step:2208/2245 train_time:262113ms step_avg:118.71ms
step:2209/2245 train_time:262231ms step_avg:118.71ms
step:2210/2245 train_time:262354ms step_avg:118.71ms
step:2211/2245 train_time:262471ms step_avg:118.71ms
step:2212/2245 train_time:262595ms step_avg:118.71ms
step:2213/2245 train_time:262712ms step_avg:118.71ms
step:2214/2245 train_time:262835ms step_avg:118.72ms
step:2215/2245 train_time:262952ms step_avg:118.71ms
step:2216/2245 train_time:263076ms step_avg:118.72ms
step:2217/2245 train_time:263193ms step_avg:118.72ms
step:2218/2245 train_time:263317ms step_avg:118.72ms
step:2219/2245 train_time:263434ms step_avg:118.72ms
step:2220/2245 train_time:263557ms step_avg:118.72ms
step:2221/2245 train_time:263673ms step_avg:118.72ms
step:2222/2245 train_time:263796ms step_avg:118.72ms
step:2223/2245 train_time:263913ms step_avg:118.72ms
step:2224/2245 train_time:264036ms step_avg:118.72ms
step:2225/2245 train_time:264152ms step_avg:118.72ms
step:2226/2245 train_time:264276ms step_avg:118.72ms
step:2227/2245 train_time:264393ms step_avg:118.72ms
step:2228/2245 train_time:264517ms step_avg:118.72ms
step:2229/2245 train_time:264633ms step_avg:118.72ms
step:2230/2245 train_time:264757ms step_avg:118.72ms
step:2231/2245 train_time:264874ms step_avg:118.72ms
step:2232/2245 train_time:264997ms step_avg:118.73ms
step:2233/2245 train_time:265114ms step_avg:118.73ms
step:2234/2245 train_time:265237ms step_avg:118.73ms
step:2235/2245 train_time:265354ms step_avg:118.73ms
step:2236/2245 train_time:265477ms step_avg:118.73ms
step:2237/2245 train_time:265594ms step_avg:118.73ms
step:2238/2245 train_time:265717ms step_avg:118.73ms
step:2239/2245 train_time:265834ms step_avg:118.73ms
step:2240/2245 train_time:265957ms step_avg:118.73ms
step:2241/2245 train_time:266074ms step_avg:118.73ms
step:2242/2245 train_time:266198ms step_avg:118.73ms
step:2243/2245 train_time:266314ms step_avg:118.73ms
step:2244/2245 train_time:266437ms step_avg:118.73ms
step:2245/2245 train_time:266555ms step_avg:118.73ms
step:2245/2245 val_loss:3.2779 train_time:266623ms step_avg:118.76ms
peak memory allocated: 29050 MiB reserved: 44456 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


def _get_gemm_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
            },
            num_stages=st,
            num_warps=wp,
        )
        for bm in (64, 128)
        for bn in (64, 128, 256)
        for bk in (32, 64, 128)
        for st, wp in ((3, 4), (4, 4), (3, 8))
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block_aX_plus_BX(
    pid,
    M,
    N,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """Same helper as in your earlier kernels, extended with N."""
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

    batch = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    return batch, pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N


@triton.autotune(
    configs=_get_gemm_configs(),
    key=["M", "N", "b_stride_r", "b_stride_c", "x_stride_r", "x_stride_c"],
)
@triton.jit
def aX_plus_BX_kernel(
    B_ptr,  # [B, M, M]   symmetric
    X_ptr,  # [B, M, N]
    C_ptr,  # [B, M, N]
    M,
    N,  # rows(X)=M, cols(X)=N
    b_stride_b,
    b_stride_r,
    b_stride_c,
    x_stride_b,
    x_stride_r,
    x_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,  # scalar a  (scale of X)
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch, m_start, n_start = _pid_to_block_aX_plus_BX(
        pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Offset base pointers to this batch
    B_ptr += batch * b_stride_b
    X_ptr += batch * x_stride_b
    C_ptr += batch * c_stride_b

    # Create index ranges for the tile
    offs_m = m_start + tl.arange(0, BLOCK_SIZE_M)
    offs_n = n_start + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # Pointers to B and X tiles
    b_ptrs = B_ptr + offs_m[:, None] * b_stride_r + offs_k[None, :] * b_stride_c
    x_ptrs = X_ptr + offs_k[:, None] * x_stride_r + offs_n[None, :] * x_stride_c

    # Accumulator, initialized with bias * alpha
    x_bias_ptrs = X_ptr + offs_m[:, None] * x_stride_r + offs_n[None, :] * x_stride_c
    acc = (
        tl.load(
            x_bias_ptrs, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N), other=0.0
        )
        * alpha
    ).to(tl.float32)

    # GEMM main loop
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        b = tl.load(b_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        x = tl.load(x_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        acc = tl.dot(b, x, acc)
        b_ptrs += BLOCK_SIZE_K * b_stride_c
        x_ptrs += BLOCK_SIZE_K * x_stride_r

    out_dtype = C_ptr.dtype.element_ty
    acc = acc.to(out_dtype)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * c_stride_r + offs_n[None, :] * c_stride_c
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def aX_plus_BX(B: Tensor, X: Tensor, a: float, *, out: Tensor = None) -> Tensor:
    """
    Fused implementation of C = a * X + B @ X
    B must be square & symmetric, X has same leading dim, arbitrary trailing cols.
    """
    if B.shape[-2] != B.shape[-1]:
        raise ValueError("B must be square")

    if B.shape[-2] != X.shape[-2]:
        raise ValueError("B and X must have the same number of rows")

    # Broadcast & batch handling (supports 2‑ or 3‑D inputs)
    M, N = X.shape[-2:]
    batch = X.shape[0] if X.ndim == 3 else 1

    if out is None:
        out = torch.empty_like(X)

    grid = lambda meta: (
        batch
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(N, meta["BLOCK_SIZE_N"]),
    )

    aX_plus_BX_kernel[grid](
        B_ptr=B,
        X_ptr=X,
        C_ptr=out,
        M=M,
        N=N,
        b_stride_b=B.stride(0) if B.ndim == 3 else 0,
        b_stride_r=B.stride(-2),
        b_stride_c=B.stride(-1),
        x_stride_b=X.stride(0) if X.ndim == 3 else 0,
        x_stride_r=X.stride(-2),
        x_stride_c=X.stride(-1),
        c_stride_b=out.stride(0) if out.ndim == 3 else 0,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=a,
    )
    return out


# Computed for num_iters=5, safety_factor=2e-2, cushion=2
# the first iteration were simply removed since we have pre-conditioning
# maybe we can do even better by re-computing those coeffs, but
# this did not yield significant improvements in our experiments
polar_express_coeffs = [
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def turbo_muon_polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932 and https://arxiv.org/pdf/2506.10935
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal. See also
    https://github.com/microsoft/dion/blob/main/dion/newton_schulz_triton.py
    This implementation is updated using the pre-conditioning method as described in
    https://github.com/thib-s/flash-newton-schulz.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Ensure spectral norm is at most 1
    # we remove the previous normalization to switch to AOL rescaling
    # Which is further explained in the paper: https://arxiv.org/pdf/2208.03160
    # which consists in computing W@W^t using ns_line_1 and then computing the
    # scaling factors: fast_inv_sqrt(reduce_sum(abs(WW^t), axis=-1)) which is a vector
    # since the main operation to compute those correspond to ns_line_1
    # we can fuse it with the first newton schulz iterate. Furthermore this gives a better
    # starting point for the newton schulz iterations as the matrix is closer to orthogonal
    # thanks to this, we can save one iteration of newton schulz.
    XXT(X, out=A)  # gram matrix A = X @ X.mT
    s = torch.rsqrt(
        A.abs().sum(dim=-1, keepdim=False) * (1.0 + 2e-2) + 1e-6
    )  # AOL rescaling vector
    X = X * s.unsqueeze(-1)  # rescale X using s making it closer to orthogonal
    # first NS iteration with reuse of A
    a, b, c = polar_express_coeffs[0]
    A = A * s.unsqueeze(-1) * s.unsqueeze(-2)
    ba_plus_cAA(A, alpha=c, beta=b, out=B)
    aX_plus_BX(B, X, a, out=C)
    X, C = C, X

    # Perform the iterations
    for a, b, c in polar_express_coeffs[1:]:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(B, X, a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = turbo_muon_polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]
Running PyTorch 2.10.0.dev20251019+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 10:04:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |
| N/A   42C    P0            116W /  700W |    3323MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2C:00.0 Off |                    0 |
| N/A   41C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9D:00.0 Off |                    0 |
| N/A   42C    P0            119W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AD:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1469MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         3340313      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    0   N/A  N/A         3340314      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3340315      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    0   N/A  N/A         3340316      C   ...ednanogpt-new-h100/bin/python        612MiB |
|    1   N/A  N/A         3340314      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    2   N/A  N/A         3340315      C   ...ednanogpt-new-h100/bin/python       1458MiB |
|    3   N/A  N/A         3340316      C   ...ednanogpt-new-h100/bin/python       1458MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:161ms step_avg:160.77ms
step:2/2245 train_time:215ms step_avg:107.31ms
step:3/2245 train_time:323ms step_avg:107.76ms
step:4/2245 train_time:441ms step_avg:110.37ms
step:5/2245 train_time:557ms step_avg:111.42ms
step:6/2245 train_time:680ms step_avg:113.28ms
step:7/2245 train_time:796ms step_avg:113.68ms
step:8/2245 train_time:918ms step_avg:114.80ms
step:9/2245 train_time:1034ms step_avg:114.92ms
step:10/2245 train_time:1157ms step_avg:115.75ms
step:11/2245 train_time:1274ms step_avg:115.82ms
step:12/2245 train_time:1397ms step_avg:116.42ms
step:13/2245 train_time:1513ms step_avg:116.39ms
step:14/2245 train_time:1636ms step_avg:116.88ms
step:15/2245 train_time:1753ms step_avg:116.85ms
step:16/2245 train_time:1875ms step_avg:117.21ms
step:17/2245 train_time:1992ms step_avg:117.17ms
step:18/2245 train_time:2115ms step_avg:117.52ms
step:19/2245 train_time:2232ms step_avg:117.45ms
step:20/2245 train_time:2355ms step_avg:117.74ms
step:21/2245 train_time:2471ms step_avg:117.67ms
step:22/2245 train_time:2594ms step_avg:117.92ms
step:23/2245 train_time:2710ms step_avg:117.84ms
step:24/2245 train_time:2833ms step_avg:118.03ms
step:25/2245 train_time:2950ms step_avg:117.98ms
step:26/2245 train_time:3072ms step_avg:118.15ms
step:27/2245 train_time:3188ms step_avg:118.08ms
step:28/2245 train_time:3311ms step_avg:118.24ms
step:29/2245 train_time:3426ms step_avg:118.15ms
step:30/2245 train_time:3549ms step_avg:118.30ms
step:31/2245 train_time:3665ms step_avg:118.22ms
step:32/2245 train_time:3787ms step_avg:118.35ms
step:33/2245 train_time:3903ms step_avg:118.28ms
step:34/2245 train_time:4025ms step_avg:118.38ms
step:35/2245 train_time:4141ms step_avg:118.32ms
step:36/2245 train_time:4263ms step_avg:118.41ms
step:37/2245 train_time:4379ms step_avg:118.35ms
step:38/2245 train_time:4501ms step_avg:118.46ms
step:39/2245 train_time:4617ms step_avg:118.39ms
step:40/2245 train_time:4739ms step_avg:118.48ms
step:41/2245 train_time:4855ms step_avg:118.42ms
step:42/2245 train_time:4977ms step_avg:118.49ms
step:43/2245 train_time:5093ms step_avg:118.43ms
step:44/2245 train_time:5215ms step_avg:118.51ms
step:45/2245 train_time:5330ms step_avg:118.45ms
step:46/2245 train_time:5452ms step_avg:118.52ms
step:47/2245 train_time:5567ms step_avg:118.45ms
step:48/2245 train_time:5689ms step_avg:118.53ms
step:49/2245 train_time:5805ms step_avg:118.46ms
step:50/2245 train_time:5926ms step_avg:118.52ms
step:51/2245 train_time:6041ms step_avg:118.46ms
step:52/2245 train_time:6163ms step_avg:118.52ms
step:53/2245 train_time:6278ms step_avg:118.46ms
step:54/2245 train_time:6400ms step_avg:118.52ms
step:55/2245 train_time:6515ms step_avg:118.45ms
step:56/2245 train_time:6636ms step_avg:118.51ms
step:57/2245 train_time:6752ms step_avg:118.45ms
step:58/2245 train_time:6874ms step_avg:118.51ms
step:59/2245 train_time:6989ms step_avg:118.46ms
step:60/2245 train_time:7111ms step_avg:118.52ms
step:61/2245 train_time:7226ms step_avg:118.46ms
step:62/2245 train_time:7348ms step_avg:118.52ms
step:63/2245 train_time:7463ms step_avg:118.46ms
step:64/2245 train_time:7584ms step_avg:118.51ms
step:65/2245 train_time:7700ms step_avg:118.46ms
step:66/2245 train_time:7821ms step_avg:118.50ms
step:67/2245 train_time:7936ms step_avg:118.45ms
step:68/2245 train_time:8058ms step_avg:118.49ms
step:69/2245 train_time:8173ms step_avg:118.44ms
step:70/2245 train_time:8294ms step_avg:118.49ms
step:71/2245 train_time:8409ms step_avg:118.44ms
step:72/2245 train_time:8531ms step_avg:118.48ms
step:73/2245 train_time:8646ms step_avg:118.44ms
step:74/2245 train_time:8768ms step_avg:118.49ms
step:75/2245 train_time:8883ms step_avg:118.43ms
step:76/2245 train_time:9004ms step_avg:118.47ms
step:77/2245 train_time:9119ms step_avg:118.43ms
step:78/2245 train_time:9241ms step_avg:118.47ms
step:79/2245 train_time:9355ms step_avg:118.42ms
step:80/2245 train_time:9477ms step_avg:118.46ms
step:81/2245 train_time:9592ms step_avg:118.42ms
step:82/2245 train_time:9714ms step_avg:118.46ms
step:83/2245 train_time:9829ms step_avg:118.42ms
step:84/2245 train_time:9950ms step_avg:118.45ms
step:85/2245 train_time:10065ms step_avg:118.41ms
step:86/2245 train_time:10186ms step_avg:118.44ms
step:87/2245 train_time:10301ms step_avg:118.40ms
step:88/2245 train_time:10422ms step_avg:118.43ms
step:89/2245 train_time:10537ms step_avg:118.40ms
step:90/2245 train_time:10658ms step_avg:118.43ms
step:91/2245 train_time:10773ms step_avg:118.38ms
step:92/2245 train_time:10894ms step_avg:118.41ms
step:93/2245 train_time:11009ms step_avg:118.38ms
step:94/2245 train_time:11130ms step_avg:118.41ms
step:95/2245 train_time:11245ms step_avg:118.37ms
step:96/2245 train_time:11366ms step_avg:118.40ms
step:97/2245 train_time:11481ms step_avg:118.36ms
step:98/2245 train_time:11602ms step_avg:118.39ms
step:99/2245 train_time:11717ms step_avg:118.35ms
step:100/2245 train_time:11839ms step_avg:118.39ms
step:101/2245 train_time:11953ms step_avg:118.35ms
step:102/2245 train_time:12074ms step_avg:118.38ms
step:103/2245 train_time:12189ms step_avg:118.34ms
step:104/2245 train_time:12310ms step_avg:118.37ms
step:105/2245 train_time:12425ms step_avg:118.33ms
step:106/2245 train_time:12547ms step_avg:118.37ms
step:107/2245 train_time:12661ms step_avg:118.33ms
step:108/2245 train_time:12783ms step_avg:118.36ms
step:109/2245 train_time:12898ms step_avg:118.33ms
step:110/2245 train_time:13019ms step_avg:118.35ms
step:111/2245 train_time:13133ms step_avg:118.32ms
step:112/2245 train_time:13254ms step_avg:118.34ms
step:113/2245 train_time:13369ms step_avg:118.31ms
step:114/2245 train_time:13491ms step_avg:118.34ms
step:115/2245 train_time:13605ms step_avg:118.31ms
step:116/2245 train_time:13726ms step_avg:118.33ms
step:117/2245 train_time:13841ms step_avg:118.30ms
step:118/2245 train_time:13962ms step_avg:118.32ms
step:119/2245 train_time:14077ms step_avg:118.29ms
step:120/2245 train_time:14198ms step_avg:118.31ms
step:121/2245 train_time:14312ms step_avg:118.28ms
step:122/2245 train_time:14433ms step_avg:118.30ms
step:123/2245 train_time:14548ms step_avg:118.27ms
step:124/2245 train_time:14669ms step_avg:118.30ms
step:125/2245 train_time:14783ms step_avg:118.27ms
step:126/2245 train_time:14904ms step_avg:118.29ms
step:127/2245 train_time:15019ms step_avg:118.26ms
step:128/2245 train_time:15140ms step_avg:118.28ms
step:129/2245 train_time:15255ms step_avg:118.25ms
step:130/2245 train_time:15376ms step_avg:118.27ms
step:131/2245 train_time:15490ms step_avg:118.24ms
step:132/2245 train_time:15611ms step_avg:118.26ms
step:133/2245 train_time:15725ms step_avg:118.23ms
step:134/2245 train_time:15846ms step_avg:118.25ms
step:135/2245 train_time:15961ms step_avg:118.23ms
step:136/2245 train_time:16081ms step_avg:118.25ms
step:137/2245 train_time:16196ms step_avg:118.22ms
step:138/2245 train_time:16317ms step_avg:118.24ms
step:139/2245 train_time:16431ms step_avg:118.21ms
step:140/2245 train_time:16552ms step_avg:118.23ms
step:141/2245 train_time:16667ms step_avg:118.20ms
step:142/2245 train_time:16788ms step_avg:118.22ms
step:143/2245 train_time:16902ms step_avg:118.20ms
step:144/2245 train_time:17023ms step_avg:118.21ms
step:145/2245 train_time:17137ms step_avg:118.19ms
step:146/2245 train_time:17258ms step_avg:118.21ms
step:147/2245 train_time:17372ms step_avg:118.18ms
step:148/2245 train_time:17493ms step_avg:118.20ms
step:149/2245 train_time:17608ms step_avg:118.17ms
step:150/2245 train_time:17728ms step_avg:118.19ms
step:151/2245 train_time:17843ms step_avg:118.16ms
step:152/2245 train_time:17963ms step_avg:118.18ms
step:153/2245 train_time:18078ms step_avg:118.16ms
step:154/2245 train_time:18199ms step_avg:118.17ms
step:155/2245 train_time:18313ms step_avg:118.15ms
step:156/2245 train_time:18434ms step_avg:118.17ms
step:157/2245 train_time:18548ms step_avg:118.14ms
step:158/2245 train_time:18669ms step_avg:118.16ms
step:159/2245 train_time:18784ms step_avg:118.14ms
step:160/2245 train_time:18904ms step_avg:118.15ms
step:161/2245 train_time:19019ms step_avg:118.13ms
step:162/2245 train_time:19139ms step_avg:118.14ms
step:163/2245 train_time:19254ms step_avg:118.12ms
step:164/2245 train_time:19374ms step_avg:118.14ms
step:165/2245 train_time:19489ms step_avg:118.11ms
step:166/2245 train_time:19610ms step_avg:118.13ms
step:167/2245 train_time:19724ms step_avg:118.11ms
step:168/2245 train_time:19845ms step_avg:118.12ms
step:169/2245 train_time:19959ms step_avg:118.10ms
step:170/2245 train_time:20080ms step_avg:118.12ms
step:171/2245 train_time:20194ms step_avg:118.09ms
step:172/2245 train_time:20315ms step_avg:118.11ms
step:173/2245 train_time:20429ms step_avg:118.09ms
step:174/2245 train_time:20550ms step_avg:118.10ms
step:175/2245 train_time:20663ms step_avg:118.08ms
step:176/2245 train_time:20784ms step_avg:118.09ms
step:177/2245 train_time:20899ms step_avg:118.07ms
step:178/2245 train_time:21019ms step_avg:118.09ms
step:179/2245 train_time:21134ms step_avg:118.06ms
step:180/2245 train_time:21254ms step_avg:118.08ms
step:181/2245 train_time:21368ms step_avg:118.06ms
step:182/2245 train_time:21489ms step_avg:118.07ms
step:183/2245 train_time:21603ms step_avg:118.05ms
step:184/2245 train_time:21724ms step_avg:118.06ms
step:185/2245 train_time:21838ms step_avg:118.04ms
step:186/2245 train_time:21959ms step_avg:118.06ms
step:187/2245 train_time:22073ms step_avg:118.04ms
step:188/2245 train_time:22193ms step_avg:118.05ms
step:189/2245 train_time:22308ms step_avg:118.03ms
step:190/2245 train_time:22428ms step_avg:118.04ms
step:191/2245 train_time:22542ms step_avg:118.02ms
step:192/2245 train_time:22663ms step_avg:118.04ms
step:193/2245 train_time:22778ms step_avg:118.02ms
step:194/2245 train_time:22898ms step_avg:118.03ms
step:195/2245 train_time:23012ms step_avg:118.01ms
step:196/2245 train_time:23133ms step_avg:118.02ms
step:197/2245 train_time:23248ms step_avg:118.01ms
step:198/2245 train_time:23368ms step_avg:118.02ms
step:199/2245 train_time:23482ms step_avg:118.00ms
step:200/2245 train_time:23603ms step_avg:118.01ms
step:201/2245 train_time:23717ms step_avg:118.00ms
step:202/2245 train_time:23838ms step_avg:118.01ms
step:203/2245 train_time:23952ms step_avg:117.99ms
step:204/2245 train_time:24072ms step_avg:118.00ms
step:205/2245 train_time:24186ms step_avg:117.98ms
step:206/2245 train_time:24306ms step_avg:117.99ms
step:207/2245 train_time:24421ms step_avg:117.97ms
step:208/2245 train_time:24541ms step_avg:117.99ms
step:209/2245 train_time:24655ms step_avg:117.97ms
step:210/2245 train_time:24776ms step_avg:117.98ms
step:211/2245 train_time:24890ms step_avg:117.96ms
step:212/2245 train_time:25011ms step_avg:117.97ms
step:213/2245 train_time:25125ms step_avg:117.96ms
step:214/2245 train_time:25245ms step_avg:117.97ms
step:215/2245 train_time:25359ms step_avg:117.95ms
step:216/2245 train_time:25480ms step_avg:117.96ms
step:217/2245 train_time:25594ms step_avg:117.94ms
step:218/2245 train_time:25714ms step_avg:117.96ms
step:219/2245 train_time:25829ms step_avg:117.94ms
step:220/2245 train_time:25949ms step_avg:117.95ms
step:221/2245 train_time:26063ms step_avg:117.93ms
step:222/2245 train_time:26184ms step_avg:117.94ms
step:223/2245 train_time:26298ms step_avg:117.93ms
step:224/2245 train_time:26418ms step_avg:117.94ms
step:225/2245 train_time:26532ms step_avg:117.92ms
step:226/2245 train_time:26652ms step_avg:117.93ms
step:227/2245 train_time:26767ms step_avg:117.92ms
step:228/2245 train_time:26888ms step_avg:117.93ms
step:229/2245 train_time:27002ms step_avg:117.91ms
step:230/2245 train_time:27122ms step_avg:117.92ms
step:231/2245 train_time:27236ms step_avg:117.91ms
step:232/2245 train_time:27356ms step_avg:117.92ms
step:233/2245 train_time:27471ms step_avg:117.90ms
step:234/2245 train_time:27591ms step_avg:117.91ms
step:235/2245 train_time:27705ms step_avg:117.89ms
step:236/2245 train_time:27826ms step_avg:117.91ms
step:237/2245 train_time:27940ms step_avg:117.89ms
step:238/2245 train_time:28061ms step_avg:117.90ms
step:239/2245 train_time:28174ms step_avg:117.88ms
step:240/2245 train_time:28295ms step_avg:117.89ms
step:241/2245 train_time:28408ms step_avg:117.88ms
step:242/2245 train_time:28529ms step_avg:117.89ms
step:243/2245 train_time:28643ms step_avg:117.87ms
step:244/2245 train_time:28763ms step_avg:117.88ms
step:245/2245 train_time:28878ms step_avg:117.87ms
step:246/2245 train_time:28998ms step_avg:117.88ms
step:247/2245 train_time:29112ms step_avg:117.86ms
step:248/2245 train_time:29233ms step_avg:117.87ms
step:249/2245 train_time:29347ms step_avg:117.86ms
step:250/2245 train_time:29467ms step_avg:117.87ms
step:250/2245 val_loss:4.0934 train_time:29533ms step_avg:118.13ms
step:251/2245 train_time:29583ms step_avg:117.86ms
step:252/2245 train_time:29702ms step_avg:117.87ms
step:253/2245 train_time:29817ms step_avg:117.85ms
step:254/2245 train_time:29937ms step_avg:117.86ms
step:255/2245 train_time:30051ms step_avg:117.85ms
step:256/2245 train_time:30171ms step_avg:117.86ms
step:257/2245 train_time:30285ms step_avg:117.84ms
step:258/2245 train_time:30406ms step_avg:117.85ms
step:259/2245 train_time:30520ms step_avg:117.84ms
step:260/2245 train_time:30640ms step_avg:117.85ms
step:261/2245 train_time:30754ms step_avg:117.83ms
step:262/2245 train_time:30874ms step_avg:117.84ms
step:263/2245 train_time:30988ms step_avg:117.83ms
step:264/2245 train_time:31108ms step_avg:117.83ms
step:265/2245 train_time:31223ms step_avg:117.82ms
step:266/2245 train_time:31343ms step_avg:117.83ms
step:267/2245 train_time:31457ms step_avg:117.82ms
step:268/2245 train_time:31577ms step_avg:117.82ms
step:269/2245 train_time:31691ms step_avg:117.81ms
step:270/2245 train_time:31811ms step_avg:117.82ms
step:271/2245 train_time:31925ms step_avg:117.80ms
step:272/2245 train_time:32046ms step_avg:117.81ms
step:273/2245 train_time:32159ms step_avg:117.80ms
step:274/2245 train_time:32280ms step_avg:117.81ms
step:275/2245 train_time:32394ms step_avg:117.80ms
step:276/2245 train_time:32514ms step_avg:117.81ms
step:277/2245 train_time:32628ms step_avg:117.79ms
step:278/2245 train_time:32749ms step_avg:117.80ms
step:279/2245 train_time:32862ms step_avg:117.79ms
step:280/2245 train_time:32983ms step_avg:117.80ms
step:281/2245 train_time:33097ms step_avg:117.78ms
step:282/2245 train_time:33217ms step_avg:117.79ms
step:283/2245 train_time:33331ms step_avg:117.78ms
step:284/2245 train_time:33452ms step_avg:117.79ms
step:285/2245 train_time:33566ms step_avg:117.77ms
step:286/2245 train_time:33686ms step_avg:117.78ms
step:287/2245 train_time:33800ms step_avg:117.77ms
step:288/2245 train_time:33921ms step_avg:117.78ms
step:289/2245 train_time:34035ms step_avg:117.77ms
step:290/2245 train_time:34155ms step_avg:117.77ms
step:291/2245 train_time:34268ms step_avg:117.76ms
step:292/2245 train_time:34389ms step_avg:117.77ms
step:293/2245 train_time:34503ms step_avg:117.76ms
step:294/2245 train_time:34623ms step_avg:117.77ms
step:295/2245 train_time:34737ms step_avg:117.75ms
step:296/2245 train_time:34858ms step_avg:117.76ms
step:297/2245 train_time:34972ms step_avg:117.75ms
step:298/2245 train_time:35092ms step_avg:117.76ms
step:299/2245 train_time:35206ms step_avg:117.75ms
step:300/2245 train_time:35326ms step_avg:117.75ms
step:301/2245 train_time:35440ms step_avg:117.74ms
step:302/2245 train_time:35561ms step_avg:117.75ms
step:303/2245 train_time:35675ms step_avg:117.74ms
step:304/2245 train_time:35795ms step_avg:117.75ms
step:305/2245 train_time:35909ms step_avg:117.73ms
step:306/2245 train_time:36029ms step_avg:117.74ms
step:307/2245 train_time:36143ms step_avg:117.73ms
step:308/2245 train_time:36263ms step_avg:117.74ms
step:309/2245 train_time:36377ms step_avg:117.73ms
step:310/2245 train_time:36498ms step_avg:117.73ms
step:311/2245 train_time:36611ms step_avg:117.72ms
step:312/2245 train_time:36732ms step_avg:117.73ms
step:313/2245 train_time:36846ms step_avg:117.72ms
step:314/2245 train_time:36967ms step_avg:117.73ms
step:315/2245 train_time:37081ms step_avg:117.72ms
step:316/2245 train_time:37201ms step_avg:117.73ms
step:317/2245 train_time:37316ms step_avg:117.72ms
step:318/2245 train_time:37436ms step_avg:117.72ms
step:319/2245 train_time:37550ms step_avg:117.71ms
step:320/2245 train_time:37671ms step_avg:117.72ms
step:321/2245 train_time:37784ms step_avg:117.71ms
step:322/2245 train_time:37904ms step_avg:117.72ms
step:323/2245 train_time:38018ms step_avg:117.70ms
step:324/2245 train_time:38139ms step_avg:117.71ms
step:325/2245 train_time:38253ms step_avg:117.70ms
step:326/2245 train_time:38373ms step_avg:117.71ms
step:327/2245 train_time:38487ms step_avg:117.70ms
step:328/2245 train_time:38608ms step_avg:117.71ms
step:329/2245 train_time:38721ms step_avg:117.69ms
step:330/2245 train_time:38842ms step_avg:117.70ms
step:331/2245 train_time:38956ms step_avg:117.69ms
step:332/2245 train_time:39077ms step_avg:117.70ms
step:333/2245 train_time:39190ms step_avg:117.69ms
step:334/2245 train_time:39311ms step_avg:117.70ms
step:335/2245 train_time:39425ms step_avg:117.69ms
step:336/2245 train_time:39545ms step_avg:117.69ms
step:337/2245 train_time:39659ms step_avg:117.68ms
step:338/2245 train_time:39779ms step_avg:117.69ms
step:339/2245 train_time:39893ms step_avg:117.68ms
step:340/2245 train_time:40013ms step_avg:117.69ms
step:341/2245 train_time:40127ms step_avg:117.67ms
step:342/2245 train_time:40247ms step_avg:117.68ms
step:343/2245 train_time:40361ms step_avg:117.67ms
step:344/2245 train_time:40482ms step_avg:117.68ms
step:345/2245 train_time:40596ms step_avg:117.67ms
step:346/2245 train_time:40716ms step_avg:117.68ms
step:347/2245 train_time:40830ms step_avg:117.67ms
step:348/2245 train_time:40950ms step_avg:117.67ms
step:349/2245 train_time:41065ms step_avg:117.66ms
step:350/2245 train_time:41185ms step_avg:117.67ms
step:351/2245 train_time:41299ms step_avg:117.66ms
step:352/2245 train_time:41419ms step_avg:117.67ms
step:353/2245 train_time:41533ms step_avg:117.66ms
step:354/2245 train_time:41653ms step_avg:117.66ms
step:355/2245 train_time:41767ms step_avg:117.65ms
step:356/2245 train_time:41887ms step_avg:117.66ms
step:357/2245 train_time:42001ms step_avg:117.65ms
step:358/2245 train_time:42121ms step_avg:117.66ms
step:359/2245 train_time:42236ms step_avg:117.65ms
step:360/2245 train_time:42355ms step_avg:117.65ms
step:361/2245 train_time:42469ms step_avg:117.64ms
step:362/2245 train_time:42590ms step_avg:117.65ms
step:363/2245 train_time:42704ms step_avg:117.64ms
step:364/2245 train_time:42824ms step_avg:117.65ms
step:365/2245 train_time:42938ms step_avg:117.64ms
step:366/2245 train_time:43059ms step_avg:117.65ms
step:367/2245 train_time:43173ms step_avg:117.64ms
step:368/2245 train_time:43293ms step_avg:117.64ms
step:369/2245 train_time:43407ms step_avg:117.63ms
step:370/2245 train_time:43527ms step_avg:117.64ms
step:371/2245 train_time:43641ms step_avg:117.63ms
step:372/2245 train_time:43762ms step_avg:117.64ms
step:373/2245 train_time:43876ms step_avg:117.63ms
step:374/2245 train_time:43996ms step_avg:117.64ms
step:375/2245 train_time:44110ms step_avg:117.63ms
step:376/2245 train_time:44230ms step_avg:117.63ms
step:377/2245 train_time:44344ms step_avg:117.62ms
step:378/2245 train_time:44464ms step_avg:117.63ms
step:379/2245 train_time:44578ms step_avg:117.62ms
step:380/2245 train_time:44699ms step_avg:117.63ms
step:381/2245 train_time:44813ms step_avg:117.62ms
step:382/2245 train_time:44933ms step_avg:117.62ms
step:383/2245 train_time:45047ms step_avg:117.62ms
step:384/2245 train_time:45167ms step_avg:117.62ms
step:385/2245 train_time:45281ms step_avg:117.61ms
step:386/2245 train_time:45401ms step_avg:117.62ms
step:387/2245 train_time:45516ms step_avg:117.61ms
step:388/2245 train_time:45636ms step_avg:117.62ms
step:389/2245 train_time:45750ms step_avg:117.61ms
step:390/2245 train_time:45870ms step_avg:117.62ms
step:391/2245 train_time:45984ms step_avg:117.61ms
step:392/2245 train_time:46105ms step_avg:117.61ms
step:393/2245 train_time:46219ms step_avg:117.61ms
step:394/2245 train_time:46339ms step_avg:117.61ms
step:395/2245 train_time:46453ms step_avg:117.60ms
step:396/2245 train_time:46573ms step_avg:117.61ms
step:397/2245 train_time:46687ms step_avg:117.60ms
step:398/2245 train_time:46808ms step_avg:117.61ms
step:399/2245 train_time:46921ms step_avg:117.60ms
step:400/2245 train_time:47042ms step_avg:117.60ms
step:401/2245 train_time:47156ms step_avg:117.60ms
step:402/2245 train_time:47277ms step_avg:117.60ms
step:403/2245 train_time:47390ms step_avg:117.59ms
step:404/2245 train_time:47511ms step_avg:117.60ms
step:405/2245 train_time:47625ms step_avg:117.59ms
step:406/2245 train_time:47746ms step_avg:117.60ms
step:407/2245 train_time:47860ms step_avg:117.59ms
step:408/2245 train_time:47980ms step_avg:117.60ms
step:409/2245 train_time:48094ms step_avg:117.59ms
step:410/2245 train_time:48214ms step_avg:117.60ms
step:411/2245 train_time:48328ms step_avg:117.59ms
step:412/2245 train_time:48448ms step_avg:117.59ms
step:413/2245 train_time:48562ms step_avg:117.58ms
step:414/2245 train_time:48683ms step_avg:117.59ms
step:415/2245 train_time:48796ms step_avg:117.58ms
step:416/2245 train_time:48916ms step_avg:117.59ms
step:417/2245 train_time:49030ms step_avg:117.58ms
step:418/2245 train_time:49151ms step_avg:117.59ms
step:419/2245 train_time:49265ms step_avg:117.58ms
step:420/2245 train_time:49385ms step_avg:117.58ms
step:421/2245 train_time:49499ms step_avg:117.57ms
step:422/2245 train_time:49619ms step_avg:117.58ms
step:423/2245 train_time:49733ms step_avg:117.57ms
step:424/2245 train_time:49853ms step_avg:117.58ms
step:425/2245 train_time:49967ms step_avg:117.57ms
step:426/2245 train_time:50087ms step_avg:117.58ms
step:427/2245 train_time:50202ms step_avg:117.57ms
step:428/2245 train_time:50322ms step_avg:117.57ms
step:429/2245 train_time:50435ms step_avg:117.56ms
step:430/2245 train_time:50555ms step_avg:117.57ms
step:431/2245 train_time:50669ms step_avg:117.56ms
step:432/2245 train_time:50789ms step_avg:117.57ms
step:433/2245 train_time:50904ms step_avg:117.56ms
step:434/2245 train_time:51024ms step_avg:117.57ms
step:435/2245 train_time:51138ms step_avg:117.56ms
step:436/2245 train_time:51258ms step_avg:117.56ms
step:437/2245 train_time:51372ms step_avg:117.56ms
step:438/2245 train_time:51492ms step_avg:117.56ms
step:439/2245 train_time:51606ms step_avg:117.55ms
step:440/2245 train_time:51726ms step_avg:117.56ms
step:441/2245 train_time:51840ms step_avg:117.55ms
step:442/2245 train_time:51960ms step_avg:117.56ms
step:443/2245 train_time:52074ms step_avg:117.55ms
step:444/2245 train_time:52194ms step_avg:117.56ms
step:445/2245 train_time:52308ms step_avg:117.55ms
step:446/2245 train_time:52429ms step_avg:117.55ms
step:447/2245 train_time:52543ms step_avg:117.55ms
step:448/2245 train_time:52663ms step_avg:117.55ms
step:449/2245 train_time:52778ms step_avg:117.55ms
step:450/2245 train_time:52898ms step_avg:117.55ms
step:451/2245 train_time:53012ms step_avg:117.54ms
step:452/2245 train_time:53132ms step_avg:117.55ms
step:453/2245 train_time:53246ms step_avg:117.54ms
step:454/2245 train_time:53367ms step_avg:117.55ms
step:455/2245 train_time:53481ms step_avg:117.54ms
step:456/2245 train_time:53601ms step_avg:117.55ms
step:457/2245 train_time:53715ms step_avg:117.54ms
step:458/2245 train_time:53835ms step_avg:117.54ms
step:459/2245 train_time:53949ms step_avg:117.53ms
step:460/2245 train_time:54069ms step_avg:117.54ms
step:461/2245 train_time:54183ms step_avg:117.53ms
step:462/2245 train_time:54303ms step_avg:117.54ms
step:463/2245 train_time:54417ms step_avg:117.53ms
step:464/2245 train_time:54538ms step_avg:117.54ms
step:465/2245 train_time:54652ms step_avg:117.53ms
step:466/2245 train_time:54771ms step_avg:117.54ms
step:467/2245 train_time:54886ms step_avg:117.53ms
step:468/2245 train_time:55006ms step_avg:117.54ms
step:469/2245 train_time:55120ms step_avg:117.53ms
step:470/2245 train_time:55240ms step_avg:117.53ms
step:471/2245 train_time:55354ms step_avg:117.52ms
step:472/2245 train_time:55474ms step_avg:117.53ms
step:473/2245 train_time:55588ms step_avg:117.52ms
step:474/2245 train_time:55708ms step_avg:117.53ms
step:475/2245 train_time:55822ms step_avg:117.52ms
step:476/2245 train_time:55943ms step_avg:117.53ms
step:477/2245 train_time:56057ms step_avg:117.52ms
step:478/2245 train_time:56178ms step_avg:117.53ms
step:479/2245 train_time:56292ms step_avg:117.52ms
step:480/2245 train_time:56412ms step_avg:117.52ms
step:481/2245 train_time:56526ms step_avg:117.52ms
step:482/2245 train_time:56646ms step_avg:117.52ms
step:483/2245 train_time:56760ms step_avg:117.52ms
step:484/2245 train_time:56880ms step_avg:117.52ms
step:485/2245 train_time:56994ms step_avg:117.51ms
step:486/2245 train_time:57114ms step_avg:117.52ms
step:487/2245 train_time:57229ms step_avg:117.51ms
step:488/2245 train_time:57349ms step_avg:117.52ms
step:489/2245 train_time:57463ms step_avg:117.51ms
step:490/2245 train_time:57583ms step_avg:117.52ms
step:491/2245 train_time:57697ms step_avg:117.51ms
step:492/2245 train_time:57817ms step_avg:117.51ms
step:493/2245 train_time:57931ms step_avg:117.51ms
step:494/2245 train_time:58051ms step_avg:117.51ms
step:495/2245 train_time:58165ms step_avg:117.51ms
step:496/2245 train_time:58286ms step_avg:117.51ms
step:497/2245 train_time:58400ms step_avg:117.51ms
step:498/2245 train_time:58520ms step_avg:117.51ms
step:499/2245 train_time:58634ms step_avg:117.50ms
step:500/2245 train_time:58754ms step_avg:117.51ms
step:500/2245 val_loss:3.8245 train_time:58819ms step_avg:117.64ms
step:501/2245 train_time:58869ms step_avg:117.50ms
step:502/2245 train_time:58988ms step_avg:117.51ms
step:503/2245 train_time:59102ms step_avg:117.50ms
step:504/2245 train_time:59223ms step_avg:117.51ms
step:505/2245 train_time:59336ms step_avg:117.50ms
step:506/2245 train_time:59457ms step_avg:117.50ms
step:507/2245 train_time:59570ms step_avg:117.50ms
step:508/2245 train_time:59691ms step_avg:117.50ms
step:509/2245 train_time:59804ms step_avg:117.49ms
step:510/2245 train_time:59924ms step_avg:117.50ms
step:511/2245 train_time:60038ms step_avg:117.49ms
step:512/2245 train_time:60158ms step_avg:117.50ms
step:513/2245 train_time:60272ms step_avg:117.49ms
step:514/2245 train_time:60393ms step_avg:117.50ms
step:515/2245 train_time:60507ms step_avg:117.49ms
step:516/2245 train_time:60627ms step_avg:117.49ms
step:517/2245 train_time:60741ms step_avg:117.49ms
step:518/2245 train_time:60861ms step_avg:117.49ms
step:519/2245 train_time:60975ms step_avg:117.49ms
step:520/2245 train_time:61096ms step_avg:117.49ms
step:521/2245 train_time:61209ms step_avg:117.48ms
step:522/2245 train_time:61329ms step_avg:117.49ms
step:523/2245 train_time:61443ms step_avg:117.48ms
step:524/2245 train_time:61564ms step_avg:117.49ms
step:525/2245 train_time:61678ms step_avg:117.48ms
step:526/2245 train_time:61799ms step_avg:117.49ms
step:527/2245 train_time:61913ms step_avg:117.48ms
step:528/2245 train_time:62033ms step_avg:117.49ms
step:529/2245 train_time:62148ms step_avg:117.48ms
step:530/2245 train_time:62268ms step_avg:117.49ms
step:531/2245 train_time:62382ms step_avg:117.48ms
step:532/2245 train_time:62502ms step_avg:117.49ms
step:533/2245 train_time:62616ms step_avg:117.48ms
step:534/2245 train_time:62736ms step_avg:117.48ms
step:535/2245 train_time:62851ms step_avg:117.48ms
step:536/2245 train_time:62971ms step_avg:117.48ms
step:537/2245 train_time:63085ms step_avg:117.48ms
step:538/2245 train_time:63205ms step_avg:117.48ms
step:539/2245 train_time:63320ms step_avg:117.48ms
step:540/2245 train_time:63440ms step_avg:117.48ms
step:541/2245 train_time:63554ms step_avg:117.48ms
step:542/2245 train_time:63674ms step_avg:117.48ms
step:543/2245 train_time:63788ms step_avg:117.47ms
step:544/2245 train_time:63908ms step_avg:117.48ms
step:545/2245 train_time:64022ms step_avg:117.47ms
step:546/2245 train_time:64143ms step_avg:117.48ms
step:547/2245 train_time:64256ms step_avg:117.47ms
step:548/2245 train_time:64377ms step_avg:117.48ms
step:549/2245 train_time:64491ms step_avg:117.47ms
step:550/2245 train_time:64611ms step_avg:117.47ms
step:551/2245 train_time:64725ms step_avg:117.47ms
step:552/2245 train_time:64845ms step_avg:117.47ms
step:553/2245 train_time:64959ms step_avg:117.47ms
step:554/2245 train_time:65080ms step_avg:117.47ms
step:555/2245 train_time:65194ms step_avg:117.47ms
step:556/2245 train_time:65314ms step_avg:117.47ms
step:557/2245 train_time:65428ms step_avg:117.46ms
step:558/2245 train_time:65548ms step_avg:117.47ms
step:559/2245 train_time:65662ms step_avg:117.46ms
step:560/2245 train_time:65782ms step_avg:117.47ms
step:561/2245 train_time:65896ms step_avg:117.46ms
step:562/2245 train_time:66016ms step_avg:117.47ms
step:563/2245 train_time:66130ms step_avg:117.46ms
step:564/2245 train_time:66251ms step_avg:117.47ms
step:565/2245 train_time:66365ms step_avg:117.46ms
step:566/2245 train_time:66485ms step_avg:117.46ms
step:567/2245 train_time:66599ms step_avg:117.46ms
step:568/2245 train_time:66719ms step_avg:117.46ms
step:569/2245 train_time:66833ms step_avg:117.46ms
step:570/2245 train_time:66953ms step_avg:117.46ms
step:571/2245 train_time:67068ms step_avg:117.46ms
step:572/2245 train_time:67188ms step_avg:117.46ms
step:573/2245 train_time:67302ms step_avg:117.46ms
step:574/2245 train_time:67422ms step_avg:117.46ms
step:575/2245 train_time:67536ms step_avg:117.45ms
step:576/2245 train_time:67657ms step_avg:117.46ms
step:577/2245 train_time:67771ms step_avg:117.45ms
step:578/2245 train_time:67891ms step_avg:117.46ms
step:579/2245 train_time:68005ms step_avg:117.45ms
step:580/2245 train_time:68125ms step_avg:117.46ms
step:581/2245 train_time:68239ms step_avg:117.45ms
step:582/2245 train_time:68359ms step_avg:117.46ms
step:583/2245 train_time:68473ms step_avg:117.45ms
step:584/2245 train_time:68593ms step_avg:117.45ms
step:585/2245 train_time:68708ms step_avg:117.45ms
step:586/2245 train_time:68827ms step_avg:117.45ms
step:587/2245 train_time:68942ms step_avg:117.45ms
step:588/2245 train_time:69062ms step_avg:117.45ms
step:589/2245 train_time:69176ms step_avg:117.45ms
step:590/2245 train_time:69297ms step_avg:117.45ms
step:591/2245 train_time:69411ms step_avg:117.45ms
step:592/2245 train_time:69531ms step_avg:117.45ms
step:593/2245 train_time:69645ms step_avg:117.44ms
step:594/2245 train_time:69765ms step_avg:117.45ms
step:595/2245 train_time:69879ms step_avg:117.44ms
step:596/2245 train_time:70000ms step_avg:117.45ms
step:597/2245 train_time:70114ms step_avg:117.44ms
step:598/2245 train_time:70234ms step_avg:117.45ms
step:599/2245 train_time:70349ms step_avg:117.44ms
step:600/2245 train_time:70470ms step_avg:117.45ms
step:601/2245 train_time:70584ms step_avg:117.44ms
step:602/2245 train_time:70704ms step_avg:117.45ms
step:603/2245 train_time:70818ms step_avg:117.44ms
step:604/2245 train_time:70939ms step_avg:117.45ms
step:605/2245 train_time:71053ms step_avg:117.44ms
step:606/2245 train_time:71173ms step_avg:117.45ms
step:607/2245 train_time:71287ms step_avg:117.44ms
step:608/2245 train_time:71408ms step_avg:117.45ms
step:609/2245 train_time:71521ms step_avg:117.44ms
step:610/2245 train_time:71642ms step_avg:117.45ms
step:611/2245 train_time:71756ms step_avg:117.44ms
step:612/2245 train_time:71876ms step_avg:117.44ms
step:613/2245 train_time:71990ms step_avg:117.44ms
step:614/2245 train_time:72111ms step_avg:117.44ms
step:615/2245 train_time:72225ms step_avg:117.44ms
step:616/2245 train_time:72345ms step_avg:117.44ms
step:617/2245 train_time:72459ms step_avg:117.44ms
step:618/2245 train_time:72579ms step_avg:117.44ms
step:619/2245 train_time:72693ms step_avg:117.44ms
step:620/2245 train_time:72814ms step_avg:117.44ms
step:621/2245 train_time:72928ms step_avg:117.44ms
step:622/2245 train_time:73048ms step_avg:117.44ms
step:623/2245 train_time:73162ms step_avg:117.43ms
step:624/2245 train_time:73282ms step_avg:117.44ms
step:625/2245 train_time:73396ms step_avg:117.43ms
step:626/2245 train_time:73517ms step_avg:117.44ms
step:627/2245 train_time:73631ms step_avg:117.43ms
step:628/2245 train_time:73751ms step_avg:117.44ms
step:629/2245 train_time:73865ms step_avg:117.43ms
step:630/2245 train_time:73985ms step_avg:117.44ms
step:631/2245 train_time:74099ms step_avg:117.43ms
step:632/2245 train_time:74219ms step_avg:117.44ms
step:633/2245 train_time:74334ms step_avg:117.43ms
step:634/2245 train_time:74454ms step_avg:117.44ms
step:635/2245 train_time:74568ms step_avg:117.43ms
step:636/2245 train_time:74688ms step_avg:117.43ms
step:637/2245 train_time:74802ms step_avg:117.43ms
step:638/2245 train_time:74922ms step_avg:117.43ms
step:639/2245 train_time:75037ms step_avg:117.43ms
step:640/2245 train_time:75157ms step_avg:117.43ms
step:641/2245 train_time:75271ms step_avg:117.43ms
step:642/2245 train_time:75392ms step_avg:117.43ms
step:643/2245 train_time:75506ms step_avg:117.43ms
step:644/2245 train_time:75626ms step_avg:117.43ms
step:645/2245 train_time:75740ms step_avg:117.43ms
step:646/2245 train_time:75861ms step_avg:117.43ms
step:647/2245 train_time:75974ms step_avg:117.43ms
step:648/2245 train_time:76095ms step_avg:117.43ms
step:649/2245 train_time:76209ms step_avg:117.43ms
step:650/2245 train_time:76329ms step_avg:117.43ms
step:651/2245 train_time:76443ms step_avg:117.42ms
step:652/2245 train_time:76563ms step_avg:117.43ms
step:653/2245 train_time:76678ms step_avg:117.42ms
step:654/2245 train_time:76798ms step_avg:117.43ms
step:655/2245 train_time:76912ms step_avg:117.42ms
step:656/2245 train_time:77033ms step_avg:117.43ms
step:657/2245 train_time:77147ms step_avg:117.42ms
step:658/2245 train_time:77267ms step_avg:117.43ms
step:659/2245 train_time:77381ms step_avg:117.42ms
step:660/2245 train_time:77501ms step_avg:117.43ms
step:661/2245 train_time:77615ms step_avg:117.42ms
step:662/2245 train_time:77736ms step_avg:117.43ms
step:663/2245 train_time:77849ms step_avg:117.42ms
step:664/2245 train_time:77970ms step_avg:117.42ms
step:665/2245 train_time:78084ms step_avg:117.42ms
step:666/2245 train_time:78204ms step_avg:117.42ms
step:667/2245 train_time:78318ms step_avg:117.42ms
step:668/2245 train_time:78439ms step_avg:117.42ms
step:669/2245 train_time:78553ms step_avg:117.42ms
step:670/2245 train_time:78673ms step_avg:117.42ms
step:671/2245 train_time:78787ms step_avg:117.42ms
step:672/2245 train_time:78907ms step_avg:117.42ms
step:673/2245 train_time:79021ms step_avg:117.42ms
step:674/2245 train_time:79142ms step_avg:117.42ms
step:675/2245 train_time:79256ms step_avg:117.42ms
step:676/2245 train_time:79376ms step_avg:117.42ms
step:677/2245 train_time:79490ms step_avg:117.42ms
step:678/2245 train_time:79611ms step_avg:117.42ms
step:679/2245 train_time:79725ms step_avg:117.42ms
step:680/2245 train_time:79845ms step_avg:117.42ms
step:681/2245 train_time:79959ms step_avg:117.41ms
step:682/2245 train_time:80079ms step_avg:117.42ms
step:683/2245 train_time:80193ms step_avg:117.41ms
step:684/2245 train_time:80313ms step_avg:117.42ms
step:685/2245 train_time:80427ms step_avg:117.41ms
step:686/2245 train_time:80547ms step_avg:117.42ms
step:687/2245 train_time:80661ms step_avg:117.41ms
step:688/2245 train_time:80781ms step_avg:117.41ms
step:689/2245 train_time:80895ms step_avg:117.41ms
step:690/2245 train_time:81015ms step_avg:117.41ms
step:691/2245 train_time:81129ms step_avg:117.41ms
step:692/2245 train_time:81250ms step_avg:117.41ms
step:693/2245 train_time:81363ms step_avg:117.41ms
step:694/2245 train_time:81484ms step_avg:117.41ms
step:695/2245 train_time:81598ms step_avg:117.41ms
step:696/2245 train_time:81718ms step_avg:117.41ms
step:697/2245 train_time:81832ms step_avg:117.41ms
step:698/2245 train_time:81953ms step_avg:117.41ms
step:699/2245 train_time:82067ms step_avg:117.41ms
step:700/2245 train_time:82187ms step_avg:117.41ms
step:701/2245 train_time:82301ms step_avg:117.40ms
step:702/2245 train_time:82421ms step_avg:117.41ms
step:703/2245 train_time:82535ms step_avg:117.40ms
step:704/2245 train_time:82655ms step_avg:117.41ms
step:705/2245 train_time:82770ms step_avg:117.40ms
step:706/2245 train_time:82890ms step_avg:117.41ms
step:707/2245 train_time:83003ms step_avg:117.40ms
step:708/2245 train_time:83123ms step_avg:117.41ms
step:709/2245 train_time:83238ms step_avg:117.40ms
step:710/2245 train_time:83358ms step_avg:117.41ms
step:711/2245 train_time:83472ms step_avg:117.40ms
step:712/2245 train_time:83592ms step_avg:117.40ms
step:713/2245 train_time:83707ms step_avg:117.40ms
step:714/2245 train_time:83827ms step_avg:117.40ms
step:715/2245 train_time:83941ms step_avg:117.40ms
step:716/2245 train_time:84062ms step_avg:117.41ms
step:717/2245 train_time:84176ms step_avg:117.40ms
step:718/2245 train_time:84296ms step_avg:117.40ms
step:719/2245 train_time:84410ms step_avg:117.40ms
step:720/2245 train_time:84530ms step_avg:117.40ms
step:721/2245 train_time:84644ms step_avg:117.40ms
step:722/2245 train_time:84764ms step_avg:117.40ms
step:723/2245 train_time:84878ms step_avg:117.40ms
step:724/2245 train_time:84999ms step_avg:117.40ms
step:725/2245 train_time:85112ms step_avg:117.40ms
step:726/2245 train_time:85233ms step_avg:117.40ms
step:727/2245 train_time:85347ms step_avg:117.40ms
step:728/2245 train_time:85467ms step_avg:117.40ms
step:729/2245 train_time:85581ms step_avg:117.40ms
step:730/2245 train_time:85702ms step_avg:117.40ms
step:731/2245 train_time:85815ms step_avg:117.39ms
step:732/2245 train_time:85936ms step_avg:117.40ms
step:733/2245 train_time:86050ms step_avg:117.39ms
step:734/2245 train_time:86171ms step_avg:117.40ms
step:735/2245 train_time:86285ms step_avg:117.39ms
step:736/2245 train_time:86406ms step_avg:117.40ms
step:737/2245 train_time:86521ms step_avg:117.40ms
step:738/2245 train_time:86643ms step_avg:117.40ms
step:739/2245 train_time:86758ms step_avg:117.40ms
step:740/2245 train_time:86880ms step_avg:117.40ms
step:741/2245 train_time:86995ms step_avg:117.40ms
step:742/2245 train_time:87117ms step_avg:117.41ms
step:743/2245 train_time:87233ms step_avg:117.41ms
step:744/2245 train_time:87355ms step_avg:117.41ms
step:745/2245 train_time:87471ms step_avg:117.41ms
step:746/2245 train_time:87593ms step_avg:117.42ms
step:747/2245 train_time:87709ms step_avg:117.42ms
step:748/2245 train_time:87831ms step_avg:117.42ms
step:749/2245 train_time:87947ms step_avg:117.42ms
step:750/2245 train_time:88069ms step_avg:117.43ms
step:750/2245 val_loss:3.6730 train_time:88135ms step_avg:117.51ms
step:751/2245 train_time:88185ms step_avg:117.42ms
step:752/2245 train_time:88306ms step_avg:117.43ms
step:753/2245 train_time:88421ms step_avg:117.42ms
step:754/2245 train_time:88543ms step_avg:117.43ms
step:755/2245 train_time:88658ms step_avg:117.43ms
step:756/2245 train_time:88780ms step_avg:117.43ms
step:757/2245 train_time:88895ms step_avg:117.43ms
step:758/2245 train_time:89017ms step_avg:117.44ms
step:759/2245 train_time:89133ms step_avg:117.43ms
step:760/2245 train_time:89255ms step_avg:117.44ms
step:761/2245 train_time:89371ms step_avg:117.44ms
step:762/2245 train_time:89494ms step_avg:117.45ms
step:763/2245 train_time:89609ms step_avg:117.44ms
step:764/2245 train_time:89732ms step_avg:117.45ms
step:765/2245 train_time:89847ms step_avg:117.45ms
step:766/2245 train_time:89969ms step_avg:117.45ms
step:767/2245 train_time:90084ms step_avg:117.45ms
step:768/2245 train_time:90206ms step_avg:117.46ms
step:769/2245 train_time:90322ms step_avg:117.45ms
step:770/2245 train_time:90444ms step_avg:117.46ms
step:771/2245 train_time:90560ms step_avg:117.46ms
step:772/2245 train_time:90682ms step_avg:117.46ms
step:773/2245 train_time:90797ms step_avg:117.46ms
step:774/2245 train_time:90919ms step_avg:117.47ms
step:775/2245 train_time:91035ms step_avg:117.46ms
step:776/2245 train_time:91157ms step_avg:117.47ms
step:777/2245 train_time:91273ms step_avg:117.47ms
step:778/2245 train_time:91395ms step_avg:117.47ms
step:779/2245 train_time:91511ms step_avg:117.47ms
step:780/2245 train_time:91633ms step_avg:117.48ms
step:781/2245 train_time:91749ms step_avg:117.48ms
step:782/2245 train_time:91871ms step_avg:117.48ms
step:783/2245 train_time:91987ms step_avg:117.48ms
step:784/2245 train_time:92108ms step_avg:117.49ms
step:785/2245 train_time:92224ms step_avg:117.48ms
step:786/2245 train_time:92346ms step_avg:117.49ms
step:787/2245 train_time:92461ms step_avg:117.49ms
step:788/2245 train_time:92583ms step_avg:117.49ms
step:789/2245 train_time:92698ms step_avg:117.49ms
step:790/2245 train_time:92820ms step_avg:117.49ms
step:791/2245 train_time:92936ms step_avg:117.49ms
step:792/2245 train_time:93057ms step_avg:117.50ms
step:793/2245 train_time:93173ms step_avg:117.49ms
step:794/2245 train_time:93294ms step_avg:117.50ms
step:795/2245 train_time:93410ms step_avg:117.50ms
step:796/2245 train_time:93532ms step_avg:117.50ms
step:797/2245 train_time:93647ms step_avg:117.50ms
step:798/2245 train_time:93769ms step_avg:117.51ms
step:799/2245 train_time:93884ms step_avg:117.50ms
step:800/2245 train_time:94006ms step_avg:117.51ms
step:801/2245 train_time:94122ms step_avg:117.51ms
step:802/2245 train_time:94245ms step_avg:117.51ms
step:803/2245 train_time:94360ms step_avg:117.51ms
step:804/2245 train_time:94482ms step_avg:117.51ms
step:805/2245 train_time:94597ms step_avg:117.51ms
step:806/2245 train_time:94720ms step_avg:117.52ms
step:807/2245 train_time:94836ms step_avg:117.52ms
step:808/2245 train_time:94957ms step_avg:117.52ms
step:809/2245 train_time:95073ms step_avg:117.52ms
step:810/2245 train_time:95195ms step_avg:117.52ms
step:811/2245 train_time:95312ms step_avg:117.52ms
step:812/2245 train_time:95435ms step_avg:117.53ms
step:813/2245 train_time:95550ms step_avg:117.53ms
step:814/2245 train_time:95672ms step_avg:117.53ms
step:815/2245 train_time:95788ms step_avg:117.53ms
step:816/2245 train_time:95909ms step_avg:117.54ms
step:817/2245 train_time:96025ms step_avg:117.53ms
step:818/2245 train_time:96147ms step_avg:117.54ms
step:819/2245 train_time:96262ms step_avg:117.54ms
step:820/2245 train_time:96384ms step_avg:117.54ms
step:821/2245 train_time:96500ms step_avg:117.54ms
step:822/2245 train_time:96622ms step_avg:117.54ms
step:823/2245 train_time:96737ms step_avg:117.54ms
step:824/2245 train_time:96859ms step_avg:117.55ms
step:825/2245 train_time:96974ms step_avg:117.54ms
step:826/2245 train_time:97096ms step_avg:117.55ms
step:827/2245 train_time:97212ms step_avg:117.55ms
step:828/2245 train_time:97333ms step_avg:117.55ms
step:829/2245 train_time:97449ms step_avg:117.55ms
step:830/2245 train_time:97571ms step_avg:117.56ms
step:831/2245 train_time:97686ms step_avg:117.55ms
step:832/2245 train_time:97808ms step_avg:117.56ms
step:833/2245 train_time:97923ms step_avg:117.55ms
step:834/2245 train_time:98045ms step_avg:117.56ms
step:835/2245 train_time:98161ms step_avg:117.56ms
step:836/2245 train_time:98283ms step_avg:117.56ms
step:837/2245 train_time:98398ms step_avg:117.56ms
step:838/2245 train_time:98520ms step_avg:117.57ms
step:839/2245 train_time:98636ms step_avg:117.56ms
step:840/2245 train_time:98758ms step_avg:117.57ms
step:841/2245 train_time:98874ms step_avg:117.57ms
step:842/2245 train_time:98995ms step_avg:117.57ms
step:843/2245 train_time:99111ms step_avg:117.57ms
step:844/2245 train_time:99233ms step_avg:117.57ms
step:845/2245 train_time:99349ms step_avg:117.57ms
step:846/2245 train_time:99471ms step_avg:117.58ms
step:847/2245 train_time:99586ms step_avg:117.58ms
step:848/2245 train_time:99709ms step_avg:117.58ms
step:849/2245 train_time:99824ms step_avg:117.58ms
step:850/2245 train_time:99946ms step_avg:117.58ms
step:851/2245 train_time:100062ms step_avg:117.58ms
step:852/2245 train_time:100184ms step_avg:117.59ms
step:853/2245 train_time:100299ms step_avg:117.58ms
step:854/2245 train_time:100421ms step_avg:117.59ms
step:855/2245 train_time:100536ms step_avg:117.59ms
step:856/2245 train_time:100658ms step_avg:117.59ms
step:857/2245 train_time:100774ms step_avg:117.59ms
step:858/2245 train_time:100896ms step_avg:117.59ms
step:859/2245 train_time:101012ms step_avg:117.59ms
step:860/2245 train_time:101134ms step_avg:117.60ms
step:861/2245 train_time:101251ms step_avg:117.60ms
step:862/2245 train_time:101372ms step_avg:117.60ms
step:863/2245 train_time:101487ms step_avg:117.60ms
step:864/2245 train_time:101609ms step_avg:117.60ms
step:865/2245 train_time:101724ms step_avg:117.60ms
step:866/2245 train_time:101846ms step_avg:117.61ms
step:867/2245 train_time:101962ms step_avg:117.60ms
step:868/2245 train_time:102084ms step_avg:117.61ms
step:869/2245 train_time:102200ms step_avg:117.61ms
step:870/2245 train_time:102322ms step_avg:117.61ms
step:871/2245 train_time:102437ms step_avg:117.61ms
step:872/2245 train_time:102559ms step_avg:117.61ms
step:873/2245 train_time:102675ms step_avg:117.61ms
step:874/2245 train_time:102797ms step_avg:117.62ms
step:875/2245 train_time:102913ms step_avg:117.61ms
step:876/2245 train_time:103035ms step_avg:117.62ms
step:877/2245 train_time:103151ms step_avg:117.62ms
step:878/2245 train_time:103273ms step_avg:117.62ms
step:879/2245 train_time:103389ms step_avg:117.62ms
step:880/2245 train_time:103511ms step_avg:117.63ms
step:881/2245 train_time:103626ms step_avg:117.62ms
step:882/2245 train_time:103748ms step_avg:117.63ms
step:883/2245 train_time:103864ms step_avg:117.63ms
step:884/2245 train_time:103986ms step_avg:117.63ms
step:885/2245 train_time:104102ms step_avg:117.63ms
step:886/2245 train_time:104224ms step_avg:117.63ms
step:887/2245 train_time:104339ms step_avg:117.63ms
step:888/2245 train_time:104461ms step_avg:117.64ms
step:889/2245 train_time:104577ms step_avg:117.63ms
step:890/2245 train_time:104699ms step_avg:117.64ms
step:891/2245 train_time:104815ms step_avg:117.64ms
step:892/2245 train_time:104936ms step_avg:117.64ms
step:893/2245 train_time:105052ms step_avg:117.64ms
step:894/2245 train_time:105175ms step_avg:117.65ms
step:895/2245 train_time:105292ms step_avg:117.64ms
step:896/2245 train_time:105414ms step_avg:117.65ms
step:897/2245 train_time:105530ms step_avg:117.65ms
step:898/2245 train_time:105651ms step_avg:117.65ms
step:899/2245 train_time:105767ms step_avg:117.65ms
step:900/2245 train_time:105888ms step_avg:117.65ms
step:901/2245 train_time:106004ms step_avg:117.65ms
step:902/2245 train_time:106126ms step_avg:117.66ms
step:903/2245 train_time:106242ms step_avg:117.65ms
step:904/2245 train_time:106364ms step_avg:117.66ms
step:905/2245 train_time:106479ms step_avg:117.66ms
step:906/2245 train_time:106601ms step_avg:117.66ms
step:907/2245 train_time:106716ms step_avg:117.66ms
step:908/2245 train_time:106839ms step_avg:117.66ms
step:909/2245 train_time:106955ms step_avg:117.66ms
step:910/2245 train_time:107076ms step_avg:117.67ms
step:911/2245 train_time:107192ms step_avg:117.66ms
step:912/2245 train_time:107315ms step_avg:117.67ms
step:913/2245 train_time:107430ms step_avg:117.67ms
step:914/2245 train_time:107553ms step_avg:117.67ms
step:915/2245 train_time:107668ms step_avg:117.67ms
step:916/2245 train_time:107790ms step_avg:117.67ms
step:917/2245 train_time:107905ms step_avg:117.67ms
step:918/2245 train_time:108027ms step_avg:117.68ms
step:919/2245 train_time:108142ms step_avg:117.67ms
step:920/2245 train_time:108264ms step_avg:117.68ms
step:921/2245 train_time:108380ms step_avg:117.68ms
step:922/2245 train_time:108501ms step_avg:117.68ms
step:923/2245 train_time:108617ms step_avg:117.68ms
step:924/2245 train_time:108739ms step_avg:117.68ms
step:925/2245 train_time:108855ms step_avg:117.68ms
step:926/2245 train_time:108976ms step_avg:117.69ms
step:927/2245 train_time:109092ms step_avg:117.68ms
step:928/2245 train_time:109214ms step_avg:117.69ms
step:929/2245 train_time:109329ms step_avg:117.69ms
step:930/2245 train_time:109451ms step_avg:117.69ms
step:931/2245 train_time:109567ms step_avg:117.69ms
step:932/2245 train_time:109688ms step_avg:117.69ms
step:933/2245 train_time:109804ms step_avg:117.69ms
step:934/2245 train_time:109927ms step_avg:117.69ms
step:935/2245 train_time:110042ms step_avg:117.69ms
step:936/2245 train_time:110164ms step_avg:117.70ms
step:937/2245 train_time:110280ms step_avg:117.69ms
step:938/2245 train_time:110402ms step_avg:117.70ms
step:939/2245 train_time:110517ms step_avg:117.70ms
step:940/2245 train_time:110639ms step_avg:117.70ms
step:941/2245 train_time:110754ms step_avg:117.70ms
step:942/2245 train_time:110876ms step_avg:117.70ms
step:943/2245 train_time:110992ms step_avg:117.70ms
step:944/2245 train_time:111114ms step_avg:117.71ms
step:945/2245 train_time:111230ms step_avg:117.70ms
step:946/2245 train_time:111351ms step_avg:117.71ms
step:947/2245 train_time:111467ms step_avg:117.71ms
step:948/2245 train_time:111589ms step_avg:117.71ms
step:949/2245 train_time:111704ms step_avg:117.71ms
step:950/2245 train_time:111826ms step_avg:117.71ms
step:951/2245 train_time:111942ms step_avg:117.71ms
step:952/2245 train_time:112064ms step_avg:117.71ms
step:953/2245 train_time:112179ms step_avg:117.71ms
step:954/2245 train_time:112302ms step_avg:117.72ms
step:955/2245 train_time:112417ms step_avg:117.71ms
step:956/2245 train_time:112539ms step_avg:117.72ms
step:957/2245 train_time:112655ms step_avg:117.72ms
step:958/2245 train_time:112777ms step_avg:117.72ms
step:959/2245 train_time:112893ms step_avg:117.72ms
step:960/2245 train_time:113015ms step_avg:117.72ms
step:961/2245 train_time:113131ms step_avg:117.72ms
step:962/2245 train_time:113253ms step_avg:117.73ms
step:963/2245 train_time:113369ms step_avg:117.72ms
step:964/2245 train_time:113491ms step_avg:117.73ms
step:965/2245 train_time:113606ms step_avg:117.73ms
step:966/2245 train_time:113728ms step_avg:117.73ms
step:967/2245 train_time:113843ms step_avg:117.73ms
step:968/2245 train_time:113965ms step_avg:117.73ms
step:969/2245 train_time:114081ms step_avg:117.73ms
step:970/2245 train_time:114203ms step_avg:117.73ms
step:971/2245 train_time:114318ms step_avg:117.73ms
step:972/2245 train_time:114440ms step_avg:117.74ms
step:973/2245 train_time:114555ms step_avg:117.73ms
step:974/2245 train_time:114677ms step_avg:117.74ms
step:975/2245 train_time:114792ms step_avg:117.74ms
step:976/2245 train_time:114915ms step_avg:117.74ms
step:977/2245 train_time:115031ms step_avg:117.74ms
step:978/2245 train_time:115153ms step_avg:117.74ms
step:979/2245 train_time:115269ms step_avg:117.74ms
step:980/2245 train_time:115392ms step_avg:117.75ms
step:981/2245 train_time:115507ms step_avg:117.74ms
step:982/2245 train_time:115629ms step_avg:117.75ms
step:983/2245 train_time:115745ms step_avg:117.75ms
step:984/2245 train_time:115867ms step_avg:117.75ms
step:985/2245 train_time:115982ms step_avg:117.75ms
step:986/2245 train_time:116104ms step_avg:117.75ms
step:987/2245 train_time:116220ms step_avg:117.75ms
step:988/2245 train_time:116341ms step_avg:117.75ms
step:989/2245 train_time:116457ms step_avg:117.75ms
step:990/2245 train_time:116579ms step_avg:117.76ms
step:991/2245 train_time:116695ms step_avg:117.75ms
step:992/2245 train_time:116817ms step_avg:117.76ms
step:993/2245 train_time:116932ms step_avg:117.76ms
step:994/2245 train_time:117054ms step_avg:117.76ms
step:995/2245 train_time:117170ms step_avg:117.76ms
step:996/2245 train_time:117292ms step_avg:117.76ms
step:997/2245 train_time:117408ms step_avg:117.76ms
step:998/2245 train_time:117530ms step_avg:117.77ms
step:999/2245 train_time:117645ms step_avg:117.76ms
step:1000/2245 train_time:117767ms step_avg:117.77ms
step:1000/2245 val_loss:3.5919 train_time:117833ms step_avg:117.83ms
step:1001/2245 train_time:117883ms step_avg:117.77ms
step:1002/2245 train_time:118004ms step_avg:117.77ms
step:1003/2245 train_time:118119ms step_avg:117.77ms
step:1004/2245 train_time:118241ms step_avg:117.77ms
step:1005/2245 train_time:118357ms step_avg:117.77ms
step:1006/2245 train_time:118479ms step_avg:117.77ms
step:1007/2245 train_time:118594ms step_avg:117.77ms
step:1008/2245 train_time:118716ms step_avg:117.77ms
step:1009/2245 train_time:118831ms step_avg:117.77ms
step:1010/2245 train_time:118953ms step_avg:117.78ms
step:1011/2245 train_time:119069ms step_avg:117.77ms
step:1012/2245 train_time:119191ms step_avg:117.78ms
step:1013/2245 train_time:119307ms step_avg:117.78ms
step:1014/2245 train_time:119429ms step_avg:117.78ms
step:1015/2245 train_time:119544ms step_avg:117.78ms
step:1016/2245 train_time:119666ms step_avg:117.78ms
step:1017/2245 train_time:119782ms step_avg:117.78ms
step:1018/2245 train_time:119904ms step_avg:117.78ms
step:1019/2245 train_time:120020ms step_avg:117.78ms
step:1020/2245 train_time:120141ms step_avg:117.79ms
step:1021/2245 train_time:120257ms step_avg:117.78ms
step:1022/2245 train_time:120379ms step_avg:117.79ms
step:1023/2245 train_time:120495ms step_avg:117.79ms
step:1024/2245 train_time:120617ms step_avg:117.79ms
step:1025/2245 train_time:120733ms step_avg:117.79ms
step:1026/2245 train_time:120854ms step_avg:117.79ms
step:1027/2245 train_time:120969ms step_avg:117.79ms
step:1028/2245 train_time:121091ms step_avg:117.79ms
step:1029/2245 train_time:121207ms step_avg:117.79ms
step:1030/2245 train_time:121329ms step_avg:117.80ms
step:1031/2245 train_time:121445ms step_avg:117.79ms
step:1032/2245 train_time:121567ms step_avg:117.80ms
step:1033/2245 train_time:121682ms step_avg:117.79ms
step:1034/2245 train_time:121804ms step_avg:117.80ms
step:1035/2245 train_time:121920ms step_avg:117.80ms
step:1036/2245 train_time:122042ms step_avg:117.80ms
step:1037/2245 train_time:122158ms step_avg:117.80ms
step:1038/2245 train_time:122280ms step_avg:117.80ms
step:1039/2245 train_time:122396ms step_avg:117.80ms
step:1040/2245 train_time:122518ms step_avg:117.81ms
step:1041/2245 train_time:122634ms step_avg:117.80ms
step:1042/2245 train_time:122756ms step_avg:117.81ms
step:1043/2245 train_time:122871ms step_avg:117.81ms
step:1044/2245 train_time:122993ms step_avg:117.81ms
step:1045/2245 train_time:123109ms step_avg:117.81ms
step:1046/2245 train_time:123230ms step_avg:117.81ms
step:1047/2245 train_time:123347ms step_avg:117.81ms
step:1048/2245 train_time:123469ms step_avg:117.81ms
step:1049/2245 train_time:123584ms step_avg:117.81ms
step:1050/2245 train_time:123707ms step_avg:117.82ms
step:1051/2245 train_time:123823ms step_avg:117.81ms
step:1052/2245 train_time:123945ms step_avg:117.82ms
step:1053/2245 train_time:124060ms step_avg:117.82ms
step:1054/2245 train_time:124182ms step_avg:117.82ms
step:1055/2245 train_time:124297ms step_avg:117.82ms
step:1056/2245 train_time:124419ms step_avg:117.82ms
step:1057/2245 train_time:124535ms step_avg:117.82ms
step:1058/2245 train_time:124656ms step_avg:117.82ms
step:1059/2245 train_time:124772ms step_avg:117.82ms
step:1060/2245 train_time:124894ms step_avg:117.82ms
step:1061/2245 train_time:125009ms step_avg:117.82ms
step:1062/2245 train_time:125132ms step_avg:117.83ms
step:1063/2245 train_time:125247ms step_avg:117.82ms
step:1064/2245 train_time:125369ms step_avg:117.83ms
step:1065/2245 train_time:125484ms step_avg:117.83ms
step:1066/2245 train_time:125606ms step_avg:117.83ms
step:1067/2245 train_time:125722ms step_avg:117.83ms
step:1068/2245 train_time:125844ms step_avg:117.83ms
step:1069/2245 train_time:125959ms step_avg:117.83ms
step:1070/2245 train_time:126081ms step_avg:117.83ms
step:1071/2245 train_time:126196ms step_avg:117.83ms
step:1072/2245 train_time:126318ms step_avg:117.83ms
step:1073/2245 train_time:126434ms step_avg:117.83ms
step:1074/2245 train_time:126556ms step_avg:117.84ms
step:1075/2245 train_time:126671ms step_avg:117.83ms
step:1076/2245 train_time:126793ms step_avg:117.84ms
step:1077/2245 train_time:126909ms step_avg:117.84ms
step:1078/2245 train_time:127031ms step_avg:117.84ms
step:1079/2245 train_time:127146ms step_avg:117.84ms
step:1080/2245 train_time:127268ms step_avg:117.84ms
step:1081/2245 train_time:127384ms step_avg:117.84ms
step:1082/2245 train_time:127506ms step_avg:117.84ms
step:1083/2245 train_time:127623ms step_avg:117.84ms
step:1084/2245 train_time:127745ms step_avg:117.85ms
step:1085/2245 train_time:127861ms step_avg:117.84ms
step:1086/2245 train_time:127983ms step_avg:117.85ms
step:1087/2245 train_time:128098ms step_avg:117.85ms
step:1088/2245 train_time:128220ms step_avg:117.85ms
step:1089/2245 train_time:128336ms step_avg:117.85ms
step:1090/2245 train_time:128458ms step_avg:117.85ms
step:1091/2245 train_time:128573ms step_avg:117.85ms
step:1092/2245 train_time:128695ms step_avg:117.85ms
step:1093/2245 train_time:128811ms step_avg:117.85ms
step:1094/2245 train_time:128933ms step_avg:117.85ms
step:1095/2245 train_time:129048ms step_avg:117.85ms
step:1096/2245 train_time:129171ms step_avg:117.86ms
step:1097/2245 train_time:129287ms step_avg:117.85ms
step:1098/2245 train_time:129409ms step_avg:117.86ms
step:1099/2245 train_time:129525ms step_avg:117.86ms
step:1100/2245 train_time:129647ms step_avg:117.86ms
step:1101/2245 train_time:129763ms step_avg:117.86ms
step:1102/2245 train_time:129885ms step_avg:117.86ms
step:1103/2245 train_time:130000ms step_avg:117.86ms
step:1104/2245 train_time:130122ms step_avg:117.86ms
step:1105/2245 train_time:130237ms step_avg:117.86ms
step:1106/2245 train_time:130360ms step_avg:117.87ms
step:1107/2245 train_time:130475ms step_avg:117.86ms
step:1108/2245 train_time:130597ms step_avg:117.87ms
step:1109/2245 train_time:130712ms step_avg:117.87ms
step:1110/2245 train_time:130835ms step_avg:117.87ms
step:1111/2245 train_time:130950ms step_avg:117.87ms
step:1112/2245 train_time:131072ms step_avg:117.87ms
step:1113/2245 train_time:131188ms step_avg:117.87ms
step:1114/2245 train_time:131310ms step_avg:117.87ms
step:1115/2245 train_time:131426ms step_avg:117.87ms
step:1116/2245 train_time:131548ms step_avg:117.87ms
step:1117/2245 train_time:131664ms step_avg:117.87ms
step:1118/2245 train_time:131786ms step_avg:117.88ms
step:1119/2245 train_time:131902ms step_avg:117.87ms
step:1120/2245 train_time:132024ms step_avg:117.88ms
step:1121/2245 train_time:132140ms step_avg:117.88ms
step:1122/2245 train_time:132261ms step_avg:117.88ms
step:1123/2245 train_time:132377ms step_avg:117.88ms
step:1124/2245 train_time:132499ms step_avg:117.88ms
step:1125/2245 train_time:132614ms step_avg:117.88ms
step:1126/2245 train_time:132736ms step_avg:117.88ms
step:1127/2245 train_time:132851ms step_avg:117.88ms
step:1128/2245 train_time:132974ms step_avg:117.88ms
step:1129/2245 train_time:133090ms step_avg:117.88ms
step:1130/2245 train_time:133212ms step_avg:117.89ms
step:1131/2245 train_time:133328ms step_avg:117.88ms
step:1132/2245 train_time:133450ms step_avg:117.89ms
step:1133/2245 train_time:133566ms step_avg:117.89ms
step:1134/2245 train_time:133687ms step_avg:117.89ms
step:1135/2245 train_time:133803ms step_avg:117.89ms
step:1136/2245 train_time:133925ms step_avg:117.89ms
step:1137/2245 train_time:134041ms step_avg:117.89ms
step:1138/2245 train_time:134163ms step_avg:117.89ms
step:1139/2245 train_time:134278ms step_avg:117.89ms
step:1140/2245 train_time:134400ms step_avg:117.89ms
step:1141/2245 train_time:134515ms step_avg:117.89ms
step:1142/2245 train_time:134637ms step_avg:117.90ms
step:1143/2245 train_time:134753ms step_avg:117.89ms
step:1144/2245 train_time:134875ms step_avg:117.90ms
step:1145/2245 train_time:134990ms step_avg:117.90ms
step:1146/2245 train_time:135112ms step_avg:117.90ms
step:1147/2245 train_time:135228ms step_avg:117.90ms
step:1148/2245 train_time:135350ms step_avg:117.90ms
step:1149/2245 train_time:135466ms step_avg:117.90ms
step:1150/2245 train_time:135588ms step_avg:117.90ms
step:1151/2245 train_time:135704ms step_avg:117.90ms
step:1152/2245 train_time:135827ms step_avg:117.90ms
step:1153/2245 train_time:135943ms step_avg:117.90ms
step:1154/2245 train_time:136065ms step_avg:117.91ms
step:1155/2245 train_time:136181ms step_avg:117.91ms
step:1156/2245 train_time:136303ms step_avg:117.91ms
step:1157/2245 train_time:136418ms step_avg:117.91ms
step:1158/2245 train_time:136540ms step_avg:117.91ms
step:1159/2245 train_time:136656ms step_avg:117.91ms
step:1160/2245 train_time:136777ms step_avg:117.91ms
step:1161/2245 train_time:136893ms step_avg:117.91ms
step:1162/2245 train_time:137015ms step_avg:117.91ms
step:1163/2245 train_time:137130ms step_avg:117.91ms
step:1164/2245 train_time:137252ms step_avg:117.91ms
step:1165/2245 train_time:137368ms step_avg:117.91ms
step:1166/2245 train_time:137490ms step_avg:117.92ms
step:1167/2245 train_time:137606ms step_avg:117.91ms
step:1168/2245 train_time:137728ms step_avg:117.92ms
step:1169/2245 train_time:137844ms step_avg:117.92ms
step:1170/2245 train_time:137967ms step_avg:117.92ms
step:1171/2245 train_time:138082ms step_avg:117.92ms
step:1172/2245 train_time:138204ms step_avg:117.92ms
step:1173/2245 train_time:138320ms step_avg:117.92ms
step:1174/2245 train_time:138442ms step_avg:117.92ms
step:1175/2245 train_time:138558ms step_avg:117.92ms
step:1176/2245 train_time:138679ms step_avg:117.92ms
step:1177/2245 train_time:138795ms step_avg:117.92ms
step:1178/2245 train_time:138917ms step_avg:117.93ms
step:1179/2245 train_time:139032ms step_avg:117.92ms
step:1180/2245 train_time:139154ms step_avg:117.93ms
step:1181/2245 train_time:139270ms step_avg:117.93ms
step:1182/2245 train_time:139392ms step_avg:117.93ms
step:1183/2245 train_time:139508ms step_avg:117.93ms
step:1184/2245 train_time:139630ms step_avg:117.93ms
step:1185/2245 train_time:139746ms step_avg:117.93ms
step:1186/2245 train_time:139868ms step_avg:117.93ms
step:1187/2245 train_time:139984ms step_avg:117.93ms
step:1188/2245 train_time:140107ms step_avg:117.94ms
step:1189/2245 train_time:140223ms step_avg:117.93ms
step:1190/2245 train_time:140346ms step_avg:117.94ms
step:1191/2245 train_time:140461ms step_avg:117.94ms
step:1192/2245 train_time:140583ms step_avg:117.94ms
step:1193/2245 train_time:140698ms step_avg:117.94ms
step:1194/2245 train_time:140820ms step_avg:117.94ms
step:1195/2245 train_time:140935ms step_avg:117.94ms
step:1196/2245 train_time:141058ms step_avg:117.94ms
step:1197/2245 train_time:141173ms step_avg:117.94ms
step:1198/2245 train_time:141295ms step_avg:117.94ms
step:1199/2245 train_time:141410ms step_avg:117.94ms
step:1200/2245 train_time:141532ms step_avg:117.94ms
step:1201/2245 train_time:141648ms step_avg:117.94ms
step:1202/2245 train_time:141771ms step_avg:117.95ms
step:1203/2245 train_time:141887ms step_avg:117.94ms
step:1204/2245 train_time:142009ms step_avg:117.95ms
step:1205/2245 train_time:142125ms step_avg:117.95ms
step:1206/2245 train_time:142247ms step_avg:117.95ms
step:1207/2245 train_time:142363ms step_avg:117.95ms
step:1208/2245 train_time:142485ms step_avg:117.95ms
step:1209/2245 train_time:142601ms step_avg:117.95ms
step:1210/2245 train_time:142723ms step_avg:117.95ms
step:1211/2245 train_time:142838ms step_avg:117.95ms
step:1212/2245 train_time:142960ms step_avg:117.95ms
step:1213/2245 train_time:143075ms step_avg:117.95ms
step:1214/2245 train_time:143197ms step_avg:117.95ms
step:1215/2245 train_time:143312ms step_avg:117.95ms
step:1216/2245 train_time:143434ms step_avg:117.96ms
step:1217/2245 train_time:143549ms step_avg:117.95ms
step:1218/2245 train_time:143671ms step_avg:117.96ms
step:1219/2245 train_time:143787ms step_avg:117.96ms
step:1220/2245 train_time:143909ms step_avg:117.96ms
step:1221/2245 train_time:144025ms step_avg:117.96ms
step:1222/2245 train_time:144147ms step_avg:117.96ms
step:1223/2245 train_time:144262ms step_avg:117.96ms
step:1224/2245 train_time:144385ms step_avg:117.96ms
step:1225/2245 train_time:144500ms step_avg:117.96ms
step:1226/2245 train_time:144622ms step_avg:117.96ms
step:1227/2245 train_time:144737ms step_avg:117.96ms
step:1228/2245 train_time:144859ms step_avg:117.96ms
step:1229/2245 train_time:144975ms step_avg:117.96ms
step:1230/2245 train_time:145096ms step_avg:117.96ms
step:1231/2245 train_time:145212ms step_avg:117.96ms
step:1232/2245 train_time:145334ms step_avg:117.97ms
step:1233/2245 train_time:145449ms step_avg:117.96ms
step:1234/2245 train_time:145572ms step_avg:117.97ms
step:1235/2245 train_time:145688ms step_avg:117.97ms
step:1236/2245 train_time:145810ms step_avg:117.97ms
step:1237/2245 train_time:145926ms step_avg:117.97ms
step:1238/2245 train_time:146048ms step_avg:117.97ms
step:1239/2245 train_time:146164ms step_avg:117.97ms
step:1240/2245 train_time:146287ms step_avg:117.97ms
step:1241/2245 train_time:146402ms step_avg:117.97ms
step:1242/2245 train_time:146525ms step_avg:117.97ms
step:1243/2245 train_time:146640ms step_avg:117.97ms
step:1244/2245 train_time:146762ms step_avg:117.98ms
step:1245/2245 train_time:146877ms step_avg:117.97ms
step:1246/2245 train_time:147000ms step_avg:117.98ms
step:1247/2245 train_time:147115ms step_avg:117.98ms
step:1248/2245 train_time:147237ms step_avg:117.98ms
step:1249/2245 train_time:147353ms step_avg:117.98ms
step:1250/2245 train_time:147475ms step_avg:117.98ms
step:1250/2245 val_loss:3.5223 train_time:147540ms step_avg:118.03ms
step:1251/2245 train_time:147591ms step_avg:117.98ms
step:1252/2245 train_time:147712ms step_avg:117.98ms
step:1253/2245 train_time:147828ms step_avg:117.98ms
step:1254/2245 train_time:147949ms step_avg:117.98ms
step:1255/2245 train_time:148065ms step_avg:117.98ms
step:1256/2245 train_time:148186ms step_avg:117.98ms
step:1257/2245 train_time:148302ms step_avg:117.98ms
step:1258/2245 train_time:148424ms step_avg:117.98ms
step:1259/2245 train_time:148539ms step_avg:117.98ms
step:1260/2245 train_time:148662ms step_avg:117.99ms
step:1261/2245 train_time:148777ms step_avg:117.98ms
step:1262/2245 train_time:148900ms step_avg:117.99ms
step:1263/2245 train_time:149016ms step_avg:117.99ms
step:1264/2245 train_time:149138ms step_avg:117.99ms
step:1265/2245 train_time:149253ms step_avg:117.99ms
step:1266/2245 train_time:149375ms step_avg:117.99ms
step:1267/2245 train_time:149490ms step_avg:117.99ms
step:1268/2245 train_time:149612ms step_avg:117.99ms
step:1269/2245 train_time:149728ms step_avg:117.99ms
step:1270/2245 train_time:149850ms step_avg:117.99ms
step:1271/2245 train_time:149965ms step_avg:117.99ms
step:1272/2245 train_time:150087ms step_avg:117.99ms
step:1273/2245 train_time:150202ms step_avg:117.99ms
step:1274/2245 train_time:150324ms step_avg:117.99ms
step:1275/2245 train_time:150439ms step_avg:117.99ms
step:1276/2245 train_time:150562ms step_avg:118.00ms
step:1277/2245 train_time:150678ms step_avg:117.99ms
step:1278/2245 train_time:150800ms step_avg:118.00ms
step:1279/2245 train_time:150916ms step_avg:117.99ms
step:1280/2245 train_time:151038ms step_avg:118.00ms
step:1281/2245 train_time:151154ms step_avg:118.00ms
step:1282/2245 train_time:151277ms step_avg:118.00ms
step:1283/2245 train_time:151393ms step_avg:118.00ms
step:1284/2245 train_time:151515ms step_avg:118.00ms
step:1285/2245 train_time:151630ms step_avg:118.00ms
step:1286/2245 train_time:151752ms step_avg:118.00ms
step:1287/2245 train_time:151868ms step_avg:118.00ms
step:1288/2245 train_time:151990ms step_avg:118.00ms
step:1289/2245 train_time:152105ms step_avg:118.00ms
step:1290/2245 train_time:152227ms step_avg:118.01ms
step:1291/2245 train_time:152343ms step_avg:118.00ms
step:1292/2245 train_time:152464ms step_avg:118.01ms
step:1293/2245 train_time:152580ms step_avg:118.00ms
step:1294/2245 train_time:152702ms step_avg:118.01ms
step:1295/2245 train_time:152817ms step_avg:118.01ms
step:1296/2245 train_time:152939ms step_avg:118.01ms
step:1297/2245 train_time:153056ms step_avg:118.01ms
step:1298/2245 train_time:153178ms step_avg:118.01ms
step:1299/2245 train_time:153294ms step_avg:118.01ms
step:1300/2245 train_time:153416ms step_avg:118.01ms
step:1301/2245 train_time:153531ms step_avg:118.01ms
step:1302/2245 train_time:153653ms step_avg:118.01ms
step:1303/2245 train_time:153768ms step_avg:118.01ms
step:1304/2245 train_time:153891ms step_avg:118.01ms
step:1305/2245 train_time:154006ms step_avg:118.01ms
step:1306/2245 train_time:154128ms step_avg:118.02ms
step:1307/2245 train_time:154243ms step_avg:118.01ms
step:1308/2245 train_time:154365ms step_avg:118.02ms
step:1309/2245 train_time:154480ms step_avg:118.01ms
step:1310/2245 train_time:154602ms step_avg:118.02ms
step:1311/2245 train_time:154717ms step_avg:118.01ms
step:1312/2245 train_time:154839ms step_avg:118.02ms
step:1313/2245 train_time:154955ms step_avg:118.02ms
step:1314/2245 train_time:155077ms step_avg:118.02ms
step:1315/2245 train_time:155192ms step_avg:118.02ms
step:1316/2245 train_time:155315ms step_avg:118.02ms
step:1317/2245 train_time:155430ms step_avg:118.02ms
step:1318/2245 train_time:155552ms step_avg:118.02ms
step:1319/2245 train_time:155668ms step_avg:118.02ms
step:1320/2245 train_time:155789ms step_avg:118.02ms
step:1321/2245 train_time:155905ms step_avg:118.02ms
step:1322/2245 train_time:156027ms step_avg:118.02ms
step:1323/2245 train_time:156143ms step_avg:118.02ms
step:1324/2245 train_time:156265ms step_avg:118.02ms
step:1325/2245 train_time:156381ms step_avg:118.02ms
step:1326/2245 train_time:156503ms step_avg:118.03ms
step:1327/2245 train_time:156619ms step_avg:118.03ms
step:1328/2245 train_time:156741ms step_avg:118.03ms
step:1329/2245 train_time:156857ms step_avg:118.03ms
step:1330/2245 train_time:156979ms step_avg:118.03ms
step:1331/2245 train_time:157095ms step_avg:118.03ms
step:1332/2245 train_time:157217ms step_avg:118.03ms
step:1333/2245 train_time:157333ms step_avg:118.03ms
step:1334/2245 train_time:157454ms step_avg:118.03ms
step:1335/2245 train_time:157570ms step_avg:118.03ms
step:1336/2245 train_time:157692ms step_avg:118.03ms
step:1337/2245 train_time:157808ms step_avg:118.03ms
step:1338/2245 train_time:157930ms step_avg:118.03ms
step:1339/2245 train_time:158046ms step_avg:118.03ms
step:1340/2245 train_time:158167ms step_avg:118.04ms
step:1341/2245 train_time:158282ms step_avg:118.03ms
step:1342/2245 train_time:158404ms step_avg:118.04ms
step:1343/2245 train_time:158520ms step_avg:118.03ms
step:1344/2245 train_time:158641ms step_avg:118.04ms
step:1345/2245 train_time:158757ms step_avg:118.04ms
step:1346/2245 train_time:158879ms step_avg:118.04ms
step:1347/2245 train_time:158996ms step_avg:118.04ms
step:1348/2245 train_time:159118ms step_avg:118.04ms
step:1349/2245 train_time:159234ms step_avg:118.04ms
step:1350/2245 train_time:159356ms step_avg:118.04ms
step:1351/2245 train_time:159471ms step_avg:118.04ms
step:1352/2245 train_time:159593ms step_avg:118.04ms
step:1353/2245 train_time:159708ms step_avg:118.04ms
step:1354/2245 train_time:159830ms step_avg:118.04ms
step:1355/2245 train_time:159946ms step_avg:118.04ms
step:1356/2245 train_time:160068ms step_avg:118.04ms
step:1357/2245 train_time:160184ms step_avg:118.04ms
step:1358/2245 train_time:160306ms step_avg:118.05ms
step:1359/2245 train_time:160421ms step_avg:118.04ms
step:1360/2245 train_time:160544ms step_avg:118.05ms
step:1361/2245 train_time:160659ms step_avg:118.04ms
step:1362/2245 train_time:160781ms step_avg:118.05ms
step:1363/2245 train_time:160897ms step_avg:118.05ms
step:1364/2245 train_time:161019ms step_avg:118.05ms
step:1365/2245 train_time:161136ms step_avg:118.05ms
step:1366/2245 train_time:161257ms step_avg:118.05ms
step:1367/2245 train_time:161374ms step_avg:118.05ms
step:1368/2245 train_time:161496ms step_avg:118.05ms
step:1369/2245 train_time:161611ms step_avg:118.05ms
step:1370/2245 train_time:161733ms step_avg:118.05ms
step:1371/2245 train_time:161848ms step_avg:118.05ms
step:1372/2245 train_time:161971ms step_avg:118.05ms
step:1373/2245 train_time:162086ms step_avg:118.05ms
step:1374/2245 train_time:162208ms step_avg:118.06ms
step:1375/2245 train_time:162323ms step_avg:118.05ms
step:1376/2245 train_time:162445ms step_avg:118.06ms
step:1377/2245 train_time:162561ms step_avg:118.05ms
step:1378/2245 train_time:162683ms step_avg:118.06ms
step:1379/2245 train_time:162798ms step_avg:118.06ms
step:1380/2245 train_time:162921ms step_avg:118.06ms
step:1381/2245 train_time:163036ms step_avg:118.06ms
step:1382/2245 train_time:163157ms step_avg:118.06ms
step:1383/2245 train_time:163274ms step_avg:118.06ms
step:1384/2245 train_time:163395ms step_avg:118.06ms
step:1385/2245 train_time:163511ms step_avg:118.06ms
step:1386/2245 train_time:163632ms step_avg:118.06ms
step:1387/2245 train_time:163748ms step_avg:118.06ms
step:1388/2245 train_time:163871ms step_avg:118.06ms
step:1389/2245 train_time:163986ms step_avg:118.06ms
step:1390/2245 train_time:164108ms step_avg:118.06ms
step:1391/2245 train_time:164224ms step_avg:118.06ms
step:1392/2245 train_time:164346ms step_avg:118.06ms
step:1393/2245 train_time:164461ms step_avg:118.06ms
step:1394/2245 train_time:164583ms step_avg:118.07ms
step:1395/2245 train_time:164699ms step_avg:118.06ms
step:1396/2245 train_time:164821ms step_avg:118.07ms
step:1397/2245 train_time:164937ms step_avg:118.07ms
step:1398/2245 train_time:165059ms step_avg:118.07ms
step:1399/2245 train_time:165174ms step_avg:118.07ms
step:1400/2245 train_time:165297ms step_avg:118.07ms
step:1401/2245 train_time:165412ms step_avg:118.07ms
step:1402/2245 train_time:165534ms step_avg:118.07ms
step:1403/2245 train_time:165649ms step_avg:118.07ms
step:1404/2245 train_time:165771ms step_avg:118.07ms
step:1405/2245 train_time:165887ms step_avg:118.07ms
step:1406/2245 train_time:166009ms step_avg:118.07ms
step:1407/2245 train_time:166124ms step_avg:118.07ms
step:1408/2245 train_time:166246ms step_avg:118.07ms
step:1409/2245 train_time:166361ms step_avg:118.07ms
step:1410/2245 train_time:166483ms step_avg:118.07ms
step:1411/2245 train_time:166599ms step_avg:118.07ms
step:1412/2245 train_time:166721ms step_avg:118.07ms
step:1413/2245 train_time:166837ms step_avg:118.07ms
step:1414/2245 train_time:166959ms step_avg:118.08ms
step:1415/2245 train_time:167075ms step_avg:118.07ms
step:1416/2245 train_time:167197ms step_avg:118.08ms
step:1417/2245 train_time:167312ms step_avg:118.08ms
step:1418/2245 train_time:167434ms step_avg:118.08ms
step:1419/2245 train_time:167550ms step_avg:118.08ms
step:1420/2245 train_time:167672ms step_avg:118.08ms
step:1421/2245 train_time:167788ms step_avg:118.08ms
step:1422/2245 train_time:167909ms step_avg:118.08ms
step:1423/2245 train_time:168025ms step_avg:118.08ms
step:1424/2245 train_time:168147ms step_avg:118.08ms
step:1425/2245 train_time:168262ms step_avg:118.08ms
step:1426/2245 train_time:168384ms step_avg:118.08ms
step:1427/2245 train_time:168500ms step_avg:118.08ms
step:1428/2245 train_time:168622ms step_avg:118.08ms
step:1429/2245 train_time:168738ms step_avg:118.08ms
step:1430/2245 train_time:168860ms step_avg:118.08ms
step:1431/2245 train_time:168975ms step_avg:118.08ms
step:1432/2245 train_time:169097ms step_avg:118.08ms
step:1433/2245 train_time:169214ms step_avg:118.08ms
step:1434/2245 train_time:169335ms step_avg:118.09ms
step:1435/2245 train_time:169451ms step_avg:118.08ms
step:1436/2245 train_time:169573ms step_avg:118.09ms
step:1437/2245 train_time:169688ms step_avg:118.08ms
step:1438/2245 train_time:169810ms step_avg:118.09ms
step:1439/2245 train_time:169925ms step_avg:118.09ms
step:1440/2245 train_time:170047ms step_avg:118.09ms
step:1441/2245 train_time:170163ms step_avg:118.09ms
step:1442/2245 train_time:170284ms step_avg:118.09ms
step:1443/2245 train_time:170400ms step_avg:118.09ms
step:1444/2245 train_time:170522ms step_avg:118.09ms
step:1445/2245 train_time:170638ms step_avg:118.09ms
step:1446/2245 train_time:170760ms step_avg:118.09ms
step:1447/2245 train_time:170876ms step_avg:118.09ms
step:1448/2245 train_time:170998ms step_avg:118.09ms
step:1449/2245 train_time:171114ms step_avg:118.09ms
step:1450/2245 train_time:171236ms step_avg:118.09ms
step:1451/2245 train_time:171352ms step_avg:118.09ms
step:1452/2245 train_time:171473ms step_avg:118.09ms
step:1453/2245 train_time:171588ms step_avg:118.09ms
step:1454/2245 train_time:171710ms step_avg:118.10ms
step:1455/2245 train_time:171826ms step_avg:118.09ms
step:1456/2245 train_time:171948ms step_avg:118.10ms
step:1457/2245 train_time:172063ms step_avg:118.09ms
step:1458/2245 train_time:172185ms step_avg:118.10ms
step:1459/2245 train_time:172301ms step_avg:118.10ms
step:1460/2245 train_time:172423ms step_avg:118.10ms
step:1461/2245 train_time:172538ms step_avg:118.10ms
step:1462/2245 train_time:172661ms step_avg:118.10ms
step:1463/2245 train_time:172777ms step_avg:118.10ms
step:1464/2245 train_time:172900ms step_avg:118.10ms
step:1465/2245 train_time:173016ms step_avg:118.10ms
step:1466/2245 train_time:173138ms step_avg:118.10ms
step:1467/2245 train_time:173254ms step_avg:118.10ms
step:1468/2245 train_time:173376ms step_avg:118.10ms
step:1469/2245 train_time:173491ms step_avg:118.10ms
step:1470/2245 train_time:173612ms step_avg:118.10ms
step:1471/2245 train_time:173728ms step_avg:118.10ms
step:1472/2245 train_time:173851ms step_avg:118.11ms
step:1473/2245 train_time:173968ms step_avg:118.10ms
step:1474/2245 train_time:174091ms step_avg:118.11ms
step:1475/2245 train_time:174207ms step_avg:118.11ms
step:1476/2245 train_time:174330ms step_avg:118.11ms
step:1477/2245 train_time:174446ms step_avg:118.11ms
step:1478/2245 train_time:174569ms step_avg:118.11ms
step:1479/2245 train_time:174686ms step_avg:118.11ms
step:1480/2245 train_time:174808ms step_avg:118.11ms
step:1481/2245 train_time:174925ms step_avg:118.11ms
step:1482/2245 train_time:175048ms step_avg:118.12ms
step:1483/2245 train_time:175165ms step_avg:118.12ms
step:1484/2245 train_time:175288ms step_avg:118.12ms
step:1485/2245 train_time:175404ms step_avg:118.12ms
step:1486/2245 train_time:175526ms step_avg:118.12ms
step:1487/2245 train_time:175643ms step_avg:118.12ms
step:1488/2245 train_time:175767ms step_avg:118.12ms
step:1489/2245 train_time:175883ms step_avg:118.12ms
step:1490/2245 train_time:176006ms step_avg:118.12ms
step:1491/2245 train_time:176122ms step_avg:118.12ms
step:1492/2245 train_time:176245ms step_avg:118.13ms
step:1493/2245 train_time:176362ms step_avg:118.13ms
step:1494/2245 train_time:176485ms step_avg:118.13ms
step:1495/2245 train_time:176602ms step_avg:118.13ms
step:1496/2245 train_time:176725ms step_avg:118.13ms
step:1497/2245 train_time:176842ms step_avg:118.13ms
step:1498/2245 train_time:176965ms step_avg:118.13ms
step:1499/2245 train_time:177081ms step_avg:118.13ms
step:1500/2245 train_time:177205ms step_avg:118.14ms
step:1500/2245 val_loss:3.4435 train_time:177271ms step_avg:118.18ms
step:1501/2245 train_time:177322ms step_avg:118.14ms
step:1502/2245 train_time:177444ms step_avg:118.14ms
step:1503/2245 train_time:177559ms step_avg:118.14ms
step:1504/2245 train_time:177682ms step_avg:118.14ms
step:1505/2245 train_time:177798ms step_avg:118.14ms
step:1506/2245 train_time:177921ms step_avg:118.14ms
step:1507/2245 train_time:178038ms step_avg:118.14ms
step:1508/2245 train_time:178160ms step_avg:118.14ms
step:1509/2245 train_time:178277ms step_avg:118.14ms
step:1510/2245 train_time:178400ms step_avg:118.15ms
step:1511/2245 train_time:178516ms step_avg:118.14ms
step:1512/2245 train_time:178640ms step_avg:118.15ms
step:1513/2245 train_time:178757ms step_avg:118.15ms
step:1514/2245 train_time:178880ms step_avg:118.15ms
step:1515/2245 train_time:178996ms step_avg:118.15ms
step:1516/2245 train_time:179119ms step_avg:118.15ms
step:1517/2245 train_time:179236ms step_avg:118.15ms
step:1518/2245 train_time:179358ms step_avg:118.15ms
step:1519/2245 train_time:179475ms step_avg:118.15ms
step:1520/2245 train_time:179598ms step_avg:118.16ms
step:1521/2245 train_time:179715ms step_avg:118.16ms
step:1522/2245 train_time:179838ms step_avg:118.16ms
step:1523/2245 train_time:179954ms step_avg:118.16ms
step:1524/2245 train_time:180077ms step_avg:118.16ms
step:1525/2245 train_time:180193ms step_avg:118.16ms
step:1526/2245 train_time:180317ms step_avg:118.16ms
step:1527/2245 train_time:180433ms step_avg:118.16ms
step:1528/2245 train_time:180555ms step_avg:118.16ms
step:1529/2245 train_time:180672ms step_avg:118.16ms
step:1530/2245 train_time:180795ms step_avg:118.17ms
step:1531/2245 train_time:180911ms step_avg:118.17ms
step:1532/2245 train_time:181034ms step_avg:118.17ms
step:1533/2245 train_time:181151ms step_avg:118.17ms
step:1534/2245 train_time:181274ms step_avg:118.17ms
step:1535/2245 train_time:181391ms step_avg:118.17ms
step:1536/2245 train_time:181514ms step_avg:118.17ms
step:1537/2245 train_time:181630ms step_avg:118.17ms
step:1538/2245 train_time:181754ms step_avg:118.18ms
step:1539/2245 train_time:181870ms step_avg:118.17ms
step:1540/2245 train_time:181994ms step_avg:118.18ms
step:1541/2245 train_time:182111ms step_avg:118.18ms
step:1542/2245 train_time:182234ms step_avg:118.18ms
step:1543/2245 train_time:182351ms step_avg:118.18ms
step:1544/2245 train_time:182474ms step_avg:118.18ms
step:1545/2245 train_time:182591ms step_avg:118.18ms
step:1546/2245 train_time:182715ms step_avg:118.19ms
step:1547/2245 train_time:182831ms step_avg:118.18ms
step:1548/2245 train_time:182954ms step_avg:118.19ms
step:1549/2245 train_time:183070ms step_avg:118.19ms
step:1550/2245 train_time:183194ms step_avg:118.19ms
step:1551/2245 train_time:183311ms step_avg:118.19ms
step:1552/2245 train_time:183434ms step_avg:118.19ms
step:1553/2245 train_time:183551ms step_avg:118.19ms
step:1554/2245 train_time:183674ms step_avg:118.19ms
step:1555/2245 train_time:183790ms step_avg:118.19ms
step:1556/2245 train_time:183913ms step_avg:118.20ms
step:1557/2245 train_time:184030ms step_avg:118.19ms
step:1558/2245 train_time:184154ms step_avg:118.20ms
step:1559/2245 train_time:184270ms step_avg:118.20ms
step:1560/2245 train_time:184393ms step_avg:118.20ms
step:1561/2245 train_time:184510ms step_avg:118.20ms
step:1562/2245 train_time:184632ms step_avg:118.20ms
step:1563/2245 train_time:184749ms step_avg:118.20ms
step:1564/2245 train_time:184872ms step_avg:118.20ms
step:1565/2245 train_time:184989ms step_avg:118.20ms
step:1566/2245 train_time:185111ms step_avg:118.21ms
step:1567/2245 train_time:185227ms step_avg:118.20ms
step:1568/2245 train_time:185350ms step_avg:118.21ms
step:1569/2245 train_time:185466ms step_avg:118.21ms
step:1570/2245 train_time:185589ms step_avg:118.21ms
step:1571/2245 train_time:185705ms step_avg:118.21ms
step:1572/2245 train_time:185828ms step_avg:118.21ms
step:1573/2245 train_time:185944ms step_avg:118.21ms
step:1574/2245 train_time:186068ms step_avg:118.21ms
step:1575/2245 train_time:186185ms step_avg:118.21ms
step:1576/2245 train_time:186308ms step_avg:118.22ms
step:1577/2245 train_time:186424ms step_avg:118.21ms
step:1578/2245 train_time:186547ms step_avg:118.22ms
step:1579/2245 train_time:186663ms step_avg:118.22ms
step:1580/2245 train_time:186786ms step_avg:118.22ms
step:1581/2245 train_time:186903ms step_avg:118.22ms
step:1582/2245 train_time:187026ms step_avg:118.22ms
step:1583/2245 train_time:187143ms step_avg:118.22ms
step:1584/2245 train_time:187266ms step_avg:118.22ms
step:1585/2245 train_time:187383ms step_avg:118.22ms
step:1586/2245 train_time:187506ms step_avg:118.23ms
step:1587/2245 train_time:187622ms step_avg:118.22ms
step:1588/2245 train_time:187745ms step_avg:118.23ms
step:1589/2245 train_time:187861ms step_avg:118.23ms
step:1590/2245 train_time:187984ms step_avg:118.23ms
step:1591/2245 train_time:188100ms step_avg:118.23ms
step:1592/2245 train_time:188223ms step_avg:118.23ms
step:1593/2245 train_time:188339ms step_avg:118.23ms
step:1594/2245 train_time:188463ms step_avg:118.23ms
step:1595/2245 train_time:188579ms step_avg:118.23ms
step:1596/2245 train_time:188702ms step_avg:118.23ms
step:1597/2245 train_time:188818ms step_avg:118.23ms
step:1598/2245 train_time:188941ms step_avg:118.24ms
step:1599/2245 train_time:189057ms step_avg:118.23ms
step:1600/2245 train_time:189181ms step_avg:118.24ms
step:1601/2245 train_time:189298ms step_avg:118.24ms
step:1602/2245 train_time:189420ms step_avg:118.24ms
step:1603/2245 train_time:189536ms step_avg:118.24ms
step:1604/2245 train_time:189659ms step_avg:118.24ms
step:1605/2245 train_time:189776ms step_avg:118.24ms
step:1606/2245 train_time:189900ms step_avg:118.24ms
step:1607/2245 train_time:190016ms step_avg:118.24ms
step:1608/2245 train_time:190139ms step_avg:118.25ms
step:1609/2245 train_time:190255ms step_avg:118.24ms
step:1610/2245 train_time:190379ms step_avg:118.25ms
step:1611/2245 train_time:190495ms step_avg:118.25ms
step:1612/2245 train_time:190618ms step_avg:118.25ms
step:1613/2245 train_time:190734ms step_avg:118.25ms
step:1614/2245 train_time:190857ms step_avg:118.25ms
step:1615/2245 train_time:190973ms step_avg:118.25ms
step:1616/2245 train_time:191097ms step_avg:118.25ms
step:1617/2245 train_time:191213ms step_avg:118.25ms
step:1618/2245 train_time:191336ms step_avg:118.25ms
step:1619/2245 train_time:191453ms step_avg:118.25ms
step:1620/2245 train_time:191575ms step_avg:118.26ms
step:1621/2245 train_time:191692ms step_avg:118.26ms
step:1622/2245 train_time:191815ms step_avg:118.26ms
step:1623/2245 train_time:191931ms step_avg:118.26ms
step:1624/2245 train_time:192054ms step_avg:118.26ms
step:1625/2245 train_time:192171ms step_avg:118.26ms
step:1626/2245 train_time:192294ms step_avg:118.26ms
step:1627/2245 train_time:192411ms step_avg:118.26ms
step:1628/2245 train_time:192534ms step_avg:118.26ms
step:1629/2245 train_time:192650ms step_avg:118.26ms
step:1630/2245 train_time:192773ms step_avg:118.27ms
step:1631/2245 train_time:192890ms step_avg:118.26ms
step:1632/2245 train_time:193012ms step_avg:118.27ms
step:1633/2245 train_time:193129ms step_avg:118.27ms
step:1634/2245 train_time:193252ms step_avg:118.27ms
step:1635/2245 train_time:193369ms step_avg:118.27ms
step:1636/2245 train_time:193491ms step_avg:118.27ms
step:1637/2245 train_time:193607ms step_avg:118.27ms
step:1638/2245 train_time:193730ms step_avg:118.27ms
step:1639/2245 train_time:193846ms step_avg:118.27ms
step:1640/2245 train_time:193968ms step_avg:118.27ms
step:1641/2245 train_time:194085ms step_avg:118.27ms
step:1642/2245 train_time:194208ms step_avg:118.28ms
step:1643/2245 train_time:194324ms step_avg:118.27ms
step:1644/2245 train_time:194447ms step_avg:118.28ms
step:1645/2245 train_time:194563ms step_avg:118.28ms
step:1646/2245 train_time:194686ms step_avg:118.28ms
step:1647/2245 train_time:194802ms step_avg:118.28ms
step:1648/2245 train_time:194925ms step_avg:118.28ms
step:1649/2245 train_time:195042ms step_avg:118.28ms
step:1650/2245 train_time:195165ms step_avg:118.28ms
step:1651/2245 train_time:195281ms step_avg:118.28ms
step:1652/2245 train_time:195404ms step_avg:118.28ms
step:1653/2245 train_time:195520ms step_avg:118.28ms
step:1654/2245 train_time:195643ms step_avg:118.29ms
step:1655/2245 train_time:195760ms step_avg:118.28ms
step:1656/2245 train_time:195882ms step_avg:118.29ms
step:1657/2245 train_time:195999ms step_avg:118.29ms
step:1658/2245 train_time:196122ms step_avg:118.29ms
step:1659/2245 train_time:196239ms step_avg:118.29ms
step:1660/2245 train_time:196362ms step_avg:118.29ms
step:1661/2245 train_time:196477ms step_avg:118.29ms
step:1662/2245 train_time:196600ms step_avg:118.29ms
step:1663/2245 train_time:196717ms step_avg:118.29ms
step:1664/2245 train_time:196840ms step_avg:118.29ms
step:1665/2245 train_time:196956ms step_avg:118.29ms
step:1666/2245 train_time:197079ms step_avg:118.29ms
step:1667/2245 train_time:197196ms step_avg:118.29ms
step:1668/2245 train_time:197319ms step_avg:118.30ms
step:1669/2245 train_time:197436ms step_avg:118.30ms
step:1670/2245 train_time:197559ms step_avg:118.30ms
step:1671/2245 train_time:197676ms step_avg:118.30ms
step:1672/2245 train_time:197799ms step_avg:118.30ms
step:1673/2245 train_time:197916ms step_avg:118.30ms
step:1674/2245 train_time:198039ms step_avg:118.30ms
step:1675/2245 train_time:198155ms step_avg:118.30ms
step:1676/2245 train_time:198278ms step_avg:118.30ms
step:1677/2245 train_time:198395ms step_avg:118.30ms
step:1678/2245 train_time:198518ms step_avg:118.31ms
step:1679/2245 train_time:198635ms step_avg:118.31ms
step:1680/2245 train_time:198758ms step_avg:118.31ms
step:1681/2245 train_time:198875ms step_avg:118.31ms
step:1682/2245 train_time:198998ms step_avg:118.31ms
step:1683/2245 train_time:199114ms step_avg:118.31ms
step:1684/2245 train_time:199237ms step_avg:118.31ms
step:1685/2245 train_time:199354ms step_avg:118.31ms
step:1686/2245 train_time:199477ms step_avg:118.31ms
step:1687/2245 train_time:199593ms step_avg:118.31ms
step:1688/2245 train_time:199716ms step_avg:118.32ms
step:1689/2245 train_time:199833ms step_avg:118.31ms
step:1690/2245 train_time:199956ms step_avg:118.32ms
step:1691/2245 train_time:200073ms step_avg:118.32ms
step:1692/2245 train_time:200196ms step_avg:118.32ms
step:1693/2245 train_time:200312ms step_avg:118.32ms
step:1694/2245 train_time:200435ms step_avg:118.32ms
step:1695/2245 train_time:200552ms step_avg:118.32ms
step:1696/2245 train_time:200674ms step_avg:118.32ms
step:1697/2245 train_time:200791ms step_avg:118.32ms
step:1698/2245 train_time:200914ms step_avg:118.32ms
step:1699/2245 train_time:201030ms step_avg:118.32ms
step:1700/2245 train_time:201153ms step_avg:118.33ms
step:1701/2245 train_time:201269ms step_avg:118.32ms
step:1702/2245 train_time:201393ms step_avg:118.33ms
step:1703/2245 train_time:201510ms step_avg:118.33ms
step:1704/2245 train_time:201632ms step_avg:118.33ms
step:1705/2245 train_time:201748ms step_avg:118.33ms
step:1706/2245 train_time:201872ms step_avg:118.33ms
step:1707/2245 train_time:201988ms step_avg:118.33ms
step:1708/2245 train_time:202111ms step_avg:118.33ms
step:1709/2245 train_time:202227ms step_avg:118.33ms
step:1710/2245 train_time:202351ms step_avg:118.33ms
step:1711/2245 train_time:202467ms step_avg:118.33ms
step:1712/2245 train_time:202589ms step_avg:118.33ms
step:1713/2245 train_time:202705ms step_avg:118.33ms
step:1714/2245 train_time:202828ms step_avg:118.34ms
step:1715/2245 train_time:202944ms step_avg:118.33ms
step:1716/2245 train_time:203068ms step_avg:118.34ms
step:1717/2245 train_time:203184ms step_avg:118.34ms
step:1718/2245 train_time:203307ms step_avg:118.34ms
step:1719/2245 train_time:203423ms step_avg:118.34ms
step:1720/2245 train_time:203546ms step_avg:118.34ms
step:1721/2245 train_time:203662ms step_avg:118.34ms
step:1722/2245 train_time:203784ms step_avg:118.34ms
step:1723/2245 train_time:203901ms step_avg:118.34ms
step:1724/2245 train_time:204024ms step_avg:118.34ms
step:1725/2245 train_time:204140ms step_avg:118.34ms
step:1726/2245 train_time:204262ms step_avg:118.34ms
step:1727/2245 train_time:204379ms step_avg:118.34ms
step:1728/2245 train_time:204503ms step_avg:118.35ms
step:1729/2245 train_time:204619ms step_avg:118.35ms
step:1730/2245 train_time:204742ms step_avg:118.35ms
step:1731/2245 train_time:204858ms step_avg:118.35ms
step:1732/2245 train_time:204982ms step_avg:118.35ms
step:1733/2245 train_time:205098ms step_avg:118.35ms
step:1734/2245 train_time:205221ms step_avg:118.35ms
step:1735/2245 train_time:205338ms step_avg:118.35ms
step:1736/2245 train_time:205462ms step_avg:118.35ms
step:1737/2245 train_time:205578ms step_avg:118.35ms
step:1738/2245 train_time:205701ms step_avg:118.35ms
step:1739/2245 train_time:205816ms step_avg:118.35ms
step:1740/2245 train_time:205939ms step_avg:118.36ms
step:1741/2245 train_time:206056ms step_avg:118.35ms
step:1742/2245 train_time:206180ms step_avg:118.36ms
step:1743/2245 train_time:206296ms step_avg:118.36ms
step:1744/2245 train_time:206419ms step_avg:118.36ms
step:1745/2245 train_time:206536ms step_avg:118.36ms
step:1746/2245 train_time:206659ms step_avg:118.36ms
step:1747/2245 train_time:206775ms step_avg:118.36ms
step:1748/2245 train_time:206898ms step_avg:118.36ms
step:1749/2245 train_time:207014ms step_avg:118.36ms
step:1750/2245 train_time:207137ms step_avg:118.36ms
step:1750/2245 val_loss:3.3783 train_time:207204ms step_avg:118.40ms
step:1751/2245 train_time:207254ms step_avg:118.36ms
step:1752/2245 train_time:207377ms step_avg:118.37ms
step:1753/2245 train_time:207492ms step_avg:118.36ms
step:1754/2245 train_time:207615ms step_avg:118.37ms
step:1755/2245 train_time:207732ms step_avg:118.37ms
step:1756/2245 train_time:207855ms step_avg:118.37ms
step:1757/2245 train_time:207971ms step_avg:118.37ms
step:1758/2245 train_time:208093ms step_avg:118.37ms
step:1759/2245 train_time:208210ms step_avg:118.37ms
step:1760/2245 train_time:208333ms step_avg:118.37ms
step:1761/2245 train_time:208450ms step_avg:118.37ms
step:1762/2245 train_time:208573ms step_avg:118.37ms
step:1763/2245 train_time:208689ms step_avg:118.37ms
step:1764/2245 train_time:208812ms step_avg:118.37ms
step:1765/2245 train_time:208929ms step_avg:118.37ms
step:1766/2245 train_time:209052ms step_avg:118.38ms
step:1767/2245 train_time:209168ms step_avg:118.37ms
step:1768/2245 train_time:209290ms step_avg:118.38ms
step:1769/2245 train_time:209407ms step_avg:118.38ms
step:1770/2245 train_time:209528ms step_avg:118.38ms
step:1771/2245 train_time:209645ms step_avg:118.38ms
step:1772/2245 train_time:209768ms step_avg:118.38ms
step:1773/2245 train_time:209884ms step_avg:118.38ms
step:1774/2245 train_time:210008ms step_avg:118.38ms
step:1775/2245 train_time:210124ms step_avg:118.38ms
step:1776/2245 train_time:210247ms step_avg:118.38ms
step:1777/2245 train_time:210364ms step_avg:118.38ms
step:1778/2245 train_time:210487ms step_avg:118.38ms
step:1779/2245 train_time:210603ms step_avg:118.38ms
step:1780/2245 train_time:210726ms step_avg:118.39ms
step:1781/2245 train_time:210843ms step_avg:118.38ms
step:1782/2245 train_time:210966ms step_avg:118.39ms
step:1783/2245 train_time:211082ms step_avg:118.39ms
step:1784/2245 train_time:211205ms step_avg:118.39ms
step:1785/2245 train_time:211322ms step_avg:118.39ms
step:1786/2245 train_time:211445ms step_avg:118.39ms
step:1787/2245 train_time:211561ms step_avg:118.39ms
step:1788/2245 train_time:211684ms step_avg:118.39ms
step:1789/2245 train_time:211800ms step_avg:118.39ms
step:1790/2245 train_time:211924ms step_avg:118.39ms
step:1791/2245 train_time:212040ms step_avg:118.39ms
step:1792/2245 train_time:212163ms step_avg:118.39ms
step:1793/2245 train_time:212279ms step_avg:118.39ms
step:1794/2245 train_time:212402ms step_avg:118.40ms
step:1795/2245 train_time:212519ms step_avg:118.39ms
step:1796/2245 train_time:212641ms step_avg:118.40ms
step:1797/2245 train_time:212757ms step_avg:118.40ms
step:1798/2245 train_time:212881ms step_avg:118.40ms
step:1799/2245 train_time:212997ms step_avg:118.40ms
step:1800/2245 train_time:213120ms step_avg:118.40ms
step:1801/2245 train_time:213237ms step_avg:118.40ms
step:1802/2245 train_time:213360ms step_avg:118.40ms
step:1803/2245 train_time:213476ms step_avg:118.40ms
step:1804/2245 train_time:213599ms step_avg:118.40ms
step:1805/2245 train_time:213715ms step_avg:118.40ms
step:1806/2245 train_time:213839ms step_avg:118.40ms
step:1807/2245 train_time:213955ms step_avg:118.40ms
step:1808/2245 train_time:214078ms step_avg:118.41ms
step:1809/2245 train_time:214195ms step_avg:118.41ms
step:1810/2245 train_time:214318ms step_avg:118.41ms
step:1811/2245 train_time:214435ms step_avg:118.41ms
step:1812/2245 train_time:214558ms step_avg:118.41ms
step:1813/2245 train_time:214675ms step_avg:118.41ms
step:1814/2245 train_time:214797ms step_avg:118.41ms
step:1815/2245 train_time:214914ms step_avg:118.41ms
step:1816/2245 train_time:215037ms step_avg:118.41ms
step:1817/2245 train_time:215154ms step_avg:118.41ms
step:1818/2245 train_time:215277ms step_avg:118.41ms
step:1819/2245 train_time:215394ms step_avg:118.41ms
step:1820/2245 train_time:215517ms step_avg:118.42ms
step:1821/2245 train_time:215633ms step_avg:118.41ms
step:1822/2245 train_time:215756ms step_avg:118.42ms
step:1823/2245 train_time:215872ms step_avg:118.42ms
step:1824/2245 train_time:215995ms step_avg:118.42ms
step:1825/2245 train_time:216111ms step_avg:118.42ms
step:1826/2245 train_time:216234ms step_avg:118.42ms
step:1827/2245 train_time:216350ms step_avg:118.42ms
step:1828/2245 train_time:216474ms step_avg:118.42ms
step:1829/2245 train_time:216591ms step_avg:118.42ms
step:1830/2245 train_time:216714ms step_avg:118.42ms
step:1831/2245 train_time:216831ms step_avg:118.42ms
step:1832/2245 train_time:216954ms step_avg:118.42ms
step:1833/2245 train_time:217071ms step_avg:118.42ms
step:1834/2245 train_time:217194ms step_avg:118.43ms
step:1835/2245 train_time:217310ms step_avg:118.42ms
step:1836/2245 train_time:217433ms step_avg:118.43ms
step:1837/2245 train_time:217550ms step_avg:118.43ms
step:1838/2245 train_time:217673ms step_avg:118.43ms
step:1839/2245 train_time:217789ms step_avg:118.43ms
step:1840/2245 train_time:217912ms step_avg:118.43ms
step:1841/2245 train_time:218029ms step_avg:118.43ms
step:1842/2245 train_time:218152ms step_avg:118.43ms
step:1843/2245 train_time:218268ms step_avg:118.43ms
step:1844/2245 train_time:218391ms step_avg:118.43ms
step:1845/2245 train_time:218507ms step_avg:118.43ms
step:1846/2245 train_time:218630ms step_avg:118.43ms
step:1847/2245 train_time:218747ms step_avg:118.43ms
step:1848/2245 train_time:218869ms step_avg:118.44ms
step:1849/2245 train_time:218986ms step_avg:118.43ms
step:1850/2245 train_time:219108ms step_avg:118.44ms
step:1851/2245 train_time:219225ms step_avg:118.44ms
step:1852/2245 train_time:219347ms step_avg:118.44ms
step:1853/2245 train_time:219463ms step_avg:118.44ms
step:1854/2245 train_time:219586ms step_avg:118.44ms
step:1855/2245 train_time:219703ms step_avg:118.44ms
step:1856/2245 train_time:219825ms step_avg:118.44ms
step:1857/2245 train_time:219943ms step_avg:118.44ms
step:1858/2245 train_time:220066ms step_avg:118.44ms
step:1859/2245 train_time:220183ms step_avg:118.44ms
step:1860/2245 train_time:220306ms step_avg:118.44ms
step:1861/2245 train_time:220422ms step_avg:118.44ms
step:1862/2245 train_time:220544ms step_avg:118.44ms
step:1863/2245 train_time:220661ms step_avg:118.44ms
step:1864/2245 train_time:220783ms step_avg:118.45ms
step:1865/2245 train_time:220900ms step_avg:118.44ms
step:1866/2245 train_time:221022ms step_avg:118.45ms
step:1867/2245 train_time:221139ms step_avg:118.45ms
step:1868/2245 train_time:221262ms step_avg:118.45ms
step:1869/2245 train_time:221379ms step_avg:118.45ms
step:1870/2245 train_time:221503ms step_avg:118.45ms
step:1871/2245 train_time:221619ms step_avg:118.45ms
step:1872/2245 train_time:221741ms step_avg:118.45ms
step:1873/2245 train_time:221858ms step_avg:118.45ms
step:1874/2245 train_time:221981ms step_avg:118.45ms
step:1875/2245 train_time:222097ms step_avg:118.45ms
step:1876/2245 train_time:222220ms step_avg:118.45ms
step:1877/2245 train_time:222337ms step_avg:118.45ms
step:1878/2245 train_time:222460ms step_avg:118.46ms
step:1879/2245 train_time:222576ms step_avg:118.45ms
step:1880/2245 train_time:222700ms step_avg:118.46ms
step:1881/2245 train_time:222816ms step_avg:118.46ms
step:1882/2245 train_time:222939ms step_avg:118.46ms
step:1883/2245 train_time:223055ms step_avg:118.46ms
step:1884/2245 train_time:223178ms step_avg:118.46ms
step:1885/2245 train_time:223295ms step_avg:118.46ms
step:1886/2245 train_time:223418ms step_avg:118.46ms
step:1887/2245 train_time:223534ms step_avg:118.46ms
step:1888/2245 train_time:223657ms step_avg:118.46ms
step:1889/2245 train_time:223774ms step_avg:118.46ms
step:1890/2245 train_time:223897ms step_avg:118.46ms
step:1891/2245 train_time:224015ms step_avg:118.46ms
step:1892/2245 train_time:224138ms step_avg:118.47ms
step:1893/2245 train_time:224255ms step_avg:118.47ms
step:1894/2245 train_time:224377ms step_avg:118.47ms
step:1895/2245 train_time:224494ms step_avg:118.47ms
step:1896/2245 train_time:224617ms step_avg:118.47ms
step:1897/2245 train_time:224733ms step_avg:118.47ms
step:1898/2245 train_time:224857ms step_avg:118.47ms
step:1899/2245 train_time:224973ms step_avg:118.47ms
step:1900/2245 train_time:225097ms step_avg:118.47ms
step:1901/2245 train_time:225213ms step_avg:118.47ms
step:1902/2245 train_time:225336ms step_avg:118.47ms
step:1903/2245 train_time:225453ms step_avg:118.47ms
step:1904/2245 train_time:225576ms step_avg:118.47ms
step:1905/2245 train_time:225692ms step_avg:118.47ms
step:1906/2245 train_time:225815ms step_avg:118.48ms
step:1907/2245 train_time:225932ms step_avg:118.47ms
step:1908/2245 train_time:226055ms step_avg:118.48ms
step:1909/2245 train_time:226171ms step_avg:118.48ms
step:1910/2245 train_time:226294ms step_avg:118.48ms
step:1911/2245 train_time:226411ms step_avg:118.48ms
step:1912/2245 train_time:226534ms step_avg:118.48ms
step:1913/2245 train_time:226650ms step_avg:118.48ms
step:1914/2245 train_time:226774ms step_avg:118.48ms
step:1915/2245 train_time:226890ms step_avg:118.48ms
step:1916/2245 train_time:227013ms step_avg:118.48ms
step:1917/2245 train_time:227129ms step_avg:118.48ms
step:1918/2245 train_time:227252ms step_avg:118.48ms
step:1919/2245 train_time:227369ms step_avg:118.48ms
step:1920/2245 train_time:227492ms step_avg:118.49ms
step:1921/2245 train_time:227608ms step_avg:118.48ms
step:1922/2245 train_time:227731ms step_avg:118.49ms
step:1923/2245 train_time:227848ms step_avg:118.49ms
step:1924/2245 train_time:227971ms step_avg:118.49ms
step:1925/2245 train_time:228087ms step_avg:118.49ms
step:1926/2245 train_time:228210ms step_avg:118.49ms
step:1927/2245 train_time:228326ms step_avg:118.49ms
step:1928/2245 train_time:228449ms step_avg:118.49ms
step:1929/2245 train_time:228565ms step_avg:118.49ms
step:1930/2245 train_time:228688ms step_avg:118.49ms
step:1931/2245 train_time:228805ms step_avg:118.49ms
step:1932/2245 train_time:228927ms step_avg:118.49ms
step:1933/2245 train_time:229044ms step_avg:118.49ms
step:1934/2245 train_time:229167ms step_avg:118.49ms
step:1935/2245 train_time:229283ms step_avg:118.49ms
step:1936/2245 train_time:229407ms step_avg:118.50ms
step:1937/2245 train_time:229523ms step_avg:118.49ms
step:1938/2245 train_time:229645ms step_avg:118.50ms
step:1939/2245 train_time:229762ms step_avg:118.50ms
step:1940/2245 train_time:229885ms step_avg:118.50ms
step:1941/2245 train_time:230002ms step_avg:118.50ms
step:1942/2245 train_time:230125ms step_avg:118.50ms
step:1943/2245 train_time:230242ms step_avg:118.50ms
step:1944/2245 train_time:230365ms step_avg:118.50ms
step:1945/2245 train_time:230482ms step_avg:118.50ms
step:1946/2245 train_time:230604ms step_avg:118.50ms
step:1947/2245 train_time:230721ms step_avg:118.50ms
step:1948/2245 train_time:230844ms step_avg:118.50ms
step:1949/2245 train_time:230961ms step_avg:118.50ms
step:1950/2245 train_time:231083ms step_avg:118.50ms
step:1951/2245 train_time:231201ms step_avg:118.50ms
step:1952/2245 train_time:231323ms step_avg:118.51ms
step:1953/2245 train_time:231439ms step_avg:118.50ms
step:1954/2245 train_time:231562ms step_avg:118.51ms
step:1955/2245 train_time:231679ms step_avg:118.51ms
step:1956/2245 train_time:231801ms step_avg:118.51ms
step:1957/2245 train_time:231918ms step_avg:118.51ms
step:1958/2245 train_time:232041ms step_avg:118.51ms
step:1959/2245 train_time:232158ms step_avg:118.51ms
step:1960/2245 train_time:232281ms step_avg:118.51ms
step:1961/2245 train_time:232398ms step_avg:118.51ms
step:1962/2245 train_time:232521ms step_avg:118.51ms
step:1963/2245 train_time:232637ms step_avg:118.51ms
step:1964/2245 train_time:232760ms step_avg:118.51ms
step:1965/2245 train_time:232877ms step_avg:118.51ms
step:1966/2245 train_time:233000ms step_avg:118.51ms
step:1967/2245 train_time:233117ms step_avg:118.51ms
step:1968/2245 train_time:233240ms step_avg:118.52ms
step:1969/2245 train_time:233357ms step_avg:118.52ms
step:1970/2245 train_time:233479ms step_avg:118.52ms
step:1971/2245 train_time:233596ms step_avg:118.52ms
step:1972/2245 train_time:233719ms step_avg:118.52ms
step:1973/2245 train_time:233836ms step_avg:118.52ms
step:1974/2245 train_time:233959ms step_avg:118.52ms
step:1975/2245 train_time:234075ms step_avg:118.52ms
step:1976/2245 train_time:234199ms step_avg:118.52ms
step:1977/2245 train_time:234315ms step_avg:118.52ms
step:1978/2245 train_time:234439ms step_avg:118.52ms
step:1979/2245 train_time:234555ms step_avg:118.52ms
step:1980/2245 train_time:234678ms step_avg:118.52ms
step:1981/2245 train_time:234794ms step_avg:118.52ms
step:1982/2245 train_time:234917ms step_avg:118.53ms
step:1983/2245 train_time:235034ms step_avg:118.52ms
step:1984/2245 train_time:235157ms step_avg:118.53ms
step:1985/2245 train_time:235273ms step_avg:118.53ms
step:1986/2245 train_time:235396ms step_avg:118.53ms
step:1987/2245 train_time:235513ms step_avg:118.53ms
step:1988/2245 train_time:235635ms step_avg:118.53ms
step:1989/2245 train_time:235752ms step_avg:118.53ms
step:1990/2245 train_time:235875ms step_avg:118.53ms
step:1991/2245 train_time:235992ms step_avg:118.53ms
step:1992/2245 train_time:236114ms step_avg:118.53ms
step:1993/2245 train_time:236232ms step_avg:118.53ms
step:1994/2245 train_time:236355ms step_avg:118.53ms
step:1995/2245 train_time:236472ms step_avg:118.53ms
step:1996/2245 train_time:236595ms step_avg:118.53ms
step:1997/2245 train_time:236712ms step_avg:118.53ms
step:1998/2245 train_time:236835ms step_avg:118.54ms
step:1999/2245 train_time:236952ms step_avg:118.54ms
step:2000/2245 train_time:237075ms step_avg:118.54ms
step:2000/2245 val_loss:3.3240 train_time:237141ms step_avg:118.57ms
step:2001/2245 train_time:237192ms step_avg:118.54ms
step:2002/2245 train_time:237313ms step_avg:118.54ms
step:2003/2245 train_time:237430ms step_avg:118.54ms
step:2004/2245 train_time:237553ms step_avg:118.54ms
step:2005/2245 train_time:237669ms step_avg:118.54ms
step:2006/2245 train_time:237793ms step_avg:118.54ms
step:2007/2245 train_time:237909ms step_avg:118.54ms
step:2008/2245 train_time:238032ms step_avg:118.54ms
step:2009/2245 train_time:238149ms step_avg:118.54ms
step:2010/2245 train_time:238273ms step_avg:118.54ms
step:2011/2245 train_time:238390ms step_avg:118.54ms
step:2012/2245 train_time:238513ms step_avg:118.55ms
step:2013/2245 train_time:238630ms step_avg:118.54ms
step:2014/2245 train_time:238753ms step_avg:118.55ms
step:2015/2245 train_time:238869ms step_avg:118.55ms
step:2016/2245 train_time:238992ms step_avg:118.55ms
step:2017/2245 train_time:239109ms step_avg:118.55ms
step:2018/2245 train_time:239233ms step_avg:118.55ms
step:2019/2245 train_time:239349ms step_avg:118.55ms
step:2020/2245 train_time:239473ms step_avg:118.55ms
step:2021/2245 train_time:239590ms step_avg:118.55ms
step:2022/2245 train_time:239712ms step_avg:118.55ms
step:2023/2245 train_time:239829ms step_avg:118.55ms
step:2024/2245 train_time:239952ms step_avg:118.55ms
step:2025/2245 train_time:240068ms step_avg:118.55ms
step:2026/2245 train_time:240190ms step_avg:118.55ms
step:2027/2245 train_time:240307ms step_avg:118.55ms
step:2028/2245 train_time:240430ms step_avg:118.56ms
step:2029/2245 train_time:240545ms step_avg:118.55ms
step:2030/2245 train_time:240668ms step_avg:118.56ms
step:2031/2245 train_time:240785ms step_avg:118.55ms
step:2032/2245 train_time:240907ms step_avg:118.56ms
step:2033/2245 train_time:241024ms step_avg:118.56ms
step:2034/2245 train_time:241146ms step_avg:118.56ms
step:2035/2245 train_time:241263ms step_avg:118.56ms
step:2036/2245 train_time:241386ms step_avg:118.56ms
step:2037/2245 train_time:241502ms step_avg:118.56ms
step:2038/2245 train_time:241625ms step_avg:118.56ms
step:2039/2245 train_time:241742ms step_avg:118.56ms
step:2040/2245 train_time:241864ms step_avg:118.56ms
step:2041/2245 train_time:241982ms step_avg:118.56ms
step:2042/2245 train_time:242104ms step_avg:118.56ms
step:2043/2245 train_time:242221ms step_avg:118.56ms
step:2044/2245 train_time:242344ms step_avg:118.56ms
step:2045/2245 train_time:242460ms step_avg:118.56ms
step:2046/2245 train_time:242584ms step_avg:118.56ms
step:2047/2245 train_time:242700ms step_avg:118.56ms
step:2048/2245 train_time:242823ms step_avg:118.57ms
step:2049/2245 train_time:242940ms step_avg:118.56ms
step:2050/2245 train_time:243062ms step_avg:118.57ms
step:2051/2245 train_time:243179ms step_avg:118.57ms
step:2052/2245 train_time:243301ms step_avg:118.57ms
step:2053/2245 train_time:243418ms step_avg:118.57ms
step:2054/2245 train_time:243540ms step_avg:118.57ms
step:2055/2245 train_time:243658ms step_avg:118.57ms
step:2056/2245 train_time:243780ms step_avg:118.57ms
step:2057/2245 train_time:243897ms step_avg:118.57ms
step:2058/2245 train_time:244020ms step_avg:118.57ms
step:2059/2245 train_time:244136ms step_avg:118.57ms
step:2060/2245 train_time:244259ms step_avg:118.57ms
step:2061/2245 train_time:244376ms step_avg:118.57ms
step:2062/2245 train_time:244498ms step_avg:118.57ms
step:2063/2245 train_time:244616ms step_avg:118.57ms
step:2064/2245 train_time:244739ms step_avg:118.58ms
step:2065/2245 train_time:244856ms step_avg:118.57ms
step:2066/2245 train_time:244979ms step_avg:118.58ms
step:2067/2245 train_time:245095ms step_avg:118.58ms
step:2068/2245 train_time:245219ms step_avg:118.58ms
step:2069/2245 train_time:245335ms step_avg:118.58ms
step:2070/2245 train_time:245458ms step_avg:118.58ms
step:2071/2245 train_time:245575ms step_avg:118.58ms
step:2072/2245 train_time:245698ms step_avg:118.58ms
step:2073/2245 train_time:245814ms step_avg:118.58ms
step:2074/2245 train_time:245937ms step_avg:118.58ms
step:2075/2245 train_time:246054ms step_avg:118.58ms
step:2076/2245 train_time:246178ms step_avg:118.58ms
step:2077/2245 train_time:246295ms step_avg:118.58ms
step:2078/2245 train_time:246417ms step_avg:118.58ms
step:2079/2245 train_time:246534ms step_avg:118.58ms
step:2080/2245 train_time:246657ms step_avg:118.59ms
step:2081/2245 train_time:246774ms step_avg:118.58ms
step:2082/2245 train_time:246897ms step_avg:118.59ms
step:2083/2245 train_time:247013ms step_avg:118.59ms
step:2084/2245 train_time:247137ms step_avg:118.59ms
step:2085/2245 train_time:247254ms step_avg:118.59ms
step:2086/2245 train_time:247377ms step_avg:118.59ms
step:2087/2245 train_time:247493ms step_avg:118.59ms
step:2088/2245 train_time:247616ms step_avg:118.59ms
step:2089/2245 train_time:247733ms step_avg:118.59ms
step:2090/2245 train_time:247855ms step_avg:118.59ms
step:2091/2245 train_time:247972ms step_avg:118.59ms
step:2092/2245 train_time:248095ms step_avg:118.59ms
step:2093/2245 train_time:248212ms step_avg:118.59ms
step:2094/2245 train_time:248335ms step_avg:118.59ms
step:2095/2245 train_time:248451ms step_avg:118.59ms
step:2096/2245 train_time:248574ms step_avg:118.59ms
step:2097/2245 train_time:248691ms step_avg:118.59ms
step:2098/2245 train_time:248814ms step_avg:118.60ms
step:2099/2245 train_time:248930ms step_avg:118.59ms
step:2100/2245 train_time:249053ms step_avg:118.60ms
step:2101/2245 train_time:249170ms step_avg:118.60ms
step:2102/2245 train_time:249293ms step_avg:118.60ms
step:2103/2245 train_time:249410ms step_avg:118.60ms
step:2104/2245 train_time:249532ms step_avg:118.60ms
step:2105/2245 train_time:249650ms step_avg:118.60ms
step:2106/2245 train_time:249772ms step_avg:118.60ms
step:2107/2245 train_time:249889ms step_avg:118.60ms
step:2108/2245 train_time:250012ms step_avg:118.60ms
step:2109/2245 train_time:250129ms step_avg:118.60ms
step:2110/2245 train_time:250252ms step_avg:118.60ms
step:2111/2245 train_time:250369ms step_avg:118.60ms
step:2112/2245 train_time:250492ms step_avg:118.60ms
step:2113/2245 train_time:250608ms step_avg:118.60ms
step:2114/2245 train_time:250731ms step_avg:118.60ms
step:2115/2245 train_time:250847ms step_avg:118.60ms
step:2116/2245 train_time:250970ms step_avg:118.61ms
step:2117/2245 train_time:251086ms step_avg:118.60ms
step:2118/2245 train_time:251209ms step_avg:118.61ms
step:2119/2245 train_time:251325ms step_avg:118.61ms
step:2120/2245 train_time:251448ms step_avg:118.61ms
step:2121/2245 train_time:251564ms step_avg:118.61ms
step:2122/2245 train_time:251687ms step_avg:118.61ms
step:2123/2245 train_time:251804ms step_avg:118.61ms
step:2124/2245 train_time:251927ms step_avg:118.61ms
step:2125/2245 train_time:252043ms step_avg:118.61ms
step:2126/2245 train_time:252166ms step_avg:118.61ms
step:2127/2245 train_time:252281ms step_avg:118.61ms
step:2128/2245 train_time:252404ms step_avg:118.61ms
step:2129/2245 train_time:252522ms step_avg:118.61ms
step:2130/2245 train_time:252645ms step_avg:118.61ms
step:2131/2245 train_time:252762ms step_avg:118.61ms
step:2132/2245 train_time:252885ms step_avg:118.61ms
step:2133/2245 train_time:253001ms step_avg:118.61ms
step:2134/2245 train_time:253125ms step_avg:118.62ms
step:2135/2245 train_time:253241ms step_avg:118.61ms
step:2136/2245 train_time:253364ms step_avg:118.62ms
step:2137/2245 train_time:253479ms step_avg:118.61ms
step:2138/2245 train_time:253602ms step_avg:118.62ms
step:2139/2245 train_time:253719ms step_avg:118.62ms
step:2140/2245 train_time:253842ms step_avg:118.62ms
step:2141/2245 train_time:253958ms step_avg:118.62ms
step:2142/2245 train_time:254081ms step_avg:118.62ms
step:2143/2245 train_time:254197ms step_avg:118.62ms
step:2144/2245 train_time:254321ms step_avg:118.62ms
step:2145/2245 train_time:254437ms step_avg:118.62ms
step:2146/2245 train_time:254559ms step_avg:118.62ms
step:2147/2245 train_time:254676ms step_avg:118.62ms
step:2148/2245 train_time:254799ms step_avg:118.62ms
step:2149/2245 train_time:254915ms step_avg:118.62ms
step:2150/2245 train_time:255038ms step_avg:118.62ms
step:2151/2245 train_time:255155ms step_avg:118.62ms
step:2152/2245 train_time:255278ms step_avg:118.62ms
step:2153/2245 train_time:255395ms step_avg:118.62ms
step:2154/2245 train_time:255519ms step_avg:118.63ms
step:2155/2245 train_time:255635ms step_avg:118.62ms
step:2156/2245 train_time:255759ms step_avg:118.63ms
step:2157/2245 train_time:255875ms step_avg:118.63ms
step:2158/2245 train_time:255998ms step_avg:118.63ms
step:2159/2245 train_time:256115ms step_avg:118.63ms
step:2160/2245 train_time:256237ms step_avg:118.63ms
step:2161/2245 train_time:256354ms step_avg:118.63ms
step:2162/2245 train_time:256477ms step_avg:118.63ms
step:2163/2245 train_time:256593ms step_avg:118.63ms
step:2164/2245 train_time:256717ms step_avg:118.63ms
step:2165/2245 train_time:256833ms step_avg:118.63ms
step:2166/2245 train_time:256956ms step_avg:118.63ms
step:2167/2245 train_time:257072ms step_avg:118.63ms
step:2168/2245 train_time:257196ms step_avg:118.63ms
step:2169/2245 train_time:257312ms step_avg:118.63ms
step:2170/2245 train_time:257435ms step_avg:118.63ms
step:2171/2245 train_time:257551ms step_avg:118.63ms
step:2172/2245 train_time:257674ms step_avg:118.63ms
step:2173/2245 train_time:257791ms step_avg:118.63ms
step:2174/2245 train_time:257915ms step_avg:118.64ms
step:2175/2245 train_time:258032ms step_avg:118.64ms
step:2176/2245 train_time:258155ms step_avg:118.64ms
step:2177/2245 train_time:258272ms step_avg:118.64ms
step:2178/2245 train_time:258395ms step_avg:118.64ms
step:2179/2245 train_time:258511ms step_avg:118.64ms
step:2180/2245 train_time:258635ms step_avg:118.64ms
step:2181/2245 train_time:258751ms step_avg:118.64ms
step:2182/2245 train_time:258874ms step_avg:118.64ms
step:2183/2245 train_time:258991ms step_avg:118.64ms
step:2184/2245 train_time:259114ms step_avg:118.64ms
step:2185/2245 train_time:259231ms step_avg:118.64ms
step:2186/2245 train_time:259354ms step_avg:118.64ms
step:2187/2245 train_time:259470ms step_avg:118.64ms
step:2188/2245 train_time:259593ms step_avg:118.64ms
step:2189/2245 train_time:259710ms step_avg:118.64ms
step:2190/2245 train_time:259833ms step_avg:118.65ms
step:2191/2245 train_time:259950ms step_avg:118.64ms
step:2192/2245 train_time:260073ms step_avg:118.65ms
step:2193/2245 train_time:260190ms step_avg:118.65ms
step:2194/2245 train_time:260313ms step_avg:118.65ms
step:2195/2245 train_time:260430ms step_avg:118.65ms
step:2196/2245 train_time:260553ms step_avg:118.65ms
step:2197/2245 train_time:260669ms step_avg:118.65ms
step:2198/2245 train_time:260792ms step_avg:118.65ms
step:2199/2245 train_time:260909ms step_avg:118.65ms
step:2200/2245 train_time:261032ms step_avg:118.65ms
step:2201/2245 train_time:261148ms step_avg:118.65ms
step:2202/2245 train_time:261271ms step_avg:118.65ms
step:2203/2245 train_time:261388ms step_avg:118.65ms
step:2204/2245 train_time:261511ms step_avg:118.65ms
step:2205/2245 train_time:261629ms step_avg:118.65ms
step:2206/2245 train_time:261752ms step_avg:118.65ms
step:2207/2245 train_time:261868ms step_avg:118.65ms
step:2208/2245 train_time:261992ms step_avg:118.66ms
step:2209/2245 train_time:262110ms step_avg:118.66ms
step:2210/2245 train_time:262233ms step_avg:118.66ms
step:2211/2245 train_time:262350ms step_avg:118.66ms
step:2212/2245 train_time:262474ms step_avg:118.66ms
step:2213/2245 train_time:262591ms step_avg:118.66ms
step:2214/2245 train_time:262715ms step_avg:118.66ms
step:2215/2245 train_time:262832ms step_avg:118.66ms
step:2216/2245 train_time:262956ms step_avg:118.66ms
step:2217/2245 train_time:263072ms step_avg:118.66ms
step:2218/2245 train_time:263195ms step_avg:118.66ms
step:2219/2245 train_time:263313ms step_avg:118.66ms
step:2220/2245 train_time:263437ms step_avg:118.67ms
step:2221/2245 train_time:263553ms step_avg:118.66ms
step:2222/2245 train_time:263676ms step_avg:118.67ms
step:2223/2245 train_time:263793ms step_avg:118.67ms
step:2224/2245 train_time:263916ms step_avg:118.67ms
step:2225/2245 train_time:264033ms step_avg:118.67ms
step:2226/2245 train_time:264157ms step_avg:118.67ms
step:2227/2245 train_time:264274ms step_avg:118.67ms
step:2228/2245 train_time:264397ms step_avg:118.67ms
step:2229/2245 train_time:264514ms step_avg:118.67ms
step:2230/2245 train_time:264638ms step_avg:118.67ms
step:2231/2245 train_time:264755ms step_avg:118.67ms
step:2232/2245 train_time:264878ms step_avg:118.67ms
step:2233/2245 train_time:264995ms step_avg:118.67ms
step:2234/2245 train_time:265118ms step_avg:118.67ms
step:2235/2245 train_time:265235ms step_avg:118.67ms
step:2236/2245 train_time:265358ms step_avg:118.68ms
step:2237/2245 train_time:265475ms step_avg:118.67ms
step:2238/2245 train_time:265598ms step_avg:118.68ms
step:2239/2245 train_time:265715ms step_avg:118.68ms
step:2240/2245 train_time:265839ms step_avg:118.68ms
step:2241/2245 train_time:265956ms step_avg:118.68ms
step:2242/2245 train_time:266079ms step_avg:118.68ms
step:2243/2245 train_time:266195ms step_avg:118.68ms
step:2244/2245 train_time:266318ms step_avg:118.68ms
step:2245/2245 train_time:266435ms step_avg:118.68ms
step:2245/2245 val_loss:3.2782 train_time:266504ms step_avg:118.71ms
peak memory allocated: 30321 MiB reserved: 44436 MiB

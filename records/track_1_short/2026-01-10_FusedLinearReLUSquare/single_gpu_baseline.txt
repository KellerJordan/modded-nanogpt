import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.3 (main, Nov  6 2025, 13:44:16) [GCC 13.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Jan 10 23:36:12 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:8D:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1183MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           17198      C   /home/ubuntu/venv/bin/python3          1174MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8297 train_time:0ms step_avg:0.02ms
step:1/1775 train_time:247ms step_avg:246.90ms
step:2/1775 train_time:498ms step_avg:248.98ms
step:3/1775 train_time:750ms step_avg:250.13ms
step:4/1775 train_time:1001ms step_avg:250.28ms
step:5/1775 train_time:1254ms step_avg:250.88ms
step:6/1775 train_time:1507ms step_avg:251.16ms
step:7/1775 train_time:1760ms step_avg:251.43ms
step:8/1775 train_time:2012ms step_avg:251.47ms
step:9/1775 train_time:2264ms step_avg:251.51ms
step:10/1775 train_time:2514ms step_avg:251.43ms
step:11/1775 train_time:2767ms step_avg:251.51ms
step:12/1775 train_time:3020ms step_avg:251.66ms
step:13/1775 train_time:3272ms step_avg:251.73ms
step:14/1775 train_time:3524ms step_avg:251.74ms
step:15/1775 train_time:3775ms step_avg:251.70ms
step:16/1775 train_time:4027ms step_avg:251.71ms
step:17/1775 train_time:4282ms step_avg:251.88ms
step:18/1775 train_time:4535ms step_avg:251.93ms
step:19/1775 train_time:4787ms step_avg:251.95ms
step:20/1775 train_time:5040ms step_avg:252.02ms
step:21/1775 train_time:5293ms step_avg:252.04ms
step:22/1775 train_time:5545ms step_avg:252.04ms
step:23/1775 train_time:5797ms step_avg:252.04ms
step:24/1775 train_time:6048ms step_avg:252.01ms
step:25/1775 train_time:6303ms step_avg:252.11ms
step:26/1775 train_time:6554ms step_avg:252.08ms
step:27/1775 train_time:6806ms step_avg:252.07ms
step:28/1775 train_time:7058ms step_avg:252.08ms
step:29/1775 train_time:7311ms step_avg:252.11ms
step:30/1775 train_time:7563ms step_avg:252.10ms
step:31/1775 train_time:7815ms step_avg:252.11ms
step:32/1775 train_time:8068ms step_avg:252.12ms
step:33/1775 train_time:8322ms step_avg:252.18ms
step:34/1775 train_time:8574ms step_avg:252.19ms
step:35/1775 train_time:8826ms step_avg:252.18ms
step:36/1775 train_time:9077ms step_avg:252.14ms
step:37/1775 train_time:9331ms step_avg:252.20ms
step:38/1775 train_time:9584ms step_avg:252.21ms
step:39/1775 train_time:9836ms step_avg:252.20ms
step:40/1775 train_time:10089ms step_avg:252.22ms
step:41/1775 train_time:10341ms step_avg:252.22ms
step:42/1775 train_time:10592ms step_avg:252.19ms
step:43/1775 train_time:10844ms step_avg:252.19ms
step:44/1775 train_time:11095ms step_avg:252.17ms
step:45/1775 train_time:11350ms step_avg:252.22ms
step:46/1775 train_time:11602ms step_avg:252.22ms
step:47/1775 train_time:11855ms step_avg:252.23ms
step:48/1775 train_time:12108ms step_avg:252.24ms
step:49/1775 train_time:12361ms step_avg:252.26ms
step:50/1775 train_time:12613ms step_avg:252.27ms
step:51/1775 train_time:12865ms step_avg:252.26ms
step:52/1775 train_time:13118ms step_avg:252.28ms
step:53/1775 train_time:13371ms step_avg:252.28ms
step:54/1775 train_time:13624ms step_avg:252.29ms
step:55/1775 train_time:13875ms step_avg:252.28ms
step:56/1775 train_time:14127ms step_avg:252.27ms
step:57/1775 train_time:14380ms step_avg:252.28ms
step:58/1775 train_time:14633ms step_avg:252.29ms
step:59/1775 train_time:14885ms step_avg:252.28ms
step:60/1775 train_time:15137ms step_avg:252.28ms
step:61/1775 train_time:15390ms step_avg:252.29ms
step:62/1775 train_time:15642ms step_avg:252.30ms
step:63/1775 train_time:15895ms step_avg:252.30ms
step:64/1775 train_time:16147ms step_avg:252.29ms
step:65/1775 train_time:16400ms step_avg:252.31ms
step:66/1775 train_time:16653ms step_avg:252.31ms
step:67/1775 train_time:16904ms step_avg:252.30ms
step:68/1775 train_time:17154ms step_avg:252.27ms
step:69/1775 train_time:17407ms step_avg:252.27ms
step:70/1775 train_time:17662ms step_avg:252.31ms
step:71/1775 train_time:17914ms step_avg:252.31ms
step:72/1775 train_time:18165ms step_avg:252.30ms
step:73/1775 train_time:18417ms step_avg:252.29ms
step:74/1775 train_time:18671ms step_avg:252.31ms
step:75/1775 train_time:18923ms step_avg:252.31ms
step:76/1775 train_time:19175ms step_avg:252.30ms
step:77/1775 train_time:19427ms step_avg:252.29ms
step:78/1775 train_time:19680ms step_avg:252.30ms
step:79/1775 train_time:19932ms step_avg:252.30ms
step:80/1775 train_time:20183ms step_avg:252.29ms
step:81/1775 train_time:20434ms step_avg:252.27ms
step:82/1775 train_time:20687ms step_avg:252.28ms
step:83/1775 train_time:20940ms step_avg:252.29ms
step:84/1775 train_time:21193ms step_avg:252.29ms
step:85/1775 train_time:21445ms step_avg:252.29ms
step:86/1775 train_time:21696ms step_avg:252.28ms
step:87/1775 train_time:21949ms step_avg:252.29ms
step:88/1775 train_time:22201ms step_avg:252.28ms
step:89/1775 train_time:22453ms step_avg:252.28ms
step:90/1775 train_time:22704ms step_avg:252.26ms
step:91/1775 train_time:22956ms step_avg:252.26ms
step:92/1775 train_time:23209ms step_avg:252.28ms
step:93/1775 train_time:23463ms step_avg:252.29ms
step:94/1775 train_time:23714ms step_avg:252.27ms
step:95/1775 train_time:23967ms step_avg:252.28ms
step:96/1775 train_time:24219ms step_avg:252.28ms
step:97/1775 train_time:24472ms step_avg:252.29ms
step:98/1775 train_time:24723ms step_avg:252.28ms
step:99/1775 train_time:24975ms step_avg:252.27ms
step:100/1775 train_time:25225ms step_avg:252.25ms
step:101/1775 train_time:25477ms step_avg:252.25ms
step:102/1775 train_time:25730ms step_avg:252.26ms
step:103/1775 train_time:25981ms step_avg:252.25ms
step:104/1775 train_time:26233ms step_avg:252.24ms
step:105/1775 train_time:26484ms step_avg:252.23ms
step:106/1775 train_time:26736ms step_avg:252.22ms
step:107/1775 train_time:26988ms step_avg:252.23ms
step:108/1775 train_time:27240ms step_avg:252.22ms
step:109/1775 train_time:27493ms step_avg:252.23ms
step:110/1775 train_time:27745ms step_avg:252.23ms
step:111/1775 train_time:27998ms step_avg:252.24ms
step:112/1775 train_time:28251ms step_avg:252.24ms
step:113/1775 train_time:28504ms step_avg:252.25ms
step:114/1775 train_time:28756ms step_avg:252.24ms
step:115/1775 train_time:29008ms step_avg:252.25ms
step:116/1775 train_time:29261ms step_avg:252.25ms
step:117/1775 train_time:29513ms step_avg:252.25ms
step:118/1775 train_time:29765ms step_avg:252.24ms
step:119/1775 train_time:30017ms step_avg:252.25ms
step:120/1775 train_time:30271ms step_avg:252.26ms
step:121/1775 train_time:30524ms step_avg:252.27ms
step:122/1775 train_time:30776ms step_avg:252.26ms
step:123/1775 train_time:31029ms step_avg:252.27ms
step:124/1775 train_time:31282ms step_avg:252.27ms
step:125/1775 train_time:31535ms step_avg:252.28ms
step:126/1775 train_time:31787ms step_avg:252.28ms
step:127/1775 train_time:32040ms step_avg:252.29ms
step:128/1775 train_time:32292ms step_avg:252.28ms
step:129/1775 train_time:32545ms step_avg:252.29ms
step:130/1775 train_time:32797ms step_avg:252.28ms
step:131/1775 train_time:33050ms step_avg:252.29ms
step:132/1775 train_time:33301ms step_avg:252.28ms
step:133/1775 train_time:33554ms step_avg:252.28ms
step:134/1775 train_time:33805ms step_avg:252.28ms
step:135/1775 train_time:34058ms step_avg:252.28ms
step:136/1775 train_time:34310ms step_avg:252.28ms
step:137/1775 train_time:34561ms step_avg:252.27ms
step:138/1775 train_time:34813ms step_avg:252.27ms
step:139/1775 train_time:35066ms step_avg:252.27ms
step:140/1775 train_time:35319ms step_avg:252.28ms
step:141/1775 train_time:35570ms step_avg:252.27ms
step:142/1775 train_time:35822ms step_avg:252.27ms
step:143/1775 train_time:36073ms step_avg:252.26ms
step:144/1775 train_time:36326ms step_avg:252.26ms
step:145/1775 train_time:36579ms step_avg:252.27ms
step:146/1775 train_time:36832ms step_avg:252.27ms
step:147/1775 train_time:37084ms step_avg:252.27ms
step:148/1775 train_time:37336ms step_avg:252.27ms
step:149/1775 train_time:37588ms step_avg:252.27ms
step:150/1775 train_time:37841ms step_avg:252.28ms
step:151/1775 train_time:38094ms step_avg:252.28ms
step:152/1775 train_time:38346ms step_avg:252.28ms
step:153/1775 train_time:38598ms step_avg:252.28ms
step:154/1775 train_time:38850ms step_avg:252.27ms
step:155/1775 train_time:39102ms step_avg:252.27ms
step:156/1775 train_time:39353ms step_avg:252.26ms
step:157/1775 train_time:39606ms step_avg:252.27ms
step:158/1775 train_time:39860ms step_avg:252.28ms
step:159/1775 train_time:40112ms step_avg:252.28ms
step:160/1775 train_time:40363ms step_avg:252.27ms
step:161/1775 train_time:40614ms step_avg:252.26ms
step:162/1775 train_time:40864ms step_avg:252.25ms
step:163/1775 train_time:41116ms step_avg:252.24ms
step:164/1775 train_time:41367ms step_avg:252.24ms
step:165/1775 train_time:41618ms step_avg:252.23ms
step:166/1775 train_time:41871ms step_avg:252.24ms
step:167/1775 train_time:42123ms step_avg:252.23ms
step:168/1775 train_time:42374ms step_avg:252.22ms
step:169/1775 train_time:42625ms step_avg:252.22ms
step:170/1775 train_time:42876ms step_avg:252.21ms
step:171/1775 train_time:43129ms step_avg:252.22ms
step:172/1775 train_time:43380ms step_avg:252.21ms
step:173/1775 train_time:43633ms step_avg:252.21ms
step:174/1775 train_time:43884ms step_avg:252.20ms
step:175/1775 train_time:44136ms step_avg:252.21ms
step:176/1775 train_time:44388ms step_avg:252.21ms
step:177/1775 train_time:44640ms step_avg:252.20ms
step:178/1775 train_time:44892ms step_avg:252.20ms
step:179/1775 train_time:45144ms step_avg:252.20ms
step:180/1775 train_time:45396ms step_avg:252.20ms
step:181/1775 train_time:45649ms step_avg:252.20ms
step:182/1775 train_time:45901ms step_avg:252.20ms
step:183/1775 train_time:46153ms step_avg:252.20ms
step:184/1775 train_time:46403ms step_avg:252.19ms
step:185/1775 train_time:46656ms step_avg:252.19ms
step:186/1775 train_time:46908ms step_avg:252.19ms
step:187/1775 train_time:47159ms step_avg:252.19ms
step:188/1775 train_time:47410ms step_avg:252.18ms
step:189/1775 train_time:47662ms step_avg:252.18ms
step:190/1775 train_time:47913ms step_avg:252.17ms
step:191/1775 train_time:48165ms step_avg:252.17ms
step:192/1775 train_time:48418ms step_avg:252.18ms
step:193/1775 train_time:48670ms step_avg:252.18ms
step:194/1775 train_time:48922ms step_avg:252.18ms
step:195/1775 train_time:49173ms step_avg:252.17ms
step:196/1775 train_time:49424ms step_avg:252.16ms
step:197/1775 train_time:49675ms step_avg:252.16ms
step:198/1775 train_time:49926ms step_avg:252.15ms
step:199/1775 train_time:50179ms step_avg:252.15ms
step:200/1775 train_time:50432ms step_avg:252.16ms
step:201/1775 train_time:50684ms step_avg:252.16ms
step:202/1775 train_time:50935ms step_avg:252.15ms
step:203/1775 train_time:51186ms step_avg:252.15ms
step:204/1775 train_time:51437ms step_avg:252.14ms
step:205/1775 train_time:51690ms step_avg:252.15ms
step:206/1775 train_time:51942ms step_avg:252.15ms
step:207/1775 train_time:52193ms step_avg:252.14ms
step:208/1775 train_time:52444ms step_avg:252.13ms
step:209/1775 train_time:52695ms step_avg:252.13ms
step:210/1775 train_time:52947ms step_avg:252.13ms
step:211/1775 train_time:53201ms step_avg:252.14ms
step:212/1775 train_time:53453ms step_avg:252.14ms
step:213/1775 train_time:53704ms step_avg:252.13ms
step:214/1775 train_time:53955ms step_avg:252.13ms
step:215/1775 train_time:54208ms step_avg:252.13ms
step:216/1775 train_time:54459ms step_avg:252.13ms
step:217/1775 train_time:54711ms step_avg:252.13ms
step:218/1775 train_time:54962ms step_avg:252.12ms
step:219/1775 train_time:55214ms step_avg:252.12ms
step:220/1775 train_time:55465ms step_avg:252.11ms
step:221/1775 train_time:55717ms step_avg:252.11ms
step:222/1775 train_time:55967ms step_avg:252.11ms
step:223/1775 train_time:56219ms step_avg:252.11ms
step:224/1775 train_time:56471ms step_avg:252.10ms
step:225/1775 train_time:56722ms step_avg:252.10ms
step:226/1775 train_time:56973ms step_avg:252.09ms
step:227/1775 train_time:57224ms step_avg:252.09ms
step:228/1775 train_time:57474ms step_avg:252.08ms
step:229/1775 train_time:57726ms step_avg:252.08ms
step:230/1775 train_time:57977ms step_avg:252.08ms
step:231/1775 train_time:58230ms step_avg:252.08ms
step:232/1775 train_time:58481ms step_avg:252.07ms
step:233/1775 train_time:58732ms step_avg:252.07ms
step:234/1775 train_time:58984ms step_avg:252.07ms
step:235/1775 train_time:59235ms step_avg:252.06ms
step:236/1775 train_time:59486ms step_avg:252.06ms
step:237/1775 train_time:59738ms step_avg:252.06ms
step:238/1775 train_time:59990ms step_avg:252.06ms
step:239/1775 train_time:60243ms step_avg:252.06ms
step:240/1775 train_time:60494ms step_avg:252.06ms
step:241/1775 train_time:60745ms step_avg:252.05ms
step:242/1775 train_time:60998ms step_avg:252.06ms
step:243/1775 train_time:61251ms step_avg:252.06ms
step:244/1775 train_time:61503ms step_avg:252.06ms
step:245/1775 train_time:61754ms step_avg:252.06ms
step:246/1775 train_time:62004ms step_avg:252.05ms
step:247/1775 train_time:62256ms step_avg:252.05ms
step:248/1775 train_time:62507ms step_avg:252.04ms
step:249/1775 train_time:62758ms step_avg:252.04ms
step:250/1775 train_time:63011ms step_avg:252.04ms
step:250/1775 val_loss:4.6011 train_time:63055ms step_avg:252.22ms
step:251/1775 train_time:63264ms step_avg:252.05ms
step:252/1775 train_time:63513ms step_avg:252.04ms
step:253/1775 train_time:63764ms step_avg:252.03ms
step:254/1775 train_time:64016ms step_avg:252.03ms
step:255/1775 train_time:64268ms step_avg:252.03ms
step:256/1775 train_time:64517ms step_avg:252.02ms
step:257/1775 train_time:64767ms step_avg:252.01ms
step:258/1775 train_time:65018ms step_avg:252.01ms
step:259/1775 train_time:65269ms step_avg:252.00ms
step:260/1775 train_time:65520ms step_avg:252.00ms
step:261/1775 train_time:65769ms step_avg:251.99ms
step:262/1775 train_time:66019ms step_avg:251.98ms
step:263/1775 train_time:66270ms step_avg:251.98ms
step:264/1775 train_time:66520ms step_avg:251.97ms
step:265/1775 train_time:66770ms step_avg:251.96ms
step:266/1775 train_time:67020ms step_avg:251.95ms
step:267/1775 train_time:67270ms step_avg:251.95ms
step:268/1775 train_time:67521ms step_avg:251.94ms
step:269/1775 train_time:67772ms step_avg:251.94ms
step:270/1775 train_time:68022ms step_avg:251.93ms
step:271/1775 train_time:68273ms step_avg:251.93ms
step:272/1775 train_time:68523ms step_avg:251.92ms
step:273/1775 train_time:68774ms step_avg:251.92ms
step:274/1775 train_time:69024ms step_avg:251.91ms
step:275/1775 train_time:69276ms step_avg:251.91ms
step:276/1775 train_time:69527ms step_avg:251.91ms
step:277/1775 train_time:69777ms step_avg:251.90ms
step:278/1775 train_time:70028ms step_avg:251.90ms
step:279/1775 train_time:70279ms step_avg:251.89ms
step:280/1775 train_time:70529ms step_avg:251.89ms
step:281/1775 train_time:70780ms step_avg:251.88ms
step:282/1775 train_time:71030ms step_avg:251.88ms
step:283/1775 train_time:71280ms step_avg:251.87ms
step:284/1775 train_time:71531ms step_avg:251.87ms
step:285/1775 train_time:71781ms step_avg:251.86ms
step:286/1775 train_time:72031ms step_avg:251.86ms
step:287/1775 train_time:72281ms step_avg:251.85ms
step:288/1775 train_time:72532ms step_avg:251.85ms
step:289/1775 train_time:72784ms step_avg:251.85ms
step:290/1775 train_time:73035ms step_avg:251.85ms
step:291/1775 train_time:73286ms step_avg:251.84ms
step:292/1775 train_time:73537ms step_avg:251.84ms
step:293/1775 train_time:73788ms step_avg:251.84ms
step:294/1775 train_time:74039ms step_avg:251.83ms
step:295/1775 train_time:74289ms step_avg:251.83ms
step:296/1775 train_time:74539ms step_avg:251.82ms
step:297/1775 train_time:74789ms step_avg:251.81ms
step:298/1775 train_time:75039ms step_avg:251.81ms
step:299/1775 train_time:75290ms step_avg:251.80ms
step:300/1775 train_time:75539ms step_avg:251.80ms
step:301/1775 train_time:75790ms step_avg:251.80ms
step:302/1775 train_time:76042ms step_avg:251.79ms
step:303/1775 train_time:76291ms step_avg:251.79ms
step:304/1775 train_time:76543ms step_avg:251.79ms
step:305/1775 train_time:76794ms step_avg:251.78ms
step:306/1775 train_time:77047ms step_avg:251.79ms
step:307/1775 train_time:77299ms step_avg:251.79ms
step:308/1775 train_time:77548ms step_avg:251.78ms
step:309/1775 train_time:77799ms step_avg:251.78ms
step:310/1775 train_time:78049ms step_avg:251.77ms
step:311/1775 train_time:78301ms step_avg:251.77ms
step:312/1775 train_time:78552ms step_avg:251.77ms
step:313/1775 train_time:78802ms step_avg:251.76ms
step:314/1775 train_time:79052ms step_avg:251.76ms
step:315/1775 train_time:79304ms step_avg:251.76ms
step:316/1775 train_time:79556ms step_avg:251.76ms
step:317/1775 train_time:79809ms step_avg:251.76ms
step:318/1775 train_time:80058ms step_avg:251.76ms
step:319/1775 train_time:80310ms step_avg:251.76ms
step:320/1775 train_time:80559ms step_avg:251.75ms
step:321/1775 train_time:80809ms step_avg:251.74ms
step:322/1775 train_time:81060ms step_avg:251.74ms
step:323/1775 train_time:81310ms step_avg:251.73ms
step:324/1775 train_time:81560ms step_avg:251.73ms
step:325/1775 train_time:81810ms step_avg:251.72ms
step:326/1775 train_time:82059ms step_avg:251.72ms
step:327/1775 train_time:82310ms step_avg:251.71ms
step:328/1775 train_time:82559ms step_avg:251.71ms
step:329/1775 train_time:82809ms step_avg:251.70ms
step:330/1775 train_time:83059ms step_avg:251.69ms
step:331/1775 train_time:83310ms step_avg:251.69ms
step:332/1775 train_time:83560ms step_avg:251.69ms
step:333/1775 train_time:83810ms step_avg:251.68ms
step:334/1775 train_time:84059ms step_avg:251.67ms
step:335/1775 train_time:84310ms step_avg:251.67ms
step:336/1775 train_time:84559ms step_avg:251.66ms
step:337/1775 train_time:84809ms step_avg:251.66ms
step:338/1775 train_time:85059ms step_avg:251.65ms
step:339/1775 train_time:85309ms step_avg:251.65ms
step:340/1775 train_time:85559ms step_avg:251.65ms
step:341/1775 train_time:85809ms step_avg:251.64ms
step:342/1775 train_time:86059ms step_avg:251.63ms
step:343/1775 train_time:86310ms step_avg:251.63ms
step:344/1775 train_time:86560ms step_avg:251.63ms
step:345/1775 train_time:86810ms step_avg:251.62ms
step:346/1775 train_time:87060ms step_avg:251.62ms
step:347/1775 train_time:87310ms step_avg:251.62ms
step:348/1775 train_time:87561ms step_avg:251.61ms
step:349/1775 train_time:87812ms step_avg:251.61ms
step:350/1775 train_time:88062ms step_avg:251.61ms
step:351/1775 train_time:88312ms step_avg:251.60ms
step:352/1775 train_time:88562ms step_avg:251.60ms
step:353/1775 train_time:88812ms step_avg:251.59ms
step:354/1775 train_time:89063ms step_avg:251.59ms
step:355/1775 train_time:89315ms step_avg:251.59ms
step:356/1775 train_time:89565ms step_avg:251.59ms
step:357/1775 train_time:89817ms step_avg:251.59ms
step:358/1775 train_time:90067ms step_avg:251.59ms
step:359/1775 train_time:90318ms step_avg:251.58ms
step:360/1775 train_time:90569ms step_avg:251.58ms
step:361/1775 train_time:90820ms step_avg:251.58ms
step:362/1775 train_time:91069ms step_avg:251.57ms
step:363/1775 train_time:91319ms step_avg:251.57ms
step:364/1775 train_time:91570ms step_avg:251.56ms
step:365/1775 train_time:91820ms step_avg:251.56ms
step:366/1775 train_time:92070ms step_avg:251.56ms
step:367/1775 train_time:92319ms step_avg:251.55ms
step:368/1775 train_time:92570ms step_avg:251.55ms
step:369/1775 train_time:92820ms step_avg:251.54ms
step:370/1775 train_time:93070ms step_avg:251.54ms
step:371/1775 train_time:93320ms step_avg:251.54ms
step:372/1775 train_time:93570ms step_avg:251.53ms
step:373/1775 train_time:93821ms step_avg:251.53ms
step:374/1775 train_time:94071ms step_avg:251.53ms
step:375/1775 train_time:94321ms step_avg:251.52ms
step:376/1775 train_time:94572ms step_avg:251.52ms
step:377/1775 train_time:94822ms step_avg:251.52ms
step:378/1775 train_time:95072ms step_avg:251.51ms
step:379/1775 train_time:95323ms step_avg:251.51ms
step:380/1775 train_time:95573ms step_avg:251.51ms
step:381/1775 train_time:95824ms step_avg:251.51ms
step:382/1775 train_time:96076ms step_avg:251.51ms
step:383/1775 train_time:96328ms step_avg:251.51ms
step:384/1775 train_time:96579ms step_avg:251.51ms
step:385/1775 train_time:96830ms step_avg:251.51ms
step:386/1775 train_time:97081ms step_avg:251.51ms
step:387/1775 train_time:97331ms step_avg:251.50ms
step:388/1775 train_time:97581ms step_avg:251.50ms
step:389/1775 train_time:97831ms step_avg:251.49ms
step:390/1775 train_time:98082ms step_avg:251.49ms
step:391/1775 train_time:98333ms step_avg:251.49ms
step:392/1775 train_time:98583ms step_avg:251.49ms
step:393/1775 train_time:98833ms step_avg:251.48ms
step:394/1775 train_time:99085ms step_avg:251.49ms
step:395/1775 train_time:99335ms step_avg:251.48ms
step:396/1775 train_time:99586ms step_avg:251.48ms
step:397/1775 train_time:99836ms step_avg:251.48ms
step:398/1775 train_time:100088ms step_avg:251.48ms
step:399/1775 train_time:100339ms step_avg:251.48ms
step:400/1775 train_time:100589ms step_avg:251.47ms
step:401/1775 train_time:100839ms step_avg:251.47ms
step:402/1775 train_time:101090ms step_avg:251.47ms
step:403/1775 train_time:101340ms step_avg:251.46ms
step:404/1775 train_time:101590ms step_avg:251.46ms
step:405/1775 train_time:101841ms step_avg:251.46ms
step:406/1775 train_time:102091ms step_avg:251.46ms
step:407/1775 train_time:102343ms step_avg:251.46ms
step:408/1775 train_time:102593ms step_avg:251.45ms
step:409/1775 train_time:102844ms step_avg:251.45ms
step:410/1775 train_time:103096ms step_avg:251.45ms
step:411/1775 train_time:103347ms step_avg:251.45ms
step:412/1775 train_time:103598ms step_avg:251.45ms
step:413/1775 train_time:103848ms step_avg:251.45ms
step:414/1775 train_time:104099ms step_avg:251.45ms
step:415/1775 train_time:104349ms step_avg:251.44ms
step:416/1775 train_time:104599ms step_avg:251.44ms
step:417/1775 train_time:104849ms step_avg:251.44ms
step:418/1775 train_time:105099ms step_avg:251.43ms
step:419/1775 train_time:105349ms step_avg:251.43ms
step:420/1775 train_time:105598ms step_avg:251.42ms
step:421/1775 train_time:105849ms step_avg:251.42ms
step:422/1775 train_time:106099ms step_avg:251.42ms
step:423/1775 train_time:106349ms step_avg:251.42ms
step:424/1775 train_time:106598ms step_avg:251.41ms
step:425/1775 train_time:106849ms step_avg:251.41ms
step:426/1775 train_time:107099ms step_avg:251.41ms
step:427/1775 train_time:107350ms step_avg:251.40ms
step:428/1775 train_time:107600ms step_avg:251.40ms
step:429/1775 train_time:107850ms step_avg:251.40ms
step:430/1775 train_time:108100ms step_avg:251.39ms
step:431/1775 train_time:108350ms step_avg:251.39ms
step:432/1775 train_time:108600ms step_avg:251.39ms
step:433/1775 train_time:108851ms step_avg:251.39ms
step:434/1775 train_time:109102ms step_avg:251.39ms
step:435/1775 train_time:109352ms step_avg:251.38ms
step:436/1775 train_time:109602ms step_avg:251.38ms
step:437/1775 train_time:109852ms step_avg:251.38ms
step:438/1775 train_time:110103ms step_avg:251.38ms
step:439/1775 train_time:110356ms step_avg:251.38ms
step:440/1775 train_time:110607ms step_avg:251.38ms
step:441/1775 train_time:110857ms step_avg:251.38ms
step:442/1775 train_time:111108ms step_avg:251.38ms
step:443/1775 train_time:111358ms step_avg:251.37ms
step:444/1775 train_time:111608ms step_avg:251.37ms
step:445/1775 train_time:111859ms step_avg:251.37ms
step:446/1775 train_time:112109ms step_avg:251.37ms
step:447/1775 train_time:112360ms step_avg:251.36ms
step:448/1775 train_time:112609ms step_avg:251.36ms
step:449/1775 train_time:112860ms step_avg:251.36ms
step:450/1775 train_time:113110ms step_avg:251.36ms
step:451/1775 train_time:113360ms step_avg:251.35ms
step:452/1775 train_time:113611ms step_avg:251.35ms
step:453/1775 train_time:113861ms step_avg:251.35ms
step:454/1775 train_time:114112ms step_avg:251.35ms
step:455/1775 train_time:114361ms step_avg:251.34ms
step:456/1775 train_time:114611ms step_avg:251.34ms
step:457/1775 train_time:114861ms step_avg:251.34ms
step:458/1775 train_time:115112ms step_avg:251.34ms
step:459/1775 train_time:115362ms step_avg:251.33ms
step:460/1775 train_time:115612ms step_avg:251.33ms
step:461/1775 train_time:115863ms step_avg:251.33ms
step:462/1775 train_time:116114ms step_avg:251.33ms
step:463/1775 train_time:116366ms step_avg:251.33ms
step:464/1775 train_time:116617ms step_avg:251.33ms
step:465/1775 train_time:116868ms step_avg:251.33ms
step:466/1775 train_time:117119ms step_avg:251.33ms
step:467/1775 train_time:117370ms step_avg:251.33ms
step:468/1775 train_time:117620ms step_avg:251.33ms
step:469/1775 train_time:117870ms step_avg:251.32ms
step:470/1775 train_time:118120ms step_avg:251.32ms
step:471/1775 train_time:118370ms step_avg:251.32ms
step:472/1775 train_time:118620ms step_avg:251.31ms
step:473/1775 train_time:118869ms step_avg:251.31ms
step:474/1775 train_time:119119ms step_avg:251.31ms
step:475/1775 train_time:119370ms step_avg:251.30ms
step:476/1775 train_time:119619ms step_avg:251.30ms
step:477/1775 train_time:119869ms step_avg:251.30ms
step:478/1775 train_time:120119ms step_avg:251.30ms
step:479/1775 train_time:120370ms step_avg:251.29ms
step:480/1775 train_time:120620ms step_avg:251.29ms
step:481/1775 train_time:120871ms step_avg:251.29ms
step:482/1775 train_time:121120ms step_avg:251.29ms
step:483/1775 train_time:121370ms step_avg:251.28ms
step:484/1775 train_time:121619ms step_avg:251.28ms
step:485/1775 train_time:121869ms step_avg:251.28ms
step:486/1775 train_time:122119ms step_avg:251.27ms
step:487/1775 train_time:122370ms step_avg:251.27ms
step:488/1775 train_time:122620ms step_avg:251.27ms
step:489/1775 train_time:122870ms step_avg:251.27ms
step:490/1775 train_time:123120ms step_avg:251.26ms
step:491/1775 train_time:123370ms step_avg:251.26ms
step:492/1775 train_time:123619ms step_avg:251.26ms
step:493/1775 train_time:123870ms step_avg:251.26ms
step:494/1775 train_time:124120ms step_avg:251.25ms
step:495/1775 train_time:124371ms step_avg:251.25ms
step:496/1775 train_time:124620ms step_avg:251.25ms
step:497/1775 train_time:124869ms step_avg:251.25ms
step:498/1775 train_time:125119ms step_avg:251.24ms
step:499/1775 train_time:125369ms step_avg:251.24ms
step:500/1775 train_time:125619ms step_avg:251.24ms
step:500/1775 val_loss:4.2664 train_time:125664ms step_avg:251.33ms
step:501/1775 train_time:125872ms step_avg:251.24ms
step:502/1775 train_time:126122ms step_avg:251.24ms
step:503/1775 train_time:126372ms step_avg:251.24ms
step:504/1775 train_time:126624ms step_avg:251.24ms
step:505/1775 train_time:126875ms step_avg:251.24ms
step:506/1775 train_time:127125ms step_avg:251.24ms
step:507/1775 train_time:127375ms step_avg:251.23ms
step:508/1775 train_time:127626ms step_avg:251.23ms
step:509/1775 train_time:127879ms step_avg:251.24ms
step:510/1775 train_time:128129ms step_avg:251.23ms
step:511/1775 train_time:128380ms step_avg:251.23ms
step:512/1775 train_time:128630ms step_avg:251.23ms
step:513/1775 train_time:128883ms step_avg:251.23ms
step:514/1775 train_time:129133ms step_avg:251.23ms
step:515/1775 train_time:129383ms step_avg:251.23ms
step:516/1775 train_time:129634ms step_avg:251.23ms
step:517/1775 train_time:129885ms step_avg:251.23ms
step:518/1775 train_time:130134ms step_avg:251.22ms
step:519/1775 train_time:130385ms step_avg:251.22ms
step:520/1775 train_time:130635ms step_avg:251.22ms
step:521/1775 train_time:130886ms step_avg:251.22ms
step:522/1775 train_time:131137ms step_avg:251.22ms
step:523/1775 train_time:131387ms step_avg:251.22ms
step:524/1775 train_time:131636ms step_avg:251.21ms
step:525/1775 train_time:131886ms step_avg:251.21ms
step:526/1775 train_time:132136ms step_avg:251.21ms
step:527/1775 train_time:132386ms step_avg:251.21ms
step:528/1775 train_time:132636ms step_avg:251.20ms
step:529/1775 train_time:132886ms step_avg:251.20ms
step:530/1775 train_time:133136ms step_avg:251.20ms
step:531/1775 train_time:133387ms step_avg:251.20ms
step:532/1775 train_time:133636ms step_avg:251.20ms
step:533/1775 train_time:133886ms step_avg:251.19ms
step:534/1775 train_time:134136ms step_avg:251.19ms
step:535/1775 train_time:134386ms step_avg:251.19ms
step:536/1775 train_time:134637ms step_avg:251.19ms
step:537/1775 train_time:134887ms step_avg:251.19ms
step:538/1775 train_time:135137ms step_avg:251.18ms
step:539/1775 train_time:135386ms step_avg:251.18ms
step:540/1775 train_time:135637ms step_avg:251.18ms
step:541/1775 train_time:135887ms step_avg:251.18ms
step:542/1775 train_time:136137ms step_avg:251.17ms
step:543/1775 train_time:136387ms step_avg:251.17ms
step:544/1775 train_time:136637ms step_avg:251.17ms
step:545/1775 train_time:136887ms step_avg:251.17ms
step:546/1775 train_time:137137ms step_avg:251.17ms
step:547/1775 train_time:137387ms step_avg:251.16ms
step:548/1775 train_time:137637ms step_avg:251.16ms
step:549/1775 train_time:137887ms step_avg:251.16ms
step:550/1775 train_time:138137ms step_avg:251.16ms
step:551/1775 train_time:138387ms step_avg:251.16ms
step:552/1775 train_time:138638ms step_avg:251.15ms
step:553/1775 train_time:138888ms step_avg:251.15ms
step:554/1775 train_time:139139ms step_avg:251.15ms
step:555/1775 train_time:139389ms step_avg:251.15ms
step:556/1775 train_time:139639ms step_avg:251.15ms
step:557/1775 train_time:139890ms step_avg:251.15ms
step:558/1775 train_time:140140ms step_avg:251.15ms
step:559/1775 train_time:140391ms step_avg:251.15ms
step:560/1775 train_time:140642ms step_avg:251.15ms
step:561/1775 train_time:140894ms step_avg:251.15ms
step:562/1775 train_time:141145ms step_avg:251.15ms
step:563/1775 train_time:141396ms step_avg:251.15ms
step:564/1775 train_time:141646ms step_avg:251.15ms
step:565/1775 train_time:141896ms step_avg:251.14ms
step:566/1775 train_time:142146ms step_avg:251.14ms
step:567/1775 train_time:142396ms step_avg:251.14ms
step:568/1775 train_time:142646ms step_avg:251.14ms
step:569/1775 train_time:142895ms step_avg:251.13ms
step:570/1775 train_time:143145ms step_avg:251.13ms
step:571/1775 train_time:143395ms step_avg:251.13ms
step:572/1775 train_time:143645ms step_avg:251.13ms
step:573/1775 train_time:143895ms step_avg:251.13ms
step:574/1775 train_time:144145ms step_avg:251.12ms
step:575/1775 train_time:144395ms step_avg:251.12ms
step:576/1775 train_time:144647ms step_avg:251.12ms
step:577/1775 train_time:144897ms step_avg:251.12ms
step:578/1775 train_time:145146ms step_avg:251.12ms
step:579/1775 train_time:145396ms step_avg:251.12ms
step:580/1775 train_time:145838ms step_avg:251.45ms
step:581/1775 train_time:146303ms step_avg:251.81ms
step:582/1775 train_time:146767ms step_avg:252.18ms
step:583/1775 train_time:147232ms step_avg:252.54ms
step:584/1775 train_time:147698ms step_avg:252.91ms
step:585/1775 train_time:148161ms step_avg:253.27ms
step:586/1775 train_time:148625ms step_avg:253.63ms
step:587/1775 train_time:149088ms step_avg:253.98ms
step:588/1775 train_time:149556ms step_avg:254.35ms
step:589/1775 train_time:150021ms step_avg:254.70ms
step:590/1775 train_time:150486ms step_avg:255.06ms
step:591/1775 train_time:150952ms step_avg:255.42ms
step:592/1775 train_time:151419ms step_avg:255.78ms
step:593/1775 train_time:151885ms step_avg:256.13ms
step:594/1775 train_time:152350ms step_avg:256.48ms
step:595/1775 train_time:152816ms step_avg:256.83ms
step:596/1775 train_time:153284ms step_avg:257.19ms
step:597/1775 train_time:153749ms step_avg:257.54ms
step:598/1775 train_time:154215ms step_avg:257.88ms
step:599/1775 train_time:154680ms step_avg:258.23ms
step:600/1775 train_time:155146ms step_avg:258.58ms
step:601/1775 train_time:155612ms step_avg:258.92ms
step:602/1775 train_time:156078ms step_avg:259.27ms
step:603/1775 train_time:156543ms step_avg:259.61ms
step:604/1775 train_time:157008ms step_avg:259.95ms
step:605/1775 train_time:157472ms step_avg:260.28ms
step:606/1775 train_time:157938ms step_avg:260.62ms
step:607/1775 train_time:158403ms step_avg:260.96ms
step:608/1775 train_time:158867ms step_avg:261.29ms
step:609/1775 train_time:159332ms step_avg:261.63ms
step:610/1775 train_time:159798ms step_avg:261.96ms
step:611/1775 train_time:160262ms step_avg:262.29ms
step:612/1775 train_time:160728ms step_avg:262.63ms
step:613/1775 train_time:161192ms step_avg:262.96ms
step:614/1775 train_time:161659ms step_avg:263.29ms
step:615/1775 train_time:162125ms step_avg:263.62ms
step:616/1775 train_time:162590ms step_avg:263.94ms
step:617/1775 train_time:163054ms step_avg:264.27ms
step:618/1775 train_time:163521ms step_avg:264.60ms
step:619/1775 train_time:163986ms step_avg:264.92ms
step:620/1775 train_time:164452ms step_avg:265.25ms
step:621/1775 train_time:164919ms step_avg:265.57ms
step:622/1775 train_time:165385ms step_avg:265.89ms
step:623/1775 train_time:165849ms step_avg:266.21ms
step:624/1775 train_time:166316ms step_avg:266.53ms
step:625/1775 train_time:166780ms step_avg:266.85ms
step:626/1775 train_time:167245ms step_avg:267.16ms
step:627/1775 train_time:167709ms step_avg:267.48ms
step:628/1775 train_time:168175ms step_avg:267.79ms
step:629/1775 train_time:168640ms step_avg:268.11ms
step:630/1775 train_time:169106ms step_avg:268.42ms
step:631/1775 train_time:169570ms step_avg:268.73ms
step:632/1775 train_time:170036ms step_avg:269.04ms
step:633/1775 train_time:170502ms step_avg:269.36ms
step:634/1775 train_time:170966ms step_avg:269.66ms
step:635/1775 train_time:171431ms step_avg:269.97ms
step:636/1775 train_time:171898ms step_avg:270.28ms
step:637/1775 train_time:172363ms step_avg:270.59ms
step:638/1775 train_time:172828ms step_avg:270.89ms
step:639/1775 train_time:173293ms step_avg:271.19ms
step:640/1775 train_time:173760ms step_avg:271.50ms
step:641/1775 train_time:174223ms step_avg:271.80ms
step:642/1775 train_time:174688ms step_avg:272.10ms
step:643/1775 train_time:175153ms step_avg:272.40ms
step:644/1775 train_time:175619ms step_avg:272.70ms
step:645/1775 train_time:176084ms step_avg:273.00ms
step:646/1775 train_time:176550ms step_avg:273.30ms
step:647/1775 train_time:177015ms step_avg:273.59ms
step:648/1775 train_time:177481ms step_avg:273.89ms
step:649/1775 train_time:177946ms step_avg:274.19ms
step:650/1775 train_time:178411ms step_avg:274.48ms
step:651/1775 train_time:178877ms step_avg:274.77ms
step:652/1775 train_time:179342ms step_avg:275.06ms
step:653/1775 train_time:179806ms step_avg:275.35ms
step:654/1775 train_time:180272ms step_avg:275.65ms
step:655/1775 train_time:180738ms step_avg:275.94ms
step:656/1775 train_time:181201ms step_avg:276.22ms
step:657/1775 train_time:181666ms step_avg:276.51ms
step:658/1775 train_time:182131ms step_avg:276.79ms
step:659/1775 train_time:182596ms step_avg:277.08ms
step:660/1775 train_time:183062ms step_avg:277.37ms
step:661/1775 train_time:183526ms step_avg:277.65ms
step:662/1775 train_time:183993ms step_avg:277.94ms
step:663/1775 train_time:184458ms step_avg:278.22ms
step:664/1775 train_time:184923ms step_avg:278.50ms
step:665/1775 train_time:185388ms step_avg:278.78ms
step:666/1775 train_time:185854ms step_avg:279.06ms
step:667/1775 train_time:186319ms step_avg:279.34ms
step:668/1775 train_time:186785ms step_avg:279.62ms
step:669/1775 train_time:187249ms step_avg:279.89ms
step:670/1775 train_time:187716ms step_avg:280.17ms
step:671/1775 train_time:188181ms step_avg:280.45ms
step:672/1775 train_time:188648ms step_avg:280.73ms
step:673/1775 train_time:189111ms step_avg:281.00ms
step:674/1775 train_time:189576ms step_avg:281.27ms
step:675/1775 train_time:190043ms step_avg:281.55ms
step:676/1775 train_time:190508ms step_avg:281.82ms
step:677/1775 train_time:190973ms step_avg:282.09ms
step:678/1775 train_time:191439ms step_avg:282.36ms
step:679/1775 train_time:191903ms step_avg:282.63ms
step:680/1775 train_time:192367ms step_avg:282.89ms
step:681/1775 train_time:192831ms step_avg:283.16ms
step:682/1775 train_time:193297ms step_avg:283.43ms
step:683/1775 train_time:193761ms step_avg:283.69ms
step:684/1775 train_time:194225ms step_avg:283.96ms
step:685/1775 train_time:194689ms step_avg:284.22ms
step:686/1775 train_time:195156ms step_avg:284.48ms
step:687/1775 train_time:195621ms step_avg:284.75ms
step:688/1775 train_time:196087ms step_avg:285.01ms
step:689/1775 train_time:196551ms step_avg:285.27ms
step:690/1775 train_time:197019ms step_avg:285.53ms
step:691/1775 train_time:197482ms step_avg:285.79ms
step:692/1775 train_time:197948ms step_avg:286.05ms
step:693/1775 train_time:198412ms step_avg:286.31ms
step:694/1775 train_time:198877ms step_avg:286.57ms
step:695/1775 train_time:199343ms step_avg:286.82ms
step:696/1775 train_time:199808ms step_avg:287.08ms
step:697/1775 train_time:200273ms step_avg:287.34ms
step:698/1775 train_time:200738ms step_avg:287.59ms
step:699/1775 train_time:201202ms step_avg:287.84ms
step:700/1775 train_time:201665ms step_avg:288.09ms
step:701/1775 train_time:202129ms step_avg:288.34ms
step:702/1775 train_time:202595ms step_avg:288.60ms
step:703/1775 train_time:203061ms step_avg:288.85ms
step:704/1775 train_time:203525ms step_avg:289.10ms
step:705/1775 train_time:203989ms step_avg:289.35ms
step:706/1775 train_time:204454ms step_avg:289.60ms
step:707/1775 train_time:204920ms step_avg:289.84ms
step:708/1775 train_time:205383ms step_avg:290.09ms
step:709/1775 train_time:205848ms step_avg:290.34ms
step:710/1775 train_time:206314ms step_avg:290.58ms
step:711/1775 train_time:206778ms step_avg:290.83ms
step:712/1775 train_time:207243ms step_avg:291.07ms
step:713/1775 train_time:207709ms step_avg:291.32ms
step:714/1775 train_time:208175ms step_avg:291.56ms
step:715/1775 train_time:208640ms step_avg:291.80ms
step:716/1775 train_time:209105ms step_avg:292.05ms
step:717/1775 train_time:209569ms step_avg:292.29ms
step:718/1775 train_time:210035ms step_avg:292.53ms
step:719/1775 train_time:210500ms step_avg:292.77ms
step:720/1775 train_time:210966ms step_avg:293.01ms
step:721/1775 train_time:211431ms step_avg:293.25ms
step:722/1775 train_time:211896ms step_avg:293.49ms
step:723/1775 train_time:212361ms step_avg:293.72ms
step:724/1775 train_time:212827ms step_avg:293.96ms
step:725/1775 train_time:213292ms step_avg:294.20ms
step:726/1775 train_time:213758ms step_avg:294.43ms
step:727/1775 train_time:214223ms step_avg:294.67ms
step:728/1775 train_time:214689ms step_avg:294.90ms
step:729/1775 train_time:215153ms step_avg:295.13ms
step:730/1775 train_time:215619ms step_avg:295.37ms
step:731/1775 train_time:216084ms step_avg:295.60ms
step:732/1775 train_time:216548ms step_avg:295.83ms
step:733/1775 train_time:217012ms step_avg:296.06ms
step:734/1775 train_time:217477ms step_avg:296.29ms
step:735/1775 train_time:217941ms step_avg:296.52ms
step:736/1775 train_time:218406ms step_avg:296.75ms
step:737/1775 train_time:218871ms step_avg:296.98ms
step:738/1775 train_time:219338ms step_avg:297.21ms
step:739/1775 train_time:219801ms step_avg:297.43ms
step:740/1775 train_time:220267ms step_avg:297.66ms
step:741/1775 train_time:220731ms step_avg:297.88ms
step:742/1775 train_time:221196ms step_avg:298.11ms
step:743/1775 train_time:221663ms step_avg:298.34ms
step:744/1775 train_time:222128ms step_avg:298.56ms
step:745/1775 train_time:222592ms step_avg:298.78ms
step:746/1775 train_time:223058ms step_avg:299.01ms
step:747/1775 train_time:223522ms step_avg:299.23ms
step:748/1775 train_time:223987ms step_avg:299.45ms
step:749/1775 train_time:224449ms step_avg:299.66ms
step:750/1775 train_time:224916ms step_avg:299.89ms
step:750/1775 val_loss:3.9817 train_time:224989ms step_avg:299.99ms
step:751/1775 train_time:225382ms step_avg:300.11ms
step:752/1775 train_time:225848ms step_avg:300.33ms
step:753/1775 train_time:226311ms step_avg:300.55ms
step:754/1775 train_time:226777ms step_avg:300.77ms
step:755/1775 train_time:227241ms step_avg:300.98ms
step:756/1775 train_time:227708ms step_avg:301.20ms
step:757/1775 train_time:228171ms step_avg:301.42ms
step:758/1775 train_time:228637ms step_avg:301.63ms
step:759/1775 train_time:229103ms step_avg:301.85ms
step:760/1775 train_time:229568ms step_avg:302.06ms
step:761/1775 train_time:230033ms step_avg:302.28ms
step:762/1775 train_time:230497ms step_avg:302.49ms
step:763/1775 train_time:230961ms step_avg:302.70ms
step:764/1775 train_time:231427ms step_avg:302.92ms
step:765/1775 train_time:231892ms step_avg:303.13ms
step:766/1775 train_time:232357ms step_avg:303.34ms
step:767/1775 train_time:232824ms step_avg:303.55ms
step:768/1775 train_time:233289ms step_avg:303.76ms
step:769/1775 train_time:233753ms step_avg:303.97ms
step:770/1775 train_time:234219ms step_avg:304.18ms
step:771/1775 train_time:234685ms step_avg:304.39ms
step:772/1775 train_time:235150ms step_avg:304.60ms
step:773/1775 train_time:235615ms step_avg:304.81ms
step:774/1775 train_time:236081ms step_avg:305.01ms
step:775/1775 train_time:236544ms step_avg:305.22ms
step:776/1775 train_time:237008ms step_avg:305.42ms
step:777/1775 train_time:237473ms step_avg:305.63ms
step:778/1775 train_time:237939ms step_avg:305.83ms
step:779/1775 train_time:238404ms step_avg:306.04ms
step:780/1775 train_time:238870ms step_avg:306.24ms
step:781/1775 train_time:239333ms step_avg:306.44ms
step:782/1775 train_time:239800ms step_avg:306.65ms
step:783/1775 train_time:240263ms step_avg:306.85ms
step:784/1775 train_time:240729ms step_avg:307.05ms
step:785/1775 train_time:241192ms step_avg:307.25ms
step:786/1775 train_time:241657ms step_avg:307.45ms
step:787/1775 train_time:242121ms step_avg:307.65ms
step:788/1775 train_time:242588ms step_avg:307.85ms
step:789/1775 train_time:243052ms step_avg:308.05ms
step:790/1775 train_time:243518ms step_avg:308.25ms
step:791/1775 train_time:243983ms step_avg:308.45ms
step:792/1775 train_time:244448ms step_avg:308.65ms
step:793/1775 train_time:244912ms step_avg:308.84ms
step:794/1775 train_time:245376ms step_avg:309.04ms
step:795/1775 train_time:245841ms step_avg:309.23ms
step:796/1775 train_time:246309ms step_avg:309.43ms
step:797/1775 train_time:246773ms step_avg:309.63ms
step:798/1775 train_time:247238ms step_avg:309.82ms
step:799/1775 train_time:247703ms step_avg:310.02ms
step:800/1775 train_time:248168ms step_avg:310.21ms
step:801/1775 train_time:248632ms step_avg:310.40ms
step:802/1775 train_time:249097ms step_avg:310.59ms
step:803/1775 train_time:249563ms step_avg:310.79ms
step:804/1775 train_time:250031ms step_avg:310.98ms
step:805/1775 train_time:250496ms step_avg:311.18ms
step:806/1775 train_time:250962ms step_avg:311.37ms
step:807/1775 train_time:251428ms step_avg:311.56ms
step:808/1775 train_time:251893ms step_avg:311.75ms
step:809/1775 train_time:252356ms step_avg:311.94ms
step:810/1775 train_time:252821ms step_avg:312.13ms
step:811/1775 train_time:253287ms step_avg:312.31ms
step:812/1775 train_time:253752ms step_avg:312.50ms
step:813/1775 train_time:254216ms step_avg:312.69ms
step:814/1775 train_time:254682ms step_avg:312.88ms
step:815/1775 train_time:255146ms step_avg:313.06ms
step:816/1775 train_time:255612ms step_avg:313.25ms
step:817/1775 train_time:256076ms step_avg:313.43ms
step:818/1775 train_time:256543ms step_avg:313.62ms
step:819/1775 train_time:257007ms step_avg:313.81ms
step:820/1775 train_time:257472ms step_avg:313.99ms
step:821/1775 train_time:257936ms step_avg:314.17ms
step:822/1775 train_time:258402ms step_avg:314.36ms
step:823/1775 train_time:258866ms step_avg:314.54ms
step:824/1775 train_time:259332ms step_avg:314.72ms
step:825/1775 train_time:259797ms step_avg:314.91ms
step:826/1775 train_time:260264ms step_avg:315.09ms
step:827/1775 train_time:260726ms step_avg:315.27ms
step:828/1775 train_time:261192ms step_avg:315.45ms
step:829/1775 train_time:261656ms step_avg:315.63ms
step:830/1775 train_time:262122ms step_avg:315.81ms
step:831/1775 train_time:262587ms step_avg:315.99ms
step:832/1775 train_time:263053ms step_avg:316.17ms
step:833/1775 train_time:263516ms step_avg:316.35ms
step:834/1775 train_time:263982ms step_avg:316.52ms
step:835/1775 train_time:264448ms step_avg:316.70ms
step:836/1775 train_time:264911ms step_avg:316.88ms
step:837/1775 train_time:265374ms step_avg:317.05ms
step:838/1775 train_time:265841ms step_avg:317.23ms
step:839/1775 train_time:266306ms step_avg:317.41ms
step:840/1775 train_time:266770ms step_avg:317.58ms
step:841/1775 train_time:267234ms step_avg:317.76ms
step:842/1775 train_time:267699ms step_avg:317.93ms
step:843/1775 train_time:268165ms step_avg:318.11ms
step:844/1775 train_time:268630ms step_avg:318.28ms
step:845/1775 train_time:269093ms step_avg:318.45ms
step:846/1775 train_time:269559ms step_avg:318.63ms
step:847/1775 train_time:270024ms step_avg:318.80ms
step:848/1775 train_time:270489ms step_avg:318.97ms
step:849/1775 train_time:270953ms step_avg:319.14ms
step:850/1775 train_time:271419ms step_avg:319.32ms
step:851/1775 train_time:271885ms step_avg:319.49ms
step:852/1775 train_time:272348ms step_avg:319.66ms
step:853/1775 train_time:272812ms step_avg:319.83ms
step:854/1775 train_time:273276ms step_avg:320.00ms
step:855/1775 train_time:273742ms step_avg:320.17ms
step:856/1775 train_time:274208ms step_avg:320.34ms
step:857/1775 train_time:274672ms step_avg:320.50ms
step:858/1775 train_time:275137ms step_avg:320.67ms
step:859/1775 train_time:275602ms step_avg:320.84ms
step:860/1775 train_time:276067ms step_avg:321.01ms
step:861/1775 train_time:276531ms step_avg:321.17ms
step:862/1775 train_time:276996ms step_avg:321.34ms
step:863/1775 train_time:277461ms step_avg:321.51ms
step:864/1775 train_time:277927ms step_avg:321.67ms
step:865/1775 train_time:278390ms step_avg:321.84ms
step:866/1775 train_time:278854ms step_avg:322.00ms
step:867/1775 train_time:279318ms step_avg:322.17ms
step:868/1775 train_time:279785ms step_avg:322.33ms
step:869/1775 train_time:280247ms step_avg:322.49ms
step:870/1775 train_time:280711ms step_avg:322.66ms
step:871/1775 train_time:281174ms step_avg:322.82ms
step:872/1775 train_time:281638ms step_avg:322.98ms
step:873/1775 train_time:282103ms step_avg:323.14ms
step:874/1775 train_time:282567ms step_avg:323.30ms
step:875/1775 train_time:283032ms step_avg:323.47ms
step:876/1775 train_time:283496ms step_avg:323.63ms
step:877/1775 train_time:283962ms step_avg:323.79ms
step:878/1775 train_time:284427ms step_avg:323.95ms
step:879/1775 train_time:284890ms step_avg:324.11ms
step:880/1775 train_time:285355ms step_avg:324.27ms
step:881/1775 train_time:285818ms step_avg:324.42ms
step:882/1775 train_time:286285ms step_avg:324.59ms
step:883/1775 train_time:286749ms step_avg:324.74ms
step:884/1775 train_time:287216ms step_avg:324.90ms
step:885/1775 train_time:287681ms step_avg:325.06ms
step:886/1775 train_time:288148ms step_avg:325.22ms
step:887/1775 train_time:288612ms step_avg:325.38ms
step:888/1775 train_time:289077ms step_avg:325.54ms
step:889/1775 train_time:289543ms step_avg:325.70ms
step:890/1775 train_time:290007ms step_avg:325.85ms
step:891/1775 train_time:290472ms step_avg:326.01ms
step:892/1775 train_time:290937ms step_avg:326.16ms
step:893/1775 train_time:291401ms step_avg:326.32ms
step:894/1775 train_time:291865ms step_avg:326.47ms
step:895/1775 train_time:292330ms step_avg:326.63ms
step:896/1775 train_time:292794ms step_avg:326.78ms
step:897/1775 train_time:293257ms step_avg:326.93ms
step:898/1775 train_time:293724ms step_avg:327.09ms
step:899/1775 train_time:294187ms step_avg:327.24ms
step:900/1775 train_time:294653ms step_avg:327.39ms
step:901/1775 train_time:295117ms step_avg:327.54ms
step:902/1775 train_time:295583ms step_avg:327.70ms
step:903/1775 train_time:296048ms step_avg:327.85ms
step:904/1775 train_time:296512ms step_avg:328.00ms
step:905/1775 train_time:296975ms step_avg:328.15ms
step:906/1775 train_time:297440ms step_avg:328.30ms
step:907/1775 train_time:297905ms step_avg:328.45ms
step:908/1775 train_time:298370ms step_avg:328.60ms
step:909/1775 train_time:298833ms step_avg:328.75ms
step:910/1775 train_time:299298ms step_avg:328.90ms
step:911/1775 train_time:299763ms step_avg:329.05ms
step:912/1775 train_time:300228ms step_avg:329.20ms
step:913/1775 train_time:300692ms step_avg:329.34ms
step:914/1775 train_time:301156ms step_avg:329.49ms
step:915/1775 train_time:301621ms step_avg:329.64ms
step:916/1775 train_time:302087ms step_avg:329.79ms
step:917/1775 train_time:302551ms step_avg:329.94ms
step:918/1775 train_time:303015ms step_avg:330.08ms
step:919/1775 train_time:303480ms step_avg:330.23ms
step:920/1775 train_time:303944ms step_avg:330.37ms
step:921/1775 train_time:304409ms step_avg:330.52ms
step:922/1775 train_time:304873ms step_avg:330.67ms
step:923/1775 train_time:305337ms step_avg:330.81ms
step:924/1775 train_time:305802ms step_avg:330.95ms
step:925/1775 train_time:306266ms step_avg:331.10ms
step:926/1775 train_time:306732ms step_avg:331.24ms
step:927/1775 train_time:307195ms step_avg:331.39ms
step:928/1775 train_time:307661ms step_avg:331.53ms
step:929/1775 train_time:308126ms step_avg:331.68ms
step:930/1775 train_time:308590ms step_avg:331.82ms
step:931/1775 train_time:309053ms step_avg:331.96ms
step:932/1775 train_time:309517ms step_avg:332.10ms
step:933/1775 train_time:309981ms step_avg:332.24ms
step:934/1775 train_time:310447ms step_avg:332.38ms
step:935/1775 train_time:310911ms step_avg:332.53ms
step:936/1775 train_time:311376ms step_avg:332.67ms
step:937/1775 train_time:311842ms step_avg:332.81ms
step:938/1775 train_time:312306ms step_avg:332.95ms
step:939/1775 train_time:312770ms step_avg:333.09ms
step:940/1775 train_time:313235ms step_avg:333.23ms
step:941/1775 train_time:313699ms step_avg:333.37ms
step:942/1775 train_time:314164ms step_avg:333.51ms
step:943/1775 train_time:314627ms step_avg:333.65ms
step:944/1775 train_time:315093ms step_avg:333.78ms
step:945/1775 train_time:315556ms step_avg:333.92ms
step:946/1775 train_time:316022ms step_avg:334.06ms
step:947/1775 train_time:316486ms step_avg:334.20ms
step:948/1775 train_time:316950ms step_avg:334.34ms
step:949/1775 train_time:317415ms step_avg:334.47ms
step:950/1775 train_time:317881ms step_avg:334.61ms
step:951/1775 train_time:318344ms step_avg:334.75ms
step:952/1775 train_time:318808ms step_avg:334.88ms
step:953/1775 train_time:319272ms step_avg:335.02ms
step:954/1775 train_time:319736ms step_avg:335.15ms
step:955/1775 train_time:320201ms step_avg:335.29ms
step:956/1775 train_time:320667ms step_avg:335.43ms
step:957/1775 train_time:321132ms step_avg:335.56ms
step:958/1775 train_time:321594ms step_avg:335.69ms
step:959/1775 train_time:322059ms step_avg:335.83ms
step:960/1775 train_time:322525ms step_avg:335.96ms
step:961/1775 train_time:322989ms step_avg:336.10ms
step:962/1775 train_time:323453ms step_avg:336.23ms
step:963/1775 train_time:323917ms step_avg:336.36ms
step:964/1775 train_time:324384ms step_avg:336.50ms
step:965/1775 train_time:324849ms step_avg:336.63ms
step:966/1775 train_time:325313ms step_avg:336.76ms
step:967/1775 train_time:325776ms step_avg:336.89ms
step:968/1775 train_time:326244ms step_avg:337.03ms
step:969/1775 train_time:326706ms step_avg:337.16ms
step:970/1775 train_time:327171ms step_avg:337.29ms
step:971/1775 train_time:327633ms step_avg:337.42ms
step:972/1775 train_time:328099ms step_avg:337.55ms
step:973/1775 train_time:328563ms step_avg:337.68ms
step:974/1775 train_time:329029ms step_avg:337.81ms
step:975/1775 train_time:329492ms step_avg:337.94ms
step:976/1775 train_time:329957ms step_avg:338.07ms
step:977/1775 train_time:330421ms step_avg:338.20ms
step:978/1775 train_time:330887ms step_avg:338.33ms
step:979/1775 train_time:331352ms step_avg:338.46ms
step:980/1775 train_time:331817ms step_avg:338.59ms
step:981/1775 train_time:332283ms step_avg:338.72ms
step:982/1775 train_time:332748ms step_avg:338.85ms
step:983/1775 train_time:333212ms step_avg:338.97ms
step:984/1775 train_time:333676ms step_avg:339.10ms
step:985/1775 train_time:334143ms step_avg:339.23ms
step:986/1775 train_time:334608ms step_avg:339.36ms
step:987/1775 train_time:335072ms step_avg:339.49ms
step:988/1775 train_time:335536ms step_avg:339.61ms
step:989/1775 train_time:336002ms step_avg:339.74ms
step:990/1775 train_time:336467ms step_avg:339.87ms
step:991/1775 train_time:336931ms step_avg:339.99ms
step:992/1775 train_time:337397ms step_avg:340.12ms
step:993/1775 train_time:337861ms step_avg:340.24ms
step:994/1775 train_time:338327ms step_avg:340.37ms
step:995/1775 train_time:338790ms step_avg:340.49ms
step:996/1775 train_time:339255ms step_avg:340.62ms
step:997/1775 train_time:339716ms step_avg:340.74ms
step:998/1775 train_time:340182ms step_avg:340.86ms
step:999/1775 train_time:340646ms step_avg:340.99ms
step:1000/1775 train_time:341110ms step_avg:341.11ms
step:1000/1775 val_loss:3.7286 train_time:341182ms step_avg:341.18ms
step:1001/1775 train_time:341575ms step_avg:341.23ms
step:1002/1775 train_time:342041ms step_avg:341.36ms
step:1003/1775 train_time:342504ms step_avg:341.48ms
step:1004/1775 train_time:342969ms step_avg:341.60ms
step:1005/1775 train_time:343433ms step_avg:341.72ms
step:1006/1775 train_time:343898ms step_avg:341.85ms
step:1007/1775 train_time:344363ms step_avg:341.97ms
step:1008/1775 train_time:344827ms step_avg:342.09ms
step:1009/1775 train_time:345290ms step_avg:342.21ms
step:1010/1775 train_time:345753ms step_avg:342.33ms
step:1011/1775 train_time:346218ms step_avg:342.45ms
step:1012/1775 train_time:346685ms step_avg:342.57ms
step:1013/1775 train_time:347149ms step_avg:342.69ms
step:1014/1775 train_time:347613ms step_avg:342.81ms
step:1015/1775 train_time:348078ms step_avg:342.93ms
step:1016/1775 train_time:348540ms step_avg:343.05ms
step:1017/1775 train_time:349006ms step_avg:343.17ms
step:1018/1775 train_time:349469ms step_avg:343.29ms
step:1019/1775 train_time:349933ms step_avg:343.41ms
step:1020/1775 train_time:350398ms step_avg:343.53ms
step:1021/1775 train_time:350862ms step_avg:343.64ms
step:1022/1775 train_time:351328ms step_avg:343.76ms
step:1023/1775 train_time:351791ms step_avg:343.88ms
step:1024/1775 train_time:352254ms step_avg:344.00ms
step:1025/1775 train_time:352719ms step_avg:344.12ms
step:1026/1775 train_time:353184ms step_avg:344.23ms
step:1027/1775 train_time:353647ms step_avg:344.35ms
step:1028/1775 train_time:354112ms step_avg:344.47ms
step:1029/1775 train_time:354575ms step_avg:344.58ms
step:1030/1775 train_time:355040ms step_avg:344.70ms
step:1031/1775 train_time:355505ms step_avg:344.82ms
step:1032/1775 train_time:355969ms step_avg:344.93ms
step:1033/1775 train_time:356432ms step_avg:345.05ms
step:1034/1775 train_time:356899ms step_avg:345.16ms
step:1035/1775 train_time:357363ms step_avg:345.28ms
step:1036/1775 train_time:357826ms step_avg:345.39ms
step:1037/1775 train_time:358290ms step_avg:345.51ms
step:1038/1775 train_time:358754ms step_avg:345.62ms
step:1039/1775 train_time:359219ms step_avg:345.74ms
step:1040/1775 train_time:359683ms step_avg:345.85ms
step:1041/1775 train_time:360146ms step_avg:345.96ms
step:1042/1775 train_time:360612ms step_avg:346.08ms
step:1043/1775 train_time:361077ms step_avg:346.19ms
step:1044/1775 train_time:361542ms step_avg:346.30ms
step:1045/1775 train_time:362006ms step_avg:346.42ms
step:1046/1775 train_time:362469ms step_avg:346.53ms
step:1047/1775 train_time:362933ms step_avg:346.64ms
step:1048/1775 train_time:363400ms step_avg:346.76ms
step:1049/1775 train_time:363863ms step_avg:346.87ms
step:1050/1775 train_time:364328ms step_avg:346.98ms
step:1051/1775 train_time:364792ms step_avg:347.09ms
step:1052/1775 train_time:365257ms step_avg:347.20ms
step:1053/1775 train_time:365722ms step_avg:347.31ms
step:1054/1775 train_time:366187ms step_avg:347.43ms
step:1055/1775 train_time:366650ms step_avg:347.54ms
step:1056/1775 train_time:367115ms step_avg:347.65ms
step:1057/1775 train_time:367579ms step_avg:347.76ms
step:1058/1775 train_time:368044ms step_avg:347.87ms
step:1059/1775 train_time:368506ms step_avg:347.98ms
step:1060/1775 train_time:368972ms step_avg:348.09ms
step:1061/1775 train_time:369436ms step_avg:348.20ms
step:1062/1775 train_time:369903ms step_avg:348.31ms
step:1063/1775 train_time:370367ms step_avg:348.42ms
step:1064/1775 train_time:370831ms step_avg:348.53ms
step:1065/1775 train_time:371296ms step_avg:348.63ms
step:1066/1775 train_time:371761ms step_avg:348.74ms
step:1067/1775 train_time:372226ms step_avg:348.85ms
step:1068/1775 train_time:372690ms step_avg:348.96ms
step:1069/1775 train_time:373155ms step_avg:349.07ms
step:1070/1775 train_time:373620ms step_avg:349.18ms
step:1071/1775 train_time:374083ms step_avg:349.28ms
step:1072/1775 train_time:374546ms step_avg:349.39ms
step:1073/1775 train_time:375009ms step_avg:349.50ms
step:1074/1775 train_time:375473ms step_avg:349.60ms
step:1075/1775 train_time:375937ms step_avg:349.71ms
step:1076/1775 train_time:376403ms step_avg:349.82ms
step:1077/1775 train_time:376866ms step_avg:349.92ms
step:1078/1775 train_time:377331ms step_avg:350.03ms
step:1079/1775 train_time:377794ms step_avg:350.13ms
step:1080/1775 train_time:378259ms step_avg:350.24ms
step:1081/1775 train_time:378723ms step_avg:350.35ms
step:1082/1775 train_time:379188ms step_avg:350.45ms
step:1083/1775 train_time:379652ms step_avg:350.56ms
step:1084/1775 train_time:380117ms step_avg:350.66ms
step:1085/1775 train_time:380581ms step_avg:350.77ms
step:1086/1775 train_time:381047ms step_avg:350.87ms
step:1087/1775 train_time:381510ms step_avg:350.98ms
step:1088/1775 train_time:381976ms step_avg:351.08ms
step:1089/1775 train_time:382440ms step_avg:351.18ms
step:1090/1775 train_time:382905ms step_avg:351.29ms
step:1091/1775 train_time:383369ms step_avg:351.39ms
step:1092/1775 train_time:383833ms step_avg:351.50ms
step:1093/1775 train_time:384298ms step_avg:351.60ms
step:1094/1775 train_time:384764ms step_avg:351.70ms
step:1095/1775 train_time:385226ms step_avg:351.81ms
step:1096/1775 train_time:385691ms step_avg:351.91ms
step:1097/1775 train_time:386154ms step_avg:352.01ms
step:1098/1775 train_time:386621ms step_avg:352.11ms
step:1099/1775 train_time:387084ms step_avg:352.21ms
step:1100/1775 train_time:387548ms step_avg:352.32ms
step:1101/1775 train_time:388013ms step_avg:352.42ms
step:1102/1775 train_time:388478ms step_avg:352.52ms
step:1103/1775 train_time:388942ms step_avg:352.62ms
step:1104/1775 train_time:389406ms step_avg:352.72ms
step:1105/1775 train_time:389870ms step_avg:352.82ms
step:1106/1775 train_time:390336ms step_avg:352.93ms
step:1107/1775 train_time:390800ms step_avg:353.03ms
step:1108/1775 train_time:391264ms step_avg:353.13ms
step:1109/1775 train_time:391728ms step_avg:353.23ms
step:1110/1775 train_time:392192ms step_avg:353.33ms
step:1111/1775 train_time:392657ms step_avg:353.43ms
step:1112/1775 train_time:393124ms step_avg:353.53ms
step:1113/1775 train_time:393586ms step_avg:353.63ms
step:1114/1775 train_time:394050ms step_avg:353.73ms
step:1115/1775 train_time:394514ms step_avg:353.82ms
step:1116/1775 train_time:394981ms step_avg:353.93ms
step:1117/1775 train_time:395443ms step_avg:354.02ms
step:1118/1775 train_time:395907ms step_avg:354.12ms
step:1119/1775 train_time:396371ms step_avg:354.22ms
step:1120/1775 train_time:396836ms step_avg:354.32ms
step:1121/1775 train_time:397301ms step_avg:354.42ms
step:1122/1775 train_time:397764ms step_avg:354.51ms
step:1123/1775 train_time:398228ms step_avg:354.61ms
step:1124/1775 train_time:398692ms step_avg:354.71ms
step:1125/1775 train_time:399157ms step_avg:354.81ms
step:1126/1775 train_time:399622ms step_avg:354.90ms
step:1127/1775 train_time:400086ms step_avg:355.00ms
step:1128/1775 train_time:400550ms step_avg:355.10ms
step:1129/1775 train_time:401013ms step_avg:355.19ms
step:1130/1775 train_time:401480ms step_avg:355.29ms
step:1131/1775 train_time:401944ms step_avg:355.39ms
step:1132/1775 train_time:402410ms step_avg:355.49ms
step:1133/1775 train_time:402875ms step_avg:355.58ms
step:1134/1775 train_time:403340ms step_avg:355.68ms
step:1135/1775 train_time:403804ms step_avg:355.77ms
step:1136/1775 train_time:404270ms step_avg:355.87ms
step:1137/1775 train_time:404734ms step_avg:355.97ms
step:1138/1775 train_time:405197ms step_avg:356.06ms
step:1139/1775 train_time:405663ms step_avg:356.16ms
step:1140/1775 train_time:406127ms step_avg:356.25ms
step:1141/1775 train_time:406592ms step_avg:356.35ms
step:1142/1775 train_time:407057ms step_avg:356.44ms
step:1143/1775 train_time:407521ms step_avg:356.54ms
step:1144/1775 train_time:407986ms step_avg:356.63ms
step:1145/1775 train_time:408450ms step_avg:356.72ms
step:1146/1775 train_time:408914ms step_avg:356.82ms
step:1147/1775 train_time:409378ms step_avg:356.91ms
step:1148/1775 train_time:409844ms step_avg:357.01ms
step:1149/1775 train_time:410305ms step_avg:357.10ms
step:1150/1775 train_time:410770ms step_avg:357.19ms
step:1151/1775 train_time:411234ms step_avg:357.28ms
step:1152/1775 train_time:411699ms step_avg:357.38ms
step:1153/1775 train_time:412163ms step_avg:357.47ms
step:1154/1775 train_time:412628ms step_avg:357.56ms
step:1155/1775 train_time:413090ms step_avg:357.65ms
step:1156/1775 train_time:413558ms step_avg:357.75ms
step:1157/1775 train_time:414022ms step_avg:357.84ms
step:1158/1775 train_time:414675ms step_avg:358.10ms
step:1159/1775 train_time:415351ms step_avg:358.37ms
step:1160/1775 train_time:416025ms step_avg:358.64ms
step:1161/1775 train_time:416698ms step_avg:358.91ms
step:1162/1775 train_time:417371ms step_avg:359.18ms
step:1163/1775 train_time:418049ms step_avg:359.46ms
step:1164/1775 train_time:418723ms step_avg:359.73ms
step:1165/1775 train_time:419400ms step_avg:360.00ms
step:1166/1775 train_time:420070ms step_avg:360.27ms
step:1167/1775 train_time:420747ms step_avg:360.54ms
step:1168/1775 train_time:421423ms step_avg:360.81ms
step:1169/1775 train_time:422100ms step_avg:361.08ms
step:1170/1775 train_time:422778ms step_avg:361.35ms
step:1171/1775 train_time:423454ms step_avg:361.62ms
step:1172/1775 train_time:424130ms step_avg:361.89ms
step:1173/1775 train_time:424808ms step_avg:362.15ms
step:1174/1775 train_time:425485ms step_avg:362.42ms
step:1175/1775 train_time:426165ms step_avg:362.69ms
step:1176/1775 train_time:426839ms step_avg:362.96ms
step:1177/1775 train_time:427509ms step_avg:363.22ms
step:1178/1775 train_time:428183ms step_avg:363.48ms
step:1179/1775 train_time:428860ms step_avg:363.75ms
step:1180/1775 train_time:429532ms step_avg:364.01ms
step:1181/1775 train_time:430208ms step_avg:364.27ms
step:1182/1775 train_time:430883ms step_avg:364.54ms
step:1183/1775 train_time:431556ms step_avg:364.80ms
step:1184/1775 train_time:432231ms step_avg:365.06ms
step:1185/1775 train_time:432905ms step_avg:365.32ms
step:1186/1775 train_time:433582ms step_avg:365.58ms
step:1187/1775 train_time:434257ms step_avg:365.84ms
step:1188/1775 train_time:434931ms step_avg:366.10ms
step:1189/1775 train_time:435608ms step_avg:366.36ms
step:1190/1775 train_time:436284ms step_avg:366.62ms
step:1191/1775 train_time:436961ms step_avg:366.89ms
step:1192/1775 train_time:437634ms step_avg:367.14ms
step:1193/1775 train_time:438308ms step_avg:367.40ms
step:1194/1775 train_time:438985ms step_avg:367.66ms
step:1195/1775 train_time:439658ms step_avg:367.91ms
step:1196/1775 train_time:440332ms step_avg:368.17ms
step:1197/1775 train_time:441005ms step_avg:368.43ms
step:1198/1775 train_time:441683ms step_avg:368.68ms
step:1199/1775 train_time:442356ms step_avg:368.94ms
step:1200/1775 train_time:443030ms step_avg:369.19ms
step:1201/1775 train_time:443702ms step_avg:369.44ms
step:1202/1775 train_time:444378ms step_avg:369.70ms
step:1203/1775 train_time:445050ms step_avg:369.95ms
step:1204/1775 train_time:445726ms step_avg:370.20ms
step:1205/1775 train_time:446400ms step_avg:370.46ms
step:1206/1775 train_time:447073ms step_avg:370.71ms
step:1207/1775 train_time:447751ms step_avg:370.96ms
step:1208/1775 train_time:448425ms step_avg:371.21ms
step:1209/1775 train_time:449101ms step_avg:371.46ms
step:1210/1775 train_time:449777ms step_avg:371.72ms
step:1211/1775 train_time:450452ms step_avg:371.97ms
step:1212/1775 train_time:451128ms step_avg:372.22ms
step:1213/1775 train_time:451803ms step_avg:372.47ms
step:1214/1775 train_time:452480ms step_avg:372.72ms
step:1215/1775 train_time:453157ms step_avg:372.97ms
step:1216/1775 train_time:453834ms step_avg:373.22ms
step:1217/1775 train_time:454509ms step_avg:373.47ms
step:1218/1775 train_time:455182ms step_avg:373.71ms
step:1219/1775 train_time:455859ms step_avg:373.96ms
step:1220/1775 train_time:456535ms step_avg:374.21ms
step:1221/1775 train_time:457213ms step_avg:374.46ms
step:1222/1775 train_time:457889ms step_avg:374.70ms
step:1223/1775 train_time:458565ms step_avg:374.95ms
step:1224/1775 train_time:459239ms step_avg:375.19ms
step:1225/1775 train_time:459914ms step_avg:375.44ms
step:1226/1775 train_time:460585ms step_avg:375.68ms
step:1227/1775 train_time:461260ms step_avg:375.93ms
step:1228/1775 train_time:461933ms step_avg:376.17ms
step:1229/1775 train_time:462608ms step_avg:376.41ms
step:1230/1775 train_time:463282ms step_avg:376.65ms
step:1231/1775 train_time:463959ms step_avg:376.90ms
step:1232/1775 train_time:464635ms step_avg:377.14ms
step:1233/1775 train_time:465308ms step_avg:377.38ms
step:1234/1775 train_time:465984ms step_avg:377.62ms
step:1235/1775 train_time:466656ms step_avg:377.86ms
step:1236/1775 train_time:467331ms step_avg:378.10ms
step:1237/1775 train_time:468009ms step_avg:378.34ms
step:1238/1775 train_time:468682ms step_avg:378.58ms
step:1239/1775 train_time:469358ms step_avg:378.82ms
step:1240/1775 train_time:470033ms step_avg:379.06ms
step:1241/1775 train_time:470709ms step_avg:379.30ms
step:1242/1775 train_time:471382ms step_avg:379.53ms
step:1243/1775 train_time:472059ms step_avg:379.77ms
step:1244/1775 train_time:472733ms step_avg:380.01ms
step:1245/1775 train_time:473409ms step_avg:380.25ms
step:1246/1775 train_time:474082ms step_avg:380.48ms
step:1247/1775 train_time:474757ms step_avg:380.72ms
step:1248/1775 train_time:475428ms step_avg:380.95ms
step:1249/1775 train_time:476107ms step_avg:381.19ms
step:1250/1775 train_time:476783ms step_avg:381.43ms
step:1250/1775 val_loss:3.5015 train_time:476884ms step_avg:381.51ms
step:1251/1775 train_time:477459ms step_avg:381.66ms
step:1252/1775 train_time:478135ms step_avg:381.90ms
step:1253/1775 train_time:478809ms step_avg:382.13ms
step:1254/1775 train_time:479485ms step_avg:382.36ms
step:1255/1775 train_time:480162ms step_avg:382.60ms
step:1256/1775 train_time:480838ms step_avg:382.83ms
step:1257/1775 train_time:481514ms step_avg:383.07ms
step:1258/1775 train_time:482190ms step_avg:383.30ms
step:1259/1775 train_time:482868ms step_avg:383.53ms
step:1260/1775 train_time:483541ms step_avg:383.76ms
step:1261/1775 train_time:484217ms step_avg:383.99ms
step:1262/1775 train_time:484892ms step_avg:384.23ms
step:1263/1775 train_time:485565ms step_avg:384.45ms
step:1264/1775 train_time:486238ms step_avg:384.68ms
step:1265/1775 train_time:486918ms step_avg:384.92ms
step:1266/1775 train_time:487593ms step_avg:385.14ms
step:1267/1775 train_time:488270ms step_avg:385.38ms
step:1268/1775 train_time:488944ms step_avg:385.60ms
step:1269/1775 train_time:489618ms step_avg:385.83ms
step:1270/1775 train_time:490295ms step_avg:386.06ms
step:1271/1775 train_time:490974ms step_avg:386.29ms
step:1272/1775 train_time:491648ms step_avg:386.52ms
step:1273/1775 train_time:492324ms step_avg:386.74ms
step:1274/1775 train_time:492996ms step_avg:386.97ms
step:1275/1775 train_time:493671ms step_avg:387.19ms
step:1276/1775 train_time:494345ms step_avg:387.42ms
step:1277/1775 train_time:495018ms step_avg:387.64ms
step:1278/1775 train_time:495694ms step_avg:387.87ms
step:1279/1775 train_time:496369ms step_avg:388.09ms
step:1280/1775 train_time:497042ms step_avg:388.31ms
step:1281/1775 train_time:497718ms step_avg:388.54ms
step:1282/1775 train_time:498391ms step_avg:388.76ms
step:1283/1775 train_time:499068ms step_avg:388.98ms
step:1284/1775 train_time:499744ms step_avg:389.21ms
step:1285/1775 train_time:500418ms step_avg:389.43ms
step:1286/1775 train_time:501095ms step_avg:389.65ms
step:1287/1775 train_time:501770ms step_avg:389.88ms
step:1288/1775 train_time:502445ms step_avg:390.10ms
step:1289/1775 train_time:503120ms step_avg:390.32ms
step:1290/1775 train_time:503793ms step_avg:390.54ms
step:1291/1775 train_time:504468ms step_avg:390.76ms
step:1292/1775 train_time:505143ms step_avg:390.98ms
step:1293/1775 train_time:505819ms step_avg:391.20ms
step:1294/1775 train_time:506494ms step_avg:391.42ms
step:1295/1775 train_time:507171ms step_avg:391.64ms
step:1296/1775 train_time:507843ms step_avg:391.85ms
step:1297/1775 train_time:508518ms step_avg:392.07ms
step:1298/1775 train_time:509192ms step_avg:392.29ms
step:1299/1775 train_time:509866ms step_avg:392.51ms
step:1300/1775 train_time:510542ms step_avg:392.72ms
step:1301/1775 train_time:511216ms step_avg:392.94ms
step:1302/1775 train_time:511891ms step_avg:393.16ms
step:1303/1775 train_time:512565ms step_avg:393.37ms
step:1304/1775 train_time:513241ms step_avg:393.59ms
step:1305/1775 train_time:513918ms step_avg:393.81ms
step:1306/1775 train_time:514594ms step_avg:394.02ms
step:1307/1775 train_time:515269ms step_avg:394.24ms
step:1308/1775 train_time:515947ms step_avg:394.45ms
step:1309/1775 train_time:516622ms step_avg:394.67ms
step:1310/1775 train_time:517299ms step_avg:394.88ms
step:1311/1775 train_time:517976ms step_avg:395.10ms
step:1312/1775 train_time:518652ms step_avg:395.31ms
step:1313/1775 train_time:519328ms step_avg:395.53ms
step:1314/1775 train_time:520002ms step_avg:395.74ms
step:1315/1775 train_time:520678ms step_avg:395.95ms
step:1316/1775 train_time:521351ms step_avg:396.16ms
step:1317/1775 train_time:522028ms step_avg:396.38ms
step:1318/1775 train_time:522702ms step_avg:396.59ms
step:1319/1775 train_time:523377ms step_avg:396.80ms
step:1320/1775 train_time:524054ms step_avg:397.01ms
step:1321/1775 train_time:524728ms step_avg:397.22ms
step:1322/1775 train_time:525404ms step_avg:397.43ms
step:1323/1775 train_time:526076ms step_avg:397.64ms
step:1324/1775 train_time:526755ms step_avg:397.85ms
step:1325/1775 train_time:527428ms step_avg:398.06ms
step:1326/1775 train_time:528103ms step_avg:398.27ms
step:1327/1775 train_time:528778ms step_avg:398.48ms
step:1328/1775 train_time:529453ms step_avg:398.68ms
step:1329/1775 train_time:530131ms step_avg:398.89ms
step:1330/1775 train_time:530806ms step_avg:399.10ms
step:1331/1775 train_time:531479ms step_avg:399.31ms
step:1332/1775 train_time:532151ms step_avg:399.51ms
step:1333/1775 train_time:532824ms step_avg:399.72ms
step:1334/1775 train_time:533500ms step_avg:399.93ms
step:1335/1775 train_time:534172ms step_avg:400.13ms
step:1336/1775 train_time:534850ms step_avg:400.34ms
step:1337/1775 train_time:535524ms step_avg:400.54ms
step:1338/1775 train_time:536200ms step_avg:400.75ms
step:1339/1775 train_time:536875ms step_avg:400.95ms
step:1340/1775 train_time:537550ms step_avg:401.16ms
step:1341/1775 train_time:538226ms step_avg:401.36ms
step:1342/1775 train_time:538898ms step_avg:401.56ms
step:1343/1775 train_time:539573ms step_avg:401.77ms
step:1344/1775 train_time:540251ms step_avg:401.97ms
step:1345/1775 train_time:540925ms step_avg:402.17ms
step:1346/1775 train_time:541601ms step_avg:402.38ms
step:1347/1775 train_time:542277ms step_avg:402.58ms
step:1348/1775 train_time:542952ms step_avg:402.78ms
step:1349/1775 train_time:543630ms step_avg:402.99ms
step:1350/1775 train_time:544303ms step_avg:403.19ms
step:1351/1775 train_time:544977ms step_avg:403.39ms
step:1352/1775 train_time:545650ms step_avg:403.59ms
step:1353/1775 train_time:546326ms step_avg:403.79ms
step:1354/1775 train_time:547002ms step_avg:403.99ms
step:1355/1775 train_time:547678ms step_avg:404.19ms
step:1356/1775 train_time:548355ms step_avg:404.39ms
step:1357/1775 train_time:549028ms step_avg:404.59ms
step:1358/1775 train_time:549702ms step_avg:404.79ms
step:1359/1775 train_time:550377ms step_avg:404.99ms
step:1360/1775 train_time:551056ms step_avg:405.19ms
step:1361/1775 train_time:551734ms step_avg:405.39ms
step:1362/1775 train_time:552409ms step_avg:405.59ms
step:1363/1775 train_time:553084ms step_avg:405.78ms
step:1364/1775 train_time:553759ms step_avg:405.98ms
step:1365/1775 train_time:554438ms step_avg:406.18ms
step:1366/1775 train_time:555115ms step_avg:406.38ms
step:1367/1775 train_time:555792ms step_avg:406.58ms
step:1368/1775 train_time:556469ms step_avg:406.78ms
step:1369/1775 train_time:557141ms step_avg:406.97ms
step:1370/1775 train_time:557818ms step_avg:407.17ms
step:1371/1775 train_time:558493ms step_avg:407.36ms
step:1372/1775 train_time:559169ms step_avg:407.56ms
step:1373/1775 train_time:559843ms step_avg:407.75ms
step:1374/1775 train_time:560518ms step_avg:407.95ms
step:1375/1775 train_time:561194ms step_avg:408.14ms
step:1376/1775 train_time:561867ms step_avg:408.33ms
step:1377/1775 train_time:562541ms step_avg:408.53ms
step:1378/1775 train_time:563214ms step_avg:408.72ms
step:1379/1775 train_time:563890ms step_avg:408.91ms
step:1380/1775 train_time:564566ms step_avg:409.11ms
step:1381/1775 train_time:565241ms step_avg:409.30ms
step:1382/1775 train_time:565916ms step_avg:409.49ms
step:1383/1775 train_time:566592ms step_avg:409.68ms
step:1384/1775 train_time:567271ms step_avg:409.88ms
step:1385/1775 train_time:567946ms step_avg:410.07ms
step:1386/1775 train_time:568621ms step_avg:410.26ms
step:1387/1775 train_time:569297ms step_avg:410.45ms
step:1388/1775 train_time:569972ms step_avg:410.64ms
step:1389/1775 train_time:570646ms step_avg:410.83ms
step:1390/1775 train_time:571318ms step_avg:411.02ms
step:1391/1775 train_time:571994ms step_avg:411.21ms
step:1392/1775 train_time:572670ms step_avg:411.40ms
step:1393/1775 train_time:573342ms step_avg:411.59ms
step:1394/1775 train_time:574018ms step_avg:411.78ms
step:1395/1775 train_time:574690ms step_avg:411.96ms
step:1396/1775 train_time:575366ms step_avg:412.15ms
step:1397/1775 train_time:576041ms step_avg:412.34ms
step:1398/1775 train_time:576714ms step_avg:412.53ms
step:1399/1775 train_time:577388ms step_avg:412.71ms
step:1400/1775 train_time:578061ms step_avg:412.90ms
step:1401/1775 train_time:578737ms step_avg:413.09ms
step:1402/1775 train_time:579413ms step_avg:413.28ms
step:1403/1775 train_time:580089ms step_avg:413.46ms
step:1404/1775 train_time:580764ms step_avg:413.65ms
step:1405/1775 train_time:581438ms step_avg:413.83ms
step:1406/1775 train_time:582114ms step_avg:414.02ms
step:1407/1775 train_time:582790ms step_avg:414.21ms
step:1408/1775 train_time:583465ms step_avg:414.39ms
step:1409/1775 train_time:584140ms step_avg:414.58ms
step:1410/1775 train_time:584816ms step_avg:414.76ms
step:1411/1775 train_time:585490ms step_avg:414.95ms
step:1412/1775 train_time:586166ms step_avg:415.13ms
step:1413/1775 train_time:586841ms step_avg:415.32ms
step:1414/1775 train_time:587517ms step_avg:415.50ms
step:1415/1775 train_time:588191ms step_avg:415.68ms
step:1416/1775 train_time:588866ms step_avg:415.87ms
step:1417/1775 train_time:589540ms step_avg:416.05ms
step:1418/1775 train_time:590218ms step_avg:416.23ms
step:1419/1775 train_time:590895ms step_avg:416.42ms
step:1420/1775 train_time:591568ms step_avg:416.60ms
step:1421/1775 train_time:592245ms step_avg:416.78ms
step:1422/1775 train_time:592922ms step_avg:416.96ms
step:1423/1775 train_time:593598ms step_avg:417.15ms
step:1424/1775 train_time:594271ms step_avg:417.33ms
step:1425/1775 train_time:594946ms step_avg:417.51ms
step:1426/1775 train_time:595620ms step_avg:417.69ms
step:1427/1775 train_time:596297ms step_avg:417.87ms
step:1428/1775 train_time:596973ms step_avg:418.05ms
step:1429/1775 train_time:597649ms step_avg:418.23ms
step:1430/1775 train_time:598325ms step_avg:418.41ms
step:1431/1775 train_time:598999ms step_avg:418.59ms
step:1432/1775 train_time:599677ms step_avg:418.77ms
step:1433/1775 train_time:600350ms step_avg:418.95ms
step:1434/1775 train_time:601026ms step_avg:419.13ms
step:1435/1775 train_time:601699ms step_avg:419.30ms
step:1436/1775 train_time:602372ms step_avg:419.48ms
step:1437/1775 train_time:603045ms step_avg:419.66ms
step:1438/1775 train_time:603722ms step_avg:419.83ms
step:1439/1775 train_time:604397ms step_avg:420.01ms
step:1440/1775 train_time:605074ms step_avg:420.19ms
step:1441/1775 train_time:605747ms step_avg:420.37ms
step:1442/1775 train_time:606422ms step_avg:420.54ms
step:1443/1775 train_time:607098ms step_avg:420.72ms
step:1444/1775 train_time:607775ms step_avg:420.90ms
step:1445/1775 train_time:608453ms step_avg:421.07ms
step:1446/1775 train_time:609128ms step_avg:421.25ms
step:1447/1775 train_time:609803ms step_avg:421.43ms
step:1448/1775 train_time:610478ms step_avg:421.60ms
step:1449/1775 train_time:611155ms step_avg:421.78ms
step:1450/1775 train_time:611831ms step_avg:421.95ms
step:1451/1775 train_time:612507ms step_avg:422.13ms
step:1452/1775 train_time:613181ms step_avg:422.30ms
step:1453/1775 train_time:613859ms step_avg:422.48ms
step:1454/1775 train_time:614534ms step_avg:422.65ms
step:1455/1775 train_time:615211ms step_avg:422.83ms
step:1456/1775 train_time:615887ms step_avg:423.00ms
step:1457/1775 train_time:616560ms step_avg:423.17ms
step:1458/1775 train_time:617238ms step_avg:423.35ms
step:1459/1775 train_time:617912ms step_avg:423.52ms
step:1460/1775 train_time:618588ms step_avg:423.69ms
step:1461/1775 train_time:619265ms step_avg:423.86ms
step:1462/1775 train_time:619940ms step_avg:424.04ms
step:1463/1775 train_time:620618ms step_avg:424.21ms
step:1464/1775 train_time:621295ms step_avg:424.38ms
step:1465/1775 train_time:621969ms step_avg:424.55ms
step:1466/1775 train_time:622640ms step_avg:424.72ms
step:1467/1775 train_time:623317ms step_avg:424.89ms
step:1468/1775 train_time:623993ms step_avg:425.06ms
step:1469/1775 train_time:624671ms step_avg:425.24ms
step:1470/1775 train_time:625347ms step_avg:425.41ms
step:1471/1775 train_time:626021ms step_avg:425.57ms
step:1472/1775 train_time:626695ms step_avg:425.74ms
step:1473/1775 train_time:627369ms step_avg:425.91ms
step:1474/1775 train_time:628041ms step_avg:426.08ms
step:1475/1775 train_time:628716ms step_avg:426.25ms
step:1476/1775 train_time:629388ms step_avg:426.41ms
step:1477/1775 train_time:630064ms step_avg:426.58ms
step:1478/1775 train_time:630738ms step_avg:426.75ms
step:1479/1775 train_time:631413ms step_avg:426.92ms
step:1480/1775 train_time:632091ms step_avg:427.09ms
step:1481/1775 train_time:632762ms step_avg:427.25ms
step:1482/1775 train_time:633438ms step_avg:427.42ms
step:1483/1775 train_time:634115ms step_avg:427.59ms
step:1484/1775 train_time:634790ms step_avg:427.76ms
step:1485/1775 train_time:635464ms step_avg:427.92ms
step:1486/1775 train_time:636139ms step_avg:428.09ms
step:1487/1775 train_time:636815ms step_avg:428.26ms
step:1488/1775 train_time:637490ms step_avg:428.42ms
step:1489/1775 train_time:638167ms step_avg:428.59ms
step:1490/1775 train_time:638843ms step_avg:428.75ms
step:1491/1775 train_time:639514ms step_avg:428.92ms
step:1492/1775 train_time:640189ms step_avg:429.08ms
step:1493/1775 train_time:640863ms step_avg:429.25ms
step:1494/1775 train_time:641536ms step_avg:429.41ms
step:1495/1775 train_time:642209ms step_avg:429.57ms
step:1496/1775 train_time:642886ms step_avg:429.74ms
step:1497/1775 train_time:643562ms step_avg:429.90ms
step:1498/1775 train_time:644239ms step_avg:430.07ms
step:1499/1775 train_time:644915ms step_avg:430.23ms
step:1500/1775 train_time:645591ms step_avg:430.39ms
step:1500/1775 val_loss:3.3756 train_time:645693ms step_avg:430.46ms
step:1501/1775 train_time:646270ms step_avg:430.56ms
step:1502/1775 train_time:646946ms step_avg:430.72ms
step:1503/1775 train_time:647621ms step_avg:430.89ms
step:1504/1775 train_time:648300ms step_avg:431.05ms
step:1505/1775 train_time:648977ms step_avg:431.21ms
step:1506/1775 train_time:649650ms step_avg:431.37ms
step:1507/1775 train_time:650326ms step_avg:431.54ms
step:1508/1775 train_time:651002ms step_avg:431.70ms
step:1509/1775 train_time:651677ms step_avg:431.86ms
step:1510/1775 train_time:652354ms step_avg:432.02ms
step:1511/1775 train_time:653031ms step_avg:432.18ms
step:1512/1775 train_time:653707ms step_avg:432.35ms
step:1513/1775 train_time:654381ms step_avg:432.51ms
step:1514/1775 train_time:655057ms step_avg:432.67ms
step:1515/1775 train_time:655731ms step_avg:432.83ms
step:1516/1775 train_time:656408ms step_avg:432.99ms
step:1517/1775 train_time:657080ms step_avg:433.14ms
step:1518/1775 train_time:657756ms step_avg:433.30ms
step:1519/1775 train_time:658432ms step_avg:433.46ms
step:1520/1775 train_time:659108ms step_avg:433.62ms
step:1521/1775 train_time:659784ms step_avg:433.78ms
step:1522/1775 train_time:660460ms step_avg:433.94ms
step:1523/1775 train_time:661133ms step_avg:434.10ms
step:1524/1775 train_time:661806ms step_avg:434.26ms
step:1525/1775 train_time:662479ms step_avg:434.41ms
step:1526/1775 train_time:663153ms step_avg:434.57ms
step:1527/1775 train_time:663827ms step_avg:434.73ms
step:1528/1775 train_time:664501ms step_avg:434.88ms
step:1529/1775 train_time:665176ms step_avg:435.04ms
step:1530/1775 train_time:665854ms step_avg:435.20ms
step:1531/1775 train_time:666529ms step_avg:435.36ms
step:1532/1775 train_time:667203ms step_avg:435.51ms
step:1533/1775 train_time:667876ms step_avg:435.67ms
step:1534/1775 train_time:668552ms step_avg:435.82ms
step:1535/1775 train_time:669229ms step_avg:435.98ms
step:1536/1775 train_time:669906ms step_avg:436.14ms
step:1537/1775 train_time:670578ms step_avg:436.29ms
step:1538/1775 train_time:671255ms step_avg:436.45ms
step:1539/1775 train_time:671929ms step_avg:436.60ms
step:1540/1775 train_time:672605ms step_avg:436.76ms
step:1541/1775 train_time:673280ms step_avg:436.91ms
step:1542/1775 train_time:673955ms step_avg:437.07ms
step:1543/1775 train_time:674632ms step_avg:437.22ms
step:1544/1775 train_time:675305ms step_avg:437.37ms
step:1545/1775 train_time:675980ms step_avg:437.53ms
step:1546/1775 train_time:676653ms step_avg:437.68ms
step:1547/1775 train_time:677325ms step_avg:437.83ms
step:1548/1775 train_time:677997ms step_avg:437.98ms
step:1549/1775 train_time:678670ms step_avg:438.13ms
step:1550/1775 train_time:679347ms step_avg:438.29ms
step:1551/1775 train_time:680024ms step_avg:438.44ms
step:1552/1775 train_time:680698ms step_avg:438.59ms
step:1553/1775 train_time:681375ms step_avg:438.75ms
step:1554/1775 train_time:682053ms step_avg:438.90ms
step:1555/1775 train_time:682727ms step_avg:439.05ms
step:1556/1775 train_time:683404ms step_avg:439.21ms
step:1557/1775 train_time:684079ms step_avg:439.36ms
step:1558/1775 train_time:684753ms step_avg:439.51ms
step:1559/1775 train_time:685429ms step_avg:439.66ms
step:1560/1775 train_time:686106ms step_avg:439.81ms
step:1561/1775 train_time:686779ms step_avg:439.96ms
step:1562/1775 train_time:687454ms step_avg:440.11ms
step:1563/1775 train_time:688128ms step_avg:440.26ms
step:1564/1775 train_time:688802ms step_avg:440.41ms
step:1565/1775 train_time:689477ms step_avg:440.56ms
step:1566/1775 train_time:690151ms step_avg:440.71ms
step:1567/1775 train_time:690826ms step_avg:440.86ms
step:1568/1775 train_time:691499ms step_avg:441.01ms
step:1569/1775 train_time:692172ms step_avg:441.15ms
step:1570/1775 train_time:692849ms step_avg:441.31ms
step:1571/1775 train_time:693522ms step_avg:441.45ms
step:1572/1775 train_time:694200ms step_avg:441.60ms
step:1573/1775 train_time:694876ms step_avg:441.75ms
step:1574/1775 train_time:695549ms step_avg:441.90ms
step:1575/1775 train_time:696223ms step_avg:442.05ms
step:1576/1775 train_time:696898ms step_avg:442.19ms
step:1577/1775 train_time:697572ms step_avg:442.34ms
step:1578/1775 train_time:698250ms step_avg:442.49ms
step:1579/1775 train_time:698922ms step_avg:442.64ms
step:1580/1775 train_time:699599ms step_avg:442.78ms
step:1581/1775 train_time:700277ms step_avg:442.93ms
step:1582/1775 train_time:700952ms step_avg:443.08ms
step:1583/1775 train_time:701629ms step_avg:443.23ms
step:1584/1775 train_time:702304ms step_avg:443.37ms
step:1585/1775 train_time:702978ms step_avg:443.52ms
step:1586/1775 train_time:703656ms step_avg:443.67ms
step:1587/1775 train_time:704332ms step_avg:443.81ms
step:1588/1775 train_time:705007ms step_avg:443.96ms
step:1589/1775 train_time:705683ms step_avg:444.10ms
step:1590/1775 train_time:706360ms step_avg:444.25ms
step:1591/1775 train_time:707038ms step_avg:444.40ms
step:1592/1775 train_time:707711ms step_avg:444.54ms
step:1593/1775 train_time:708389ms step_avg:444.69ms
step:1594/1775 train_time:709060ms step_avg:444.83ms
step:1595/1775 train_time:709732ms step_avg:444.97ms
step:1596/1775 train_time:710410ms step_avg:445.12ms
step:1597/1775 train_time:711084ms step_avg:445.26ms
step:1598/1775 train_time:711759ms step_avg:445.41ms
step:1599/1775 train_time:712435ms step_avg:445.55ms
step:1600/1775 train_time:713110ms step_avg:445.69ms
step:1601/1775 train_time:713783ms step_avg:445.84ms
step:1602/1775 train_time:714460ms step_avg:445.98ms
step:1603/1775 train_time:715134ms step_avg:446.12ms
step:1604/1775 train_time:715806ms step_avg:446.26ms
step:1605/1775 train_time:716480ms step_avg:446.40ms
step:1606/1775 train_time:717154ms step_avg:446.55ms
step:1607/1775 train_time:717826ms step_avg:446.69ms
step:1608/1775 train_time:718501ms step_avg:446.83ms
step:1609/1775 train_time:719173ms step_avg:446.97ms
step:1610/1775 train_time:719849ms step_avg:447.11ms
step:1611/1775 train_time:720525ms step_avg:447.25ms
step:1612/1775 train_time:721196ms step_avg:447.39ms
step:1613/1775 train_time:721873ms step_avg:447.53ms
step:1614/1775 train_time:722551ms step_avg:447.68ms
step:1615/1775 train_time:723225ms step_avg:447.82ms
step:1616/1775 train_time:723901ms step_avg:447.96ms
step:1617/1775 train_time:724579ms step_avg:448.10ms
step:1618/1775 train_time:725252ms step_avg:448.24ms
step:1619/1775 train_time:725927ms step_avg:448.38ms
step:1620/1775 train_time:726604ms step_avg:448.52ms
step:1621/1775 train_time:727279ms step_avg:448.66ms
step:1622/1775 train_time:727957ms step_avg:448.80ms
step:1623/1775 train_time:728633ms step_avg:448.94ms
step:1624/1775 train_time:729308ms step_avg:449.08ms
step:1625/1775 train_time:729985ms step_avg:449.22ms
step:1626/1775 train_time:730657ms step_avg:449.36ms
step:1627/1775 train_time:731333ms step_avg:449.50ms
step:1628/1775 train_time:732008ms step_avg:449.64ms
step:1629/1775 train_time:732683ms step_avg:449.77ms
step:1630/1775 train_time:733360ms step_avg:449.91ms
step:1631/1775 train_time:734034ms step_avg:450.05ms
step:1632/1775 train_time:734713ms step_avg:450.19ms
step:1633/1775 train_time:735389ms step_avg:450.33ms
step:1634/1775 train_time:736061ms step_avg:450.47ms
step:1635/1775 train_time:736732ms step_avg:450.60ms
step:1636/1775 train_time:737407ms step_avg:450.74ms
step:1637/1775 train_time:738085ms step_avg:450.88ms
step:1638/1775 train_time:738759ms step_avg:451.01ms
step:1639/1775 train_time:739439ms step_avg:451.15ms
step:1640/1775 train_time:740111ms step_avg:451.29ms
step:1641/1775 train_time:740787ms step_avg:451.42ms
step:1642/1775 train_time:741466ms step_avg:451.56ms
step:1643/1775 train_time:742140ms step_avg:451.70ms
step:1644/1775 train_time:742813ms step_avg:451.83ms
step:1645/1775 train_time:743490ms step_avg:451.97ms
step:1646/1775 train_time:744165ms step_avg:452.11ms
step:1647/1775 train_time:744842ms step_avg:452.24ms
step:1648/1775 train_time:745519ms step_avg:452.38ms
step:1649/1775 train_time:746190ms step_avg:452.51ms
step:1650/1775 train_time:746864ms step_avg:452.64ms
step:1651/1775 train_time:747537ms step_avg:452.78ms
step:1652/1775 train_time:748213ms step_avg:452.91ms
step:1653/1775 train_time:748889ms step_avg:453.05ms
step:1654/1775 train_time:749564ms step_avg:453.18ms
step:1655/1775 train_time:750242ms step_avg:453.32ms
step:1656/1775 train_time:750918ms step_avg:453.45ms
step:1657/1775 train_time:751593ms step_avg:453.59ms
step:1658/1775 train_time:752268ms step_avg:453.72ms
step:1659/1775 train_time:752942ms step_avg:453.85ms
step:1660/1775 train_time:753616ms step_avg:453.99ms
step:1661/1775 train_time:754288ms step_avg:454.12ms
step:1662/1775 train_time:754963ms step_avg:454.25ms
step:1663/1775 train_time:755640ms step_avg:454.38ms
step:1664/1775 train_time:756318ms step_avg:454.52ms
step:1665/1775 train_time:756995ms step_avg:454.65ms
step:1666/1775 train_time:757665ms step_avg:454.78ms
step:1667/1775 train_time:758340ms step_avg:454.91ms
step:1668/1775 train_time:759015ms step_avg:455.05ms
step:1669/1775 train_time:759691ms step_avg:455.18ms
step:1670/1775 train_time:760366ms step_avg:455.31ms
step:1671/1775 train_time:761040ms step_avg:455.44ms
step:1672/1775 train_time:761716ms step_avg:455.57ms
step:1673/1775 train_time:762393ms step_avg:455.70ms
step:1674/1775 train_time:763070ms step_avg:455.84ms
step:1675/1775 train_time:763745ms step_avg:455.97ms
step:1676/1775 train_time:764418ms step_avg:456.10ms
step:1677/1775 train_time:765093ms step_avg:456.23ms
step:1678/1775 train_time:765766ms step_avg:456.36ms
step:1679/1775 train_time:766442ms step_avg:456.49ms
step:1680/1775 train_time:767117ms step_avg:456.62ms
step:1681/1775 train_time:767793ms step_avg:456.75ms
step:1682/1775 train_time:768468ms step_avg:456.88ms
step:1683/1775 train_time:769145ms step_avg:457.01ms
step:1684/1775 train_time:769822ms step_avg:457.14ms
step:1685/1775 train_time:770495ms step_avg:457.27ms
step:1686/1775 train_time:771169ms step_avg:457.40ms
step:1687/1775 train_time:771843ms step_avg:457.52ms
step:1688/1775 train_time:772520ms step_avg:457.65ms
step:1689/1775 train_time:773196ms step_avg:457.78ms
step:1690/1775 train_time:773869ms step_avg:457.91ms
step:1691/1775 train_time:774546ms step_avg:458.04ms
step:1692/1775 train_time:775220ms step_avg:458.17ms
step:1693/1775 train_time:775891ms step_avg:458.29ms
step:1694/1775 train_time:776568ms step_avg:458.42ms
step:1695/1775 train_time:777241ms step_avg:458.55ms
step:1696/1775 train_time:777915ms step_avg:458.68ms
step:1697/1775 train_time:778588ms step_avg:458.80ms
step:1698/1775 train_time:779263ms step_avg:458.93ms
step:1699/1775 train_time:779938ms step_avg:459.06ms
step:1700/1775 train_time:780614ms step_avg:459.18ms
step:1701/1775 train_time:781290ms step_avg:459.31ms
step:1702/1775 train_time:781964ms step_avg:459.44ms
step:1703/1775 train_time:782639ms step_avg:459.57ms
step:1704/1775 train_time:783315ms step_avg:459.69ms
step:1705/1775 train_time:783990ms step_avg:459.82ms
step:1706/1775 train_time:784667ms step_avg:459.95ms
step:1707/1775 train_time:785342ms step_avg:460.07ms
step:1708/1775 train_time:786016ms step_avg:460.20ms
step:1709/1775 train_time:786693ms step_avg:460.32ms
step:1710/1775 train_time:787366ms step_avg:460.45ms
step:1711/1775 train_time:788041ms step_avg:460.57ms
step:1712/1775 train_time:788715ms step_avg:460.70ms
step:1713/1775 train_time:789388ms step_avg:460.82ms
step:1714/1775 train_time:790065ms step_avg:460.95ms
step:1715/1775 train_time:790738ms step_avg:461.07ms
step:1716/1775 train_time:791414ms step_avg:461.20ms
step:1717/1775 train_time:792089ms step_avg:461.32ms
step:1718/1775 train_time:792762ms step_avg:461.44ms
step:1719/1775 train_time:793438ms step_avg:461.57ms
step:1720/1775 train_time:794110ms step_avg:461.69ms
step:1721/1775 train_time:794785ms step_avg:461.82ms
step:1722/1775 train_time:795462ms step_avg:461.94ms
step:1723/1775 train_time:796135ms step_avg:462.06ms
step:1724/1775 train_time:796812ms step_avg:462.19ms
step:1725/1775 train_time:797488ms step_avg:462.31ms
step:1726/1775 train_time:798161ms step_avg:462.43ms
step:1727/1775 train_time:798835ms step_avg:462.56ms
step:1728/1775 train_time:799507ms step_avg:462.68ms
step:1729/1775 train_time:800179ms step_avg:462.80ms
step:1730/1775 train_time:800858ms step_avg:462.92ms
step:1731/1775 train_time:801534ms step_avg:463.05ms
step:1732/1775 train_time:802211ms step_avg:463.17ms
step:1733/1775 train_time:802882ms step_avg:463.29ms
step:1734/1775 train_time:803559ms step_avg:463.41ms
step:1735/1775 train_time:804236ms step_avg:463.54ms
step:1736/1775 train_time:804917ms step_avg:463.66ms
step:1737/1775 train_time:805597ms step_avg:463.79ms
step:1738/1775 train_time:806275ms step_avg:463.91ms
step:1739/1775 train_time:806951ms step_avg:464.03ms
step:1740/1775 train_time:807632ms step_avg:464.16ms
step:1741/1775 train_time:808308ms step_avg:464.28ms
step:1742/1775 train_time:808986ms step_avg:464.40ms
step:1743/1775 train_time:809663ms step_avg:464.52ms
step:1744/1775 train_time:810342ms step_avg:464.65ms
step:1745/1775 train_time:811018ms step_avg:464.77ms
step:1746/1775 train_time:811695ms step_avg:464.89ms
step:1747/1775 train_time:812372ms step_avg:465.01ms
step:1748/1775 train_time:813051ms step_avg:465.13ms
step:1749/1775 train_time:813726ms step_avg:465.25ms
step:1750/1775 train_time:814404ms step_avg:465.37ms
step:1750/1775 val_loss:3.2841 train_time:814506ms step_avg:465.43ms
step:1751/1775 train_time:815081ms step_avg:465.49ms
step:1752/1775 train_time:815758ms step_avg:465.62ms
step:1753/1775 train_time:816438ms step_avg:465.74ms
step:1754/1775 train_time:817114ms step_avg:465.86ms
step:1755/1775 train_time:817792ms step_avg:465.98ms
step:1756/1775 train_time:818470ms step_avg:466.10ms
step:1757/1775 train_time:819148ms step_avg:466.22ms
step:1758/1775 train_time:819825ms step_avg:466.34ms
step:1759/1775 train_time:820505ms step_avg:466.46ms
step:1760/1775 train_time:821177ms step_avg:466.58ms
step:1761/1775 train_time:821853ms step_avg:466.70ms
step:1762/1775 train_time:822533ms step_avg:466.82ms
step:1763/1775 train_time:823210ms step_avg:466.94ms
step:1764/1775 train_time:823890ms step_avg:467.06ms
step:1765/1775 train_time:824571ms step_avg:467.18ms
step:1766/1775 train_time:825248ms step_avg:467.30ms
step:1767/1775 train_time:825926ms step_avg:467.42ms
step:1768/1775 train_time:826605ms step_avg:467.54ms
step:1769/1775 train_time:827283ms step_avg:467.66ms
step:1770/1775 train_time:827957ms step_avg:467.77ms
step:1771/1775 train_time:828634ms step_avg:467.89ms
step:1772/1775 train_time:829311ms step_avg:468.01ms
step:1773/1775 train_time:829985ms step_avg:468.12ms
step:1774/1775 train_time:830666ms step_avg:468.24ms
step:1775/1775 train_time:831344ms step_avg:468.36ms
step:1775/1775 val_loss:3.2777 train_time:831445ms step_avg:468.42ms
peak memory allocated: 32826 MiB reserved: 47960 MiB

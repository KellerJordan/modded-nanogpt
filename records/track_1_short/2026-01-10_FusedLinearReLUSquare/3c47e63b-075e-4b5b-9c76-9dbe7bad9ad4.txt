import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,  #
                                 M, N, K,  #
                                 BLOCK_SIZE_M: tl.constexpr,  #
                                 BLOCK_SIZE_N: tl.constexpr,  #
                                 BLOCK_SIZE_K: tl.constexpr,  #
                                 GROUP_SIZE_M: tl.constexpr,  #
                                 NUM_SMS: tl.constexpr,  #
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,#
        M, N, K,  #
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,  #
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Jan 11 04:32:51 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           25239      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    1   N/A  N/A           25240      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    2   N/A  N/A           25241      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    3   N/A  N/A           25242      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    4   N/A  N/A           25243      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    5   N/A  N/A           25244      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    6   N/A  N/A           25245      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    7   N/A  N/A           25246      C   /home/ubuntu/venv/bin/python3          1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8318 train_time:0ms step_avg:0.42ms
step:1/1775 train_time:109ms step_avg:108.93ms
step:2/1775 train_time:142ms step_avg:70.77ms
step:3/1775 train_time:170ms step_avg:56.70ms
step:4/1775 train_time:200ms step_avg:50.09ms
step:5/1775 train_time:228ms step_avg:45.53ms
step:6/1775 train_time:312ms step_avg:51.99ms
step:7/1775 train_time:334ms step_avg:47.65ms
step:8/1775 train_time:482ms step_avg:60.24ms
step:9/1775 train_time:513ms step_avg:56.96ms
step:10/1775 train_time:546ms step_avg:54.56ms
step:11/1775 train_time:577ms step_avg:52.41ms
step:12/1775 train_time:610ms step_avg:50.82ms
step:13/1775 train_time:641ms step_avg:49.31ms
step:14/1775 train_time:674ms step_avg:48.17ms
step:15/1775 train_time:706ms step_avg:47.04ms
step:16/1775 train_time:739ms step_avg:46.18ms
step:17/1775 train_time:770ms step_avg:45.28ms
step:18/1775 train_time:803ms step_avg:44.61ms
step:19/1775 train_time:834ms step_avg:43.90ms
step:20/1775 train_time:867ms step_avg:43.36ms
step:21/1775 train_time:898ms step_avg:42.78ms
step:22/1775 train_time:932ms step_avg:42.35ms
step:23/1775 train_time:963ms step_avg:41.85ms
step:24/1775 train_time:996ms step_avg:41.50ms
step:25/1775 train_time:1027ms step_avg:41.08ms
step:26/1775 train_time:1061ms step_avg:40.79ms
step:27/1775 train_time:1092ms step_avg:40.43ms
step:28/1775 train_time:1125ms step_avg:40.19ms
step:29/1775 train_time:1156ms step_avg:39.87ms
step:30/1775 train_time:1190ms step_avg:39.66ms
step:31/1775 train_time:1221ms step_avg:39.39ms
step:32/1775 train_time:1254ms step_avg:39.19ms
step:33/1775 train_time:1285ms step_avg:38.94ms
step:34/1775 train_time:1318ms step_avg:38.78ms
step:35/1775 train_time:1351ms step_avg:38.61ms
step:36/1775 train_time:1389ms step_avg:38.59ms
step:37/1775 train_time:1422ms step_avg:38.44ms
step:38/1775 train_time:1457ms step_avg:38.35ms
step:39/1775 train_time:1490ms step_avg:38.20ms
step:40/1775 train_time:1525ms step_avg:38.12ms
step:41/1775 train_time:1557ms step_avg:37.97ms
step:42/1775 train_time:1591ms step_avg:37.88ms
step:43/1775 train_time:1623ms step_avg:37.73ms
step:44/1775 train_time:1656ms step_avg:37.63ms
step:45/1775 train_time:1687ms step_avg:37.49ms
step:46/1775 train_time:1721ms step_avg:37.41ms
step:47/1775 train_time:1752ms step_avg:37.27ms
step:48/1775 train_time:1785ms step_avg:37.19ms
step:49/1775 train_time:1816ms step_avg:37.07ms
step:50/1775 train_time:1850ms step_avg:36.99ms
step:51/1775 train_time:1881ms step_avg:36.88ms
step:52/1775 train_time:1914ms step_avg:36.81ms
step:53/1775 train_time:1946ms step_avg:36.71ms
step:54/1775 train_time:1979ms step_avg:36.65ms
step:55/1775 train_time:2010ms step_avg:36.55ms
step:56/1775 train_time:2043ms step_avg:36.49ms
step:57/1775 train_time:2074ms step_avg:36.39ms
step:58/1775 train_time:2108ms step_avg:36.35ms
step:59/1775 train_time:2139ms step_avg:36.26ms
step:60/1775 train_time:2172ms step_avg:36.21ms
step:61/1775 train_time:2204ms step_avg:36.13ms
step:62/1775 train_time:2237ms step_avg:36.08ms
step:63/1775 train_time:2268ms step_avg:36.01ms
step:64/1775 train_time:2303ms step_avg:35.98ms
step:65/1775 train_time:2334ms step_avg:35.91ms
step:66/1775 train_time:2368ms step_avg:35.89ms
step:67/1775 train_time:2401ms step_avg:35.83ms
step:68/1775 train_time:2434ms step_avg:35.80ms
step:69/1775 train_time:2466ms step_avg:35.74ms
step:70/1775 train_time:2500ms step_avg:35.71ms
step:71/1775 train_time:2531ms step_avg:35.65ms
step:72/1775 train_time:2565ms step_avg:35.62ms
step:73/1775 train_time:2597ms step_avg:35.57ms
step:74/1775 train_time:2631ms step_avg:35.55ms
step:75/1775 train_time:2663ms step_avg:35.50ms
step:76/1775 train_time:2696ms step_avg:35.47ms
step:77/1775 train_time:2728ms step_avg:35.42ms
step:78/1775 train_time:2761ms step_avg:35.40ms
step:79/1775 train_time:2793ms step_avg:35.35ms
step:80/1775 train_time:2827ms step_avg:35.33ms
step:81/1775 train_time:2858ms step_avg:35.29ms
step:82/1775 train_time:2892ms step_avg:35.26ms
step:83/1775 train_time:2923ms step_avg:35.22ms
step:84/1775 train_time:2956ms step_avg:35.19ms
step:85/1775 train_time:2987ms step_avg:35.15ms
step:86/1775 train_time:3021ms step_avg:35.13ms
step:87/1775 train_time:3053ms step_avg:35.09ms
step:88/1775 train_time:3086ms step_avg:35.07ms
step:89/1775 train_time:3117ms step_avg:35.03ms
step:90/1775 train_time:3151ms step_avg:35.01ms
step:91/1775 train_time:3182ms step_avg:34.97ms
step:92/1775 train_time:3216ms step_avg:34.96ms
step:93/1775 train_time:3248ms step_avg:34.92ms
step:94/1775 train_time:3282ms step_avg:34.91ms
step:95/1775 train_time:3313ms step_avg:34.87ms
step:96/1775 train_time:3346ms step_avg:34.86ms
step:97/1775 train_time:3378ms step_avg:34.83ms
step:98/1775 train_time:3412ms step_avg:34.82ms
step:99/1775 train_time:3444ms step_avg:34.79ms
step:100/1775 train_time:3478ms step_avg:34.78ms
step:101/1775 train_time:3509ms step_avg:34.75ms
step:102/1775 train_time:3543ms step_avg:34.73ms
step:103/1775 train_time:3574ms step_avg:34.70ms
step:104/1775 train_time:3609ms step_avg:34.70ms
step:105/1775 train_time:3640ms step_avg:34.67ms
step:106/1775 train_time:3673ms step_avg:34.65ms
step:107/1775 train_time:3704ms step_avg:34.62ms
step:108/1775 train_time:3738ms step_avg:34.61ms
step:109/1775 train_time:3770ms step_avg:34.59ms
step:110/1775 train_time:3804ms step_avg:34.59ms
step:111/1775 train_time:3836ms step_avg:34.56ms
step:112/1775 train_time:3869ms step_avg:34.55ms
step:113/1775 train_time:3900ms step_avg:34.52ms
step:114/1775 train_time:3933ms step_avg:34.50ms
step:115/1775 train_time:3965ms step_avg:34.48ms
step:116/1775 train_time:3998ms step_avg:34.47ms
step:117/1775 train_time:4030ms step_avg:34.44ms
step:118/1775 train_time:4064ms step_avg:34.44ms
step:119/1775 train_time:4095ms step_avg:34.41ms
step:120/1775 train_time:4129ms step_avg:34.41ms
step:121/1775 train_time:4160ms step_avg:34.38ms
step:122/1775 train_time:4193ms step_avg:34.37ms
step:123/1775 train_time:4225ms step_avg:34.35ms
step:124/1775 train_time:4258ms step_avg:34.34ms
step:125/1775 train_time:4290ms step_avg:34.32ms
step:126/1775 train_time:4324ms step_avg:34.32ms
step:127/1775 train_time:4355ms step_avg:34.29ms
step:128/1775 train_time:4389ms step_avg:34.29ms
step:129/1775 train_time:4421ms step_avg:34.27ms
step:130/1775 train_time:4455ms step_avg:34.27ms
step:131/1775 train_time:4486ms step_avg:34.24ms
step:132/1775 train_time:4520ms step_avg:34.24ms
step:133/1775 train_time:4551ms step_avg:34.22ms
step:134/1775 train_time:4585ms step_avg:34.21ms
step:135/1775 train_time:4616ms step_avg:34.19ms
step:136/1775 train_time:4650ms step_avg:34.19ms
step:137/1775 train_time:4682ms step_avg:34.17ms
step:138/1775 train_time:4715ms step_avg:34.17ms
step:139/1775 train_time:4746ms step_avg:34.15ms
step:140/1775 train_time:4780ms step_avg:34.15ms
step:141/1775 train_time:4812ms step_avg:34.13ms
step:142/1775 train_time:4845ms step_avg:34.12ms
step:143/1775 train_time:4876ms step_avg:34.10ms
step:144/1775 train_time:4910ms step_avg:34.09ms
step:145/1775 train_time:4941ms step_avg:34.08ms
step:146/1775 train_time:4975ms step_avg:34.07ms
step:147/1775 train_time:5007ms step_avg:34.06ms
step:148/1775 train_time:5040ms step_avg:34.06ms
step:149/1775 train_time:5071ms step_avg:34.04ms
step:150/1775 train_time:5105ms step_avg:34.03ms
step:151/1775 train_time:5136ms step_avg:34.01ms
step:152/1775 train_time:5169ms step_avg:34.01ms
step:153/1775 train_time:5201ms step_avg:33.99ms
step:154/1775 train_time:5234ms step_avg:33.99ms
step:155/1775 train_time:5265ms step_avg:33.97ms
step:156/1775 train_time:5299ms step_avg:33.97ms
step:157/1775 train_time:5330ms step_avg:33.95ms
step:158/1775 train_time:5364ms step_avg:33.95ms
step:159/1775 train_time:5395ms step_avg:33.93ms
step:160/1775 train_time:5429ms step_avg:33.93ms
step:161/1775 train_time:5461ms step_avg:33.92ms
step:162/1775 train_time:5494ms step_avg:33.92ms
step:163/1775 train_time:5526ms step_avg:33.90ms
step:164/1775 train_time:5560ms step_avg:33.90ms
step:165/1775 train_time:5591ms step_avg:33.89ms
step:166/1775 train_time:5625ms step_avg:33.89ms
step:167/1775 train_time:5656ms step_avg:33.87ms
step:168/1775 train_time:5690ms step_avg:33.87ms
step:169/1775 train_time:5722ms step_avg:33.86ms
step:170/1775 train_time:5755ms step_avg:33.86ms
step:171/1775 train_time:5787ms step_avg:33.84ms
step:172/1775 train_time:5821ms step_avg:33.84ms
step:173/1775 train_time:5852ms step_avg:33.82ms
step:174/1775 train_time:5885ms step_avg:33.82ms
step:175/1775 train_time:5917ms step_avg:33.81ms
step:176/1775 train_time:5951ms step_avg:33.81ms
step:177/1775 train_time:5982ms step_avg:33.79ms
step:178/1775 train_time:6015ms step_avg:33.79ms
step:179/1775 train_time:6047ms step_avg:33.78ms
step:180/1775 train_time:6080ms step_avg:33.78ms
step:181/1775 train_time:6111ms step_avg:33.76ms
step:182/1775 train_time:6145ms step_avg:33.76ms
step:183/1775 train_time:6176ms step_avg:33.75ms
step:184/1775 train_time:6210ms step_avg:33.75ms
step:185/1775 train_time:6241ms step_avg:33.73ms
step:186/1775 train_time:6274ms step_avg:33.73ms
step:187/1775 train_time:6306ms step_avg:33.72ms
step:188/1775 train_time:6339ms step_avg:33.72ms
step:189/1775 train_time:6371ms step_avg:33.71ms
step:190/1775 train_time:6405ms step_avg:33.71ms
step:191/1775 train_time:6436ms step_avg:33.70ms
step:192/1775 train_time:6470ms step_avg:33.70ms
step:193/1775 train_time:6501ms step_avg:33.68ms
step:194/1775 train_time:6534ms step_avg:33.68ms
step:195/1775 train_time:6566ms step_avg:33.67ms
step:196/1775 train_time:6600ms step_avg:33.67ms
step:197/1775 train_time:6631ms step_avg:33.66ms
step:198/1775 train_time:6665ms step_avg:33.66ms
step:199/1775 train_time:6696ms step_avg:33.65ms
step:200/1775 train_time:6731ms step_avg:33.65ms
step:201/1775 train_time:6762ms step_avg:33.64ms
step:202/1775 train_time:6795ms step_avg:33.64ms
step:203/1775 train_time:6827ms step_avg:33.63ms
step:204/1775 train_time:6860ms step_avg:33.63ms
step:205/1775 train_time:6891ms step_avg:33.62ms
step:206/1775 train_time:6925ms step_avg:33.62ms
step:207/1775 train_time:6957ms step_avg:33.61ms
step:208/1775 train_time:6991ms step_avg:33.61ms
step:209/1775 train_time:7022ms step_avg:33.60ms
step:210/1775 train_time:7055ms step_avg:33.60ms
step:211/1775 train_time:7087ms step_avg:33.59ms
step:212/1775 train_time:7120ms step_avg:33.59ms
step:213/1775 train_time:7151ms step_avg:33.57ms
step:214/1775 train_time:7185ms step_avg:33.57ms
step:215/1775 train_time:7216ms step_avg:33.56ms
step:216/1775 train_time:7250ms step_avg:33.57ms
step:217/1775 train_time:7282ms step_avg:33.56ms
step:218/1775 train_time:7315ms step_avg:33.56ms
step:219/1775 train_time:7346ms step_avg:33.55ms
step:220/1775 train_time:7380ms step_avg:33.54ms
step:221/1775 train_time:7411ms step_avg:33.54ms
step:222/1775 train_time:7445ms step_avg:33.54ms
step:223/1775 train_time:7476ms step_avg:33.53ms
step:224/1775 train_time:7510ms step_avg:33.53ms
step:225/1775 train_time:7541ms step_avg:33.52ms
step:226/1775 train_time:7574ms step_avg:33.52ms
step:227/1775 train_time:7606ms step_avg:33.51ms
step:228/1775 train_time:7640ms step_avg:33.51ms
step:229/1775 train_time:7672ms step_avg:33.50ms
step:230/1775 train_time:7705ms step_avg:33.50ms
step:231/1775 train_time:7736ms step_avg:33.49ms
step:232/1775 train_time:7770ms step_avg:33.49ms
step:233/1775 train_time:7801ms step_avg:33.48ms
step:234/1775 train_time:7835ms step_avg:33.48ms
step:235/1775 train_time:7866ms step_avg:33.47ms
step:236/1775 train_time:7900ms step_avg:33.48ms
step:237/1775 train_time:7931ms step_avg:33.47ms
step:238/1775 train_time:7965ms step_avg:33.47ms
step:239/1775 train_time:7997ms step_avg:33.46ms
step:240/1775 train_time:8030ms step_avg:33.46ms
step:241/1775 train_time:8061ms step_avg:33.45ms
step:242/1775 train_time:8094ms step_avg:33.45ms
step:243/1775 train_time:8126ms step_avg:33.44ms
step:244/1775 train_time:8160ms step_avg:33.44ms
step:245/1775 train_time:8191ms step_avg:33.43ms
step:246/1775 train_time:8224ms step_avg:33.43ms
step:247/1775 train_time:8255ms step_avg:33.42ms
step:248/1775 train_time:8289ms step_avg:33.42ms
step:249/1775 train_time:8321ms step_avg:33.42ms
step:250/1775 train_time:8354ms step_avg:33.42ms
step:250/1775 val_loss:4.5968 train_time:8393ms step_avg:33.57ms
step:251/1775 train_time:8417ms step_avg:33.53ms
step:252/1775 train_time:8441ms step_avg:33.50ms
step:253/1775 train_time:8462ms step_avg:33.45ms
step:254/1775 train_time:8487ms step_avg:33.41ms
step:255/1775 train_time:8520ms step_avg:33.41ms
step:256/1775 train_time:8555ms step_avg:33.42ms
step:257/1775 train_time:8586ms step_avg:33.41ms
step:258/1775 train_time:8620ms step_avg:33.41ms
step:259/1775 train_time:8651ms step_avg:33.40ms
step:260/1775 train_time:8685ms step_avg:33.40ms
step:261/1775 train_time:8716ms step_avg:33.40ms
step:262/1775 train_time:8750ms step_avg:33.40ms
step:263/1775 train_time:8781ms step_avg:33.39ms
step:264/1775 train_time:8814ms step_avg:33.39ms
step:265/1775 train_time:8845ms step_avg:33.38ms
step:266/1775 train_time:8879ms step_avg:33.38ms
step:267/1775 train_time:8910ms step_avg:33.37ms
step:268/1775 train_time:8943ms step_avg:33.37ms
step:269/1775 train_time:8974ms step_avg:33.36ms
step:270/1775 train_time:9007ms step_avg:33.36ms
step:271/1775 train_time:9038ms step_avg:33.35ms
step:272/1775 train_time:9072ms step_avg:33.35ms
step:273/1775 train_time:9104ms step_avg:33.35ms
step:274/1775 train_time:9137ms step_avg:33.35ms
step:275/1775 train_time:9168ms step_avg:33.34ms
step:276/1775 train_time:9202ms step_avg:33.34ms
step:277/1775 train_time:9233ms step_avg:33.33ms
step:278/1775 train_time:9266ms step_avg:33.33ms
step:279/1775 train_time:9298ms step_avg:33.33ms
step:280/1775 train_time:9332ms step_avg:33.33ms
step:281/1775 train_time:9363ms step_avg:33.32ms
step:282/1775 train_time:9398ms step_avg:33.33ms
step:283/1775 train_time:9430ms step_avg:33.32ms
step:284/1775 train_time:9463ms step_avg:33.32ms
step:285/1775 train_time:9495ms step_avg:33.32ms
step:286/1775 train_time:9529ms step_avg:33.32ms
step:287/1775 train_time:9561ms step_avg:33.31ms
step:288/1775 train_time:9595ms step_avg:33.32ms
step:289/1775 train_time:9627ms step_avg:33.31ms
step:290/1775 train_time:9661ms step_avg:33.31ms
step:291/1775 train_time:9692ms step_avg:33.31ms
step:292/1775 train_time:9725ms step_avg:33.31ms
step:293/1775 train_time:9757ms step_avg:33.30ms
step:294/1775 train_time:9790ms step_avg:33.30ms
step:295/1775 train_time:9822ms step_avg:33.29ms
step:296/1775 train_time:9855ms step_avg:33.29ms
step:297/1775 train_time:9887ms step_avg:33.29ms
step:298/1775 train_time:9920ms step_avg:33.29ms
step:299/1775 train_time:9951ms step_avg:33.28ms
step:300/1775 train_time:9984ms step_avg:33.28ms
step:301/1775 train_time:10015ms step_avg:33.27ms
step:302/1775 train_time:10049ms step_avg:33.27ms
step:303/1775 train_time:10080ms step_avg:33.27ms
step:304/1775 train_time:10113ms step_avg:33.27ms
step:305/1775 train_time:10144ms step_avg:33.26ms
step:306/1775 train_time:10178ms step_avg:33.26ms
step:307/1775 train_time:10209ms step_avg:33.25ms
step:308/1775 train_time:10242ms step_avg:33.25ms
step:309/1775 train_time:10273ms step_avg:33.25ms
step:310/1775 train_time:10307ms step_avg:33.25ms
step:311/1775 train_time:10338ms step_avg:33.24ms
step:312/1775 train_time:10372ms step_avg:33.24ms
step:313/1775 train_time:10404ms step_avg:33.24ms
step:314/1775 train_time:10438ms step_avg:33.24ms
step:315/1775 train_time:10470ms step_avg:33.24ms
step:316/1775 train_time:10504ms step_avg:33.24ms
step:317/1775 train_time:10535ms step_avg:33.23ms
step:318/1775 train_time:10570ms step_avg:33.24ms
step:319/1775 train_time:10601ms step_avg:33.23ms
step:320/1775 train_time:10635ms step_avg:33.24ms
step:321/1775 train_time:10667ms step_avg:33.23ms
step:322/1775 train_time:10700ms step_avg:33.23ms
step:323/1775 train_time:10731ms step_avg:33.22ms
step:324/1775 train_time:10765ms step_avg:33.23ms
step:325/1775 train_time:10797ms step_avg:33.22ms
step:326/1775 train_time:10831ms step_avg:33.22ms
step:327/1775 train_time:10862ms step_avg:33.22ms
step:328/1775 train_time:10895ms step_avg:33.22ms
step:329/1775 train_time:10927ms step_avg:33.21ms
step:330/1775 train_time:10960ms step_avg:33.21ms
step:331/1775 train_time:10991ms step_avg:33.21ms
step:332/1775 train_time:11025ms step_avg:33.21ms
step:333/1775 train_time:11056ms step_avg:33.20ms
step:334/1775 train_time:11090ms step_avg:33.20ms
step:335/1775 train_time:11121ms step_avg:33.20ms
step:336/1775 train_time:11155ms step_avg:33.20ms
step:337/1775 train_time:11186ms step_avg:33.19ms
step:338/1775 train_time:11219ms step_avg:33.19ms
step:339/1775 train_time:11250ms step_avg:33.19ms
step:340/1775 train_time:11283ms step_avg:33.19ms
step:341/1775 train_time:11315ms step_avg:33.18ms
step:342/1775 train_time:11348ms step_avg:33.18ms
step:343/1775 train_time:11379ms step_avg:33.18ms
step:344/1775 train_time:11414ms step_avg:33.18ms
step:345/1775 train_time:11446ms step_avg:33.18ms
step:346/1775 train_time:11480ms step_avg:33.18ms
step:347/1775 train_time:11511ms step_avg:33.17ms
step:348/1775 train_time:11545ms step_avg:33.18ms
step:349/1775 train_time:11576ms step_avg:33.17ms
step:350/1775 train_time:11610ms step_avg:33.17ms
step:351/1775 train_time:11641ms step_avg:33.17ms
step:352/1775 train_time:11676ms step_avg:33.17ms
step:353/1775 train_time:11707ms step_avg:33.16ms
step:354/1775 train_time:11741ms step_avg:33.17ms
step:355/1775 train_time:11772ms step_avg:33.16ms
step:356/1775 train_time:11806ms step_avg:33.16ms
step:357/1775 train_time:11837ms step_avg:33.16ms
step:358/1775 train_time:11871ms step_avg:33.16ms
step:359/1775 train_time:11902ms step_avg:33.15ms
step:360/1775 train_time:11936ms step_avg:33.15ms
step:361/1775 train_time:11967ms step_avg:33.15ms
step:362/1775 train_time:12000ms step_avg:33.15ms
step:363/1775 train_time:12031ms step_avg:33.14ms
step:364/1775 train_time:12065ms step_avg:33.15ms
step:365/1775 train_time:12096ms step_avg:33.14ms
step:366/1775 train_time:12130ms step_avg:33.14ms
step:367/1775 train_time:12161ms step_avg:33.14ms
step:368/1775 train_time:12194ms step_avg:33.14ms
step:369/1775 train_time:12226ms step_avg:33.13ms
step:370/1775 train_time:12260ms step_avg:33.13ms
step:371/1775 train_time:12291ms step_avg:33.13ms
step:372/1775 train_time:12325ms step_avg:33.13ms
step:373/1775 train_time:12356ms step_avg:33.13ms
step:374/1775 train_time:12389ms step_avg:33.13ms
step:375/1775 train_time:12421ms step_avg:33.12ms
step:376/1775 train_time:12455ms step_avg:33.12ms
step:377/1775 train_time:12486ms step_avg:33.12ms
step:378/1775 train_time:12520ms step_avg:33.12ms
step:379/1775 train_time:12552ms step_avg:33.12ms
step:380/1775 train_time:12585ms step_avg:33.12ms
step:381/1775 train_time:12616ms step_avg:33.11ms
step:382/1775 train_time:12650ms step_avg:33.12ms
step:383/1775 train_time:12681ms step_avg:33.11ms
step:384/1775 train_time:12715ms step_avg:33.11ms
step:385/1775 train_time:12747ms step_avg:33.11ms
step:386/1775 train_time:12780ms step_avg:33.11ms
step:387/1775 train_time:12811ms step_avg:33.10ms
step:388/1775 train_time:12845ms step_avg:33.11ms
step:389/1775 train_time:12876ms step_avg:33.10ms
step:390/1775 train_time:12910ms step_avg:33.10ms
step:391/1775 train_time:12941ms step_avg:33.10ms
step:392/1775 train_time:12975ms step_avg:33.10ms
step:393/1775 train_time:13007ms step_avg:33.10ms
step:394/1775 train_time:13040ms step_avg:33.10ms
step:395/1775 train_time:13071ms step_avg:33.09ms
step:396/1775 train_time:13104ms step_avg:33.09ms
step:397/1775 train_time:13136ms step_avg:33.09ms
step:398/1775 train_time:13170ms step_avg:33.09ms
step:399/1775 train_time:13201ms step_avg:33.08ms
step:400/1775 train_time:13234ms step_avg:33.09ms
step:401/1775 train_time:13266ms step_avg:33.08ms
step:402/1775 train_time:13299ms step_avg:33.08ms
step:403/1775 train_time:13331ms step_avg:33.08ms
step:404/1775 train_time:13365ms step_avg:33.08ms
step:405/1775 train_time:13396ms step_avg:33.08ms
step:406/1775 train_time:13431ms step_avg:33.08ms
step:407/1775 train_time:13462ms step_avg:33.08ms
step:408/1775 train_time:13495ms step_avg:33.08ms
step:409/1775 train_time:13527ms step_avg:33.07ms
step:410/1775 train_time:13560ms step_avg:33.07ms
step:411/1775 train_time:13592ms step_avg:33.07ms
step:412/1775 train_time:13626ms step_avg:33.07ms
step:413/1775 train_time:13657ms step_avg:33.07ms
step:414/1775 train_time:13691ms step_avg:33.07ms
step:415/1775 train_time:13722ms step_avg:33.07ms
step:416/1775 train_time:13756ms step_avg:33.07ms
step:417/1775 train_time:13788ms step_avg:33.06ms
step:418/1775 train_time:13821ms step_avg:33.06ms
step:419/1775 train_time:13852ms step_avg:33.06ms
step:420/1775 train_time:13886ms step_avg:33.06ms
step:421/1775 train_time:13917ms step_avg:33.06ms
step:422/1775 train_time:13951ms step_avg:33.06ms
step:423/1775 train_time:13982ms step_avg:33.05ms
step:424/1775 train_time:14016ms step_avg:33.06ms
step:425/1775 train_time:14047ms step_avg:33.05ms
step:426/1775 train_time:14081ms step_avg:33.05ms
step:427/1775 train_time:14112ms step_avg:33.05ms
step:428/1775 train_time:14145ms step_avg:33.05ms
step:429/1775 train_time:14177ms step_avg:33.05ms
step:430/1775 train_time:14210ms step_avg:33.05ms
step:431/1775 train_time:14241ms step_avg:33.04ms
step:432/1775 train_time:14275ms step_avg:33.04ms
step:433/1775 train_time:14307ms step_avg:33.04ms
step:434/1775 train_time:14340ms step_avg:33.04ms
step:435/1775 train_time:14371ms step_avg:33.04ms
step:436/1775 train_time:14404ms step_avg:33.04ms
step:437/1775 train_time:14436ms step_avg:33.03ms
step:438/1775 train_time:14469ms step_avg:33.04ms
step:439/1775 train_time:14501ms step_avg:33.03ms
step:440/1775 train_time:14535ms step_avg:33.03ms
step:441/1775 train_time:14566ms step_avg:33.03ms
step:442/1775 train_time:14599ms step_avg:33.03ms
step:443/1775 train_time:14631ms step_avg:33.03ms
step:444/1775 train_time:14664ms step_avg:33.03ms
step:445/1775 train_time:14696ms step_avg:33.02ms
step:446/1775 train_time:14729ms step_avg:33.03ms
step:447/1775 train_time:14760ms step_avg:33.02ms
step:448/1775 train_time:14794ms step_avg:33.02ms
step:449/1775 train_time:14825ms step_avg:33.02ms
step:450/1775 train_time:14859ms step_avg:33.02ms
step:451/1775 train_time:14890ms step_avg:33.02ms
step:452/1775 train_time:14923ms step_avg:33.02ms
step:453/1775 train_time:14955ms step_avg:33.01ms
step:454/1775 train_time:14989ms step_avg:33.01ms
step:455/1775 train_time:15020ms step_avg:33.01ms
step:456/1775 train_time:15054ms step_avg:33.01ms
step:457/1775 train_time:15085ms step_avg:33.01ms
step:458/1775 train_time:15119ms step_avg:33.01ms
step:459/1775 train_time:15150ms step_avg:33.01ms
step:460/1775 train_time:15183ms step_avg:33.01ms
step:461/1775 train_time:15215ms step_avg:33.00ms
step:462/1775 train_time:15249ms step_avg:33.01ms
step:463/1775 train_time:15280ms step_avg:33.00ms
step:464/1775 train_time:15314ms step_avg:33.00ms
step:465/1775 train_time:15345ms step_avg:33.00ms
step:466/1775 train_time:15379ms step_avg:33.00ms
step:467/1775 train_time:15411ms step_avg:33.00ms
step:468/1775 train_time:15444ms step_avg:33.00ms
step:469/1775 train_time:15476ms step_avg:33.00ms
step:470/1775 train_time:15509ms step_avg:33.00ms
step:471/1775 train_time:15540ms step_avg:32.99ms
step:472/1775 train_time:15574ms step_avg:33.00ms
step:473/1775 train_time:15605ms step_avg:32.99ms
step:474/1775 train_time:15639ms step_avg:32.99ms
step:475/1775 train_time:15671ms step_avg:32.99ms
step:476/1775 train_time:15704ms step_avg:32.99ms
step:477/1775 train_time:15736ms step_avg:32.99ms
step:478/1775 train_time:15770ms step_avg:32.99ms
step:479/1775 train_time:15801ms step_avg:32.99ms
step:480/1775 train_time:15835ms step_avg:32.99ms
step:481/1775 train_time:15866ms step_avg:32.99ms
step:482/1775 train_time:15900ms step_avg:32.99ms
step:483/1775 train_time:15931ms step_avg:32.98ms
step:484/1775 train_time:15964ms step_avg:32.98ms
step:485/1775 train_time:15996ms step_avg:32.98ms
step:486/1775 train_time:16030ms step_avg:32.98ms
step:487/1775 train_time:16061ms step_avg:32.98ms
step:488/1775 train_time:16095ms step_avg:32.98ms
step:489/1775 train_time:16126ms step_avg:32.98ms
step:490/1775 train_time:16160ms step_avg:32.98ms
step:491/1775 train_time:16191ms step_avg:32.98ms
step:492/1775 train_time:16225ms step_avg:32.98ms
step:493/1775 train_time:16257ms step_avg:32.97ms
step:494/1775 train_time:16290ms step_avg:32.98ms
step:495/1775 train_time:16321ms step_avg:32.97ms
step:496/1775 train_time:16355ms step_avg:32.97ms
step:497/1775 train_time:16387ms step_avg:32.97ms
step:498/1775 train_time:16420ms step_avg:32.97ms
step:499/1775 train_time:16452ms step_avg:32.97ms
step:500/1775 train_time:16485ms step_avg:32.97ms
step:500/1775 val_loss:4.2729 train_time:16525ms step_avg:33.05ms
step:501/1775 train_time:16549ms step_avg:33.03ms
step:502/1775 train_time:16572ms step_avg:33.01ms
step:503/1775 train_time:16593ms step_avg:32.99ms
step:504/1775 train_time:16619ms step_avg:32.97ms
step:505/1775 train_time:16652ms step_avg:32.97ms
step:506/1775 train_time:16688ms step_avg:32.98ms
step:507/1775 train_time:16720ms step_avg:32.98ms
step:508/1775 train_time:16753ms step_avg:32.98ms
step:509/1775 train_time:16784ms step_avg:32.98ms
step:510/1775 train_time:16819ms step_avg:32.98ms
step:511/1775 train_time:16850ms step_avg:32.97ms
step:512/1775 train_time:16883ms step_avg:32.97ms
step:513/1775 train_time:16914ms step_avg:32.97ms
step:514/1775 train_time:16947ms step_avg:32.97ms
step:515/1775 train_time:16978ms step_avg:32.97ms
step:516/1775 train_time:17011ms step_avg:32.97ms
step:517/1775 train_time:17042ms step_avg:32.96ms
step:518/1775 train_time:17075ms step_avg:32.96ms
step:519/1775 train_time:17106ms step_avg:32.96ms
step:520/1775 train_time:17140ms step_avg:32.96ms
step:521/1775 train_time:17171ms step_avg:32.96ms
step:522/1775 train_time:17204ms step_avg:32.96ms
step:523/1775 train_time:17235ms step_avg:32.95ms
step:524/1775 train_time:17268ms step_avg:32.95ms
step:525/1775 train_time:17299ms step_avg:32.95ms
step:526/1775 train_time:17332ms step_avg:32.95ms
step:527/1775 train_time:17364ms step_avg:32.95ms
step:528/1775 train_time:17397ms step_avg:32.95ms
step:529/1775 train_time:17428ms step_avg:32.94ms
step:530/1775 train_time:17462ms step_avg:32.95ms
step:531/1775 train_time:17493ms step_avg:32.94ms
step:532/1775 train_time:17527ms step_avg:32.95ms
step:533/1775 train_time:17559ms step_avg:32.94ms
step:534/1775 train_time:17593ms step_avg:32.95ms
step:535/1775 train_time:17625ms step_avg:32.94ms
step:536/1775 train_time:17660ms step_avg:32.95ms
step:537/1775 train_time:17691ms step_avg:32.94ms
step:538/1775 train_time:17726ms step_avg:32.95ms
step:539/1775 train_time:17757ms step_avg:32.94ms
step:540/1775 train_time:17790ms step_avg:32.95ms
step:541/1775 train_time:17822ms step_avg:32.94ms
step:542/1775 train_time:17855ms step_avg:32.94ms
step:543/1775 train_time:17887ms step_avg:32.94ms
step:544/1775 train_time:17921ms step_avg:32.94ms
step:545/1775 train_time:17952ms step_avg:32.94ms
step:546/1775 train_time:17985ms step_avg:32.94ms
step:547/1775 train_time:18016ms step_avg:32.94ms
step:548/1775 train_time:18050ms step_avg:32.94ms
step:549/1775 train_time:18081ms step_avg:32.93ms
step:550/1775 train_time:18114ms step_avg:32.94ms
step:551/1775 train_time:18146ms step_avg:32.93ms
step:552/1775 train_time:18179ms step_avg:32.93ms
step:553/1775 train_time:18210ms step_avg:32.93ms
step:554/1775 train_time:18243ms step_avg:32.93ms
step:555/1775 train_time:18274ms step_avg:32.93ms
step:556/1775 train_time:18308ms step_avg:32.93ms
step:557/1775 train_time:18339ms step_avg:32.92ms
step:558/1775 train_time:18372ms step_avg:32.92ms
step:559/1775 train_time:18403ms step_avg:32.92ms
step:560/1775 train_time:18437ms step_avg:32.92ms
step:561/1775 train_time:18468ms step_avg:32.92ms
step:562/1775 train_time:18502ms step_avg:32.92ms
step:563/1775 train_time:18533ms step_avg:32.92ms
step:564/1775 train_time:18567ms step_avg:32.92ms
step:565/1775 train_time:18599ms step_avg:32.92ms
step:566/1775 train_time:18633ms step_avg:32.92ms
step:567/1775 train_time:18665ms step_avg:32.92ms
step:568/1775 train_time:18699ms step_avg:32.92ms
step:569/1775 train_time:18731ms step_avg:32.92ms
step:570/1775 train_time:18764ms step_avg:32.92ms
step:571/1775 train_time:18796ms step_avg:32.92ms
step:572/1775 train_time:18830ms step_avg:32.92ms
step:573/1775 train_time:18861ms step_avg:32.92ms
step:574/1775 train_time:18894ms step_avg:32.92ms
step:575/1775 train_time:18926ms step_avg:32.91ms
step:576/1775 train_time:18960ms step_avg:32.92ms
step:577/1775 train_time:18991ms step_avg:32.91ms
step:578/1775 train_time:19025ms step_avg:32.91ms
step:579/1775 train_time:19056ms step_avg:32.91ms
step:580/1775 train_time:19091ms step_avg:32.92ms
step:581/1775 train_time:19149ms step_avg:32.96ms
step:582/1775 train_time:19209ms step_avg:33.00ms
step:583/1775 train_time:19266ms step_avg:33.05ms
step:584/1775 train_time:19327ms step_avg:33.09ms
step:585/1775 train_time:19386ms step_avg:33.14ms
step:586/1775 train_time:19448ms step_avg:33.19ms
step:587/1775 train_time:19506ms step_avg:33.23ms
step:588/1775 train_time:19568ms step_avg:33.28ms
step:589/1775 train_time:19626ms step_avg:33.32ms
step:590/1775 train_time:19688ms step_avg:33.37ms
step:591/1775 train_time:19747ms step_avg:33.41ms
step:592/1775 train_time:19808ms step_avg:33.46ms
step:593/1775 train_time:19866ms step_avg:33.50ms
step:594/1775 train_time:19927ms step_avg:33.55ms
step:595/1775 train_time:19985ms step_avg:33.59ms
step:596/1775 train_time:20046ms step_avg:33.64ms
step:597/1775 train_time:20105ms step_avg:33.68ms
step:598/1775 train_time:20165ms step_avg:33.72ms
step:599/1775 train_time:20224ms step_avg:33.76ms
step:600/1775 train_time:20284ms step_avg:33.81ms
step:601/1775 train_time:20343ms step_avg:33.85ms
step:602/1775 train_time:20404ms step_avg:33.89ms
step:603/1775 train_time:20462ms step_avg:33.93ms
step:604/1775 train_time:20524ms step_avg:33.98ms
step:605/1775 train_time:20583ms step_avg:34.02ms
step:606/1775 train_time:20645ms step_avg:34.07ms
step:607/1775 train_time:20703ms step_avg:34.11ms
step:608/1775 train_time:20764ms step_avg:34.15ms
step:609/1775 train_time:20823ms step_avg:34.19ms
step:610/1775 train_time:20884ms step_avg:34.24ms
step:611/1775 train_time:20942ms step_avg:34.28ms
step:612/1775 train_time:21003ms step_avg:34.32ms
step:613/1775 train_time:21062ms step_avg:34.36ms
step:614/1775 train_time:21122ms step_avg:34.40ms
step:615/1775 train_time:21180ms step_avg:34.44ms
step:616/1775 train_time:21241ms step_avg:34.48ms
step:617/1775 train_time:21300ms step_avg:34.52ms
step:618/1775 train_time:21360ms step_avg:34.56ms
step:619/1775 train_time:21418ms step_avg:34.60ms
step:620/1775 train_time:21480ms step_avg:34.64ms
step:621/1775 train_time:21538ms step_avg:34.68ms
step:622/1775 train_time:21600ms step_avg:34.73ms
step:623/1775 train_time:21659ms step_avg:34.77ms
step:624/1775 train_time:21720ms step_avg:34.81ms
step:625/1775 train_time:21779ms step_avg:34.85ms
step:626/1775 train_time:21840ms step_avg:34.89ms
step:627/1775 train_time:21898ms step_avg:34.93ms
step:628/1775 train_time:21959ms step_avg:34.97ms
step:629/1775 train_time:22017ms step_avg:35.00ms
step:630/1775 train_time:22078ms step_avg:35.04ms
step:631/1775 train_time:22136ms step_avg:35.08ms
step:632/1775 train_time:22196ms step_avg:35.12ms
step:633/1775 train_time:22254ms step_avg:35.16ms
step:634/1775 train_time:22315ms step_avg:35.20ms
step:635/1775 train_time:22372ms step_avg:35.23ms
step:636/1775 train_time:22434ms step_avg:35.27ms
step:637/1775 train_time:22492ms step_avg:35.31ms
step:638/1775 train_time:22553ms step_avg:35.35ms
step:639/1775 train_time:22611ms step_avg:35.39ms
step:640/1775 train_time:22673ms step_avg:35.43ms
step:641/1775 train_time:22732ms step_avg:35.46ms
step:642/1775 train_time:22792ms step_avg:35.50ms
step:643/1775 train_time:22850ms step_avg:35.54ms
step:644/1775 train_time:22912ms step_avg:35.58ms
step:645/1775 train_time:22970ms step_avg:35.61ms
step:646/1775 train_time:23031ms step_avg:35.65ms
step:647/1775 train_time:23089ms step_avg:35.69ms
step:648/1775 train_time:23149ms step_avg:35.72ms
step:649/1775 train_time:23207ms step_avg:35.76ms
step:650/1775 train_time:23268ms step_avg:35.80ms
step:651/1775 train_time:23327ms step_avg:35.83ms
step:652/1775 train_time:23387ms step_avg:35.87ms
step:653/1775 train_time:23445ms step_avg:35.90ms
step:654/1775 train_time:23507ms step_avg:35.94ms
step:655/1775 train_time:23565ms step_avg:35.98ms
step:656/1775 train_time:23626ms step_avg:36.01ms
step:657/1775 train_time:23685ms step_avg:36.05ms
step:658/1775 train_time:23746ms step_avg:36.09ms
step:659/1775 train_time:23805ms step_avg:36.12ms
step:660/1775 train_time:23866ms step_avg:36.16ms
step:661/1775 train_time:23925ms step_avg:36.19ms
step:662/1775 train_time:23985ms step_avg:36.23ms
step:663/1775 train_time:24043ms step_avg:36.26ms
step:664/1775 train_time:24103ms step_avg:36.30ms
step:665/1775 train_time:24161ms step_avg:36.33ms
step:666/1775 train_time:24222ms step_avg:36.37ms
step:667/1775 train_time:24280ms step_avg:36.40ms
step:668/1775 train_time:24342ms step_avg:36.44ms
step:669/1775 train_time:24402ms step_avg:36.47ms
step:670/1775 train_time:24462ms step_avg:36.51ms
step:671/1775 train_time:24521ms step_avg:36.54ms
step:672/1775 train_time:24581ms step_avg:36.58ms
step:673/1775 train_time:24640ms step_avg:36.61ms
step:674/1775 train_time:24701ms step_avg:36.65ms
step:675/1775 train_time:24759ms step_avg:36.68ms
step:676/1775 train_time:24819ms step_avg:36.72ms
step:677/1775 train_time:24878ms step_avg:36.75ms
step:678/1775 train_time:24939ms step_avg:36.78ms
step:679/1775 train_time:24996ms step_avg:36.81ms
step:680/1775 train_time:25057ms step_avg:36.85ms
step:681/1775 train_time:25115ms step_avg:36.88ms
step:682/1775 train_time:25175ms step_avg:36.91ms
step:683/1775 train_time:25234ms step_avg:36.95ms
step:684/1775 train_time:25294ms step_avg:36.98ms
step:685/1775 train_time:25352ms step_avg:37.01ms
step:686/1775 train_time:25413ms step_avg:37.05ms
step:687/1775 train_time:25472ms step_avg:37.08ms
step:688/1775 train_time:25533ms step_avg:37.11ms
step:689/1775 train_time:25592ms step_avg:37.14ms
step:690/1775 train_time:25652ms step_avg:37.18ms
step:691/1775 train_time:25711ms step_avg:37.21ms
step:692/1775 train_time:25772ms step_avg:37.24ms
step:693/1775 train_time:25830ms step_avg:37.27ms
step:694/1775 train_time:25891ms step_avg:37.31ms
step:695/1775 train_time:25949ms step_avg:37.34ms
step:696/1775 train_time:26009ms step_avg:37.37ms
step:697/1775 train_time:26067ms step_avg:37.40ms
step:698/1775 train_time:26129ms step_avg:37.43ms
step:699/1775 train_time:26187ms step_avg:37.46ms
step:700/1775 train_time:26247ms step_avg:37.50ms
step:701/1775 train_time:26306ms step_avg:37.53ms
step:702/1775 train_time:26367ms step_avg:37.56ms
step:703/1775 train_time:26425ms step_avg:37.59ms
step:704/1775 train_time:26486ms step_avg:37.62ms
step:705/1775 train_time:26544ms step_avg:37.65ms
step:706/1775 train_time:26606ms step_avg:37.68ms
step:707/1775 train_time:26663ms step_avg:37.71ms
step:708/1775 train_time:26724ms step_avg:37.75ms
step:709/1775 train_time:26783ms step_avg:37.78ms
step:710/1775 train_time:26845ms step_avg:37.81ms
step:711/1775 train_time:26904ms step_avg:37.84ms
step:712/1775 train_time:26964ms step_avg:37.87ms
step:713/1775 train_time:27022ms step_avg:37.90ms
step:714/1775 train_time:27083ms step_avg:37.93ms
step:715/1775 train_time:27142ms step_avg:37.96ms
step:716/1775 train_time:27203ms step_avg:37.99ms
step:717/1775 train_time:27261ms step_avg:38.02ms
step:718/1775 train_time:27322ms step_avg:38.05ms
step:719/1775 train_time:27380ms step_avg:38.08ms
step:720/1775 train_time:27441ms step_avg:38.11ms
step:721/1775 train_time:27499ms step_avg:38.14ms
step:722/1775 train_time:27561ms step_avg:38.17ms
step:723/1775 train_time:27619ms step_avg:38.20ms
step:724/1775 train_time:27679ms step_avg:38.23ms
step:725/1775 train_time:27738ms step_avg:38.26ms
step:726/1775 train_time:27799ms step_avg:38.29ms
step:727/1775 train_time:27858ms step_avg:38.32ms
step:728/1775 train_time:27918ms step_avg:38.35ms
step:729/1775 train_time:27976ms step_avg:38.38ms
step:730/1775 train_time:28037ms step_avg:38.41ms
step:731/1775 train_time:28095ms step_avg:38.43ms
step:732/1775 train_time:28156ms step_avg:38.46ms
step:733/1775 train_time:28214ms step_avg:38.49ms
step:734/1775 train_time:28275ms step_avg:38.52ms
step:735/1775 train_time:28333ms step_avg:38.55ms
step:736/1775 train_time:28394ms step_avg:38.58ms
step:737/1775 train_time:28452ms step_avg:38.60ms
step:738/1775 train_time:28513ms step_avg:38.64ms
step:739/1775 train_time:28571ms step_avg:38.66ms
step:740/1775 train_time:28632ms step_avg:38.69ms
step:741/1775 train_time:28690ms step_avg:38.72ms
step:742/1775 train_time:28751ms step_avg:38.75ms
step:743/1775 train_time:28810ms step_avg:38.77ms
step:744/1775 train_time:28870ms step_avg:38.80ms
step:745/1775 train_time:28929ms step_avg:38.83ms
step:746/1775 train_time:28989ms step_avg:38.86ms
step:747/1775 train_time:29048ms step_avg:38.89ms
step:748/1775 train_time:29109ms step_avg:38.92ms
step:749/1775 train_time:29167ms step_avg:38.94ms
step:750/1775 train_time:29228ms step_avg:38.97ms
step:750/1775 val_loss:3.9911 train_time:29298ms step_avg:39.06ms
step:751/1775 train_time:29322ms step_avg:39.04ms
step:752/1775 train_time:29348ms step_avg:39.03ms
step:753/1775 train_time:29409ms step_avg:39.06ms
step:754/1775 train_time:29470ms step_avg:39.09ms
step:755/1775 train_time:29531ms step_avg:39.11ms
step:756/1775 train_time:29592ms step_avg:39.14ms
step:757/1775 train_time:29650ms step_avg:39.17ms
step:758/1775 train_time:29710ms step_avg:39.20ms
step:759/1775 train_time:29768ms step_avg:39.22ms
step:760/1775 train_time:29828ms step_avg:39.25ms
step:761/1775 train_time:29885ms step_avg:39.27ms
step:762/1775 train_time:29946ms step_avg:39.30ms
step:763/1775 train_time:30003ms step_avg:39.32ms
step:764/1775 train_time:30063ms step_avg:39.35ms
step:765/1775 train_time:30121ms step_avg:39.37ms
step:766/1775 train_time:30182ms step_avg:39.40ms
step:767/1775 train_time:30241ms step_avg:39.43ms
step:768/1775 train_time:30301ms step_avg:39.45ms
step:769/1775 train_time:30362ms step_avg:39.48ms
step:770/1775 train_time:30424ms step_avg:39.51ms
step:771/1775 train_time:30484ms step_avg:39.54ms
step:772/1775 train_time:30545ms step_avg:39.57ms
step:773/1775 train_time:30604ms step_avg:39.59ms
step:774/1775 train_time:30665ms step_avg:39.62ms
step:775/1775 train_time:30723ms step_avg:39.64ms
step:776/1775 train_time:30783ms step_avg:39.67ms
step:777/1775 train_time:30841ms step_avg:39.69ms
step:778/1775 train_time:30902ms step_avg:39.72ms
step:779/1775 train_time:30959ms step_avg:39.74ms
step:780/1775 train_time:31020ms step_avg:39.77ms
step:781/1775 train_time:31078ms step_avg:39.79ms
step:782/1775 train_time:31138ms step_avg:39.82ms
step:783/1775 train_time:31196ms step_avg:39.84ms
step:784/1775 train_time:31256ms step_avg:39.87ms
step:785/1775 train_time:31315ms step_avg:39.89ms
step:786/1775 train_time:31377ms step_avg:39.92ms
step:787/1775 train_time:31437ms step_avg:39.95ms
step:788/1775 train_time:31499ms step_avg:39.97ms
step:789/1775 train_time:31558ms step_avg:40.00ms
step:790/1775 train_time:31619ms step_avg:40.02ms
step:791/1775 train_time:31676ms step_avg:40.05ms
step:792/1775 train_time:31737ms step_avg:40.07ms
step:793/1775 train_time:31796ms step_avg:40.10ms
step:794/1775 train_time:31856ms step_avg:40.12ms
step:795/1775 train_time:31913ms step_avg:40.14ms
step:796/1775 train_time:31973ms step_avg:40.17ms
step:797/1775 train_time:32031ms step_avg:40.19ms
step:798/1775 train_time:32092ms step_avg:40.22ms
step:799/1775 train_time:32149ms step_avg:40.24ms
step:800/1775 train_time:32210ms step_avg:40.26ms
step:801/1775 train_time:32268ms step_avg:40.28ms
step:802/1775 train_time:32330ms step_avg:40.31ms
step:803/1775 train_time:32389ms step_avg:40.33ms
step:804/1775 train_time:32450ms step_avg:40.36ms
step:805/1775 train_time:32510ms step_avg:40.39ms
step:806/1775 train_time:32572ms step_avg:40.41ms
step:807/1775 train_time:32629ms step_avg:40.43ms
step:808/1775 train_time:32691ms step_avg:40.46ms
step:809/1775 train_time:32749ms step_avg:40.48ms
step:810/1775 train_time:32809ms step_avg:40.51ms
step:811/1775 train_time:32867ms step_avg:40.53ms
step:812/1775 train_time:32927ms step_avg:40.55ms
step:813/1775 train_time:32985ms step_avg:40.57ms
step:814/1775 train_time:33046ms step_avg:40.60ms
step:815/1775 train_time:33104ms step_avg:40.62ms
step:816/1775 train_time:33163ms step_avg:40.64ms
step:817/1775 train_time:33222ms step_avg:40.66ms
step:818/1775 train_time:33283ms step_avg:40.69ms
step:819/1775 train_time:33342ms step_avg:40.71ms
step:820/1775 train_time:33403ms step_avg:40.74ms
step:821/1775 train_time:33462ms step_avg:40.76ms
step:822/1775 train_time:33524ms step_avg:40.78ms
step:823/1775 train_time:33583ms step_avg:40.81ms
step:824/1775 train_time:33644ms step_avg:40.83ms
step:825/1775 train_time:33702ms step_avg:40.85ms
step:826/1775 train_time:33763ms step_avg:40.87ms
step:827/1775 train_time:33821ms step_avg:40.90ms
step:828/1775 train_time:33882ms step_avg:40.92ms
step:829/1775 train_time:33940ms step_avg:40.94ms
step:830/1775 train_time:34000ms step_avg:40.96ms
step:831/1775 train_time:34058ms step_avg:40.98ms
step:832/1775 train_time:34119ms step_avg:41.01ms
step:833/1775 train_time:34177ms step_avg:41.03ms
step:834/1775 train_time:34237ms step_avg:41.05ms
step:835/1775 train_time:34297ms step_avg:41.07ms
step:836/1775 train_time:34359ms step_avg:41.10ms
step:837/1775 train_time:34417ms step_avg:41.12ms
step:838/1775 train_time:34478ms step_avg:41.14ms
step:839/1775 train_time:34537ms step_avg:41.16ms
step:840/1775 train_time:34598ms step_avg:41.19ms
step:841/1775 train_time:34657ms step_avg:41.21ms
step:842/1775 train_time:34718ms step_avg:41.23ms
step:843/1775 train_time:34776ms step_avg:41.25ms
step:844/1775 train_time:34836ms step_avg:41.28ms
step:845/1775 train_time:34894ms step_avg:41.30ms
step:846/1775 train_time:34954ms step_avg:41.32ms
step:847/1775 train_time:35011ms step_avg:41.34ms
step:848/1775 train_time:35073ms step_avg:41.36ms
step:849/1775 train_time:35131ms step_avg:41.38ms
step:850/1775 train_time:35192ms step_avg:41.40ms
step:851/1775 train_time:35250ms step_avg:41.42ms
step:852/1775 train_time:35311ms step_avg:41.45ms
step:853/1775 train_time:35369ms step_avg:41.46ms
step:854/1775 train_time:35430ms step_avg:41.49ms
step:855/1775 train_time:35490ms step_avg:41.51ms
step:856/1775 train_time:35551ms step_avg:41.53ms
step:857/1775 train_time:35610ms step_avg:41.55ms
step:858/1775 train_time:35671ms step_avg:41.57ms
step:859/1775 train_time:35729ms step_avg:41.59ms
step:860/1775 train_time:35790ms step_avg:41.62ms
step:861/1775 train_time:35847ms step_avg:41.63ms
step:862/1775 train_time:35908ms step_avg:41.66ms
step:863/1775 train_time:35967ms step_avg:41.68ms
step:864/1775 train_time:36028ms step_avg:41.70ms
step:865/1775 train_time:36085ms step_avg:41.72ms
step:866/1775 train_time:36146ms step_avg:41.74ms
step:867/1775 train_time:36204ms step_avg:41.76ms
step:868/1775 train_time:36265ms step_avg:41.78ms
step:869/1775 train_time:36324ms step_avg:41.80ms
step:870/1775 train_time:36385ms step_avg:41.82ms
step:871/1775 train_time:36443ms step_avg:41.84ms
step:872/1775 train_time:36504ms step_avg:41.86ms
step:873/1775 train_time:36563ms step_avg:41.88ms
step:874/1775 train_time:36623ms step_avg:41.90ms
step:875/1775 train_time:36682ms step_avg:41.92ms
step:876/1775 train_time:36743ms step_avg:41.94ms
step:877/1775 train_time:36801ms step_avg:41.96ms
step:878/1775 train_time:36861ms step_avg:41.98ms
step:879/1775 train_time:36919ms step_avg:42.00ms
step:880/1775 train_time:36980ms step_avg:42.02ms
step:881/1775 train_time:37040ms step_avg:42.04ms
step:882/1775 train_time:37100ms step_avg:42.06ms
step:883/1775 train_time:37158ms step_avg:42.08ms
step:884/1775 train_time:37218ms step_avg:42.10ms
step:885/1775 train_time:37277ms step_avg:42.12ms
step:886/1775 train_time:37338ms step_avg:42.14ms
step:887/1775 train_time:37397ms step_avg:42.16ms
step:888/1775 train_time:37457ms step_avg:42.18ms
step:889/1775 train_time:37516ms step_avg:42.20ms
step:890/1775 train_time:37577ms step_avg:42.22ms
step:891/1775 train_time:37636ms step_avg:42.24ms
step:892/1775 train_time:37698ms step_avg:42.26ms
step:893/1775 train_time:37756ms step_avg:42.28ms
step:894/1775 train_time:37817ms step_avg:42.30ms
step:895/1775 train_time:37875ms step_avg:42.32ms
step:896/1775 train_time:37935ms step_avg:42.34ms
step:897/1775 train_time:37994ms step_avg:42.36ms
step:898/1775 train_time:38054ms step_avg:42.38ms
step:899/1775 train_time:38112ms step_avg:42.39ms
step:900/1775 train_time:38172ms step_avg:42.41ms
step:901/1775 train_time:38231ms step_avg:42.43ms
step:902/1775 train_time:38293ms step_avg:42.45ms
step:903/1775 train_time:38351ms step_avg:42.47ms
step:904/1775 train_time:38412ms step_avg:42.49ms
step:905/1775 train_time:38471ms step_avg:42.51ms
step:906/1775 train_time:38532ms step_avg:42.53ms
step:907/1775 train_time:38591ms step_avg:42.55ms
step:908/1775 train_time:38652ms step_avg:42.57ms
step:909/1775 train_time:38710ms step_avg:42.59ms
step:910/1775 train_time:38771ms step_avg:42.61ms
step:911/1775 train_time:38829ms step_avg:42.62ms
step:912/1775 train_time:38890ms step_avg:42.64ms
step:913/1775 train_time:38948ms step_avg:42.66ms
step:914/1775 train_time:39010ms step_avg:42.68ms
step:915/1775 train_time:39068ms step_avg:42.70ms
step:916/1775 train_time:39128ms step_avg:42.72ms
step:917/1775 train_time:39187ms step_avg:42.73ms
step:918/1775 train_time:39247ms step_avg:42.75ms
step:919/1775 train_time:39305ms step_avg:42.77ms
step:920/1775 train_time:39366ms step_avg:42.79ms
step:921/1775 train_time:39425ms step_avg:42.81ms
step:922/1775 train_time:39486ms step_avg:42.83ms
step:923/1775 train_time:39544ms step_avg:42.84ms
step:924/1775 train_time:39604ms step_avg:42.86ms
step:925/1775 train_time:39663ms step_avg:42.88ms
step:926/1775 train_time:39724ms step_avg:42.90ms
step:927/1775 train_time:39782ms step_avg:42.92ms
step:928/1775 train_time:39843ms step_avg:42.93ms
step:929/1775 train_time:39901ms step_avg:42.95ms
step:930/1775 train_time:39962ms step_avg:42.97ms
step:931/1775 train_time:40020ms step_avg:42.99ms
step:932/1775 train_time:40081ms step_avg:43.00ms
step:933/1775 train_time:40139ms step_avg:43.02ms
step:934/1775 train_time:40200ms step_avg:43.04ms
step:935/1775 train_time:40259ms step_avg:43.06ms
step:936/1775 train_time:40319ms step_avg:43.08ms
step:937/1775 train_time:40378ms step_avg:43.09ms
step:938/1775 train_time:40440ms step_avg:43.11ms
step:939/1775 train_time:40498ms step_avg:43.13ms
step:940/1775 train_time:40559ms step_avg:43.15ms
step:941/1775 train_time:40618ms step_avg:43.16ms
step:942/1775 train_time:40679ms step_avg:43.18ms
step:943/1775 train_time:40738ms step_avg:43.20ms
step:944/1775 train_time:40799ms step_avg:43.22ms
step:945/1775 train_time:40856ms step_avg:43.23ms
step:946/1775 train_time:40917ms step_avg:43.25ms
step:947/1775 train_time:40975ms step_avg:43.27ms
step:948/1775 train_time:41036ms step_avg:43.29ms
step:949/1775 train_time:41094ms step_avg:43.30ms
step:950/1775 train_time:41154ms step_avg:43.32ms
step:951/1775 train_time:41212ms step_avg:43.34ms
step:952/1775 train_time:41273ms step_avg:43.35ms
step:953/1775 train_time:41331ms step_avg:43.37ms
step:954/1775 train_time:41393ms step_avg:43.39ms
step:955/1775 train_time:41451ms step_avg:43.40ms
step:956/1775 train_time:41513ms step_avg:43.42ms
step:957/1775 train_time:41572ms step_avg:43.44ms
step:958/1775 train_time:41633ms step_avg:43.46ms
step:959/1775 train_time:41692ms step_avg:43.47ms
step:960/1775 train_time:41752ms step_avg:43.49ms
step:961/1775 train_time:41811ms step_avg:43.51ms
step:962/1775 train_time:41871ms step_avg:43.53ms
step:963/1775 train_time:41930ms step_avg:43.54ms
step:964/1775 train_time:41990ms step_avg:43.56ms
step:965/1775 train_time:42049ms step_avg:43.57ms
step:966/1775 train_time:42110ms step_avg:43.59ms
step:967/1775 train_time:42168ms step_avg:43.61ms
step:968/1775 train_time:42228ms step_avg:43.62ms
step:969/1775 train_time:42287ms step_avg:43.64ms
step:970/1775 train_time:42348ms step_avg:43.66ms
step:971/1775 train_time:42406ms step_avg:43.67ms
step:972/1775 train_time:42467ms step_avg:43.69ms
step:973/1775 train_time:42525ms step_avg:43.70ms
step:974/1775 train_time:42586ms step_avg:43.72ms
step:975/1775 train_time:42644ms step_avg:43.74ms
step:976/1775 train_time:42705ms step_avg:43.75ms
step:977/1775 train_time:42763ms step_avg:43.77ms
step:978/1775 train_time:42824ms step_avg:43.79ms
step:979/1775 train_time:42883ms step_avg:43.80ms
step:980/1775 train_time:42943ms step_avg:43.82ms
step:981/1775 train_time:43002ms step_avg:43.83ms
step:982/1775 train_time:43062ms step_avg:43.85ms
step:983/1775 train_time:43120ms step_avg:43.87ms
step:984/1775 train_time:43181ms step_avg:43.88ms
step:985/1775 train_time:43239ms step_avg:43.90ms
step:986/1775 train_time:43299ms step_avg:43.91ms
step:987/1775 train_time:43358ms step_avg:43.93ms
step:988/1775 train_time:43419ms step_avg:43.95ms
step:989/1775 train_time:43477ms step_avg:43.96ms
step:990/1775 train_time:43539ms step_avg:43.98ms
step:991/1775 train_time:43598ms step_avg:43.99ms
step:992/1775 train_time:43658ms step_avg:44.01ms
step:993/1775 train_time:43718ms step_avg:44.03ms
step:994/1775 train_time:43778ms step_avg:44.04ms
step:995/1775 train_time:43837ms step_avg:44.06ms
step:996/1775 train_time:43898ms step_avg:44.07ms
step:997/1775 train_time:43955ms step_avg:44.09ms
step:998/1775 train_time:44015ms step_avg:44.10ms
step:999/1775 train_time:44074ms step_avg:44.12ms
step:1000/1775 train_time:44135ms step_avg:44.13ms
step:1000/1775 val_loss:3.7403 train_time:44205ms step_avg:44.20ms
step:1001/1775 train_time:44229ms step_avg:44.18ms
step:1002/1775 train_time:44256ms step_avg:44.17ms
step:1003/1775 train_time:44315ms step_avg:44.18ms
step:1004/1775 train_time:44377ms step_avg:44.20ms
step:1005/1775 train_time:44436ms step_avg:44.22ms
step:1006/1775 train_time:44497ms step_avg:44.23ms
step:1007/1775 train_time:44556ms step_avg:44.25ms
step:1008/1775 train_time:44615ms step_avg:44.26ms
step:1009/1775 train_time:44673ms step_avg:44.27ms
step:1010/1775 train_time:44734ms step_avg:44.29ms
step:1011/1775 train_time:44792ms step_avg:44.30ms
step:1012/1775 train_time:44853ms step_avg:44.32ms
step:1013/1775 train_time:44911ms step_avg:44.33ms
step:1014/1775 train_time:44971ms step_avg:44.35ms
step:1015/1775 train_time:45028ms step_avg:44.36ms
step:1016/1775 train_time:45089ms step_avg:44.38ms
step:1017/1775 train_time:45148ms step_avg:44.39ms
step:1018/1775 train_time:45211ms step_avg:44.41ms
step:1019/1775 train_time:45270ms step_avg:44.43ms
step:1020/1775 train_time:45333ms step_avg:44.44ms
step:1021/1775 train_time:45392ms step_avg:44.46ms
step:1022/1775 train_time:45454ms step_avg:44.48ms
step:1023/1775 train_time:45512ms step_avg:44.49ms
step:1024/1775 train_time:45573ms step_avg:44.51ms
step:1025/1775 train_time:45631ms step_avg:44.52ms
step:1026/1775 train_time:45691ms step_avg:44.53ms
step:1027/1775 train_time:45749ms step_avg:44.55ms
step:1028/1775 train_time:45809ms step_avg:44.56ms
step:1029/1775 train_time:45867ms step_avg:44.57ms
step:1030/1775 train_time:45927ms step_avg:44.59ms
step:1031/1775 train_time:45983ms step_avg:44.60ms
step:1032/1775 train_time:46044ms step_avg:44.62ms
step:1033/1775 train_time:46102ms step_avg:44.63ms
step:1034/1775 train_time:46163ms step_avg:44.65ms
step:1035/1775 train_time:46222ms step_avg:44.66ms
step:1036/1775 train_time:46283ms step_avg:44.68ms
step:1037/1775 train_time:46343ms step_avg:44.69ms
step:1038/1775 train_time:46405ms step_avg:44.71ms
step:1039/1775 train_time:46464ms step_avg:44.72ms
step:1040/1775 train_time:46527ms step_avg:44.74ms
step:1041/1775 train_time:46585ms step_avg:44.75ms
step:1042/1775 train_time:46645ms step_avg:44.77ms
step:1043/1775 train_time:46703ms step_avg:44.78ms
step:1044/1775 train_time:46764ms step_avg:44.79ms
step:1045/1775 train_time:46821ms step_avg:44.81ms
step:1046/1775 train_time:46882ms step_avg:44.82ms
step:1047/1775 train_time:46939ms step_avg:44.83ms
step:1048/1775 train_time:47000ms step_avg:44.85ms
step:1049/1775 train_time:47058ms step_avg:44.86ms
step:1050/1775 train_time:47118ms step_avg:44.87ms
step:1051/1775 train_time:47177ms step_avg:44.89ms
step:1052/1775 train_time:47238ms step_avg:44.90ms
step:1053/1775 train_time:47297ms step_avg:44.92ms
step:1054/1775 train_time:47358ms step_avg:44.93ms
step:1055/1775 train_time:47416ms step_avg:44.94ms
step:1056/1775 train_time:47478ms step_avg:44.96ms
step:1057/1775 train_time:47537ms step_avg:44.97ms
step:1058/1775 train_time:47597ms step_avg:44.99ms
step:1059/1775 train_time:47656ms step_avg:45.00ms
step:1060/1775 train_time:47717ms step_avg:45.02ms
step:1061/1775 train_time:47775ms step_avg:45.03ms
step:1062/1775 train_time:47835ms step_avg:45.04ms
step:1063/1775 train_time:47893ms step_avg:45.05ms
step:1064/1775 train_time:47953ms step_avg:45.07ms
step:1065/1775 train_time:48010ms step_avg:45.08ms
step:1066/1775 train_time:48071ms step_avg:45.09ms
step:1067/1775 train_time:48130ms step_avg:45.11ms
step:1068/1775 train_time:48191ms step_avg:45.12ms
step:1069/1775 train_time:48250ms step_avg:45.14ms
step:1070/1775 train_time:48311ms step_avg:45.15ms
step:1071/1775 train_time:48371ms step_avg:45.16ms
step:1072/1775 train_time:48431ms step_avg:45.18ms
step:1073/1775 train_time:48490ms step_avg:45.19ms
step:1074/1775 train_time:48552ms step_avg:45.21ms
step:1075/1775 train_time:48610ms step_avg:45.22ms
step:1076/1775 train_time:48672ms step_avg:45.23ms
step:1077/1775 train_time:48730ms step_avg:45.25ms
step:1078/1775 train_time:48790ms step_avg:45.26ms
step:1079/1775 train_time:48848ms step_avg:45.27ms
step:1080/1775 train_time:48908ms step_avg:45.29ms
step:1081/1775 train_time:48967ms step_avg:45.30ms
step:1082/1775 train_time:49027ms step_avg:45.31ms
step:1083/1775 train_time:49084ms step_avg:45.32ms
step:1084/1775 train_time:49145ms step_avg:45.34ms
step:1085/1775 train_time:49203ms step_avg:45.35ms
step:1086/1775 train_time:49264ms step_avg:45.36ms
step:1087/1775 train_time:49323ms step_avg:45.37ms
step:1088/1775 train_time:49383ms step_avg:45.39ms
step:1089/1775 train_time:49442ms step_avg:45.40ms
step:1090/1775 train_time:49504ms step_avg:45.42ms
step:1091/1775 train_time:49562ms step_avg:45.43ms
step:1092/1775 train_time:49624ms step_avg:45.44ms
step:1093/1775 train_time:49681ms step_avg:45.45ms
step:1094/1775 train_time:49742ms step_avg:45.47ms
step:1095/1775 train_time:49800ms step_avg:45.48ms
step:1096/1775 train_time:49860ms step_avg:45.49ms
step:1097/1775 train_time:49918ms step_avg:45.50ms
step:1098/1775 train_time:49978ms step_avg:45.52ms
step:1099/1775 train_time:50037ms step_avg:45.53ms
step:1100/1775 train_time:50097ms step_avg:45.54ms
step:1101/1775 train_time:50156ms step_avg:45.56ms
step:1102/1775 train_time:50217ms step_avg:45.57ms
step:1103/1775 train_time:50276ms step_avg:45.58ms
step:1104/1775 train_time:50337ms step_avg:45.60ms
step:1105/1775 train_time:50396ms step_avg:45.61ms
step:1106/1775 train_time:50457ms step_avg:45.62ms
step:1107/1775 train_time:50516ms step_avg:45.63ms
step:1108/1775 train_time:50577ms step_avg:45.65ms
step:1109/1775 train_time:50636ms step_avg:45.66ms
step:1110/1775 train_time:50696ms step_avg:45.67ms
step:1111/1775 train_time:50755ms step_avg:45.68ms
step:1112/1775 train_time:50816ms step_avg:45.70ms
step:1113/1775 train_time:50875ms step_avg:45.71ms
step:1114/1775 train_time:50935ms step_avg:45.72ms
step:1115/1775 train_time:50993ms step_avg:45.73ms
step:1116/1775 train_time:51053ms step_avg:45.75ms
step:1117/1775 train_time:51111ms step_avg:45.76ms
step:1118/1775 train_time:51173ms step_avg:45.77ms
step:1119/1775 train_time:51231ms step_avg:45.78ms
step:1120/1775 train_time:51293ms step_avg:45.80ms
step:1121/1775 train_time:51351ms step_avg:45.81ms
step:1122/1775 train_time:51413ms step_avg:45.82ms
step:1123/1775 train_time:51472ms step_avg:45.83ms
step:1124/1775 train_time:51532ms step_avg:45.85ms
step:1125/1775 train_time:51591ms step_avg:45.86ms
step:1126/1775 train_time:51651ms step_avg:45.87ms
step:1127/1775 train_time:51709ms step_avg:45.88ms
step:1128/1775 train_time:51771ms step_avg:45.90ms
step:1129/1775 train_time:51830ms step_avg:45.91ms
step:1130/1775 train_time:51891ms step_avg:45.92ms
step:1131/1775 train_time:51950ms step_avg:45.93ms
step:1132/1775 train_time:52011ms step_avg:45.95ms
step:1133/1775 train_time:52069ms step_avg:45.96ms
step:1134/1775 train_time:52130ms step_avg:45.97ms
step:1135/1775 train_time:52188ms step_avg:45.98ms
step:1136/1775 train_time:52249ms step_avg:45.99ms
step:1137/1775 train_time:52307ms step_avg:46.00ms
step:1138/1775 train_time:52367ms step_avg:46.02ms
step:1139/1775 train_time:52426ms step_avg:46.03ms
step:1140/1775 train_time:52487ms step_avg:46.04ms
step:1141/1775 train_time:52545ms step_avg:46.05ms
step:1142/1775 train_time:52606ms step_avg:46.07ms
step:1143/1775 train_time:52665ms step_avg:46.08ms
step:1144/1775 train_time:52726ms step_avg:46.09ms
step:1145/1775 train_time:52784ms step_avg:46.10ms
step:1146/1775 train_time:52845ms step_avg:46.11ms
step:1147/1775 train_time:52903ms step_avg:46.12ms
step:1148/1775 train_time:52964ms step_avg:46.14ms
step:1149/1775 train_time:53023ms step_avg:46.15ms
step:1150/1775 train_time:53083ms step_avg:46.16ms
step:1151/1775 train_time:53141ms step_avg:46.17ms
step:1152/1775 train_time:53202ms step_avg:46.18ms
step:1153/1775 train_time:53259ms step_avg:46.19ms
step:1154/1775 train_time:53321ms step_avg:46.21ms
step:1155/1775 train_time:53379ms step_avg:46.22ms
step:1156/1775 train_time:53441ms step_avg:46.23ms
step:1157/1775 train_time:53499ms step_avg:46.24ms
step:1158/1775 train_time:53565ms step_avg:46.26ms
step:1159/1775 train_time:53650ms step_avg:46.29ms
step:1160/1775 train_time:53736ms step_avg:46.32ms
step:1161/1775 train_time:53821ms step_avg:46.36ms
step:1162/1775 train_time:53908ms step_avg:46.39ms
step:1163/1775 train_time:53991ms step_avg:46.42ms
step:1164/1775 train_time:54078ms step_avg:46.46ms
step:1165/1775 train_time:54162ms step_avg:46.49ms
step:1166/1775 train_time:54250ms step_avg:46.53ms
step:1167/1775 train_time:54333ms step_avg:46.56ms
step:1168/1775 train_time:54421ms step_avg:46.59ms
step:1169/1775 train_time:54505ms step_avg:46.63ms
step:1170/1775 train_time:54592ms step_avg:46.66ms
step:1171/1775 train_time:54677ms step_avg:46.69ms
step:1172/1775 train_time:54763ms step_avg:46.73ms
step:1173/1775 train_time:54847ms step_avg:46.76ms
step:1174/1775 train_time:54933ms step_avg:46.79ms
step:1175/1775 train_time:55017ms step_avg:46.82ms
step:1176/1775 train_time:55102ms step_avg:46.86ms
step:1177/1775 train_time:55188ms step_avg:46.89ms
step:1178/1775 train_time:55274ms step_avg:46.92ms
step:1179/1775 train_time:55359ms step_avg:46.95ms
step:1180/1775 train_time:55447ms step_avg:46.99ms
step:1181/1775 train_time:55530ms step_avg:47.02ms
step:1182/1775 train_time:55617ms step_avg:47.05ms
step:1183/1775 train_time:55700ms step_avg:47.08ms
step:1184/1775 train_time:55787ms step_avg:47.12ms
step:1185/1775 train_time:55871ms step_avg:47.15ms
step:1186/1775 train_time:55958ms step_avg:47.18ms
step:1187/1775 train_time:56042ms step_avg:47.21ms
step:1188/1775 train_time:56128ms step_avg:47.25ms
step:1189/1775 train_time:56211ms step_avg:47.28ms
step:1190/1775 train_time:56297ms step_avg:47.31ms
step:1191/1775 train_time:56382ms step_avg:47.34ms
step:1192/1775 train_time:56470ms step_avg:47.37ms
step:1193/1775 train_time:56553ms step_avg:47.40ms
step:1194/1775 train_time:56640ms step_avg:47.44ms
step:1195/1775 train_time:56724ms step_avg:47.47ms
step:1196/1775 train_time:56811ms step_avg:47.50ms
step:1197/1775 train_time:56895ms step_avg:47.53ms
step:1198/1775 train_time:56981ms step_avg:47.56ms
step:1199/1775 train_time:57065ms step_avg:47.59ms
step:1200/1775 train_time:57151ms step_avg:47.63ms
step:1201/1775 train_time:57236ms step_avg:47.66ms
step:1202/1775 train_time:57324ms step_avg:47.69ms
step:1203/1775 train_time:57409ms step_avg:47.72ms
step:1204/1775 train_time:57494ms step_avg:47.75ms
step:1205/1775 train_time:57579ms step_avg:47.78ms
step:1206/1775 train_time:57666ms step_avg:47.82ms
step:1207/1775 train_time:57750ms step_avg:47.85ms
step:1208/1775 train_time:57836ms step_avg:47.88ms
step:1209/1775 train_time:57919ms step_avg:47.91ms
step:1210/1775 train_time:58005ms step_avg:47.94ms
step:1211/1775 train_time:58089ms step_avg:47.97ms
step:1212/1775 train_time:58176ms step_avg:48.00ms
step:1213/1775 train_time:58260ms step_avg:48.03ms
step:1214/1775 train_time:58348ms step_avg:48.06ms
step:1215/1775 train_time:58431ms step_avg:48.09ms
step:1216/1775 train_time:58519ms step_avg:48.12ms
step:1217/1775 train_time:58604ms step_avg:48.15ms
step:1218/1775 train_time:58690ms step_avg:48.19ms
step:1219/1775 train_time:58775ms step_avg:48.22ms
step:1220/1775 train_time:58862ms step_avg:48.25ms
step:1221/1775 train_time:58947ms step_avg:48.28ms
step:1222/1775 train_time:59032ms step_avg:48.31ms
step:1223/1775 train_time:59117ms step_avg:48.34ms
step:1224/1775 train_time:59203ms step_avg:48.37ms
step:1225/1775 train_time:59289ms step_avg:48.40ms
step:1226/1775 train_time:59376ms step_avg:48.43ms
step:1227/1775 train_time:59459ms step_avg:48.46ms
step:1228/1775 train_time:59546ms step_avg:48.49ms
step:1229/1775 train_time:59629ms step_avg:48.52ms
step:1230/1775 train_time:59717ms step_avg:48.55ms
step:1231/1775 train_time:59802ms step_avg:48.58ms
step:1232/1775 train_time:59890ms step_avg:48.61ms
step:1233/1775 train_time:59973ms step_avg:48.64ms
step:1234/1775 train_time:60060ms step_avg:48.67ms
step:1235/1775 train_time:60145ms step_avg:48.70ms
step:1236/1775 train_time:60232ms step_avg:48.73ms
step:1237/1775 train_time:60317ms step_avg:48.76ms
step:1238/1775 train_time:60403ms step_avg:48.79ms
step:1239/1775 train_time:60488ms step_avg:48.82ms
step:1240/1775 train_time:60573ms step_avg:48.85ms
step:1241/1775 train_time:60658ms step_avg:48.88ms
step:1242/1775 train_time:60744ms step_avg:48.91ms
step:1243/1775 train_time:60829ms step_avg:48.94ms
step:1244/1775 train_time:60914ms step_avg:48.97ms
step:1245/1775 train_time:60998ms step_avg:48.99ms
step:1246/1775 train_time:61087ms step_avg:49.03ms
step:1247/1775 train_time:61171ms step_avg:49.05ms
step:1248/1775 train_time:61257ms step_avg:49.08ms
step:1249/1775 train_time:61341ms step_avg:49.11ms
step:1250/1775 train_time:61428ms step_avg:49.14ms
step:1250/1775 val_loss:3.5051 train_time:61524ms step_avg:49.22ms
step:1251/1775 train_time:61549ms step_avg:49.20ms
step:1252/1775 train_time:61602ms step_avg:49.20ms
step:1253/1775 train_time:61692ms step_avg:49.24ms
step:1254/1775 train_time:61779ms step_avg:49.27ms
step:1255/1775 train_time:61862ms step_avg:49.29ms
step:1256/1775 train_time:61947ms step_avg:49.32ms
step:1257/1775 train_time:62029ms step_avg:49.35ms
step:1258/1775 train_time:62113ms step_avg:49.37ms
step:1259/1775 train_time:62197ms step_avg:49.40ms
step:1260/1775 train_time:62284ms step_avg:49.43ms
step:1261/1775 train_time:62367ms step_avg:49.46ms
step:1262/1775 train_time:62453ms step_avg:49.49ms
step:1263/1775 train_time:62538ms step_avg:49.52ms
step:1264/1775 train_time:62629ms step_avg:49.55ms
step:1265/1775 train_time:62714ms step_avg:49.58ms
step:1266/1775 train_time:62805ms step_avg:49.61ms
step:1267/1775 train_time:62890ms step_avg:49.64ms
step:1268/1775 train_time:62975ms step_avg:49.67ms
step:1269/1775 train_time:63058ms step_avg:49.69ms
step:1270/1775 train_time:63143ms step_avg:49.72ms
step:1271/1775 train_time:63226ms step_avg:49.75ms
step:1272/1775 train_time:63312ms step_avg:49.77ms
step:1273/1775 train_time:63395ms step_avg:49.80ms
step:1274/1775 train_time:63483ms step_avg:49.83ms
step:1275/1775 train_time:63569ms step_avg:49.86ms
step:1276/1775 train_time:63656ms step_avg:49.89ms
step:1277/1775 train_time:63742ms step_avg:49.92ms
step:1278/1775 train_time:63830ms step_avg:49.95ms
step:1279/1775 train_time:63913ms step_avg:49.97ms
step:1280/1775 train_time:64001ms step_avg:50.00ms
step:1281/1775 train_time:64084ms step_avg:50.03ms
step:1282/1775 train_time:64171ms step_avg:50.06ms
step:1283/1775 train_time:64254ms step_avg:50.08ms
step:1284/1775 train_time:64339ms step_avg:50.11ms
step:1285/1775 train_time:64424ms step_avg:50.14ms
step:1286/1775 train_time:64512ms step_avg:50.16ms
step:1287/1775 train_time:64596ms step_avg:50.19ms
step:1288/1775 train_time:64684ms step_avg:50.22ms
step:1289/1775 train_time:64770ms step_avg:50.25ms
step:1290/1775 train_time:64855ms step_avg:50.28ms
step:1291/1775 train_time:64940ms step_avg:50.30ms
step:1292/1775 train_time:65025ms step_avg:50.33ms
step:1293/1775 train_time:65110ms step_avg:50.36ms
step:1294/1775 train_time:65195ms step_avg:50.38ms
step:1295/1775 train_time:65279ms step_avg:50.41ms
step:1296/1775 train_time:65365ms step_avg:50.44ms
step:1297/1775 train_time:65448ms step_avg:50.46ms
step:1298/1775 train_time:65535ms step_avg:50.49ms
step:1299/1775 train_time:65620ms step_avg:50.52ms
step:1300/1775 train_time:65708ms step_avg:50.54ms
step:1301/1775 train_time:65792ms step_avg:50.57ms
step:1302/1775 train_time:65879ms step_avg:50.60ms
step:1303/1775 train_time:65963ms step_avg:50.62ms
step:1304/1775 train_time:66049ms step_avg:50.65ms
step:1305/1775 train_time:66132ms step_avg:50.68ms
step:1306/1775 train_time:66219ms step_avg:50.70ms
step:1307/1775 train_time:66302ms step_avg:50.73ms
step:1308/1775 train_time:66389ms step_avg:50.76ms
step:1309/1775 train_time:66473ms step_avg:50.78ms
step:1310/1775 train_time:66560ms step_avg:50.81ms
step:1311/1775 train_time:66643ms step_avg:50.83ms
step:1312/1775 train_time:66732ms step_avg:50.86ms
step:1313/1775 train_time:66815ms step_avg:50.89ms
step:1314/1775 train_time:66904ms step_avg:50.92ms
step:1315/1775 train_time:66988ms step_avg:50.94ms
step:1316/1775 train_time:67074ms step_avg:50.97ms
step:1317/1775 train_time:67157ms step_avg:50.99ms
step:1318/1775 train_time:67245ms step_avg:51.02ms
step:1319/1775 train_time:67330ms step_avg:51.05ms
step:1320/1775 train_time:67415ms step_avg:51.07ms
step:1321/1775 train_time:67498ms step_avg:51.10ms
step:1322/1775 train_time:67585ms step_avg:51.12ms
step:1323/1775 train_time:67672ms step_avg:51.15ms
step:1324/1775 train_time:67758ms step_avg:51.18ms
step:1325/1775 train_time:67841ms step_avg:51.20ms
step:1326/1775 train_time:67929ms step_avg:51.23ms
step:1327/1775 train_time:68012ms step_avg:51.25ms
step:1328/1775 train_time:68099ms step_avg:51.28ms
step:1329/1775 train_time:68183ms step_avg:51.30ms
step:1330/1775 train_time:68270ms step_avg:51.33ms
step:1331/1775 train_time:68353ms step_avg:51.35ms
step:1332/1775 train_time:68439ms step_avg:51.38ms
step:1333/1775 train_time:68522ms step_avg:51.40ms
step:1334/1775 train_time:68610ms step_avg:51.43ms
step:1335/1775 train_time:68694ms step_avg:51.46ms
step:1336/1775 train_time:68782ms step_avg:51.48ms
step:1337/1775 train_time:68867ms step_avg:51.51ms
step:1338/1775 train_time:68953ms step_avg:51.53ms
step:1339/1775 train_time:69037ms step_avg:51.56ms
step:1340/1775 train_time:69123ms step_avg:51.58ms
step:1341/1775 train_time:69208ms step_avg:51.61ms
step:1342/1775 train_time:69294ms step_avg:51.63ms
step:1343/1775 train_time:69379ms step_avg:51.66ms
step:1344/1775 train_time:69465ms step_avg:51.69ms
step:1345/1775 train_time:69550ms step_avg:51.71ms
step:1346/1775 train_time:69636ms step_avg:51.74ms
step:1347/1775 train_time:69720ms step_avg:51.76ms
step:1348/1775 train_time:69809ms step_avg:51.79ms
step:1349/1775 train_time:69893ms step_avg:51.81ms
step:1350/1775 train_time:69979ms step_avg:51.84ms
step:1351/1775 train_time:70063ms step_avg:51.86ms
step:1352/1775 train_time:70149ms step_avg:51.89ms
step:1353/1775 train_time:70232ms step_avg:51.91ms
step:1354/1775 train_time:70320ms step_avg:51.93ms
step:1355/1775 train_time:70404ms step_avg:51.96ms
step:1356/1775 train_time:70491ms step_avg:51.98ms
step:1357/1775 train_time:70574ms step_avg:52.01ms
step:1358/1775 train_time:70661ms step_avg:52.03ms
step:1359/1775 train_time:70745ms step_avg:52.06ms
step:1360/1775 train_time:70832ms step_avg:52.08ms
step:1361/1775 train_time:70916ms step_avg:52.11ms
step:1362/1775 train_time:71003ms step_avg:52.13ms
step:1363/1775 train_time:71087ms step_avg:52.16ms
step:1364/1775 train_time:71173ms step_avg:52.18ms
step:1365/1775 train_time:71257ms step_avg:52.20ms
step:1366/1775 train_time:71344ms step_avg:52.23ms
step:1367/1775 train_time:71429ms step_avg:52.25ms
step:1368/1775 train_time:71514ms step_avg:52.28ms
step:1369/1775 train_time:71599ms step_avg:52.30ms
step:1370/1775 train_time:71687ms step_avg:52.33ms
step:1371/1775 train_time:71771ms step_avg:52.35ms
step:1372/1775 train_time:71859ms step_avg:52.38ms
step:1373/1775 train_time:71943ms step_avg:52.40ms
step:1374/1775 train_time:72030ms step_avg:52.42ms
step:1375/1775 train_time:72114ms step_avg:52.45ms
step:1376/1775 train_time:72201ms step_avg:52.47ms
step:1377/1775 train_time:72285ms step_avg:52.49ms
step:1378/1775 train_time:72372ms step_avg:52.52ms
step:1379/1775 train_time:72456ms step_avg:52.54ms
step:1380/1775 train_time:72544ms step_avg:52.57ms
step:1381/1775 train_time:72629ms step_avg:52.59ms
step:1382/1775 train_time:72715ms step_avg:52.62ms
step:1383/1775 train_time:72799ms step_avg:52.64ms
step:1384/1775 train_time:72885ms step_avg:52.66ms
step:1385/1775 train_time:72971ms step_avg:52.69ms
step:1386/1775 train_time:73057ms step_avg:52.71ms
step:1387/1775 train_time:73139ms step_avg:52.73ms
step:1388/1775 train_time:73225ms step_avg:52.76ms
step:1389/1775 train_time:73310ms step_avg:52.78ms
step:1390/1775 train_time:73396ms step_avg:52.80ms
step:1391/1775 train_time:73480ms step_avg:52.83ms
step:1392/1775 train_time:73567ms step_avg:52.85ms
step:1393/1775 train_time:73651ms step_avg:52.87ms
step:1394/1775 train_time:73738ms step_avg:52.90ms
step:1395/1775 train_time:73821ms step_avg:52.92ms
step:1396/1775 train_time:73909ms step_avg:52.94ms
step:1397/1775 train_time:73993ms step_avg:52.97ms
step:1398/1775 train_time:74080ms step_avg:52.99ms
step:1399/1775 train_time:74163ms step_avg:53.01ms
step:1400/1775 train_time:74251ms step_avg:53.04ms
step:1401/1775 train_time:74334ms step_avg:53.06ms
step:1402/1775 train_time:74421ms step_avg:53.08ms
step:1403/1775 train_time:74505ms step_avg:53.10ms
step:1404/1775 train_time:74592ms step_avg:53.13ms
step:1405/1775 train_time:74676ms step_avg:53.15ms
step:1406/1775 train_time:74763ms step_avg:53.17ms
step:1407/1775 train_time:74847ms step_avg:53.20ms
step:1408/1775 train_time:74934ms step_avg:53.22ms
step:1409/1775 train_time:75018ms step_avg:53.24ms
step:1410/1775 train_time:75105ms step_avg:53.27ms
step:1411/1775 train_time:75190ms step_avg:53.29ms
step:1412/1775 train_time:75275ms step_avg:53.31ms
step:1413/1775 train_time:75358ms step_avg:53.33ms
step:1414/1775 train_time:75446ms step_avg:53.36ms
step:1415/1775 train_time:75530ms step_avg:53.38ms
step:1416/1775 train_time:75615ms step_avg:53.40ms
step:1417/1775 train_time:75700ms step_avg:53.42ms
step:1418/1775 train_time:75787ms step_avg:53.45ms
step:1419/1775 train_time:75872ms step_avg:53.47ms
step:1420/1775 train_time:75958ms step_avg:53.49ms
step:1421/1775 train_time:76042ms step_avg:53.51ms
step:1422/1775 train_time:76129ms step_avg:53.54ms
step:1423/1775 train_time:76212ms step_avg:53.56ms
step:1424/1775 train_time:76299ms step_avg:53.58ms
step:1425/1775 train_time:76383ms step_avg:53.60ms
step:1426/1775 train_time:76471ms step_avg:53.63ms
step:1427/1775 train_time:76554ms step_avg:53.65ms
step:1428/1775 train_time:76640ms step_avg:53.67ms
step:1429/1775 train_time:76724ms step_avg:53.69ms
step:1430/1775 train_time:76813ms step_avg:53.72ms
step:1431/1775 train_time:76896ms step_avg:53.74ms
step:1432/1775 train_time:76982ms step_avg:53.76ms
step:1433/1775 train_time:77068ms step_avg:53.78ms
step:1434/1775 train_time:77154ms step_avg:53.80ms
step:1435/1775 train_time:77237ms step_avg:53.82ms
step:1436/1775 train_time:77324ms step_avg:53.85ms
step:1437/1775 train_time:77409ms step_avg:53.87ms
step:1438/1775 train_time:77495ms step_avg:53.89ms
step:1439/1775 train_time:77579ms step_avg:53.91ms
step:1440/1775 train_time:77665ms step_avg:53.93ms
step:1441/1775 train_time:77749ms step_avg:53.96ms
step:1442/1775 train_time:77836ms step_avg:53.98ms
step:1443/1775 train_time:77920ms step_avg:54.00ms
step:1444/1775 train_time:78007ms step_avg:54.02ms
step:1445/1775 train_time:78092ms step_avg:54.04ms
step:1446/1775 train_time:78179ms step_avg:54.07ms
step:1447/1775 train_time:78264ms step_avg:54.09ms
step:1448/1775 train_time:78351ms step_avg:54.11ms
step:1449/1775 train_time:78434ms step_avg:54.13ms
step:1450/1775 train_time:78521ms step_avg:54.15ms
step:1451/1775 train_time:78605ms step_avg:54.17ms
step:1452/1775 train_time:78692ms step_avg:54.20ms
step:1453/1775 train_time:78775ms step_avg:54.22ms
step:1454/1775 train_time:78864ms step_avg:54.24ms
step:1455/1775 train_time:78948ms step_avg:54.26ms
step:1456/1775 train_time:79034ms step_avg:54.28ms
step:1457/1775 train_time:79118ms step_avg:54.30ms
step:1458/1775 train_time:79205ms step_avg:54.32ms
step:1459/1775 train_time:79290ms step_avg:54.35ms
step:1460/1775 train_time:79375ms step_avg:54.37ms
step:1461/1775 train_time:79460ms step_avg:54.39ms
step:1462/1775 train_time:79547ms step_avg:54.41ms
step:1463/1775 train_time:79632ms step_avg:54.43ms
step:1464/1775 train_time:79718ms step_avg:54.45ms
step:1465/1775 train_time:79802ms step_avg:54.47ms
step:1466/1775 train_time:79890ms step_avg:54.50ms
step:1467/1775 train_time:79974ms step_avg:54.52ms
step:1468/1775 train_time:80062ms step_avg:54.54ms
step:1469/1775 train_time:80146ms step_avg:54.56ms
step:1470/1775 train_time:80232ms step_avg:54.58ms
step:1471/1775 train_time:80316ms step_avg:54.60ms
step:1472/1775 train_time:80403ms step_avg:54.62ms
step:1473/1775 train_time:80488ms step_avg:54.64ms
step:1474/1775 train_time:80574ms step_avg:54.66ms
step:1475/1775 train_time:80659ms step_avg:54.68ms
step:1476/1775 train_time:80746ms step_avg:54.71ms
step:1477/1775 train_time:80831ms step_avg:54.73ms
step:1478/1775 train_time:80916ms step_avg:54.75ms
step:1479/1775 train_time:81000ms step_avg:54.77ms
step:1480/1775 train_time:81087ms step_avg:54.79ms
step:1481/1775 train_time:81171ms step_avg:54.81ms
step:1482/1775 train_time:81256ms step_avg:54.83ms
step:1483/1775 train_time:81341ms step_avg:54.85ms
step:1484/1775 train_time:81427ms step_avg:54.87ms
step:1485/1775 train_time:81511ms step_avg:54.89ms
step:1486/1775 train_time:81597ms step_avg:54.91ms
step:1487/1775 train_time:81681ms step_avg:54.93ms
step:1488/1775 train_time:81769ms step_avg:54.95ms
step:1489/1775 train_time:81851ms step_avg:54.97ms
step:1490/1775 train_time:81939ms step_avg:54.99ms
step:1491/1775 train_time:82023ms step_avg:55.01ms
step:1492/1775 train_time:82112ms step_avg:55.03ms
step:1493/1775 train_time:82195ms step_avg:55.05ms
step:1494/1775 train_time:82281ms step_avg:55.07ms
step:1495/1775 train_time:82366ms step_avg:55.09ms
step:1496/1775 train_time:82452ms step_avg:55.12ms
step:1497/1775 train_time:82538ms step_avg:55.14ms
step:1498/1775 train_time:82623ms step_avg:55.16ms
step:1499/1775 train_time:82708ms step_avg:55.18ms
step:1500/1775 train_time:82794ms step_avg:55.20ms
step:1500/1775 val_loss:3.3763 train_time:82890ms step_avg:55.26ms
step:1501/1775 train_time:82915ms step_avg:55.24ms
step:1502/1775 train_time:82969ms step_avg:55.24ms
step:1503/1775 train_time:83057ms step_avg:55.26ms
step:1504/1775 train_time:83148ms step_avg:55.28ms
step:1505/1775 train_time:83232ms step_avg:55.30ms
step:1506/1775 train_time:83318ms step_avg:55.32ms
step:1507/1775 train_time:83402ms step_avg:55.34ms
step:1508/1775 train_time:83489ms step_avg:55.36ms
step:1509/1775 train_time:83571ms step_avg:55.38ms
step:1510/1775 train_time:83657ms step_avg:55.40ms
step:1511/1775 train_time:83739ms step_avg:55.42ms
step:1512/1775 train_time:83826ms step_avg:55.44ms
step:1513/1775 train_time:83911ms step_avg:55.46ms
step:1514/1775 train_time:84000ms step_avg:55.48ms
step:1515/1775 train_time:84088ms step_avg:55.50ms
step:1516/1775 train_time:84176ms step_avg:55.53ms
step:1517/1775 train_time:84260ms step_avg:55.54ms
step:1518/1775 train_time:84346ms step_avg:55.56ms
step:1519/1775 train_time:84430ms step_avg:55.58ms
step:1520/1775 train_time:84517ms step_avg:55.60ms
step:1521/1775 train_time:84599ms step_avg:55.62ms
step:1522/1775 train_time:84685ms step_avg:55.64ms
step:1523/1775 train_time:84767ms step_avg:55.66ms
step:1524/1775 train_time:84856ms step_avg:55.68ms
step:1525/1775 train_time:84941ms step_avg:55.70ms
step:1526/1775 train_time:85031ms step_avg:55.72ms
step:1527/1775 train_time:85116ms step_avg:55.74ms
step:1528/1775 train_time:85201ms step_avg:55.76ms
step:1529/1775 train_time:85286ms step_avg:55.78ms
step:1530/1775 train_time:85371ms step_avg:55.80ms
step:1531/1775 train_time:85455ms step_avg:55.82ms
step:1532/1775 train_time:85539ms step_avg:55.84ms
step:1533/1775 train_time:85623ms step_avg:55.85ms
step:1534/1775 train_time:85711ms step_avg:55.87ms
step:1535/1775 train_time:85796ms step_avg:55.89ms
step:1536/1775 train_time:85882ms step_avg:55.91ms
step:1537/1775 train_time:85968ms step_avg:55.93ms
step:1538/1775 train_time:86056ms step_avg:55.95ms
step:1539/1775 train_time:86139ms step_avg:55.97ms
step:1540/1775 train_time:86228ms step_avg:55.99ms
step:1541/1775 train_time:86312ms step_avg:56.01ms
step:1542/1775 train_time:86399ms step_avg:56.03ms
step:1543/1775 train_time:86482ms step_avg:56.05ms
step:1544/1775 train_time:86567ms step_avg:56.07ms
step:1545/1775 train_time:86651ms step_avg:56.09ms
step:1546/1775 train_time:86738ms step_avg:56.10ms
step:1547/1775 train_time:86821ms step_avg:56.12ms
step:1548/1775 train_time:86908ms step_avg:56.14ms
step:1549/1775 train_time:86993ms step_avg:56.16ms
step:1550/1775 train_time:87079ms step_avg:56.18ms
step:1551/1775 train_time:87164ms step_avg:56.20ms
step:1552/1775 train_time:87252ms step_avg:56.22ms
step:1553/1775 train_time:87337ms step_avg:56.24ms
step:1554/1775 train_time:87423ms step_avg:56.26ms
step:1555/1775 train_time:87506ms step_avg:56.27ms
step:1556/1775 train_time:87593ms step_avg:56.29ms
step:1557/1775 train_time:87677ms step_avg:56.31ms
step:1558/1775 train_time:87763ms step_avg:56.33ms
step:1559/1775 train_time:87846ms step_avg:56.35ms
step:1560/1775 train_time:87934ms step_avg:56.37ms
step:1561/1775 train_time:88018ms step_avg:56.39ms
step:1562/1775 train_time:88106ms step_avg:56.41ms
step:1563/1775 train_time:88192ms step_avg:56.42ms
step:1564/1775 train_time:88278ms step_avg:56.44ms
step:1565/1775 train_time:88362ms step_avg:56.46ms
step:1566/1775 train_time:88449ms step_avg:56.48ms
step:1567/1775 train_time:88532ms step_avg:56.50ms
step:1568/1775 train_time:88618ms step_avg:56.52ms
step:1569/1775 train_time:88702ms step_avg:56.53ms
step:1570/1775 train_time:88789ms step_avg:56.55ms
step:1571/1775 train_time:88873ms step_avg:56.57ms
step:1572/1775 train_time:88959ms step_avg:56.59ms
step:1573/1775 train_time:89044ms step_avg:56.61ms
step:1574/1775 train_time:89132ms step_avg:56.63ms
step:1575/1775 train_time:89216ms step_avg:56.64ms
step:1576/1775 train_time:89302ms step_avg:56.66ms
step:1577/1775 train_time:89386ms step_avg:56.68ms
step:1578/1775 train_time:89473ms step_avg:56.70ms
step:1579/1775 train_time:89557ms step_avg:56.72ms
step:1580/1775 train_time:89644ms step_avg:56.74ms
step:1581/1775 train_time:89727ms step_avg:56.75ms
step:1582/1775 train_time:89815ms step_avg:56.77ms
step:1583/1775 train_time:89899ms step_avg:56.79ms
step:1584/1775 train_time:89985ms step_avg:56.81ms
step:1585/1775 train_time:90069ms step_avg:56.83ms
step:1586/1775 train_time:90157ms step_avg:56.85ms
step:1587/1775 train_time:90241ms step_avg:56.86ms
step:1588/1775 train_time:90329ms step_avg:56.88ms
step:1589/1775 train_time:90414ms step_avg:56.90ms
step:1590/1775 train_time:90499ms step_avg:56.92ms
step:1591/1775 train_time:90583ms step_avg:56.93ms
step:1592/1775 train_time:90670ms step_avg:56.95ms
step:1593/1775 train_time:90755ms step_avg:56.97ms
step:1594/1775 train_time:90840ms step_avg:56.99ms
step:1595/1775 train_time:90926ms step_avg:57.01ms
step:1596/1775 train_time:91012ms step_avg:57.03ms
step:1597/1775 train_time:91098ms step_avg:57.04ms
step:1598/1775 train_time:91184ms step_avg:57.06ms
step:1599/1775 train_time:91267ms step_avg:57.08ms
step:1600/1775 train_time:91354ms step_avg:57.10ms
step:1601/1775 train_time:91438ms step_avg:57.11ms
step:1602/1775 train_time:91526ms step_avg:57.13ms
step:1603/1775 train_time:91609ms step_avg:57.15ms
step:1604/1775 train_time:91697ms step_avg:57.17ms
step:1605/1775 train_time:91781ms step_avg:57.18ms
step:1606/1775 train_time:91868ms step_avg:57.20ms
step:1607/1775 train_time:91952ms step_avg:57.22ms
step:1608/1775 train_time:92039ms step_avg:57.24ms
step:1609/1775 train_time:92123ms step_avg:57.25ms
step:1610/1775 train_time:92209ms step_avg:57.27ms
step:1611/1775 train_time:92295ms step_avg:57.29ms
step:1612/1775 train_time:92380ms step_avg:57.31ms
step:1613/1775 train_time:92465ms step_avg:57.32ms
step:1614/1775 train_time:92550ms step_avg:57.34ms
step:1615/1775 train_time:92636ms step_avg:57.36ms
step:1616/1775 train_time:92722ms step_avg:57.38ms
step:1617/1775 train_time:92805ms step_avg:57.39ms
step:1618/1775 train_time:92892ms step_avg:57.41ms
step:1619/1775 train_time:92976ms step_avg:57.43ms
step:1620/1775 train_time:93063ms step_avg:57.45ms
step:1621/1775 train_time:93146ms step_avg:57.46ms
step:1622/1775 train_time:93233ms step_avg:57.48ms
step:1623/1775 train_time:93317ms step_avg:57.50ms
step:1624/1775 train_time:93404ms step_avg:57.51ms
step:1625/1775 train_time:93488ms step_avg:57.53ms
step:1626/1775 train_time:93574ms step_avg:57.55ms
step:1627/1775 train_time:93657ms step_avg:57.56ms
step:1628/1775 train_time:93745ms step_avg:57.58ms
step:1629/1775 train_time:93830ms step_avg:57.60ms
step:1630/1775 train_time:93918ms step_avg:57.62ms
step:1631/1775 train_time:94003ms step_avg:57.63ms
step:1632/1775 train_time:94091ms step_avg:57.65ms
step:1633/1775 train_time:94176ms step_avg:57.67ms
step:1634/1775 train_time:94261ms step_avg:57.69ms
step:1635/1775 train_time:94345ms step_avg:57.70ms
step:1636/1775 train_time:94433ms step_avg:57.72ms
step:1637/1775 train_time:94517ms step_avg:57.74ms
step:1638/1775 train_time:94604ms step_avg:57.76ms
step:1639/1775 train_time:94688ms step_avg:57.77ms
step:1640/1775 train_time:94775ms step_avg:57.79ms
step:1641/1775 train_time:94858ms step_avg:57.80ms
step:1642/1775 train_time:94946ms step_avg:57.82ms
step:1643/1775 train_time:95029ms step_avg:57.84ms
step:1644/1775 train_time:95116ms step_avg:57.86ms
step:1645/1775 train_time:95200ms step_avg:57.87ms
step:1646/1775 train_time:95287ms step_avg:57.89ms
step:1647/1775 train_time:95370ms step_avg:57.91ms
step:1648/1775 train_time:95457ms step_avg:57.92ms
step:1649/1775 train_time:95540ms step_avg:57.94ms
step:1650/1775 train_time:95627ms step_avg:57.96ms
step:1651/1775 train_time:95712ms step_avg:57.97ms
step:1652/1775 train_time:95798ms step_avg:57.99ms
step:1653/1775 train_time:95882ms step_avg:58.01ms
step:1654/1775 train_time:95969ms step_avg:58.02ms
step:1655/1775 train_time:96054ms step_avg:58.04ms
step:1656/1775 train_time:96140ms step_avg:58.06ms
step:1657/1775 train_time:96225ms step_avg:58.07ms
step:1658/1775 train_time:96312ms step_avg:58.09ms
step:1659/1775 train_time:96397ms step_avg:58.11ms
step:1660/1775 train_time:96482ms step_avg:58.12ms
step:1661/1775 train_time:96566ms step_avg:58.14ms
step:1662/1775 train_time:96654ms step_avg:58.16ms
step:1663/1775 train_time:96737ms step_avg:58.17ms
step:1664/1775 train_time:96825ms step_avg:58.19ms
step:1665/1775 train_time:96909ms step_avg:58.20ms
step:1666/1775 train_time:96997ms step_avg:58.22ms
step:1667/1775 train_time:97080ms step_avg:58.24ms
step:1668/1775 train_time:97168ms step_avg:58.25ms
step:1669/1775 train_time:97252ms step_avg:58.27ms
step:1670/1775 train_time:97339ms step_avg:58.29ms
step:1671/1775 train_time:97423ms step_avg:58.30ms
step:1672/1775 train_time:97511ms step_avg:58.32ms
step:1673/1775 train_time:97596ms step_avg:58.34ms
step:1674/1775 train_time:97682ms step_avg:58.35ms
step:1675/1775 train_time:97765ms step_avg:58.37ms
step:1676/1775 train_time:97853ms step_avg:58.38ms
step:1677/1775 train_time:97937ms step_avg:58.40ms
step:1678/1775 train_time:98023ms step_avg:58.42ms
step:1679/1775 train_time:98108ms step_avg:58.43ms
step:1680/1775 train_time:98196ms step_avg:58.45ms
step:1681/1775 train_time:98278ms step_avg:58.46ms
step:1682/1775 train_time:98366ms step_avg:58.48ms
step:1683/1775 train_time:98450ms step_avg:58.50ms
step:1684/1775 train_time:98538ms step_avg:58.51ms
step:1685/1775 train_time:98621ms step_avg:58.53ms
step:1686/1775 train_time:98709ms step_avg:58.55ms
step:1687/1775 train_time:98793ms step_avg:58.56ms
step:1688/1775 train_time:98879ms step_avg:58.58ms
step:1689/1775 train_time:98962ms step_avg:58.59ms
step:1690/1775 train_time:99049ms step_avg:58.61ms
step:1691/1775 train_time:99134ms step_avg:58.62ms
step:1692/1775 train_time:99219ms step_avg:58.64ms
step:1693/1775 train_time:99304ms step_avg:58.66ms
step:1694/1775 train_time:99392ms step_avg:58.67ms
step:1695/1775 train_time:99477ms step_avg:58.69ms
step:1696/1775 train_time:99563ms step_avg:58.70ms
step:1697/1775 train_time:99647ms step_avg:58.72ms
step:1698/1775 train_time:99735ms step_avg:58.74ms
step:1699/1775 train_time:99818ms step_avg:58.75ms
step:1700/1775 train_time:99905ms step_avg:58.77ms
step:1701/1775 train_time:99989ms step_avg:58.78ms
step:1702/1775 train_time:100076ms step_avg:58.80ms
step:1703/1775 train_time:100159ms step_avg:58.81ms
step:1704/1775 train_time:100246ms step_avg:58.83ms
step:1705/1775 train_time:100331ms step_avg:58.85ms
step:1706/1775 train_time:100418ms step_avg:58.86ms
step:1707/1775 train_time:100501ms step_avg:58.88ms
step:1708/1775 train_time:100588ms step_avg:58.89ms
step:1709/1775 train_time:100672ms step_avg:58.91ms
step:1710/1775 train_time:100760ms step_avg:58.92ms
step:1711/1775 train_time:100843ms step_avg:58.94ms
step:1712/1775 train_time:100930ms step_avg:58.95ms
step:1713/1775 train_time:101015ms step_avg:58.97ms
step:1714/1775 train_time:101100ms step_avg:58.99ms
step:1715/1775 train_time:101184ms step_avg:59.00ms
step:1716/1775 train_time:101272ms step_avg:59.02ms
step:1717/1775 train_time:101355ms step_avg:59.03ms
step:1718/1775 train_time:101441ms step_avg:59.05ms
step:1719/1775 train_time:101525ms step_avg:59.06ms
step:1720/1775 train_time:101613ms step_avg:59.08ms
step:1721/1775 train_time:101697ms step_avg:59.09ms
step:1722/1775 train_time:101784ms step_avg:59.11ms
step:1723/1775 train_time:101868ms step_avg:59.12ms
step:1724/1775 train_time:101955ms step_avg:59.14ms
step:1725/1775 train_time:102039ms step_avg:59.15ms
step:1726/1775 train_time:102128ms step_avg:59.17ms
step:1727/1775 train_time:102211ms step_avg:59.18ms
step:1728/1775 train_time:102298ms step_avg:59.20ms
step:1729/1775 train_time:102382ms step_avg:59.21ms
step:1730/1775 train_time:102469ms step_avg:59.23ms
step:1731/1775 train_time:102555ms step_avg:59.25ms
step:1732/1775 train_time:102640ms step_avg:59.26ms
step:1733/1775 train_time:102725ms step_avg:59.28ms
step:1734/1775 train_time:102812ms step_avg:59.29ms
step:1735/1775 train_time:102897ms step_avg:59.31ms
step:1736/1775 train_time:102986ms step_avg:59.32ms
step:1737/1775 train_time:103072ms step_avg:59.34ms
step:1738/1775 train_time:103159ms step_avg:59.36ms
step:1739/1775 train_time:103244ms step_avg:59.37ms
step:1740/1775 train_time:103330ms step_avg:59.39ms
step:1741/1775 train_time:103415ms step_avg:59.40ms
step:1742/1775 train_time:103502ms step_avg:59.42ms
step:1743/1775 train_time:103586ms step_avg:59.43ms
step:1744/1775 train_time:103674ms step_avg:59.45ms
step:1745/1775 train_time:103758ms step_avg:59.46ms
step:1746/1775 train_time:103845ms step_avg:59.48ms
step:1747/1775 train_time:103931ms step_avg:59.49ms
step:1748/1775 train_time:104018ms step_avg:59.51ms
step:1749/1775 train_time:104103ms step_avg:59.52ms
step:1750/1775 train_time:104191ms step_avg:59.54ms
step:1750/1775 val_loss:3.2854 train_time:104285ms step_avg:59.59ms
step:1751/1775 train_time:104310ms step_avg:59.57ms
step:1752/1775 train_time:104363ms step_avg:59.57ms
step:1753/1775 train_time:104449ms step_avg:59.58ms
step:1754/1775 train_time:104536ms step_avg:59.60ms
step:1755/1775 train_time:104621ms step_avg:59.61ms
step:1756/1775 train_time:104708ms step_avg:59.63ms
step:1757/1775 train_time:104792ms step_avg:59.64ms
step:1758/1775 train_time:104878ms step_avg:59.66ms
step:1759/1775 train_time:104961ms step_avg:59.67ms
step:1760/1775 train_time:105049ms step_avg:59.69ms
step:1761/1775 train_time:105133ms step_avg:59.70ms
step:1762/1775 train_time:105219ms step_avg:59.72ms
step:1763/1775 train_time:105305ms step_avg:59.73ms
step:1764/1775 train_time:105395ms step_avg:59.75ms
step:1765/1775 train_time:105481ms step_avg:59.76ms
step:1766/1775 train_time:105571ms step_avg:59.78ms
step:1767/1775 train_time:105654ms step_avg:59.79ms
step:1768/1775 train_time:105740ms step_avg:59.81ms
step:1769/1775 train_time:105825ms step_avg:59.82ms
step:1770/1775 train_time:105912ms step_avg:59.84ms
step:1771/1775 train_time:105995ms step_avg:59.85ms
step:1772/1775 train_time:106082ms step_avg:59.87ms
step:1773/1775 train_time:106167ms step_avg:59.88ms
step:1774/1775 train_time:106255ms step_avg:59.90ms
step:1775/1775 train_time:106339ms step_avg:59.91ms
step:1775/1775 val_loss:3.2790 train_time:106438ms step_avg:59.97ms
peak memory allocated: 29148 MiB reserved: 44818 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,  #
                                 M, N, K,  #
                                 BLOCK_SIZE_M: tl.constexpr,  #
                                 BLOCK_SIZE_N: tl.constexpr,  #
                                 BLOCK_SIZE_K: tl.constexpr,  #
                                 GROUP_SIZE_M: tl.constexpr,  #
                                 NUM_SMS: tl.constexpr,  #
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,#
        M, N, K,  #
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,  #
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Jan 11 04:18:58 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            8300      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    1   N/A  N/A            8301      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    2   N/A  N/A            8302      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    3   N/A  N/A            8303      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    4   N/A  N/A            8304      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    5   N/A  N/A            8305      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    6   N/A  N/A            8306      C   /home/ubuntu/venv/bin/python3          1512MiB |
|    7   N/A  N/A            8307      C   /home/ubuntu/venv/bin/python3          1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8327 train_time:0ms step_avg:0.04ms
step:1/1775 train_time:88ms step_avg:88.19ms
step:2/1775 train_time:115ms step_avg:57.53ms
step:3/1775 train_time:138ms step_avg:46.05ms
step:4/1775 train_time:162ms step_avg:40.62ms
step:5/1775 train_time:188ms step_avg:37.65ms
step:6/1775 train_time:441ms step_avg:73.57ms
step:7/1775 train_time:463ms step_avg:66.19ms
step:8/1775 train_time:487ms step_avg:60.83ms
step:9/1775 train_time:508ms step_avg:56.44ms
step:10/1775 train_time:540ms step_avg:54.02ms
step:11/1775 train_time:571ms step_avg:51.91ms
step:12/1775 train_time:604ms step_avg:50.35ms
step:13/1775 train_time:635ms step_avg:48.85ms
step:14/1775 train_time:669ms step_avg:47.77ms
step:15/1775 train_time:700ms step_avg:46.67ms
step:16/1775 train_time:733ms step_avg:45.82ms
step:17/1775 train_time:764ms step_avg:44.96ms
step:18/1775 train_time:798ms step_avg:44.32ms
step:19/1775 train_time:829ms step_avg:43.64ms
step:20/1775 train_time:863ms step_avg:43.14ms
step:21/1775 train_time:894ms step_avg:42.56ms
step:22/1775 train_time:927ms step_avg:42.14ms
step:23/1775 train_time:958ms step_avg:41.65ms
step:24/1775 train_time:991ms step_avg:41.31ms
step:25/1775 train_time:1022ms step_avg:40.89ms
step:26/1775 train_time:1056ms step_avg:40.61ms
step:27/1775 train_time:1087ms step_avg:40.26ms
step:28/1775 train_time:1121ms step_avg:40.02ms
step:29/1775 train_time:1152ms step_avg:39.71ms
step:30/1775 train_time:1185ms step_avg:39.50ms
step:31/1775 train_time:1216ms step_avg:39.22ms
step:32/1775 train_time:1249ms step_avg:39.04ms
step:33/1775 train_time:1281ms step_avg:38.81ms
step:34/1775 train_time:1315ms step_avg:38.67ms
step:35/1775 train_time:1347ms step_avg:38.48ms
step:36/1775 train_time:1381ms step_avg:38.37ms
step:37/1775 train_time:1414ms step_avg:38.21ms
step:38/1775 train_time:1449ms step_avg:38.12ms
step:39/1775 train_time:1480ms step_avg:37.96ms
step:40/1775 train_time:1514ms step_avg:37.85ms
step:41/1775 train_time:1546ms step_avg:37.70ms
step:42/1775 train_time:1579ms step_avg:37.61ms
step:43/1775 train_time:1611ms step_avg:37.47ms
step:44/1775 train_time:1645ms step_avg:37.39ms
step:45/1775 train_time:1677ms step_avg:37.26ms
step:46/1775 train_time:1710ms step_avg:37.18ms
step:47/1775 train_time:1742ms step_avg:37.05ms
step:48/1775 train_time:1775ms step_avg:36.98ms
step:49/1775 train_time:1806ms step_avg:36.87ms
step:50/1775 train_time:1840ms step_avg:36.80ms
step:51/1775 train_time:1871ms step_avg:36.68ms
step:52/1775 train_time:1904ms step_avg:36.62ms
step:53/1775 train_time:1935ms step_avg:36.51ms
step:54/1775 train_time:1969ms step_avg:36.46ms
step:55/1775 train_time:2000ms step_avg:36.36ms
step:56/1775 train_time:2033ms step_avg:36.30ms
step:57/1775 train_time:2064ms step_avg:36.21ms
step:58/1775 train_time:2097ms step_avg:36.16ms
step:59/1775 train_time:2128ms step_avg:36.08ms
step:60/1775 train_time:2162ms step_avg:36.03ms
step:61/1775 train_time:2193ms step_avg:35.95ms
step:62/1775 train_time:2226ms step_avg:35.90ms
step:63/1775 train_time:2257ms step_avg:35.83ms
step:64/1775 train_time:2291ms step_avg:35.80ms
step:65/1775 train_time:2323ms step_avg:35.74ms
step:66/1775 train_time:2357ms step_avg:35.72ms
step:67/1775 train_time:2389ms step_avg:35.65ms
step:68/1775 train_time:2422ms step_avg:35.62ms
step:69/1775 train_time:2454ms step_avg:35.56ms
step:70/1775 train_time:2488ms step_avg:35.54ms
step:71/1775 train_time:2520ms step_avg:35.49ms
step:72/1775 train_time:2553ms step_avg:35.46ms
step:73/1775 train_time:2585ms step_avg:35.41ms
step:74/1775 train_time:2619ms step_avg:35.39ms
step:75/1775 train_time:2650ms step_avg:35.34ms
step:76/1775 train_time:2684ms step_avg:35.32ms
step:77/1775 train_time:2715ms step_avg:35.27ms
step:78/1775 train_time:2750ms step_avg:35.25ms
step:79/1775 train_time:2781ms step_avg:35.20ms
step:80/1775 train_time:2814ms step_avg:35.18ms
step:81/1775 train_time:2845ms step_avg:35.13ms
step:82/1775 train_time:2878ms step_avg:35.10ms
step:83/1775 train_time:2910ms step_avg:35.06ms
step:84/1775 train_time:2944ms step_avg:35.04ms
step:85/1775 train_time:2975ms step_avg:35.00ms
step:86/1775 train_time:3009ms step_avg:34.99ms
step:87/1775 train_time:3040ms step_avg:34.94ms
step:88/1775 train_time:3073ms step_avg:34.92ms
step:89/1775 train_time:3105ms step_avg:34.88ms
step:90/1775 train_time:3138ms step_avg:34.87ms
step:91/1775 train_time:3169ms step_avg:34.83ms
step:92/1775 train_time:3204ms step_avg:34.82ms
step:93/1775 train_time:3235ms step_avg:34.78ms
step:94/1775 train_time:3268ms step_avg:34.77ms
step:95/1775 train_time:3300ms step_avg:34.74ms
step:96/1775 train_time:3334ms step_avg:34.73ms
step:97/1775 train_time:3365ms step_avg:34.69ms
step:98/1775 train_time:3399ms step_avg:34.68ms
step:99/1775 train_time:3430ms step_avg:34.65ms
step:100/1775 train_time:3465ms step_avg:34.65ms
step:101/1775 train_time:3497ms step_avg:34.62ms
step:102/1775 train_time:3530ms step_avg:34.61ms
step:103/1775 train_time:3562ms step_avg:34.58ms
step:104/1775 train_time:3596ms step_avg:34.57ms
step:105/1775 train_time:3627ms step_avg:34.54ms
step:106/1775 train_time:3660ms step_avg:34.53ms
step:107/1775 train_time:3692ms step_avg:34.50ms
step:108/1775 train_time:3726ms step_avg:34.50ms
step:109/1775 train_time:3757ms step_avg:34.46ms
step:110/1775 train_time:3790ms step_avg:34.46ms
step:111/1775 train_time:3822ms step_avg:34.43ms
step:112/1775 train_time:3855ms step_avg:34.42ms
step:113/1775 train_time:3886ms step_avg:34.39ms
step:114/1775 train_time:3920ms step_avg:34.39ms
step:115/1775 train_time:3951ms step_avg:34.36ms
step:116/1775 train_time:3985ms step_avg:34.35ms
step:117/1775 train_time:4016ms step_avg:34.32ms
step:118/1775 train_time:4049ms step_avg:34.32ms
step:119/1775 train_time:4081ms step_avg:34.29ms
step:120/1775 train_time:4114ms step_avg:34.28ms
step:121/1775 train_time:4145ms step_avg:34.26ms
step:122/1775 train_time:4179ms step_avg:34.25ms
step:123/1775 train_time:4210ms step_avg:34.23ms
step:124/1775 train_time:4244ms step_avg:34.22ms
step:125/1775 train_time:4275ms step_avg:34.20ms
step:126/1775 train_time:4308ms step_avg:34.19ms
step:127/1775 train_time:4340ms step_avg:34.17ms
step:128/1775 train_time:4373ms step_avg:34.17ms
step:129/1775 train_time:4405ms step_avg:34.14ms
step:130/1775 train_time:4438ms step_avg:34.14ms
step:131/1775 train_time:4470ms step_avg:34.12ms
step:132/1775 train_time:4504ms step_avg:34.12ms
step:133/1775 train_time:4535ms step_avg:34.10ms
step:134/1775 train_time:4569ms step_avg:34.09ms
step:135/1775 train_time:4600ms step_avg:34.08ms
step:136/1775 train_time:4634ms step_avg:34.07ms
step:137/1775 train_time:4666ms step_avg:34.06ms
step:138/1775 train_time:4699ms step_avg:34.05ms
step:139/1775 train_time:4730ms step_avg:34.03ms
step:140/1775 train_time:4765ms step_avg:34.03ms
step:141/1775 train_time:4796ms step_avg:34.01ms
step:142/1775 train_time:4829ms step_avg:34.01ms
step:143/1775 train_time:4861ms step_avg:33.99ms
step:144/1775 train_time:4894ms step_avg:33.99ms
step:145/1775 train_time:4925ms step_avg:33.97ms
step:146/1775 train_time:4959ms step_avg:33.96ms
step:147/1775 train_time:4990ms step_avg:33.94ms
step:148/1775 train_time:5024ms step_avg:33.94ms
step:149/1775 train_time:5055ms step_avg:33.93ms
step:150/1775 train_time:5089ms step_avg:33.92ms
step:151/1775 train_time:5120ms step_avg:33.91ms
step:152/1775 train_time:5153ms step_avg:33.90ms
step:153/1775 train_time:5184ms step_avg:33.89ms
step:154/1775 train_time:5218ms step_avg:33.88ms
step:155/1775 train_time:5249ms step_avg:33.86ms
step:156/1775 train_time:5283ms step_avg:33.86ms
step:157/1775 train_time:5315ms step_avg:33.85ms
step:158/1775 train_time:5348ms step_avg:33.85ms
step:159/1775 train_time:5379ms step_avg:33.83ms
step:160/1775 train_time:5413ms step_avg:33.83ms
step:161/1775 train_time:5444ms step_avg:33.82ms
step:162/1775 train_time:5479ms step_avg:33.82ms
step:163/1775 train_time:5510ms step_avg:33.81ms
step:164/1775 train_time:5544ms step_avg:33.80ms
step:165/1775 train_time:5575ms step_avg:33.79ms
step:166/1775 train_time:5609ms step_avg:33.79ms
step:167/1775 train_time:5640ms step_avg:33.77ms
step:168/1775 train_time:5674ms step_avg:33.77ms
step:169/1775 train_time:5706ms step_avg:33.76ms
step:170/1775 train_time:5739ms step_avg:33.76ms
step:171/1775 train_time:5770ms step_avg:33.74ms
step:172/1775 train_time:5804ms step_avg:33.74ms
step:173/1775 train_time:5835ms step_avg:33.73ms
step:174/1775 train_time:5869ms step_avg:33.73ms
step:175/1775 train_time:5900ms step_avg:33.71ms
step:176/1775 train_time:5934ms step_avg:33.72ms
step:177/1775 train_time:5965ms step_avg:33.70ms
step:178/1775 train_time:5998ms step_avg:33.70ms
step:179/1775 train_time:6029ms step_avg:33.68ms
step:180/1775 train_time:6063ms step_avg:33.68ms
step:181/1775 train_time:6094ms step_avg:33.67ms
step:182/1775 train_time:6128ms step_avg:33.67ms
step:183/1775 train_time:6160ms step_avg:33.66ms
step:184/1775 train_time:6193ms step_avg:33.66ms
step:185/1775 train_time:6224ms step_avg:33.64ms
step:186/1775 train_time:6257ms step_avg:33.64ms
step:187/1775 train_time:6289ms step_avg:33.63ms
step:188/1775 train_time:6322ms step_avg:33.63ms
step:189/1775 train_time:6353ms step_avg:33.62ms
step:190/1775 train_time:6387ms step_avg:33.62ms
step:191/1775 train_time:6418ms step_avg:33.60ms
step:192/1775 train_time:6452ms step_avg:33.60ms
step:193/1775 train_time:6483ms step_avg:33.59ms
step:194/1775 train_time:6517ms step_avg:33.59ms
step:195/1775 train_time:6548ms step_avg:33.58ms
step:196/1775 train_time:6582ms step_avg:33.58ms
step:197/1775 train_time:6613ms step_avg:33.57ms
step:198/1775 train_time:6646ms step_avg:33.57ms
step:199/1775 train_time:6678ms step_avg:33.56ms
step:200/1775 train_time:6712ms step_avg:33.56ms
step:201/1775 train_time:6743ms step_avg:33.55ms
step:202/1775 train_time:6777ms step_avg:33.55ms
step:203/1775 train_time:6808ms step_avg:33.54ms
step:204/1775 train_time:6842ms step_avg:33.54ms
step:205/1775 train_time:6873ms step_avg:33.53ms
step:206/1775 train_time:6907ms step_avg:33.53ms
step:207/1775 train_time:6938ms step_avg:33.52ms
step:208/1775 train_time:6973ms step_avg:33.52ms
step:209/1775 train_time:7004ms step_avg:33.51ms
step:210/1775 train_time:7037ms step_avg:33.51ms
step:211/1775 train_time:7069ms step_avg:33.50ms
step:212/1775 train_time:7103ms step_avg:33.50ms
step:213/1775 train_time:7134ms step_avg:33.49ms
step:214/1775 train_time:7167ms step_avg:33.49ms
step:215/1775 train_time:7199ms step_avg:33.48ms
step:216/1775 train_time:7232ms step_avg:33.48ms
step:217/1775 train_time:7263ms step_avg:33.47ms
step:218/1775 train_time:7297ms step_avg:33.47ms
step:219/1775 train_time:7328ms step_avg:33.46ms
step:220/1775 train_time:7362ms step_avg:33.46ms
step:221/1775 train_time:7393ms step_avg:33.45ms
step:222/1775 train_time:7426ms step_avg:33.45ms
step:223/1775 train_time:7457ms step_avg:33.44ms
step:224/1775 train_time:7491ms step_avg:33.44ms
step:225/1775 train_time:7523ms step_avg:33.43ms
step:226/1775 train_time:7556ms step_avg:33.43ms
step:227/1775 train_time:7588ms step_avg:33.43ms
step:228/1775 train_time:7622ms step_avg:33.43ms
step:229/1775 train_time:7653ms step_avg:33.42ms
step:230/1775 train_time:7686ms step_avg:33.42ms
step:231/1775 train_time:7718ms step_avg:33.41ms
step:232/1775 train_time:7752ms step_avg:33.41ms
step:233/1775 train_time:7783ms step_avg:33.41ms
step:234/1775 train_time:7817ms step_avg:33.41ms
step:235/1775 train_time:7848ms step_avg:33.40ms
step:236/1775 train_time:7882ms step_avg:33.40ms
step:237/1775 train_time:7913ms step_avg:33.39ms
step:238/1775 train_time:7946ms step_avg:33.39ms
step:239/1775 train_time:7978ms step_avg:33.38ms
step:240/1775 train_time:8012ms step_avg:33.38ms
step:241/1775 train_time:8044ms step_avg:33.38ms
step:242/1775 train_time:8077ms step_avg:33.38ms
step:243/1775 train_time:8108ms step_avg:33.37ms
step:244/1775 train_time:8141ms step_avg:33.37ms
step:245/1775 train_time:8173ms step_avg:33.36ms
step:246/1775 train_time:8206ms step_avg:33.36ms
step:247/1775 train_time:8237ms step_avg:33.35ms
step:248/1775 train_time:8271ms step_avg:33.35ms
step:249/1775 train_time:8303ms step_avg:33.34ms
step:250/1775 train_time:8336ms step_avg:33.34ms
step:250/1775 val_loss:4.5939 train_time:8375ms step_avg:33.50ms
step:251/1775 train_time:8399ms step_avg:33.46ms
step:252/1775 train_time:8422ms step_avg:33.42ms
step:253/1775 train_time:8443ms step_avg:33.37ms
step:254/1775 train_time:8470ms step_avg:33.34ms
step:255/1775 train_time:8503ms step_avg:33.34ms
step:256/1775 train_time:8538ms step_avg:33.35ms
step:257/1775 train_time:8570ms step_avg:33.35ms
step:258/1775 train_time:8604ms step_avg:33.35ms
step:259/1775 train_time:8635ms step_avg:33.34ms
step:260/1775 train_time:8669ms step_avg:33.34ms
step:261/1775 train_time:8701ms step_avg:33.34ms
step:262/1775 train_time:8734ms step_avg:33.34ms
step:263/1775 train_time:8765ms step_avg:33.33ms
step:264/1775 train_time:8798ms step_avg:33.33ms
step:265/1775 train_time:8830ms step_avg:33.32ms
step:266/1775 train_time:8863ms step_avg:33.32ms
step:267/1775 train_time:8894ms step_avg:33.31ms
step:268/1775 train_time:8928ms step_avg:33.31ms
step:269/1775 train_time:8959ms step_avg:33.30ms
step:270/1775 train_time:8992ms step_avg:33.30ms
step:271/1775 train_time:9023ms step_avg:33.30ms
step:272/1775 train_time:9056ms step_avg:33.29ms
step:273/1775 train_time:9088ms step_avg:33.29ms
step:274/1775 train_time:9121ms step_avg:33.29ms
step:275/1775 train_time:9152ms step_avg:33.28ms
step:276/1775 train_time:9185ms step_avg:33.28ms
step:277/1775 train_time:9216ms step_avg:33.27ms
step:278/1775 train_time:9250ms step_avg:33.27ms
step:279/1775 train_time:9281ms step_avg:33.26ms
step:280/1775 train_time:9315ms step_avg:33.27ms
step:281/1775 train_time:9347ms step_avg:33.26ms
step:282/1775 train_time:9381ms step_avg:33.27ms
step:283/1775 train_time:9413ms step_avg:33.26ms
step:284/1775 train_time:9447ms step_avg:33.26ms
step:285/1775 train_time:9479ms step_avg:33.26ms
step:286/1775 train_time:9513ms step_avg:33.26ms
step:287/1775 train_time:9544ms step_avg:33.26ms
step:288/1775 train_time:9578ms step_avg:33.26ms
step:289/1775 train_time:9610ms step_avg:33.25ms
step:290/1775 train_time:9643ms step_avg:33.25ms
step:291/1775 train_time:9674ms step_avg:33.25ms
step:292/1775 train_time:9708ms step_avg:33.25ms
step:293/1775 train_time:9739ms step_avg:33.24ms
step:294/1775 train_time:9773ms step_avg:33.24ms
step:295/1775 train_time:9804ms step_avg:33.23ms
step:296/1775 train_time:9838ms step_avg:33.24ms
step:297/1775 train_time:9869ms step_avg:33.23ms
step:298/1775 train_time:9902ms step_avg:33.23ms
step:299/1775 train_time:9933ms step_avg:33.22ms
step:300/1775 train_time:9967ms step_avg:33.22ms
step:301/1775 train_time:9998ms step_avg:33.22ms
step:302/1775 train_time:10032ms step_avg:33.22ms
step:303/1775 train_time:10063ms step_avg:33.21ms
step:304/1775 train_time:10096ms step_avg:33.21ms
step:305/1775 train_time:10128ms step_avg:33.21ms
step:306/1775 train_time:10161ms step_avg:33.21ms
step:307/1775 train_time:10192ms step_avg:33.20ms
step:308/1775 train_time:10225ms step_avg:33.20ms
step:309/1775 train_time:10257ms step_avg:33.19ms
step:310/1775 train_time:10290ms step_avg:33.19ms
step:311/1775 train_time:10322ms step_avg:33.19ms
step:312/1775 train_time:10356ms step_avg:33.19ms
step:313/1775 train_time:10387ms step_avg:33.19ms
step:314/1775 train_time:10422ms step_avg:33.19ms
step:315/1775 train_time:10454ms step_avg:33.19ms
step:316/1775 train_time:10488ms step_avg:33.19ms
step:317/1775 train_time:10519ms step_avg:33.18ms
step:318/1775 train_time:10553ms step_avg:33.19ms
step:319/1775 train_time:10585ms step_avg:33.18ms
step:320/1775 train_time:10619ms step_avg:33.18ms
step:321/1775 train_time:10650ms step_avg:33.18ms
step:322/1775 train_time:10683ms step_avg:33.18ms
step:323/1775 train_time:10714ms step_avg:33.17ms
step:324/1775 train_time:10748ms step_avg:33.17ms
step:325/1775 train_time:10779ms step_avg:33.17ms
step:326/1775 train_time:10813ms step_avg:33.17ms
step:327/1775 train_time:10845ms step_avg:33.16ms
step:328/1775 train_time:10878ms step_avg:33.17ms
step:329/1775 train_time:10910ms step_avg:33.16ms
step:330/1775 train_time:10944ms step_avg:33.16ms
step:331/1775 train_time:10974ms step_avg:33.16ms
step:332/1775 train_time:11008ms step_avg:33.16ms
step:333/1775 train_time:11039ms step_avg:33.15ms
step:334/1775 train_time:11073ms step_avg:33.15ms
step:335/1775 train_time:11104ms step_avg:33.15ms
step:336/1775 train_time:11137ms step_avg:33.15ms
step:337/1775 train_time:11169ms step_avg:33.14ms
step:338/1775 train_time:11202ms step_avg:33.14ms
step:339/1775 train_time:11233ms step_avg:33.14ms
step:340/1775 train_time:11267ms step_avg:33.14ms
step:341/1775 train_time:11298ms step_avg:33.13ms
step:342/1775 train_time:11331ms step_avg:33.13ms
step:343/1775 train_time:11363ms step_avg:33.13ms
step:344/1775 train_time:11397ms step_avg:33.13ms
step:345/1775 train_time:11429ms step_avg:33.13ms
step:346/1775 train_time:11462ms step_avg:33.13ms
step:347/1775 train_time:11494ms step_avg:33.12ms
step:348/1775 train_time:11528ms step_avg:33.13ms
step:349/1775 train_time:11560ms step_avg:33.12ms
step:350/1775 train_time:11594ms step_avg:33.13ms
step:351/1775 train_time:11625ms step_avg:33.12ms
step:352/1775 train_time:11659ms step_avg:33.12ms
step:353/1775 train_time:11691ms step_avg:33.12ms
step:354/1775 train_time:11724ms step_avg:33.12ms
step:355/1775 train_time:11756ms step_avg:33.11ms
step:356/1775 train_time:11790ms step_avg:33.12ms
step:357/1775 train_time:11821ms step_avg:33.11ms
step:358/1775 train_time:11855ms step_avg:33.11ms
step:359/1775 train_time:11886ms step_avg:33.11ms
step:360/1775 train_time:11919ms step_avg:33.11ms
step:361/1775 train_time:11951ms step_avg:33.11ms
step:362/1775 train_time:11984ms step_avg:33.11ms
step:363/1775 train_time:12016ms step_avg:33.10ms
step:364/1775 train_time:12049ms step_avg:33.10ms
step:365/1775 train_time:12080ms step_avg:33.10ms
step:366/1775 train_time:12114ms step_avg:33.10ms
step:367/1775 train_time:12145ms step_avg:33.09ms
step:368/1775 train_time:12178ms step_avg:33.09ms
step:369/1775 train_time:12210ms step_avg:33.09ms
step:370/1775 train_time:12243ms step_avg:33.09ms
step:371/1775 train_time:12275ms step_avg:33.09ms
step:372/1775 train_time:12308ms step_avg:33.09ms
step:373/1775 train_time:12340ms step_avg:33.08ms
step:374/1775 train_time:12373ms step_avg:33.08ms
step:375/1775 train_time:12405ms step_avg:33.08ms
step:376/1775 train_time:12439ms step_avg:33.08ms
step:377/1775 train_time:12470ms step_avg:33.08ms
step:378/1775 train_time:12504ms step_avg:33.08ms
step:379/1775 train_time:12536ms step_avg:33.08ms
step:380/1775 train_time:12569ms step_avg:33.08ms
step:381/1775 train_time:12601ms step_avg:33.07ms
step:382/1775 train_time:12634ms step_avg:33.07ms
step:383/1775 train_time:12666ms step_avg:33.07ms
step:384/1775 train_time:12699ms step_avg:33.07ms
step:385/1775 train_time:12730ms step_avg:33.07ms
step:386/1775 train_time:12764ms step_avg:33.07ms
step:387/1775 train_time:12796ms step_avg:33.06ms
step:388/1775 train_time:12829ms step_avg:33.07ms
step:389/1775 train_time:12860ms step_avg:33.06ms
step:390/1775 train_time:12894ms step_avg:33.06ms
step:391/1775 train_time:12926ms step_avg:33.06ms
step:392/1775 train_time:12959ms step_avg:33.06ms
step:393/1775 train_time:12991ms step_avg:33.05ms
step:394/1775 train_time:13024ms step_avg:33.06ms
step:395/1775 train_time:13055ms step_avg:33.05ms
step:396/1775 train_time:13089ms step_avg:33.05ms
step:397/1775 train_time:13120ms step_avg:33.05ms
step:398/1775 train_time:13154ms step_avg:33.05ms
step:399/1775 train_time:13186ms step_avg:33.05ms
step:400/1775 train_time:13219ms step_avg:33.05ms
step:401/1775 train_time:13250ms step_avg:33.04ms
step:402/1775 train_time:13284ms step_avg:33.04ms
step:403/1775 train_time:13315ms step_avg:33.04ms
step:404/1775 train_time:13349ms step_avg:33.04ms
step:405/1775 train_time:13380ms step_avg:33.04ms
step:406/1775 train_time:13415ms step_avg:33.04ms
step:407/1775 train_time:13447ms step_avg:33.04ms
step:408/1775 train_time:13480ms step_avg:33.04ms
step:409/1775 train_time:13512ms step_avg:33.04ms
step:410/1775 train_time:13545ms step_avg:33.04ms
step:411/1775 train_time:13577ms step_avg:33.03ms
step:412/1775 train_time:13612ms step_avg:33.04ms
step:413/1775 train_time:13643ms step_avg:33.03ms
step:414/1775 train_time:13676ms step_avg:33.03ms
step:415/1775 train_time:13708ms step_avg:33.03ms
step:416/1775 train_time:13741ms step_avg:33.03ms
step:417/1775 train_time:13772ms step_avg:33.03ms
step:418/1775 train_time:13806ms step_avg:33.03ms
step:419/1775 train_time:13837ms step_avg:33.02ms
step:420/1775 train_time:13871ms step_avg:33.03ms
step:421/1775 train_time:13902ms step_avg:33.02ms
step:422/1775 train_time:13936ms step_avg:33.02ms
step:423/1775 train_time:13967ms step_avg:33.02ms
step:424/1775 train_time:14001ms step_avg:33.02ms
step:425/1775 train_time:14032ms step_avg:33.02ms
step:426/1775 train_time:14066ms step_avg:33.02ms
step:427/1775 train_time:14098ms step_avg:33.02ms
step:428/1775 train_time:14131ms step_avg:33.02ms
step:429/1775 train_time:14162ms step_avg:33.01ms
step:430/1775 train_time:14196ms step_avg:33.01ms
step:431/1775 train_time:14228ms step_avg:33.01ms
step:432/1775 train_time:14261ms step_avg:33.01ms
step:433/1775 train_time:14292ms step_avg:33.01ms
step:434/1775 train_time:14326ms step_avg:33.01ms
step:435/1775 train_time:14357ms step_avg:33.00ms
step:436/1775 train_time:14391ms step_avg:33.01ms
step:437/1775 train_time:14423ms step_avg:33.00ms
step:438/1775 train_time:14456ms step_avg:33.00ms
step:439/1775 train_time:14488ms step_avg:33.00ms
step:440/1775 train_time:14522ms step_avg:33.00ms
step:441/1775 train_time:14554ms step_avg:33.00ms
step:442/1775 train_time:14587ms step_avg:33.00ms
step:443/1775 train_time:14619ms step_avg:33.00ms
step:444/1775 train_time:14652ms step_avg:33.00ms
step:445/1775 train_time:14683ms step_avg:33.00ms
step:446/1775 train_time:14717ms step_avg:33.00ms
step:447/1775 train_time:14748ms step_avg:32.99ms
step:448/1775 train_time:14782ms step_avg:32.99ms
step:449/1775 train_time:14813ms step_avg:32.99ms
step:450/1775 train_time:14846ms step_avg:32.99ms
step:451/1775 train_time:14877ms step_avg:32.99ms
step:452/1775 train_time:14911ms step_avg:32.99ms
step:453/1775 train_time:14942ms step_avg:32.99ms
step:454/1775 train_time:14976ms step_avg:32.99ms
step:455/1775 train_time:15008ms step_avg:32.98ms
step:456/1775 train_time:15041ms step_avg:32.99ms
step:457/1775 train_time:15072ms step_avg:32.98ms
step:458/1775 train_time:15106ms step_avg:32.98ms
step:459/1775 train_time:15137ms step_avg:32.98ms
step:460/1775 train_time:15172ms step_avg:32.98ms
step:461/1775 train_time:15203ms step_avg:32.98ms
step:462/1775 train_time:15236ms step_avg:32.98ms
step:463/1775 train_time:15268ms step_avg:32.98ms
step:464/1775 train_time:15302ms step_avg:32.98ms
step:465/1775 train_time:15333ms step_avg:32.97ms
step:466/1775 train_time:15366ms step_avg:32.97ms
step:467/1775 train_time:15398ms step_avg:32.97ms
step:468/1775 train_time:15431ms step_avg:32.97ms
step:469/1775 train_time:15463ms step_avg:32.97ms
step:470/1775 train_time:15496ms step_avg:32.97ms
step:471/1775 train_time:15528ms step_avg:32.97ms
step:472/1775 train_time:15562ms step_avg:32.97ms
step:473/1775 train_time:15594ms step_avg:32.97ms
step:474/1775 train_time:15627ms step_avg:32.97ms
step:475/1775 train_time:15658ms step_avg:32.97ms
step:476/1775 train_time:15692ms step_avg:32.97ms
step:477/1775 train_time:15723ms step_avg:32.96ms
step:478/1775 train_time:15757ms step_avg:32.96ms
step:479/1775 train_time:15788ms step_avg:32.96ms
step:480/1775 train_time:15822ms step_avg:32.96ms
step:481/1775 train_time:15853ms step_avg:32.96ms
step:482/1775 train_time:15886ms step_avg:32.96ms
step:483/1775 train_time:15917ms step_avg:32.96ms
step:484/1775 train_time:15952ms step_avg:32.96ms
step:485/1775 train_time:15983ms step_avg:32.95ms
step:486/1775 train_time:16016ms step_avg:32.96ms
step:487/1775 train_time:16047ms step_avg:32.95ms
step:488/1775 train_time:16081ms step_avg:32.95ms
step:489/1775 train_time:16112ms step_avg:32.95ms
step:490/1775 train_time:16146ms step_avg:32.95ms
step:491/1775 train_time:16177ms step_avg:32.95ms
step:492/1775 train_time:16211ms step_avg:32.95ms
step:493/1775 train_time:16243ms step_avg:32.95ms
step:494/1775 train_time:16276ms step_avg:32.95ms
step:495/1775 train_time:16308ms step_avg:32.94ms
step:496/1775 train_time:16341ms step_avg:32.95ms
step:497/1775 train_time:16372ms step_avg:32.94ms
step:498/1775 train_time:16406ms step_avg:32.94ms
step:499/1775 train_time:16437ms step_avg:32.94ms
step:500/1775 train_time:16471ms step_avg:32.94ms
step:500/1775 val_loss:4.2785 train_time:16511ms step_avg:33.02ms
step:501/1775 train_time:16534ms step_avg:33.00ms
step:502/1775 train_time:16558ms step_avg:32.98ms
step:503/1775 train_time:16579ms step_avg:32.96ms
step:504/1775 train_time:16605ms step_avg:32.95ms
step:505/1775 train_time:16638ms step_avg:32.95ms
step:506/1775 train_time:16675ms step_avg:32.95ms
step:507/1775 train_time:16708ms step_avg:32.96ms
step:508/1775 train_time:16742ms step_avg:32.96ms
step:509/1775 train_time:16773ms step_avg:32.95ms
step:510/1775 train_time:16807ms step_avg:32.96ms
step:511/1775 train_time:16838ms step_avg:32.95ms
step:512/1775 train_time:16871ms step_avg:32.95ms
step:513/1775 train_time:16902ms step_avg:32.95ms
step:514/1775 train_time:16936ms step_avg:32.95ms
step:515/1775 train_time:16967ms step_avg:32.95ms
step:516/1775 train_time:17000ms step_avg:32.95ms
step:517/1775 train_time:17031ms step_avg:32.94ms
step:518/1775 train_time:17064ms step_avg:32.94ms
step:519/1775 train_time:17095ms step_avg:32.94ms
step:520/1775 train_time:17128ms step_avg:32.94ms
step:521/1775 train_time:17159ms step_avg:32.94ms
step:522/1775 train_time:17192ms step_avg:32.94ms
step:523/1775 train_time:17223ms step_avg:32.93ms
step:524/1775 train_time:17256ms step_avg:32.93ms
step:525/1775 train_time:17288ms step_avg:32.93ms
step:526/1775 train_time:17321ms step_avg:32.93ms
step:527/1775 train_time:17352ms step_avg:32.93ms
step:528/1775 train_time:17385ms step_avg:32.93ms
step:529/1775 train_time:17416ms step_avg:32.92ms
step:530/1775 train_time:17451ms step_avg:32.93ms
step:531/1775 train_time:17483ms step_avg:32.92ms
step:532/1775 train_time:17517ms step_avg:32.93ms
step:533/1775 train_time:17549ms step_avg:32.92ms
step:534/1775 train_time:17584ms step_avg:32.93ms
step:535/1775 train_time:17616ms step_avg:32.93ms
step:536/1775 train_time:17649ms step_avg:32.93ms
step:537/1775 train_time:17681ms step_avg:32.93ms
step:538/1775 train_time:17714ms step_avg:32.93ms
step:539/1775 train_time:17746ms step_avg:32.92ms
step:540/1775 train_time:17780ms step_avg:32.93ms
step:541/1775 train_time:17811ms step_avg:32.92ms
step:542/1775 train_time:17845ms step_avg:32.92ms
step:543/1775 train_time:17876ms step_avg:32.92ms
step:544/1775 train_time:17910ms step_avg:32.92ms
step:545/1775 train_time:17941ms step_avg:32.92ms
step:546/1775 train_time:17974ms step_avg:32.92ms
step:547/1775 train_time:18005ms step_avg:32.92ms
step:548/1775 train_time:18039ms step_avg:32.92ms
step:549/1775 train_time:18070ms step_avg:32.92ms
step:550/1775 train_time:18104ms step_avg:32.92ms
step:551/1775 train_time:18135ms step_avg:32.91ms
step:552/1775 train_time:18168ms step_avg:32.91ms
step:553/1775 train_time:18199ms step_avg:32.91ms
step:554/1775 train_time:18232ms step_avg:32.91ms
step:555/1775 train_time:18263ms step_avg:32.91ms
step:556/1775 train_time:18296ms step_avg:32.91ms
step:557/1775 train_time:18327ms step_avg:32.90ms
step:558/1775 train_time:18362ms step_avg:32.91ms
step:559/1775 train_time:18393ms step_avg:32.90ms
step:560/1775 train_time:18426ms step_avg:32.90ms
step:561/1775 train_time:18457ms step_avg:32.90ms
step:562/1775 train_time:18491ms step_avg:32.90ms
step:563/1775 train_time:18523ms step_avg:32.90ms
step:564/1775 train_time:18556ms step_avg:32.90ms
step:565/1775 train_time:18588ms step_avg:32.90ms
step:566/1775 train_time:18622ms step_avg:32.90ms
step:567/1775 train_time:18653ms step_avg:32.90ms
step:568/1775 train_time:18687ms step_avg:32.90ms
step:569/1775 train_time:18718ms step_avg:32.90ms
step:570/1775 train_time:18752ms step_avg:32.90ms
step:571/1775 train_time:18783ms step_avg:32.90ms
step:572/1775 train_time:18817ms step_avg:32.90ms
step:573/1775 train_time:18849ms step_avg:32.89ms
step:574/1775 train_time:18882ms step_avg:32.90ms
step:575/1775 train_time:18913ms step_avg:32.89ms
step:576/1775 train_time:18947ms step_avg:32.89ms
step:577/1775 train_time:18978ms step_avg:32.89ms
step:578/1775 train_time:19012ms step_avg:32.89ms
step:579/1775 train_time:19043ms step_avg:32.89ms
step:580/1775 train_time:19078ms step_avg:32.89ms
step:581/1775 train_time:19136ms step_avg:32.94ms
step:582/1775 train_time:19196ms step_avg:32.98ms
step:583/1775 train_time:19253ms step_avg:33.02ms
step:584/1775 train_time:19313ms step_avg:33.07ms
step:585/1775 train_time:19372ms step_avg:33.11ms
step:586/1775 train_time:19433ms step_avg:33.16ms
step:587/1775 train_time:19491ms step_avg:33.21ms
step:588/1775 train_time:19552ms step_avg:33.25ms
step:589/1775 train_time:19611ms step_avg:33.30ms
step:590/1775 train_time:19672ms step_avg:33.34ms
step:591/1775 train_time:19731ms step_avg:33.39ms
step:592/1775 train_time:19792ms step_avg:33.43ms
step:593/1775 train_time:19851ms step_avg:33.48ms
step:594/1775 train_time:19912ms step_avg:33.52ms
step:595/1775 train_time:19971ms step_avg:33.57ms
step:596/1775 train_time:20031ms step_avg:33.61ms
step:597/1775 train_time:20090ms step_avg:33.65ms
step:598/1775 train_time:20151ms step_avg:33.70ms
step:599/1775 train_time:20209ms step_avg:33.74ms
step:600/1775 train_time:20270ms step_avg:33.78ms
step:601/1775 train_time:20329ms step_avg:33.82ms
step:602/1775 train_time:20390ms step_avg:33.87ms
step:603/1775 train_time:20448ms step_avg:33.91ms
step:604/1775 train_time:20510ms step_avg:33.96ms
step:605/1775 train_time:20568ms step_avg:34.00ms
step:606/1775 train_time:20630ms step_avg:34.04ms
step:607/1775 train_time:20689ms step_avg:34.08ms
step:608/1775 train_time:20749ms step_avg:34.13ms
step:609/1775 train_time:20808ms step_avg:34.17ms
step:610/1775 train_time:20869ms step_avg:34.21ms
step:611/1775 train_time:20928ms step_avg:34.25ms
step:612/1775 train_time:20988ms step_avg:34.29ms
step:613/1775 train_time:21046ms step_avg:34.33ms
step:614/1775 train_time:21108ms step_avg:34.38ms
step:615/1775 train_time:21166ms step_avg:34.42ms
step:616/1775 train_time:21227ms step_avg:34.46ms
step:617/1775 train_time:21285ms step_avg:34.50ms
step:618/1775 train_time:21346ms step_avg:34.54ms
step:619/1775 train_time:21405ms step_avg:34.58ms
step:620/1775 train_time:21464ms step_avg:34.62ms
step:621/1775 train_time:21523ms step_avg:34.66ms
step:622/1775 train_time:21585ms step_avg:34.70ms
step:623/1775 train_time:21643ms step_avg:34.74ms
step:624/1775 train_time:21705ms step_avg:34.78ms
step:625/1775 train_time:21764ms step_avg:34.82ms
step:626/1775 train_time:21825ms step_avg:34.86ms
step:627/1775 train_time:21883ms step_avg:34.90ms
step:628/1775 train_time:21944ms step_avg:34.94ms
step:629/1775 train_time:22003ms step_avg:34.98ms
step:630/1775 train_time:22063ms step_avg:35.02ms
step:631/1775 train_time:22121ms step_avg:35.06ms
step:632/1775 train_time:22181ms step_avg:35.10ms
step:633/1775 train_time:22240ms step_avg:35.13ms
step:634/1775 train_time:22300ms step_avg:35.17ms
step:635/1775 train_time:22357ms step_avg:35.21ms
step:636/1775 train_time:22418ms step_avg:35.25ms
step:637/1775 train_time:22477ms step_avg:35.29ms
step:638/1775 train_time:22537ms step_avg:35.33ms
step:639/1775 train_time:22596ms step_avg:35.36ms
step:640/1775 train_time:22656ms step_avg:35.40ms
step:641/1775 train_time:22714ms step_avg:35.44ms
step:642/1775 train_time:22776ms step_avg:35.48ms
step:643/1775 train_time:22834ms step_avg:35.51ms
step:644/1775 train_time:22895ms step_avg:35.55ms
step:645/1775 train_time:22954ms step_avg:35.59ms
step:646/1775 train_time:23015ms step_avg:35.63ms
step:647/1775 train_time:23074ms step_avg:35.66ms
step:648/1775 train_time:23134ms step_avg:35.70ms
step:649/1775 train_time:23192ms step_avg:35.73ms
step:650/1775 train_time:23252ms step_avg:35.77ms
step:651/1775 train_time:23311ms step_avg:35.81ms
step:652/1775 train_time:23372ms step_avg:35.85ms
step:653/1775 train_time:23431ms step_avg:35.88ms
step:654/1775 train_time:23491ms step_avg:35.92ms
step:655/1775 train_time:23549ms step_avg:35.95ms
step:656/1775 train_time:23610ms step_avg:35.99ms
step:657/1775 train_time:23669ms step_avg:36.03ms
step:658/1775 train_time:23730ms step_avg:36.06ms
step:659/1775 train_time:23788ms step_avg:36.10ms
step:660/1775 train_time:23849ms step_avg:36.13ms
step:661/1775 train_time:23908ms step_avg:36.17ms
step:662/1775 train_time:23969ms step_avg:36.21ms
step:663/1775 train_time:24027ms step_avg:36.24ms
step:664/1775 train_time:24087ms step_avg:36.28ms
step:665/1775 train_time:24147ms step_avg:36.31ms
step:666/1775 train_time:24206ms step_avg:36.35ms
step:667/1775 train_time:24264ms step_avg:36.38ms
step:668/1775 train_time:24325ms step_avg:36.42ms
step:669/1775 train_time:24384ms step_avg:36.45ms
step:670/1775 train_time:24445ms step_avg:36.49ms
step:671/1775 train_time:24503ms step_avg:36.52ms
step:672/1775 train_time:24563ms step_avg:36.55ms
step:673/1775 train_time:24623ms step_avg:36.59ms
step:674/1775 train_time:24683ms step_avg:36.62ms
step:675/1775 train_time:24742ms step_avg:36.65ms
step:676/1775 train_time:24803ms step_avg:36.69ms
step:677/1775 train_time:24861ms step_avg:36.72ms
step:678/1775 train_time:24923ms step_avg:36.76ms
step:679/1775 train_time:24981ms step_avg:36.79ms
step:680/1775 train_time:25042ms step_avg:36.83ms
step:681/1775 train_time:25100ms step_avg:36.86ms
step:682/1775 train_time:25160ms step_avg:36.89ms
step:683/1775 train_time:25218ms step_avg:36.92ms
step:684/1775 train_time:25278ms step_avg:36.96ms
step:685/1775 train_time:25336ms step_avg:36.99ms
step:686/1775 train_time:25397ms step_avg:37.02ms
step:687/1775 train_time:25455ms step_avg:37.05ms
step:688/1775 train_time:25516ms step_avg:37.09ms
step:689/1775 train_time:25575ms step_avg:37.12ms
step:690/1775 train_time:25634ms step_avg:37.15ms
step:691/1775 train_time:25693ms step_avg:37.18ms
step:692/1775 train_time:25754ms step_avg:37.22ms
step:693/1775 train_time:25812ms step_avg:37.25ms
step:694/1775 train_time:25873ms step_avg:37.28ms
step:695/1775 train_time:25931ms step_avg:37.31ms
step:696/1775 train_time:25992ms step_avg:37.34ms
step:697/1775 train_time:26051ms step_avg:37.38ms
step:698/1775 train_time:26112ms step_avg:37.41ms
step:699/1775 train_time:26171ms step_avg:37.44ms
step:700/1775 train_time:26231ms step_avg:37.47ms
step:701/1775 train_time:26290ms step_avg:37.50ms
step:702/1775 train_time:26351ms step_avg:37.54ms
step:703/1775 train_time:26408ms step_avg:37.57ms
step:704/1775 train_time:26470ms step_avg:37.60ms
step:705/1775 train_time:26528ms step_avg:37.63ms
step:706/1775 train_time:26589ms step_avg:37.66ms
step:707/1775 train_time:26647ms step_avg:37.69ms
step:708/1775 train_time:26707ms step_avg:37.72ms
step:709/1775 train_time:26765ms step_avg:37.75ms
step:710/1775 train_time:26827ms step_avg:37.78ms
step:711/1775 train_time:26885ms step_avg:37.81ms
step:712/1775 train_time:26946ms step_avg:37.85ms
step:713/1775 train_time:27005ms step_avg:37.88ms
step:714/1775 train_time:27066ms step_avg:37.91ms
step:715/1775 train_time:27124ms step_avg:37.94ms
step:716/1775 train_time:27185ms step_avg:37.97ms
step:717/1775 train_time:27245ms step_avg:38.00ms
step:718/1775 train_time:27305ms step_avg:38.03ms
step:719/1775 train_time:27364ms step_avg:38.06ms
step:720/1775 train_time:27425ms step_avg:38.09ms
step:721/1775 train_time:27483ms step_avg:38.12ms
step:722/1775 train_time:27544ms step_avg:38.15ms
step:723/1775 train_time:27602ms step_avg:38.18ms
step:724/1775 train_time:27663ms step_avg:38.21ms
step:725/1775 train_time:27721ms step_avg:38.24ms
step:726/1775 train_time:27782ms step_avg:38.27ms
step:727/1775 train_time:27840ms step_avg:38.29ms
step:728/1775 train_time:27901ms step_avg:38.33ms
step:729/1775 train_time:27959ms step_avg:38.35ms
step:730/1775 train_time:28020ms step_avg:38.38ms
step:731/1775 train_time:28078ms step_avg:38.41ms
step:732/1775 train_time:28140ms step_avg:38.44ms
step:733/1775 train_time:28198ms step_avg:38.47ms
step:734/1775 train_time:28258ms step_avg:38.50ms
step:735/1775 train_time:28317ms step_avg:38.53ms
step:736/1775 train_time:28378ms step_avg:38.56ms
step:737/1775 train_time:28435ms step_avg:38.58ms
step:738/1775 train_time:28496ms step_avg:38.61ms
step:739/1775 train_time:28554ms step_avg:38.64ms
step:740/1775 train_time:28615ms step_avg:38.67ms
step:741/1775 train_time:28673ms step_avg:38.70ms
step:742/1775 train_time:28733ms step_avg:38.72ms
step:743/1775 train_time:28792ms step_avg:38.75ms
step:744/1775 train_time:28853ms step_avg:38.78ms
step:745/1775 train_time:28912ms step_avg:38.81ms
step:746/1775 train_time:28972ms step_avg:38.84ms
step:747/1775 train_time:29030ms step_avg:38.86ms
step:748/1775 train_time:29091ms step_avg:38.89ms
step:749/1775 train_time:29149ms step_avg:38.92ms
step:750/1775 train_time:29210ms step_avg:38.95ms
step:750/1775 val_loss:3.9983 train_time:29279ms step_avg:39.04ms
step:751/1775 train_time:29303ms step_avg:39.02ms
step:752/1775 train_time:29330ms step_avg:39.00ms
step:753/1775 train_time:29390ms step_avg:39.03ms
step:754/1775 train_time:29452ms step_avg:39.06ms
step:755/1775 train_time:29511ms step_avg:39.09ms
step:756/1775 train_time:29571ms step_avg:39.12ms
step:757/1775 train_time:29629ms step_avg:39.14ms
step:758/1775 train_time:29688ms step_avg:39.17ms
step:759/1775 train_time:29746ms step_avg:39.19ms
step:760/1775 train_time:29806ms step_avg:39.22ms
step:761/1775 train_time:29863ms step_avg:39.24ms
step:762/1775 train_time:29924ms step_avg:39.27ms
step:763/1775 train_time:29981ms step_avg:39.29ms
step:764/1775 train_time:30041ms step_avg:39.32ms
step:765/1775 train_time:30098ms step_avg:39.34ms
step:766/1775 train_time:30159ms step_avg:39.37ms
step:767/1775 train_time:30219ms step_avg:39.40ms
step:768/1775 train_time:30281ms step_avg:39.43ms
step:769/1775 train_time:30341ms step_avg:39.46ms
step:770/1775 train_time:30405ms step_avg:39.49ms
step:771/1775 train_time:30464ms step_avg:39.51ms
step:772/1775 train_time:30526ms step_avg:39.54ms
step:773/1775 train_time:30585ms step_avg:39.57ms
step:774/1775 train_time:30645ms step_avg:39.59ms
step:775/1775 train_time:30703ms step_avg:39.62ms
step:776/1775 train_time:30763ms step_avg:39.64ms
step:777/1775 train_time:30821ms step_avg:39.67ms
step:778/1775 train_time:30881ms step_avg:39.69ms
step:779/1775 train_time:30939ms step_avg:39.72ms
step:780/1775 train_time:30999ms step_avg:39.74ms
step:781/1775 train_time:31056ms step_avg:39.76ms
step:782/1775 train_time:31117ms step_avg:39.79ms
step:783/1775 train_time:31175ms step_avg:39.82ms
step:784/1775 train_time:31237ms step_avg:39.84ms
step:785/1775 train_time:31296ms step_avg:39.87ms
step:786/1775 train_time:31357ms step_avg:39.89ms
step:787/1775 train_time:31417ms step_avg:39.92ms
step:788/1775 train_time:31479ms step_avg:39.95ms
step:789/1775 train_time:31537ms step_avg:39.97ms
step:790/1775 train_time:31599ms step_avg:40.00ms
step:791/1775 train_time:31658ms step_avg:40.02ms
step:792/1775 train_time:31719ms step_avg:40.05ms
step:793/1775 train_time:31776ms step_avg:40.07ms
step:794/1775 train_time:31836ms step_avg:40.10ms
step:795/1775 train_time:31894ms step_avg:40.12ms
step:796/1775 train_time:31954ms step_avg:40.14ms
step:797/1775 train_time:32011ms step_avg:40.16ms
step:798/1775 train_time:32071ms step_avg:40.19ms
step:799/1775 train_time:32128ms step_avg:40.21ms
step:800/1775 train_time:32189ms step_avg:40.24ms
step:801/1775 train_time:32248ms step_avg:40.26ms
step:802/1775 train_time:32309ms step_avg:40.29ms
step:803/1775 train_time:32368ms step_avg:40.31ms
step:804/1775 train_time:32430ms step_avg:40.34ms
step:805/1775 train_time:32488ms step_avg:40.36ms
step:806/1775 train_time:32549ms step_avg:40.38ms
step:807/1775 train_time:32607ms step_avg:40.41ms
step:808/1775 train_time:32668ms step_avg:40.43ms
step:809/1775 train_time:32726ms step_avg:40.45ms
step:810/1775 train_time:32786ms step_avg:40.48ms
step:811/1775 train_time:32844ms step_avg:40.50ms
step:812/1775 train_time:32905ms step_avg:40.52ms
step:813/1775 train_time:32963ms step_avg:40.54ms
step:814/1775 train_time:33023ms step_avg:40.57ms
step:815/1775 train_time:33082ms step_avg:40.59ms
step:816/1775 train_time:33143ms step_avg:40.62ms
step:817/1775 train_time:33202ms step_avg:40.64ms
step:818/1775 train_time:33264ms step_avg:40.66ms
step:819/1775 train_time:33322ms step_avg:40.69ms
step:820/1775 train_time:33383ms step_avg:40.71ms
step:821/1775 train_time:33441ms step_avg:40.73ms
step:822/1775 train_time:33502ms step_avg:40.76ms
step:823/1775 train_time:33562ms step_avg:40.78ms
step:824/1775 train_time:33622ms step_avg:40.80ms
step:825/1775 train_time:33680ms step_avg:40.82ms
step:826/1775 train_time:33740ms step_avg:40.85ms
step:827/1775 train_time:33799ms step_avg:40.87ms
step:828/1775 train_time:33859ms step_avg:40.89ms
step:829/1775 train_time:33917ms step_avg:40.91ms
step:830/1775 train_time:33978ms step_avg:40.94ms
step:831/1775 train_time:34037ms step_avg:40.96ms
step:832/1775 train_time:34098ms step_avg:40.98ms
step:833/1775 train_time:34156ms step_avg:41.00ms
step:834/1775 train_time:34217ms step_avg:41.03ms
step:835/1775 train_time:34276ms step_avg:41.05ms
step:836/1775 train_time:34337ms step_avg:41.07ms
step:837/1775 train_time:34396ms step_avg:41.09ms
step:838/1775 train_time:34457ms step_avg:41.12ms
step:839/1775 train_time:34516ms step_avg:41.14ms
step:840/1775 train_time:34577ms step_avg:41.16ms
step:841/1775 train_time:34635ms step_avg:41.18ms
step:842/1775 train_time:34696ms step_avg:41.21ms
step:843/1775 train_time:34754ms step_avg:41.23ms
step:844/1775 train_time:34815ms step_avg:41.25ms
step:845/1775 train_time:34873ms step_avg:41.27ms
step:846/1775 train_time:34933ms step_avg:41.29ms
step:847/1775 train_time:34991ms step_avg:41.31ms
step:848/1775 train_time:35051ms step_avg:41.33ms
step:849/1775 train_time:35108ms step_avg:41.35ms
step:850/1775 train_time:35169ms step_avg:41.38ms
step:851/1775 train_time:35226ms step_avg:41.39ms
step:852/1775 train_time:35288ms step_avg:41.42ms
step:853/1775 train_time:35346ms step_avg:41.44ms
step:854/1775 train_time:35407ms step_avg:41.46ms
step:855/1775 train_time:35465ms step_avg:41.48ms
step:856/1775 train_time:35525ms step_avg:41.50ms
step:857/1775 train_time:35584ms step_avg:41.52ms
step:858/1775 train_time:35644ms step_avg:41.54ms
step:859/1775 train_time:35703ms step_avg:41.56ms
step:860/1775 train_time:35764ms step_avg:41.59ms
step:861/1775 train_time:35823ms step_avg:41.61ms
step:862/1775 train_time:35884ms step_avg:41.63ms
step:863/1775 train_time:35943ms step_avg:41.65ms
step:864/1775 train_time:36003ms step_avg:41.67ms
step:865/1775 train_time:36062ms step_avg:41.69ms
step:866/1775 train_time:36122ms step_avg:41.71ms
step:867/1775 train_time:36180ms step_avg:41.73ms
step:868/1775 train_time:36241ms step_avg:41.75ms
step:869/1775 train_time:36300ms step_avg:41.77ms
step:870/1775 train_time:36361ms step_avg:41.79ms
step:871/1775 train_time:36420ms step_avg:41.81ms
step:872/1775 train_time:36480ms step_avg:41.84ms
step:873/1775 train_time:36539ms step_avg:41.85ms
step:874/1775 train_time:36600ms step_avg:41.88ms
step:875/1775 train_time:36658ms step_avg:41.90ms
step:876/1775 train_time:36720ms step_avg:41.92ms
step:877/1775 train_time:36778ms step_avg:41.94ms
step:878/1775 train_time:36839ms step_avg:41.96ms
step:879/1775 train_time:36897ms step_avg:41.98ms
step:880/1775 train_time:36958ms step_avg:42.00ms
step:881/1775 train_time:37017ms step_avg:42.02ms
step:882/1775 train_time:37077ms step_avg:42.04ms
step:883/1775 train_time:37135ms step_avg:42.06ms
step:884/1775 train_time:37195ms step_avg:42.08ms
step:885/1775 train_time:37253ms step_avg:42.09ms
step:886/1775 train_time:37314ms step_avg:42.11ms
step:887/1775 train_time:37372ms step_avg:42.13ms
step:888/1775 train_time:37434ms step_avg:42.16ms
step:889/1775 train_time:37491ms step_avg:42.17ms
step:890/1775 train_time:37553ms step_avg:42.19ms
step:891/1775 train_time:37611ms step_avg:42.21ms
step:892/1775 train_time:37671ms step_avg:42.23ms
step:893/1775 train_time:37730ms step_avg:42.25ms
step:894/1775 train_time:37790ms step_avg:42.27ms
step:895/1775 train_time:37848ms step_avg:42.29ms
step:896/1775 train_time:37909ms step_avg:42.31ms
step:897/1775 train_time:37967ms step_avg:42.33ms
step:898/1775 train_time:38028ms step_avg:42.35ms
step:899/1775 train_time:38086ms step_avg:42.37ms
step:900/1775 train_time:38147ms step_avg:42.39ms
step:901/1775 train_time:38205ms step_avg:42.40ms
step:902/1775 train_time:38266ms step_avg:42.42ms
step:903/1775 train_time:38324ms step_avg:42.44ms
step:904/1775 train_time:38385ms step_avg:42.46ms
step:905/1775 train_time:38444ms step_avg:42.48ms
step:906/1775 train_time:38504ms step_avg:42.50ms
step:907/1775 train_time:38563ms step_avg:42.52ms
step:908/1775 train_time:38624ms step_avg:42.54ms
step:909/1775 train_time:38682ms step_avg:42.55ms
step:910/1775 train_time:38743ms step_avg:42.57ms
step:911/1775 train_time:38801ms step_avg:42.59ms
step:912/1775 train_time:38862ms step_avg:42.61ms
step:913/1775 train_time:38920ms step_avg:42.63ms
step:914/1775 train_time:38981ms step_avg:42.65ms
step:915/1775 train_time:39039ms step_avg:42.67ms
step:916/1775 train_time:39099ms step_avg:42.69ms
step:917/1775 train_time:39157ms step_avg:42.70ms
step:918/1775 train_time:39219ms step_avg:42.72ms
step:919/1775 train_time:39277ms step_avg:42.74ms
step:920/1775 train_time:39338ms step_avg:42.76ms
step:921/1775 train_time:39397ms step_avg:42.78ms
step:922/1775 train_time:39458ms step_avg:42.80ms
step:923/1775 train_time:39517ms step_avg:42.81ms
step:924/1775 train_time:39577ms step_avg:42.83ms
step:925/1775 train_time:39636ms step_avg:42.85ms
step:926/1775 train_time:39696ms step_avg:42.87ms
step:927/1775 train_time:39754ms step_avg:42.88ms
step:928/1775 train_time:39814ms step_avg:42.90ms
step:929/1775 train_time:39872ms step_avg:42.92ms
step:930/1775 train_time:39933ms step_avg:42.94ms
step:931/1775 train_time:39991ms step_avg:42.95ms
step:932/1775 train_time:40051ms step_avg:42.97ms
step:933/1775 train_time:40110ms step_avg:42.99ms
step:934/1775 train_time:40170ms step_avg:43.01ms
step:935/1775 train_time:40228ms step_avg:43.03ms
step:936/1775 train_time:40289ms step_avg:43.04ms
step:937/1775 train_time:40346ms step_avg:43.06ms
step:938/1775 train_time:40407ms step_avg:43.08ms
step:939/1775 train_time:40465ms step_avg:43.09ms
step:940/1775 train_time:40527ms step_avg:43.11ms
step:941/1775 train_time:40585ms step_avg:43.13ms
step:942/1775 train_time:40646ms step_avg:43.15ms
step:943/1775 train_time:40705ms step_avg:43.17ms
step:944/1775 train_time:40765ms step_avg:43.18ms
step:945/1775 train_time:40823ms step_avg:43.20ms
step:946/1775 train_time:40884ms step_avg:43.22ms
step:947/1775 train_time:40942ms step_avg:43.23ms
step:948/1775 train_time:41003ms step_avg:43.25ms
step:949/1775 train_time:41062ms step_avg:43.27ms
step:950/1775 train_time:41122ms step_avg:43.29ms
step:951/1775 train_time:41180ms step_avg:43.30ms
step:952/1775 train_time:41241ms step_avg:43.32ms
step:953/1775 train_time:41300ms step_avg:43.34ms
step:954/1775 train_time:41360ms step_avg:43.35ms
step:955/1775 train_time:41419ms step_avg:43.37ms
step:956/1775 train_time:41480ms step_avg:43.39ms
step:957/1775 train_time:41538ms step_avg:43.40ms
step:958/1775 train_time:41599ms step_avg:43.42ms
step:959/1775 train_time:41657ms step_avg:43.44ms
step:960/1775 train_time:41718ms step_avg:43.46ms
step:961/1775 train_time:41776ms step_avg:43.47ms
step:962/1775 train_time:41837ms step_avg:43.49ms
step:963/1775 train_time:41895ms step_avg:43.51ms
step:964/1775 train_time:41956ms step_avg:43.52ms
step:965/1775 train_time:42014ms step_avg:43.54ms
step:966/1775 train_time:42074ms step_avg:43.55ms
step:967/1775 train_time:42133ms step_avg:43.57ms
step:968/1775 train_time:42193ms step_avg:43.59ms
step:969/1775 train_time:42252ms step_avg:43.60ms
step:970/1775 train_time:42312ms step_avg:43.62ms
step:971/1775 train_time:42370ms step_avg:43.64ms
step:972/1775 train_time:42432ms step_avg:43.65ms
step:973/1775 train_time:42489ms step_avg:43.67ms
step:974/1775 train_time:42549ms step_avg:43.69ms
step:975/1775 train_time:42608ms step_avg:43.70ms
step:976/1775 train_time:42668ms step_avg:43.72ms
step:977/1775 train_time:42727ms step_avg:43.73ms
step:978/1775 train_time:42788ms step_avg:43.75ms
step:979/1775 train_time:42845ms step_avg:43.76ms
step:980/1775 train_time:42907ms step_avg:43.78ms
step:981/1775 train_time:42964ms step_avg:43.80ms
step:982/1775 train_time:43026ms step_avg:43.81ms
step:983/1775 train_time:43084ms step_avg:43.83ms
step:984/1775 train_time:43145ms step_avg:43.85ms
step:985/1775 train_time:43203ms step_avg:43.86ms
step:986/1775 train_time:43264ms step_avg:43.88ms
step:987/1775 train_time:43322ms step_avg:43.89ms
step:988/1775 train_time:43383ms step_avg:43.91ms
step:989/1775 train_time:43441ms step_avg:43.92ms
step:990/1775 train_time:43502ms step_avg:43.94ms
step:991/1775 train_time:43561ms step_avg:43.96ms
step:992/1775 train_time:43622ms step_avg:43.97ms
step:993/1775 train_time:43680ms step_avg:43.99ms
step:994/1775 train_time:43740ms step_avg:44.00ms
step:995/1775 train_time:43799ms step_avg:44.02ms
step:996/1775 train_time:43860ms step_avg:44.04ms
step:997/1775 train_time:43918ms step_avg:44.05ms
step:998/1775 train_time:43979ms step_avg:44.07ms
step:999/1775 train_time:44038ms step_avg:44.08ms
step:1000/1775 train_time:44099ms step_avg:44.10ms
step:1000/1775 val_loss:3.7371 train_time:44169ms step_avg:44.17ms
step:1001/1775 train_time:44192ms step_avg:44.15ms
step:1002/1775 train_time:44219ms step_avg:44.13ms
step:1003/1775 train_time:44279ms step_avg:44.15ms
step:1004/1775 train_time:44343ms step_avg:44.17ms
step:1005/1775 train_time:44401ms step_avg:44.18ms
step:1006/1775 train_time:44463ms step_avg:44.20ms
step:1007/1775 train_time:44521ms step_avg:44.21ms
step:1008/1775 train_time:44581ms step_avg:44.23ms
step:1009/1775 train_time:44639ms step_avg:44.24ms
step:1010/1775 train_time:44699ms step_avg:44.26ms
step:1011/1775 train_time:44757ms step_avg:44.27ms
step:1012/1775 train_time:44818ms step_avg:44.29ms
step:1013/1775 train_time:44875ms step_avg:44.30ms
step:1014/1775 train_time:44936ms step_avg:44.32ms
step:1015/1775 train_time:44993ms step_avg:44.33ms
step:1016/1775 train_time:45054ms step_avg:44.34ms
step:1017/1775 train_time:45112ms step_avg:44.36ms
step:1018/1775 train_time:45173ms step_avg:44.37ms
step:1019/1775 train_time:45233ms step_avg:44.39ms
step:1020/1775 train_time:45296ms step_avg:44.41ms
step:1021/1775 train_time:45356ms step_avg:44.42ms
step:1022/1775 train_time:45417ms step_avg:44.44ms
step:1023/1775 train_time:45476ms step_avg:44.45ms
step:1024/1775 train_time:45536ms step_avg:44.47ms
step:1025/1775 train_time:45594ms step_avg:44.48ms
step:1026/1775 train_time:45654ms step_avg:44.50ms
step:1027/1775 train_time:45713ms step_avg:44.51ms
step:1028/1775 train_time:45773ms step_avg:44.53ms
step:1029/1775 train_time:45832ms step_avg:44.54ms
step:1030/1775 train_time:45892ms step_avg:44.56ms
step:1031/1775 train_time:45950ms step_avg:44.57ms
step:1032/1775 train_time:46009ms step_avg:44.58ms
step:1033/1775 train_time:46068ms step_avg:44.60ms
step:1034/1775 train_time:46128ms step_avg:44.61ms
step:1035/1775 train_time:46187ms step_avg:44.63ms
step:1036/1775 train_time:46250ms step_avg:44.64ms
step:1037/1775 train_time:46310ms step_avg:44.66ms
step:1038/1775 train_time:46371ms step_avg:44.67ms
step:1039/1775 train_time:46431ms step_avg:44.69ms
step:1040/1775 train_time:46493ms step_avg:44.71ms
step:1041/1775 train_time:46552ms step_avg:44.72ms
step:1042/1775 train_time:46615ms step_avg:44.74ms
step:1043/1775 train_time:46670ms step_avg:44.75ms
step:1044/1775 train_time:46730ms step_avg:44.76ms
step:1045/1775 train_time:46787ms step_avg:44.77ms
step:1046/1775 train_time:46847ms step_avg:44.79ms
step:1047/1775 train_time:46905ms step_avg:44.80ms
step:1048/1775 train_time:46965ms step_avg:44.81ms
step:1049/1775 train_time:47022ms step_avg:44.83ms
step:1050/1775 train_time:47083ms step_avg:44.84ms
step:1051/1775 train_time:47142ms step_avg:44.85ms
step:1052/1775 train_time:47202ms step_avg:44.87ms
step:1053/1775 train_time:47262ms step_avg:44.88ms
step:1054/1775 train_time:47322ms step_avg:44.90ms
step:1055/1775 train_time:47380ms step_avg:44.91ms
step:1056/1775 train_time:47442ms step_avg:44.93ms
step:1057/1775 train_time:47500ms step_avg:44.94ms
step:1058/1775 train_time:47562ms step_avg:44.95ms
step:1059/1775 train_time:47619ms step_avg:44.97ms
step:1060/1775 train_time:47679ms step_avg:44.98ms
step:1061/1775 train_time:47738ms step_avg:44.99ms
step:1062/1775 train_time:47799ms step_avg:45.01ms
step:1063/1775 train_time:47857ms step_avg:45.02ms
step:1064/1775 train_time:47918ms step_avg:45.04ms
step:1065/1775 train_time:47976ms step_avg:45.05ms
step:1066/1775 train_time:48038ms step_avg:45.06ms
step:1067/1775 train_time:48096ms step_avg:45.08ms
step:1068/1775 train_time:48158ms step_avg:45.09ms
step:1069/1775 train_time:48216ms step_avg:45.10ms
step:1070/1775 train_time:48277ms step_avg:45.12ms
step:1071/1775 train_time:48336ms step_avg:45.13ms
step:1072/1775 train_time:48396ms step_avg:45.15ms
step:1073/1775 train_time:48455ms step_avg:45.16ms
step:1074/1775 train_time:48516ms step_avg:45.17ms
step:1075/1775 train_time:48575ms step_avg:45.19ms
step:1076/1775 train_time:48636ms step_avg:45.20ms
step:1077/1775 train_time:48694ms step_avg:45.21ms
step:1078/1775 train_time:48754ms step_avg:45.23ms
step:1079/1775 train_time:48813ms step_avg:45.24ms
step:1080/1775 train_time:48874ms step_avg:45.25ms
step:1081/1775 train_time:48932ms step_avg:45.27ms
step:1082/1775 train_time:48993ms step_avg:45.28ms
step:1083/1775 train_time:49051ms step_avg:45.29ms
step:1084/1775 train_time:49112ms step_avg:45.31ms
step:1085/1775 train_time:49171ms step_avg:45.32ms
step:1086/1775 train_time:49231ms step_avg:45.33ms
step:1087/1775 train_time:49290ms step_avg:45.34ms
step:1088/1775 train_time:49351ms step_avg:45.36ms
step:1089/1775 train_time:49409ms step_avg:45.37ms
step:1090/1775 train_time:49471ms step_avg:45.39ms
step:1091/1775 train_time:49529ms step_avg:45.40ms
step:1092/1775 train_time:49591ms step_avg:45.41ms
step:1093/1775 train_time:49649ms step_avg:45.42ms
step:1094/1775 train_time:49710ms step_avg:45.44ms
step:1095/1775 train_time:49769ms step_avg:45.45ms
step:1096/1775 train_time:49829ms step_avg:45.46ms
step:1097/1775 train_time:49887ms step_avg:45.48ms
step:1098/1775 train_time:49947ms step_avg:45.49ms
step:1099/1775 train_time:50005ms step_avg:45.50ms
step:1100/1775 train_time:50066ms step_avg:45.51ms
step:1101/1775 train_time:50123ms step_avg:45.52ms
step:1102/1775 train_time:50183ms step_avg:45.54ms
step:1103/1775 train_time:50241ms step_avg:45.55ms
step:1104/1775 train_time:50302ms step_avg:45.56ms
step:1105/1775 train_time:50361ms step_avg:45.58ms
step:1106/1775 train_time:50421ms step_avg:45.59ms
step:1107/1775 train_time:50480ms step_avg:45.60ms
step:1108/1775 train_time:50540ms step_avg:45.61ms
step:1109/1775 train_time:50599ms step_avg:45.63ms
step:1110/1775 train_time:50660ms step_avg:45.64ms
step:1111/1775 train_time:50718ms step_avg:45.65ms
step:1112/1775 train_time:50779ms step_avg:45.66ms
step:1113/1775 train_time:50838ms step_avg:45.68ms
step:1114/1775 train_time:50899ms step_avg:45.69ms
step:1115/1775 train_time:50958ms step_avg:45.70ms
step:1116/1775 train_time:51018ms step_avg:45.71ms
step:1117/1775 train_time:51076ms step_avg:45.73ms
step:1118/1775 train_time:51137ms step_avg:45.74ms
step:1119/1775 train_time:51194ms step_avg:45.75ms
step:1120/1775 train_time:51256ms step_avg:45.76ms
step:1121/1775 train_time:51316ms step_avg:45.78ms
step:1122/1775 train_time:51376ms step_avg:45.79ms
step:1123/1775 train_time:51434ms step_avg:45.80ms
step:1124/1775 train_time:51496ms step_avg:45.81ms
step:1125/1775 train_time:51555ms step_avg:45.83ms
step:1126/1775 train_time:51617ms step_avg:45.84ms
step:1127/1775 train_time:51674ms step_avg:45.85ms
step:1128/1775 train_time:51735ms step_avg:45.86ms
step:1129/1775 train_time:51793ms step_avg:45.88ms
step:1130/1775 train_time:51854ms step_avg:45.89ms
step:1131/1775 train_time:51912ms step_avg:45.90ms
step:1132/1775 train_time:51974ms step_avg:45.91ms
step:1133/1775 train_time:52032ms step_avg:45.92ms
step:1134/1775 train_time:52093ms step_avg:45.94ms
step:1135/1775 train_time:52151ms step_avg:45.95ms
step:1136/1775 train_time:52212ms step_avg:45.96ms
step:1137/1775 train_time:52271ms step_avg:45.97ms
step:1138/1775 train_time:52331ms step_avg:45.99ms
step:1139/1775 train_time:52390ms step_avg:46.00ms
step:1140/1775 train_time:52451ms step_avg:46.01ms
step:1141/1775 train_time:52510ms step_avg:46.02ms
step:1142/1775 train_time:52572ms step_avg:46.03ms
step:1143/1775 train_time:52630ms step_avg:46.05ms
step:1144/1775 train_time:52690ms step_avg:46.06ms
step:1145/1775 train_time:52748ms step_avg:46.07ms
step:1146/1775 train_time:52809ms step_avg:46.08ms
step:1147/1775 train_time:52867ms step_avg:46.09ms
step:1148/1775 train_time:52928ms step_avg:46.10ms
step:1149/1775 train_time:52986ms step_avg:46.12ms
step:1150/1775 train_time:53047ms step_avg:46.13ms
step:1151/1775 train_time:53105ms step_avg:46.14ms
step:1152/1775 train_time:53166ms step_avg:46.15ms
step:1153/1775 train_time:53223ms step_avg:46.16ms
step:1154/1775 train_time:53285ms step_avg:46.17ms
step:1155/1775 train_time:53343ms step_avg:46.18ms
step:1156/1775 train_time:53404ms step_avg:46.20ms
step:1157/1775 train_time:53462ms step_avg:46.21ms
step:1158/1775 train_time:53528ms step_avg:46.22ms
step:1159/1775 train_time:53612ms step_avg:46.26ms
step:1160/1775 train_time:53699ms step_avg:46.29ms
step:1161/1775 train_time:53784ms step_avg:46.33ms
step:1162/1775 train_time:53870ms step_avg:46.36ms
step:1163/1775 train_time:53954ms step_avg:46.39ms
step:1164/1775 train_time:54041ms step_avg:46.43ms
step:1165/1775 train_time:54126ms step_avg:46.46ms
step:1166/1775 train_time:54213ms step_avg:46.50ms
step:1167/1775 train_time:54298ms step_avg:46.53ms
step:1168/1775 train_time:54386ms step_avg:46.56ms
step:1169/1775 train_time:54470ms step_avg:46.60ms
step:1170/1775 train_time:54557ms step_avg:46.63ms
step:1171/1775 train_time:54639ms step_avg:46.66ms
step:1172/1775 train_time:54726ms step_avg:46.69ms
step:1173/1775 train_time:54810ms step_avg:46.73ms
step:1174/1775 train_time:54896ms step_avg:46.76ms
step:1175/1775 train_time:54980ms step_avg:46.79ms
step:1176/1775 train_time:55066ms step_avg:46.82ms
step:1177/1775 train_time:55150ms step_avg:46.86ms
step:1178/1775 train_time:55237ms step_avg:46.89ms
step:1179/1775 train_time:55322ms step_avg:46.92ms
step:1180/1775 train_time:55409ms step_avg:46.96ms
step:1181/1775 train_time:55494ms step_avg:46.99ms
step:1182/1775 train_time:55581ms step_avg:47.02ms
step:1183/1775 train_time:55665ms step_avg:47.05ms
step:1184/1775 train_time:55751ms step_avg:47.09ms
step:1185/1775 train_time:55835ms step_avg:47.12ms
step:1186/1775 train_time:55921ms step_avg:47.15ms
step:1187/1775 train_time:56006ms step_avg:47.18ms
step:1188/1775 train_time:56093ms step_avg:47.22ms
step:1189/1775 train_time:56176ms step_avg:47.25ms
step:1190/1775 train_time:56262ms step_avg:47.28ms
step:1191/1775 train_time:56346ms step_avg:47.31ms
step:1192/1775 train_time:56435ms step_avg:47.34ms
step:1193/1775 train_time:56519ms step_avg:47.38ms
step:1194/1775 train_time:56605ms step_avg:47.41ms
step:1195/1775 train_time:56689ms step_avg:47.44ms
step:1196/1775 train_time:56776ms step_avg:47.47ms
step:1197/1775 train_time:56861ms step_avg:47.50ms
step:1198/1775 train_time:56948ms step_avg:47.54ms
step:1199/1775 train_time:57032ms step_avg:47.57ms
step:1200/1775 train_time:57117ms step_avg:47.60ms
step:1201/1775 train_time:57203ms step_avg:47.63ms
step:1202/1775 train_time:57290ms step_avg:47.66ms
step:1203/1775 train_time:57373ms step_avg:47.69ms
step:1204/1775 train_time:57460ms step_avg:47.72ms
step:1205/1775 train_time:57545ms step_avg:47.75ms
step:1206/1775 train_time:57632ms step_avg:47.79ms
step:1207/1775 train_time:57716ms step_avg:47.82ms
step:1208/1775 train_time:57802ms step_avg:47.85ms
step:1209/1775 train_time:57886ms step_avg:47.88ms
step:1210/1775 train_time:57972ms step_avg:47.91ms
step:1211/1775 train_time:58057ms step_avg:47.94ms
step:1212/1775 train_time:58143ms step_avg:47.97ms
step:1213/1775 train_time:58227ms step_avg:48.00ms
step:1214/1775 train_time:58314ms step_avg:48.03ms
step:1215/1775 train_time:58397ms step_avg:48.06ms
step:1216/1775 train_time:58486ms step_avg:48.10ms
step:1217/1775 train_time:58571ms step_avg:48.13ms
step:1218/1775 train_time:58657ms step_avg:48.16ms
step:1219/1775 train_time:58739ms step_avg:48.19ms
step:1220/1775 train_time:58826ms step_avg:48.22ms
step:1221/1775 train_time:58910ms step_avg:48.25ms
step:1222/1775 train_time:58997ms step_avg:48.28ms
step:1223/1775 train_time:59080ms step_avg:48.31ms
step:1224/1775 train_time:59169ms step_avg:48.34ms
step:1225/1775 train_time:59253ms step_avg:48.37ms
step:1226/1775 train_time:59339ms step_avg:48.40ms
step:1227/1775 train_time:59423ms step_avg:48.43ms
step:1228/1775 train_time:59510ms step_avg:48.46ms
step:1229/1775 train_time:59595ms step_avg:48.49ms
step:1230/1775 train_time:59681ms step_avg:48.52ms
step:1231/1775 train_time:59765ms step_avg:48.55ms
step:1232/1775 train_time:59852ms step_avg:48.58ms
step:1233/1775 train_time:59936ms step_avg:48.61ms
step:1234/1775 train_time:60022ms step_avg:48.64ms
step:1235/1775 train_time:60106ms step_avg:48.67ms
step:1236/1775 train_time:60193ms step_avg:48.70ms
step:1237/1775 train_time:60276ms step_avg:48.73ms
step:1238/1775 train_time:60363ms step_avg:48.76ms
step:1239/1775 train_time:60448ms step_avg:48.79ms
step:1240/1775 train_time:60535ms step_avg:48.82ms
step:1241/1775 train_time:60619ms step_avg:48.85ms
step:1242/1775 train_time:60706ms step_avg:48.88ms
step:1243/1775 train_time:60790ms step_avg:48.91ms
step:1244/1775 train_time:60876ms step_avg:48.94ms
step:1245/1775 train_time:60960ms step_avg:48.96ms
step:1246/1775 train_time:61048ms step_avg:49.00ms
step:1247/1775 train_time:61133ms step_avg:49.02ms
step:1248/1775 train_time:61219ms step_avg:49.05ms
step:1249/1775 train_time:61304ms step_avg:49.08ms
step:1250/1775 train_time:61391ms step_avg:49.11ms
step:1250/1775 val_loss:3.5050 train_time:61486ms step_avg:49.19ms
step:1251/1775 train_time:61510ms step_avg:49.17ms
step:1252/1775 train_time:61564ms step_avg:49.17ms
step:1253/1775 train_time:61651ms step_avg:49.20ms
step:1254/1775 train_time:61740ms step_avg:49.23ms
step:1255/1775 train_time:61824ms step_avg:49.26ms
step:1256/1775 train_time:61910ms step_avg:49.29ms
step:1257/1775 train_time:61994ms step_avg:49.32ms
step:1258/1775 train_time:62080ms step_avg:49.35ms
step:1259/1775 train_time:62162ms step_avg:49.37ms
step:1260/1775 train_time:62248ms step_avg:49.40ms
step:1261/1775 train_time:62331ms step_avg:49.43ms
step:1262/1775 train_time:62417ms step_avg:49.46ms
step:1263/1775 train_time:62503ms step_avg:49.49ms
step:1264/1775 train_time:62593ms step_avg:49.52ms
step:1265/1775 train_time:62680ms step_avg:49.55ms
step:1266/1775 train_time:62768ms step_avg:49.58ms
step:1267/1775 train_time:62854ms step_avg:49.61ms
step:1268/1775 train_time:62940ms step_avg:49.64ms
step:1269/1775 train_time:63023ms step_avg:49.66ms
step:1270/1775 train_time:63110ms step_avg:49.69ms
step:1271/1775 train_time:63193ms step_avg:49.72ms
step:1272/1775 train_time:63278ms step_avg:49.75ms
step:1273/1775 train_time:63362ms step_avg:49.77ms
step:1274/1775 train_time:63449ms step_avg:49.80ms
step:1275/1775 train_time:63536ms step_avg:49.83ms
step:1276/1775 train_time:63625ms step_avg:49.86ms
step:1277/1775 train_time:63710ms step_avg:49.89ms
step:1278/1775 train_time:63797ms step_avg:49.92ms
step:1279/1775 train_time:63882ms step_avg:49.95ms
step:1280/1775 train_time:63967ms step_avg:49.97ms
step:1281/1775 train_time:64051ms step_avg:50.00ms
step:1282/1775 train_time:64136ms step_avg:50.03ms
step:1283/1775 train_time:64221ms step_avg:50.06ms
step:1284/1775 train_time:64306ms step_avg:50.08ms
step:1285/1775 train_time:64388ms step_avg:50.11ms
step:1286/1775 train_time:64476ms step_avg:50.14ms
step:1287/1775 train_time:64563ms step_avg:50.17ms
step:1288/1775 train_time:64650ms step_avg:50.19ms
step:1289/1775 train_time:64735ms step_avg:50.22ms
step:1290/1775 train_time:64823ms step_avg:50.25ms
step:1291/1775 train_time:64906ms step_avg:50.28ms
step:1292/1775 train_time:64991ms step_avg:50.30ms
step:1293/1775 train_time:65074ms step_avg:50.33ms
step:1294/1775 train_time:65161ms step_avg:50.36ms
step:1295/1775 train_time:65245ms step_avg:50.38ms
step:1296/1775 train_time:65330ms step_avg:50.41ms
step:1297/1775 train_time:65416ms step_avg:50.44ms
step:1298/1775 train_time:65504ms step_avg:50.47ms
step:1299/1775 train_time:65589ms step_avg:50.49ms
step:1300/1775 train_time:65677ms step_avg:50.52ms
step:1301/1775 train_time:65761ms step_avg:50.55ms
step:1302/1775 train_time:65847ms step_avg:50.57ms
step:1303/1775 train_time:65930ms step_avg:50.60ms
step:1304/1775 train_time:66017ms step_avg:50.63ms
step:1305/1775 train_time:66101ms step_avg:50.65ms
step:1306/1775 train_time:66187ms step_avg:50.68ms
step:1307/1775 train_time:66270ms step_avg:50.70ms
step:1308/1775 train_time:66358ms step_avg:50.73ms
step:1309/1775 train_time:66441ms step_avg:50.76ms
step:1310/1775 train_time:66529ms step_avg:50.79ms
step:1311/1775 train_time:66614ms step_avg:50.81ms
step:1312/1775 train_time:66702ms step_avg:50.84ms
step:1313/1775 train_time:66785ms step_avg:50.86ms
step:1314/1775 train_time:66872ms step_avg:50.89ms
step:1315/1775 train_time:66956ms step_avg:50.92ms
step:1316/1775 train_time:67043ms step_avg:50.94ms
step:1317/1775 train_time:67126ms step_avg:50.97ms
step:1318/1775 train_time:67213ms step_avg:51.00ms
step:1319/1775 train_time:67296ms step_avg:51.02ms
step:1320/1775 train_time:67382ms step_avg:51.05ms
step:1321/1775 train_time:67466ms step_avg:51.07ms
step:1322/1775 train_time:67554ms step_avg:51.10ms
step:1323/1775 train_time:67639ms step_avg:51.13ms
step:1324/1775 train_time:67725ms step_avg:51.15ms
step:1325/1775 train_time:67808ms step_avg:51.18ms
step:1326/1775 train_time:67896ms step_avg:51.20ms
step:1327/1775 train_time:67981ms step_avg:51.23ms
step:1328/1775 train_time:68066ms step_avg:51.25ms
step:1329/1775 train_time:68151ms step_avg:51.28ms
step:1330/1775 train_time:68237ms step_avg:51.31ms
step:1331/1775 train_time:68320ms step_avg:51.33ms
step:1332/1775 train_time:68406ms step_avg:51.36ms
step:1333/1775 train_time:68490ms step_avg:51.38ms
step:1334/1775 train_time:68578ms step_avg:51.41ms
step:1335/1775 train_time:68663ms step_avg:51.43ms
step:1336/1775 train_time:68749ms step_avg:51.46ms
step:1337/1775 train_time:68834ms step_avg:51.48ms
step:1338/1775 train_time:68922ms step_avg:51.51ms
step:1339/1775 train_time:69005ms step_avg:51.54ms
step:1340/1775 train_time:69093ms step_avg:51.56ms
step:1341/1775 train_time:69176ms step_avg:51.59ms
step:1342/1775 train_time:69263ms step_avg:51.61ms
step:1343/1775 train_time:69347ms step_avg:51.64ms
step:1344/1775 train_time:69433ms step_avg:51.66ms
step:1345/1775 train_time:69518ms step_avg:51.69ms
step:1346/1775 train_time:69604ms step_avg:51.71ms
step:1347/1775 train_time:69689ms step_avg:51.74ms
step:1348/1775 train_time:69777ms step_avg:51.76ms
step:1349/1775 train_time:69862ms step_avg:51.79ms
step:1350/1775 train_time:69948ms step_avg:51.81ms
step:1351/1775 train_time:70032ms step_avg:51.84ms
step:1352/1775 train_time:70119ms step_avg:51.86ms
step:1353/1775 train_time:70202ms step_avg:51.89ms
step:1354/1775 train_time:70288ms step_avg:51.91ms
step:1355/1775 train_time:70372ms step_avg:51.94ms
step:1356/1775 train_time:70460ms step_avg:51.96ms
step:1357/1775 train_time:70543ms step_avg:51.98ms
step:1358/1775 train_time:70631ms step_avg:52.01ms
step:1359/1775 train_time:70715ms step_avg:52.03ms
step:1360/1775 train_time:70802ms step_avg:52.06ms
step:1361/1775 train_time:70887ms step_avg:52.08ms
step:1362/1775 train_time:70973ms step_avg:52.11ms
step:1363/1775 train_time:71059ms step_avg:52.13ms
step:1364/1775 train_time:71145ms step_avg:52.16ms
step:1365/1775 train_time:71230ms step_avg:52.18ms
step:1366/1775 train_time:71316ms step_avg:52.21ms
step:1367/1775 train_time:71400ms step_avg:52.23ms
step:1368/1775 train_time:71487ms step_avg:52.26ms
step:1369/1775 train_time:71571ms step_avg:52.28ms
step:1370/1775 train_time:71658ms step_avg:52.30ms
step:1371/1775 train_time:71743ms step_avg:52.33ms
step:1372/1775 train_time:71829ms step_avg:52.35ms
step:1373/1775 train_time:71913ms step_avg:52.38ms
step:1374/1775 train_time:71999ms step_avg:52.40ms
step:1375/1775 train_time:72083ms step_avg:52.42ms
step:1376/1775 train_time:72170ms step_avg:52.45ms
step:1377/1775 train_time:72254ms step_avg:52.47ms
step:1378/1775 train_time:72341ms step_avg:52.50ms
step:1379/1775 train_time:72425ms step_avg:52.52ms
step:1380/1775 train_time:72512ms step_avg:52.55ms
step:1381/1775 train_time:72596ms step_avg:52.57ms
step:1382/1775 train_time:72683ms step_avg:52.59ms
step:1383/1775 train_time:72767ms step_avg:52.62ms
step:1384/1775 train_time:72853ms step_avg:52.64ms
step:1385/1775 train_time:72938ms step_avg:52.66ms
step:1386/1775 train_time:73025ms step_avg:52.69ms
step:1387/1775 train_time:73107ms step_avg:52.71ms
step:1388/1775 train_time:73195ms step_avg:52.73ms
step:1389/1775 train_time:73280ms step_avg:52.76ms
step:1390/1775 train_time:73365ms step_avg:52.78ms
step:1391/1775 train_time:73449ms step_avg:52.80ms
step:1392/1775 train_time:73537ms step_avg:52.83ms
step:1393/1775 train_time:73621ms step_avg:52.85ms
step:1394/1775 train_time:73708ms step_avg:52.88ms
step:1395/1775 train_time:73792ms step_avg:52.90ms
step:1396/1775 train_time:73879ms step_avg:52.92ms
step:1397/1775 train_time:73963ms step_avg:52.94ms
step:1398/1775 train_time:74049ms step_avg:52.97ms
step:1399/1775 train_time:74133ms step_avg:52.99ms
step:1400/1775 train_time:74221ms step_avg:53.01ms
step:1401/1775 train_time:74304ms step_avg:53.04ms
step:1402/1775 train_time:74390ms step_avg:53.06ms
step:1403/1775 train_time:74474ms step_avg:53.08ms
step:1404/1775 train_time:74562ms step_avg:53.11ms
step:1405/1775 train_time:74644ms step_avg:53.13ms
step:1406/1775 train_time:74732ms step_avg:53.15ms
step:1407/1775 train_time:74816ms step_avg:53.17ms
step:1408/1775 train_time:74903ms step_avg:53.20ms
step:1409/1775 train_time:74987ms step_avg:53.22ms
step:1410/1775 train_time:75074ms step_avg:53.24ms
step:1411/1775 train_time:75158ms step_avg:53.27ms
step:1412/1775 train_time:75245ms step_avg:53.29ms
step:1413/1775 train_time:75329ms step_avg:53.31ms
step:1414/1775 train_time:75417ms step_avg:53.34ms
step:1415/1775 train_time:75501ms step_avg:53.36ms
step:1416/1775 train_time:75587ms step_avg:53.38ms
step:1417/1775 train_time:75671ms step_avg:53.40ms
step:1418/1775 train_time:75759ms step_avg:53.43ms
step:1419/1775 train_time:75843ms step_avg:53.45ms
step:1420/1775 train_time:75929ms step_avg:53.47ms
step:1421/1775 train_time:76013ms step_avg:53.49ms
step:1422/1775 train_time:76100ms step_avg:53.52ms
step:1423/1775 train_time:76183ms step_avg:53.54ms
step:1424/1775 train_time:76270ms step_avg:53.56ms
step:1425/1775 train_time:76355ms step_avg:53.58ms
step:1426/1775 train_time:76442ms step_avg:53.61ms
step:1427/1775 train_time:76526ms step_avg:53.63ms
step:1428/1775 train_time:76612ms step_avg:53.65ms
step:1429/1775 train_time:76696ms step_avg:53.67ms
step:1430/1775 train_time:76784ms step_avg:53.70ms
step:1431/1775 train_time:76867ms step_avg:53.72ms
step:1432/1775 train_time:76954ms step_avg:53.74ms
step:1433/1775 train_time:77038ms step_avg:53.76ms
step:1434/1775 train_time:77124ms step_avg:53.78ms
step:1435/1775 train_time:77208ms step_avg:53.80ms
step:1436/1775 train_time:77295ms step_avg:53.83ms
step:1437/1775 train_time:77380ms step_avg:53.85ms
step:1438/1775 train_time:77465ms step_avg:53.87ms
step:1439/1775 train_time:77550ms step_avg:53.89ms
step:1440/1775 train_time:77636ms step_avg:53.91ms
step:1441/1775 train_time:77722ms step_avg:53.94ms
step:1442/1775 train_time:77808ms step_avg:53.96ms
step:1443/1775 train_time:77891ms step_avg:53.98ms
step:1444/1775 train_time:77978ms step_avg:54.00ms
step:1445/1775 train_time:78063ms step_avg:54.02ms
step:1446/1775 train_time:78150ms step_avg:54.05ms
step:1447/1775 train_time:78233ms step_avg:54.07ms
step:1448/1775 train_time:78321ms step_avg:54.09ms
step:1449/1775 train_time:78405ms step_avg:54.11ms
step:1450/1775 train_time:78490ms step_avg:54.13ms
step:1451/1775 train_time:78575ms step_avg:54.15ms
step:1452/1775 train_time:78662ms step_avg:54.18ms
step:1453/1775 train_time:78746ms step_avg:54.20ms
step:1454/1775 train_time:78833ms step_avg:54.22ms
step:1455/1775 train_time:78917ms step_avg:54.24ms
step:1456/1775 train_time:79004ms step_avg:54.26ms
step:1457/1775 train_time:79088ms step_avg:54.28ms
step:1458/1775 train_time:79175ms step_avg:54.30ms
step:1459/1775 train_time:79259ms step_avg:54.32ms
step:1460/1775 train_time:79345ms step_avg:54.35ms
step:1461/1775 train_time:79429ms step_avg:54.37ms
step:1462/1775 train_time:79516ms step_avg:54.39ms
step:1463/1775 train_time:79600ms step_avg:54.41ms
step:1464/1775 train_time:79686ms step_avg:54.43ms
step:1465/1775 train_time:79770ms step_avg:54.45ms
step:1466/1775 train_time:79859ms step_avg:54.47ms
step:1467/1775 train_time:79943ms step_avg:54.49ms
step:1468/1775 train_time:80029ms step_avg:54.52ms
step:1469/1775 train_time:80112ms step_avg:54.54ms
step:1470/1775 train_time:80199ms step_avg:54.56ms
step:1471/1775 train_time:80282ms step_avg:54.58ms
step:1472/1775 train_time:80369ms step_avg:54.60ms
step:1473/1775 train_time:80453ms step_avg:54.62ms
step:1474/1775 train_time:80540ms step_avg:54.64ms
step:1475/1775 train_time:80623ms step_avg:54.66ms
step:1476/1775 train_time:80711ms step_avg:54.68ms
step:1477/1775 train_time:80795ms step_avg:54.70ms
step:1478/1775 train_time:80882ms step_avg:54.72ms
step:1479/1775 train_time:80965ms step_avg:54.74ms
step:1480/1775 train_time:81052ms step_avg:54.77ms
step:1481/1775 train_time:81136ms step_avg:54.78ms
step:1482/1775 train_time:81223ms step_avg:54.81ms
step:1483/1775 train_time:81307ms step_avg:54.83ms
step:1484/1775 train_time:81393ms step_avg:54.85ms
step:1485/1775 train_time:81478ms step_avg:54.87ms
step:1486/1775 train_time:81564ms step_avg:54.89ms
step:1487/1775 train_time:81649ms step_avg:54.91ms
step:1488/1775 train_time:81736ms step_avg:54.93ms
step:1489/1775 train_time:81821ms step_avg:54.95ms
step:1490/1775 train_time:81908ms step_avg:54.97ms
step:1491/1775 train_time:81991ms step_avg:54.99ms
step:1492/1775 train_time:82079ms step_avg:55.01ms
step:1493/1775 train_time:82163ms step_avg:55.03ms
step:1494/1775 train_time:82251ms step_avg:55.05ms
step:1495/1775 train_time:82335ms step_avg:55.07ms
step:1496/1775 train_time:82422ms step_avg:55.10ms
step:1497/1775 train_time:82507ms step_avg:55.11ms
step:1498/1775 train_time:82593ms step_avg:55.14ms
step:1499/1775 train_time:82677ms step_avg:55.16ms
step:1500/1775 train_time:82764ms step_avg:55.18ms
step:1500/1775 val_loss:3.3763 train_time:82859ms step_avg:55.24ms
step:1501/1775 train_time:82884ms step_avg:55.22ms
step:1502/1775 train_time:82940ms step_avg:55.22ms
step:1503/1775 train_time:83029ms step_avg:55.24ms
step:1504/1775 train_time:83115ms step_avg:55.26ms
step:1505/1775 train_time:83198ms step_avg:55.28ms
step:1506/1775 train_time:83284ms step_avg:55.30ms
step:1507/1775 train_time:83367ms step_avg:55.32ms
step:1508/1775 train_time:83454ms step_avg:55.34ms
step:1509/1775 train_time:83536ms step_avg:55.36ms
step:1510/1775 train_time:83622ms step_avg:55.38ms
step:1511/1775 train_time:83705ms step_avg:55.40ms
step:1512/1775 train_time:83792ms step_avg:55.42ms
step:1513/1775 train_time:83880ms step_avg:55.44ms
step:1514/1775 train_time:83970ms step_avg:55.46ms
step:1515/1775 train_time:84055ms step_avg:55.48ms
step:1516/1775 train_time:84142ms step_avg:55.50ms
step:1517/1775 train_time:84225ms step_avg:55.52ms
step:1518/1775 train_time:84311ms step_avg:55.54ms
step:1519/1775 train_time:84394ms step_avg:55.56ms
step:1520/1775 train_time:84482ms step_avg:55.58ms
step:1521/1775 train_time:84565ms step_avg:55.60ms
step:1522/1775 train_time:84651ms step_avg:55.62ms
step:1523/1775 train_time:84733ms step_avg:55.64ms
step:1524/1775 train_time:84821ms step_avg:55.66ms
step:1525/1775 train_time:84908ms step_avg:55.68ms
step:1526/1775 train_time:84995ms step_avg:55.70ms
step:1527/1775 train_time:85080ms step_avg:55.72ms
step:1528/1775 train_time:85167ms step_avg:55.74ms
step:1529/1775 train_time:85251ms step_avg:55.76ms
step:1530/1775 train_time:85336ms step_avg:55.78ms
step:1531/1775 train_time:85419ms step_avg:55.79ms
step:1532/1775 train_time:85505ms step_avg:55.81ms
step:1533/1775 train_time:85588ms step_avg:55.83ms
step:1534/1775 train_time:85675ms step_avg:55.85ms
step:1535/1775 train_time:85759ms step_avg:55.87ms
step:1536/1775 train_time:85847ms step_avg:55.89ms
step:1537/1775 train_time:85931ms step_avg:55.91ms
step:1538/1775 train_time:86020ms step_avg:55.93ms
step:1539/1775 train_time:86106ms step_avg:55.95ms
step:1540/1775 train_time:86191ms step_avg:55.97ms
step:1541/1775 train_time:86276ms step_avg:55.99ms
step:1542/1775 train_time:86362ms step_avg:56.01ms
step:1543/1775 train_time:86446ms step_avg:56.02ms
step:1544/1775 train_time:86531ms step_avg:56.04ms
step:1545/1775 train_time:86615ms step_avg:56.06ms
step:1546/1775 train_time:86702ms step_avg:56.08ms
step:1547/1775 train_time:86787ms step_avg:56.10ms
step:1548/1775 train_time:86874ms step_avg:56.12ms
step:1549/1775 train_time:86958ms step_avg:56.14ms
step:1550/1775 train_time:87046ms step_avg:56.16ms
step:1551/1775 train_time:87130ms step_avg:56.18ms
step:1552/1775 train_time:87216ms step_avg:56.20ms
step:1553/1775 train_time:87300ms step_avg:56.21ms
step:1554/1775 train_time:87387ms step_avg:56.23ms
step:1555/1775 train_time:87470ms step_avg:56.25ms
step:1556/1775 train_time:87558ms step_avg:56.27ms
step:1557/1775 train_time:87641ms step_avg:56.29ms
step:1558/1775 train_time:87728ms step_avg:56.31ms
step:1559/1775 train_time:87811ms step_avg:56.33ms
step:1560/1775 train_time:87898ms step_avg:56.35ms
step:1561/1775 train_time:87983ms step_avg:56.36ms
step:1562/1775 train_time:88070ms step_avg:56.38ms
step:1563/1775 train_time:88155ms step_avg:56.40ms
step:1564/1775 train_time:88242ms step_avg:56.42ms
step:1565/1775 train_time:88326ms step_avg:56.44ms
step:1566/1775 train_time:88413ms step_avg:56.46ms
step:1567/1775 train_time:88495ms step_avg:56.47ms
step:1568/1775 train_time:88583ms step_avg:56.49ms
step:1569/1775 train_time:88667ms step_avg:56.51ms
step:1570/1775 train_time:88754ms step_avg:56.53ms
step:1571/1775 train_time:88838ms step_avg:56.55ms
step:1572/1775 train_time:88926ms step_avg:56.57ms
step:1573/1775 train_time:89010ms step_avg:56.59ms
step:1574/1775 train_time:89098ms step_avg:56.61ms
step:1575/1775 train_time:89183ms step_avg:56.62ms
step:1576/1775 train_time:89269ms step_avg:56.64ms
step:1577/1775 train_time:89354ms step_avg:56.66ms
step:1578/1775 train_time:89440ms step_avg:56.68ms
step:1579/1775 train_time:89525ms step_avg:56.70ms
step:1580/1775 train_time:89611ms step_avg:56.72ms
step:1581/1775 train_time:89694ms step_avg:56.73ms
step:1582/1775 train_time:89780ms step_avg:56.75ms
step:1583/1775 train_time:89865ms step_avg:56.77ms
step:1584/1775 train_time:89951ms step_avg:56.79ms
step:1585/1775 train_time:90037ms step_avg:56.81ms
step:1586/1775 train_time:90125ms step_avg:56.83ms
step:1587/1775 train_time:90208ms step_avg:56.84ms
step:1588/1775 train_time:90296ms step_avg:56.86ms
step:1589/1775 train_time:90380ms step_avg:56.88ms
step:1590/1775 train_time:90467ms step_avg:56.90ms
step:1591/1775 train_time:90550ms step_avg:56.91ms
step:1592/1775 train_time:90638ms step_avg:56.93ms
step:1593/1775 train_time:90722ms step_avg:56.95ms
step:1594/1775 train_time:90808ms step_avg:56.97ms
step:1595/1775 train_time:90892ms step_avg:56.99ms
step:1596/1775 train_time:90979ms step_avg:57.00ms
step:1597/1775 train_time:91064ms step_avg:57.02ms
step:1598/1775 train_time:91151ms step_avg:57.04ms
step:1599/1775 train_time:91236ms step_avg:57.06ms
step:1600/1775 train_time:91324ms step_avg:57.08ms
step:1601/1775 train_time:91407ms step_avg:57.09ms
step:1602/1775 train_time:91494ms step_avg:57.11ms
step:1603/1775 train_time:91578ms step_avg:57.13ms
step:1604/1775 train_time:91665ms step_avg:57.15ms
step:1605/1775 train_time:91749ms step_avg:57.16ms
step:1606/1775 train_time:91835ms step_avg:57.18ms
step:1607/1775 train_time:91919ms step_avg:57.20ms
step:1608/1775 train_time:92007ms step_avg:57.22ms
step:1609/1775 train_time:92091ms step_avg:57.23ms
step:1610/1775 train_time:92178ms step_avg:57.25ms
step:1611/1775 train_time:92262ms step_avg:57.27ms
step:1612/1775 train_time:92349ms step_avg:57.29ms
step:1613/1775 train_time:92434ms step_avg:57.31ms
step:1614/1775 train_time:92520ms step_avg:57.32ms
step:1615/1775 train_time:92605ms step_avg:57.34ms
step:1616/1775 train_time:92690ms step_avg:57.36ms
step:1617/1775 train_time:92775ms step_avg:57.38ms
step:1618/1775 train_time:92863ms step_avg:57.39ms
step:1619/1775 train_time:92948ms step_avg:57.41ms
step:1620/1775 train_time:93036ms step_avg:57.43ms
step:1621/1775 train_time:93119ms step_avg:57.45ms
step:1622/1775 train_time:93206ms step_avg:57.46ms
step:1623/1775 train_time:93289ms step_avg:57.48ms
step:1624/1775 train_time:93377ms step_avg:57.50ms
step:1625/1775 train_time:93461ms step_avg:57.51ms
step:1626/1775 train_time:93548ms step_avg:57.53ms
step:1627/1775 train_time:93631ms step_avg:57.55ms
step:1628/1775 train_time:93719ms step_avg:57.57ms
step:1629/1775 train_time:93803ms step_avg:57.58ms
step:1630/1775 train_time:93890ms step_avg:57.60ms
step:1631/1775 train_time:93975ms step_avg:57.62ms
step:1632/1775 train_time:94063ms step_avg:57.64ms
step:1633/1775 train_time:94147ms step_avg:57.65ms
step:1634/1775 train_time:94233ms step_avg:57.67ms
step:1635/1775 train_time:94317ms step_avg:57.69ms
step:1636/1775 train_time:94405ms step_avg:57.70ms
step:1637/1775 train_time:94489ms step_avg:57.72ms
step:1638/1775 train_time:94575ms step_avg:57.74ms
step:1639/1775 train_time:94658ms step_avg:57.75ms
step:1640/1775 train_time:94747ms step_avg:57.77ms
step:1641/1775 train_time:94829ms step_avg:57.79ms
step:1642/1775 train_time:94916ms step_avg:57.81ms
step:1643/1775 train_time:95000ms step_avg:57.82ms
step:1644/1775 train_time:95087ms step_avg:57.84ms
step:1645/1775 train_time:95170ms step_avg:57.85ms
step:1646/1775 train_time:95258ms step_avg:57.87ms
step:1647/1775 train_time:95344ms step_avg:57.89ms
step:1648/1775 train_time:95429ms step_avg:57.91ms
step:1649/1775 train_time:95514ms step_avg:57.92ms
step:1650/1775 train_time:95599ms step_avg:57.94ms
step:1651/1775 train_time:95685ms step_avg:57.96ms
step:1652/1775 train_time:95770ms step_avg:57.97ms
step:1653/1775 train_time:95855ms step_avg:57.99ms
step:1654/1775 train_time:95942ms step_avg:58.01ms
step:1655/1775 train_time:96027ms step_avg:58.02ms
step:1656/1775 train_time:96112ms step_avg:58.04ms
step:1657/1775 train_time:96196ms step_avg:58.05ms
step:1658/1775 train_time:96284ms step_avg:58.07ms
step:1659/1775 train_time:96368ms step_avg:58.09ms
step:1660/1775 train_time:96455ms step_avg:58.11ms
step:1661/1775 train_time:96539ms step_avg:58.12ms
step:1662/1775 train_time:96626ms step_avg:58.14ms
step:1663/1775 train_time:96709ms step_avg:58.15ms
step:1664/1775 train_time:96796ms step_avg:58.17ms
step:1665/1775 train_time:96880ms step_avg:58.19ms
step:1666/1775 train_time:96967ms step_avg:58.20ms
step:1667/1775 train_time:97051ms step_avg:58.22ms
step:1668/1775 train_time:97139ms step_avg:58.24ms
step:1669/1775 train_time:97223ms step_avg:58.25ms
step:1670/1775 train_time:97309ms step_avg:58.27ms
step:1671/1775 train_time:97394ms step_avg:58.28ms
step:1672/1775 train_time:97480ms step_avg:58.30ms
step:1673/1775 train_time:97565ms step_avg:58.32ms
step:1674/1775 train_time:97652ms step_avg:58.33ms
step:1675/1775 train_time:97736ms step_avg:58.35ms
step:1676/1775 train_time:97822ms step_avg:58.37ms
step:1677/1775 train_time:97906ms step_avg:58.38ms
step:1678/1775 train_time:97992ms step_avg:58.40ms
step:1679/1775 train_time:98077ms step_avg:58.41ms
step:1680/1775 train_time:98164ms step_avg:58.43ms
step:1681/1775 train_time:98248ms step_avg:58.45ms
step:1682/1775 train_time:98335ms step_avg:58.46ms
step:1683/1775 train_time:98419ms step_avg:58.48ms
step:1684/1775 train_time:98506ms step_avg:58.50ms
step:1685/1775 train_time:98589ms step_avg:58.51ms
step:1686/1775 train_time:98677ms step_avg:58.53ms
step:1687/1775 train_time:98760ms step_avg:58.54ms
step:1688/1775 train_time:98848ms step_avg:58.56ms
step:1689/1775 train_time:98932ms step_avg:58.57ms
step:1690/1775 train_time:99019ms step_avg:58.59ms
step:1691/1775 train_time:99103ms step_avg:58.61ms
step:1692/1775 train_time:99190ms step_avg:58.62ms
step:1693/1775 train_time:99273ms step_avg:58.64ms
step:1694/1775 train_time:99361ms step_avg:58.65ms
step:1695/1775 train_time:99445ms step_avg:58.67ms
step:1696/1775 train_time:99531ms step_avg:58.69ms
step:1697/1775 train_time:99615ms step_avg:58.70ms
step:1698/1775 train_time:99703ms step_avg:58.72ms
step:1699/1775 train_time:99788ms step_avg:58.73ms
step:1700/1775 train_time:99874ms step_avg:58.75ms
step:1701/1775 train_time:99958ms step_avg:58.76ms
step:1702/1775 train_time:100045ms step_avg:58.78ms
step:1703/1775 train_time:100129ms step_avg:58.80ms
step:1704/1775 train_time:100216ms step_avg:58.81ms
step:1705/1775 train_time:100300ms step_avg:58.83ms
step:1706/1775 train_time:100387ms step_avg:58.84ms
step:1707/1775 train_time:100470ms step_avg:58.86ms
step:1708/1775 train_time:100556ms step_avg:58.87ms
step:1709/1775 train_time:100641ms step_avg:58.89ms
step:1710/1775 train_time:100727ms step_avg:58.90ms
step:1711/1775 train_time:100811ms step_avg:58.92ms
step:1712/1775 train_time:100898ms step_avg:58.94ms
step:1713/1775 train_time:100981ms step_avg:58.95ms
step:1714/1775 train_time:101070ms step_avg:58.97ms
step:1715/1775 train_time:101155ms step_avg:58.98ms
step:1716/1775 train_time:101241ms step_avg:59.00ms
step:1717/1775 train_time:101324ms step_avg:59.01ms
step:1718/1775 train_time:101411ms step_avg:59.03ms
step:1719/1775 train_time:101495ms step_avg:59.04ms
step:1720/1775 train_time:101583ms step_avg:59.06ms
step:1721/1775 train_time:101666ms step_avg:59.07ms
step:1722/1775 train_time:101753ms step_avg:59.09ms
step:1723/1775 train_time:101837ms step_avg:59.10ms
step:1724/1775 train_time:101924ms step_avg:59.12ms
step:1725/1775 train_time:102008ms step_avg:59.14ms
step:1726/1775 train_time:102095ms step_avg:59.15ms
step:1727/1775 train_time:102179ms step_avg:59.17ms
step:1728/1775 train_time:102266ms step_avg:59.18ms
step:1729/1775 train_time:102351ms step_avg:59.20ms
step:1730/1775 train_time:102438ms step_avg:59.21ms
step:1731/1775 train_time:102521ms step_avg:59.23ms
step:1732/1775 train_time:102608ms step_avg:59.24ms
step:1733/1775 train_time:102692ms step_avg:59.26ms
step:1734/1775 train_time:102780ms step_avg:59.27ms
step:1735/1775 train_time:102863ms step_avg:59.29ms
step:1736/1775 train_time:102954ms step_avg:59.31ms
step:1737/1775 train_time:103040ms step_avg:59.32ms
step:1738/1775 train_time:103128ms step_avg:59.34ms
step:1739/1775 train_time:103211ms step_avg:59.35ms
step:1740/1775 train_time:103299ms step_avg:59.37ms
step:1741/1775 train_time:103384ms step_avg:59.38ms
step:1742/1775 train_time:103470ms step_avg:59.40ms
step:1743/1775 train_time:103555ms step_avg:59.41ms
step:1744/1775 train_time:103642ms step_avg:59.43ms
step:1745/1775 train_time:103726ms step_avg:59.44ms
step:1746/1775 train_time:103813ms step_avg:59.46ms
step:1747/1775 train_time:103899ms step_avg:59.47ms
step:1748/1775 train_time:103987ms step_avg:59.49ms
step:1749/1775 train_time:104070ms step_avg:59.50ms
step:1750/1775 train_time:104158ms step_avg:59.52ms
step:1750/1775 val_loss:3.2845 train_time:104255ms step_avg:59.57ms
step:1751/1775 train_time:104280ms step_avg:59.55ms
step:1752/1775 train_time:104333ms step_avg:59.55ms
step:1753/1775 train_time:104423ms step_avg:59.57ms
step:1754/1775 train_time:104511ms step_avg:59.58ms
step:1755/1775 train_time:104596ms step_avg:59.60ms
step:1756/1775 train_time:104683ms step_avg:59.61ms
step:1757/1775 train_time:104767ms step_avg:59.63ms
step:1758/1775 train_time:104852ms step_avg:59.64ms
step:1759/1775 train_time:104936ms step_avg:59.66ms
step:1760/1775 train_time:105024ms step_avg:59.67ms
step:1761/1775 train_time:105107ms step_avg:59.69ms
step:1762/1775 train_time:105195ms step_avg:59.70ms
step:1763/1775 train_time:105282ms step_avg:59.72ms
step:1764/1775 train_time:105371ms step_avg:59.73ms
step:1765/1775 train_time:105457ms step_avg:59.75ms
step:1766/1775 train_time:105546ms step_avg:59.77ms
step:1767/1775 train_time:105629ms step_avg:59.78ms
step:1768/1775 train_time:105716ms step_avg:59.79ms
step:1769/1775 train_time:105798ms step_avg:59.81ms
step:1770/1775 train_time:105885ms step_avg:59.82ms
step:1771/1775 train_time:105969ms step_avg:59.84ms
step:1772/1775 train_time:106056ms step_avg:59.85ms
step:1773/1775 train_time:106141ms step_avg:59.86ms
step:1774/1775 train_time:106228ms step_avg:59.88ms
step:1775/1775 train_time:106313ms step_avg:59.89ms
step:1775/1775 val_loss:3.2778 train_time:106411ms step_avg:59.95ms
peak memory allocated: 29739 MiB reserved: 44898 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,  #
                                 M, N, K,  #
                                 BLOCK_SIZE_M: tl.constexpr,  #
                                 BLOCK_SIZE_N: tl.constexpr,  #
                                 BLOCK_SIZE_K: tl.constexpr,  #
                                 GROUP_SIZE_M: tl.constexpr,  #
                                 NUM_SMS: tl.constexpr,  #
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,#
        M, N, K,  #
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,  #
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        #x = F.linear(x, self.c_fc.type_as(x))
        #x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        #x = F.linear(x, self.c_proj.T.type_as(x))
        #FusedLinearReLUSquareFunction.apply(x, self.c_fc.type_as(x), self.c_proj.type_as(x))
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.3 (main, Nov  6 2025, 13:44:16) [GCC 13.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Jan 10 23:13:45 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:8D:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    1183MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           15014      C   /home/ubuntu/venv/bin/python3          1174MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8310 train_time:0ms step_avg:0.28ms
step:1/1775 train_time:257ms step_avg:256.86ms
step:2/1775 train_time:502ms step_avg:251.10ms
step:3/1775 train_time:748ms step_avg:249.24ms
step:4/1775 train_time:994ms step_avg:248.43ms
step:5/1775 train_time:1244ms step_avg:248.76ms
step:6/1775 train_time:1493ms step_avg:248.76ms
step:7/1775 train_time:1739ms step_avg:248.48ms
step:8/1775 train_time:1985ms step_avg:248.18ms
step:9/1775 train_time:2233ms step_avg:248.07ms
step:10/1775 train_time:2483ms step_avg:248.28ms
step:11/1775 train_time:2731ms step_avg:248.25ms
step:12/1775 train_time:2977ms step_avg:248.10ms
step:13/1775 train_time:3226ms step_avg:248.13ms
step:14/1775 train_time:3474ms step_avg:248.11ms
step:15/1775 train_time:3721ms step_avg:248.04ms
step:16/1775 train_time:3968ms step_avg:247.99ms
step:17/1775 train_time:4215ms step_avg:247.95ms
step:18/1775 train_time:4464ms step_avg:247.98ms
step:19/1775 train_time:4711ms step_avg:247.95ms
step:20/1775 train_time:4958ms step_avg:247.91ms
step:21/1775 train_time:5206ms step_avg:247.89ms
step:22/1775 train_time:5453ms step_avg:247.88ms
step:23/1775 train_time:5701ms step_avg:247.88ms
step:24/1775 train_time:5948ms step_avg:247.84ms
step:25/1775 train_time:6195ms step_avg:247.78ms
step:26/1775 train_time:6443ms step_avg:247.80ms
step:27/1775 train_time:6691ms step_avg:247.81ms
step:28/1775 train_time:6940ms step_avg:247.85ms
step:29/1775 train_time:7188ms step_avg:247.88ms
step:30/1775 train_time:7437ms step_avg:247.91ms
step:31/1775 train_time:7685ms step_avg:247.91ms
step:32/1775 train_time:7934ms step_avg:247.94ms
step:33/1775 train_time:8182ms step_avg:247.94ms
step:34/1775 train_time:8431ms step_avg:247.97ms
step:35/1775 train_time:8680ms step_avg:248.00ms
step:36/1775 train_time:8928ms step_avg:247.99ms
step:37/1775 train_time:9175ms step_avg:247.97ms
step:38/1775 train_time:9423ms step_avg:247.96ms
step:39/1775 train_time:9671ms step_avg:247.99ms
step:40/1775 train_time:9919ms step_avg:247.98ms
step:41/1775 train_time:10167ms step_avg:247.97ms
step:42/1775 train_time:10413ms step_avg:247.94ms
step:43/1775 train_time:10661ms step_avg:247.94ms
step:44/1775 train_time:10909ms step_avg:247.94ms
step:45/1775 train_time:11157ms step_avg:247.94ms
step:46/1775 train_time:11405ms step_avg:247.94ms
step:47/1775 train_time:11654ms step_avg:247.97ms
step:48/1775 train_time:11902ms step_avg:247.96ms
step:49/1775 train_time:12149ms step_avg:247.94ms
step:50/1775 train_time:12396ms step_avg:247.92ms
step:51/1775 train_time:12645ms step_avg:247.93ms
step:52/1775 train_time:12892ms step_avg:247.93ms
step:53/1775 train_time:13141ms step_avg:247.95ms
step:54/1775 train_time:13389ms step_avg:247.95ms
step:55/1775 train_time:13636ms step_avg:247.94ms
step:56/1775 train_time:13883ms step_avg:247.91ms
step:57/1775 train_time:14131ms step_avg:247.92ms
step:58/1775 train_time:14377ms step_avg:247.88ms
step:59/1775 train_time:14626ms step_avg:247.89ms
step:60/1775 train_time:14874ms step_avg:247.90ms
step:61/1775 train_time:15123ms step_avg:247.91ms
step:62/1775 train_time:15371ms step_avg:247.93ms
step:63/1775 train_time:15620ms step_avg:247.93ms
step:64/1775 train_time:15867ms step_avg:247.92ms
step:65/1775 train_time:16114ms step_avg:247.91ms
step:66/1775 train_time:16363ms step_avg:247.93ms
step:67/1775 train_time:16612ms step_avg:247.94ms
step:68/1775 train_time:16861ms step_avg:247.95ms
step:69/1775 train_time:17109ms step_avg:247.96ms
step:70/1775 train_time:17357ms step_avg:247.96ms
step:71/1775 train_time:17605ms step_avg:247.96ms
step:72/1775 train_time:17855ms step_avg:247.99ms
step:73/1775 train_time:18102ms step_avg:247.98ms
step:74/1775 train_time:18351ms step_avg:247.99ms
step:75/1775 train_time:18600ms step_avg:248.00ms
step:76/1775 train_time:18846ms step_avg:247.98ms
step:77/1775 train_time:19094ms step_avg:247.97ms
step:78/1775 train_time:19342ms step_avg:247.98ms
step:79/1775 train_time:19592ms step_avg:248.00ms
step:80/1775 train_time:19843ms step_avg:248.03ms
step:81/1775 train_time:20090ms step_avg:248.03ms
step:82/1775 train_time:20337ms step_avg:248.02ms
step:83/1775 train_time:20586ms step_avg:248.02ms
step:84/1775 train_time:20834ms step_avg:248.02ms
step:85/1775 train_time:21083ms step_avg:248.03ms
step:86/1775 train_time:21331ms step_avg:248.04ms
step:87/1775 train_time:21581ms step_avg:248.05ms
step:88/1775 train_time:21828ms step_avg:248.04ms
step:89/1775 train_time:22074ms step_avg:248.02ms
step:90/1775 train_time:22321ms step_avg:248.01ms
step:91/1775 train_time:22570ms step_avg:248.02ms
step:92/1775 train_time:22818ms step_avg:248.03ms
step:93/1775 train_time:23065ms step_avg:248.01ms
step:94/1775 train_time:23312ms step_avg:248.01ms
step:95/1775 train_time:23560ms step_avg:248.00ms
step:96/1775 train_time:23808ms step_avg:248.00ms
step:97/1775 train_time:24054ms step_avg:247.98ms
step:98/1775 train_time:24302ms step_avg:247.98ms
step:99/1775 train_time:24550ms step_avg:247.98ms
step:100/1775 train_time:24799ms step_avg:247.99ms
step:101/1775 train_time:25047ms step_avg:247.99ms
step:102/1775 train_time:25293ms step_avg:247.97ms
step:103/1775 train_time:25542ms step_avg:247.98ms
step:104/1775 train_time:25791ms step_avg:248.00ms
step:105/1775 train_time:26042ms step_avg:248.02ms
step:106/1775 train_time:26290ms step_avg:248.02ms
step:107/1775 train_time:26537ms step_avg:248.00ms
step:108/1775 train_time:26784ms step_avg:248.00ms
step:109/1775 train_time:27034ms step_avg:248.02ms
step:110/1775 train_time:27283ms step_avg:248.03ms
step:111/1775 train_time:27532ms step_avg:248.03ms
step:112/1775 train_time:27780ms step_avg:248.04ms
step:113/1775 train_time:28029ms step_avg:248.04ms
step:114/1775 train_time:28278ms step_avg:248.05ms
step:115/1775 train_time:28523ms step_avg:248.03ms
step:116/1775 train_time:28771ms step_avg:248.03ms
step:117/1775 train_time:29020ms step_avg:248.03ms
step:118/1775 train_time:29269ms step_avg:248.04ms
step:119/1775 train_time:29517ms step_avg:248.04ms
step:120/1775 train_time:29764ms step_avg:248.04ms
step:121/1775 train_time:30013ms step_avg:248.04ms
step:122/1775 train_time:30262ms step_avg:248.05ms
step:123/1775 train_time:30511ms step_avg:248.06ms
step:124/1775 train_time:30758ms step_avg:248.05ms
step:125/1775 train_time:31005ms step_avg:248.04ms
step:126/1775 train_time:31252ms step_avg:248.04ms
step:127/1775 train_time:31501ms step_avg:248.04ms
step:128/1775 train_time:31750ms step_avg:248.04ms
step:129/1775 train_time:31996ms step_avg:248.03ms
step:130/1775 train_time:32244ms step_avg:248.03ms
step:131/1775 train_time:32492ms step_avg:248.03ms
step:132/1775 train_time:32741ms step_avg:248.04ms
step:133/1775 train_time:32990ms step_avg:248.04ms
step:134/1775 train_time:33237ms step_avg:248.04ms
step:135/1775 train_time:33484ms step_avg:248.03ms
step:136/1775 train_time:33730ms step_avg:248.02ms
step:137/1775 train_time:33976ms step_avg:248.00ms
step:138/1775 train_time:34224ms step_avg:248.00ms
step:139/1775 train_time:34471ms step_avg:247.99ms
step:140/1775 train_time:34718ms step_avg:247.99ms
step:141/1775 train_time:34965ms step_avg:247.98ms
step:142/1775 train_time:35212ms step_avg:247.97ms
step:143/1775 train_time:35460ms step_avg:247.97ms
step:144/1775 train_time:35707ms step_avg:247.96ms
step:145/1775 train_time:35953ms step_avg:247.95ms
step:146/1775 train_time:36202ms step_avg:247.96ms
step:147/1775 train_time:36449ms step_avg:247.95ms
step:148/1775 train_time:36698ms step_avg:247.96ms
step:149/1775 train_time:36946ms step_avg:247.96ms
step:150/1775 train_time:37193ms step_avg:247.95ms
step:151/1775 train_time:37440ms step_avg:247.95ms
step:152/1775 train_time:37688ms step_avg:247.95ms
step:153/1775 train_time:37935ms step_avg:247.94ms
step:154/1775 train_time:38181ms step_avg:247.93ms
step:155/1775 train_time:38427ms step_avg:247.92ms
step:156/1775 train_time:38675ms step_avg:247.91ms
step:157/1775 train_time:38923ms step_avg:247.92ms
step:158/1775 train_time:39171ms step_avg:247.92ms
step:159/1775 train_time:39420ms step_avg:247.93ms
step:160/1775 train_time:39667ms step_avg:247.92ms
step:161/1775 train_time:39913ms step_avg:247.91ms
step:162/1775 train_time:40160ms step_avg:247.90ms
step:163/1775 train_time:40406ms step_avg:247.89ms
step:164/1775 train_time:40654ms step_avg:247.89ms
step:165/1775 train_time:40902ms step_avg:247.89ms
step:166/1775 train_time:41150ms step_avg:247.89ms
step:167/1775 train_time:41397ms step_avg:247.88ms
step:168/1775 train_time:41644ms step_avg:247.88ms
step:169/1775 train_time:41892ms step_avg:247.88ms
step:170/1775 train_time:42139ms step_avg:247.88ms
step:171/1775 train_time:42387ms step_avg:247.87ms
step:172/1775 train_time:42633ms step_avg:247.87ms
step:173/1775 train_time:42881ms step_avg:247.87ms
step:174/1775 train_time:43130ms step_avg:247.87ms
step:175/1775 train_time:43377ms step_avg:247.87ms
step:176/1775 train_time:43623ms step_avg:247.86ms
step:177/1775 train_time:43871ms step_avg:247.86ms
step:178/1775 train_time:44117ms step_avg:247.85ms
step:179/1775 train_time:44363ms step_avg:247.84ms
step:180/1775 train_time:44609ms step_avg:247.83ms
step:181/1775 train_time:44856ms step_avg:247.82ms
step:182/1775 train_time:45104ms step_avg:247.82ms
step:183/1775 train_time:45351ms step_avg:247.82ms
step:184/1775 train_time:45597ms step_avg:247.81ms
step:185/1775 train_time:45843ms step_avg:247.80ms
step:186/1775 train_time:46089ms step_avg:247.79ms
step:187/1775 train_time:46337ms step_avg:247.79ms
step:188/1775 train_time:46584ms step_avg:247.79ms
step:189/1775 train_time:46830ms step_avg:247.78ms
step:190/1775 train_time:47076ms step_avg:247.77ms
step:191/1775 train_time:47323ms step_avg:247.76ms
step:192/1775 train_time:47571ms step_avg:247.76ms
step:193/1775 train_time:47818ms step_avg:247.76ms
step:194/1775 train_time:48064ms step_avg:247.75ms
step:195/1775 train_time:48312ms step_avg:247.75ms
step:196/1775 train_time:48562ms step_avg:247.77ms
step:197/1775 train_time:48811ms step_avg:247.77ms
step:198/1775 train_time:49060ms step_avg:247.78ms
step:199/1775 train_time:49305ms step_avg:247.76ms
step:200/1775 train_time:49552ms step_avg:247.76ms
step:201/1775 train_time:49800ms step_avg:247.76ms
step:202/1775 train_time:50047ms step_avg:247.76ms
step:203/1775 train_time:50294ms step_avg:247.75ms
step:204/1775 train_time:50541ms step_avg:247.75ms
step:205/1775 train_time:50789ms step_avg:247.75ms
step:206/1775 train_time:51036ms step_avg:247.75ms
step:207/1775 train_time:51283ms step_avg:247.74ms
step:208/1775 train_time:51531ms step_avg:247.74ms
step:209/1775 train_time:51778ms step_avg:247.74ms
step:210/1775 train_time:52025ms step_avg:247.74ms
step:211/1775 train_time:52272ms step_avg:247.74ms
step:212/1775 train_time:52522ms step_avg:247.74ms
step:213/1775 train_time:52770ms step_avg:247.75ms
step:214/1775 train_time:53016ms step_avg:247.74ms
step:215/1775 train_time:53263ms step_avg:247.73ms
step:216/1775 train_time:53511ms step_avg:247.73ms
step:217/1775 train_time:53758ms step_avg:247.73ms
step:218/1775 train_time:54005ms step_avg:247.73ms
step:219/1775 train_time:54252ms step_avg:247.73ms
step:220/1775 train_time:54498ms step_avg:247.72ms
step:221/1775 train_time:54743ms step_avg:247.71ms
step:222/1775 train_time:54992ms step_avg:247.71ms
step:223/1775 train_time:55239ms step_avg:247.71ms
step:224/1775 train_time:55486ms step_avg:247.71ms
step:225/1775 train_time:55734ms step_avg:247.71ms
step:226/1775 train_time:55984ms step_avg:247.72ms
step:227/1775 train_time:56231ms step_avg:247.71ms
step:228/1775 train_time:56477ms step_avg:247.71ms
step:229/1775 train_time:56724ms step_avg:247.70ms
step:230/1775 train_time:56971ms step_avg:247.70ms
step:231/1775 train_time:57218ms step_avg:247.70ms
step:232/1775 train_time:57464ms step_avg:247.69ms
step:233/1775 train_time:57710ms step_avg:247.68ms
step:234/1775 train_time:57956ms step_avg:247.68ms
step:235/1775 train_time:58203ms step_avg:247.67ms
step:236/1775 train_time:58450ms step_avg:247.67ms
step:237/1775 train_time:58697ms step_avg:247.67ms
step:238/1775 train_time:58944ms step_avg:247.66ms
step:239/1775 train_time:59192ms step_avg:247.66ms
step:240/1775 train_time:59440ms step_avg:247.67ms
step:241/1775 train_time:59687ms step_avg:247.66ms
step:242/1775 train_time:59933ms step_avg:247.66ms
step:243/1775 train_time:60181ms step_avg:247.66ms
step:244/1775 train_time:60430ms step_avg:247.66ms
step:245/1775 train_time:60676ms step_avg:247.66ms
step:246/1775 train_time:60923ms step_avg:247.65ms
step:247/1775 train_time:61170ms step_avg:247.65ms
step:248/1775 train_time:61416ms step_avg:247.64ms
step:249/1775 train_time:61664ms step_avg:247.65ms
step:250/1775 train_time:61911ms step_avg:247.64ms
step:250/1775 val_loss:4.6007 train_time:61956ms step_avg:247.82ms
step:251/1775 train_time:62159ms step_avg:247.65ms
step:252/1775 train_time:62404ms step_avg:247.63ms
step:253/1775 train_time:62652ms step_avg:247.64ms
step:254/1775 train_time:62898ms step_avg:247.63ms
step:255/1775 train_time:63144ms step_avg:247.62ms
step:256/1775 train_time:63390ms step_avg:247.62ms
step:257/1775 train_time:63637ms step_avg:247.61ms
step:258/1775 train_time:63884ms step_avg:247.61ms
step:259/1775 train_time:64131ms step_avg:247.61ms
step:260/1775 train_time:64377ms step_avg:247.60ms
step:261/1775 train_time:64623ms step_avg:247.60ms
step:262/1775 train_time:64869ms step_avg:247.59ms
step:263/1775 train_time:65116ms step_avg:247.59ms
step:264/1775 train_time:65362ms step_avg:247.58ms
step:265/1775 train_time:65609ms step_avg:247.58ms
step:266/1775 train_time:65856ms step_avg:247.58ms
step:267/1775 train_time:66103ms step_avg:247.58ms
step:268/1775 train_time:66350ms step_avg:247.57ms
step:269/1775 train_time:66595ms step_avg:247.57ms
step:270/1775 train_time:66843ms step_avg:247.57ms
step:271/1775 train_time:67088ms step_avg:247.56ms
step:272/1775 train_time:67334ms step_avg:247.55ms
step:273/1775 train_time:67581ms step_avg:247.55ms
step:274/1775 train_time:67828ms step_avg:247.55ms
step:275/1775 train_time:68073ms step_avg:247.54ms
step:276/1775 train_time:68320ms step_avg:247.54ms
step:277/1775 train_time:68565ms step_avg:247.53ms
step:278/1775 train_time:68813ms step_avg:247.53ms
step:279/1775 train_time:69059ms step_avg:247.52ms
step:280/1775 train_time:69305ms step_avg:247.52ms
step:281/1775 train_time:69552ms step_avg:247.52ms
step:282/1775 train_time:69798ms step_avg:247.51ms
step:283/1775 train_time:70044ms step_avg:247.51ms
step:284/1775 train_time:70293ms step_avg:247.51ms
step:285/1775 train_time:70538ms step_avg:247.50ms
step:286/1775 train_time:70784ms step_avg:247.50ms
step:287/1775 train_time:71030ms step_avg:247.49ms
step:288/1775 train_time:71276ms step_avg:247.49ms
step:289/1775 train_time:71523ms step_avg:247.48ms
step:290/1775 train_time:71770ms step_avg:247.48ms
step:291/1775 train_time:72015ms step_avg:247.47ms
step:292/1775 train_time:72261ms step_avg:247.47ms
step:293/1775 train_time:72508ms step_avg:247.47ms
step:294/1775 train_time:72754ms step_avg:247.46ms
step:295/1775 train_time:73000ms step_avg:247.46ms
step:296/1775 train_time:73245ms step_avg:247.45ms
step:297/1775 train_time:73491ms step_avg:247.44ms
step:298/1775 train_time:73737ms step_avg:247.44ms
step:299/1775 train_time:73985ms step_avg:247.44ms
step:300/1775 train_time:74231ms step_avg:247.44ms
step:301/1775 train_time:74476ms step_avg:247.43ms
step:302/1775 train_time:74722ms step_avg:247.42ms
step:303/1775 train_time:74969ms step_avg:247.42ms
step:304/1775 train_time:75215ms step_avg:247.42ms
step:305/1775 train_time:75460ms step_avg:247.41ms
step:306/1775 train_time:75706ms step_avg:247.41ms
step:307/1775 train_time:75954ms step_avg:247.41ms
step:308/1775 train_time:76201ms step_avg:247.41ms
step:309/1775 train_time:76447ms step_avg:247.40ms
step:310/1775 train_time:76693ms step_avg:247.40ms
step:311/1775 train_time:76940ms step_avg:247.39ms
step:312/1775 train_time:77187ms step_avg:247.39ms
step:313/1775 train_time:77434ms step_avg:247.39ms
step:314/1775 train_time:77679ms step_avg:247.39ms
step:315/1775 train_time:77925ms step_avg:247.38ms
step:316/1775 train_time:78172ms step_avg:247.38ms
step:317/1775 train_time:78418ms step_avg:247.38ms
step:318/1775 train_time:78664ms step_avg:247.37ms
step:319/1775 train_time:78912ms step_avg:247.37ms
step:320/1775 train_time:79157ms step_avg:247.37ms
step:321/1775 train_time:79403ms step_avg:247.36ms
step:322/1775 train_time:79650ms step_avg:247.36ms
step:323/1775 train_time:79896ms step_avg:247.36ms
step:324/1775 train_time:80143ms step_avg:247.35ms
step:325/1775 train_time:80388ms step_avg:247.35ms
step:326/1775 train_time:80635ms step_avg:247.35ms
step:327/1775 train_time:80881ms step_avg:247.34ms
step:328/1775 train_time:81126ms step_avg:247.34ms
step:329/1775 train_time:81372ms step_avg:247.33ms
step:330/1775 train_time:81618ms step_avg:247.33ms
step:331/1775 train_time:81863ms step_avg:247.32ms
step:332/1775 train_time:82110ms step_avg:247.32ms
step:333/1775 train_time:82356ms step_avg:247.32ms
step:334/1775 train_time:82602ms step_avg:247.31ms
step:335/1775 train_time:82849ms step_avg:247.31ms
step:336/1775 train_time:83096ms step_avg:247.31ms
step:337/1775 train_time:83342ms step_avg:247.31ms
step:338/1775 train_time:83587ms step_avg:247.30ms
step:339/1775 train_time:83833ms step_avg:247.30ms
step:340/1775 train_time:84080ms step_avg:247.29ms
step:341/1775 train_time:84326ms step_avg:247.29ms
step:342/1775 train_time:84571ms step_avg:247.28ms
step:343/1775 train_time:84817ms step_avg:247.28ms
step:344/1775 train_time:85064ms step_avg:247.28ms
step:345/1775 train_time:85313ms step_avg:247.28ms
step:346/1775 train_time:85558ms step_avg:247.28ms
step:347/1775 train_time:85803ms step_avg:247.27ms
step:348/1775 train_time:86050ms step_avg:247.27ms
step:349/1775 train_time:86295ms step_avg:247.26ms
step:350/1775 train_time:86543ms step_avg:247.26ms
step:351/1775 train_time:86788ms step_avg:247.26ms
step:352/1775 train_time:87034ms step_avg:247.25ms
step:353/1775 train_time:87281ms step_avg:247.25ms
step:354/1775 train_time:87526ms step_avg:247.25ms
step:355/1775 train_time:87774ms step_avg:247.25ms
step:356/1775 train_time:88020ms step_avg:247.25ms
step:357/1775 train_time:88265ms step_avg:247.24ms
step:358/1775 train_time:88511ms step_avg:247.24ms
step:359/1775 train_time:88757ms step_avg:247.23ms
step:360/1775 train_time:89003ms step_avg:247.23ms
step:361/1775 train_time:89248ms step_avg:247.22ms
step:362/1775 train_time:89494ms step_avg:247.22ms
step:363/1775 train_time:89738ms step_avg:247.21ms
step:364/1775 train_time:89984ms step_avg:247.21ms
step:365/1775 train_time:90230ms step_avg:247.21ms
step:366/1775 train_time:90476ms step_avg:247.20ms
step:367/1775 train_time:90723ms step_avg:247.20ms
step:368/1775 train_time:90970ms step_avg:247.20ms
step:369/1775 train_time:91215ms step_avg:247.19ms
step:370/1775 train_time:91460ms step_avg:247.19ms
step:371/1775 train_time:91705ms step_avg:247.18ms
step:372/1775 train_time:91951ms step_avg:247.18ms
step:373/1775 train_time:92197ms step_avg:247.18ms
step:374/1775 train_time:92444ms step_avg:247.18ms
step:375/1775 train_time:92689ms step_avg:247.17ms
step:376/1775 train_time:92935ms step_avg:247.17ms
step:377/1775 train_time:93183ms step_avg:247.17ms
step:378/1775 train_time:93428ms step_avg:247.17ms
step:379/1775 train_time:93674ms step_avg:247.16ms
step:380/1775 train_time:93921ms step_avg:247.16ms
step:381/1775 train_time:94168ms step_avg:247.16ms
step:382/1775 train_time:94415ms step_avg:247.16ms
step:383/1775 train_time:94661ms step_avg:247.16ms
step:384/1775 train_time:94906ms step_avg:247.15ms
step:385/1775 train_time:95153ms step_avg:247.15ms
step:386/1775 train_time:95398ms step_avg:247.15ms
step:387/1775 train_time:95644ms step_avg:247.14ms
step:388/1775 train_time:95889ms step_avg:247.14ms
step:389/1775 train_time:96135ms step_avg:247.13ms
step:390/1775 train_time:96383ms step_avg:247.14ms
step:391/1775 train_time:96629ms step_avg:247.13ms
step:392/1775 train_time:96876ms step_avg:247.13ms
step:393/1775 train_time:97122ms step_avg:247.13ms
step:394/1775 train_time:97368ms step_avg:247.13ms
step:395/1775 train_time:97614ms step_avg:247.12ms
step:396/1775 train_time:97859ms step_avg:247.12ms
step:397/1775 train_time:98105ms step_avg:247.12ms
step:398/1775 train_time:98352ms step_avg:247.11ms
step:399/1775 train_time:98598ms step_avg:247.11ms
step:400/1775 train_time:98844ms step_avg:247.11ms
step:401/1775 train_time:99090ms step_avg:247.11ms
step:402/1775 train_time:99335ms step_avg:247.10ms
step:403/1775 train_time:99581ms step_avg:247.10ms
step:404/1775 train_time:99827ms step_avg:247.10ms
step:405/1775 train_time:100073ms step_avg:247.09ms
step:406/1775 train_time:100319ms step_avg:247.09ms
step:407/1775 train_time:100565ms step_avg:247.09ms
step:408/1775 train_time:100812ms step_avg:247.09ms
step:409/1775 train_time:101058ms step_avg:247.09ms
step:410/1775 train_time:101306ms step_avg:247.09ms
step:411/1775 train_time:101553ms step_avg:247.09ms
step:412/1775 train_time:101798ms step_avg:247.08ms
step:413/1775 train_time:102044ms step_avg:247.08ms
step:414/1775 train_time:102292ms step_avg:247.08ms
step:415/1775 train_time:102537ms step_avg:247.08ms
step:416/1775 train_time:102783ms step_avg:247.07ms
step:417/1775 train_time:103029ms step_avg:247.07ms
step:418/1775 train_time:103275ms step_avg:247.07ms
step:419/1775 train_time:103522ms step_avg:247.07ms
step:420/1775 train_time:103768ms step_avg:247.07ms
step:421/1775 train_time:104013ms step_avg:247.06ms
step:422/1775 train_time:104258ms step_avg:247.06ms
step:423/1775 train_time:104504ms step_avg:247.05ms
step:424/1775 train_time:104750ms step_avg:247.05ms
step:425/1775 train_time:104995ms step_avg:247.05ms
step:426/1775 train_time:105241ms step_avg:247.04ms
step:427/1775 train_time:105487ms step_avg:247.04ms
step:428/1775 train_time:105734ms step_avg:247.04ms
step:429/1775 train_time:105980ms step_avg:247.04ms
step:430/1775 train_time:106225ms step_avg:247.04ms
step:431/1775 train_time:106471ms step_avg:247.03ms
step:432/1775 train_time:106717ms step_avg:247.03ms
step:433/1775 train_time:106964ms step_avg:247.03ms
step:434/1775 train_time:107209ms step_avg:247.03ms
step:435/1775 train_time:107455ms step_avg:247.02ms
step:436/1775 train_time:107702ms step_avg:247.02ms
step:437/1775 train_time:107948ms step_avg:247.02ms
step:438/1775 train_time:108195ms step_avg:247.02ms
step:439/1775 train_time:108442ms step_avg:247.02ms
step:440/1775 train_time:108687ms step_avg:247.02ms
step:441/1775 train_time:108933ms step_avg:247.01ms
step:442/1775 train_time:109178ms step_avg:247.01ms
step:443/1775 train_time:109424ms step_avg:247.01ms
step:444/1775 train_time:109670ms step_avg:247.00ms
step:445/1775 train_time:109915ms step_avg:247.00ms
step:446/1775 train_time:110161ms step_avg:247.00ms
step:447/1775 train_time:110407ms step_avg:246.99ms
step:448/1775 train_time:110652ms step_avg:246.99ms
step:449/1775 train_time:110897ms step_avg:246.99ms
step:450/1775 train_time:111143ms step_avg:246.98ms
step:451/1775 train_time:111388ms step_avg:246.98ms
step:452/1775 train_time:111633ms step_avg:246.98ms
step:453/1775 train_time:111879ms step_avg:246.97ms
step:454/1775 train_time:112125ms step_avg:246.97ms
step:455/1775 train_time:112370ms step_avg:246.97ms
step:456/1775 train_time:112615ms step_avg:246.96ms
step:457/1775 train_time:112861ms step_avg:246.96ms
step:458/1775 train_time:113108ms step_avg:246.96ms
step:459/1775 train_time:113354ms step_avg:246.96ms
step:460/1775 train_time:113599ms step_avg:246.95ms
step:461/1775 train_time:113844ms step_avg:246.95ms
step:462/1775 train_time:114090ms step_avg:246.95ms
step:463/1775 train_time:114336ms step_avg:246.95ms
step:464/1775 train_time:114581ms step_avg:246.94ms
step:465/1775 train_time:114826ms step_avg:246.94ms
step:466/1775 train_time:115073ms step_avg:246.94ms
step:467/1775 train_time:115319ms step_avg:246.94ms
step:468/1775 train_time:115564ms step_avg:246.93ms
step:469/1775 train_time:115812ms step_avg:246.93ms
step:470/1775 train_time:116057ms step_avg:246.93ms
step:471/1775 train_time:116303ms step_avg:246.93ms
step:472/1775 train_time:116549ms step_avg:246.93ms
step:473/1775 train_time:116795ms step_avg:246.92ms
step:474/1775 train_time:117041ms step_avg:246.92ms
step:475/1775 train_time:117287ms step_avg:246.92ms
step:476/1775 train_time:117533ms step_avg:246.92ms
step:477/1775 train_time:117778ms step_avg:246.91ms
step:478/1775 train_time:118025ms step_avg:246.91ms
step:479/1775 train_time:118272ms step_avg:246.91ms
step:480/1775 train_time:118517ms step_avg:246.91ms
step:481/1775 train_time:118763ms step_avg:246.91ms
step:482/1775 train_time:119008ms step_avg:246.90ms
step:483/1775 train_time:119254ms step_avg:246.90ms
step:484/1775 train_time:119501ms step_avg:246.90ms
step:485/1775 train_time:119747ms step_avg:246.90ms
step:486/1775 train_time:119993ms step_avg:246.90ms
step:487/1775 train_time:120239ms step_avg:246.90ms
step:488/1775 train_time:120485ms step_avg:246.90ms
step:489/1775 train_time:120733ms step_avg:246.90ms
step:490/1775 train_time:120978ms step_avg:246.89ms
step:491/1775 train_time:121224ms step_avg:246.89ms
step:492/1775 train_time:121468ms step_avg:246.89ms
step:493/1775 train_time:121715ms step_avg:246.89ms
step:494/1775 train_time:121962ms step_avg:246.89ms
step:495/1775 train_time:122208ms step_avg:246.89ms
step:496/1775 train_time:122454ms step_avg:246.88ms
step:497/1775 train_time:122701ms step_avg:246.88ms
step:498/1775 train_time:122947ms step_avg:246.88ms
step:499/1775 train_time:123194ms step_avg:246.88ms
step:500/1775 train_time:123440ms step_avg:246.88ms
step:500/1775 val_loss:4.2774 train_time:123484ms step_avg:246.97ms
step:501/1775 train_time:123687ms step_avg:246.88ms
step:502/1775 train_time:123932ms step_avg:246.88ms
step:503/1775 train_time:124176ms step_avg:246.87ms
step:504/1775 train_time:124422ms step_avg:246.87ms
step:505/1775 train_time:124668ms step_avg:246.87ms
step:506/1775 train_time:124915ms step_avg:246.87ms
step:507/1775 train_time:125161ms step_avg:246.87ms
step:508/1775 train_time:125407ms step_avg:246.86ms
step:509/1775 train_time:125654ms step_avg:246.86ms
step:510/1775 train_time:125900ms step_avg:246.86ms
step:511/1775 train_time:126146ms step_avg:246.86ms
step:512/1775 train_time:126391ms step_avg:246.86ms
step:513/1775 train_time:126637ms step_avg:246.86ms
step:514/1775 train_time:126883ms step_avg:246.85ms
step:515/1775 train_time:127128ms step_avg:246.85ms
step:516/1775 train_time:127376ms step_avg:246.85ms
step:517/1775 train_time:127623ms step_avg:246.85ms
step:518/1775 train_time:127869ms step_avg:246.85ms
step:519/1775 train_time:128114ms step_avg:246.85ms
step:520/1775 train_time:128359ms step_avg:246.84ms
step:521/1775 train_time:128606ms step_avg:246.84ms
step:522/1775 train_time:128852ms step_avg:246.84ms
step:523/1775 train_time:129097ms step_avg:246.84ms
step:524/1775 train_time:129343ms step_avg:246.84ms
step:525/1775 train_time:129589ms step_avg:246.84ms
step:526/1775 train_time:129836ms step_avg:246.84ms
step:527/1775 train_time:130082ms step_avg:246.83ms
step:528/1775 train_time:130327ms step_avg:246.83ms
step:529/1775 train_time:130573ms step_avg:246.83ms
step:530/1775 train_time:130819ms step_avg:246.83ms
step:531/1775 train_time:131065ms step_avg:246.83ms
step:532/1775 train_time:131310ms step_avg:246.82ms
step:533/1775 train_time:131556ms step_avg:246.82ms
step:534/1775 train_time:131803ms step_avg:246.82ms
step:535/1775 train_time:132049ms step_avg:246.82ms
step:536/1775 train_time:132294ms step_avg:246.82ms
step:537/1775 train_time:132540ms step_avg:246.82ms
step:538/1775 train_time:132787ms step_avg:246.82ms
step:539/1775 train_time:133033ms step_avg:246.81ms
step:540/1775 train_time:133278ms step_avg:246.81ms
step:541/1775 train_time:133525ms step_avg:246.81ms
step:542/1775 train_time:133770ms step_avg:246.81ms
step:543/1775 train_time:134017ms step_avg:246.81ms
step:544/1775 train_time:134262ms step_avg:246.80ms
step:545/1775 train_time:134508ms step_avg:246.80ms
step:546/1775 train_time:134755ms step_avg:246.80ms
step:547/1775 train_time:135002ms step_avg:246.80ms
step:548/1775 train_time:135247ms step_avg:246.80ms
step:549/1775 train_time:135493ms step_avg:246.80ms
step:550/1775 train_time:135739ms step_avg:246.80ms
step:551/1775 train_time:135986ms step_avg:246.80ms
step:552/1775 train_time:136232ms step_avg:246.80ms
step:553/1775 train_time:136478ms step_avg:246.80ms
step:554/1775 train_time:136723ms step_avg:246.79ms
step:555/1775 train_time:136970ms step_avg:246.79ms
step:556/1775 train_time:137215ms step_avg:246.79ms
step:557/1775 train_time:137461ms step_avg:246.79ms
step:558/1775 train_time:137706ms step_avg:246.78ms
step:559/1775 train_time:137952ms step_avg:246.78ms
step:560/1775 train_time:138197ms step_avg:246.78ms
step:561/1775 train_time:138444ms step_avg:246.78ms
step:562/1775 train_time:138689ms step_avg:246.78ms
step:563/1775 train_time:138935ms step_avg:246.78ms
step:564/1775 train_time:139180ms step_avg:246.77ms
step:565/1775 train_time:139426ms step_avg:246.77ms
step:566/1775 train_time:139672ms step_avg:246.77ms
step:567/1775 train_time:139917ms step_avg:246.77ms
step:568/1775 train_time:140164ms step_avg:246.77ms
step:569/1775 train_time:140410ms step_avg:246.77ms
step:570/1775 train_time:140656ms step_avg:246.77ms
step:571/1775 train_time:140902ms step_avg:246.76ms
step:572/1775 train_time:141148ms step_avg:246.76ms
step:573/1775 train_time:141394ms step_avg:246.76ms
step:574/1775 train_time:141642ms step_avg:246.76ms
step:575/1775 train_time:141888ms step_avg:246.76ms
step:576/1775 train_time:142135ms step_avg:246.76ms
step:577/1775 train_time:142380ms step_avg:246.76ms
step:578/1775 train_time:142627ms step_avg:246.76ms
step:579/1775 train_time:142873ms step_avg:246.76ms
step:580/1775 train_time:143305ms step_avg:247.08ms
step:581/1775 train_time:143762ms step_avg:247.44ms
step:582/1775 train_time:144215ms step_avg:247.79ms
step:583/1775 train_time:144669ms step_avg:248.15ms
step:584/1775 train_time:145125ms step_avg:248.50ms
step:585/1775 train_time:145581ms step_avg:248.86ms
step:586/1775 train_time:146037ms step_avg:249.21ms
step:587/1775 train_time:146490ms step_avg:249.56ms
step:588/1775 train_time:146946ms step_avg:249.91ms
step:589/1775 train_time:147402ms step_avg:250.26ms
step:590/1775 train_time:147856ms step_avg:250.60ms
step:591/1775 train_time:148311ms step_avg:250.95ms
step:592/1775 train_time:148766ms step_avg:251.29ms
step:593/1775 train_time:149222ms step_avg:251.64ms
step:594/1775 train_time:149679ms step_avg:251.99ms
step:595/1775 train_time:150136ms step_avg:252.33ms
step:596/1775 train_time:150590ms step_avg:252.67ms
step:597/1775 train_time:151044ms step_avg:253.01ms
step:598/1775 train_time:151502ms step_avg:253.35ms
step:599/1775 train_time:151957ms step_avg:253.68ms
step:600/1775 train_time:152411ms step_avg:254.02ms
step:601/1775 train_time:152865ms step_avg:254.35ms
step:602/1775 train_time:153322ms step_avg:254.69ms
step:603/1775 train_time:153778ms step_avg:255.02ms
step:604/1775 train_time:154233ms step_avg:255.35ms
step:605/1775 train_time:154689ms step_avg:255.68ms
step:606/1775 train_time:155144ms step_avg:256.01ms
step:607/1775 train_time:155598ms step_avg:256.34ms
step:608/1775 train_time:156054ms step_avg:256.67ms
step:609/1775 train_time:156507ms step_avg:256.99ms
step:610/1775 train_time:156962ms step_avg:257.31ms
step:611/1775 train_time:157417ms step_avg:257.64ms
step:612/1775 train_time:157872ms step_avg:257.96ms
step:613/1775 train_time:158326ms step_avg:258.28ms
step:614/1775 train_time:158783ms step_avg:258.60ms
step:615/1775 train_time:159237ms step_avg:258.92ms
step:616/1775 train_time:159693ms step_avg:259.24ms
step:617/1775 train_time:160147ms step_avg:259.56ms
step:618/1775 train_time:160604ms step_avg:259.88ms
step:619/1775 train_time:161060ms step_avg:260.19ms
step:620/1775 train_time:161515ms step_avg:260.51ms
step:621/1775 train_time:161971ms step_avg:260.82ms
step:622/1775 train_time:162425ms step_avg:261.13ms
step:623/1775 train_time:162882ms step_avg:261.45ms
step:624/1775 train_time:163339ms step_avg:261.76ms
step:625/1775 train_time:163793ms step_avg:262.07ms
step:626/1775 train_time:164247ms step_avg:262.38ms
step:627/1775 train_time:164702ms step_avg:262.68ms
step:628/1775 train_time:165158ms step_avg:262.99ms
step:629/1775 train_time:165613ms step_avg:263.30ms
step:630/1775 train_time:166067ms step_avg:263.60ms
step:631/1775 train_time:166522ms step_avg:263.90ms
step:632/1775 train_time:166978ms step_avg:264.21ms
step:633/1775 train_time:167433ms step_avg:264.51ms
step:634/1775 train_time:167888ms step_avg:264.81ms
step:635/1775 train_time:168342ms step_avg:265.11ms
step:636/1775 train_time:168799ms step_avg:265.41ms
step:637/1775 train_time:169253ms step_avg:265.70ms
step:638/1775 train_time:169707ms step_avg:266.00ms
step:639/1775 train_time:170162ms step_avg:266.29ms
step:640/1775 train_time:170620ms step_avg:266.59ms
step:641/1775 train_time:171073ms step_avg:266.89ms
step:642/1775 train_time:171528ms step_avg:267.18ms
step:643/1775 train_time:171983ms step_avg:267.47ms
step:644/1775 train_time:172440ms step_avg:267.76ms
step:645/1775 train_time:172892ms step_avg:268.05ms
step:646/1775 train_time:173347ms step_avg:268.34ms
step:647/1775 train_time:173802ms step_avg:268.63ms
step:648/1775 train_time:174259ms step_avg:268.92ms
step:649/1775 train_time:174714ms step_avg:269.20ms
step:650/1775 train_time:175168ms step_avg:269.49ms
step:651/1775 train_time:175623ms step_avg:269.77ms
step:652/1775 train_time:176078ms step_avg:270.06ms
step:653/1775 train_time:176532ms step_avg:270.34ms
step:654/1775 train_time:176986ms step_avg:270.62ms
step:655/1775 train_time:177441ms step_avg:270.90ms
step:656/1775 train_time:177897ms step_avg:271.19ms
step:657/1775 train_time:178351ms step_avg:271.46ms
step:658/1775 train_time:178807ms step_avg:271.74ms
step:659/1775 train_time:179261ms step_avg:272.02ms
step:660/1775 train_time:179719ms step_avg:272.30ms
step:661/1775 train_time:180171ms step_avg:272.57ms
step:662/1775 train_time:180626ms step_avg:272.85ms
step:663/1775 train_time:181083ms step_avg:273.13ms
step:664/1775 train_time:181539ms step_avg:273.40ms
step:665/1775 train_time:181994ms step_avg:273.68ms
step:666/1775 train_time:182448ms step_avg:273.95ms
step:667/1775 train_time:182903ms step_avg:274.22ms
step:668/1775 train_time:183359ms step_avg:274.49ms
step:669/1775 train_time:183815ms step_avg:274.76ms
step:670/1775 train_time:184268ms step_avg:275.03ms
step:671/1775 train_time:184722ms step_avg:275.29ms
step:672/1775 train_time:185179ms step_avg:275.56ms
step:673/1775 train_time:185633ms step_avg:275.83ms
step:674/1775 train_time:186088ms step_avg:276.10ms
step:675/1775 train_time:186543ms step_avg:276.36ms
step:676/1775 train_time:187001ms step_avg:276.63ms
step:677/1775 train_time:187456ms step_avg:276.89ms
step:678/1775 train_time:187910ms step_avg:277.15ms
step:679/1775 train_time:188366ms step_avg:277.42ms
step:680/1775 train_time:188822ms step_avg:277.68ms
step:681/1775 train_time:189277ms step_avg:277.94ms
step:682/1775 train_time:189733ms step_avg:278.20ms
step:683/1775 train_time:190185ms step_avg:278.46ms
step:684/1775 train_time:190640ms step_avg:278.71ms
step:685/1775 train_time:191092ms step_avg:278.97ms
step:686/1775 train_time:191549ms step_avg:279.23ms
step:687/1775 train_time:192002ms step_avg:279.48ms
step:688/1775 train_time:192458ms step_avg:279.74ms
step:689/1775 train_time:192911ms step_avg:279.99ms
step:690/1775 train_time:193366ms step_avg:280.24ms
step:691/1775 train_time:193820ms step_avg:280.49ms
step:692/1775 train_time:194277ms step_avg:280.75ms
step:693/1775 train_time:194729ms step_avg:280.99ms
step:694/1775 train_time:195184ms step_avg:281.24ms
step:695/1775 train_time:195639ms step_avg:281.49ms
step:696/1775 train_time:196093ms step_avg:281.74ms
step:697/1775 train_time:196547ms step_avg:281.99ms
step:698/1775 train_time:197002ms step_avg:282.24ms
step:699/1775 train_time:197457ms step_avg:282.49ms
step:700/1775 train_time:197912ms step_avg:282.73ms
step:701/1775 train_time:198367ms step_avg:282.98ms
step:702/1775 train_time:198824ms step_avg:283.22ms
step:703/1775 train_time:199278ms step_avg:283.47ms
step:704/1775 train_time:199733ms step_avg:283.71ms
step:705/1775 train_time:200187ms step_avg:283.95ms
step:706/1775 train_time:200644ms step_avg:284.20ms
step:707/1775 train_time:201099ms step_avg:284.44ms
step:708/1775 train_time:201554ms step_avg:284.68ms
step:709/1775 train_time:202007ms step_avg:284.92ms
step:710/1775 train_time:202465ms step_avg:285.16ms
step:711/1775 train_time:202920ms step_avg:285.40ms
step:712/1775 train_time:203376ms step_avg:285.64ms
step:713/1775 train_time:203829ms step_avg:285.88ms
step:714/1775 train_time:204285ms step_avg:286.11ms
step:715/1775 train_time:204740ms step_avg:286.35ms
step:716/1775 train_time:205196ms step_avg:286.59ms
step:717/1775 train_time:205650ms step_avg:286.82ms
step:718/1775 train_time:206106ms step_avg:287.06ms
step:719/1775 train_time:206561ms step_avg:287.29ms
step:720/1775 train_time:207018ms step_avg:287.52ms
step:721/1775 train_time:207471ms step_avg:287.75ms
step:722/1775 train_time:207926ms step_avg:287.99ms
step:723/1775 train_time:208381ms step_avg:288.22ms
step:724/1775 train_time:208837ms step_avg:288.45ms
step:725/1775 train_time:209289ms step_avg:288.67ms
step:726/1775 train_time:209745ms step_avg:288.91ms
step:727/1775 train_time:210202ms step_avg:289.14ms
step:728/1775 train_time:210657ms step_avg:289.36ms
step:729/1775 train_time:211111ms step_avg:289.59ms
step:730/1775 train_time:211565ms step_avg:289.82ms
step:731/1775 train_time:212022ms step_avg:290.04ms
step:732/1775 train_time:212479ms step_avg:290.27ms
step:733/1775 train_time:212931ms step_avg:290.49ms
step:734/1775 train_time:213385ms step_avg:290.71ms
step:735/1775 train_time:213839ms step_avg:290.94ms
step:736/1775 train_time:214294ms step_avg:291.16ms
step:737/1775 train_time:214748ms step_avg:291.38ms
step:738/1775 train_time:215203ms step_avg:291.60ms
step:739/1775 train_time:215658ms step_avg:291.82ms
step:740/1775 train_time:216112ms step_avg:292.04ms
step:741/1775 train_time:216565ms step_avg:292.26ms
step:742/1775 train_time:217021ms step_avg:292.48ms
step:743/1775 train_time:217475ms step_avg:292.70ms
step:744/1775 train_time:217930ms step_avg:292.92ms
step:745/1775 train_time:218383ms step_avg:293.13ms
step:746/1775 train_time:218842ms step_avg:293.35ms
step:747/1775 train_time:219297ms step_avg:293.57ms
step:748/1775 train_time:219750ms step_avg:293.78ms
step:749/1775 train_time:220204ms step_avg:294.00ms
step:750/1775 train_time:220662ms step_avg:294.22ms
step:750/1775 val_loss:3.9966 train_time:220734ms step_avg:294.31ms
step:751/1775 train_time:221116ms step_avg:294.43ms
step:752/1775 train_time:221571ms step_avg:294.64ms
step:753/1775 train_time:222026ms step_avg:294.86ms
step:754/1775 train_time:222482ms step_avg:295.07ms
step:755/1775 train_time:222936ms step_avg:295.28ms
step:756/1775 train_time:223391ms step_avg:295.49ms
step:757/1775 train_time:223845ms step_avg:295.70ms
step:758/1775 train_time:224302ms step_avg:295.91ms
step:759/1775 train_time:224756ms step_avg:296.12ms
step:760/1775 train_time:225211ms step_avg:296.33ms
step:761/1775 train_time:225665ms step_avg:296.54ms
step:762/1775 train_time:226123ms step_avg:296.75ms
step:763/1775 train_time:226577ms step_avg:296.95ms
step:764/1775 train_time:227032ms step_avg:297.16ms
step:765/1775 train_time:227487ms step_avg:297.37ms
step:766/1775 train_time:227944ms step_avg:297.58ms
step:767/1775 train_time:228400ms step_avg:297.78ms
step:768/1775 train_time:228853ms step_avg:297.99ms
step:769/1775 train_time:229308ms step_avg:298.19ms
step:770/1775 train_time:229764ms step_avg:298.40ms
step:771/1775 train_time:230219ms step_avg:298.60ms
step:772/1775 train_time:230674ms step_avg:298.80ms
step:773/1775 train_time:231127ms step_avg:299.00ms
step:774/1775 train_time:231584ms step_avg:299.20ms
step:775/1775 train_time:232038ms step_avg:299.40ms
step:776/1775 train_time:232493ms step_avg:299.60ms
step:777/1775 train_time:232945ms step_avg:299.80ms
step:778/1775 train_time:233404ms step_avg:300.01ms
step:779/1775 train_time:233860ms step_avg:300.20ms
step:780/1775 train_time:234315ms step_avg:300.40ms
step:781/1775 train_time:234766ms step_avg:300.60ms
step:782/1775 train_time:235224ms step_avg:300.80ms
step:783/1775 train_time:235678ms step_avg:300.99ms
step:784/1775 train_time:236133ms step_avg:301.19ms
step:785/1775 train_time:236585ms step_avg:301.38ms
step:786/1775 train_time:237041ms step_avg:301.58ms
step:787/1775 train_time:237496ms step_avg:301.77ms
step:788/1775 train_time:237950ms step_avg:301.97ms
step:789/1775 train_time:238405ms step_avg:302.16ms
step:790/1775 train_time:238861ms step_avg:302.36ms
step:791/1775 train_time:239316ms step_avg:302.55ms
step:792/1775 train_time:239770ms step_avg:302.74ms
step:793/1775 train_time:240225ms step_avg:302.93ms
step:794/1775 train_time:240680ms step_avg:303.12ms
step:795/1775 train_time:241133ms step_avg:303.31ms
step:796/1775 train_time:241590ms step_avg:303.50ms
step:797/1775 train_time:242044ms step_avg:303.69ms
step:798/1775 train_time:242498ms step_avg:303.88ms
step:799/1775 train_time:242950ms step_avg:304.07ms
step:800/1775 train_time:243405ms step_avg:304.26ms
step:801/1775 train_time:243862ms step_avg:304.45ms
step:802/1775 train_time:244316ms step_avg:304.63ms
step:803/1775 train_time:244769ms step_avg:304.82ms
step:804/1775 train_time:245225ms step_avg:305.01ms
step:805/1775 train_time:245680ms step_avg:305.19ms
step:806/1775 train_time:246134ms step_avg:305.38ms
step:807/1775 train_time:246587ms step_avg:305.56ms
step:808/1775 train_time:247043ms step_avg:305.75ms
step:809/1775 train_time:247496ms step_avg:305.93ms
step:810/1775 train_time:247951ms step_avg:306.11ms
step:811/1775 train_time:248404ms step_avg:306.29ms
step:812/1775 train_time:248862ms step_avg:306.48ms
step:813/1775 train_time:249315ms step_avg:306.66ms
step:814/1775 train_time:249770ms step_avg:306.84ms
step:815/1775 train_time:250226ms step_avg:307.03ms
step:816/1775 train_time:250682ms step_avg:307.21ms
step:817/1775 train_time:251137ms step_avg:307.39ms
step:818/1775 train_time:251592ms step_avg:307.57ms
step:819/1775 train_time:252045ms step_avg:307.75ms
step:820/1775 train_time:252500ms step_avg:307.93ms
step:821/1775 train_time:252953ms step_avg:308.10ms
step:822/1775 train_time:253407ms step_avg:308.28ms
step:823/1775 train_time:253862ms step_avg:308.46ms
step:824/1775 train_time:254318ms step_avg:308.64ms
step:825/1775 train_time:254771ms step_avg:308.81ms
step:826/1775 train_time:255226ms step_avg:308.99ms
step:827/1775 train_time:255681ms step_avg:309.17ms
step:828/1775 train_time:256136ms step_avg:309.34ms
step:829/1775 train_time:256589ms step_avg:309.52ms
step:830/1775 train_time:257046ms step_avg:309.69ms
step:831/1775 train_time:257502ms step_avg:309.87ms
step:832/1775 train_time:257956ms step_avg:310.04ms
step:833/1775 train_time:258410ms step_avg:310.22ms
step:834/1775 train_time:258865ms step_avg:310.39ms
step:835/1775 train_time:259318ms step_avg:310.56ms
step:836/1775 train_time:259772ms step_avg:310.73ms
step:837/1775 train_time:260226ms step_avg:310.90ms
step:838/1775 train_time:260682ms step_avg:311.08ms
step:839/1775 train_time:261137ms step_avg:311.25ms
step:840/1775 train_time:261592ms step_avg:311.42ms
step:841/1775 train_time:262045ms step_avg:311.59ms
step:842/1775 train_time:262502ms step_avg:311.76ms
step:843/1775 train_time:262956ms step_avg:311.93ms
step:844/1775 train_time:263411ms step_avg:312.10ms
step:845/1775 train_time:263865ms step_avg:312.27ms
step:846/1775 train_time:264321ms step_avg:312.44ms
step:847/1775 train_time:264774ms step_avg:312.60ms
step:848/1775 train_time:265231ms step_avg:312.77ms
step:849/1775 train_time:265685ms step_avg:312.94ms
step:850/1775 train_time:266142ms step_avg:313.11ms
step:851/1775 train_time:266596ms step_avg:313.27ms
step:852/1775 train_time:267049ms step_avg:313.44ms
step:853/1775 train_time:267504ms step_avg:313.60ms
step:854/1775 train_time:267959ms step_avg:313.77ms
step:855/1775 train_time:268414ms step_avg:313.93ms
step:856/1775 train_time:268868ms step_avg:314.10ms
step:857/1775 train_time:269323ms step_avg:314.26ms
step:858/1775 train_time:269779ms step_avg:314.43ms
step:859/1775 train_time:270232ms step_avg:314.59ms
step:860/1775 train_time:270688ms step_avg:314.75ms
step:861/1775 train_time:271143ms step_avg:314.92ms
step:862/1775 train_time:271599ms step_avg:315.08ms
step:863/1775 train_time:272052ms step_avg:315.24ms
step:864/1775 train_time:272507ms step_avg:315.40ms
step:865/1775 train_time:272961ms step_avg:315.56ms
step:866/1775 train_time:273415ms step_avg:315.72ms
step:867/1775 train_time:273869ms step_avg:315.88ms
step:868/1775 train_time:274326ms step_avg:316.04ms
step:869/1775 train_time:274779ms step_avg:316.20ms
step:870/1775 train_time:275233ms step_avg:316.36ms
step:871/1775 train_time:275687ms step_avg:316.52ms
step:872/1775 train_time:276142ms step_avg:316.68ms
step:873/1775 train_time:276597ms step_avg:316.83ms
step:874/1775 train_time:277050ms step_avg:316.99ms
step:875/1775 train_time:277504ms step_avg:317.15ms
step:876/1775 train_time:277961ms step_avg:317.31ms
step:877/1775 train_time:278414ms step_avg:317.46ms
step:878/1775 train_time:278869ms step_avg:317.62ms
step:879/1775 train_time:279323ms step_avg:317.77ms
step:880/1775 train_time:279779ms step_avg:317.93ms
step:881/1775 train_time:280232ms step_avg:318.08ms
step:882/1775 train_time:280686ms step_avg:318.24ms
step:883/1775 train_time:281141ms step_avg:318.39ms
step:884/1775 train_time:281596ms step_avg:318.55ms
step:885/1775 train_time:282050ms step_avg:318.70ms
step:886/1775 train_time:282506ms step_avg:318.86ms
step:887/1775 train_time:282962ms step_avg:319.01ms
step:888/1775 train_time:283416ms step_avg:319.16ms
step:889/1775 train_time:283870ms step_avg:319.31ms
step:890/1775 train_time:284325ms step_avg:319.47ms
step:891/1775 train_time:284781ms step_avg:319.62ms
step:892/1775 train_time:285235ms step_avg:319.77ms
step:893/1775 train_time:285688ms step_avg:319.92ms
step:894/1775 train_time:286145ms step_avg:320.07ms
step:895/1775 train_time:286600ms step_avg:320.22ms
step:896/1775 train_time:287053ms step_avg:320.37ms
step:897/1775 train_time:287506ms step_avg:320.52ms
step:898/1775 train_time:287963ms step_avg:320.67ms
step:899/1775 train_time:288417ms step_avg:320.82ms
step:900/1775 train_time:288872ms step_avg:320.97ms
step:901/1775 train_time:289325ms step_avg:321.11ms
step:902/1775 train_time:289781ms step_avg:321.26ms
step:903/1775 train_time:290235ms step_avg:321.41ms
step:904/1775 train_time:290687ms step_avg:321.56ms
step:905/1775 train_time:291142ms step_avg:321.70ms
step:906/1775 train_time:291598ms step_avg:321.85ms
step:907/1775 train_time:292050ms step_avg:322.00ms
step:908/1775 train_time:292506ms step_avg:322.14ms
step:909/1775 train_time:292960ms step_avg:322.29ms
step:910/1775 train_time:293417ms step_avg:322.44ms
step:911/1775 train_time:293871ms step_avg:322.58ms
step:912/1775 train_time:294325ms step_avg:322.72ms
step:913/1775 train_time:294783ms step_avg:322.87ms
step:914/1775 train_time:295238ms step_avg:323.02ms
step:915/1775 train_time:295691ms step_avg:323.16ms
step:916/1775 train_time:296146ms step_avg:323.30ms
step:917/1775 train_time:296600ms step_avg:323.45ms
step:918/1775 train_time:297052ms step_avg:323.59ms
step:919/1775 train_time:297507ms step_avg:323.73ms
step:920/1775 train_time:297962ms step_avg:323.87ms
step:921/1775 train_time:298415ms step_avg:324.01ms
step:922/1775 train_time:298869ms step_avg:324.15ms
step:923/1775 train_time:299324ms step_avg:324.30ms
step:924/1775 train_time:299781ms step_avg:324.44ms
step:925/1775 train_time:300235ms step_avg:324.58ms
step:926/1775 train_time:300689ms step_avg:324.72ms
step:927/1775 train_time:301143ms step_avg:324.86ms
step:928/1775 train_time:301599ms step_avg:325.00ms
step:929/1775 train_time:302051ms step_avg:325.14ms
step:930/1775 train_time:302507ms step_avg:325.28ms
step:931/1775 train_time:302961ms step_avg:325.41ms
step:932/1775 train_time:303416ms step_avg:325.55ms
step:933/1775 train_time:303868ms step_avg:325.69ms
step:934/1775 train_time:304323ms step_avg:325.83ms
step:935/1775 train_time:304778ms step_avg:325.97ms
step:936/1775 train_time:305232ms step_avg:326.10ms
step:937/1775 train_time:305686ms step_avg:326.24ms
step:938/1775 train_time:306140ms step_avg:326.37ms
step:939/1775 train_time:306594ms step_avg:326.51ms
step:940/1775 train_time:307047ms step_avg:326.65ms
step:941/1775 train_time:307502ms step_avg:326.78ms
step:942/1775 train_time:307958ms step_avg:326.92ms
step:943/1775 train_time:308410ms step_avg:327.05ms
step:944/1775 train_time:308866ms step_avg:327.19ms
step:945/1775 train_time:309321ms step_avg:327.32ms
step:946/1775 train_time:309776ms step_avg:327.46ms
step:947/1775 train_time:310230ms step_avg:327.59ms
step:948/1775 train_time:310685ms step_avg:327.73ms
step:949/1775 train_time:311141ms step_avg:327.86ms
step:950/1775 train_time:311595ms step_avg:327.99ms
step:951/1775 train_time:312048ms step_avg:328.13ms
step:952/1775 train_time:312503ms step_avg:328.26ms
step:953/1775 train_time:312957ms step_avg:328.39ms
step:954/1775 train_time:313409ms step_avg:328.52ms
step:955/1775 train_time:313864ms step_avg:328.65ms
step:956/1775 train_time:314320ms step_avg:328.79ms
step:957/1775 train_time:314773ms step_avg:328.92ms
step:958/1775 train_time:315228ms step_avg:329.05ms
step:959/1775 train_time:315684ms step_avg:329.18ms
step:960/1775 train_time:316141ms step_avg:329.31ms
step:961/1775 train_time:316593ms step_avg:329.44ms
step:962/1775 train_time:317047ms step_avg:329.57ms
step:963/1775 train_time:317503ms step_avg:329.70ms
step:964/1775 train_time:317957ms step_avg:329.83ms
step:965/1775 train_time:318410ms step_avg:329.96ms
step:966/1775 train_time:318864ms step_avg:330.09ms
step:967/1775 train_time:319319ms step_avg:330.22ms
step:968/1775 train_time:319774ms step_avg:330.34ms
step:969/1775 train_time:320226ms step_avg:330.47ms
step:970/1775 train_time:320683ms step_avg:330.60ms
step:971/1775 train_time:321136ms step_avg:330.73ms
step:972/1775 train_time:321590ms step_avg:330.85ms
step:973/1775 train_time:322042ms step_avg:330.98ms
step:974/1775 train_time:322498ms step_avg:331.11ms
step:975/1775 train_time:322951ms step_avg:331.23ms
step:976/1775 train_time:323406ms step_avg:331.36ms
step:977/1775 train_time:323862ms step_avg:331.49ms
step:978/1775 train_time:324315ms step_avg:331.61ms
step:979/1775 train_time:324768ms step_avg:331.73ms
step:980/1775 train_time:325224ms step_avg:331.86ms
step:981/1775 train_time:325678ms step_avg:331.99ms
step:982/1775 train_time:326131ms step_avg:332.11ms
step:983/1775 train_time:326585ms step_avg:332.23ms
step:984/1775 train_time:327041ms step_avg:332.36ms
step:985/1775 train_time:327495ms step_avg:332.48ms
step:986/1775 train_time:327950ms step_avg:332.61ms
step:987/1775 train_time:328402ms step_avg:332.73ms
step:988/1775 train_time:328859ms step_avg:332.85ms
step:989/1775 train_time:329314ms step_avg:332.98ms
step:990/1775 train_time:329770ms step_avg:333.10ms
step:991/1775 train_time:330223ms step_avg:333.22ms
step:992/1775 train_time:330679ms step_avg:333.35ms
step:993/1775 train_time:331132ms step_avg:333.47ms
step:994/1775 train_time:331587ms step_avg:333.59ms
step:995/1775 train_time:332042ms step_avg:333.71ms
step:996/1775 train_time:332495ms step_avg:333.83ms
step:997/1775 train_time:332948ms step_avg:333.95ms
step:998/1775 train_time:333403ms step_avg:334.07ms
step:999/1775 train_time:333859ms step_avg:334.19ms
step:1000/1775 train_time:334312ms step_avg:334.31ms
step:1000/1775 val_loss:3.7334 train_time:334384ms step_avg:334.38ms
step:1001/1775 train_time:334766ms step_avg:334.43ms
step:1002/1775 train_time:335220ms step_avg:334.55ms
step:1003/1775 train_time:335672ms step_avg:334.67ms
step:1004/1775 train_time:336127ms step_avg:334.79ms
step:1005/1775 train_time:336579ms step_avg:334.90ms
step:1006/1775 train_time:337035ms step_avg:335.02ms
step:1007/1775 train_time:337489ms step_avg:335.14ms
step:1008/1775 train_time:337944ms step_avg:335.26ms
step:1009/1775 train_time:338398ms step_avg:335.38ms
step:1010/1775 train_time:338853ms step_avg:335.50ms
step:1011/1775 train_time:339307ms step_avg:335.62ms
step:1012/1775 train_time:339761ms step_avg:335.73ms
step:1013/1775 train_time:340214ms step_avg:335.85ms
step:1014/1775 train_time:340669ms step_avg:335.97ms
step:1015/1775 train_time:341124ms step_avg:336.08ms
step:1016/1775 train_time:341577ms step_avg:336.20ms
step:1017/1775 train_time:342031ms step_avg:336.31ms
step:1018/1775 train_time:342487ms step_avg:336.43ms
step:1019/1775 train_time:342940ms step_avg:336.55ms
step:1020/1775 train_time:343394ms step_avg:336.66ms
step:1021/1775 train_time:343848ms step_avg:336.78ms
step:1022/1775 train_time:344303ms step_avg:336.89ms
step:1023/1775 train_time:344756ms step_avg:337.00ms
step:1024/1775 train_time:345213ms step_avg:337.12ms
step:1025/1775 train_time:345667ms step_avg:337.24ms
step:1026/1775 train_time:346120ms step_avg:337.35ms
step:1027/1775 train_time:346573ms step_avg:337.46ms
step:1028/1775 train_time:347029ms step_avg:337.58ms
step:1029/1775 train_time:347483ms step_avg:337.69ms
step:1030/1775 train_time:347935ms step_avg:337.80ms
step:1031/1775 train_time:348390ms step_avg:337.91ms
step:1032/1775 train_time:348845ms step_avg:338.03ms
step:1033/1775 train_time:349298ms step_avg:338.14ms
step:1034/1775 train_time:349756ms step_avg:338.26ms
step:1035/1775 train_time:350209ms step_avg:338.37ms
step:1036/1775 train_time:350665ms step_avg:338.48ms
step:1037/1775 train_time:351117ms step_avg:338.59ms
step:1038/1775 train_time:351573ms step_avg:338.70ms
step:1039/1775 train_time:352028ms step_avg:338.81ms
step:1040/1775 train_time:352483ms step_avg:338.93ms
step:1041/1775 train_time:352935ms step_avg:339.04ms
step:1042/1775 train_time:353390ms step_avg:339.15ms
step:1043/1775 train_time:353846ms step_avg:339.26ms
step:1044/1775 train_time:354299ms step_avg:339.37ms
step:1045/1775 train_time:354754ms step_avg:339.48ms
step:1046/1775 train_time:355209ms step_avg:339.59ms
step:1047/1775 train_time:355661ms step_avg:339.70ms
step:1048/1775 train_time:356116ms step_avg:339.81ms
step:1049/1775 train_time:356569ms step_avg:339.91ms
step:1050/1775 train_time:357025ms step_avg:340.02ms
step:1051/1775 train_time:357478ms step_avg:340.13ms
step:1052/1775 train_time:357934ms step_avg:340.24ms
step:1053/1775 train_time:358389ms step_avg:340.35ms
step:1054/1775 train_time:358845ms step_avg:340.46ms
step:1055/1775 train_time:359296ms step_avg:340.56ms
step:1056/1775 train_time:359751ms step_avg:340.67ms
step:1057/1775 train_time:360204ms step_avg:340.78ms
step:1058/1775 train_time:360658ms step_avg:340.89ms
step:1059/1775 train_time:361112ms step_avg:340.99ms
step:1060/1775 train_time:361566ms step_avg:341.10ms
step:1061/1775 train_time:362018ms step_avg:341.20ms
step:1062/1775 train_time:362473ms step_avg:341.31ms
step:1063/1775 train_time:362927ms step_avg:341.42ms
step:1064/1775 train_time:363382ms step_avg:341.52ms
step:1065/1775 train_time:363836ms step_avg:341.63ms
step:1066/1775 train_time:364292ms step_avg:341.74ms
step:1067/1775 train_time:364744ms step_avg:341.84ms
step:1068/1775 train_time:365198ms step_avg:341.95ms
step:1069/1775 train_time:365650ms step_avg:342.05ms
step:1070/1775 train_time:366105ms step_avg:342.15ms
step:1071/1775 train_time:366558ms step_avg:342.26ms
step:1072/1775 train_time:367013ms step_avg:342.36ms
step:1073/1775 train_time:367468ms step_avg:342.47ms
step:1074/1775 train_time:367921ms step_avg:342.57ms
step:1075/1775 train_time:368373ms step_avg:342.67ms
step:1076/1775 train_time:368829ms step_avg:342.78ms
step:1077/1775 train_time:369280ms step_avg:342.88ms
step:1078/1775 train_time:369736ms step_avg:342.98ms
step:1079/1775 train_time:370192ms step_avg:343.09ms
step:1080/1775 train_time:370646ms step_avg:343.19ms
step:1081/1775 train_time:371099ms step_avg:343.29ms
step:1082/1775 train_time:371553ms step_avg:343.39ms
step:1083/1775 train_time:372009ms step_avg:343.50ms
step:1084/1775 train_time:372463ms step_avg:343.60ms
step:1085/1775 train_time:372916ms step_avg:343.70ms
step:1086/1775 train_time:373372ms step_avg:343.80ms
step:1087/1775 train_time:373827ms step_avg:343.91ms
step:1088/1775 train_time:374281ms step_avg:344.01ms
step:1089/1775 train_time:374734ms step_avg:344.11ms
step:1090/1775 train_time:375190ms step_avg:344.21ms
step:1091/1775 train_time:375644ms step_avg:344.31ms
step:1092/1775 train_time:376099ms step_avg:344.41ms
step:1093/1775 train_time:376551ms step_avg:344.51ms
step:1094/1775 train_time:377007ms step_avg:344.61ms
step:1095/1775 train_time:377460ms step_avg:344.71ms
step:1096/1775 train_time:377914ms step_avg:344.81ms
step:1097/1775 train_time:378369ms step_avg:344.91ms
step:1098/1775 train_time:378824ms step_avg:345.01ms
step:1099/1775 train_time:379277ms step_avg:345.11ms
step:1100/1775 train_time:379732ms step_avg:345.21ms
step:1101/1775 train_time:380187ms step_avg:345.31ms
step:1102/1775 train_time:380641ms step_avg:345.41ms
step:1103/1775 train_time:381094ms step_avg:345.51ms
step:1104/1775 train_time:381550ms step_avg:345.61ms
step:1105/1775 train_time:382003ms step_avg:345.70ms
step:1106/1775 train_time:382458ms step_avg:345.80ms
step:1107/1775 train_time:382911ms step_avg:345.90ms
step:1108/1775 train_time:383367ms step_avg:346.00ms
step:1109/1775 train_time:383820ms step_avg:346.10ms
step:1110/1775 train_time:384276ms step_avg:346.19ms
step:1111/1775 train_time:384732ms step_avg:346.29ms
step:1112/1775 train_time:385187ms step_avg:346.39ms
step:1113/1775 train_time:385639ms step_avg:346.49ms
step:1114/1775 train_time:386095ms step_avg:346.58ms
step:1115/1775 train_time:386549ms step_avg:346.68ms
step:1116/1775 train_time:387005ms step_avg:346.78ms
step:1117/1775 train_time:387457ms step_avg:346.87ms
step:1118/1775 train_time:387914ms step_avg:346.97ms
step:1119/1775 train_time:388367ms step_avg:347.07ms
step:1120/1775 train_time:388820ms step_avg:347.16ms
step:1121/1775 train_time:389272ms step_avg:347.25ms
step:1122/1775 train_time:389728ms step_avg:347.35ms
step:1123/1775 train_time:390181ms step_avg:347.45ms
step:1124/1775 train_time:390636ms step_avg:347.54ms
step:1125/1775 train_time:391090ms step_avg:347.64ms
step:1126/1775 train_time:391545ms step_avg:347.73ms
step:1127/1775 train_time:391998ms step_avg:347.82ms
step:1128/1775 train_time:392453ms step_avg:347.92ms
step:1129/1775 train_time:392906ms step_avg:348.01ms
step:1130/1775 train_time:393361ms step_avg:348.11ms
step:1131/1775 train_time:393815ms step_avg:348.20ms
step:1132/1775 train_time:394271ms step_avg:348.30ms
step:1133/1775 train_time:394724ms step_avg:348.39ms
step:1134/1775 train_time:395178ms step_avg:348.48ms
step:1135/1775 train_time:395632ms step_avg:348.57ms
step:1136/1775 train_time:396088ms step_avg:348.67ms
step:1137/1775 train_time:396543ms step_avg:348.76ms
step:1138/1775 train_time:396996ms step_avg:348.85ms
step:1139/1775 train_time:397450ms step_avg:348.95ms
step:1140/1775 train_time:397904ms step_avg:349.04ms
step:1141/1775 train_time:398357ms step_avg:349.13ms
step:1142/1775 train_time:398812ms step_avg:349.22ms
step:1143/1775 train_time:399267ms step_avg:349.31ms
step:1144/1775 train_time:399721ms step_avg:349.41ms
step:1145/1775 train_time:400174ms step_avg:349.50ms
step:1146/1775 train_time:400629ms step_avg:349.59ms
step:1147/1775 train_time:401082ms step_avg:349.68ms
step:1148/1775 train_time:401537ms step_avg:349.77ms
step:1149/1775 train_time:401990ms step_avg:349.86ms
step:1150/1775 train_time:402447ms step_avg:349.95ms
step:1151/1775 train_time:402897ms step_avg:350.04ms
step:1152/1775 train_time:403353ms step_avg:350.13ms
step:1153/1775 train_time:403808ms step_avg:350.22ms
step:1154/1775 train_time:404261ms step_avg:350.31ms
step:1155/1775 train_time:404715ms step_avg:350.40ms
step:1156/1775 train_time:405171ms step_avg:350.49ms
step:1157/1775 train_time:405628ms step_avg:350.59ms
step:1158/1775 train_time:406266ms step_avg:350.83ms
step:1159/1775 train_time:406923ms step_avg:351.10ms
step:1160/1775 train_time:407583ms step_avg:351.36ms
step:1161/1775 train_time:408235ms step_avg:351.62ms
step:1162/1775 train_time:408895ms step_avg:351.89ms
step:1163/1775 train_time:409552ms step_avg:352.15ms
step:1164/1775 train_time:410209ms step_avg:352.41ms
step:1165/1775 train_time:410871ms step_avg:352.68ms
step:1166/1775 train_time:411530ms step_avg:352.94ms
step:1167/1775 train_time:412186ms step_avg:353.20ms
step:1168/1775 train_time:412844ms step_avg:353.46ms
step:1169/1775 train_time:413501ms step_avg:353.72ms
step:1170/1775 train_time:414156ms step_avg:353.98ms
step:1171/1775 train_time:414814ms step_avg:354.24ms
step:1172/1775 train_time:415473ms step_avg:354.50ms
step:1173/1775 train_time:416130ms step_avg:354.76ms
step:1174/1775 train_time:416790ms step_avg:355.02ms
step:1175/1775 train_time:417449ms step_avg:355.28ms
step:1176/1775 train_time:418109ms step_avg:355.53ms
step:1177/1775 train_time:418767ms step_avg:355.79ms
step:1178/1775 train_time:419424ms step_avg:356.05ms
step:1179/1775 train_time:420083ms step_avg:356.30ms
step:1180/1775 train_time:420740ms step_avg:356.56ms
step:1181/1775 train_time:421396ms step_avg:356.81ms
step:1182/1775 train_time:422054ms step_avg:357.07ms
step:1183/1775 train_time:422712ms step_avg:357.32ms
step:1184/1775 train_time:423371ms step_avg:357.58ms
step:1185/1775 train_time:424029ms step_avg:357.83ms
step:1186/1775 train_time:424690ms step_avg:358.09ms
step:1187/1775 train_time:425348ms step_avg:358.34ms
step:1188/1775 train_time:426009ms step_avg:358.59ms
step:1189/1775 train_time:426666ms step_avg:358.84ms
step:1190/1775 train_time:427322ms step_avg:359.09ms
step:1191/1775 train_time:427981ms step_avg:359.35ms
step:1192/1775 train_time:428639ms step_avg:359.60ms
step:1193/1775 train_time:429293ms step_avg:359.84ms
step:1194/1775 train_time:429952ms step_avg:360.09ms
step:1195/1775 train_time:430610ms step_avg:360.34ms
step:1196/1775 train_time:431272ms step_avg:360.60ms
step:1197/1775 train_time:431930ms step_avg:360.84ms
step:1198/1775 train_time:432589ms step_avg:361.09ms
step:1199/1775 train_time:433245ms step_avg:361.34ms
step:1200/1775 train_time:433903ms step_avg:361.59ms
step:1201/1775 train_time:434560ms step_avg:361.83ms
step:1202/1775 train_time:435216ms step_avg:362.08ms
step:1203/1775 train_time:435873ms step_avg:362.32ms
step:1204/1775 train_time:436532ms step_avg:362.57ms
step:1205/1775 train_time:437191ms step_avg:362.81ms
step:1206/1775 train_time:437849ms step_avg:363.06ms
step:1207/1775 train_time:438508ms step_avg:363.30ms
step:1208/1775 train_time:439167ms step_avg:363.55ms
step:1209/1775 train_time:439826ms step_avg:363.79ms
step:1210/1775 train_time:440485ms step_avg:364.04ms
step:1211/1775 train_time:441142ms step_avg:364.28ms
step:1212/1775 train_time:441799ms step_avg:364.52ms
step:1213/1775 train_time:442456ms step_avg:364.76ms
step:1214/1775 train_time:443113ms step_avg:365.00ms
step:1215/1775 train_time:443772ms step_avg:365.24ms
step:1216/1775 train_time:444432ms step_avg:365.49ms
step:1217/1775 train_time:445091ms step_avg:365.73ms
step:1218/1775 train_time:445750ms step_avg:365.97ms
step:1219/1775 train_time:446407ms step_avg:366.21ms
step:1220/1775 train_time:447067ms step_avg:366.45ms
step:1221/1775 train_time:447725ms step_avg:366.69ms
step:1222/1775 train_time:448383ms step_avg:366.93ms
step:1223/1775 train_time:449041ms step_avg:367.16ms
step:1224/1775 train_time:449696ms step_avg:367.40ms
step:1225/1775 train_time:450353ms step_avg:367.63ms
step:1226/1775 train_time:451011ms step_avg:367.87ms
step:1227/1775 train_time:451670ms step_avg:368.11ms
step:1228/1775 train_time:452329ms step_avg:368.35ms
step:1229/1775 train_time:452986ms step_avg:368.58ms
step:1230/1775 train_time:453646ms step_avg:368.82ms
step:1231/1775 train_time:454306ms step_avg:369.05ms
step:1232/1775 train_time:454966ms step_avg:369.29ms
step:1233/1775 train_time:455623ms step_avg:369.52ms
step:1234/1775 train_time:456280ms step_avg:369.76ms
step:1235/1775 train_time:456936ms step_avg:369.99ms
step:1236/1775 train_time:457595ms step_avg:370.22ms
step:1237/1775 train_time:458251ms step_avg:370.45ms
step:1238/1775 train_time:458910ms step_avg:370.69ms
step:1239/1775 train_time:459569ms step_avg:370.92ms
step:1240/1775 train_time:460229ms step_avg:371.15ms
step:1241/1775 train_time:460886ms step_avg:371.38ms
step:1242/1775 train_time:461547ms step_avg:371.62ms
step:1243/1775 train_time:462205ms step_avg:371.85ms
step:1244/1775 train_time:462865ms step_avg:372.08ms
step:1245/1775 train_time:463524ms step_avg:372.31ms
step:1246/1775 train_time:464179ms step_avg:372.54ms
step:1247/1775 train_time:464836ms step_avg:372.76ms
step:1248/1775 train_time:465494ms step_avg:372.99ms
step:1249/1775 train_time:466152ms step_avg:373.22ms
step:1250/1775 train_time:466812ms step_avg:373.45ms
step:1250/1775 val_loss:3.5027 train_time:466913ms step_avg:373.53ms
step:1251/1775 train_time:467471ms step_avg:373.68ms
step:1252/1775 train_time:468130ms step_avg:373.91ms
step:1253/1775 train_time:468789ms step_avg:374.13ms
step:1254/1775 train_time:469452ms step_avg:374.36ms
step:1255/1775 train_time:470111ms step_avg:374.59ms
step:1256/1775 train_time:470770ms step_avg:374.82ms
step:1257/1775 train_time:471428ms step_avg:375.04ms
step:1258/1775 train_time:472089ms step_avg:375.27ms
step:1259/1775 train_time:472746ms step_avg:375.49ms
step:1260/1775 train_time:473407ms step_avg:375.72ms
step:1261/1775 train_time:474068ms step_avg:375.95ms
step:1262/1775 train_time:474728ms step_avg:376.17ms
step:1263/1775 train_time:475389ms step_avg:376.40ms
step:1264/1775 train_time:476051ms step_avg:376.62ms
step:1265/1775 train_time:476713ms step_avg:376.85ms
step:1266/1775 train_time:477375ms step_avg:377.07ms
step:1267/1775 train_time:478034ms step_avg:377.30ms
step:1268/1775 train_time:478696ms step_avg:377.52ms
step:1269/1775 train_time:479358ms step_avg:377.74ms
step:1270/1775 train_time:480023ms step_avg:377.97ms
step:1271/1775 train_time:480683ms step_avg:378.19ms
step:1272/1775 train_time:481346ms step_avg:378.42ms
step:1273/1775 train_time:482006ms step_avg:378.64ms
step:1274/1775 train_time:482668ms step_avg:378.86ms
step:1275/1775 train_time:483327ms step_avg:379.08ms
step:1276/1775 train_time:483990ms step_avg:379.30ms
step:1277/1775 train_time:484648ms step_avg:379.52ms
step:1278/1775 train_time:485307ms step_avg:379.74ms
step:1279/1775 train_time:485968ms step_avg:379.96ms
step:1280/1775 train_time:486629ms step_avg:380.18ms
step:1281/1775 train_time:487288ms step_avg:380.40ms
step:1282/1775 train_time:487948ms step_avg:380.61ms
step:1283/1775 train_time:488606ms step_avg:380.83ms
step:1284/1775 train_time:489268ms step_avg:381.05ms
step:1285/1775 train_time:489927ms step_avg:381.27ms
step:1286/1775 train_time:490590ms step_avg:381.49ms
step:1287/1775 train_time:491249ms step_avg:381.70ms
step:1288/1775 train_time:491910ms step_avg:381.92ms
step:1289/1775 train_time:492567ms step_avg:382.13ms
step:1290/1775 train_time:493228ms step_avg:382.35ms
step:1291/1775 train_time:493889ms step_avg:382.56ms
step:1292/1775 train_time:494548ms step_avg:382.78ms
step:1293/1775 train_time:495205ms step_avg:382.99ms
step:1294/1775 train_time:495867ms step_avg:383.21ms
step:1295/1775 train_time:496527ms step_avg:383.42ms
step:1296/1775 train_time:497187ms step_avg:383.63ms
step:1297/1775 train_time:497848ms step_avg:383.85ms
step:1298/1775 train_time:498507ms step_avg:384.06ms
step:1299/1775 train_time:499168ms step_avg:384.27ms
step:1300/1775 train_time:499830ms step_avg:384.48ms
step:1301/1775 train_time:500488ms step_avg:384.69ms
step:1302/1775 train_time:501149ms step_avg:384.91ms
step:1303/1775 train_time:501809ms step_avg:385.12ms
step:1304/1775 train_time:502471ms step_avg:385.33ms
step:1305/1775 train_time:503129ms step_avg:385.54ms
step:1306/1775 train_time:503791ms step_avg:385.75ms
step:1307/1775 train_time:504451ms step_avg:385.96ms
step:1308/1775 train_time:505113ms step_avg:386.17ms
step:1309/1775 train_time:505770ms step_avg:386.38ms
step:1310/1775 train_time:506434ms step_avg:386.59ms
step:1311/1775 train_time:507093ms step_avg:386.80ms
step:1312/1775 train_time:507753ms step_avg:387.01ms
step:1313/1775 train_time:508412ms step_avg:387.21ms
step:1314/1775 train_time:509071ms step_avg:387.42ms
step:1315/1775 train_time:509732ms step_avg:387.63ms
step:1316/1775 train_time:510393ms step_avg:387.84ms
step:1317/1775 train_time:511051ms step_avg:388.04ms
step:1318/1775 train_time:511711ms step_avg:388.25ms
step:1319/1775 train_time:512372ms step_avg:388.45ms
step:1320/1775 train_time:513033ms step_avg:388.66ms
step:1321/1775 train_time:513693ms step_avg:388.87ms
step:1322/1775 train_time:514355ms step_avg:389.07ms
step:1323/1775 train_time:515016ms step_avg:389.28ms
step:1324/1775 train_time:515679ms step_avg:389.49ms
step:1325/1775 train_time:516337ms step_avg:389.69ms
step:1326/1775 train_time:517000ms step_avg:389.89ms
step:1327/1775 train_time:517662ms step_avg:390.10ms
step:1328/1775 train_time:518326ms step_avg:390.31ms
step:1329/1775 train_time:518985ms step_avg:390.51ms
step:1330/1775 train_time:519645ms step_avg:390.71ms
step:1331/1775 train_time:520307ms step_avg:390.91ms
step:1332/1775 train_time:520968ms step_avg:391.12ms
step:1333/1775 train_time:521626ms step_avg:391.32ms
step:1334/1775 train_time:522286ms step_avg:391.52ms
step:1335/1775 train_time:522946ms step_avg:391.72ms
step:1336/1775 train_time:523607ms step_avg:391.92ms
step:1337/1775 train_time:524267ms step_avg:392.12ms
step:1338/1775 train_time:524929ms step_avg:392.32ms
step:1339/1775 train_time:525589ms step_avg:392.52ms
step:1340/1775 train_time:526250ms step_avg:392.72ms
step:1341/1775 train_time:526907ms step_avg:392.92ms
step:1342/1775 train_time:527569ms step_avg:393.12ms
step:1343/1775 train_time:528227ms step_avg:393.32ms
step:1344/1775 train_time:528888ms step_avg:393.52ms
step:1345/1775 train_time:529549ms step_avg:393.72ms
step:1346/1775 train_time:530210ms step_avg:393.91ms
step:1347/1775 train_time:530869ms step_avg:394.11ms
step:1348/1775 train_time:531529ms step_avg:394.31ms
step:1349/1775 train_time:532189ms step_avg:394.51ms
step:1350/1775 train_time:532850ms step_avg:394.70ms
step:1351/1775 train_time:533508ms step_avg:394.90ms
step:1352/1775 train_time:534170ms step_avg:395.10ms
step:1353/1775 train_time:534830ms step_avg:395.29ms
step:1354/1775 train_time:535492ms step_avg:395.49ms
step:1355/1775 train_time:536151ms step_avg:395.68ms
step:1356/1775 train_time:536812ms step_avg:395.88ms
step:1357/1775 train_time:537468ms step_avg:396.07ms
step:1358/1775 train_time:538129ms step_avg:396.27ms
step:1359/1775 train_time:538788ms step_avg:396.46ms
step:1360/1775 train_time:539451ms step_avg:396.66ms
step:1361/1775 train_time:540109ms step_avg:396.85ms
step:1362/1775 train_time:540771ms step_avg:397.04ms
step:1363/1775 train_time:541429ms step_avg:397.23ms
step:1364/1775 train_time:542091ms step_avg:397.43ms
step:1365/1775 train_time:542751ms step_avg:397.62ms
step:1366/1775 train_time:543409ms step_avg:397.81ms
step:1367/1775 train_time:544068ms step_avg:398.00ms
step:1368/1775 train_time:544731ms step_avg:398.20ms
step:1369/1775 train_time:545391ms step_avg:398.39ms
step:1370/1775 train_time:546053ms step_avg:398.58ms
step:1371/1775 train_time:546711ms step_avg:398.77ms
step:1372/1775 train_time:547372ms step_avg:398.96ms
step:1373/1775 train_time:548030ms step_avg:399.15ms
step:1374/1775 train_time:548688ms step_avg:399.34ms
step:1375/1775 train_time:549349ms step_avg:399.53ms
step:1376/1775 train_time:550009ms step_avg:399.72ms
step:1377/1775 train_time:550670ms step_avg:399.91ms
step:1378/1775 train_time:551327ms step_avg:400.09ms
step:1379/1775 train_time:551987ms step_avg:400.28ms
step:1380/1775 train_time:552650ms step_avg:400.47ms
step:1381/1775 train_time:553308ms step_avg:400.66ms
step:1382/1775 train_time:553971ms step_avg:400.85ms
step:1383/1775 train_time:554629ms step_avg:401.03ms
step:1384/1775 train_time:555291ms step_avg:401.22ms
step:1385/1775 train_time:555949ms step_avg:401.41ms
step:1386/1775 train_time:556609ms step_avg:401.59ms
step:1387/1775 train_time:557268ms step_avg:401.78ms
step:1388/1775 train_time:557931ms step_avg:401.97ms
step:1389/1775 train_time:558591ms step_avg:402.15ms
step:1390/1775 train_time:559251ms step_avg:402.34ms
step:1391/1775 train_time:559911ms step_avg:402.52ms
step:1392/1775 train_time:560572ms step_avg:402.71ms
step:1393/1775 train_time:561232ms step_avg:402.89ms
step:1394/1775 train_time:561892ms step_avg:403.08ms
step:1395/1775 train_time:562551ms step_avg:403.26ms
step:1396/1775 train_time:563214ms step_avg:403.45ms
step:1397/1775 train_time:563873ms step_avg:403.63ms
step:1398/1775 train_time:564536ms step_avg:403.82ms
step:1399/1775 train_time:565197ms step_avg:404.00ms
step:1400/1775 train_time:565859ms step_avg:404.18ms
step:1401/1775 train_time:566519ms step_avg:404.37ms
step:1402/1775 train_time:567180ms step_avg:404.55ms
step:1403/1775 train_time:567841ms step_avg:404.73ms
step:1404/1775 train_time:568504ms step_avg:404.92ms
step:1405/1775 train_time:569164ms step_avg:405.10ms
step:1406/1775 train_time:569826ms step_avg:405.28ms
step:1407/1775 train_time:570486ms step_avg:405.46ms
step:1408/1775 train_time:571148ms step_avg:405.65ms
step:1409/1775 train_time:571808ms step_avg:405.83ms
step:1410/1775 train_time:572468ms step_avg:406.01ms
step:1411/1775 train_time:573128ms step_avg:406.19ms
step:1412/1775 train_time:573789ms step_avg:406.37ms
step:1413/1775 train_time:574449ms step_avg:406.55ms
step:1414/1775 train_time:575109ms step_avg:406.73ms
step:1415/1775 train_time:575769ms step_avg:406.90ms
step:1416/1775 train_time:576428ms step_avg:407.08ms
step:1417/1775 train_time:577088ms step_avg:407.26ms
step:1418/1775 train_time:577749ms step_avg:407.44ms
step:1419/1775 train_time:578410ms step_avg:407.62ms
step:1420/1775 train_time:579073ms step_avg:407.80ms
step:1421/1775 train_time:579730ms step_avg:407.97ms
step:1422/1775 train_time:580391ms step_avg:408.15ms
step:1423/1775 train_time:581049ms step_avg:408.33ms
step:1424/1775 train_time:581708ms step_avg:408.50ms
step:1425/1775 train_time:582369ms step_avg:408.68ms
step:1426/1775 train_time:583029ms step_avg:408.86ms
step:1427/1775 train_time:583689ms step_avg:409.03ms
step:1428/1775 train_time:584349ms step_avg:409.21ms
step:1429/1775 train_time:585008ms step_avg:409.38ms
step:1430/1775 train_time:585670ms step_avg:409.56ms
step:1431/1775 train_time:586330ms step_avg:409.73ms
step:1432/1775 train_time:586992ms step_avg:409.91ms
step:1433/1775 train_time:587653ms step_avg:410.09ms
step:1434/1775 train_time:588314ms step_avg:410.26ms
step:1435/1775 train_time:588973ms step_avg:410.43ms
step:1436/1775 train_time:589634ms step_avg:410.61ms
step:1437/1775 train_time:590293ms step_avg:410.78ms
step:1438/1775 train_time:590954ms step_avg:410.96ms
step:1439/1775 train_time:591614ms step_avg:411.13ms
step:1440/1775 train_time:592277ms step_avg:411.30ms
step:1441/1775 train_time:592940ms step_avg:411.48ms
step:1442/1775 train_time:593602ms step_avg:411.65ms
step:1443/1775 train_time:594262ms step_avg:411.82ms
step:1444/1775 train_time:594925ms step_avg:412.00ms
step:1445/1775 train_time:595586ms step_avg:412.17ms
step:1446/1775 train_time:596248ms step_avg:412.34ms
step:1447/1775 train_time:596905ms step_avg:412.51ms
step:1448/1775 train_time:597566ms step_avg:412.68ms
step:1449/1775 train_time:598226ms step_avg:412.85ms
step:1450/1775 train_time:598888ms step_avg:413.03ms
step:1451/1775 train_time:599547ms step_avg:413.20ms
step:1452/1775 train_time:600206ms step_avg:413.37ms
step:1453/1775 train_time:600864ms step_avg:413.53ms
step:1454/1775 train_time:601526ms step_avg:413.70ms
step:1455/1775 train_time:602185ms step_avg:413.87ms
step:1456/1775 train_time:602847ms step_avg:414.04ms
step:1457/1775 train_time:603505ms step_avg:414.21ms
step:1458/1775 train_time:604167ms step_avg:414.38ms
step:1459/1775 train_time:604825ms step_avg:414.55ms
step:1460/1775 train_time:605489ms step_avg:414.72ms
step:1461/1775 train_time:606146ms step_avg:414.88ms
step:1462/1775 train_time:606807ms step_avg:415.05ms
step:1463/1775 train_time:607468ms step_avg:415.22ms
step:1464/1775 train_time:608129ms step_avg:415.39ms
step:1465/1775 train_time:608789ms step_avg:415.56ms
step:1466/1775 train_time:609451ms step_avg:415.72ms
step:1467/1775 train_time:610111ms step_avg:415.89ms
step:1468/1775 train_time:610771ms step_avg:416.06ms
step:1469/1775 train_time:611433ms step_avg:416.22ms
step:1470/1775 train_time:612094ms step_avg:416.39ms
step:1471/1775 train_time:612752ms step_avg:416.55ms
step:1472/1775 train_time:613414ms step_avg:416.72ms
step:1473/1775 train_time:614074ms step_avg:416.89ms
step:1474/1775 train_time:614734ms step_avg:417.05ms
step:1475/1775 train_time:615392ms step_avg:417.21ms
step:1476/1775 train_time:616051ms step_avg:417.38ms
step:1477/1775 train_time:616712ms step_avg:417.54ms
step:1478/1775 train_time:617374ms step_avg:417.71ms
step:1479/1775 train_time:618032ms step_avg:417.87ms
step:1480/1775 train_time:618693ms step_avg:418.04ms
step:1481/1775 train_time:619353ms step_avg:418.20ms
step:1482/1775 train_time:620013ms step_avg:418.36ms
step:1483/1775 train_time:620672ms step_avg:418.52ms
step:1484/1775 train_time:621334ms step_avg:418.69ms
step:1485/1775 train_time:621992ms step_avg:418.85ms
step:1486/1775 train_time:622653ms step_avg:419.01ms
step:1487/1775 train_time:623311ms step_avg:419.17ms
step:1488/1775 train_time:623971ms step_avg:419.34ms
step:1489/1775 train_time:624631ms step_avg:419.50ms
step:1490/1775 train_time:625289ms step_avg:419.66ms
step:1491/1775 train_time:625947ms step_avg:419.82ms
step:1492/1775 train_time:626606ms step_avg:419.98ms
step:1493/1775 train_time:627266ms step_avg:420.14ms
step:1494/1775 train_time:627926ms step_avg:420.30ms
step:1495/1775 train_time:628584ms step_avg:420.46ms
step:1496/1775 train_time:629246ms step_avg:420.62ms
step:1497/1775 train_time:629907ms step_avg:420.78ms
step:1498/1775 train_time:630569ms step_avg:420.94ms
step:1499/1775 train_time:631229ms step_avg:421.10ms
step:1500/1775 train_time:631891ms step_avg:421.26ms
step:1500/1775 val_loss:3.3763 train_time:631992ms step_avg:421.33ms
step:1501/1775 train_time:632547ms step_avg:421.42ms
step:1502/1775 train_time:633207ms step_avg:421.58ms
step:1503/1775 train_time:633863ms step_avg:421.73ms
step:1504/1775 train_time:634522ms step_avg:421.89ms
step:1505/1775 train_time:635182ms step_avg:422.05ms
step:1506/1775 train_time:635842ms step_avg:422.21ms
step:1507/1775 train_time:636498ms step_avg:422.36ms
step:1508/1775 train_time:637158ms step_avg:422.52ms
step:1509/1775 train_time:637817ms step_avg:422.68ms
step:1510/1775 train_time:638474ms step_avg:422.83ms
step:1511/1775 train_time:639131ms step_avg:422.99ms
step:1512/1775 train_time:639788ms step_avg:423.14ms
step:1513/1775 train_time:640444ms step_avg:423.29ms
step:1514/1775 train_time:641105ms step_avg:423.45ms
step:1515/1775 train_time:641761ms step_avg:423.60ms
step:1516/1775 train_time:642421ms step_avg:423.76ms
step:1517/1775 train_time:643077ms step_avg:423.91ms
step:1518/1775 train_time:643738ms step_avg:424.07ms
step:1519/1775 train_time:644397ms step_avg:424.22ms
step:1520/1775 train_time:645056ms step_avg:424.38ms
step:1521/1775 train_time:645713ms step_avg:424.53ms
step:1522/1775 train_time:646372ms step_avg:424.69ms
step:1523/1775 train_time:647028ms step_avg:424.84ms
step:1524/1775 train_time:647685ms step_avg:424.99ms
step:1525/1775 train_time:648341ms step_avg:425.14ms
step:1526/1775 train_time:649001ms step_avg:425.30ms
step:1527/1775 train_time:649658ms step_avg:425.45ms
step:1528/1775 train_time:650318ms step_avg:425.60ms
step:1529/1775 train_time:650976ms step_avg:425.75ms
step:1530/1775 train_time:651636ms step_avg:425.91ms
step:1531/1775 train_time:652293ms step_avg:426.06ms
step:1532/1775 train_time:652951ms step_avg:426.21ms
step:1533/1775 train_time:653610ms step_avg:426.36ms
step:1534/1775 train_time:654268ms step_avg:426.51ms
step:1535/1775 train_time:654924ms step_avg:426.66ms
step:1536/1775 train_time:655584ms step_avg:426.81ms
step:1537/1775 train_time:656242ms step_avg:426.96ms
step:1538/1775 train_time:656903ms step_avg:427.12ms
step:1539/1775 train_time:657560ms step_avg:427.26ms
step:1540/1775 train_time:658220ms step_avg:427.42ms
step:1541/1775 train_time:658879ms step_avg:427.57ms
step:1542/1775 train_time:659538ms step_avg:427.72ms
step:1543/1775 train_time:660197ms step_avg:427.87ms
step:1544/1775 train_time:660855ms step_avg:428.01ms
step:1545/1775 train_time:661509ms step_avg:428.16ms
step:1546/1775 train_time:662166ms step_avg:428.31ms
step:1547/1775 train_time:662822ms step_avg:428.46ms
step:1548/1775 train_time:663481ms step_avg:428.61ms
step:1549/1775 train_time:664138ms step_avg:428.75ms
step:1550/1775 train_time:664797ms step_avg:428.90ms
step:1551/1775 train_time:665454ms step_avg:429.05ms
step:1552/1775 train_time:666113ms step_avg:429.20ms
step:1553/1775 train_time:666766ms step_avg:429.34ms
step:1554/1775 train_time:667425ms step_avg:429.49ms
step:1555/1775 train_time:668084ms step_avg:429.64ms
step:1556/1775 train_time:668740ms step_avg:429.78ms
step:1557/1775 train_time:669399ms step_avg:429.93ms
step:1558/1775 train_time:670058ms step_avg:430.08ms
step:1559/1775 train_time:670716ms step_avg:430.22ms
step:1560/1775 train_time:671374ms step_avg:430.37ms
step:1561/1775 train_time:672030ms step_avg:430.51ms
step:1562/1775 train_time:672689ms step_avg:430.66ms
step:1563/1775 train_time:673344ms step_avg:430.80ms
step:1564/1775 train_time:674003ms step_avg:430.95ms
step:1565/1775 train_time:674662ms step_avg:431.09ms
step:1566/1775 train_time:675321ms step_avg:431.24ms
step:1567/1775 train_time:675978ms step_avg:431.38ms
step:1568/1775 train_time:676639ms step_avg:431.53ms
step:1569/1775 train_time:677296ms step_avg:431.67ms
step:1570/1775 train_time:677956ms step_avg:431.82ms
step:1571/1775 train_time:678614ms step_avg:431.96ms
step:1572/1775 train_time:679271ms step_avg:432.11ms
step:1573/1775 train_time:679927ms step_avg:432.25ms
step:1574/1775 train_time:680584ms step_avg:432.39ms
step:1575/1775 train_time:681241ms step_avg:432.53ms
step:1576/1775 train_time:681901ms step_avg:432.68ms
step:1577/1775 train_time:682556ms step_avg:432.82ms
step:1578/1775 train_time:683215ms step_avg:432.96ms
step:1579/1775 train_time:683875ms step_avg:433.11ms
step:1580/1775 train_time:684532ms step_avg:433.25ms
step:1581/1775 train_time:685187ms step_avg:433.39ms
step:1582/1775 train_time:685847ms step_avg:433.53ms
step:1583/1775 train_time:686503ms step_avg:433.67ms
step:1584/1775 train_time:687161ms step_avg:433.81ms
step:1585/1775 train_time:687820ms step_avg:433.96ms
step:1586/1775 train_time:688479ms step_avg:434.10ms
step:1587/1775 train_time:689135ms step_avg:434.24ms
step:1588/1775 train_time:689797ms step_avg:434.38ms
step:1589/1775 train_time:690454ms step_avg:434.52ms
step:1590/1775 train_time:691112ms step_avg:434.66ms
step:1591/1775 train_time:691767ms step_avg:434.80ms
step:1592/1775 train_time:692424ms step_avg:434.94ms
step:1593/1775 train_time:693080ms step_avg:435.08ms
step:1594/1775 train_time:693739ms step_avg:435.22ms
step:1595/1775 train_time:694398ms step_avg:435.36ms
step:1596/1775 train_time:695059ms step_avg:435.50ms
step:1597/1775 train_time:695716ms step_avg:435.64ms
step:1598/1775 train_time:696376ms step_avg:435.78ms
step:1599/1775 train_time:697034ms step_avg:435.92ms
step:1600/1775 train_time:697692ms step_avg:436.06ms
step:1601/1775 train_time:698347ms step_avg:436.19ms
step:1602/1775 train_time:699004ms step_avg:436.33ms
step:1603/1775 train_time:699661ms step_avg:436.47ms
step:1604/1775 train_time:700320ms step_avg:436.61ms
step:1605/1775 train_time:700978ms step_avg:436.75ms
step:1606/1775 train_time:701638ms step_avg:436.89ms
step:1607/1775 train_time:702296ms step_avg:437.02ms
step:1608/1775 train_time:702955ms step_avg:437.16ms
step:1609/1775 train_time:703613ms step_avg:437.30ms
step:1610/1775 train_time:704270ms step_avg:437.43ms
step:1611/1775 train_time:704927ms step_avg:437.57ms
step:1612/1775 train_time:705585ms step_avg:437.71ms
step:1613/1775 train_time:706242ms step_avg:437.84ms
step:1614/1775 train_time:706903ms step_avg:437.98ms
step:1615/1775 train_time:707560ms step_avg:438.12ms
step:1616/1775 train_time:708219ms step_avg:438.25ms
step:1617/1775 train_time:708877ms step_avg:438.39ms
step:1618/1775 train_time:709535ms step_avg:438.53ms
step:1619/1775 train_time:710195ms step_avg:438.66ms
step:1620/1775 train_time:710857ms step_avg:438.80ms
step:1621/1775 train_time:711513ms step_avg:438.93ms
step:1622/1775 train_time:712172ms step_avg:439.07ms
step:1623/1775 train_time:712827ms step_avg:439.20ms
step:1624/1775 train_time:713483ms step_avg:439.34ms
step:1625/1775 train_time:714141ms step_avg:439.47ms
step:1626/1775 train_time:714801ms step_avg:439.61ms
step:1627/1775 train_time:715458ms step_avg:439.74ms
step:1628/1775 train_time:716118ms step_avg:439.88ms
step:1629/1775 train_time:716776ms step_avg:440.01ms
step:1630/1775 train_time:717434ms step_avg:440.14ms
step:1631/1775 train_time:718093ms step_avg:440.28ms
step:1632/1775 train_time:718753ms step_avg:440.41ms
step:1633/1775 train_time:719410ms step_avg:440.54ms
step:1634/1775 train_time:720069ms step_avg:440.68ms
step:1635/1775 train_time:720725ms step_avg:440.81ms
step:1636/1775 train_time:721383ms step_avg:440.94ms
step:1637/1775 train_time:722041ms step_avg:441.08ms
step:1638/1775 train_time:722701ms step_avg:441.21ms
step:1639/1775 train_time:723359ms step_avg:441.34ms
step:1640/1775 train_time:724018ms step_avg:441.47ms
step:1641/1775 train_time:724678ms step_avg:441.61ms
step:1642/1775 train_time:725336ms step_avg:441.74ms
step:1643/1775 train_time:725994ms step_avg:441.87ms
step:1644/1775 train_time:726651ms step_avg:442.00ms
step:1645/1775 train_time:727306ms step_avg:442.13ms
step:1646/1775 train_time:727964ms step_avg:442.26ms
step:1647/1775 train_time:728620ms step_avg:442.39ms
step:1648/1775 train_time:729279ms step_avg:442.52ms
step:1649/1775 train_time:729936ms step_avg:442.65ms
step:1650/1775 train_time:730596ms step_avg:442.79ms
step:1651/1775 train_time:731252ms step_avg:442.91ms
step:1652/1775 train_time:731911ms step_avg:443.05ms
step:1653/1775 train_time:732566ms step_avg:443.17ms
step:1654/1775 train_time:733223ms step_avg:443.30ms
step:1655/1775 train_time:733881ms step_avg:443.43ms
step:1656/1775 train_time:734540ms step_avg:443.56ms
step:1657/1775 train_time:735197ms step_avg:443.69ms
step:1658/1775 train_time:735857ms step_avg:443.82ms
step:1659/1775 train_time:736516ms step_avg:443.95ms
step:1660/1775 train_time:737172ms step_avg:444.08ms
step:1661/1775 train_time:737829ms step_avg:444.21ms
step:1662/1775 train_time:738486ms step_avg:444.34ms
step:1663/1775 train_time:739142ms step_avg:444.46ms
step:1664/1775 train_time:739803ms step_avg:444.59ms
step:1665/1775 train_time:740460ms step_avg:444.72ms
step:1666/1775 train_time:741119ms step_avg:444.85ms
step:1667/1775 train_time:741776ms step_avg:444.98ms
step:1668/1775 train_time:742436ms step_avg:445.11ms
step:1669/1775 train_time:743090ms step_avg:445.23ms
step:1670/1775 train_time:743746ms step_avg:445.36ms
step:1671/1775 train_time:744403ms step_avg:445.48ms
step:1672/1775 train_time:745062ms step_avg:445.61ms
step:1673/1775 train_time:745720ms step_avg:445.74ms
step:1674/1775 train_time:746381ms step_avg:445.87ms
step:1675/1775 train_time:747040ms step_avg:445.99ms
step:1676/1775 train_time:747699ms step_avg:446.12ms
step:1677/1775 train_time:748356ms step_avg:446.25ms
step:1678/1775 train_time:749015ms step_avg:446.37ms
step:1679/1775 train_time:749672ms step_avg:446.50ms
step:1680/1775 train_time:750327ms step_avg:446.62ms
step:1681/1775 train_time:750984ms step_avg:446.75ms
step:1682/1775 train_time:751642ms step_avg:446.87ms
step:1683/1775 train_time:752299ms step_avg:447.00ms
step:1684/1775 train_time:752961ms step_avg:447.13ms
step:1685/1775 train_time:753617ms step_avg:447.25ms
step:1686/1775 train_time:754277ms step_avg:447.38ms
step:1687/1775 train_time:754935ms step_avg:447.50ms
step:1688/1775 train_time:755589ms step_avg:447.62ms
step:1689/1775 train_time:756247ms step_avg:447.75ms
step:1690/1775 train_time:756906ms step_avg:447.87ms
step:1691/1775 train_time:757561ms step_avg:448.00ms
step:1692/1775 train_time:758222ms step_avg:448.12ms
step:1693/1775 train_time:758880ms step_avg:448.25ms
step:1694/1775 train_time:759539ms step_avg:448.37ms
step:1695/1775 train_time:760196ms step_avg:448.49ms
step:1696/1775 train_time:760853ms step_avg:448.62ms
step:1697/1775 train_time:761508ms step_avg:448.74ms
step:1698/1775 train_time:762165ms step_avg:448.86ms
step:1699/1775 train_time:762823ms step_avg:448.98ms
step:1700/1775 train_time:763482ms step_avg:449.11ms
step:1701/1775 train_time:764141ms step_avg:449.23ms
step:1702/1775 train_time:764801ms step_avg:449.35ms
step:1703/1775 train_time:765458ms step_avg:449.48ms
step:1704/1775 train_time:766119ms step_avg:449.60ms
step:1705/1775 train_time:766777ms step_avg:449.72ms
step:1706/1775 train_time:767437ms step_avg:449.85ms
step:1707/1775 train_time:768095ms step_avg:449.97ms
step:1708/1775 train_time:768753ms step_avg:450.09ms
step:1709/1775 train_time:769409ms step_avg:450.21ms
step:1710/1775 train_time:770066ms step_avg:450.33ms
step:1711/1775 train_time:770723ms step_avg:450.45ms
step:1712/1775 train_time:771381ms step_avg:450.57ms
step:1713/1775 train_time:772042ms step_avg:450.70ms
step:1714/1775 train_time:772702ms step_avg:450.82ms
step:1715/1775 train_time:773357ms step_avg:450.94ms
step:1716/1775 train_time:774015ms step_avg:451.06ms
step:1717/1775 train_time:774672ms step_avg:451.18ms
step:1718/1775 train_time:775329ms step_avg:451.30ms
step:1719/1775 train_time:775986ms step_avg:451.42ms
step:1720/1775 train_time:776642ms step_avg:451.54ms
step:1721/1775 train_time:777301ms step_avg:451.66ms
step:1722/1775 train_time:777961ms step_avg:451.78ms
step:1723/1775 train_time:778619ms step_avg:451.90ms
step:1724/1775 train_time:779279ms step_avg:452.02ms
step:1725/1775 train_time:779937ms step_avg:452.14ms
step:1726/1775 train_time:780595ms step_avg:452.26ms
step:1727/1775 train_time:781251ms step_avg:452.37ms
step:1728/1775 train_time:781909ms step_avg:452.49ms
step:1729/1775 train_time:782564ms step_avg:452.61ms
step:1730/1775 train_time:783223ms step_avg:452.73ms
step:1731/1775 train_time:783881ms step_avg:452.85ms
step:1732/1775 train_time:784540ms step_avg:452.97ms
step:1733/1775 train_time:785198ms step_avg:453.09ms
step:1734/1775 train_time:785857ms step_avg:453.20ms
step:1735/1775 train_time:786513ms step_avg:453.32ms
step:1736/1775 train_time:787180ms step_avg:453.44ms
step:1737/1775 train_time:787839ms step_avg:453.56ms
step:1738/1775 train_time:788500ms step_avg:453.68ms
step:1739/1775 train_time:789161ms step_avg:453.80ms
step:1740/1775 train_time:789822ms step_avg:453.92ms
step:1741/1775 train_time:790481ms step_avg:454.04ms
step:1742/1775 train_time:791144ms step_avg:454.16ms
step:1743/1775 train_time:791804ms step_avg:454.28ms
step:1744/1775 train_time:792464ms step_avg:454.39ms
step:1745/1775 train_time:793124ms step_avg:454.51ms
step:1746/1775 train_time:793786ms step_avg:454.63ms
step:1747/1775 train_time:794444ms step_avg:454.75ms
step:1748/1775 train_time:795106ms step_avg:454.87ms
step:1749/1775 train_time:795765ms step_avg:454.98ms
step:1750/1775 train_time:796426ms step_avg:455.10ms
step:1750/1775 val_loss:3.2845 train_time:796526ms step_avg:455.16ms
step:1751/1775 train_time:797087ms step_avg:455.22ms
step:1752/1775 train_time:797749ms step_avg:455.34ms
step:1753/1775 train_time:798408ms step_avg:455.45ms
step:1754/1775 train_time:799071ms step_avg:455.57ms
step:1755/1775 train_time:799730ms step_avg:455.69ms
step:1756/1775 train_time:800392ms step_avg:455.80ms
step:1757/1775 train_time:801052ms step_avg:455.92ms
step:1758/1775 train_time:801713ms step_avg:456.04ms
step:1759/1775 train_time:802373ms step_avg:456.15ms
step:1760/1775 train_time:803034ms step_avg:456.27ms
step:1761/1775 train_time:803693ms step_avg:456.38ms
step:1762/1775 train_time:804354ms step_avg:456.50ms
step:1763/1775 train_time:805014ms step_avg:456.62ms
step:1764/1775 train_time:805676ms step_avg:456.73ms
step:1765/1775 train_time:806335ms step_avg:456.85ms
step:1766/1775 train_time:806996ms step_avg:456.96ms
step:1767/1775 train_time:807656ms step_avg:457.08ms
step:1768/1775 train_time:808317ms step_avg:457.19ms
step:1769/1775 train_time:808977ms step_avg:457.31ms
step:1770/1775 train_time:809638ms step_avg:457.42ms
step:1771/1775 train_time:810295ms step_avg:457.54ms
step:1772/1775 train_time:810956ms step_avg:457.65ms
step:1773/1775 train_time:811615ms step_avg:457.76ms
step:1774/1775 train_time:812276ms step_avg:457.88ms
step:1775/1775 train_time:812933ms step_avg:457.99ms
step:1775/1775 val_loss:3.2780 train_time:813033ms step_avg:458.05ms
peak memory allocated: 32827 MiB reserved: 46440 MiB

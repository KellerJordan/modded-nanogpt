import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,  #
                                 M, N, K,  #
                                 BLOCK_SIZE_M: tl.constexpr,  #
                                 BLOCK_SIZE_N: tl.constexpr,  #
                                 BLOCK_SIZE_K: tl.constexpr,  #
                                 GROUP_SIZE_M: tl.constexpr,  #
                                 NUM_SMS: tl.constexpr,  #
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,#
        M, N, K,  #
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,  #
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.3 (main, Nov  6 2025, 13:44:16) [GCC 13.3.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Jan 10 23:54:51 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:8D:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1183MiB /  81559MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           18680      C   /home/ubuntu/venv/bin/python3          1174MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8315 train_time:0ms step_avg:0.29ms
step:1/1775 train_time:255ms step_avg:255.39ms
step:2/1775 train_time:500ms step_avg:250.24ms
step:3/1775 train_time:747ms step_avg:248.94ms
step:4/1775 train_time:992ms step_avg:248.06ms
step:5/1775 train_time:1241ms step_avg:248.20ms
step:6/1775 train_time:1490ms step_avg:248.32ms
step:7/1775 train_time:1736ms step_avg:248.02ms
step:8/1775 train_time:1982ms step_avg:247.71ms
step:9/1775 train_time:2230ms step_avg:247.77ms
step:10/1775 train_time:2478ms step_avg:247.84ms
step:11/1775 train_time:2727ms step_avg:247.90ms
step:12/1775 train_time:2974ms step_avg:247.86ms
step:13/1775 train_time:3223ms step_avg:247.90ms
step:14/1775 train_time:3471ms step_avg:247.95ms
step:15/1775 train_time:3719ms step_avg:247.95ms
step:16/1775 train_time:3968ms step_avg:247.98ms
step:17/1775 train_time:4218ms step_avg:248.09ms
step:18/1775 train_time:4466ms step_avg:248.10ms
step:19/1775 train_time:4713ms step_avg:248.05ms
step:20/1775 train_time:4960ms step_avg:247.98ms
step:21/1775 train_time:5205ms step_avg:247.88ms
step:22/1775 train_time:5452ms step_avg:247.84ms
step:23/1775 train_time:5701ms step_avg:247.85ms
step:24/1775 train_time:5947ms step_avg:247.79ms
step:25/1775 train_time:6195ms step_avg:247.81ms
step:26/1775 train_time:6442ms step_avg:247.79ms
step:27/1775 train_time:6690ms step_avg:247.78ms
step:28/1775 train_time:6938ms step_avg:247.79ms
step:29/1775 train_time:7187ms step_avg:247.82ms
step:30/1775 train_time:7436ms step_avg:247.87ms
step:31/1775 train_time:7685ms step_avg:247.90ms
step:32/1775 train_time:7932ms step_avg:247.87ms
step:33/1775 train_time:8178ms step_avg:247.81ms
step:34/1775 train_time:8424ms step_avg:247.76ms
step:35/1775 train_time:8672ms step_avg:247.76ms
step:36/1775 train_time:8918ms step_avg:247.73ms
step:37/1775 train_time:9167ms step_avg:247.75ms
step:38/1775 train_time:9414ms step_avg:247.75ms
step:39/1775 train_time:9663ms step_avg:247.77ms
step:40/1775 train_time:9909ms step_avg:247.73ms
step:41/1775 train_time:10155ms step_avg:247.69ms
step:42/1775 train_time:10401ms step_avg:247.65ms
step:43/1775 train_time:10651ms step_avg:247.69ms
step:44/1775 train_time:10899ms step_avg:247.69ms
step:45/1775 train_time:11147ms step_avg:247.71ms
step:46/1775 train_time:11395ms step_avg:247.72ms
step:47/1775 train_time:11643ms step_avg:247.72ms
step:48/1775 train_time:11890ms step_avg:247.70ms
step:49/1775 train_time:12137ms step_avg:247.70ms
step:50/1775 train_time:12384ms step_avg:247.67ms
step:51/1775 train_time:12633ms step_avg:247.70ms
step:52/1775 train_time:12880ms step_avg:247.69ms
step:53/1775 train_time:13126ms step_avg:247.67ms
step:54/1775 train_time:13374ms step_avg:247.66ms
step:55/1775 train_time:13620ms step_avg:247.64ms
step:56/1775 train_time:13868ms step_avg:247.65ms
step:57/1775 train_time:14117ms step_avg:247.67ms
step:58/1775 train_time:14367ms step_avg:247.70ms
step:59/1775 train_time:14615ms step_avg:247.72ms
step:60/1775 train_time:14863ms step_avg:247.72ms
step:61/1775 train_time:15110ms step_avg:247.70ms
step:62/1775 train_time:15358ms step_avg:247.71ms
step:63/1775 train_time:15606ms step_avg:247.72ms
step:64/1775 train_time:15854ms step_avg:247.72ms
step:65/1775 train_time:16102ms step_avg:247.72ms
step:66/1775 train_time:16350ms step_avg:247.73ms
step:67/1775 train_time:16599ms step_avg:247.74ms
step:68/1775 train_time:16846ms step_avg:247.74ms
step:69/1775 train_time:17097ms step_avg:247.78ms
step:70/1775 train_time:17343ms step_avg:247.76ms
step:71/1775 train_time:17589ms step_avg:247.73ms
step:72/1775 train_time:17836ms step_avg:247.72ms
step:73/1775 train_time:18083ms step_avg:247.72ms
step:74/1775 train_time:18332ms step_avg:247.72ms
step:75/1775 train_time:18579ms step_avg:247.72ms
step:76/1775 train_time:18826ms step_avg:247.71ms
step:77/1775 train_time:19074ms step_avg:247.71ms
step:78/1775 train_time:19322ms step_avg:247.72ms
step:79/1775 train_time:19570ms step_avg:247.72ms
step:80/1775 train_time:19818ms step_avg:247.73ms
step:81/1775 train_time:20066ms step_avg:247.73ms
step:82/1775 train_time:20313ms step_avg:247.72ms
step:83/1775 train_time:20560ms step_avg:247.71ms
step:84/1775 train_time:20808ms step_avg:247.72ms
step:85/1775 train_time:21056ms step_avg:247.71ms
step:86/1775 train_time:21304ms step_avg:247.72ms
step:87/1775 train_time:21551ms step_avg:247.72ms
step:88/1775 train_time:21798ms step_avg:247.71ms
step:89/1775 train_time:22046ms step_avg:247.70ms
step:90/1775 train_time:22293ms step_avg:247.70ms
step:91/1775 train_time:22540ms step_avg:247.69ms
step:92/1775 train_time:22787ms step_avg:247.69ms
step:93/1775 train_time:23034ms step_avg:247.68ms
step:94/1775 train_time:23281ms step_avg:247.67ms
step:95/1775 train_time:23529ms step_avg:247.67ms
step:96/1775 train_time:23777ms step_avg:247.68ms
step:97/1775 train_time:24026ms step_avg:247.69ms
step:98/1775 train_time:24275ms step_avg:247.70ms
step:99/1775 train_time:24521ms step_avg:247.69ms
step:100/1775 train_time:24769ms step_avg:247.69ms
step:101/1775 train_time:25017ms step_avg:247.69ms
step:102/1775 train_time:25265ms step_avg:247.70ms
step:103/1775 train_time:25513ms step_avg:247.70ms
step:104/1775 train_time:25760ms step_avg:247.69ms
step:105/1775 train_time:26007ms step_avg:247.69ms
step:106/1775 train_time:26257ms step_avg:247.70ms
step:107/1775 train_time:26505ms step_avg:247.71ms
step:108/1775 train_time:26752ms step_avg:247.71ms
step:109/1775 train_time:26999ms step_avg:247.70ms
step:110/1775 train_time:27247ms step_avg:247.70ms
step:111/1775 train_time:27495ms step_avg:247.70ms
step:112/1775 train_time:27743ms step_avg:247.70ms
step:113/1775 train_time:27989ms step_avg:247.69ms
step:114/1775 train_time:28237ms step_avg:247.69ms
step:115/1775 train_time:28486ms step_avg:247.70ms
step:116/1775 train_time:28733ms step_avg:247.70ms
step:117/1775 train_time:28980ms step_avg:247.69ms
step:118/1775 train_time:29227ms step_avg:247.69ms
step:119/1775 train_time:29475ms step_avg:247.69ms
step:120/1775 train_time:29722ms step_avg:247.68ms
step:121/1775 train_time:29970ms step_avg:247.68ms
step:122/1775 train_time:30216ms step_avg:247.68ms
step:123/1775 train_time:30463ms step_avg:247.67ms
step:124/1775 train_time:30711ms step_avg:247.67ms
step:125/1775 train_time:30959ms step_avg:247.67ms
step:126/1775 train_time:31207ms step_avg:247.67ms
step:127/1775 train_time:31453ms step_avg:247.67ms
step:128/1775 train_time:31700ms step_avg:247.65ms
step:129/1775 train_time:31948ms step_avg:247.66ms
step:130/1775 train_time:32194ms step_avg:247.65ms
step:131/1775 train_time:32441ms step_avg:247.64ms
step:132/1775 train_time:32688ms step_avg:247.64ms
step:133/1775 train_time:32938ms step_avg:247.66ms
step:134/1775 train_time:33186ms step_avg:247.66ms
step:135/1775 train_time:33434ms step_avg:247.66ms
step:136/1775 train_time:33681ms step_avg:247.66ms
step:137/1775 train_time:33929ms step_avg:247.66ms
step:138/1775 train_time:34178ms step_avg:247.67ms
step:139/1775 train_time:34426ms step_avg:247.67ms
step:140/1775 train_time:34672ms step_avg:247.66ms
step:141/1775 train_time:34918ms step_avg:247.65ms
step:142/1775 train_time:35166ms step_avg:247.65ms
step:143/1775 train_time:35413ms step_avg:247.65ms
step:144/1775 train_time:35661ms step_avg:247.65ms
step:145/1775 train_time:35908ms step_avg:247.64ms
step:146/1775 train_time:36154ms step_avg:247.63ms
step:147/1775 train_time:36400ms step_avg:247.62ms
step:148/1775 train_time:36648ms step_avg:247.62ms
step:149/1775 train_time:36896ms step_avg:247.63ms
step:150/1775 train_time:37144ms step_avg:247.63ms
step:151/1775 train_time:37390ms step_avg:247.61ms
step:152/1775 train_time:37638ms step_avg:247.62ms
step:153/1775 train_time:37887ms step_avg:247.63ms
step:154/1775 train_time:38136ms step_avg:247.64ms
step:155/1775 train_time:38383ms step_avg:247.63ms
step:156/1775 train_time:38629ms step_avg:247.62ms
step:157/1775 train_time:38876ms step_avg:247.62ms
step:158/1775 train_time:39122ms step_avg:247.61ms
step:159/1775 train_time:39369ms step_avg:247.60ms
step:160/1775 train_time:39617ms step_avg:247.61ms
step:161/1775 train_time:39866ms step_avg:247.61ms
step:162/1775 train_time:40113ms step_avg:247.61ms
step:163/1775 train_time:40359ms step_avg:247.60ms
step:164/1775 train_time:40608ms step_avg:247.61ms
step:165/1775 train_time:40856ms step_avg:247.61ms
step:166/1775 train_time:41102ms step_avg:247.60ms
step:167/1775 train_time:41348ms step_avg:247.59ms
step:168/1775 train_time:41595ms step_avg:247.59ms
step:169/1775 train_time:41843ms step_avg:247.59ms
step:170/1775 train_time:42091ms step_avg:247.59ms
step:171/1775 train_time:42338ms step_avg:247.59ms
step:172/1775 train_time:42585ms step_avg:247.59ms
step:173/1775 train_time:42832ms step_avg:247.58ms
step:174/1775 train_time:43079ms step_avg:247.58ms
step:175/1775 train_time:43325ms step_avg:247.57ms
step:176/1775 train_time:43572ms step_avg:247.57ms
step:177/1775 train_time:43818ms step_avg:247.56ms
step:178/1775 train_time:44067ms step_avg:247.57ms
step:179/1775 train_time:44316ms step_avg:247.58ms
step:180/1775 train_time:44564ms step_avg:247.58ms
step:181/1775 train_time:44811ms step_avg:247.57ms
step:182/1775 train_time:45058ms step_avg:247.57ms
step:183/1775 train_time:45304ms step_avg:247.56ms
step:184/1775 train_time:45551ms step_avg:247.56ms
step:185/1775 train_time:45797ms step_avg:247.55ms
step:186/1775 train_time:46044ms step_avg:247.55ms
step:187/1775 train_time:46290ms step_avg:247.54ms
step:188/1775 train_time:46537ms step_avg:247.54ms
step:189/1775 train_time:46784ms step_avg:247.54ms
step:190/1775 train_time:47031ms step_avg:247.53ms
step:191/1775 train_time:47277ms step_avg:247.52ms
step:192/1775 train_time:47523ms step_avg:247.52ms
step:193/1775 train_time:47770ms step_avg:247.51ms
step:194/1775 train_time:48016ms step_avg:247.51ms
step:195/1775 train_time:48263ms step_avg:247.50ms
step:196/1775 train_time:48511ms step_avg:247.50ms
step:197/1775 train_time:48758ms step_avg:247.50ms
step:198/1775 train_time:49004ms step_avg:247.49ms
step:199/1775 train_time:49250ms step_avg:247.49ms
step:200/1775 train_time:49497ms step_avg:247.49ms
step:201/1775 train_time:49745ms step_avg:247.49ms
step:202/1775 train_time:49992ms step_avg:247.48ms
step:203/1775 train_time:50239ms step_avg:247.48ms
step:204/1775 train_time:50485ms step_avg:247.48ms
step:205/1775 train_time:50732ms step_avg:247.47ms
step:206/1775 train_time:50977ms step_avg:247.46ms
step:207/1775 train_time:51225ms step_avg:247.47ms
step:208/1775 train_time:51471ms step_avg:247.46ms
step:209/1775 train_time:51720ms step_avg:247.46ms
step:210/1775 train_time:51966ms step_avg:247.46ms
step:211/1775 train_time:52213ms step_avg:247.46ms
step:212/1775 train_time:52460ms step_avg:247.45ms
step:213/1775 train_time:52706ms step_avg:247.45ms
step:214/1775 train_time:52952ms step_avg:247.44ms
step:215/1775 train_time:53201ms step_avg:247.44ms
step:216/1775 train_time:53446ms step_avg:247.44ms
step:217/1775 train_time:53693ms step_avg:247.43ms
step:218/1775 train_time:53939ms step_avg:247.43ms
step:219/1775 train_time:54188ms step_avg:247.43ms
step:220/1775 train_time:54436ms step_avg:247.44ms
step:221/1775 train_time:54684ms step_avg:247.44ms
step:222/1775 train_time:54930ms step_avg:247.43ms
step:223/1775 train_time:55176ms step_avg:247.43ms
step:224/1775 train_time:55423ms step_avg:247.43ms
step:225/1775 train_time:55669ms step_avg:247.42ms
step:226/1775 train_time:55915ms step_avg:247.41ms
step:227/1775 train_time:56161ms step_avg:247.40ms
step:228/1775 train_time:56408ms step_avg:247.40ms
step:229/1775 train_time:56656ms step_avg:247.41ms
step:230/1775 train_time:56903ms step_avg:247.40ms
step:231/1775 train_time:57149ms step_avg:247.40ms
step:232/1775 train_time:57395ms step_avg:247.39ms
step:233/1775 train_time:57642ms step_avg:247.39ms
step:234/1775 train_time:57889ms step_avg:247.39ms
step:235/1775 train_time:58137ms step_avg:247.39ms
step:236/1775 train_time:58383ms step_avg:247.38ms
step:237/1775 train_time:58628ms step_avg:247.38ms
step:238/1775 train_time:58876ms step_avg:247.38ms
step:239/1775 train_time:59123ms step_avg:247.38ms
step:240/1775 train_time:59369ms step_avg:247.37ms
step:241/1775 train_time:59616ms step_avg:247.37ms
step:242/1775 train_time:59862ms step_avg:247.36ms
step:243/1775 train_time:60108ms step_avg:247.36ms
step:244/1775 train_time:60355ms step_avg:247.36ms
step:245/1775 train_time:60600ms step_avg:247.35ms
step:246/1775 train_time:60849ms step_avg:247.35ms
step:247/1775 train_time:61095ms step_avg:247.35ms
step:248/1775 train_time:61340ms step_avg:247.34ms
step:249/1775 train_time:61586ms step_avg:247.33ms
step:250/1775 train_time:61833ms step_avg:247.33ms
step:250/1775 val_loss:4.6075 train_time:61877ms step_avg:247.51ms
step:251/1775 train_time:62082ms step_avg:247.34ms
step:252/1775 train_time:62329ms step_avg:247.34ms
step:253/1775 train_time:62574ms step_avg:247.33ms
step:254/1775 train_time:62822ms step_avg:247.33ms
step:255/1775 train_time:63071ms step_avg:247.34ms
step:256/1775 train_time:63320ms step_avg:247.34ms
step:257/1775 train_time:63566ms step_avg:247.34ms
step:258/1775 train_time:63812ms step_avg:247.33ms
step:259/1775 train_time:64060ms step_avg:247.34ms
step:260/1775 train_time:64306ms step_avg:247.33ms
step:261/1775 train_time:64552ms step_avg:247.33ms
step:262/1775 train_time:64799ms step_avg:247.33ms
step:263/1775 train_time:65047ms step_avg:247.33ms
step:264/1775 train_time:65294ms step_avg:247.33ms
step:265/1775 train_time:65541ms step_avg:247.32ms
step:266/1775 train_time:65788ms step_avg:247.32ms
step:267/1775 train_time:66033ms step_avg:247.32ms
step:268/1775 train_time:66280ms step_avg:247.31ms
step:269/1775 train_time:66526ms step_avg:247.31ms
step:270/1775 train_time:66773ms step_avg:247.31ms
step:271/1775 train_time:67020ms step_avg:247.31ms
step:272/1775 train_time:67267ms step_avg:247.31ms
step:273/1775 train_time:67513ms step_avg:247.30ms
step:274/1775 train_time:67760ms step_avg:247.30ms
step:275/1775 train_time:68006ms step_avg:247.29ms
step:276/1775 train_time:68252ms step_avg:247.29ms
step:277/1775 train_time:68499ms step_avg:247.29ms
step:278/1775 train_time:68745ms step_avg:247.28ms
step:279/1775 train_time:68991ms step_avg:247.28ms
step:280/1775 train_time:69239ms step_avg:247.28ms
step:281/1775 train_time:69487ms step_avg:247.28ms
step:282/1775 train_time:69733ms step_avg:247.28ms
step:283/1775 train_time:69979ms step_avg:247.28ms
step:284/1775 train_time:70224ms step_avg:247.27ms
step:285/1775 train_time:70470ms step_avg:247.26ms
step:286/1775 train_time:70718ms step_avg:247.27ms
step:287/1775 train_time:70965ms step_avg:247.26ms
step:288/1775 train_time:71211ms step_avg:247.26ms
step:289/1775 train_time:71456ms step_avg:247.25ms
step:290/1775 train_time:71703ms step_avg:247.25ms
step:291/1775 train_time:71950ms step_avg:247.25ms
step:292/1775 train_time:72196ms step_avg:247.25ms
step:293/1775 train_time:72442ms step_avg:247.24ms
step:294/1775 train_time:72691ms step_avg:247.25ms
step:295/1775 train_time:72938ms step_avg:247.25ms
step:296/1775 train_time:73184ms step_avg:247.24ms
step:297/1775 train_time:73430ms step_avg:247.24ms
step:298/1775 train_time:73675ms step_avg:247.23ms
step:299/1775 train_time:73922ms step_avg:247.23ms
step:300/1775 train_time:74167ms step_avg:247.22ms
step:301/1775 train_time:74413ms step_avg:247.22ms
step:302/1775 train_time:74660ms step_avg:247.22ms
step:303/1775 train_time:74906ms step_avg:247.22ms
step:304/1775 train_time:75152ms step_avg:247.21ms
step:305/1775 train_time:75398ms step_avg:247.21ms
step:306/1775 train_time:75646ms step_avg:247.21ms
step:307/1775 train_time:75892ms step_avg:247.21ms
step:308/1775 train_time:76138ms step_avg:247.20ms
step:309/1775 train_time:76383ms step_avg:247.19ms
step:310/1775 train_time:76630ms step_avg:247.19ms
step:311/1775 train_time:76877ms step_avg:247.19ms
step:312/1775 train_time:77124ms step_avg:247.19ms
step:313/1775 train_time:77370ms step_avg:247.19ms
step:314/1775 train_time:77615ms step_avg:247.18ms
step:315/1775 train_time:77861ms step_avg:247.18ms
step:316/1775 train_time:78108ms step_avg:247.18ms
step:317/1775 train_time:78355ms step_avg:247.18ms
step:318/1775 train_time:78601ms step_avg:247.17ms
step:319/1775 train_time:78848ms step_avg:247.17ms
step:320/1775 train_time:79095ms step_avg:247.17ms
step:321/1775 train_time:79342ms step_avg:247.17ms
step:322/1775 train_time:79588ms step_avg:247.17ms
step:323/1775 train_time:79834ms step_avg:247.16ms
step:324/1775 train_time:80081ms step_avg:247.16ms
step:325/1775 train_time:80328ms step_avg:247.16ms
step:326/1775 train_time:80574ms step_avg:247.16ms
step:327/1775 train_time:80820ms step_avg:247.16ms
step:328/1775 train_time:81065ms step_avg:247.15ms
step:329/1775 train_time:81313ms step_avg:247.15ms
step:330/1775 train_time:81560ms step_avg:247.15ms
step:331/1775 train_time:81809ms step_avg:247.16ms
step:332/1775 train_time:82055ms step_avg:247.15ms
step:333/1775 train_time:82301ms step_avg:247.15ms
step:334/1775 train_time:82547ms step_avg:247.15ms
step:335/1775 train_time:82793ms step_avg:247.14ms
step:336/1775 train_time:83040ms step_avg:247.14ms
step:337/1775 train_time:83285ms step_avg:247.14ms
step:338/1775 train_time:83532ms step_avg:247.13ms
step:339/1775 train_time:83779ms step_avg:247.13ms
step:340/1775 train_time:84024ms step_avg:247.13ms
step:341/1775 train_time:84269ms step_avg:247.12ms
step:342/1775 train_time:84515ms step_avg:247.12ms
step:343/1775 train_time:84761ms step_avg:247.12ms
step:344/1775 train_time:85008ms step_avg:247.12ms
step:345/1775 train_time:85256ms step_avg:247.12ms
step:346/1775 train_time:85503ms step_avg:247.12ms
step:347/1775 train_time:85749ms step_avg:247.12ms
step:348/1775 train_time:85995ms step_avg:247.11ms
step:349/1775 train_time:86241ms step_avg:247.11ms
step:350/1775 train_time:86489ms step_avg:247.11ms
step:351/1775 train_time:86735ms step_avg:247.11ms
step:352/1775 train_time:86983ms step_avg:247.11ms
step:353/1775 train_time:87229ms step_avg:247.11ms
step:354/1775 train_time:87475ms step_avg:247.10ms
step:355/1775 train_time:87721ms step_avg:247.10ms
step:356/1775 train_time:87968ms step_avg:247.10ms
step:357/1775 train_time:88215ms step_avg:247.10ms
step:358/1775 train_time:88461ms step_avg:247.10ms
step:359/1775 train_time:88709ms step_avg:247.10ms
step:360/1775 train_time:88954ms step_avg:247.09ms
step:361/1775 train_time:89201ms step_avg:247.09ms
step:362/1775 train_time:89447ms step_avg:247.09ms
step:363/1775 train_time:89693ms step_avg:247.09ms
step:364/1775 train_time:89940ms step_avg:247.09ms
step:365/1775 train_time:90188ms step_avg:247.09ms
step:366/1775 train_time:90434ms step_avg:247.09ms
step:367/1775 train_time:90680ms step_avg:247.09ms
step:368/1775 train_time:90927ms step_avg:247.09ms
step:369/1775 train_time:91173ms step_avg:247.08ms
step:370/1775 train_time:91419ms step_avg:247.08ms
step:371/1775 train_time:91664ms step_avg:247.07ms
step:372/1775 train_time:91910ms step_avg:247.07ms
step:373/1775 train_time:92157ms step_avg:247.07ms
step:374/1775 train_time:92403ms step_avg:247.07ms
step:375/1775 train_time:92650ms step_avg:247.07ms
step:376/1775 train_time:92897ms step_avg:247.07ms
step:377/1775 train_time:93143ms step_avg:247.06ms
step:378/1775 train_time:93389ms step_avg:247.06ms
step:379/1775 train_time:93636ms step_avg:247.06ms
step:380/1775 train_time:93883ms step_avg:247.06ms
step:381/1775 train_time:94131ms step_avg:247.06ms
step:382/1775 train_time:94376ms step_avg:247.06ms
step:383/1775 train_time:94622ms step_avg:247.06ms
step:384/1775 train_time:94870ms step_avg:247.06ms
step:385/1775 train_time:95117ms step_avg:247.06ms
step:386/1775 train_time:95363ms step_avg:247.06ms
step:387/1775 train_time:95609ms step_avg:247.05ms
step:388/1775 train_time:95855ms step_avg:247.05ms
step:389/1775 train_time:96103ms step_avg:247.05ms
step:390/1775 train_time:96350ms step_avg:247.05ms
step:391/1775 train_time:96595ms step_avg:247.05ms
step:392/1775 train_time:96840ms step_avg:247.04ms
step:393/1775 train_time:97087ms step_avg:247.04ms
step:394/1775 train_time:97334ms step_avg:247.04ms
step:395/1775 train_time:97581ms step_avg:247.04ms
step:396/1775 train_time:97829ms step_avg:247.04ms
step:397/1775 train_time:98077ms step_avg:247.05ms
step:398/1775 train_time:98323ms step_avg:247.04ms
step:399/1775 train_time:98569ms step_avg:247.04ms
step:400/1775 train_time:98815ms step_avg:247.04ms
step:401/1775 train_time:99062ms step_avg:247.04ms
step:402/1775 train_time:99309ms step_avg:247.04ms
step:403/1775 train_time:99557ms step_avg:247.04ms
step:404/1775 train_time:99803ms step_avg:247.04ms
step:405/1775 train_time:100049ms step_avg:247.03ms
step:406/1775 train_time:100295ms step_avg:247.03ms
step:407/1775 train_time:100540ms step_avg:247.03ms
step:408/1775 train_time:100787ms step_avg:247.03ms
step:409/1775 train_time:101033ms step_avg:247.02ms
step:410/1775 train_time:101280ms step_avg:247.02ms
step:411/1775 train_time:101524ms step_avg:247.02ms
step:412/1775 train_time:101772ms step_avg:247.02ms
step:413/1775 train_time:102019ms step_avg:247.02ms
step:414/1775 train_time:102266ms step_avg:247.02ms
step:415/1775 train_time:102511ms step_avg:247.02ms
step:416/1775 train_time:102758ms step_avg:247.02ms
step:417/1775 train_time:103005ms step_avg:247.01ms
step:418/1775 train_time:103251ms step_avg:247.01ms
step:419/1775 train_time:103498ms step_avg:247.01ms
step:420/1775 train_time:103744ms step_avg:247.01ms
step:421/1775 train_time:103990ms step_avg:247.01ms
step:422/1775 train_time:104234ms step_avg:247.00ms
step:423/1775 train_time:104481ms step_avg:247.00ms
step:424/1775 train_time:104729ms step_avg:247.00ms
step:425/1775 train_time:104976ms step_avg:247.00ms
step:426/1775 train_time:105222ms step_avg:247.00ms
step:427/1775 train_time:105468ms step_avg:247.00ms
step:428/1775 train_time:105715ms step_avg:247.00ms
step:429/1775 train_time:105961ms step_avg:247.00ms
step:430/1775 train_time:106209ms step_avg:247.00ms
step:431/1775 train_time:106454ms step_avg:246.99ms
step:432/1775 train_time:106702ms step_avg:247.00ms
step:433/1775 train_time:106949ms step_avg:246.99ms
step:434/1775 train_time:107194ms step_avg:246.99ms
step:435/1775 train_time:107441ms step_avg:246.99ms
step:436/1775 train_time:107687ms step_avg:246.99ms
step:437/1775 train_time:107934ms step_avg:246.99ms
step:438/1775 train_time:108180ms step_avg:246.99ms
step:439/1775 train_time:108426ms step_avg:246.98ms
step:440/1775 train_time:108673ms step_avg:246.98ms
step:441/1775 train_time:108919ms step_avg:246.98ms
step:442/1775 train_time:109164ms step_avg:246.98ms
step:443/1775 train_time:109410ms step_avg:246.98ms
step:444/1775 train_time:109656ms step_avg:246.97ms
step:445/1775 train_time:109903ms step_avg:246.97ms
step:446/1775 train_time:110150ms step_avg:246.97ms
step:447/1775 train_time:110397ms step_avg:246.97ms
step:448/1775 train_time:110642ms step_avg:246.97ms
step:449/1775 train_time:110888ms step_avg:246.97ms
step:450/1775 train_time:111135ms step_avg:246.97ms
step:451/1775 train_time:111382ms step_avg:246.97ms
step:452/1775 train_time:111628ms step_avg:246.97ms
step:453/1775 train_time:111875ms step_avg:246.96ms
step:454/1775 train_time:112121ms step_avg:246.96ms
step:455/1775 train_time:112370ms step_avg:246.97ms
step:456/1775 train_time:112616ms step_avg:246.96ms
step:457/1775 train_time:112862ms step_avg:246.96ms
step:458/1775 train_time:113109ms step_avg:246.96ms
step:459/1775 train_time:113355ms step_avg:246.96ms
step:460/1775 train_time:113600ms step_avg:246.96ms
step:461/1775 train_time:113847ms step_avg:246.96ms
step:462/1775 train_time:114094ms step_avg:246.96ms
step:463/1775 train_time:114341ms step_avg:246.96ms
step:464/1775 train_time:114587ms step_avg:246.96ms
step:465/1775 train_time:114833ms step_avg:246.95ms
step:466/1775 train_time:115080ms step_avg:246.95ms
step:467/1775 train_time:115327ms step_avg:246.95ms
step:468/1775 train_time:115574ms step_avg:246.95ms
step:469/1775 train_time:115822ms step_avg:246.96ms
step:470/1775 train_time:116069ms step_avg:246.95ms
step:471/1775 train_time:116315ms step_avg:246.95ms
step:472/1775 train_time:116562ms step_avg:246.95ms
step:473/1775 train_time:116808ms step_avg:246.95ms
step:474/1775 train_time:117054ms step_avg:246.95ms
step:475/1775 train_time:117301ms step_avg:246.95ms
step:476/1775 train_time:117548ms step_avg:246.95ms
step:477/1775 train_time:117795ms step_avg:246.95ms
step:478/1775 train_time:118042ms step_avg:246.95ms
step:479/1775 train_time:118288ms step_avg:246.95ms
step:480/1775 train_time:118534ms step_avg:246.95ms
step:481/1775 train_time:118782ms step_avg:246.95ms
step:482/1775 train_time:119028ms step_avg:246.95ms
step:483/1775 train_time:119274ms step_avg:246.94ms
step:484/1775 train_time:119521ms step_avg:246.94ms
step:485/1775 train_time:119769ms step_avg:246.95ms
step:486/1775 train_time:120015ms step_avg:246.94ms
step:487/1775 train_time:120261ms step_avg:246.94ms
step:488/1775 train_time:120507ms step_avg:246.94ms
step:489/1775 train_time:120753ms step_avg:246.94ms
step:490/1775 train_time:120999ms step_avg:246.94ms
step:491/1775 train_time:121244ms step_avg:246.93ms
step:492/1775 train_time:121490ms step_avg:246.93ms
step:493/1775 train_time:121736ms step_avg:246.93ms
step:494/1775 train_time:121982ms step_avg:246.93ms
step:495/1775 train_time:122230ms step_avg:246.93ms
step:496/1775 train_time:122475ms step_avg:246.93ms
step:497/1775 train_time:122722ms step_avg:246.93ms
step:498/1775 train_time:122970ms step_avg:246.93ms
step:499/1775 train_time:123218ms step_avg:246.93ms
step:500/1775 train_time:123464ms step_avg:246.93ms
step:500/1775 val_loss:4.2756 train_time:123508ms step_avg:247.02ms
step:501/1775 train_time:123715ms step_avg:246.94ms
step:502/1775 train_time:123960ms step_avg:246.93ms
step:503/1775 train_time:124206ms step_avg:246.93ms
step:504/1775 train_time:124455ms step_avg:246.94ms
step:505/1775 train_time:124704ms step_avg:246.94ms
step:506/1775 train_time:124949ms step_avg:246.93ms
step:507/1775 train_time:125194ms step_avg:246.93ms
step:508/1775 train_time:125441ms step_avg:246.93ms
step:509/1775 train_time:125688ms step_avg:246.93ms
step:510/1775 train_time:125934ms step_avg:246.93ms
step:511/1775 train_time:126179ms step_avg:246.93ms
step:512/1775 train_time:126426ms step_avg:246.93ms
step:513/1775 train_time:126675ms step_avg:246.93ms
step:514/1775 train_time:126921ms step_avg:246.93ms
step:515/1775 train_time:127167ms step_avg:246.93ms
step:516/1775 train_time:127413ms step_avg:246.93ms
step:517/1775 train_time:127659ms step_avg:246.92ms
step:518/1775 train_time:127906ms step_avg:246.92ms
step:519/1775 train_time:128152ms step_avg:246.92ms
step:520/1775 train_time:128398ms step_avg:246.92ms
step:521/1775 train_time:128647ms step_avg:246.92ms
step:522/1775 train_time:128893ms step_avg:246.92ms
step:523/1775 train_time:129139ms step_avg:246.92ms
step:524/1775 train_time:129386ms step_avg:246.92ms
step:525/1775 train_time:129632ms step_avg:246.92ms
step:526/1775 train_time:129880ms step_avg:246.92ms
step:527/1775 train_time:130126ms step_avg:246.92ms
step:528/1775 train_time:130372ms step_avg:246.92ms
step:529/1775 train_time:130618ms step_avg:246.91ms
step:530/1775 train_time:130865ms step_avg:246.92ms
step:531/1775 train_time:131112ms step_avg:246.92ms
step:532/1775 train_time:131358ms step_avg:246.91ms
step:533/1775 train_time:131604ms step_avg:246.91ms
step:534/1775 train_time:131851ms step_avg:246.91ms
step:535/1775 train_time:132097ms step_avg:246.91ms
step:536/1775 train_time:132344ms step_avg:246.91ms
step:537/1775 train_time:132590ms step_avg:246.91ms
step:538/1775 train_time:132836ms step_avg:246.91ms
step:539/1775 train_time:133084ms step_avg:246.91ms
step:540/1775 train_time:133330ms step_avg:246.91ms
step:541/1775 train_time:133577ms step_avg:246.91ms
step:542/1775 train_time:133824ms step_avg:246.91ms
step:543/1775 train_time:134070ms step_avg:246.91ms
step:544/1775 train_time:134317ms step_avg:246.91ms
step:545/1775 train_time:134564ms step_avg:246.91ms
step:546/1775 train_time:134810ms step_avg:246.90ms
step:547/1775 train_time:135056ms step_avg:246.90ms
step:548/1775 train_time:135302ms step_avg:246.90ms
step:549/1775 train_time:135548ms step_avg:246.90ms
step:550/1775 train_time:135795ms step_avg:246.90ms
step:551/1775 train_time:136042ms step_avg:246.90ms
step:552/1775 train_time:136288ms step_avg:246.90ms
step:553/1775 train_time:136535ms step_avg:246.90ms
step:554/1775 train_time:136781ms step_avg:246.90ms
step:555/1775 train_time:137027ms step_avg:246.90ms
step:556/1775 train_time:137275ms step_avg:246.90ms
step:557/1775 train_time:137520ms step_avg:246.89ms
step:558/1775 train_time:137765ms step_avg:246.89ms
step:559/1775 train_time:138011ms step_avg:246.89ms
step:560/1775 train_time:138257ms step_avg:246.89ms
step:561/1775 train_time:138505ms step_avg:246.89ms
step:562/1775 train_time:138750ms step_avg:246.89ms
step:563/1775 train_time:138997ms step_avg:246.89ms
step:564/1775 train_time:139243ms step_avg:246.88ms
step:565/1775 train_time:139490ms step_avg:246.88ms
step:566/1775 train_time:139735ms step_avg:246.88ms
step:567/1775 train_time:139982ms step_avg:246.88ms
step:568/1775 train_time:140228ms step_avg:246.88ms
step:569/1775 train_time:140474ms step_avg:246.88ms
step:570/1775 train_time:140720ms step_avg:246.88ms
step:571/1775 train_time:140966ms step_avg:246.88ms
step:572/1775 train_time:141213ms step_avg:246.88ms
step:573/1775 train_time:141459ms step_avg:246.87ms
step:574/1775 train_time:141705ms step_avg:246.87ms
step:575/1775 train_time:141952ms step_avg:246.87ms
step:576/1775 train_time:142199ms step_avg:246.87ms
step:577/1775 train_time:142445ms step_avg:246.87ms
step:578/1775 train_time:142691ms step_avg:246.87ms
step:579/1775 train_time:142937ms step_avg:246.87ms
step:580/1775 train_time:143371ms step_avg:247.19ms
step:581/1775 train_time:143827ms step_avg:247.55ms
step:582/1775 train_time:144284ms step_avg:247.91ms
step:583/1775 train_time:144744ms step_avg:248.27ms
step:584/1775 train_time:145201ms step_avg:248.63ms
step:585/1775 train_time:145659ms step_avg:248.99ms
step:586/1775 train_time:146114ms step_avg:249.34ms
step:587/1775 train_time:146570ms step_avg:249.69ms
step:588/1775 train_time:147027ms step_avg:250.05ms
step:589/1775 train_time:147484ms step_avg:250.40ms
step:590/1775 train_time:147942ms step_avg:250.75ms
step:591/1775 train_time:148399ms step_avg:251.10ms
step:592/1775 train_time:148855ms step_avg:251.44ms
step:593/1775 train_time:149311ms step_avg:251.79ms
step:594/1775 train_time:149769ms step_avg:252.14ms
step:595/1775 train_time:150226ms step_avg:252.48ms
step:596/1775 train_time:150685ms step_avg:252.83ms
step:597/1775 train_time:151144ms step_avg:253.17ms
step:598/1775 train_time:151602ms step_avg:253.52ms
step:599/1775 train_time:152059ms step_avg:253.85ms
step:600/1775 train_time:152517ms step_avg:254.20ms
step:601/1775 train_time:152975ms step_avg:254.53ms
step:602/1775 train_time:153431ms step_avg:254.87ms
step:603/1775 train_time:153888ms step_avg:255.20ms
step:604/1775 train_time:154347ms step_avg:255.54ms
step:605/1775 train_time:154803ms step_avg:255.87ms
step:606/1775 train_time:155260ms step_avg:256.21ms
step:607/1775 train_time:155717ms step_avg:256.54ms
step:608/1775 train_time:156173ms step_avg:256.86ms
step:609/1775 train_time:156627ms step_avg:257.19ms
step:610/1775 train_time:157086ms step_avg:257.52ms
step:611/1775 train_time:157545ms step_avg:257.85ms
step:612/1775 train_time:158004ms step_avg:258.18ms
step:613/1775 train_time:158463ms step_avg:258.50ms
step:614/1775 train_time:158923ms step_avg:258.83ms
step:615/1775 train_time:159380ms step_avg:259.16ms
step:616/1775 train_time:159839ms step_avg:259.48ms
step:617/1775 train_time:160295ms step_avg:259.80ms
step:618/1775 train_time:160752ms step_avg:260.12ms
step:619/1775 train_time:161208ms step_avg:260.43ms
step:620/1775 train_time:161667ms step_avg:260.75ms
step:621/1775 train_time:162124ms step_avg:261.07ms
step:622/1775 train_time:162583ms step_avg:261.39ms
step:623/1775 train_time:163039ms step_avg:261.70ms
step:624/1775 train_time:163497ms step_avg:262.01ms
step:625/1775 train_time:163953ms step_avg:262.32ms
step:626/1775 train_time:164411ms step_avg:262.64ms
step:627/1775 train_time:164867ms step_avg:262.95ms
step:628/1775 train_time:165325ms step_avg:263.26ms
step:629/1775 train_time:165783ms step_avg:263.57ms
step:630/1775 train_time:166243ms step_avg:263.88ms
step:631/1775 train_time:166700ms step_avg:264.18ms
step:632/1775 train_time:167155ms step_avg:264.49ms
step:633/1775 train_time:167611ms step_avg:264.79ms
step:634/1775 train_time:168069ms step_avg:265.09ms
step:635/1775 train_time:168525ms step_avg:265.39ms
step:636/1775 train_time:168985ms step_avg:265.70ms
step:637/1775 train_time:169442ms step_avg:266.00ms
step:638/1775 train_time:169901ms step_avg:266.30ms
step:639/1775 train_time:170358ms step_avg:266.60ms
step:640/1775 train_time:170815ms step_avg:266.90ms
step:641/1775 train_time:171270ms step_avg:267.19ms
step:642/1775 train_time:171728ms step_avg:267.49ms
step:643/1775 train_time:172185ms step_avg:267.78ms
step:644/1775 train_time:172642ms step_avg:268.08ms
step:645/1775 train_time:173099ms step_avg:268.37ms
step:646/1775 train_time:173556ms step_avg:268.66ms
step:647/1775 train_time:174011ms step_avg:268.95ms
step:648/1775 train_time:174469ms step_avg:269.24ms
step:649/1775 train_time:174926ms step_avg:269.53ms
step:650/1775 train_time:175384ms step_avg:269.82ms
step:651/1775 train_time:175842ms step_avg:270.11ms
step:652/1775 train_time:176300ms step_avg:270.40ms
step:653/1775 train_time:176756ms step_avg:270.68ms
step:654/1775 train_time:177214ms step_avg:270.97ms
step:655/1775 train_time:177670ms step_avg:271.25ms
step:656/1775 train_time:178127ms step_avg:271.53ms
step:657/1775 train_time:178583ms step_avg:271.82ms
step:658/1775 train_time:179042ms step_avg:272.10ms
step:659/1775 train_time:179500ms step_avg:272.38ms
step:660/1775 train_time:179958ms step_avg:272.66ms
step:661/1775 train_time:180413ms step_avg:272.94ms
step:662/1775 train_time:180868ms step_avg:273.21ms
step:663/1775 train_time:181324ms step_avg:273.49ms
step:664/1775 train_time:181783ms step_avg:273.77ms
step:665/1775 train_time:182239ms step_avg:274.04ms
step:666/1775 train_time:182698ms step_avg:274.32ms
step:667/1775 train_time:183154ms step_avg:274.59ms
step:668/1775 train_time:183612ms step_avg:274.87ms
step:669/1775 train_time:184067ms step_avg:275.14ms
step:670/1775 train_time:184526ms step_avg:275.41ms
step:671/1775 train_time:184983ms step_avg:275.68ms
step:672/1775 train_time:185441ms step_avg:275.95ms
step:673/1775 train_time:185897ms step_avg:276.22ms
step:674/1775 train_time:186353ms step_avg:276.49ms
step:675/1775 train_time:186809ms step_avg:276.75ms
step:676/1775 train_time:187265ms step_avg:277.02ms
step:677/1775 train_time:187722ms step_avg:277.28ms
step:678/1775 train_time:188180ms step_avg:277.55ms
step:679/1775 train_time:188636ms step_avg:277.81ms
step:680/1775 train_time:189092ms step_avg:278.08ms
step:681/1775 train_time:189550ms step_avg:278.34ms
step:682/1775 train_time:190006ms step_avg:278.60ms
step:683/1775 train_time:190464ms step_avg:278.86ms
step:684/1775 train_time:190921ms step_avg:279.12ms
step:685/1775 train_time:191378ms step_avg:279.38ms
step:686/1775 train_time:191835ms step_avg:279.64ms
step:687/1775 train_time:192289ms step_avg:279.90ms
step:688/1775 train_time:192748ms step_avg:280.16ms
step:689/1775 train_time:193205ms step_avg:280.41ms
step:690/1775 train_time:193664ms step_avg:280.67ms
step:691/1775 train_time:194121ms step_avg:280.93ms
step:692/1775 train_time:194581ms step_avg:281.19ms
step:693/1775 train_time:195037ms step_avg:281.44ms
step:694/1775 train_time:195494ms step_avg:281.69ms
step:695/1775 train_time:195949ms step_avg:281.94ms
step:696/1775 train_time:196406ms step_avg:282.19ms
step:697/1775 train_time:196864ms step_avg:282.44ms
step:698/1775 train_time:197322ms step_avg:282.70ms
step:699/1775 train_time:197779ms step_avg:282.95ms
step:700/1775 train_time:198236ms step_avg:283.19ms
step:701/1775 train_time:198691ms step_avg:283.44ms
step:702/1775 train_time:199147ms step_avg:283.68ms
step:703/1775 train_time:199603ms step_avg:283.93ms
step:704/1775 train_time:200061ms step_avg:284.18ms
step:705/1775 train_time:200518ms step_avg:284.42ms
step:706/1775 train_time:200974ms step_avg:284.67ms
step:707/1775 train_time:201430ms step_avg:284.91ms
step:708/1775 train_time:201888ms step_avg:285.15ms
step:709/1775 train_time:202345ms step_avg:285.39ms
step:710/1775 train_time:202804ms step_avg:285.64ms
step:711/1775 train_time:203261ms step_avg:285.88ms
step:712/1775 train_time:203719ms step_avg:286.12ms
step:713/1775 train_time:204176ms step_avg:286.36ms
step:714/1775 train_time:204633ms step_avg:286.60ms
step:715/1775 train_time:205087ms step_avg:286.83ms
step:716/1775 train_time:205544ms step_avg:287.07ms
step:717/1775 train_time:206000ms step_avg:287.31ms
step:718/1775 train_time:206459ms step_avg:287.55ms
step:719/1775 train_time:206916ms step_avg:287.78ms
step:720/1775 train_time:207372ms step_avg:288.02ms
step:721/1775 train_time:207826ms step_avg:288.25ms
step:722/1775 train_time:208286ms step_avg:288.48ms
step:723/1775 train_time:208742ms step_avg:288.72ms
step:724/1775 train_time:209202ms step_avg:288.95ms
step:725/1775 train_time:209657ms step_avg:289.18ms
step:726/1775 train_time:210113ms step_avg:289.41ms
step:727/1775 train_time:210569ms step_avg:289.64ms
step:728/1775 train_time:211027ms step_avg:289.87ms
step:729/1775 train_time:211483ms step_avg:290.10ms
step:730/1775 train_time:211942ms step_avg:290.33ms
step:731/1775 train_time:212399ms step_avg:290.56ms
step:732/1775 train_time:212855ms step_avg:290.79ms
step:733/1775 train_time:213309ms step_avg:291.01ms
step:734/1775 train_time:213765ms step_avg:291.23ms
step:735/1775 train_time:214222ms step_avg:291.46ms
step:736/1775 train_time:214679ms step_avg:291.68ms
step:737/1775 train_time:215136ms step_avg:291.91ms
step:738/1775 train_time:215591ms step_avg:292.13ms
step:739/1775 train_time:216047ms step_avg:292.35ms
step:740/1775 train_time:216506ms step_avg:292.58ms
step:741/1775 train_time:216961ms step_avg:292.80ms
step:742/1775 train_time:217420ms step_avg:293.02ms
step:743/1775 train_time:217875ms step_avg:293.24ms
step:744/1775 train_time:218330ms step_avg:293.45ms
step:745/1775 train_time:218786ms step_avg:293.67ms
step:746/1775 train_time:219244ms step_avg:293.89ms
step:747/1775 train_time:219701ms step_avg:294.11ms
step:748/1775 train_time:220156ms step_avg:294.33ms
step:749/1775 train_time:220614ms step_avg:294.54ms
step:750/1775 train_time:221069ms step_avg:294.76ms
step:750/1775 val_loss:3.9917 train_time:221142ms step_avg:294.86ms
step:751/1775 train_time:221524ms step_avg:294.97ms
step:752/1775 train_time:221981ms step_avg:295.19ms
step:753/1775 train_time:222435ms step_avg:295.40ms
step:754/1775 train_time:222891ms step_avg:295.61ms
step:755/1775 train_time:223346ms step_avg:295.82ms
step:756/1775 train_time:223800ms step_avg:296.03ms
step:757/1775 train_time:224254ms step_avg:296.24ms
step:758/1775 train_time:224708ms step_avg:296.45ms
step:759/1775 train_time:225162ms step_avg:296.66ms
step:760/1775 train_time:225618ms step_avg:296.87ms
step:761/1775 train_time:226072ms step_avg:297.07ms
step:762/1775 train_time:226527ms step_avg:297.28ms
step:763/1775 train_time:226982ms step_avg:297.49ms
step:764/1775 train_time:227439ms step_avg:297.69ms
step:765/1775 train_time:227895ms step_avg:297.90ms
step:766/1775 train_time:228348ms step_avg:298.10ms
step:767/1775 train_time:228804ms step_avg:298.31ms
step:768/1775 train_time:229260ms step_avg:298.52ms
step:769/1775 train_time:229714ms step_avg:298.72ms
step:770/1775 train_time:230168ms step_avg:298.92ms
step:771/1775 train_time:230620ms step_avg:299.12ms
step:772/1775 train_time:231076ms step_avg:299.32ms
step:773/1775 train_time:231528ms step_avg:299.52ms
step:774/1775 train_time:231983ms step_avg:299.72ms
step:775/1775 train_time:232439ms step_avg:299.92ms
step:776/1775 train_time:232895ms step_avg:300.12ms
step:777/1775 train_time:233348ms step_avg:300.32ms
step:778/1775 train_time:233803ms step_avg:300.52ms
step:779/1775 train_time:234258ms step_avg:300.72ms
step:780/1775 train_time:234714ms step_avg:300.91ms
step:781/1775 train_time:235167ms step_avg:301.11ms
step:782/1775 train_time:235624ms step_avg:301.31ms
step:783/1775 train_time:236079ms step_avg:301.51ms
step:784/1775 train_time:236534ms step_avg:301.70ms
step:785/1775 train_time:236988ms step_avg:301.90ms
step:786/1775 train_time:237441ms step_avg:302.09ms
step:787/1775 train_time:237895ms step_avg:302.28ms
step:788/1775 train_time:238350ms step_avg:302.47ms
step:789/1775 train_time:238803ms step_avg:302.67ms
step:790/1775 train_time:239259ms step_avg:302.86ms
step:791/1775 train_time:239712ms step_avg:303.05ms
step:792/1775 train_time:240167ms step_avg:303.24ms
step:793/1775 train_time:240621ms step_avg:303.43ms
step:794/1775 train_time:241076ms step_avg:303.62ms
step:795/1775 train_time:241529ms step_avg:303.81ms
step:796/1775 train_time:241983ms step_avg:304.00ms
step:797/1775 train_time:242439ms step_avg:304.19ms
step:798/1775 train_time:242896ms step_avg:304.38ms
step:799/1775 train_time:243349ms step_avg:304.57ms
step:800/1775 train_time:243803ms step_avg:304.75ms
step:801/1775 train_time:244260ms step_avg:304.94ms
step:802/1775 train_time:244716ms step_avg:305.13ms
step:803/1775 train_time:245169ms step_avg:305.32ms
step:804/1775 train_time:245625ms step_avg:305.50ms
step:805/1775 train_time:246080ms step_avg:305.69ms
step:806/1775 train_time:246534ms step_avg:305.87ms
step:807/1775 train_time:246988ms step_avg:306.06ms
step:808/1775 train_time:247442ms step_avg:306.24ms
step:809/1775 train_time:247898ms step_avg:306.43ms
step:810/1775 train_time:248354ms step_avg:306.61ms
step:811/1775 train_time:248807ms step_avg:306.79ms
step:812/1775 train_time:249263ms step_avg:306.97ms
step:813/1775 train_time:249717ms step_avg:307.16ms
step:814/1775 train_time:250172ms step_avg:307.34ms
step:815/1775 train_time:250625ms step_avg:307.52ms
step:816/1775 train_time:251082ms step_avg:307.70ms
step:817/1775 train_time:251537ms step_avg:307.88ms
step:818/1775 train_time:251992ms step_avg:308.06ms
step:819/1775 train_time:252445ms step_avg:308.24ms
step:820/1775 train_time:252901ms step_avg:308.42ms
step:821/1775 train_time:253357ms step_avg:308.60ms
step:822/1775 train_time:253812ms step_avg:308.77ms
step:823/1775 train_time:254266ms step_avg:308.95ms
step:824/1775 train_time:254722ms step_avg:309.13ms
step:825/1775 train_time:255176ms step_avg:309.30ms
step:826/1775 train_time:255629ms step_avg:309.48ms
step:827/1775 train_time:256081ms step_avg:309.65ms
step:828/1775 train_time:256538ms step_avg:309.83ms
step:829/1775 train_time:256990ms step_avg:310.00ms
step:830/1775 train_time:257447ms step_avg:310.18ms
step:831/1775 train_time:257901ms step_avg:310.35ms
step:832/1775 train_time:258356ms step_avg:310.52ms
step:833/1775 train_time:258810ms step_avg:310.70ms
step:834/1775 train_time:259264ms step_avg:310.87ms
step:835/1775 train_time:259718ms step_avg:311.04ms
step:836/1775 train_time:260172ms step_avg:311.21ms
step:837/1775 train_time:260626ms step_avg:311.38ms
step:838/1775 train_time:261080ms step_avg:311.55ms
step:839/1775 train_time:261535ms step_avg:311.72ms
step:840/1775 train_time:261989ms step_avg:311.89ms
step:841/1775 train_time:262442ms step_avg:312.06ms
step:842/1775 train_time:262898ms step_avg:312.23ms
step:843/1775 train_time:263351ms step_avg:312.40ms
step:844/1775 train_time:263805ms step_avg:312.57ms
step:845/1775 train_time:264258ms step_avg:312.73ms
step:846/1775 train_time:264714ms step_avg:312.90ms
step:847/1775 train_time:265166ms step_avg:313.07ms
step:848/1775 train_time:265623ms step_avg:313.23ms
step:849/1775 train_time:266078ms step_avg:313.40ms
step:850/1775 train_time:266533ms step_avg:313.57ms
step:851/1775 train_time:266986ms step_avg:313.73ms
step:852/1775 train_time:267441ms step_avg:313.90ms
step:853/1775 train_time:267895ms step_avg:314.06ms
step:854/1775 train_time:268350ms step_avg:314.23ms
step:855/1775 train_time:268803ms step_avg:314.39ms
step:856/1775 train_time:269258ms step_avg:314.55ms
step:857/1775 train_time:269715ms step_avg:314.72ms
step:858/1775 train_time:270167ms step_avg:314.88ms
step:859/1775 train_time:270620ms step_avg:315.04ms
step:860/1775 train_time:271077ms step_avg:315.21ms
step:861/1775 train_time:271531ms step_avg:315.37ms
step:862/1775 train_time:271985ms step_avg:315.53ms
step:863/1775 train_time:272440ms step_avg:315.69ms
step:864/1775 train_time:272894ms step_avg:315.85ms
step:865/1775 train_time:273346ms step_avg:316.01ms
step:866/1775 train_time:273802ms step_avg:316.17ms
step:867/1775 train_time:274257ms step_avg:316.33ms
step:868/1775 train_time:274711ms step_avg:316.49ms
step:869/1775 train_time:275164ms step_avg:316.64ms
step:870/1775 train_time:275618ms step_avg:316.80ms
step:871/1775 train_time:276072ms step_avg:316.96ms
step:872/1775 train_time:276525ms step_avg:317.12ms
step:873/1775 train_time:276979ms step_avg:317.27ms
step:874/1775 train_time:277437ms step_avg:317.43ms
step:875/1775 train_time:277889ms step_avg:317.59ms
step:876/1775 train_time:278343ms step_avg:317.74ms
step:877/1775 train_time:278797ms step_avg:317.90ms
step:878/1775 train_time:279251ms step_avg:318.05ms
step:879/1775 train_time:279704ms step_avg:318.21ms
step:880/1775 train_time:280161ms step_avg:318.36ms
step:881/1775 train_time:280613ms step_avg:318.52ms
step:882/1775 train_time:281066ms step_avg:318.67ms
step:883/1775 train_time:281522ms step_avg:318.82ms
step:884/1775 train_time:281977ms step_avg:318.98ms
step:885/1775 train_time:282432ms step_avg:319.13ms
step:886/1775 train_time:282887ms step_avg:319.29ms
step:887/1775 train_time:283341ms step_avg:319.44ms
step:888/1775 train_time:283796ms step_avg:319.59ms
step:889/1775 train_time:284250ms step_avg:319.74ms
step:890/1775 train_time:284703ms step_avg:319.89ms
step:891/1775 train_time:285158ms step_avg:320.04ms
step:892/1775 train_time:285614ms step_avg:320.19ms
step:893/1775 train_time:286067ms step_avg:320.34ms
step:894/1775 train_time:286522ms step_avg:320.49ms
step:895/1775 train_time:286978ms step_avg:320.65ms
step:896/1775 train_time:287432ms step_avg:320.79ms
step:897/1775 train_time:287885ms step_avg:320.94ms
step:898/1775 train_time:288342ms step_avg:321.09ms
step:899/1775 train_time:288796ms step_avg:321.24ms
step:900/1775 train_time:289250ms step_avg:321.39ms
step:901/1775 train_time:289704ms step_avg:321.54ms
step:902/1775 train_time:290160ms step_avg:321.68ms
step:903/1775 train_time:290613ms step_avg:321.83ms
step:904/1775 train_time:291067ms step_avg:321.98ms
step:905/1775 train_time:291521ms step_avg:322.12ms
step:906/1775 train_time:291977ms step_avg:322.27ms
step:907/1775 train_time:292430ms step_avg:322.41ms
step:908/1775 train_time:292883ms step_avg:322.56ms
step:909/1775 train_time:293340ms step_avg:322.71ms
step:910/1775 train_time:293793ms step_avg:322.85ms
step:911/1775 train_time:294246ms step_avg:322.99ms
step:912/1775 train_time:294701ms step_avg:323.14ms
step:913/1775 train_time:295156ms step_avg:323.28ms
step:914/1775 train_time:295609ms step_avg:323.42ms
step:915/1775 train_time:296062ms step_avg:323.57ms
step:916/1775 train_time:296518ms step_avg:323.71ms
step:917/1775 train_time:296971ms step_avg:323.85ms
step:918/1775 train_time:297425ms step_avg:323.99ms
step:919/1775 train_time:297878ms step_avg:324.13ms
step:920/1775 train_time:298336ms step_avg:324.28ms
step:921/1775 train_time:298789ms step_avg:324.42ms
step:922/1775 train_time:299244ms step_avg:324.56ms
step:923/1775 train_time:299698ms step_avg:324.70ms
step:924/1775 train_time:300153ms step_avg:324.84ms
step:925/1775 train_time:300606ms step_avg:324.98ms
step:926/1775 train_time:301062ms step_avg:325.12ms
step:927/1775 train_time:301515ms step_avg:325.26ms
step:928/1775 train_time:301970ms step_avg:325.40ms
step:929/1775 train_time:302425ms step_avg:325.54ms
step:930/1775 train_time:302880ms step_avg:325.68ms
step:931/1775 train_time:303334ms step_avg:325.81ms
step:932/1775 train_time:303787ms step_avg:325.95ms
step:933/1775 train_time:304242ms step_avg:326.09ms
step:934/1775 train_time:304699ms step_avg:326.23ms
step:935/1775 train_time:305153ms step_avg:326.37ms
step:936/1775 train_time:305607ms step_avg:326.50ms
step:937/1775 train_time:306060ms step_avg:326.64ms
step:938/1775 train_time:306513ms step_avg:326.77ms
step:939/1775 train_time:306967ms step_avg:326.91ms
step:940/1775 train_time:307424ms step_avg:327.05ms
step:941/1775 train_time:307878ms step_avg:327.18ms
step:942/1775 train_time:308334ms step_avg:327.32ms
step:943/1775 train_time:308786ms step_avg:327.45ms
step:944/1775 train_time:309242ms step_avg:327.59ms
step:945/1775 train_time:309697ms step_avg:327.72ms
step:946/1775 train_time:310151ms step_avg:327.85ms
step:947/1775 train_time:310605ms step_avg:327.99ms
step:948/1775 train_time:311061ms step_avg:328.12ms
step:949/1775 train_time:311515ms step_avg:328.26ms
step:950/1775 train_time:311970ms step_avg:328.39ms
step:951/1775 train_time:312423ms step_avg:328.52ms
step:952/1775 train_time:312878ms step_avg:328.65ms
step:953/1775 train_time:313331ms step_avg:328.78ms
step:954/1775 train_time:313785ms step_avg:328.91ms
step:955/1775 train_time:314239ms step_avg:329.05ms
step:956/1775 train_time:314694ms step_avg:329.18ms
step:957/1775 train_time:315148ms step_avg:329.31ms
step:958/1775 train_time:315601ms step_avg:329.44ms
step:959/1775 train_time:316056ms step_avg:329.57ms
step:960/1775 train_time:316510ms step_avg:329.70ms
step:961/1775 train_time:316963ms step_avg:329.83ms
step:962/1775 train_time:317419ms step_avg:329.96ms
step:963/1775 train_time:317873ms step_avg:330.09ms
step:964/1775 train_time:318326ms step_avg:330.21ms
step:965/1775 train_time:318780ms step_avg:330.34ms
step:966/1775 train_time:319237ms step_avg:330.47ms
step:967/1775 train_time:319689ms step_avg:330.60ms
step:968/1775 train_time:320144ms step_avg:330.73ms
step:969/1775 train_time:320597ms step_avg:330.85ms
step:970/1775 train_time:321051ms step_avg:330.98ms
step:971/1775 train_time:321505ms step_avg:331.11ms
step:972/1775 train_time:321959ms step_avg:331.23ms
step:973/1775 train_time:322414ms step_avg:331.36ms
step:974/1775 train_time:322867ms step_avg:331.49ms
step:975/1775 train_time:323323ms step_avg:331.61ms
step:976/1775 train_time:323779ms step_avg:331.74ms
step:977/1775 train_time:324232ms step_avg:331.87ms
step:978/1775 train_time:324685ms step_avg:331.99ms
step:979/1775 train_time:325139ms step_avg:332.11ms
step:980/1775 train_time:325593ms step_avg:332.24ms
step:981/1775 train_time:326046ms step_avg:332.36ms
step:982/1775 train_time:326501ms step_avg:332.49ms
step:983/1775 train_time:326954ms step_avg:332.61ms
step:984/1775 train_time:327408ms step_avg:332.73ms
step:985/1775 train_time:327862ms step_avg:332.85ms
step:986/1775 train_time:328318ms step_avg:332.98ms
step:987/1775 train_time:328771ms step_avg:333.10ms
step:988/1775 train_time:329224ms step_avg:333.22ms
step:989/1775 train_time:329679ms step_avg:333.35ms
step:990/1775 train_time:330135ms step_avg:333.47ms
step:991/1775 train_time:330588ms step_avg:333.59ms
step:992/1775 train_time:331042ms step_avg:333.71ms
step:993/1775 train_time:331498ms step_avg:333.83ms
step:994/1775 train_time:331952ms step_avg:333.96ms
step:995/1775 train_time:332406ms step_avg:334.08ms
step:996/1775 train_time:332861ms step_avg:334.20ms
step:997/1775 train_time:333315ms step_avg:334.32ms
step:998/1775 train_time:333768ms step_avg:334.44ms
step:999/1775 train_time:334224ms step_avg:334.56ms
step:1000/1775 train_time:334679ms step_avg:334.68ms
step:1000/1775 val_loss:3.7322 train_time:334752ms step_avg:334.75ms
step:1001/1775 train_time:335133ms step_avg:334.80ms
step:1002/1775 train_time:335589ms step_avg:334.92ms
step:1003/1775 train_time:336042ms step_avg:335.04ms
step:1004/1775 train_time:336497ms step_avg:335.16ms
step:1005/1775 train_time:336950ms step_avg:335.27ms
step:1006/1775 train_time:337405ms step_avg:335.39ms
step:1007/1775 train_time:337860ms step_avg:335.51ms
step:1008/1775 train_time:338313ms step_avg:335.63ms
step:1009/1775 train_time:338767ms step_avg:335.75ms
step:1010/1775 train_time:339223ms step_avg:335.86ms
step:1011/1775 train_time:339674ms step_avg:335.98ms
step:1012/1775 train_time:340131ms step_avg:336.10ms
step:1013/1775 train_time:340585ms step_avg:336.21ms
step:1014/1775 train_time:341039ms step_avg:336.33ms
step:1015/1775 train_time:341494ms step_avg:336.45ms
step:1016/1775 train_time:341948ms step_avg:336.56ms
step:1017/1775 train_time:342404ms step_avg:336.68ms
step:1018/1775 train_time:342857ms step_avg:336.80ms
step:1019/1775 train_time:343312ms step_avg:336.91ms
step:1020/1775 train_time:343768ms step_avg:337.03ms
step:1021/1775 train_time:344220ms step_avg:337.14ms
step:1022/1775 train_time:344673ms step_avg:337.25ms
step:1023/1775 train_time:345128ms step_avg:337.37ms
step:1024/1775 train_time:345582ms step_avg:337.48ms
step:1025/1775 train_time:346034ms step_avg:337.59ms
step:1026/1775 train_time:346489ms step_avg:337.71ms
step:1027/1775 train_time:346942ms step_avg:337.82ms
step:1028/1775 train_time:347396ms step_avg:337.93ms
step:1029/1775 train_time:347849ms step_avg:338.05ms
step:1030/1775 train_time:348304ms step_avg:338.16ms
step:1031/1775 train_time:348756ms step_avg:338.27ms
step:1032/1775 train_time:349210ms step_avg:338.38ms
step:1033/1775 train_time:349665ms step_avg:338.49ms
step:1034/1775 train_time:350119ms step_avg:338.61ms
step:1035/1775 train_time:350574ms step_avg:338.72ms
step:1036/1775 train_time:351029ms step_avg:338.83ms
step:1037/1775 train_time:351483ms step_avg:338.94ms
step:1038/1775 train_time:351937ms step_avg:339.05ms
step:1039/1775 train_time:352391ms step_avg:339.16ms
step:1040/1775 train_time:352848ms step_avg:339.28ms
step:1041/1775 train_time:353301ms step_avg:339.39ms
step:1042/1775 train_time:353756ms step_avg:339.50ms
step:1043/1775 train_time:354209ms step_avg:339.61ms
step:1044/1775 train_time:354664ms step_avg:339.72ms
step:1045/1775 train_time:355118ms step_avg:339.83ms
step:1046/1775 train_time:355571ms step_avg:339.93ms
step:1047/1775 train_time:356026ms step_avg:340.04ms
step:1048/1775 train_time:356480ms step_avg:340.15ms
step:1049/1775 train_time:356932ms step_avg:340.26ms
step:1050/1775 train_time:357388ms step_avg:340.37ms
step:1051/1775 train_time:357842ms step_avg:340.48ms
step:1052/1775 train_time:358295ms step_avg:340.58ms
step:1053/1775 train_time:358749ms step_avg:340.69ms
step:1054/1775 train_time:359204ms step_avg:340.80ms
step:1055/1775 train_time:359657ms step_avg:340.91ms
step:1056/1775 train_time:360110ms step_avg:341.01ms
step:1057/1775 train_time:360565ms step_avg:341.12ms
step:1058/1775 train_time:361019ms step_avg:341.23ms
step:1059/1775 train_time:361472ms step_avg:341.33ms
step:1060/1775 train_time:361927ms step_avg:341.44ms
step:1061/1775 train_time:362379ms step_avg:341.54ms
step:1062/1775 train_time:362833ms step_avg:341.65ms
step:1063/1775 train_time:363287ms step_avg:341.76ms
step:1064/1775 train_time:363742ms step_avg:341.86ms
step:1065/1775 train_time:364194ms step_avg:341.97ms
step:1066/1775 train_time:364650ms step_avg:342.07ms
step:1067/1775 train_time:365105ms step_avg:342.18ms
step:1068/1775 train_time:365562ms step_avg:342.29ms
step:1069/1775 train_time:366015ms step_avg:342.39ms
step:1070/1775 train_time:366472ms step_avg:342.50ms
step:1071/1775 train_time:366926ms step_avg:342.60ms
step:1072/1775 train_time:367382ms step_avg:342.71ms
step:1073/1775 train_time:367833ms step_avg:342.81ms
step:1074/1775 train_time:368289ms step_avg:342.91ms
step:1075/1775 train_time:368744ms step_avg:343.02ms
step:1076/1775 train_time:369199ms step_avg:343.12ms
step:1077/1775 train_time:369650ms step_avg:343.22ms
step:1078/1775 train_time:370106ms step_avg:343.33ms
step:1079/1775 train_time:370558ms step_avg:343.43ms
step:1080/1775 train_time:371010ms step_avg:343.53ms
step:1081/1775 train_time:371467ms step_avg:343.63ms
step:1082/1775 train_time:371921ms step_avg:343.73ms
step:1083/1775 train_time:372373ms step_avg:343.84ms
step:1084/1775 train_time:372830ms step_avg:343.94ms
step:1085/1775 train_time:373285ms step_avg:344.04ms
step:1086/1775 train_time:373740ms step_avg:344.14ms
step:1087/1775 train_time:374194ms step_avg:344.24ms
step:1088/1775 train_time:374650ms step_avg:344.35ms
step:1089/1775 train_time:375104ms step_avg:344.45ms
step:1090/1775 train_time:375558ms step_avg:344.55ms
step:1091/1775 train_time:376011ms step_avg:344.65ms
step:1092/1775 train_time:376466ms step_avg:344.75ms
step:1093/1775 train_time:376919ms step_avg:344.85ms
step:1094/1775 train_time:377373ms step_avg:344.95ms
step:1095/1775 train_time:377827ms step_avg:345.05ms
step:1096/1775 train_time:378282ms step_avg:345.15ms
step:1097/1775 train_time:378735ms step_avg:345.25ms
step:1098/1775 train_time:379190ms step_avg:345.35ms
step:1099/1775 train_time:379645ms step_avg:345.45ms
step:1100/1775 train_time:380099ms step_avg:345.54ms
step:1101/1775 train_time:380552ms step_avg:345.64ms
step:1102/1775 train_time:381008ms step_avg:345.74ms
step:1103/1775 train_time:381462ms step_avg:345.84ms
step:1104/1775 train_time:381916ms step_avg:345.94ms
step:1105/1775 train_time:382370ms step_avg:346.04ms
step:1106/1775 train_time:382826ms step_avg:346.14ms
step:1107/1775 train_time:383280ms step_avg:346.23ms
step:1108/1775 train_time:383734ms step_avg:346.33ms
step:1109/1775 train_time:384188ms step_avg:346.43ms
step:1110/1775 train_time:384644ms step_avg:346.53ms
step:1111/1775 train_time:385096ms step_avg:346.62ms
step:1112/1775 train_time:385550ms step_avg:346.72ms
step:1113/1775 train_time:386004ms step_avg:346.81ms
step:1114/1775 train_time:386458ms step_avg:346.91ms
step:1115/1775 train_time:386912ms step_avg:347.01ms
step:1116/1775 train_time:387367ms step_avg:347.10ms
step:1117/1775 train_time:387821ms step_avg:347.20ms
step:1118/1775 train_time:388273ms step_avg:347.29ms
step:1119/1775 train_time:388727ms step_avg:347.39ms
step:1120/1775 train_time:389182ms step_avg:347.48ms
step:1121/1775 train_time:389635ms step_avg:347.58ms
step:1122/1775 train_time:390089ms step_avg:347.67ms
step:1123/1775 train_time:390542ms step_avg:347.77ms
step:1124/1775 train_time:390996ms step_avg:347.86ms
step:1125/1775 train_time:391450ms step_avg:347.96ms
step:1126/1775 train_time:391905ms step_avg:348.05ms
step:1127/1775 train_time:392358ms step_avg:348.14ms
step:1128/1775 train_time:392813ms step_avg:348.24ms
step:1129/1775 train_time:393267ms step_avg:348.33ms
step:1130/1775 train_time:393722ms step_avg:348.43ms
step:1131/1775 train_time:394174ms step_avg:348.52ms
step:1132/1775 train_time:394629ms step_avg:348.61ms
step:1133/1775 train_time:395084ms step_avg:348.71ms
step:1134/1775 train_time:395536ms step_avg:348.80ms
step:1135/1775 train_time:395989ms step_avg:348.89ms
step:1136/1775 train_time:396444ms step_avg:348.98ms
step:1137/1775 train_time:396897ms step_avg:349.07ms
step:1138/1775 train_time:397350ms step_avg:349.17ms
step:1139/1775 train_time:397805ms step_avg:349.26ms
step:1140/1775 train_time:398259ms step_avg:349.35ms
step:1141/1775 train_time:398712ms step_avg:349.44ms
step:1142/1775 train_time:399167ms step_avg:349.53ms
step:1143/1775 train_time:399620ms step_avg:349.62ms
step:1144/1775 train_time:400075ms step_avg:349.72ms
step:1145/1775 train_time:400529ms step_avg:349.81ms
step:1146/1775 train_time:400983ms step_avg:349.90ms
step:1147/1775 train_time:401435ms step_avg:349.99ms
step:1148/1775 train_time:401889ms step_avg:350.08ms
step:1149/1775 train_time:402341ms step_avg:350.17ms
step:1150/1775 train_time:402795ms step_avg:350.26ms
step:1151/1775 train_time:403250ms step_avg:350.35ms
step:1152/1775 train_time:403706ms step_avg:350.44ms
step:1153/1775 train_time:404160ms step_avg:350.53ms
step:1154/1775 train_time:404614ms step_avg:350.62ms
step:1155/1775 train_time:405067ms step_avg:350.71ms
step:1156/1775 train_time:405520ms step_avg:350.80ms
step:1157/1775 train_time:405975ms step_avg:350.89ms
step:1158/1775 train_time:406610ms step_avg:351.13ms
step:1159/1775 train_time:407268ms step_avg:351.40ms
step:1160/1775 train_time:407928ms step_avg:351.66ms
step:1161/1775 train_time:408584ms step_avg:351.92ms
step:1162/1775 train_time:409245ms step_avg:352.19ms
step:1163/1775 train_time:409902ms step_avg:352.45ms
step:1164/1775 train_time:410564ms step_avg:352.72ms
step:1165/1775 train_time:411222ms step_avg:352.98ms
step:1166/1775 train_time:411880ms step_avg:353.24ms
step:1167/1775 train_time:412538ms step_avg:353.50ms
step:1168/1775 train_time:413196ms step_avg:353.76ms
step:1169/1775 train_time:413851ms step_avg:354.02ms
step:1170/1775 train_time:414511ms step_avg:354.28ms
step:1171/1775 train_time:415169ms step_avg:354.54ms
step:1172/1775 train_time:415829ms step_avg:354.80ms
step:1173/1775 train_time:416486ms step_avg:355.06ms
step:1174/1775 train_time:417146ms step_avg:355.32ms
step:1175/1775 train_time:417803ms step_avg:355.58ms
step:1176/1775 train_time:418464ms step_avg:355.84ms
step:1177/1775 train_time:419121ms step_avg:356.09ms
step:1178/1775 train_time:419781ms step_avg:356.35ms
step:1179/1775 train_time:420438ms step_avg:356.61ms
step:1180/1775 train_time:421094ms step_avg:356.86ms
step:1181/1775 train_time:421751ms step_avg:357.11ms
step:1182/1775 train_time:422409ms step_avg:357.37ms
step:1183/1775 train_time:423069ms step_avg:357.62ms
step:1184/1775 train_time:423728ms step_avg:357.88ms
step:1185/1775 train_time:424387ms step_avg:358.13ms
step:1186/1775 train_time:425046ms step_avg:358.39ms
step:1187/1775 train_time:425702ms step_avg:358.64ms
step:1188/1775 train_time:426360ms step_avg:358.89ms
step:1189/1775 train_time:427014ms step_avg:359.14ms
step:1190/1775 train_time:427671ms step_avg:359.39ms
step:1191/1775 train_time:428329ms step_avg:359.64ms
step:1192/1775 train_time:428990ms step_avg:359.89ms
step:1193/1775 train_time:429648ms step_avg:360.14ms
step:1194/1775 train_time:430308ms step_avg:360.39ms
step:1195/1775 train_time:430965ms step_avg:360.64ms
step:1196/1775 train_time:431625ms step_avg:360.89ms
step:1197/1775 train_time:432281ms step_avg:361.14ms
step:1198/1775 train_time:432942ms step_avg:361.39ms
step:1199/1775 train_time:433599ms step_avg:361.63ms
step:1200/1775 train_time:434255ms step_avg:361.88ms
step:1201/1775 train_time:434908ms step_avg:362.12ms
step:1202/1775 train_time:435567ms step_avg:362.37ms
step:1203/1775 train_time:436225ms step_avg:362.61ms
step:1204/1775 train_time:436885ms step_avg:362.86ms
step:1205/1775 train_time:437543ms step_avg:363.11ms
step:1206/1775 train_time:438202ms step_avg:363.35ms
step:1207/1775 train_time:438860ms step_avg:363.60ms
step:1208/1775 train_time:439517ms step_avg:363.84ms
step:1209/1775 train_time:440172ms step_avg:364.08ms
step:1210/1775 train_time:440831ms step_avg:364.32ms
step:1211/1775 train_time:441488ms step_avg:364.56ms
step:1212/1775 train_time:442148ms step_avg:364.81ms
step:1213/1775 train_time:442807ms step_avg:365.05ms
step:1214/1775 train_time:443466ms step_avg:365.29ms
step:1215/1775 train_time:444125ms step_avg:365.54ms
step:1216/1775 train_time:444786ms step_avg:365.78ms
step:1217/1775 train_time:445443ms step_avg:366.02ms
step:1218/1775 train_time:446103ms step_avg:366.26ms
step:1219/1775 train_time:446760ms step_avg:366.50ms
step:1220/1775 train_time:447416ms step_avg:366.73ms
step:1221/1775 train_time:448073ms step_avg:366.97ms
step:1222/1775 train_time:448732ms step_avg:367.21ms
step:1223/1775 train_time:449390ms step_avg:367.45ms
step:1224/1775 train_time:450051ms step_avg:367.69ms
step:1225/1775 train_time:450709ms step_avg:367.93ms
step:1226/1775 train_time:451367ms step_avg:368.16ms
step:1227/1775 train_time:452026ms step_avg:368.40ms
step:1228/1775 train_time:452686ms step_avg:368.64ms
step:1229/1775 train_time:453343ms step_avg:368.87ms
step:1230/1775 train_time:454003ms step_avg:369.11ms
step:1231/1775 train_time:454662ms step_avg:369.34ms
step:1232/1775 train_time:455323ms step_avg:369.58ms
step:1233/1775 train_time:455981ms step_avg:369.81ms
step:1234/1775 train_time:456638ms step_avg:370.05ms
step:1235/1775 train_time:457293ms step_avg:370.28ms
step:1236/1775 train_time:457953ms step_avg:370.51ms
step:1237/1775 train_time:458609ms step_avg:370.74ms
step:1238/1775 train_time:459269ms step_avg:370.98ms
step:1239/1775 train_time:459927ms step_avg:371.21ms
step:1240/1775 train_time:460586ms step_avg:371.44ms
step:1241/1775 train_time:461244ms step_avg:371.67ms
step:1242/1775 train_time:461905ms step_avg:371.90ms
step:1243/1775 train_time:462563ms step_avg:372.13ms
step:1244/1775 train_time:463219ms step_avg:372.36ms
step:1245/1775 train_time:463877ms step_avg:372.59ms
step:1246/1775 train_time:464534ms step_avg:372.82ms
step:1247/1775 train_time:465189ms step_avg:373.05ms
step:1248/1775 train_time:465848ms step_avg:373.28ms
step:1249/1775 train_time:466506ms step_avg:373.50ms
step:1250/1775 train_time:467163ms step_avg:373.73ms
step:1250/1775 val_loss:3.5054 train_time:467263ms step_avg:373.81ms
step:1251/1775 train_time:467819ms step_avg:373.96ms
step:1252/1775 train_time:468478ms step_avg:374.18ms
step:1253/1775 train_time:469135ms step_avg:374.41ms
step:1254/1775 train_time:469795ms step_avg:374.64ms
step:1255/1775 train_time:470455ms step_avg:374.86ms
step:1256/1775 train_time:471116ms step_avg:375.09ms
step:1257/1775 train_time:471772ms step_avg:375.32ms
step:1258/1775 train_time:472433ms step_avg:375.54ms
step:1259/1775 train_time:473089ms step_avg:375.77ms
step:1260/1775 train_time:473743ms step_avg:375.99ms
step:1261/1775 train_time:474401ms step_avg:376.21ms
step:1262/1775 train_time:475059ms step_avg:376.43ms
step:1263/1775 train_time:475717ms step_avg:376.66ms
step:1264/1775 train_time:476377ms step_avg:376.88ms
step:1265/1775 train_time:477035ms step_avg:377.10ms
step:1266/1775 train_time:477695ms step_avg:377.33ms
step:1267/1775 train_time:478354ms step_avg:377.55ms
step:1268/1775 train_time:479013ms step_avg:377.77ms
step:1269/1775 train_time:479669ms step_avg:377.99ms
step:1270/1775 train_time:480328ms step_avg:378.21ms
step:1271/1775 train_time:480984ms step_avg:378.43ms
step:1272/1775 train_time:481642ms step_avg:378.65ms
step:1273/1775 train_time:482299ms step_avg:378.87ms
step:1274/1775 train_time:482960ms step_avg:379.09ms
step:1275/1775 train_time:483617ms step_avg:379.31ms
step:1276/1775 train_time:484277ms step_avg:379.53ms
step:1277/1775 train_time:484932ms step_avg:379.74ms
step:1278/1775 train_time:485593ms step_avg:379.96ms
step:1279/1775 train_time:486253ms step_avg:380.18ms
step:1280/1775 train_time:486911ms step_avg:380.40ms
step:1281/1775 train_time:487570ms step_avg:380.62ms
step:1282/1775 train_time:488226ms step_avg:380.83ms
step:1283/1775 train_time:488883ms step_avg:381.05ms
step:1284/1775 train_time:489540ms step_avg:381.26ms
step:1285/1775 train_time:490196ms step_avg:381.48ms
step:1286/1775 train_time:490855ms step_avg:381.69ms
step:1287/1775 train_time:491511ms step_avg:381.90ms
step:1288/1775 train_time:492169ms step_avg:382.12ms
step:1289/1775 train_time:492823ms step_avg:382.33ms
step:1290/1775 train_time:493481ms step_avg:382.54ms
step:1291/1775 train_time:494137ms step_avg:382.76ms
step:1292/1775 train_time:494797ms step_avg:382.97ms
step:1293/1775 train_time:495453ms step_avg:383.18ms
step:1294/1775 train_time:496114ms step_avg:383.40ms
step:1295/1775 train_time:496770ms step_avg:383.61ms
step:1296/1775 train_time:497428ms step_avg:383.82ms
step:1297/1775 train_time:498082ms step_avg:384.03ms
step:1298/1775 train_time:498740ms step_avg:384.24ms
step:1299/1775 train_time:499397ms step_avg:384.45ms
step:1300/1775 train_time:500056ms step_avg:384.66ms
step:1301/1775 train_time:500714ms step_avg:384.87ms
step:1302/1775 train_time:501373ms step_avg:385.08ms
step:1303/1775 train_time:502033ms step_avg:385.29ms
step:1304/1775 train_time:502693ms step_avg:385.50ms
step:1305/1775 train_time:503353ms step_avg:385.71ms
step:1306/1775 train_time:504010ms step_avg:385.92ms
step:1307/1775 train_time:504665ms step_avg:386.12ms
step:1308/1775 train_time:505326ms step_avg:386.34ms
step:1309/1775 train_time:505981ms step_avg:386.54ms
step:1310/1775 train_time:506641ms step_avg:386.75ms
step:1311/1775 train_time:507296ms step_avg:386.95ms
step:1312/1775 train_time:507955ms step_avg:387.16ms
step:1313/1775 train_time:508614ms step_avg:387.37ms
step:1314/1775 train_time:509273ms step_avg:387.57ms
step:1315/1775 train_time:509930ms step_avg:387.78ms
step:1316/1775 train_time:510590ms step_avg:387.99ms
step:1317/1775 train_time:511247ms step_avg:388.19ms
step:1318/1775 train_time:511905ms step_avg:388.40ms
step:1319/1775 train_time:512562ms step_avg:388.60ms
step:1320/1775 train_time:513220ms step_avg:388.80ms
step:1321/1775 train_time:513876ms step_avg:389.01ms
step:1322/1775 train_time:514536ms step_avg:389.21ms
step:1323/1775 train_time:515194ms step_avg:389.41ms
step:1324/1775 train_time:515853ms step_avg:389.62ms
step:1325/1775 train_time:516512ms step_avg:389.82ms
step:1326/1775 train_time:517170ms step_avg:390.02ms
step:1327/1775 train_time:517825ms step_avg:390.22ms
step:1328/1775 train_time:518482ms step_avg:390.42ms
step:1329/1775 train_time:519139ms step_avg:390.62ms
step:1330/1775 train_time:519798ms step_avg:390.83ms
step:1331/1775 train_time:520459ms step_avg:391.03ms
step:1332/1775 train_time:521118ms step_avg:391.23ms
step:1333/1775 train_time:521776ms step_avg:391.43ms
step:1334/1775 train_time:522437ms step_avg:391.63ms
step:1335/1775 train_time:523094ms step_avg:391.83ms
step:1336/1775 train_time:523753ms step_avg:392.03ms
step:1337/1775 train_time:524410ms step_avg:392.23ms
step:1338/1775 train_time:525069ms step_avg:392.43ms
step:1339/1775 train_time:525726ms step_avg:392.63ms
step:1340/1775 train_time:526384ms step_avg:392.82ms
step:1341/1775 train_time:527039ms step_avg:393.02ms
step:1342/1775 train_time:527698ms step_avg:393.22ms
step:1343/1775 train_time:528357ms step_avg:393.42ms
step:1344/1775 train_time:529018ms step_avg:393.61ms
step:1345/1775 train_time:529674ms step_avg:393.81ms
step:1346/1775 train_time:530334ms step_avg:394.01ms
step:1347/1775 train_time:530992ms step_avg:394.20ms
step:1348/1775 train_time:531649ms step_avg:394.40ms
step:1349/1775 train_time:532304ms step_avg:394.59ms
step:1350/1775 train_time:532961ms step_avg:394.79ms
step:1351/1775 train_time:533618ms step_avg:394.98ms
step:1352/1775 train_time:534278ms step_avg:395.18ms
step:1353/1775 train_time:534936ms step_avg:395.37ms
step:1354/1775 train_time:535595ms step_avg:395.56ms
step:1355/1775 train_time:536253ms step_avg:395.76ms
step:1356/1775 train_time:536913ms step_avg:395.95ms
step:1357/1775 train_time:537572ms step_avg:396.15ms
step:1358/1775 train_time:538228ms step_avg:396.34ms
step:1359/1775 train_time:538884ms step_avg:396.53ms
step:1360/1775 train_time:539542ms step_avg:396.72ms
step:1361/1775 train_time:540199ms step_avg:396.91ms
step:1362/1775 train_time:540859ms step_avg:397.11ms
step:1363/1775 train_time:541516ms step_avg:397.30ms
step:1364/1775 train_time:542177ms step_avg:397.49ms
step:1365/1775 train_time:542836ms step_avg:397.68ms
step:1366/1775 train_time:543495ms step_avg:397.87ms
step:1367/1775 train_time:544154ms step_avg:398.06ms
step:1368/1775 train_time:544814ms step_avg:398.26ms
step:1369/1775 train_time:545471ms step_avg:398.44ms
step:1370/1775 train_time:546131ms step_avg:398.64ms
step:1371/1775 train_time:546788ms step_avg:398.82ms
step:1372/1775 train_time:547447ms step_avg:399.01ms
step:1373/1775 train_time:548101ms step_avg:399.20ms
step:1374/1775 train_time:548759ms step_avg:399.39ms
step:1375/1775 train_time:549417ms step_avg:399.58ms
step:1376/1775 train_time:550075ms step_avg:399.76ms
step:1377/1775 train_time:550732ms step_avg:399.95ms
step:1378/1775 train_time:551391ms step_avg:400.14ms
step:1379/1775 train_time:552048ms step_avg:400.32ms
step:1380/1775 train_time:552705ms step_avg:400.51ms
step:1381/1775 train_time:553361ms step_avg:400.70ms
step:1382/1775 train_time:554021ms step_avg:400.88ms
step:1383/1775 train_time:554678ms step_avg:401.07ms
step:1384/1775 train_time:555339ms step_avg:401.26ms
step:1385/1775 train_time:555995ms step_avg:401.44ms
step:1386/1775 train_time:556656ms step_avg:401.63ms
step:1387/1775 train_time:557313ms step_avg:401.81ms
step:1388/1775 train_time:557969ms step_avg:401.99ms
step:1389/1775 train_time:558625ms step_avg:402.18ms
step:1390/1775 train_time:559284ms step_avg:402.36ms
step:1391/1775 train_time:559939ms step_avg:402.54ms
step:1392/1775 train_time:560598ms step_avg:402.73ms
step:1393/1775 train_time:561256ms step_avg:402.91ms
step:1394/1775 train_time:561913ms step_avg:403.09ms
step:1395/1775 train_time:562573ms step_avg:403.28ms
step:1396/1775 train_time:563233ms step_avg:403.46ms
step:1397/1775 train_time:563889ms step_avg:403.64ms
step:1398/1775 train_time:564548ms step_avg:403.83ms
step:1399/1775 train_time:565204ms step_avg:404.01ms
step:1400/1775 train_time:565861ms step_avg:404.19ms
step:1401/1775 train_time:566519ms step_avg:404.37ms
step:1402/1775 train_time:567178ms step_avg:404.55ms
step:1403/1775 train_time:567835ms step_avg:404.73ms
step:1404/1775 train_time:568494ms step_avg:404.91ms
step:1405/1775 train_time:569152ms step_avg:405.09ms
step:1406/1775 train_time:569810ms step_avg:405.27ms
step:1407/1775 train_time:570467ms step_avg:405.45ms
step:1408/1775 train_time:571124ms step_avg:405.63ms
step:1409/1775 train_time:571781ms step_avg:405.81ms
step:1410/1775 train_time:572439ms step_avg:405.99ms
step:1411/1775 train_time:573097ms step_avg:406.16ms
step:1412/1775 train_time:573758ms step_avg:406.34ms
step:1413/1775 train_time:574415ms step_avg:406.52ms
step:1414/1775 train_time:575075ms step_avg:406.70ms
step:1415/1775 train_time:575733ms step_avg:406.88ms
step:1416/1775 train_time:576393ms step_avg:407.06ms
step:1417/1775 train_time:577050ms step_avg:407.23ms
step:1418/1775 train_time:577710ms step_avg:407.41ms
step:1419/1775 train_time:578367ms step_avg:407.59ms
step:1420/1775 train_time:579025ms step_avg:407.76ms
step:1421/1775 train_time:579681ms step_avg:407.94ms
step:1422/1775 train_time:580341ms step_avg:408.12ms
step:1423/1775 train_time:580997ms step_avg:408.29ms
step:1424/1775 train_time:581658ms step_avg:408.47ms
step:1425/1775 train_time:582314ms step_avg:408.64ms
step:1426/1775 train_time:582970ms step_avg:408.81ms
step:1427/1775 train_time:583628ms step_avg:408.99ms
step:1428/1775 train_time:584286ms step_avg:409.16ms
step:1429/1775 train_time:584942ms step_avg:409.34ms
step:1430/1775 train_time:585600ms step_avg:409.51ms
step:1431/1775 train_time:586258ms step_avg:409.68ms
step:1432/1775 train_time:586918ms step_avg:409.86ms
step:1433/1775 train_time:587574ms step_avg:410.03ms
step:1434/1775 train_time:588235ms step_avg:410.21ms
step:1435/1775 train_time:588892ms step_avg:410.38ms
step:1436/1775 train_time:589551ms step_avg:410.55ms
step:1437/1775 train_time:590208ms step_avg:410.72ms
step:1438/1775 train_time:590865ms step_avg:410.89ms
step:1439/1775 train_time:591521ms step_avg:411.06ms
step:1440/1775 train_time:592179ms step_avg:411.24ms
step:1441/1775 train_time:592838ms step_avg:411.41ms
step:1442/1775 train_time:593496ms step_avg:411.58ms
step:1443/1775 train_time:594154ms step_avg:411.75ms
step:1444/1775 train_time:594814ms step_avg:411.92ms
step:1445/1775 train_time:595474ms step_avg:412.09ms
step:1446/1775 train_time:596134ms step_avg:412.26ms
step:1447/1775 train_time:596791ms step_avg:412.43ms
step:1448/1775 train_time:597452ms step_avg:412.60ms
step:1449/1775 train_time:598108ms step_avg:412.77ms
step:1450/1775 train_time:598769ms step_avg:412.94ms
step:1451/1775 train_time:599423ms step_avg:413.11ms
step:1452/1775 train_time:600082ms step_avg:413.28ms
step:1453/1775 train_time:600739ms step_avg:413.45ms
step:1454/1775 train_time:601398ms step_avg:413.62ms
step:1455/1775 train_time:602056ms step_avg:413.78ms
step:1456/1775 train_time:602714ms step_avg:413.95ms
step:1457/1775 train_time:603370ms step_avg:414.12ms
step:1458/1775 train_time:604032ms step_avg:414.29ms
step:1459/1775 train_time:604690ms step_avg:414.46ms
step:1460/1775 train_time:605348ms step_avg:414.62ms
step:1461/1775 train_time:606006ms step_avg:414.79ms
step:1462/1775 train_time:606664ms step_avg:414.95ms
step:1463/1775 train_time:607320ms step_avg:415.12ms
step:1464/1775 train_time:607980ms step_avg:415.29ms
step:1465/1775 train_time:608637ms step_avg:415.45ms
step:1466/1775 train_time:609295ms step_avg:415.62ms
step:1467/1775 train_time:609955ms step_avg:415.78ms
step:1468/1775 train_time:610615ms step_avg:415.95ms
step:1469/1775 train_time:611272ms step_avg:416.11ms
step:1470/1775 train_time:611932ms step_avg:416.28ms
step:1471/1775 train_time:612584ms step_avg:416.44ms
step:1472/1775 train_time:613243ms step_avg:416.61ms
step:1473/1775 train_time:613899ms step_avg:416.77ms
step:1474/1775 train_time:614557ms step_avg:416.93ms
step:1475/1775 train_time:615214ms step_avg:417.09ms
step:1476/1775 train_time:615873ms step_avg:417.26ms
step:1477/1775 train_time:616532ms step_avg:417.42ms
step:1478/1775 train_time:617191ms step_avg:417.59ms
step:1479/1775 train_time:617849ms step_avg:417.75ms
step:1480/1775 train_time:618507ms step_avg:417.91ms
step:1481/1775 train_time:619164ms step_avg:418.07ms
step:1482/1775 train_time:619821ms step_avg:418.23ms
step:1483/1775 train_time:620476ms step_avg:418.39ms
step:1484/1775 train_time:621136ms step_avg:418.56ms
step:1485/1775 train_time:621794ms step_avg:418.72ms
step:1486/1775 train_time:622454ms step_avg:418.88ms
step:1487/1775 train_time:623112ms step_avg:419.04ms
step:1488/1775 train_time:623771ms step_avg:419.20ms
step:1489/1775 train_time:624429ms step_avg:419.36ms
step:1490/1775 train_time:625087ms step_avg:419.52ms
step:1491/1775 train_time:625743ms step_avg:419.68ms
step:1492/1775 train_time:626403ms step_avg:419.84ms
step:1493/1775 train_time:627059ms step_avg:420.00ms
step:1494/1775 train_time:627718ms step_avg:420.16ms
step:1495/1775 train_time:628375ms step_avg:420.32ms
step:1496/1775 train_time:629035ms step_avg:420.48ms
step:1497/1775 train_time:629693ms step_avg:420.64ms
step:1498/1775 train_time:630351ms step_avg:420.80ms
step:1499/1775 train_time:631012ms step_avg:420.96ms
step:1500/1775 train_time:631670ms step_avg:421.11ms
step:1500/1775 val_loss:3.3774 train_time:631770ms step_avg:421.18ms
step:1501/1775 train_time:632327ms step_avg:421.27ms
step:1502/1775 train_time:632987ms step_avg:421.43ms
step:1503/1775 train_time:633644ms step_avg:421.59ms
step:1504/1775 train_time:634303ms step_avg:421.74ms
step:1505/1775 train_time:634961ms step_avg:421.90ms
step:1506/1775 train_time:635620ms step_avg:422.06ms
step:1507/1775 train_time:636278ms step_avg:422.22ms
step:1508/1775 train_time:636938ms step_avg:422.37ms
step:1509/1775 train_time:637596ms step_avg:422.53ms
step:1510/1775 train_time:638253ms step_avg:422.68ms
step:1511/1775 train_time:638913ms step_avg:422.84ms
step:1512/1775 train_time:639568ms step_avg:423.00ms
step:1513/1775 train_time:640225ms step_avg:423.15ms
step:1514/1775 train_time:640884ms step_avg:423.31ms
step:1515/1775 train_time:641542ms step_avg:423.46ms
step:1516/1775 train_time:642201ms step_avg:423.62ms
step:1517/1775 train_time:642859ms step_avg:423.77ms
step:1518/1775 train_time:643518ms step_avg:423.92ms
step:1519/1775 train_time:644177ms step_avg:424.08ms
step:1520/1775 train_time:644836ms step_avg:424.23ms
step:1521/1775 train_time:645494ms step_avg:424.39ms
step:1522/1775 train_time:646153ms step_avg:424.54ms
step:1523/1775 train_time:646809ms step_avg:424.69ms
step:1524/1775 train_time:647465ms step_avg:424.85ms
step:1525/1775 train_time:648121ms step_avg:425.00ms
step:1526/1775 train_time:648782ms step_avg:425.15ms
step:1527/1775 train_time:649437ms step_avg:425.30ms
step:1528/1775 train_time:650093ms step_avg:425.45ms
step:1529/1775 train_time:650750ms step_avg:425.60ms
step:1530/1775 train_time:651407ms step_avg:425.76ms
step:1531/1775 train_time:652066ms step_avg:425.91ms
step:1532/1775 train_time:652724ms step_avg:426.06ms
step:1533/1775 train_time:653381ms step_avg:426.21ms
step:1534/1775 train_time:654040ms step_avg:426.36ms
step:1535/1775 train_time:654700ms step_avg:426.51ms
step:1536/1775 train_time:655359ms step_avg:426.67ms
step:1537/1775 train_time:656018ms step_avg:426.82ms
step:1538/1775 train_time:656679ms step_avg:426.97ms
step:1539/1775 train_time:657338ms step_avg:427.12ms
step:1540/1775 train_time:658000ms step_avg:427.27ms
step:1541/1775 train_time:658658ms step_avg:427.42ms
step:1542/1775 train_time:659316ms step_avg:427.57ms
step:1543/1775 train_time:659973ms step_avg:427.72ms
step:1544/1775 train_time:660631ms step_avg:427.87ms
step:1545/1775 train_time:661286ms step_avg:428.02ms
step:1546/1775 train_time:661944ms step_avg:428.17ms
step:1547/1775 train_time:662600ms step_avg:428.31ms
step:1548/1775 train_time:663260ms step_avg:428.46ms
step:1549/1775 train_time:663917ms step_avg:428.61ms
step:1550/1775 train_time:664575ms step_avg:428.76ms
step:1551/1775 train_time:665232ms step_avg:428.91ms
step:1552/1775 train_time:665889ms step_avg:429.05ms
step:1553/1775 train_time:666545ms step_avg:429.20ms
step:1554/1775 train_time:667206ms step_avg:429.35ms
step:1555/1775 train_time:667864ms step_avg:429.49ms
step:1556/1775 train_time:668523ms step_avg:429.64ms
step:1557/1775 train_time:669181ms step_avg:429.79ms
step:1558/1775 train_time:669840ms step_avg:429.94ms
step:1559/1775 train_time:670497ms step_avg:430.08ms
step:1560/1775 train_time:671155ms step_avg:430.23ms
step:1561/1775 train_time:671809ms step_avg:430.37ms
step:1562/1775 train_time:672467ms step_avg:430.52ms
step:1563/1775 train_time:673123ms step_avg:430.66ms
step:1564/1775 train_time:673784ms step_avg:430.81ms
step:1565/1775 train_time:674440ms step_avg:430.95ms
step:1566/1775 train_time:675101ms step_avg:431.10ms
step:1567/1775 train_time:675758ms step_avg:431.24ms
step:1568/1775 train_time:676420ms step_avg:431.39ms
step:1569/1775 train_time:677078ms step_avg:431.53ms
step:1570/1775 train_time:677740ms step_avg:431.68ms
step:1571/1775 train_time:678396ms step_avg:431.82ms
step:1572/1775 train_time:679056ms step_avg:431.97ms
step:1573/1775 train_time:679712ms step_avg:432.11ms
step:1574/1775 train_time:680369ms step_avg:432.25ms
step:1575/1775 train_time:681024ms step_avg:432.40ms
step:1576/1775 train_time:681683ms step_avg:432.54ms
step:1577/1775 train_time:682340ms step_avg:432.68ms
step:1578/1775 train_time:683000ms step_avg:432.83ms
step:1579/1775 train_time:683657ms step_avg:432.97ms
step:1580/1775 train_time:684316ms step_avg:433.11ms
step:1581/1775 train_time:684972ms step_avg:433.25ms
step:1582/1775 train_time:685631ms step_avg:433.40ms
step:1583/1775 train_time:686285ms step_avg:433.53ms
step:1584/1775 train_time:686943ms step_avg:433.68ms
step:1585/1775 train_time:687600ms step_avg:433.82ms
step:1586/1775 train_time:688261ms step_avg:433.96ms
step:1587/1775 train_time:688920ms step_avg:434.10ms
step:1588/1775 train_time:689581ms step_avg:434.24ms
step:1589/1775 train_time:690237ms step_avg:434.38ms
step:1590/1775 train_time:690896ms step_avg:434.53ms
step:1591/1775 train_time:691553ms step_avg:434.67ms
step:1592/1775 train_time:692211ms step_avg:434.81ms
step:1593/1775 train_time:692867ms step_avg:434.94ms
step:1594/1775 train_time:693526ms step_avg:435.09ms
step:1595/1775 train_time:694183ms step_avg:435.22ms
step:1596/1775 train_time:694843ms step_avg:435.37ms
step:1597/1775 train_time:695501ms step_avg:435.50ms
step:1598/1775 train_time:696160ms step_avg:435.64ms
step:1599/1775 train_time:696818ms step_avg:435.78ms
step:1600/1775 train_time:697476ms step_avg:435.92ms
step:1601/1775 train_time:698134ms step_avg:436.06ms
step:1602/1775 train_time:698793ms step_avg:436.20ms
step:1603/1775 train_time:699448ms step_avg:436.34ms
step:1604/1775 train_time:700105ms step_avg:436.47ms
step:1605/1775 train_time:700763ms step_avg:436.61ms
step:1606/1775 train_time:701420ms step_avg:436.75ms
step:1607/1775 train_time:702079ms step_avg:436.89ms
step:1608/1775 train_time:702738ms step_avg:437.03ms
step:1609/1775 train_time:703397ms step_avg:437.16ms
step:1610/1775 train_time:704057ms step_avg:437.30ms
step:1611/1775 train_time:704712ms step_avg:437.44ms
step:1612/1775 train_time:705370ms step_avg:437.57ms
step:1613/1775 train_time:706025ms step_avg:437.71ms
step:1614/1775 train_time:706685ms step_avg:437.85ms
step:1615/1775 train_time:707341ms step_avg:437.98ms
step:1616/1775 train_time:708002ms step_avg:438.12ms
step:1617/1775 train_time:708660ms step_avg:438.26ms
step:1618/1775 train_time:709320ms step_avg:438.39ms
step:1619/1775 train_time:709976ms step_avg:438.53ms
step:1620/1775 train_time:710635ms step_avg:438.66ms
step:1621/1775 train_time:711293ms step_avg:438.80ms
step:1622/1775 train_time:711952ms step_avg:438.93ms
step:1623/1775 train_time:712611ms step_avg:439.07ms
step:1624/1775 train_time:713267ms step_avg:439.20ms
step:1625/1775 train_time:713925ms step_avg:439.34ms
step:1626/1775 train_time:714585ms step_avg:439.47ms
step:1627/1775 train_time:715242ms step_avg:439.61ms
step:1628/1775 train_time:715900ms step_avg:439.74ms
step:1629/1775 train_time:716559ms step_avg:439.88ms
step:1630/1775 train_time:717222ms step_avg:440.01ms
step:1631/1775 train_time:717880ms step_avg:440.15ms
step:1632/1775 train_time:718540ms step_avg:440.28ms
step:1633/1775 train_time:719197ms step_avg:440.41ms
step:1634/1775 train_time:719859ms step_avg:440.55ms
step:1635/1775 train_time:720517ms step_avg:440.68ms
step:1636/1775 train_time:721177ms step_avg:440.82ms
step:1637/1775 train_time:721835ms step_avg:440.95ms
step:1638/1775 train_time:722493ms step_avg:441.08ms
step:1639/1775 train_time:723151ms step_avg:441.21ms
step:1640/1775 train_time:723809ms step_avg:441.35ms
step:1641/1775 train_time:724465ms step_avg:441.48ms
step:1642/1775 train_time:725123ms step_avg:441.61ms
step:1643/1775 train_time:725778ms step_avg:441.74ms
step:1644/1775 train_time:726434ms step_avg:441.87ms
step:1645/1775 train_time:727093ms step_avg:442.00ms
step:1646/1775 train_time:727751ms step_avg:442.13ms
step:1647/1775 train_time:728406ms step_avg:442.26ms
step:1648/1775 train_time:729065ms step_avg:442.39ms
step:1649/1775 train_time:729722ms step_avg:442.52ms
step:1650/1775 train_time:730380ms step_avg:442.65ms
step:1651/1775 train_time:731039ms step_avg:442.79ms
step:1652/1775 train_time:731700ms step_avg:442.92ms
step:1653/1775 train_time:732357ms step_avg:443.05ms
step:1654/1775 train_time:733014ms step_avg:443.18ms
step:1655/1775 train_time:733669ms step_avg:443.30ms
step:1656/1775 train_time:734329ms step_avg:443.44ms
step:1657/1775 train_time:734986ms step_avg:443.56ms
step:1658/1775 train_time:735644ms step_avg:443.69ms
step:1659/1775 train_time:736302ms step_avg:443.82ms
step:1660/1775 train_time:736962ms step_avg:443.95ms
step:1661/1775 train_time:737616ms step_avg:444.08ms
step:1662/1775 train_time:738276ms step_avg:444.21ms
step:1663/1775 train_time:738934ms step_avg:444.34ms
step:1664/1775 train_time:739594ms step_avg:444.47ms
step:1665/1775 train_time:740252ms step_avg:444.60ms
step:1666/1775 train_time:740910ms step_avg:444.72ms
step:1667/1775 train_time:741568ms step_avg:444.85ms
step:1668/1775 train_time:742227ms step_avg:444.98ms
step:1669/1775 train_time:742883ms step_avg:445.11ms
step:1670/1775 train_time:743542ms step_avg:445.24ms
step:1671/1775 train_time:744201ms step_avg:445.36ms
step:1672/1775 train_time:744861ms step_avg:445.49ms
step:1673/1775 train_time:745520ms step_avg:445.62ms
step:1674/1775 train_time:746180ms step_avg:445.75ms
step:1675/1775 train_time:746839ms step_avg:445.87ms
step:1676/1775 train_time:747496ms step_avg:446.00ms
step:1677/1775 train_time:748157ms step_avg:446.13ms
step:1678/1775 train_time:748817ms step_avg:446.26ms
step:1679/1775 train_time:749473ms step_avg:446.38ms
step:1680/1775 train_time:750130ms step_avg:446.51ms
step:1681/1775 train_time:750785ms step_avg:446.63ms
step:1682/1775 train_time:751444ms step_avg:446.76ms
step:1683/1775 train_time:752103ms step_avg:446.88ms
step:1684/1775 train_time:752763ms step_avg:447.01ms
step:1685/1775 train_time:753421ms step_avg:447.13ms
step:1686/1775 train_time:754081ms step_avg:447.26ms
step:1687/1775 train_time:754739ms step_avg:447.39ms
step:1688/1775 train_time:755397ms step_avg:447.51ms
step:1689/1775 train_time:756055ms step_avg:447.63ms
step:1690/1775 train_time:756712ms step_avg:447.76ms
step:1691/1775 train_time:757367ms step_avg:447.88ms
step:1692/1775 train_time:758025ms step_avg:448.01ms
step:1693/1775 train_time:758681ms step_avg:448.13ms
step:1694/1775 train_time:759341ms step_avg:448.25ms
step:1695/1775 train_time:760000ms step_avg:448.38ms
step:1696/1775 train_time:760659ms step_avg:448.50ms
step:1697/1775 train_time:761314ms step_avg:448.62ms
step:1698/1775 train_time:761973ms step_avg:448.75ms
step:1699/1775 train_time:762629ms step_avg:448.87ms
step:1700/1775 train_time:763287ms step_avg:448.99ms
step:1701/1775 train_time:763943ms step_avg:449.11ms
step:1702/1775 train_time:764603ms step_avg:449.24ms
step:1703/1775 train_time:765258ms step_avg:449.36ms
step:1704/1775 train_time:765919ms step_avg:449.48ms
step:1705/1775 train_time:766577ms step_avg:449.61ms
step:1706/1775 train_time:767236ms step_avg:449.73ms
step:1707/1775 train_time:767895ms step_avg:449.85ms
step:1708/1775 train_time:768550ms step_avg:449.97ms
step:1709/1775 train_time:769208ms step_avg:450.09ms
step:1710/1775 train_time:769866ms step_avg:450.21ms
step:1711/1775 train_time:770523ms step_avg:450.33ms
step:1712/1775 train_time:771183ms step_avg:450.46ms
step:1713/1775 train_time:771839ms step_avg:450.58ms
step:1714/1775 train_time:772498ms step_avg:450.70ms
step:1715/1775 train_time:773156ms step_avg:450.82ms
step:1716/1775 train_time:773812ms step_avg:450.94ms
step:1717/1775 train_time:774469ms step_avg:451.06ms
step:1718/1775 train_time:775128ms step_avg:451.18ms
step:1719/1775 train_time:775784ms step_avg:451.30ms
step:1720/1775 train_time:776443ms step_avg:451.42ms
step:1721/1775 train_time:777099ms step_avg:451.54ms
step:1722/1775 train_time:777760ms step_avg:451.66ms
step:1723/1775 train_time:778417ms step_avg:451.78ms
step:1724/1775 train_time:779075ms step_avg:451.90ms
step:1725/1775 train_time:779731ms step_avg:452.02ms
step:1726/1775 train_time:780389ms step_avg:452.14ms
step:1727/1775 train_time:781044ms step_avg:452.26ms
step:1728/1775 train_time:781703ms step_avg:452.37ms
step:1729/1775 train_time:782360ms step_avg:452.49ms
step:1730/1775 train_time:783021ms step_avg:452.61ms
step:1731/1775 train_time:783677ms step_avg:452.73ms
step:1732/1775 train_time:784340ms step_avg:452.85ms
step:1733/1775 train_time:784997ms step_avg:452.97ms
step:1734/1775 train_time:785656ms step_avg:453.09ms
step:1735/1775 train_time:786311ms step_avg:453.21ms
step:1736/1775 train_time:786979ms step_avg:453.33ms
step:1737/1775 train_time:787640ms step_avg:453.45ms
step:1738/1775 train_time:788301ms step_avg:453.57ms
step:1739/1775 train_time:788963ms step_avg:453.69ms
step:1740/1775 train_time:789625ms step_avg:453.81ms
step:1741/1775 train_time:790285ms step_avg:453.93ms
step:1742/1775 train_time:790946ms step_avg:454.05ms
step:1743/1775 train_time:791606ms step_avg:454.16ms
step:1744/1775 train_time:792265ms step_avg:454.28ms
step:1745/1775 train_time:792923ms step_avg:454.40ms
step:1746/1775 train_time:793586ms step_avg:454.52ms
step:1747/1775 train_time:794245ms step_avg:454.63ms
step:1748/1775 train_time:794907ms step_avg:454.75ms
step:1749/1775 train_time:795566ms step_avg:454.87ms
step:1750/1775 train_time:796227ms step_avg:454.99ms
step:1750/1775 val_loss:3.2861 train_time:796327ms step_avg:455.04ms
step:1751/1775 train_time:796888ms step_avg:455.10ms
step:1752/1775 train_time:797549ms step_avg:455.22ms
step:1753/1775 train_time:798210ms step_avg:455.34ms
step:1754/1775 train_time:798873ms step_avg:455.46ms
step:1755/1775 train_time:799532ms step_avg:455.57ms
step:1756/1775 train_time:800193ms step_avg:455.69ms
step:1757/1775 train_time:800852ms step_avg:455.81ms
step:1758/1775 train_time:801513ms step_avg:455.92ms
step:1759/1775 train_time:802174ms step_avg:456.04ms
step:1760/1775 train_time:802835ms step_avg:456.16ms
step:1761/1775 train_time:803493ms step_avg:456.27ms
step:1762/1775 train_time:804156ms step_avg:456.39ms
step:1763/1775 train_time:804816ms step_avg:456.50ms
step:1764/1775 train_time:805475ms step_avg:456.62ms
step:1765/1775 train_time:806135ms step_avg:456.73ms
step:1766/1775 train_time:806796ms step_avg:456.85ms
step:1767/1775 train_time:807452ms step_avg:456.96ms
step:1768/1775 train_time:808115ms step_avg:457.08ms
step:1769/1775 train_time:808772ms step_avg:457.19ms
step:1770/1775 train_time:809432ms step_avg:457.31ms
step:1771/1775 train_time:810092ms step_avg:457.42ms
step:1772/1775 train_time:810753ms step_avg:457.54ms
step:1773/1775 train_time:811411ms step_avg:457.65ms
step:1774/1775 train_time:812070ms step_avg:457.76ms
step:1775/1775 train_time:812729ms step_avg:457.88ms
step:1775/1775 val_loss:3.2796 train_time:812830ms step_avg:457.93ms
peak memory allocated: 32826 MiB reserved: 46600 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Thu Nov  6 04:37:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     82279      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     82280      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     82281      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     82282      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     82283      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     82284      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     82285      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     82286      C   /root/.venv/bin/python3                         0MiB |
|    1   N/A  N/A     82280      C   /root/.venv/bin/python3                         0MiB |
|    2   N/A  N/A     82281      C   /root/.venv/bin/python3                         0MiB |
|    3   N/A  N/A     82282      C   /root/.venv/bin/python3                         0MiB |
|    4   N/A  N/A     82283      C   /root/.venv/bin/python3                         0MiB |
|    5   N/A  N/A     82284      C   /root/.venv/bin/python3                         0MiB |
|    6   N/A  N/A     82285      C   /root/.venv/bin/python3                         0MiB |
|    7   N/A  N/A     82286      C   /root/.venv/bin/python3                         0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:121ms step_avg:120.56ms
step:2/2285 train_time:142ms step_avg:71.03ms
step:3/2285 train_time:180ms step_avg:60.09ms
step:4/2285 train_time:237ms step_avg:59.20ms
step:5/2285 train_time:297ms step_avg:59.30ms
step:6/2285 train_time:355ms step_avg:59.18ms
step:7/2285 train_time:416ms step_avg:59.47ms
step:8/2285 train_time:475ms step_avg:59.36ms
step:9/2285 train_time:536ms step_avg:59.54ms
step:10/2285 train_time:595ms step_avg:59.46ms
step:11/2285 train_time:656ms step_avg:59.60ms
step:12/2285 train_time:715ms step_avg:59.55ms
step:13/2285 train_time:776ms step_avg:59.71ms
step:14/2285 train_time:835ms step_avg:59.65ms
step:15/2285 train_time:896ms step_avg:59.75ms
step:16/2285 train_time:956ms step_avg:59.72ms
step:17/2285 train_time:1020ms step_avg:60.00ms
step:18/2285 train_time:1083ms step_avg:60.19ms
step:19/2285 train_time:1149ms step_avg:60.47ms
step:20/2285 train_time:1209ms step_avg:60.44ms
step:21/2285 train_time:1271ms step_avg:60.54ms
step:22/2285 train_time:1331ms step_avg:60.49ms
step:23/2285 train_time:1393ms step_avg:60.58ms
step:24/2285 train_time:1452ms step_avg:60.52ms
step:25/2285 train_time:1515ms step_avg:60.58ms
step:26/2285 train_time:1573ms step_avg:60.50ms
step:27/2285 train_time:1635ms step_avg:60.54ms
step:28/2285 train_time:1694ms step_avg:60.49ms
step:29/2285 train_time:1755ms step_avg:60.52ms
step:30/2285 train_time:1814ms step_avg:60.47ms
step:31/2285 train_time:1876ms step_avg:60.53ms
step:32/2285 train_time:1937ms step_avg:60.52ms
step:33/2285 train_time:2000ms step_avg:60.62ms
step:34/2285 train_time:2062ms step_avg:60.63ms
step:35/2285 train_time:2126ms step_avg:60.73ms
step:36/2285 train_time:2185ms step_avg:60.71ms
step:37/2285 train_time:2249ms step_avg:60.77ms
step:38/2285 train_time:2308ms step_avg:60.73ms
step:39/2285 train_time:2369ms step_avg:60.75ms
step:40/2285 train_time:2429ms step_avg:60.72ms
step:41/2285 train_time:2491ms step_avg:60.75ms
step:42/2285 train_time:2549ms step_avg:60.70ms
step:43/2285 train_time:2611ms step_avg:60.72ms
step:44/2285 train_time:2670ms step_avg:60.68ms
step:45/2285 train_time:2732ms step_avg:60.71ms
step:46/2285 train_time:2791ms step_avg:60.68ms
step:47/2285 train_time:2854ms step_avg:60.72ms
step:48/2285 train_time:2913ms step_avg:60.69ms
step:49/2285 train_time:2976ms step_avg:60.73ms
step:50/2285 train_time:3036ms step_avg:60.72ms
step:51/2285 train_time:3099ms step_avg:60.76ms
step:52/2285 train_time:3160ms step_avg:60.76ms
step:53/2285 train_time:3223ms step_avg:60.81ms
step:54/2285 train_time:3283ms step_avg:60.80ms
step:55/2285 train_time:3346ms step_avg:60.83ms
step:56/2285 train_time:3405ms step_avg:60.80ms
step:57/2285 train_time:3466ms step_avg:60.81ms
step:58/2285 train_time:3526ms step_avg:60.78ms
step:59/2285 train_time:3587ms step_avg:60.80ms
step:60/2285 train_time:3647ms step_avg:60.78ms
step:61/2285 train_time:3708ms step_avg:60.79ms
step:62/2285 train_time:3768ms step_avg:60.77ms
step:63/2285 train_time:3829ms step_avg:60.78ms
step:64/2285 train_time:3890ms step_avg:60.78ms
step:65/2285 train_time:3952ms step_avg:60.81ms
step:66/2285 train_time:4012ms step_avg:60.78ms
step:67/2285 train_time:4074ms step_avg:60.81ms
step:68/2285 train_time:4133ms step_avg:60.79ms
step:69/2285 train_time:4196ms step_avg:60.80ms
step:70/2285 train_time:4255ms step_avg:60.79ms
step:71/2285 train_time:4317ms step_avg:60.80ms
step:72/2285 train_time:4377ms step_avg:60.80ms
step:73/2285 train_time:4440ms step_avg:60.82ms
step:74/2285 train_time:4500ms step_avg:60.81ms
step:75/2285 train_time:4563ms step_avg:60.83ms
step:76/2285 train_time:4623ms step_avg:60.82ms
step:77/2285 train_time:4684ms step_avg:60.84ms
step:78/2285 train_time:4744ms step_avg:60.82ms
step:79/2285 train_time:4805ms step_avg:60.83ms
step:80/2285 train_time:4865ms step_avg:60.81ms
step:81/2285 train_time:4926ms step_avg:60.82ms
step:82/2285 train_time:4985ms step_avg:60.80ms
step:83/2285 train_time:5047ms step_avg:60.81ms
step:84/2285 train_time:5106ms step_avg:60.79ms
step:85/2285 train_time:5169ms step_avg:60.81ms
step:86/2285 train_time:5228ms step_avg:60.79ms
step:87/2285 train_time:5291ms step_avg:60.82ms
step:88/2285 train_time:5350ms step_avg:60.80ms
step:89/2285 train_time:5414ms step_avg:60.83ms
step:90/2285 train_time:5473ms step_avg:60.81ms
step:91/2285 train_time:5536ms step_avg:60.83ms
step:92/2285 train_time:5596ms step_avg:60.83ms
step:93/2285 train_time:5657ms step_avg:60.83ms
step:94/2285 train_time:5717ms step_avg:60.82ms
step:95/2285 train_time:5779ms step_avg:60.83ms
step:96/2285 train_time:5839ms step_avg:60.82ms
step:97/2285 train_time:5900ms step_avg:60.83ms
step:98/2285 train_time:5960ms step_avg:60.81ms
step:99/2285 train_time:6023ms step_avg:60.84ms
step:100/2285 train_time:6083ms step_avg:60.83ms
step:101/2285 train_time:6145ms step_avg:60.84ms
step:102/2285 train_time:6204ms step_avg:60.82ms
step:103/2285 train_time:6266ms step_avg:60.84ms
step:104/2285 train_time:6326ms step_avg:60.82ms
step:105/2285 train_time:6388ms step_avg:60.84ms
step:106/2285 train_time:6446ms step_avg:60.81ms
step:107/2285 train_time:6508ms step_avg:60.83ms
step:108/2285 train_time:6568ms step_avg:60.81ms
step:109/2285 train_time:6630ms step_avg:60.83ms
step:110/2285 train_time:6690ms step_avg:60.82ms
step:111/2285 train_time:6753ms step_avg:60.84ms
step:112/2285 train_time:6812ms step_avg:60.82ms
step:113/2285 train_time:6874ms step_avg:60.83ms
step:114/2285 train_time:6933ms step_avg:60.82ms
step:115/2285 train_time:6995ms step_avg:60.83ms
step:116/2285 train_time:7055ms step_avg:60.82ms
step:117/2285 train_time:7116ms step_avg:60.82ms
step:118/2285 train_time:7176ms step_avg:60.81ms
step:119/2285 train_time:7238ms step_avg:60.82ms
step:120/2285 train_time:7298ms step_avg:60.81ms
step:121/2285 train_time:7360ms step_avg:60.82ms
step:122/2285 train_time:7419ms step_avg:60.81ms
step:123/2285 train_time:7481ms step_avg:60.82ms
step:124/2285 train_time:7541ms step_avg:60.81ms
step:125/2285 train_time:7602ms step_avg:60.82ms
step:126/2285 train_time:7661ms step_avg:60.81ms
step:127/2285 train_time:7724ms step_avg:60.82ms
step:128/2285 train_time:7783ms step_avg:60.80ms
step:129/2285 train_time:7844ms step_avg:60.81ms
step:130/2285 train_time:7903ms step_avg:60.79ms
step:131/2285 train_time:7964ms step_avg:60.80ms
step:132/2285 train_time:8024ms step_avg:60.79ms
step:133/2285 train_time:8085ms step_avg:60.79ms
step:134/2285 train_time:8145ms step_avg:60.78ms
step:135/2285 train_time:8206ms step_avg:60.79ms
step:136/2285 train_time:8265ms step_avg:60.77ms
step:137/2285 train_time:8327ms step_avg:60.78ms
step:138/2285 train_time:8387ms step_avg:60.77ms
step:139/2285 train_time:8449ms step_avg:60.78ms
step:140/2285 train_time:8508ms step_avg:60.77ms
step:141/2285 train_time:8570ms step_avg:60.78ms
step:142/2285 train_time:8629ms step_avg:60.77ms
step:143/2285 train_time:8691ms step_avg:60.77ms
step:144/2285 train_time:8749ms step_avg:60.76ms
step:145/2285 train_time:8812ms step_avg:60.77ms
step:146/2285 train_time:8871ms step_avg:60.76ms
step:147/2285 train_time:8933ms step_avg:60.77ms
step:148/2285 train_time:8992ms step_avg:60.76ms
step:149/2285 train_time:9053ms step_avg:60.76ms
step:150/2285 train_time:9112ms step_avg:60.75ms
step:151/2285 train_time:9174ms step_avg:60.75ms
step:152/2285 train_time:9233ms step_avg:60.75ms
step:153/2285 train_time:9295ms step_avg:60.75ms
step:154/2285 train_time:9355ms step_avg:60.75ms
step:155/2285 train_time:9416ms step_avg:60.75ms
step:156/2285 train_time:9476ms step_avg:60.74ms
step:157/2285 train_time:9538ms step_avg:60.75ms
step:158/2285 train_time:9597ms step_avg:60.74ms
step:159/2285 train_time:9659ms step_avg:60.75ms
step:160/2285 train_time:9719ms step_avg:60.74ms
step:161/2285 train_time:9781ms step_avg:60.75ms
step:162/2285 train_time:9840ms step_avg:60.74ms
step:163/2285 train_time:9901ms step_avg:60.75ms
step:164/2285 train_time:9960ms step_avg:60.73ms
step:165/2285 train_time:10022ms step_avg:60.74ms
step:166/2285 train_time:10081ms step_avg:60.73ms
step:167/2285 train_time:10142ms step_avg:60.73ms
step:168/2285 train_time:10201ms step_avg:60.72ms
step:169/2285 train_time:10263ms step_avg:60.73ms
step:170/2285 train_time:10323ms step_avg:60.72ms
step:171/2285 train_time:10384ms step_avg:60.73ms
step:172/2285 train_time:10443ms step_avg:60.72ms
step:173/2285 train_time:10505ms step_avg:60.72ms
step:174/2285 train_time:10563ms step_avg:60.71ms
step:175/2285 train_time:10625ms step_avg:60.71ms
step:176/2285 train_time:10684ms step_avg:60.70ms
step:177/2285 train_time:10745ms step_avg:60.71ms
step:178/2285 train_time:10804ms step_avg:60.70ms
step:179/2285 train_time:10866ms step_avg:60.70ms
step:180/2285 train_time:10925ms step_avg:60.69ms
step:181/2285 train_time:10987ms step_avg:60.70ms
step:182/2285 train_time:11045ms step_avg:60.69ms
step:183/2285 train_time:11107ms step_avg:60.69ms
step:184/2285 train_time:11166ms step_avg:60.68ms
step:185/2285 train_time:11227ms step_avg:60.69ms
step:186/2285 train_time:11286ms step_avg:60.68ms
step:187/2285 train_time:11348ms step_avg:60.68ms
step:188/2285 train_time:11407ms step_avg:60.68ms
step:189/2285 train_time:11469ms step_avg:60.68ms
step:190/2285 train_time:11528ms step_avg:60.67ms
step:191/2285 train_time:11590ms step_avg:60.68ms
step:192/2285 train_time:11649ms step_avg:60.67ms
step:193/2285 train_time:11710ms step_avg:60.68ms
step:194/2285 train_time:11770ms step_avg:60.67ms
step:195/2285 train_time:11831ms step_avg:60.67ms
step:196/2285 train_time:11890ms step_avg:60.67ms
step:197/2285 train_time:11952ms step_avg:60.67ms
step:198/2285 train_time:12011ms step_avg:60.66ms
step:199/2285 train_time:12072ms step_avg:60.67ms
step:200/2285 train_time:12131ms step_avg:60.66ms
step:201/2285 train_time:12193ms step_avg:60.66ms
step:202/2285 train_time:12252ms step_avg:60.65ms
step:203/2285 train_time:12313ms step_avg:60.66ms
step:204/2285 train_time:12372ms step_avg:60.65ms
step:205/2285 train_time:12434ms step_avg:60.65ms
step:206/2285 train_time:12494ms step_avg:60.65ms
step:207/2285 train_time:12555ms step_avg:60.65ms
step:208/2285 train_time:12614ms step_avg:60.65ms
step:209/2285 train_time:12677ms step_avg:60.66ms
step:210/2285 train_time:12737ms step_avg:60.65ms
step:211/2285 train_time:12799ms step_avg:60.66ms
step:212/2285 train_time:12858ms step_avg:60.65ms
step:213/2285 train_time:12921ms step_avg:60.66ms
step:214/2285 train_time:12980ms step_avg:60.65ms
step:215/2285 train_time:13042ms step_avg:60.66ms
step:216/2285 train_time:13101ms step_avg:60.65ms
step:217/2285 train_time:13162ms step_avg:60.66ms
step:218/2285 train_time:13222ms step_avg:60.65ms
step:219/2285 train_time:13284ms step_avg:60.66ms
step:220/2285 train_time:13342ms step_avg:60.65ms
step:221/2285 train_time:13403ms step_avg:60.65ms
step:222/2285 train_time:13463ms step_avg:60.64ms
step:223/2285 train_time:13524ms step_avg:60.65ms
step:224/2285 train_time:13584ms step_avg:60.64ms
step:225/2285 train_time:13645ms step_avg:60.64ms
step:226/2285 train_time:13704ms step_avg:60.64ms
step:227/2285 train_time:13765ms step_avg:60.64ms
step:228/2285 train_time:13824ms step_avg:60.63ms
step:229/2285 train_time:13886ms step_avg:60.64ms
step:230/2285 train_time:13944ms step_avg:60.63ms
step:231/2285 train_time:14006ms step_avg:60.63ms
step:232/2285 train_time:14065ms step_avg:60.62ms
step:233/2285 train_time:14126ms step_avg:60.63ms
step:234/2285 train_time:14185ms step_avg:60.62ms
step:235/2285 train_time:14247ms step_avg:60.63ms
step:236/2285 train_time:14306ms step_avg:60.62ms
step:237/2285 train_time:14367ms step_avg:60.62ms
step:238/2285 train_time:14426ms step_avg:60.61ms
step:239/2285 train_time:14487ms step_avg:60.62ms
step:240/2285 train_time:14546ms step_avg:60.61ms
step:241/2285 train_time:14608ms step_avg:60.61ms
step:242/2285 train_time:14666ms step_avg:60.60ms
step:243/2285 train_time:14727ms step_avg:60.61ms
step:244/2285 train_time:14786ms step_avg:60.60ms
step:245/2285 train_time:14848ms step_avg:60.61ms
step:246/2285 train_time:14907ms step_avg:60.60ms
step:247/2285 train_time:14969ms step_avg:60.60ms
step:248/2285 train_time:15028ms step_avg:60.60ms
step:249/2285 train_time:15090ms step_avg:60.60ms
step:250/2285 train_time:15149ms step_avg:60.60ms
step:250/2285 val_loss:4.0691 train_time:15212ms step_avg:60.85ms
step:251/2285 train_time:15233ms step_avg:60.69ms
step:252/2285 train_time:15271ms step_avg:60.60ms
step:253/2285 train_time:15336ms step_avg:60.62ms
step:254/2285 train_time:15401ms step_avg:60.63ms
step:255/2285 train_time:15464ms step_avg:60.64ms
step:256/2285 train_time:15523ms step_avg:60.64ms
step:257/2285 train_time:15584ms step_avg:60.64ms
step:258/2285 train_time:15642ms step_avg:60.63ms
step:259/2285 train_time:15703ms step_avg:60.63ms
step:260/2285 train_time:15761ms step_avg:60.62ms
step:261/2285 train_time:15822ms step_avg:60.62ms
step:262/2285 train_time:15880ms step_avg:60.61ms
step:263/2285 train_time:15941ms step_avg:60.61ms
step:264/2285 train_time:15999ms step_avg:60.60ms
step:265/2285 train_time:16060ms step_avg:60.61ms
step:266/2285 train_time:16119ms step_avg:60.60ms
step:267/2285 train_time:16181ms step_avg:60.60ms
step:268/2285 train_time:16240ms step_avg:60.60ms
step:269/2285 train_time:16304ms step_avg:60.61ms
step:270/2285 train_time:16365ms step_avg:60.61ms
step:271/2285 train_time:16427ms step_avg:60.62ms
step:272/2285 train_time:16487ms step_avg:60.61ms
step:273/2285 train_time:16548ms step_avg:60.62ms
step:274/2285 train_time:16608ms step_avg:60.61ms
step:275/2285 train_time:16668ms step_avg:60.61ms
step:276/2285 train_time:16727ms step_avg:60.60ms
step:277/2285 train_time:16787ms step_avg:60.60ms
step:278/2285 train_time:16846ms step_avg:60.60ms
step:279/2285 train_time:16907ms step_avg:60.60ms
step:280/2285 train_time:16966ms step_avg:60.59ms
step:281/2285 train_time:17027ms step_avg:60.59ms
step:282/2285 train_time:17085ms step_avg:60.59ms
step:283/2285 train_time:17146ms step_avg:60.59ms
step:284/2285 train_time:17207ms step_avg:60.59ms
step:285/2285 train_time:17269ms step_avg:60.59ms
step:286/2285 train_time:17329ms step_avg:60.59ms
step:287/2285 train_time:17391ms step_avg:60.60ms
step:288/2285 train_time:17451ms step_avg:60.59ms
step:289/2285 train_time:17513ms step_avg:60.60ms
step:290/2285 train_time:17572ms step_avg:60.59ms
step:291/2285 train_time:17633ms step_avg:60.59ms
step:292/2285 train_time:17691ms step_avg:60.59ms
step:293/2285 train_time:17753ms step_avg:60.59ms
step:294/2285 train_time:17811ms step_avg:60.58ms
step:295/2285 train_time:17872ms step_avg:60.58ms
step:296/2285 train_time:17930ms step_avg:60.58ms
step:297/2285 train_time:17992ms step_avg:60.58ms
step:298/2285 train_time:18051ms step_avg:60.57ms
step:299/2285 train_time:18112ms step_avg:60.57ms
step:300/2285 train_time:18171ms step_avg:60.57ms
step:301/2285 train_time:18232ms step_avg:60.57ms
step:302/2285 train_time:18292ms step_avg:60.57ms
step:303/2285 train_time:18353ms step_avg:60.57ms
step:304/2285 train_time:18412ms step_avg:60.57ms
step:305/2285 train_time:18474ms step_avg:60.57ms
step:306/2285 train_time:18532ms step_avg:60.56ms
step:307/2285 train_time:18594ms step_avg:60.57ms
step:308/2285 train_time:18653ms step_avg:60.56ms
step:309/2285 train_time:18713ms step_avg:60.56ms
step:310/2285 train_time:18772ms step_avg:60.56ms
step:311/2285 train_time:18834ms step_avg:60.56ms
step:312/2285 train_time:18892ms step_avg:60.55ms
step:313/2285 train_time:18954ms step_avg:60.56ms
step:314/2285 train_time:19013ms step_avg:60.55ms
step:315/2285 train_time:19074ms step_avg:60.55ms
step:316/2285 train_time:19133ms step_avg:60.55ms
step:317/2285 train_time:19196ms step_avg:60.55ms
step:318/2285 train_time:19255ms step_avg:60.55ms
step:319/2285 train_time:19316ms step_avg:60.55ms
step:320/2285 train_time:19376ms step_avg:60.55ms
step:321/2285 train_time:19437ms step_avg:60.55ms
step:322/2285 train_time:19496ms step_avg:60.55ms
step:323/2285 train_time:19558ms step_avg:60.55ms
step:324/2285 train_time:19617ms step_avg:60.54ms
step:325/2285 train_time:19678ms step_avg:60.55ms
step:326/2285 train_time:19736ms step_avg:60.54ms
step:327/2285 train_time:19798ms step_avg:60.54ms
step:328/2285 train_time:19857ms step_avg:60.54ms
step:329/2285 train_time:19919ms step_avg:60.54ms
step:330/2285 train_time:19977ms step_avg:60.54ms
step:331/2285 train_time:20039ms step_avg:60.54ms
step:332/2285 train_time:20098ms step_avg:60.54ms
step:333/2285 train_time:20161ms step_avg:60.54ms
step:334/2285 train_time:20221ms step_avg:60.54ms
step:335/2285 train_time:20283ms step_avg:60.55ms
step:336/2285 train_time:20342ms step_avg:60.54ms
step:337/2285 train_time:20403ms step_avg:60.54ms
step:338/2285 train_time:20462ms step_avg:60.54ms
step:339/2285 train_time:20523ms step_avg:60.54ms
step:340/2285 train_time:20583ms step_avg:60.54ms
step:341/2285 train_time:20643ms step_avg:60.54ms
step:342/2285 train_time:20702ms step_avg:60.53ms
step:343/2285 train_time:20764ms step_avg:60.54ms
step:344/2285 train_time:20823ms step_avg:60.53ms
step:345/2285 train_time:20885ms step_avg:60.54ms
step:346/2285 train_time:20945ms step_avg:60.53ms
step:347/2285 train_time:21006ms step_avg:60.54ms
step:348/2285 train_time:21066ms step_avg:60.53ms
step:349/2285 train_time:21127ms step_avg:60.54ms
step:350/2285 train_time:21186ms step_avg:60.53ms
step:351/2285 train_time:21247ms step_avg:60.53ms
step:352/2285 train_time:21306ms step_avg:60.53ms
step:353/2285 train_time:21368ms step_avg:60.53ms
step:354/2285 train_time:21427ms step_avg:60.53ms
step:355/2285 train_time:21489ms step_avg:60.53ms
step:356/2285 train_time:21548ms step_avg:60.53ms
step:357/2285 train_time:21609ms step_avg:60.53ms
step:358/2285 train_time:21668ms step_avg:60.53ms
step:359/2285 train_time:21729ms step_avg:60.53ms
step:360/2285 train_time:21789ms step_avg:60.52ms
step:361/2285 train_time:21852ms step_avg:60.53ms
step:362/2285 train_time:21910ms step_avg:60.53ms
step:363/2285 train_time:21972ms step_avg:60.53ms
step:364/2285 train_time:22030ms step_avg:60.52ms
step:365/2285 train_time:22091ms step_avg:60.52ms
step:366/2285 train_time:22150ms step_avg:60.52ms
step:367/2285 train_time:22212ms step_avg:60.52ms
step:368/2285 train_time:22271ms step_avg:60.52ms
step:369/2285 train_time:22332ms step_avg:60.52ms
step:370/2285 train_time:22390ms step_avg:60.51ms
step:371/2285 train_time:22452ms step_avg:60.52ms
step:372/2285 train_time:22511ms step_avg:60.51ms
step:373/2285 train_time:22572ms step_avg:60.52ms
step:374/2285 train_time:22631ms step_avg:60.51ms
step:375/2285 train_time:22693ms step_avg:60.51ms
step:376/2285 train_time:22752ms step_avg:60.51ms
step:377/2285 train_time:22814ms step_avg:60.51ms
step:378/2285 train_time:22872ms step_avg:60.51ms
step:379/2285 train_time:22934ms step_avg:60.51ms
step:380/2285 train_time:22994ms step_avg:60.51ms
step:381/2285 train_time:23055ms step_avg:60.51ms
step:382/2285 train_time:23114ms step_avg:60.51ms
step:383/2285 train_time:23176ms step_avg:60.51ms
step:384/2285 train_time:23236ms step_avg:60.51ms
step:385/2285 train_time:23297ms step_avg:60.51ms
step:386/2285 train_time:23357ms step_avg:60.51ms
step:387/2285 train_time:23419ms step_avg:60.51ms
step:388/2285 train_time:23478ms step_avg:60.51ms
step:389/2285 train_time:23540ms step_avg:60.51ms
step:390/2285 train_time:23600ms step_avg:60.51ms
step:391/2285 train_time:23661ms step_avg:60.51ms
step:392/2285 train_time:23720ms step_avg:60.51ms
step:393/2285 train_time:23781ms step_avg:60.51ms
step:394/2285 train_time:23840ms step_avg:60.51ms
step:395/2285 train_time:23902ms step_avg:60.51ms
step:396/2285 train_time:23961ms step_avg:60.51ms
step:397/2285 train_time:24023ms step_avg:60.51ms
step:398/2285 train_time:24082ms step_avg:60.51ms
step:399/2285 train_time:24144ms step_avg:60.51ms
step:400/2285 train_time:24203ms step_avg:60.51ms
step:401/2285 train_time:24265ms step_avg:60.51ms
step:402/2285 train_time:24324ms step_avg:60.51ms
step:403/2285 train_time:24386ms step_avg:60.51ms
step:404/2285 train_time:24444ms step_avg:60.51ms
step:405/2285 train_time:24506ms step_avg:60.51ms
step:406/2285 train_time:24565ms step_avg:60.51ms
step:407/2285 train_time:24626ms step_avg:60.51ms
step:408/2285 train_time:24685ms step_avg:60.50ms
step:409/2285 train_time:24747ms step_avg:60.51ms
step:410/2285 train_time:24806ms step_avg:60.50ms
step:411/2285 train_time:24868ms step_avg:60.51ms
step:412/2285 train_time:24927ms step_avg:60.50ms
step:413/2285 train_time:24988ms step_avg:60.50ms
step:414/2285 train_time:25047ms step_avg:60.50ms
step:415/2285 train_time:25109ms step_avg:60.50ms
step:416/2285 train_time:25168ms step_avg:60.50ms
step:417/2285 train_time:25229ms step_avg:60.50ms
step:418/2285 train_time:25289ms step_avg:60.50ms
step:419/2285 train_time:25350ms step_avg:60.50ms
step:420/2285 train_time:25410ms step_avg:60.50ms
step:421/2285 train_time:25471ms step_avg:60.50ms
step:422/2285 train_time:25530ms step_avg:60.50ms
step:423/2285 train_time:25591ms step_avg:60.50ms
step:424/2285 train_time:25650ms step_avg:60.49ms
step:425/2285 train_time:25711ms step_avg:60.50ms
step:426/2285 train_time:25770ms step_avg:60.49ms
step:427/2285 train_time:25832ms step_avg:60.50ms
step:428/2285 train_time:25891ms step_avg:60.49ms
step:429/2285 train_time:25952ms step_avg:60.49ms
step:430/2285 train_time:26011ms step_avg:60.49ms
step:431/2285 train_time:26073ms step_avg:60.49ms
step:432/2285 train_time:26131ms step_avg:60.49ms
step:433/2285 train_time:26193ms step_avg:60.49ms
step:434/2285 train_time:26252ms step_avg:60.49ms
step:435/2285 train_time:26313ms step_avg:60.49ms
step:436/2285 train_time:26373ms step_avg:60.49ms
step:437/2285 train_time:26434ms step_avg:60.49ms
step:438/2285 train_time:26492ms step_avg:60.49ms
step:439/2285 train_time:26554ms step_avg:60.49ms
step:440/2285 train_time:26612ms step_avg:60.48ms
step:441/2285 train_time:26674ms step_avg:60.48ms
step:442/2285 train_time:26732ms step_avg:60.48ms
step:443/2285 train_time:26794ms step_avg:60.48ms
step:444/2285 train_time:26853ms step_avg:60.48ms
step:445/2285 train_time:26914ms step_avg:60.48ms
step:446/2285 train_time:26973ms step_avg:60.48ms
step:447/2285 train_time:27034ms step_avg:60.48ms
step:448/2285 train_time:27093ms step_avg:60.47ms
step:449/2285 train_time:27154ms step_avg:60.48ms
step:450/2285 train_time:27213ms step_avg:60.47ms
step:451/2285 train_time:27275ms step_avg:60.48ms
step:452/2285 train_time:27333ms step_avg:60.47ms
step:453/2285 train_time:27395ms step_avg:60.47ms
step:454/2285 train_time:27453ms step_avg:60.47ms
step:455/2285 train_time:27515ms step_avg:60.47ms
step:456/2285 train_time:27574ms step_avg:60.47ms
step:457/2285 train_time:27635ms step_avg:60.47ms
step:458/2285 train_time:27694ms step_avg:60.47ms
step:459/2285 train_time:27755ms step_avg:60.47ms
step:460/2285 train_time:27814ms step_avg:60.47ms
step:461/2285 train_time:27876ms step_avg:60.47ms
step:462/2285 train_time:27935ms step_avg:60.46ms
step:463/2285 train_time:27996ms step_avg:60.47ms
step:464/2285 train_time:28055ms step_avg:60.46ms
step:465/2285 train_time:28116ms step_avg:60.47ms
step:466/2285 train_time:28175ms step_avg:60.46ms
step:467/2285 train_time:28237ms step_avg:60.46ms
step:468/2285 train_time:28296ms step_avg:60.46ms
step:469/2285 train_time:28357ms step_avg:60.46ms
step:470/2285 train_time:28416ms step_avg:60.46ms
step:471/2285 train_time:28477ms step_avg:60.46ms
step:472/2285 train_time:28536ms step_avg:60.46ms
step:473/2285 train_time:28598ms step_avg:60.46ms
step:474/2285 train_time:28656ms step_avg:60.46ms
step:475/2285 train_time:28717ms step_avg:60.46ms
step:476/2285 train_time:28776ms step_avg:60.45ms
step:477/2285 train_time:28839ms step_avg:60.46ms
step:478/2285 train_time:28897ms step_avg:60.45ms
step:479/2285 train_time:28958ms step_avg:60.45ms
step:480/2285 train_time:29017ms step_avg:60.45ms
step:481/2285 train_time:29079ms step_avg:60.45ms
step:482/2285 train_time:29138ms step_avg:60.45ms
step:483/2285 train_time:29199ms step_avg:60.45ms
step:484/2285 train_time:29259ms step_avg:60.45ms
step:485/2285 train_time:29321ms step_avg:60.45ms
step:486/2285 train_time:29380ms step_avg:60.45ms
step:487/2285 train_time:29442ms step_avg:60.46ms
step:488/2285 train_time:29500ms step_avg:60.45ms
step:489/2285 train_time:29561ms step_avg:60.45ms
step:490/2285 train_time:29620ms step_avg:60.45ms
step:491/2285 train_time:29682ms step_avg:60.45ms
step:492/2285 train_time:29741ms step_avg:60.45ms
step:493/2285 train_time:29803ms step_avg:60.45ms
step:494/2285 train_time:29862ms step_avg:60.45ms
step:495/2285 train_time:29924ms step_avg:60.45ms
step:496/2285 train_time:29983ms step_avg:60.45ms
step:497/2285 train_time:30044ms step_avg:60.45ms
step:498/2285 train_time:30104ms step_avg:60.45ms
step:499/2285 train_time:30165ms step_avg:60.45ms
step:500/2285 train_time:30225ms step_avg:60.45ms
step:500/2285 val_loss:3.8107 train_time:30287ms step_avg:60.57ms
step:501/2285 train_time:30310ms step_avg:60.50ms
step:502/2285 train_time:30347ms step_avg:60.45ms
step:503/2285 train_time:30410ms step_avg:60.46ms
step:504/2285 train_time:30473ms step_avg:60.46ms
step:505/2285 train_time:30534ms step_avg:60.46ms
step:506/2285 train_time:30593ms step_avg:60.46ms
step:507/2285 train_time:30654ms step_avg:60.46ms
step:508/2285 train_time:30713ms step_avg:60.46ms
step:509/2285 train_time:30774ms step_avg:60.46ms
step:510/2285 train_time:30833ms step_avg:60.46ms
step:511/2285 train_time:30894ms step_avg:60.46ms
step:512/2285 train_time:30952ms step_avg:60.45ms
step:513/2285 train_time:31014ms step_avg:60.46ms
step:514/2285 train_time:31073ms step_avg:60.45ms
step:515/2285 train_time:31134ms step_avg:60.45ms
step:516/2285 train_time:31193ms step_avg:60.45ms
step:517/2285 train_time:31255ms step_avg:60.45ms
step:518/2285 train_time:31316ms step_avg:60.46ms
step:519/2285 train_time:31379ms step_avg:60.46ms
step:520/2285 train_time:31439ms step_avg:60.46ms
step:521/2285 train_time:31502ms step_avg:60.46ms
step:522/2285 train_time:31560ms step_avg:60.46ms
step:523/2285 train_time:31622ms step_avg:60.46ms
step:524/2285 train_time:31680ms step_avg:60.46ms
step:525/2285 train_time:31741ms step_avg:60.46ms
step:526/2285 train_time:31800ms step_avg:60.46ms
step:527/2285 train_time:31861ms step_avg:60.46ms
step:528/2285 train_time:31919ms step_avg:60.45ms
step:529/2285 train_time:31980ms step_avg:60.45ms
step:530/2285 train_time:32039ms step_avg:60.45ms
step:531/2285 train_time:32100ms step_avg:60.45ms
step:532/2285 train_time:32158ms step_avg:60.45ms
step:533/2285 train_time:32220ms step_avg:60.45ms
step:534/2285 train_time:32279ms step_avg:60.45ms
step:535/2285 train_time:32342ms step_avg:60.45ms
step:536/2285 train_time:32402ms step_avg:60.45ms
step:537/2285 train_time:32464ms step_avg:60.45ms
step:538/2285 train_time:32523ms step_avg:60.45ms
step:539/2285 train_time:32584ms step_avg:60.45ms
step:540/2285 train_time:32642ms step_avg:60.45ms
step:541/2285 train_time:32704ms step_avg:60.45ms
step:542/2285 train_time:32763ms step_avg:60.45ms
step:543/2285 train_time:32824ms step_avg:60.45ms
step:544/2285 train_time:32883ms step_avg:60.45ms
step:545/2285 train_time:32944ms step_avg:60.45ms
step:546/2285 train_time:33003ms step_avg:60.44ms
step:547/2285 train_time:33064ms step_avg:60.45ms
step:548/2285 train_time:33123ms step_avg:60.44ms
step:549/2285 train_time:33185ms step_avg:60.45ms
step:550/2285 train_time:33244ms step_avg:60.44ms
step:551/2285 train_time:33306ms step_avg:60.45ms
step:552/2285 train_time:33365ms step_avg:60.44ms
step:553/2285 train_time:33426ms step_avg:60.45ms
step:554/2285 train_time:33485ms step_avg:60.44ms
step:555/2285 train_time:33547ms step_avg:60.44ms
step:556/2285 train_time:33605ms step_avg:60.44ms
step:557/2285 train_time:33667ms step_avg:60.44ms
step:558/2285 train_time:33725ms step_avg:60.44ms
step:559/2285 train_time:33786ms step_avg:60.44ms
step:560/2285 train_time:33845ms step_avg:60.44ms
step:561/2285 train_time:33907ms step_avg:60.44ms
step:562/2285 train_time:33966ms step_avg:60.44ms
step:563/2285 train_time:34028ms step_avg:60.44ms
step:564/2285 train_time:34087ms step_avg:60.44ms
step:565/2285 train_time:34148ms step_avg:60.44ms
step:566/2285 train_time:34207ms step_avg:60.44ms
step:567/2285 train_time:34268ms step_avg:60.44ms
step:568/2285 train_time:34327ms step_avg:60.43ms
step:569/2285 train_time:34388ms step_avg:60.44ms
step:570/2285 train_time:34447ms step_avg:60.43ms
step:571/2285 train_time:34509ms step_avg:60.44ms
step:572/2285 train_time:34568ms step_avg:60.43ms
step:573/2285 train_time:34630ms step_avg:60.44ms
step:574/2285 train_time:34688ms step_avg:60.43ms
step:575/2285 train_time:34750ms step_avg:60.43ms
step:576/2285 train_time:34809ms step_avg:60.43ms
step:577/2285 train_time:34871ms step_avg:60.43ms
step:578/2285 train_time:34930ms step_avg:60.43ms
step:579/2285 train_time:34991ms step_avg:60.43ms
step:580/2285 train_time:35051ms step_avg:60.43ms
step:581/2285 train_time:35113ms step_avg:60.44ms
step:582/2285 train_time:35172ms step_avg:60.43ms
step:583/2285 train_time:35233ms step_avg:60.43ms
step:584/2285 train_time:35292ms step_avg:60.43ms
step:585/2285 train_time:35354ms step_avg:60.43ms
step:586/2285 train_time:35413ms step_avg:60.43ms
step:587/2285 train_time:35475ms step_avg:60.43ms
step:588/2285 train_time:35535ms step_avg:60.43ms
step:589/2285 train_time:35597ms step_avg:60.44ms
step:590/2285 train_time:35657ms step_avg:60.44ms
step:591/2285 train_time:35719ms step_avg:60.44ms
step:592/2285 train_time:35778ms step_avg:60.44ms
step:593/2285 train_time:35840ms step_avg:60.44ms
step:594/2285 train_time:35899ms step_avg:60.44ms
step:595/2285 train_time:35960ms step_avg:60.44ms
step:596/2285 train_time:36020ms step_avg:60.44ms
step:597/2285 train_time:36081ms step_avg:60.44ms
step:598/2285 train_time:36140ms step_avg:60.43ms
step:599/2285 train_time:36201ms step_avg:60.44ms
step:600/2285 train_time:36261ms step_avg:60.43ms
step:601/2285 train_time:36322ms step_avg:60.44ms
step:602/2285 train_time:36380ms step_avg:60.43ms
step:603/2285 train_time:36442ms step_avg:60.43ms
step:604/2285 train_time:36501ms step_avg:60.43ms
step:605/2285 train_time:36563ms step_avg:60.43ms
step:606/2285 train_time:36622ms step_avg:60.43ms
step:607/2285 train_time:36685ms step_avg:60.44ms
step:608/2285 train_time:36744ms step_avg:60.43ms
step:609/2285 train_time:36805ms step_avg:60.44ms
step:610/2285 train_time:36864ms step_avg:60.43ms
step:611/2285 train_time:36925ms step_avg:60.43ms
step:612/2285 train_time:36984ms step_avg:60.43ms
step:613/2285 train_time:37045ms step_avg:60.43ms
step:614/2285 train_time:37104ms step_avg:60.43ms
step:615/2285 train_time:37166ms step_avg:60.43ms
step:616/2285 train_time:37225ms step_avg:60.43ms
step:617/2285 train_time:37286ms step_avg:60.43ms
step:618/2285 train_time:37345ms step_avg:60.43ms
step:619/2285 train_time:37406ms step_avg:60.43ms
step:620/2285 train_time:37466ms step_avg:60.43ms
step:621/2285 train_time:37527ms step_avg:60.43ms
step:622/2285 train_time:37586ms step_avg:60.43ms
step:623/2285 train_time:37648ms step_avg:60.43ms
step:624/2285 train_time:37708ms step_avg:60.43ms
step:625/2285 train_time:37769ms step_avg:60.43ms
step:626/2285 train_time:37828ms step_avg:60.43ms
step:627/2285 train_time:37890ms step_avg:60.43ms
step:628/2285 train_time:37949ms step_avg:60.43ms
step:629/2285 train_time:38010ms step_avg:60.43ms
step:630/2285 train_time:38069ms step_avg:60.43ms
step:631/2285 train_time:38131ms step_avg:60.43ms
step:632/2285 train_time:38190ms step_avg:60.43ms
step:633/2285 train_time:38252ms step_avg:60.43ms
step:634/2285 train_time:38311ms step_avg:60.43ms
step:635/2285 train_time:38373ms step_avg:60.43ms
step:636/2285 train_time:38432ms step_avg:60.43ms
step:637/2285 train_time:38494ms step_avg:60.43ms
step:638/2285 train_time:38554ms step_avg:60.43ms
step:639/2285 train_time:38616ms step_avg:60.43ms
step:640/2285 train_time:38675ms step_avg:60.43ms
step:641/2285 train_time:38737ms step_avg:60.43ms
step:642/2285 train_time:38797ms step_avg:60.43ms
step:643/2285 train_time:38858ms step_avg:60.43ms
step:644/2285 train_time:38918ms step_avg:60.43ms
step:645/2285 train_time:38979ms step_avg:60.43ms
step:646/2285 train_time:39037ms step_avg:60.43ms
step:647/2285 train_time:39099ms step_avg:60.43ms
step:648/2285 train_time:39158ms step_avg:60.43ms
step:649/2285 train_time:39220ms step_avg:60.43ms
step:650/2285 train_time:39279ms step_avg:60.43ms
step:651/2285 train_time:39341ms step_avg:60.43ms
step:652/2285 train_time:39400ms step_avg:60.43ms
step:653/2285 train_time:39461ms step_avg:60.43ms
step:654/2285 train_time:39520ms step_avg:60.43ms
step:655/2285 train_time:39582ms step_avg:60.43ms
step:656/2285 train_time:39641ms step_avg:60.43ms
step:657/2285 train_time:39703ms step_avg:60.43ms
step:658/2285 train_time:39761ms step_avg:60.43ms
step:659/2285 train_time:39823ms step_avg:60.43ms
step:660/2285 train_time:39882ms step_avg:60.43ms
step:661/2285 train_time:39943ms step_avg:60.43ms
step:662/2285 train_time:40002ms step_avg:60.43ms
step:663/2285 train_time:40064ms step_avg:60.43ms
step:664/2285 train_time:40123ms step_avg:60.43ms
step:665/2285 train_time:40185ms step_avg:60.43ms
step:666/2285 train_time:40244ms step_avg:60.43ms
step:667/2285 train_time:40305ms step_avg:60.43ms
step:668/2285 train_time:40364ms step_avg:60.42ms
step:669/2285 train_time:40425ms step_avg:60.43ms
step:670/2285 train_time:40484ms step_avg:60.42ms
step:671/2285 train_time:40545ms step_avg:60.42ms
step:672/2285 train_time:40604ms step_avg:60.42ms
step:673/2285 train_time:40666ms step_avg:60.42ms
step:674/2285 train_time:40725ms step_avg:60.42ms
step:675/2285 train_time:40786ms step_avg:60.42ms
step:676/2285 train_time:40845ms step_avg:60.42ms
step:677/2285 train_time:40908ms step_avg:60.43ms
step:678/2285 train_time:40967ms step_avg:60.42ms
step:679/2285 train_time:41028ms step_avg:60.42ms
step:680/2285 train_time:41087ms step_avg:60.42ms
step:681/2285 train_time:41149ms step_avg:60.42ms
step:682/2285 train_time:41208ms step_avg:60.42ms
step:683/2285 train_time:41270ms step_avg:60.42ms
step:684/2285 train_time:41328ms step_avg:60.42ms
step:685/2285 train_time:41390ms step_avg:60.42ms
step:686/2285 train_time:41449ms step_avg:60.42ms
step:687/2285 train_time:41510ms step_avg:60.42ms
step:688/2285 train_time:41569ms step_avg:60.42ms
step:689/2285 train_time:41631ms step_avg:60.42ms
step:690/2285 train_time:41690ms step_avg:60.42ms
step:691/2285 train_time:41752ms step_avg:60.42ms
step:692/2285 train_time:41811ms step_avg:60.42ms
step:693/2285 train_time:41873ms step_avg:60.42ms
step:694/2285 train_time:41933ms step_avg:60.42ms
step:695/2285 train_time:41994ms step_avg:60.42ms
step:696/2285 train_time:42054ms step_avg:60.42ms
step:697/2285 train_time:42117ms step_avg:60.43ms
step:698/2285 train_time:42176ms step_avg:60.42ms
step:699/2285 train_time:42238ms step_avg:60.43ms
step:700/2285 train_time:42297ms step_avg:60.42ms
step:701/2285 train_time:42358ms step_avg:60.43ms
step:702/2285 train_time:42418ms step_avg:60.42ms
step:703/2285 train_time:42479ms step_avg:60.43ms
step:704/2285 train_time:42539ms step_avg:60.43ms
step:705/2285 train_time:42601ms step_avg:60.43ms
step:706/2285 train_time:42660ms step_avg:60.42ms
step:707/2285 train_time:42721ms step_avg:60.43ms
step:708/2285 train_time:42780ms step_avg:60.42ms
step:709/2285 train_time:42842ms step_avg:60.43ms
step:710/2285 train_time:42901ms step_avg:60.42ms
step:711/2285 train_time:42963ms step_avg:60.43ms
step:712/2285 train_time:43022ms step_avg:60.42ms
step:713/2285 train_time:43084ms step_avg:60.43ms
step:714/2285 train_time:43143ms step_avg:60.42ms
step:715/2285 train_time:43204ms step_avg:60.43ms
step:716/2285 train_time:43263ms step_avg:60.42ms
step:717/2285 train_time:43325ms step_avg:60.43ms
step:718/2285 train_time:43384ms step_avg:60.42ms
step:719/2285 train_time:43445ms step_avg:60.42ms
step:720/2285 train_time:43504ms step_avg:60.42ms
step:721/2285 train_time:43566ms step_avg:60.42ms
step:722/2285 train_time:43624ms step_avg:60.42ms
step:723/2285 train_time:43686ms step_avg:60.42ms
step:724/2285 train_time:43745ms step_avg:60.42ms
step:725/2285 train_time:43807ms step_avg:60.42ms
step:726/2285 train_time:43865ms step_avg:60.42ms
step:727/2285 train_time:43927ms step_avg:60.42ms
step:728/2285 train_time:43987ms step_avg:60.42ms
step:729/2285 train_time:44048ms step_avg:60.42ms
step:730/2285 train_time:44107ms step_avg:60.42ms
step:731/2285 train_time:44169ms step_avg:60.42ms
step:732/2285 train_time:44228ms step_avg:60.42ms
step:733/2285 train_time:44289ms step_avg:60.42ms
step:734/2285 train_time:44348ms step_avg:60.42ms
step:735/2285 train_time:44410ms step_avg:60.42ms
step:736/2285 train_time:44469ms step_avg:60.42ms
step:737/2285 train_time:44531ms step_avg:60.42ms
step:738/2285 train_time:44590ms step_avg:60.42ms
step:739/2285 train_time:44652ms step_avg:60.42ms
step:740/2285 train_time:44711ms step_avg:60.42ms
step:741/2285 train_time:44772ms step_avg:60.42ms
step:742/2285 train_time:44832ms step_avg:60.42ms
step:743/2285 train_time:44894ms step_avg:60.42ms
step:744/2285 train_time:44954ms step_avg:60.42ms
step:745/2285 train_time:45016ms step_avg:60.42ms
step:746/2285 train_time:45075ms step_avg:60.42ms
step:747/2285 train_time:45137ms step_avg:60.42ms
step:748/2285 train_time:45197ms step_avg:60.42ms
step:749/2285 train_time:45259ms step_avg:60.43ms
step:750/2285 train_time:45318ms step_avg:60.42ms
step:750/2285 val_loss:3.6704 train_time:45381ms step_avg:60.51ms
step:751/2285 train_time:45402ms step_avg:60.46ms
step:752/2285 train_time:45441ms step_avg:60.43ms
step:753/2285 train_time:45502ms step_avg:60.43ms
step:754/2285 train_time:45562ms step_avg:60.43ms
step:755/2285 train_time:45624ms step_avg:60.43ms
step:756/2285 train_time:45684ms step_avg:60.43ms
step:757/2285 train_time:45745ms step_avg:60.43ms
step:758/2285 train_time:45804ms step_avg:60.43ms
step:759/2285 train_time:45866ms step_avg:60.43ms
step:760/2285 train_time:45925ms step_avg:60.43ms
step:761/2285 train_time:45986ms step_avg:60.43ms
step:762/2285 train_time:46046ms step_avg:60.43ms
step:763/2285 train_time:46108ms step_avg:60.43ms
step:764/2285 train_time:46167ms step_avg:60.43ms
step:765/2285 train_time:46228ms step_avg:60.43ms
step:766/2285 train_time:46294ms step_avg:60.44ms
step:767/2285 train_time:46360ms step_avg:60.44ms
step:768/2285 train_time:46421ms step_avg:60.44ms
step:769/2285 train_time:46483ms step_avg:60.45ms
step:770/2285 train_time:46543ms step_avg:60.45ms
step:771/2285 train_time:46605ms step_avg:60.45ms
step:772/2285 train_time:46665ms step_avg:60.45ms
step:773/2285 train_time:46726ms step_avg:60.45ms
step:774/2285 train_time:46786ms step_avg:60.45ms
step:775/2285 train_time:46847ms step_avg:60.45ms
step:776/2285 train_time:46907ms step_avg:60.45ms
step:777/2285 train_time:46968ms step_avg:60.45ms
step:778/2285 train_time:47028ms step_avg:60.45ms
step:779/2285 train_time:47089ms step_avg:60.45ms
step:780/2285 train_time:47150ms step_avg:60.45ms
step:781/2285 train_time:47214ms step_avg:60.45ms
step:782/2285 train_time:47275ms step_avg:60.45ms
step:783/2285 train_time:47338ms step_avg:60.46ms
step:784/2285 train_time:47398ms step_avg:60.46ms
step:785/2285 train_time:47461ms step_avg:60.46ms
step:786/2285 train_time:47521ms step_avg:60.46ms
step:787/2285 train_time:47583ms step_avg:60.46ms
step:788/2285 train_time:47643ms step_avg:60.46ms
step:789/2285 train_time:47705ms step_avg:60.46ms
step:790/2285 train_time:47765ms step_avg:60.46ms
step:791/2285 train_time:47826ms step_avg:60.46ms
step:792/2285 train_time:47886ms step_avg:60.46ms
step:793/2285 train_time:47947ms step_avg:60.46ms
step:794/2285 train_time:48007ms step_avg:60.46ms
step:795/2285 train_time:48069ms step_avg:60.46ms
step:796/2285 train_time:48129ms step_avg:60.46ms
step:797/2285 train_time:48191ms step_avg:60.47ms
step:798/2285 train_time:48252ms step_avg:60.47ms
step:799/2285 train_time:48315ms step_avg:60.47ms
step:800/2285 train_time:48376ms step_avg:60.47ms
step:801/2285 train_time:48439ms step_avg:60.47ms
step:802/2285 train_time:48498ms step_avg:60.47ms
step:803/2285 train_time:48560ms step_avg:60.47ms
step:804/2285 train_time:48620ms step_avg:60.47ms
step:805/2285 train_time:48683ms step_avg:60.48ms
step:806/2285 train_time:48742ms step_avg:60.47ms
step:807/2285 train_time:48804ms step_avg:60.48ms
step:808/2285 train_time:48864ms step_avg:60.47ms
step:809/2285 train_time:48926ms step_avg:60.48ms
step:810/2285 train_time:48985ms step_avg:60.48ms
step:811/2285 train_time:49048ms step_avg:60.48ms
step:812/2285 train_time:49108ms step_avg:60.48ms
step:813/2285 train_time:49170ms step_avg:60.48ms
step:814/2285 train_time:49231ms step_avg:60.48ms
step:815/2285 train_time:49293ms step_avg:60.48ms
step:816/2285 train_time:49353ms step_avg:60.48ms
step:817/2285 train_time:49416ms step_avg:60.48ms
step:818/2285 train_time:49476ms step_avg:60.48ms
step:819/2285 train_time:49538ms step_avg:60.49ms
step:820/2285 train_time:49598ms step_avg:60.49ms
step:821/2285 train_time:49661ms step_avg:60.49ms
step:822/2285 train_time:49721ms step_avg:60.49ms
step:823/2285 train_time:49783ms step_avg:60.49ms
step:824/2285 train_time:49842ms step_avg:60.49ms
step:825/2285 train_time:49904ms step_avg:60.49ms
step:826/2285 train_time:49964ms step_avg:60.49ms
step:827/2285 train_time:50026ms step_avg:60.49ms
step:828/2285 train_time:50086ms step_avg:60.49ms
step:829/2285 train_time:50149ms step_avg:60.49ms
step:830/2285 train_time:50210ms step_avg:60.49ms
step:831/2285 train_time:50272ms step_avg:60.50ms
step:832/2285 train_time:50332ms step_avg:60.50ms
step:833/2285 train_time:50395ms step_avg:60.50ms
step:834/2285 train_time:50455ms step_avg:60.50ms
step:835/2285 train_time:50517ms step_avg:60.50ms
step:836/2285 train_time:50576ms step_avg:60.50ms
step:837/2285 train_time:50638ms step_avg:60.50ms
step:838/2285 train_time:50698ms step_avg:60.50ms
step:839/2285 train_time:50761ms step_avg:60.50ms
step:840/2285 train_time:50820ms step_avg:60.50ms
step:841/2285 train_time:50882ms step_avg:60.50ms
step:842/2285 train_time:50942ms step_avg:60.50ms
step:843/2285 train_time:51005ms step_avg:60.50ms
step:844/2285 train_time:51065ms step_avg:60.50ms
step:845/2285 train_time:51127ms step_avg:60.51ms
step:846/2285 train_time:51189ms step_avg:60.51ms
step:847/2285 train_time:51252ms step_avg:60.51ms
step:848/2285 train_time:51311ms step_avg:60.51ms
step:849/2285 train_time:51374ms step_avg:60.51ms
step:850/2285 train_time:51433ms step_avg:60.51ms
step:851/2285 train_time:51495ms step_avg:60.51ms
step:852/2285 train_time:51555ms step_avg:60.51ms
step:853/2285 train_time:51617ms step_avg:60.51ms
step:854/2285 train_time:51676ms step_avg:60.51ms
step:855/2285 train_time:51739ms step_avg:60.51ms
step:856/2285 train_time:51799ms step_avg:60.51ms
step:857/2285 train_time:51862ms step_avg:60.52ms
step:858/2285 train_time:51921ms step_avg:60.51ms
step:859/2285 train_time:51983ms step_avg:60.52ms
step:860/2285 train_time:52043ms step_avg:60.52ms
step:861/2285 train_time:52106ms step_avg:60.52ms
step:862/2285 train_time:52166ms step_avg:60.52ms
step:863/2285 train_time:52229ms step_avg:60.52ms
step:864/2285 train_time:52289ms step_avg:60.52ms
step:865/2285 train_time:52352ms step_avg:60.52ms
step:866/2285 train_time:52412ms step_avg:60.52ms
step:867/2285 train_time:52475ms step_avg:60.52ms
step:868/2285 train_time:52534ms step_avg:60.52ms
step:869/2285 train_time:52596ms step_avg:60.53ms
step:870/2285 train_time:52656ms step_avg:60.52ms
step:871/2285 train_time:52718ms step_avg:60.53ms
step:872/2285 train_time:52778ms step_avg:60.53ms
step:873/2285 train_time:52841ms step_avg:60.53ms
step:874/2285 train_time:52901ms step_avg:60.53ms
step:875/2285 train_time:52964ms step_avg:60.53ms
step:876/2285 train_time:53023ms step_avg:60.53ms
step:877/2285 train_time:53086ms step_avg:60.53ms
step:878/2285 train_time:53146ms step_avg:60.53ms
step:879/2285 train_time:53208ms step_avg:60.53ms
step:880/2285 train_time:53268ms step_avg:60.53ms
step:881/2285 train_time:53331ms step_avg:60.53ms
step:882/2285 train_time:53391ms step_avg:60.53ms
step:883/2285 train_time:53454ms step_avg:60.54ms
step:884/2285 train_time:53513ms step_avg:60.54ms
step:885/2285 train_time:53576ms step_avg:60.54ms
step:886/2285 train_time:53635ms step_avg:60.54ms
step:887/2285 train_time:53697ms step_avg:60.54ms
step:888/2285 train_time:53757ms step_avg:60.54ms
step:889/2285 train_time:53819ms step_avg:60.54ms
step:890/2285 train_time:53878ms step_avg:60.54ms
step:891/2285 train_time:53941ms step_avg:60.54ms
step:892/2285 train_time:54001ms step_avg:60.54ms
step:893/2285 train_time:54064ms step_avg:60.54ms
step:894/2285 train_time:54123ms step_avg:60.54ms
step:895/2285 train_time:54186ms step_avg:60.54ms
step:896/2285 train_time:54246ms step_avg:60.54ms
step:897/2285 train_time:54309ms step_avg:60.55ms
step:898/2285 train_time:54370ms step_avg:60.55ms
step:899/2285 train_time:54433ms step_avg:60.55ms
step:900/2285 train_time:54492ms step_avg:60.55ms
step:901/2285 train_time:54555ms step_avg:60.55ms
step:902/2285 train_time:54615ms step_avg:60.55ms
step:903/2285 train_time:54678ms step_avg:60.55ms
step:904/2285 train_time:54737ms step_avg:60.55ms
step:905/2285 train_time:54799ms step_avg:60.55ms
step:906/2285 train_time:54859ms step_avg:60.55ms
step:907/2285 train_time:54921ms step_avg:60.55ms
step:908/2285 train_time:54981ms step_avg:60.55ms
step:909/2285 train_time:55044ms step_avg:60.55ms
step:910/2285 train_time:55103ms step_avg:60.55ms
step:911/2285 train_time:55166ms step_avg:60.56ms
step:912/2285 train_time:55226ms step_avg:60.55ms
step:913/2285 train_time:55289ms step_avg:60.56ms
step:914/2285 train_time:55350ms step_avg:60.56ms
step:915/2285 train_time:55412ms step_avg:60.56ms
step:916/2285 train_time:55472ms step_avg:60.56ms
step:917/2285 train_time:55535ms step_avg:60.56ms
step:918/2285 train_time:55594ms step_avg:60.56ms
step:919/2285 train_time:55657ms step_avg:60.56ms
step:920/2285 train_time:55716ms step_avg:60.56ms
step:921/2285 train_time:55778ms step_avg:60.56ms
step:922/2285 train_time:55837ms step_avg:60.56ms
step:923/2285 train_time:55899ms step_avg:60.56ms
step:924/2285 train_time:55959ms step_avg:60.56ms
step:925/2285 train_time:56021ms step_avg:60.56ms
step:926/2285 train_time:56081ms step_avg:60.56ms
step:927/2285 train_time:56144ms step_avg:60.57ms
step:928/2285 train_time:56204ms step_avg:60.56ms
step:929/2285 train_time:56267ms step_avg:60.57ms
step:930/2285 train_time:56327ms step_avg:60.57ms
step:931/2285 train_time:56390ms step_avg:60.57ms
step:932/2285 train_time:56451ms step_avg:60.57ms
step:933/2285 train_time:56513ms step_avg:60.57ms
step:934/2285 train_time:56574ms step_avg:60.57ms
step:935/2285 train_time:56636ms step_avg:60.57ms
step:936/2285 train_time:56695ms step_avg:60.57ms
step:937/2285 train_time:56757ms step_avg:60.57ms
step:938/2285 train_time:56816ms step_avg:60.57ms
step:939/2285 train_time:56879ms step_avg:60.57ms
step:940/2285 train_time:56939ms step_avg:60.57ms
step:941/2285 train_time:57002ms step_avg:60.58ms
step:942/2285 train_time:57060ms step_avg:60.57ms
step:943/2285 train_time:57123ms step_avg:60.58ms
step:944/2285 train_time:57184ms step_avg:60.58ms
step:945/2285 train_time:57246ms step_avg:60.58ms
step:946/2285 train_time:57306ms step_avg:60.58ms
step:947/2285 train_time:57369ms step_avg:60.58ms
step:948/2285 train_time:57429ms step_avg:60.58ms
step:949/2285 train_time:57492ms step_avg:60.58ms
step:950/2285 train_time:57552ms step_avg:60.58ms
step:951/2285 train_time:57614ms step_avg:60.58ms
step:952/2285 train_time:57673ms step_avg:60.58ms
step:953/2285 train_time:57737ms step_avg:60.58ms
step:954/2285 train_time:57796ms step_avg:60.58ms
step:955/2285 train_time:57858ms step_avg:60.58ms
step:956/2285 train_time:57918ms step_avg:60.58ms
step:957/2285 train_time:57980ms step_avg:60.59ms
step:958/2285 train_time:58039ms step_avg:60.58ms
step:959/2285 train_time:58102ms step_avg:60.59ms
step:960/2285 train_time:58162ms step_avg:60.59ms
step:961/2285 train_time:58225ms step_avg:60.59ms
step:962/2285 train_time:58285ms step_avg:60.59ms
step:963/2285 train_time:58348ms step_avg:60.59ms
step:964/2285 train_time:58409ms step_avg:60.59ms
step:965/2285 train_time:58471ms step_avg:60.59ms
step:966/2285 train_time:58531ms step_avg:60.59ms
step:967/2285 train_time:58593ms step_avg:60.59ms
step:968/2285 train_time:58653ms step_avg:60.59ms
step:969/2285 train_time:58715ms step_avg:60.59ms
step:970/2285 train_time:58775ms step_avg:60.59ms
step:971/2285 train_time:58837ms step_avg:60.59ms
step:972/2285 train_time:58896ms step_avg:60.59ms
step:973/2285 train_time:58959ms step_avg:60.59ms
step:974/2285 train_time:59019ms step_avg:60.59ms
step:975/2285 train_time:59081ms step_avg:60.60ms
step:976/2285 train_time:59142ms step_avg:60.60ms
step:977/2285 train_time:59204ms step_avg:60.60ms
step:978/2285 train_time:59264ms step_avg:60.60ms
step:979/2285 train_time:59327ms step_avg:60.60ms
step:980/2285 train_time:59387ms step_avg:60.60ms
step:981/2285 train_time:59449ms step_avg:60.60ms
step:982/2285 train_time:59509ms step_avg:60.60ms
step:983/2285 train_time:59572ms step_avg:60.60ms
step:984/2285 train_time:59632ms step_avg:60.60ms
step:985/2285 train_time:59695ms step_avg:60.60ms
step:986/2285 train_time:59755ms step_avg:60.60ms
step:987/2285 train_time:59817ms step_avg:60.61ms
step:988/2285 train_time:59877ms step_avg:60.60ms
step:989/2285 train_time:59939ms step_avg:60.61ms
step:990/2285 train_time:59998ms step_avg:60.60ms
step:991/2285 train_time:60060ms step_avg:60.61ms
step:992/2285 train_time:60120ms step_avg:60.60ms
step:993/2285 train_time:60183ms step_avg:60.61ms
step:994/2285 train_time:60242ms step_avg:60.61ms
step:995/2285 train_time:60305ms step_avg:60.61ms
step:996/2285 train_time:60365ms step_avg:60.61ms
step:997/2285 train_time:60427ms step_avg:60.61ms
step:998/2285 train_time:60488ms step_avg:60.61ms
step:999/2285 train_time:60551ms step_avg:60.61ms
step:1000/2285 train_time:60611ms step_avg:60.61ms
step:1000/2285 val_loss:3.5644 train_time:60674ms step_avg:60.67ms
step:1001/2285 train_time:60693ms step_avg:60.63ms
step:1002/2285 train_time:60735ms step_avg:60.61ms
step:1003/2285 train_time:60803ms step_avg:60.62ms
step:1004/2285 train_time:60865ms step_avg:60.62ms
step:1005/2285 train_time:60927ms step_avg:60.62ms
step:1006/2285 train_time:60987ms step_avg:60.62ms
step:1007/2285 train_time:61049ms step_avg:60.62ms
step:1008/2285 train_time:61108ms step_avg:60.62ms
step:1009/2285 train_time:61170ms step_avg:60.62ms
step:1010/2285 train_time:61230ms step_avg:60.62ms
step:1011/2285 train_time:61292ms step_avg:60.62ms
step:1012/2285 train_time:61351ms step_avg:60.62ms
step:1013/2285 train_time:61413ms step_avg:60.63ms
step:1014/2285 train_time:61474ms step_avg:60.63ms
step:1015/2285 train_time:61536ms step_avg:60.63ms
step:1016/2285 train_time:61596ms step_avg:60.63ms
step:1017/2285 train_time:61659ms step_avg:60.63ms
step:1018/2285 train_time:61720ms step_avg:60.63ms
step:1019/2285 train_time:61785ms step_avg:60.63ms
step:1020/2285 train_time:61845ms step_avg:60.63ms
step:1021/2285 train_time:61907ms step_avg:60.63ms
step:1022/2285 train_time:61967ms step_avg:60.63ms
step:1023/2285 train_time:62030ms step_avg:60.64ms
step:1024/2285 train_time:62089ms step_avg:60.63ms
step:1025/2285 train_time:62151ms step_avg:60.64ms
step:1026/2285 train_time:62211ms step_avg:60.63ms
step:1027/2285 train_time:62274ms step_avg:60.64ms
step:1028/2285 train_time:62334ms step_avg:60.64ms
step:1029/2285 train_time:62396ms step_avg:60.64ms
step:1030/2285 train_time:62456ms step_avg:60.64ms
step:1031/2285 train_time:62518ms step_avg:60.64ms
step:1032/2285 train_time:62579ms step_avg:60.64ms
step:1033/2285 train_time:62641ms step_avg:60.64ms
step:1034/2285 train_time:62702ms step_avg:60.64ms
step:1035/2285 train_time:62764ms step_avg:60.64ms
step:1036/2285 train_time:62825ms step_avg:60.64ms
step:1037/2285 train_time:62888ms step_avg:60.64ms
step:1038/2285 train_time:62949ms step_avg:60.64ms
step:1039/2285 train_time:63012ms step_avg:60.65ms
step:1040/2285 train_time:63072ms step_avg:60.65ms
step:1041/2285 train_time:63134ms step_avg:60.65ms
step:1042/2285 train_time:63194ms step_avg:60.65ms
step:1043/2285 train_time:63256ms step_avg:60.65ms
step:1044/2285 train_time:63315ms step_avg:60.65ms
step:1045/2285 train_time:63377ms step_avg:60.65ms
step:1046/2285 train_time:63437ms step_avg:60.65ms
step:1047/2285 train_time:63499ms step_avg:60.65ms
step:1048/2285 train_time:63559ms step_avg:60.65ms
step:1049/2285 train_time:63622ms step_avg:60.65ms
step:1050/2285 train_time:63682ms step_avg:60.65ms
step:1051/2285 train_time:63745ms step_avg:60.65ms
step:1052/2285 train_time:63804ms step_avg:60.65ms
step:1053/2285 train_time:63867ms step_avg:60.65ms
step:1054/2285 train_time:63928ms step_avg:60.65ms
step:1055/2285 train_time:63991ms step_avg:60.65ms
step:1056/2285 train_time:64051ms step_avg:60.65ms
step:1057/2285 train_time:64113ms step_avg:60.66ms
step:1058/2285 train_time:64173ms step_avg:60.66ms
step:1059/2285 train_time:64235ms step_avg:60.66ms
step:1060/2285 train_time:64295ms step_avg:60.66ms
step:1061/2285 train_time:64357ms step_avg:60.66ms
step:1062/2285 train_time:64419ms step_avg:60.66ms
step:1063/2285 train_time:64480ms step_avg:60.66ms
step:1064/2285 train_time:64539ms step_avg:60.66ms
step:1065/2285 train_time:64602ms step_avg:60.66ms
step:1066/2285 train_time:64662ms step_avg:60.66ms
step:1067/2285 train_time:64724ms step_avg:60.66ms
step:1068/2285 train_time:64785ms step_avg:60.66ms
step:1069/2285 train_time:64847ms step_avg:60.66ms
step:1070/2285 train_time:64907ms step_avg:60.66ms
step:1071/2285 train_time:64970ms step_avg:60.66ms
step:1072/2285 train_time:65030ms step_avg:60.66ms
step:1073/2285 train_time:65093ms step_avg:60.66ms
step:1074/2285 train_time:65152ms step_avg:60.66ms
step:1075/2285 train_time:65214ms step_avg:60.66ms
step:1076/2285 train_time:65275ms step_avg:60.66ms
step:1077/2285 train_time:65337ms step_avg:60.67ms
step:1078/2285 train_time:65397ms step_avg:60.67ms
step:1079/2285 train_time:65459ms step_avg:60.67ms
step:1080/2285 train_time:65519ms step_avg:60.67ms
step:1081/2285 train_time:65581ms step_avg:60.67ms
step:1082/2285 train_time:65641ms step_avg:60.67ms
step:1083/2285 train_time:65703ms step_avg:60.67ms
step:1084/2285 train_time:65763ms step_avg:60.67ms
step:1085/2285 train_time:65826ms step_avg:60.67ms
step:1086/2285 train_time:65885ms step_avg:60.67ms
step:1087/2285 train_time:65948ms step_avg:60.67ms
step:1088/2285 train_time:66008ms step_avg:60.67ms
step:1089/2285 train_time:66071ms step_avg:60.67ms
step:1090/2285 train_time:66131ms step_avg:60.67ms
step:1091/2285 train_time:66193ms step_avg:60.67ms
step:1092/2285 train_time:66253ms step_avg:60.67ms
step:1093/2285 train_time:66316ms step_avg:60.67ms
step:1094/2285 train_time:66376ms step_avg:60.67ms
step:1095/2285 train_time:66438ms step_avg:60.67ms
step:1096/2285 train_time:66498ms step_avg:60.67ms
step:1097/2285 train_time:66561ms step_avg:60.68ms
step:1098/2285 train_time:66621ms step_avg:60.67ms
step:1099/2285 train_time:66683ms step_avg:60.68ms
step:1100/2285 train_time:66743ms step_avg:60.68ms
step:1101/2285 train_time:66805ms step_avg:60.68ms
step:1102/2285 train_time:66864ms step_avg:60.68ms
step:1103/2285 train_time:66928ms step_avg:60.68ms
step:1104/2285 train_time:66987ms step_avg:60.68ms
step:1105/2285 train_time:67050ms step_avg:60.68ms
step:1106/2285 train_time:67110ms step_avg:60.68ms
step:1107/2285 train_time:67173ms step_avg:60.68ms
step:1108/2285 train_time:67233ms step_avg:60.68ms
step:1109/2285 train_time:67296ms step_avg:60.68ms
step:1110/2285 train_time:67356ms step_avg:60.68ms
step:1111/2285 train_time:67418ms step_avg:60.68ms
step:1112/2285 train_time:67478ms step_avg:60.68ms
step:1113/2285 train_time:67541ms step_avg:60.68ms
step:1114/2285 train_time:67600ms step_avg:60.68ms
step:1115/2285 train_time:67662ms step_avg:60.68ms
step:1116/2285 train_time:67722ms step_avg:60.68ms
step:1117/2285 train_time:67785ms step_avg:60.68ms
step:1118/2285 train_time:67844ms step_avg:60.68ms
step:1119/2285 train_time:67906ms step_avg:60.68ms
step:1120/2285 train_time:67967ms step_avg:60.68ms
step:1121/2285 train_time:68029ms step_avg:60.69ms
step:1122/2285 train_time:68089ms step_avg:60.69ms
step:1123/2285 train_time:68151ms step_avg:60.69ms
step:1124/2285 train_time:68211ms step_avg:60.69ms
step:1125/2285 train_time:68274ms step_avg:60.69ms
step:1126/2285 train_time:68334ms step_avg:60.69ms
step:1127/2285 train_time:68397ms step_avg:60.69ms
step:1128/2285 train_time:68458ms step_avg:60.69ms
step:1129/2285 train_time:68520ms step_avg:60.69ms
step:1130/2285 train_time:68580ms step_avg:60.69ms
step:1131/2285 train_time:68642ms step_avg:60.69ms
step:1132/2285 train_time:68702ms step_avg:60.69ms
step:1133/2285 train_time:68765ms step_avg:60.69ms
step:1134/2285 train_time:68825ms step_avg:60.69ms
step:1135/2285 train_time:68887ms step_avg:60.69ms
step:1136/2285 train_time:68946ms step_avg:60.69ms
step:1137/2285 train_time:69009ms step_avg:60.69ms
step:1138/2285 train_time:69069ms step_avg:60.69ms
step:1139/2285 train_time:69131ms step_avg:60.69ms
step:1140/2285 train_time:69191ms step_avg:60.69ms
step:1141/2285 train_time:69253ms step_avg:60.70ms
step:1142/2285 train_time:69313ms step_avg:60.69ms
step:1143/2285 train_time:69376ms step_avg:60.70ms
step:1144/2285 train_time:69436ms step_avg:60.70ms
step:1145/2285 train_time:69498ms step_avg:60.70ms
step:1146/2285 train_time:69559ms step_avg:60.70ms
step:1147/2285 train_time:69622ms step_avg:60.70ms
step:1148/2285 train_time:69682ms step_avg:60.70ms
step:1149/2285 train_time:69744ms step_avg:60.70ms
step:1150/2285 train_time:69804ms step_avg:60.70ms
step:1151/2285 train_time:69866ms step_avg:60.70ms
step:1152/2285 train_time:69926ms step_avg:60.70ms
step:1153/2285 train_time:69989ms step_avg:60.70ms
step:1154/2285 train_time:70048ms step_avg:60.70ms
step:1155/2285 train_time:70111ms step_avg:60.70ms
step:1156/2285 train_time:70171ms step_avg:60.70ms
step:1157/2285 train_time:70233ms step_avg:60.70ms
step:1158/2285 train_time:70293ms step_avg:60.70ms
step:1159/2285 train_time:70356ms step_avg:60.70ms
step:1160/2285 train_time:70416ms step_avg:60.70ms
step:1161/2285 train_time:70479ms step_avg:60.71ms
step:1162/2285 train_time:70539ms step_avg:60.70ms
step:1163/2285 train_time:70601ms step_avg:60.71ms
step:1164/2285 train_time:70662ms step_avg:60.71ms
step:1165/2285 train_time:70723ms step_avg:60.71ms
step:1166/2285 train_time:70783ms step_avg:60.71ms
step:1167/2285 train_time:70845ms step_avg:60.71ms
step:1168/2285 train_time:70905ms step_avg:60.71ms
step:1169/2285 train_time:70968ms step_avg:60.71ms
step:1170/2285 train_time:71028ms step_avg:60.71ms
step:1171/2285 train_time:71090ms step_avg:60.71ms
step:1172/2285 train_time:71150ms step_avg:60.71ms
step:1173/2285 train_time:71213ms step_avg:60.71ms
step:1174/2285 train_time:71273ms step_avg:60.71ms
step:1175/2285 train_time:71336ms step_avg:60.71ms
step:1176/2285 train_time:71395ms step_avg:60.71ms
step:1177/2285 train_time:71458ms step_avg:60.71ms
step:1178/2285 train_time:71518ms step_avg:60.71ms
step:1179/2285 train_time:71581ms step_avg:60.71ms
step:1180/2285 train_time:71641ms step_avg:60.71ms
step:1181/2285 train_time:71704ms step_avg:60.71ms
step:1182/2285 train_time:71763ms step_avg:60.71ms
step:1183/2285 train_time:71825ms step_avg:60.71ms
step:1184/2285 train_time:71884ms step_avg:60.71ms
step:1185/2285 train_time:71947ms step_avg:60.71ms
step:1186/2285 train_time:72007ms step_avg:60.71ms
step:1187/2285 train_time:72070ms step_avg:60.72ms
step:1188/2285 train_time:72130ms step_avg:60.72ms
step:1189/2285 train_time:72193ms step_avg:60.72ms
step:1190/2285 train_time:72252ms step_avg:60.72ms
step:1191/2285 train_time:72315ms step_avg:60.72ms
step:1192/2285 train_time:72376ms step_avg:60.72ms
step:1193/2285 train_time:72438ms step_avg:60.72ms
step:1194/2285 train_time:72498ms step_avg:60.72ms
step:1195/2285 train_time:72560ms step_avg:60.72ms
step:1196/2285 train_time:72620ms step_avg:60.72ms
step:1197/2285 train_time:72683ms step_avg:60.72ms
step:1198/2285 train_time:72742ms step_avg:60.72ms
step:1199/2285 train_time:72804ms step_avg:60.72ms
step:1200/2285 train_time:72864ms step_avg:60.72ms
step:1201/2285 train_time:72927ms step_avg:60.72ms
step:1202/2285 train_time:72986ms step_avg:60.72ms
step:1203/2285 train_time:73048ms step_avg:60.72ms
step:1204/2285 train_time:73108ms step_avg:60.72ms
step:1205/2285 train_time:73171ms step_avg:60.72ms
step:1206/2285 train_time:73231ms step_avg:60.72ms
step:1207/2285 train_time:73294ms step_avg:60.72ms
step:1208/2285 train_time:73355ms step_avg:60.72ms
step:1209/2285 train_time:73418ms step_avg:60.73ms
step:1210/2285 train_time:73478ms step_avg:60.73ms
step:1211/2285 train_time:73540ms step_avg:60.73ms
step:1212/2285 train_time:73600ms step_avg:60.73ms
step:1213/2285 train_time:73662ms step_avg:60.73ms
step:1214/2285 train_time:73722ms step_avg:60.73ms
step:1215/2285 train_time:73784ms step_avg:60.73ms
step:1216/2285 train_time:73844ms step_avg:60.73ms
step:1217/2285 train_time:73906ms step_avg:60.73ms
step:1218/2285 train_time:73966ms step_avg:60.73ms
step:1219/2285 train_time:74029ms step_avg:60.73ms
step:1220/2285 train_time:74089ms step_avg:60.73ms
step:1221/2285 train_time:74151ms step_avg:60.73ms
step:1222/2285 train_time:74211ms step_avg:60.73ms
step:1223/2285 train_time:74274ms step_avg:60.73ms
step:1224/2285 train_time:74334ms step_avg:60.73ms
step:1225/2285 train_time:74398ms step_avg:60.73ms
step:1226/2285 train_time:74458ms step_avg:60.73ms
step:1227/2285 train_time:74520ms step_avg:60.73ms
step:1228/2285 train_time:74580ms step_avg:60.73ms
step:1229/2285 train_time:74643ms step_avg:60.73ms
step:1230/2285 train_time:74702ms step_avg:60.73ms
step:1231/2285 train_time:74765ms step_avg:60.74ms
step:1232/2285 train_time:74825ms step_avg:60.73ms
step:1233/2285 train_time:74887ms step_avg:60.74ms
step:1234/2285 train_time:74946ms step_avg:60.73ms
step:1235/2285 train_time:75009ms step_avg:60.74ms
step:1236/2285 train_time:75068ms step_avg:60.74ms
step:1237/2285 train_time:75131ms step_avg:60.74ms
step:1238/2285 train_time:75191ms step_avg:60.74ms
step:1239/2285 train_time:75254ms step_avg:60.74ms
step:1240/2285 train_time:75315ms step_avg:60.74ms
step:1241/2285 train_time:75377ms step_avg:60.74ms
step:1242/2285 train_time:75437ms step_avg:60.74ms
step:1243/2285 train_time:75500ms step_avg:60.74ms
step:1244/2285 train_time:75560ms step_avg:60.74ms
step:1245/2285 train_time:75623ms step_avg:60.74ms
step:1246/2285 train_time:75683ms step_avg:60.74ms
step:1247/2285 train_time:75745ms step_avg:60.74ms
step:1248/2285 train_time:75804ms step_avg:60.74ms
step:1249/2285 train_time:75866ms step_avg:60.74ms
step:1250/2285 train_time:75926ms step_avg:60.74ms
step:1250/2285 val_loss:3.4966 train_time:75989ms step_avg:60.79ms
step:1251/2285 train_time:76008ms step_avg:60.76ms
step:1252/2285 train_time:76051ms step_avg:60.74ms
step:1253/2285 train_time:76117ms step_avg:60.75ms
step:1254/2285 train_time:76180ms step_avg:60.75ms
step:1255/2285 train_time:76243ms step_avg:60.75ms
step:1256/2285 train_time:76303ms step_avg:60.75ms
step:1257/2285 train_time:76365ms step_avg:60.75ms
step:1258/2285 train_time:76424ms step_avg:60.75ms
step:1259/2285 train_time:76486ms step_avg:60.75ms
step:1260/2285 train_time:76545ms step_avg:60.75ms
step:1261/2285 train_time:76606ms step_avg:60.75ms
step:1262/2285 train_time:76665ms step_avg:60.75ms
step:1263/2285 train_time:76727ms step_avg:60.75ms
step:1264/2285 train_time:76786ms step_avg:60.75ms
step:1265/2285 train_time:76847ms step_avg:60.75ms
step:1266/2285 train_time:76907ms step_avg:60.75ms
step:1267/2285 train_time:76970ms step_avg:60.75ms
step:1268/2285 train_time:77031ms step_avg:60.75ms
step:1269/2285 train_time:77096ms step_avg:60.75ms
step:1270/2285 train_time:77158ms step_avg:60.75ms
step:1271/2285 train_time:77221ms step_avg:60.76ms
step:1272/2285 train_time:77282ms step_avg:60.76ms
step:1273/2285 train_time:77344ms step_avg:60.76ms
step:1274/2285 train_time:77403ms step_avg:60.76ms
step:1275/2285 train_time:77465ms step_avg:60.76ms
step:1276/2285 train_time:77524ms step_avg:60.76ms
step:1277/2285 train_time:77586ms step_avg:60.76ms
step:1278/2285 train_time:77646ms step_avg:60.76ms
step:1279/2285 train_time:77707ms step_avg:60.76ms
step:1280/2285 train_time:77767ms step_avg:60.76ms
step:1281/2285 train_time:77828ms step_avg:60.76ms
step:1282/2285 train_time:77887ms step_avg:60.75ms
step:1283/2285 train_time:77950ms step_avg:60.76ms
step:1284/2285 train_time:78010ms step_avg:60.76ms
step:1285/2285 train_time:78074ms step_avg:60.76ms
step:1286/2285 train_time:78135ms step_avg:60.76ms
step:1287/2285 train_time:78198ms step_avg:60.76ms
step:1288/2285 train_time:78258ms step_avg:60.76ms
step:1289/2285 train_time:78320ms step_avg:60.76ms
step:1290/2285 train_time:78381ms step_avg:60.76ms
step:1291/2285 train_time:78443ms step_avg:60.76ms
step:1292/2285 train_time:78503ms step_avg:60.76ms
step:1293/2285 train_time:78564ms step_avg:60.76ms
step:1294/2285 train_time:78623ms step_avg:60.76ms
step:1295/2285 train_time:78685ms step_avg:60.76ms
step:1296/2285 train_time:78744ms step_avg:60.76ms
step:1297/2285 train_time:78807ms step_avg:60.76ms
step:1298/2285 train_time:78866ms step_avg:60.76ms
step:1299/2285 train_time:78928ms step_avg:60.76ms
step:1300/2285 train_time:78989ms step_avg:60.76ms
step:1301/2285 train_time:79053ms step_avg:60.76ms
step:1302/2285 train_time:79112ms step_avg:60.76ms
step:1303/2285 train_time:79175ms step_avg:60.76ms
step:1304/2285 train_time:79235ms step_avg:60.76ms
step:1305/2285 train_time:79298ms step_avg:60.77ms
step:1306/2285 train_time:79358ms step_avg:60.76ms
step:1307/2285 train_time:79421ms step_avg:60.77ms
step:1308/2285 train_time:79481ms step_avg:60.77ms
step:1309/2285 train_time:79543ms step_avg:60.77ms
step:1310/2285 train_time:79603ms step_avg:60.77ms
step:1311/2285 train_time:79664ms step_avg:60.77ms
step:1312/2285 train_time:79724ms step_avg:60.77ms
step:1313/2285 train_time:79785ms step_avg:60.77ms
step:1314/2285 train_time:79845ms step_avg:60.76ms
step:1315/2285 train_time:79907ms step_avg:60.77ms
step:1316/2285 train_time:79966ms step_avg:60.76ms
step:1317/2285 train_time:80029ms step_avg:60.77ms
step:1318/2285 train_time:80090ms step_avg:60.77ms
step:1319/2285 train_time:80153ms step_avg:60.77ms
step:1320/2285 train_time:80213ms step_avg:60.77ms
step:1321/2285 train_time:80276ms step_avg:60.77ms
step:1322/2285 train_time:80337ms step_avg:60.77ms
step:1323/2285 train_time:80400ms step_avg:60.77ms
step:1324/2285 train_time:80460ms step_avg:60.77ms
step:1325/2285 train_time:80522ms step_avg:60.77ms
step:1326/2285 train_time:80582ms step_avg:60.77ms
step:1327/2285 train_time:80644ms step_avg:60.77ms
step:1328/2285 train_time:80704ms step_avg:60.77ms
step:1329/2285 train_time:80766ms step_avg:60.77ms
step:1330/2285 train_time:80825ms step_avg:60.77ms
step:1331/2285 train_time:80887ms step_avg:60.77ms
step:1332/2285 train_time:80947ms step_avg:60.77ms
step:1333/2285 train_time:81010ms step_avg:60.77ms
step:1334/2285 train_time:81070ms step_avg:60.77ms
step:1335/2285 train_time:81133ms step_avg:60.77ms
step:1336/2285 train_time:81193ms step_avg:60.77ms
step:1337/2285 train_time:81256ms step_avg:60.77ms
step:1338/2285 train_time:81316ms step_avg:60.77ms
step:1339/2285 train_time:81378ms step_avg:60.78ms
step:1340/2285 train_time:81438ms step_avg:60.77ms
step:1341/2285 train_time:81501ms step_avg:60.78ms
step:1342/2285 train_time:81561ms step_avg:60.78ms
step:1343/2285 train_time:81623ms step_avg:60.78ms
step:1344/2285 train_time:81683ms step_avg:60.78ms
step:1345/2285 train_time:81745ms step_avg:60.78ms
step:1346/2285 train_time:81805ms step_avg:60.78ms
step:1347/2285 train_time:81867ms step_avg:60.78ms
step:1348/2285 train_time:81927ms step_avg:60.78ms
step:1349/2285 train_time:81989ms step_avg:60.78ms
step:1350/2285 train_time:82050ms step_avg:60.78ms
step:1351/2285 train_time:82113ms step_avg:60.78ms
step:1352/2285 train_time:82172ms step_avg:60.78ms
step:1353/2285 train_time:82235ms step_avg:60.78ms
step:1354/2285 train_time:82296ms step_avg:60.78ms
step:1355/2285 train_time:82359ms step_avg:60.78ms
step:1356/2285 train_time:82419ms step_avg:60.78ms
step:1357/2285 train_time:82482ms step_avg:60.78ms
step:1358/2285 train_time:82541ms step_avg:60.78ms
step:1359/2285 train_time:82603ms step_avg:60.78ms
step:1360/2285 train_time:82663ms step_avg:60.78ms
step:1361/2285 train_time:82725ms step_avg:60.78ms
step:1362/2285 train_time:82785ms step_avg:60.78ms
step:1363/2285 train_time:82847ms step_avg:60.78ms
step:1364/2285 train_time:82906ms step_avg:60.78ms
step:1365/2285 train_time:82968ms step_avg:60.78ms
step:1366/2285 train_time:83027ms step_avg:60.78ms
step:1367/2285 train_time:83091ms step_avg:60.78ms
step:1368/2285 train_time:83151ms step_avg:60.78ms
step:1369/2285 train_time:83213ms step_avg:60.78ms
step:1370/2285 train_time:83273ms step_avg:60.78ms
step:1371/2285 train_time:83336ms step_avg:60.78ms
step:1372/2285 train_time:83397ms step_avg:60.78ms
step:1373/2285 train_time:83459ms step_avg:60.79ms
step:1374/2285 train_time:83519ms step_avg:60.79ms
step:1375/2285 train_time:83582ms step_avg:60.79ms
step:1376/2285 train_time:83642ms step_avg:60.79ms
step:1377/2285 train_time:83704ms step_avg:60.79ms
step:1378/2285 train_time:83764ms step_avg:60.79ms
step:1379/2285 train_time:83826ms step_avg:60.79ms
step:1380/2285 train_time:83886ms step_avg:60.79ms
step:1381/2285 train_time:83948ms step_avg:60.79ms
step:1382/2285 train_time:84008ms step_avg:60.79ms
step:1383/2285 train_time:84071ms step_avg:60.79ms
step:1384/2285 train_time:84131ms step_avg:60.79ms
step:1385/2285 train_time:84193ms step_avg:60.79ms
step:1386/2285 train_time:84254ms step_avg:60.79ms
step:1387/2285 train_time:84316ms step_avg:60.79ms
step:1388/2285 train_time:84377ms step_avg:60.79ms
step:1389/2285 train_time:84439ms step_avg:60.79ms
step:1390/2285 train_time:84499ms step_avg:60.79ms
step:1391/2285 train_time:84562ms step_avg:60.79ms
step:1392/2285 train_time:84622ms step_avg:60.79ms
step:1393/2285 train_time:84684ms step_avg:60.79ms
step:1394/2285 train_time:84743ms step_avg:60.79ms
step:1395/2285 train_time:84805ms step_avg:60.79ms
step:1396/2285 train_time:84865ms step_avg:60.79ms
step:1397/2285 train_time:84926ms step_avg:60.79ms
step:1398/2285 train_time:84986ms step_avg:60.79ms
step:1399/2285 train_time:85049ms step_avg:60.79ms
step:1400/2285 train_time:85109ms step_avg:60.79ms
step:1401/2285 train_time:85172ms step_avg:60.79ms
step:1402/2285 train_time:85232ms step_avg:60.79ms
step:1403/2285 train_time:85294ms step_avg:60.79ms
step:1404/2285 train_time:85355ms step_avg:60.79ms
step:1405/2285 train_time:85418ms step_avg:60.80ms
step:1406/2285 train_time:85478ms step_avg:60.80ms
step:1407/2285 train_time:85541ms step_avg:60.80ms
step:1408/2285 train_time:85602ms step_avg:60.80ms
step:1409/2285 train_time:85664ms step_avg:60.80ms
step:1410/2285 train_time:85723ms step_avg:60.80ms
step:1411/2285 train_time:85785ms step_avg:60.80ms
step:1412/2285 train_time:85845ms step_avg:60.80ms
step:1413/2285 train_time:85907ms step_avg:60.80ms
step:1414/2285 train_time:85967ms step_avg:60.80ms
step:1415/2285 train_time:86029ms step_avg:60.80ms
step:1416/2285 train_time:86089ms step_avg:60.80ms
step:1417/2285 train_time:86152ms step_avg:60.80ms
step:1418/2285 train_time:86211ms step_avg:60.80ms
step:1419/2285 train_time:86274ms step_avg:60.80ms
step:1420/2285 train_time:86334ms step_avg:60.80ms
step:1421/2285 train_time:86397ms step_avg:60.80ms
step:1422/2285 train_time:86458ms step_avg:60.80ms
step:1423/2285 train_time:86520ms step_avg:60.80ms
step:1424/2285 train_time:86580ms step_avg:60.80ms
step:1425/2285 train_time:86643ms step_avg:60.80ms
step:1426/2285 train_time:86703ms step_avg:60.80ms
step:1427/2285 train_time:86765ms step_avg:60.80ms
step:1428/2285 train_time:86825ms step_avg:60.80ms
step:1429/2285 train_time:86887ms step_avg:60.80ms
step:1430/2285 train_time:86947ms step_avg:60.80ms
step:1431/2285 train_time:87009ms step_avg:60.80ms
step:1432/2285 train_time:87068ms step_avg:60.80ms
step:1433/2285 train_time:87131ms step_avg:60.80ms
step:1434/2285 train_time:87191ms step_avg:60.80ms
step:1435/2285 train_time:87253ms step_avg:60.80ms
step:1436/2285 train_time:87313ms step_avg:60.80ms
step:1437/2285 train_time:87375ms step_avg:60.80ms
step:1438/2285 train_time:87436ms step_avg:60.80ms
step:1439/2285 train_time:87498ms step_avg:60.80ms
step:1440/2285 train_time:87558ms step_avg:60.80ms
step:1441/2285 train_time:87621ms step_avg:60.81ms
step:1442/2285 train_time:87682ms step_avg:60.81ms
step:1443/2285 train_time:87744ms step_avg:60.81ms
step:1444/2285 train_time:87804ms step_avg:60.81ms
step:1445/2285 train_time:87867ms step_avg:60.81ms
step:1446/2285 train_time:87926ms step_avg:60.81ms
step:1447/2285 train_time:87989ms step_avg:60.81ms
step:1448/2285 train_time:88048ms step_avg:60.81ms
step:1449/2285 train_time:88110ms step_avg:60.81ms
step:1450/2285 train_time:88170ms step_avg:60.81ms
step:1451/2285 train_time:88232ms step_avg:60.81ms
step:1452/2285 train_time:88293ms step_avg:60.81ms
step:1453/2285 train_time:88355ms step_avg:60.81ms
step:1454/2285 train_time:88415ms step_avg:60.81ms
step:1455/2285 train_time:88478ms step_avg:60.81ms
step:1456/2285 train_time:88538ms step_avg:60.81ms
step:1457/2285 train_time:88601ms step_avg:60.81ms
step:1458/2285 train_time:88661ms step_avg:60.81ms
step:1459/2285 train_time:88723ms step_avg:60.81ms
step:1460/2285 train_time:88784ms step_avg:60.81ms
step:1461/2285 train_time:88846ms step_avg:60.81ms
step:1462/2285 train_time:88907ms step_avg:60.81ms
step:1463/2285 train_time:88969ms step_avg:60.81ms
step:1464/2285 train_time:89028ms step_avg:60.81ms
step:1465/2285 train_time:89091ms step_avg:60.81ms
step:1466/2285 train_time:89150ms step_avg:60.81ms
step:1467/2285 train_time:89213ms step_avg:60.81ms
step:1468/2285 train_time:89272ms step_avg:60.81ms
step:1469/2285 train_time:89335ms step_avg:60.81ms
step:1470/2285 train_time:89395ms step_avg:60.81ms
step:1471/2285 train_time:89458ms step_avg:60.81ms
step:1472/2285 train_time:89518ms step_avg:60.81ms
step:1473/2285 train_time:89581ms step_avg:60.82ms
step:1474/2285 train_time:89641ms step_avg:60.81ms
step:1475/2285 train_time:89704ms step_avg:60.82ms
step:1476/2285 train_time:89763ms step_avg:60.82ms
step:1477/2285 train_time:89826ms step_avg:60.82ms
step:1478/2285 train_time:89885ms step_avg:60.82ms
step:1479/2285 train_time:89948ms step_avg:60.82ms
step:1480/2285 train_time:90008ms step_avg:60.82ms
step:1481/2285 train_time:90070ms step_avg:60.82ms
step:1482/2285 train_time:90129ms step_avg:60.82ms
step:1483/2285 train_time:90192ms step_avg:60.82ms
step:1484/2285 train_time:90252ms step_avg:60.82ms
step:1485/2285 train_time:90314ms step_avg:60.82ms
step:1486/2285 train_time:90374ms step_avg:60.82ms
step:1487/2285 train_time:90436ms step_avg:60.82ms
step:1488/2285 train_time:90497ms step_avg:60.82ms
step:1489/2285 train_time:90560ms step_avg:60.82ms
step:1490/2285 train_time:90620ms step_avg:60.82ms
step:1491/2285 train_time:90683ms step_avg:60.82ms
step:1492/2285 train_time:90743ms step_avg:60.82ms
step:1493/2285 train_time:90805ms step_avg:60.82ms
step:1494/2285 train_time:90865ms step_avg:60.82ms
step:1495/2285 train_time:90927ms step_avg:60.82ms
step:1496/2285 train_time:90986ms step_avg:60.82ms
step:1497/2285 train_time:91049ms step_avg:60.82ms
step:1498/2285 train_time:91108ms step_avg:60.82ms
step:1499/2285 train_time:91171ms step_avg:60.82ms
step:1500/2285 train_time:91231ms step_avg:60.82ms
step:1500/2285 val_loss:3.4236 train_time:91295ms step_avg:60.86ms
step:1501/2285 train_time:91316ms step_avg:60.84ms
step:1502/2285 train_time:91357ms step_avg:60.82ms
step:1503/2285 train_time:91422ms step_avg:60.83ms
step:1504/2285 train_time:91483ms step_avg:60.83ms
step:1505/2285 train_time:91546ms step_avg:60.83ms
step:1506/2285 train_time:91607ms step_avg:60.83ms
step:1507/2285 train_time:91669ms step_avg:60.83ms
step:1508/2285 train_time:91728ms step_avg:60.83ms
step:1509/2285 train_time:91790ms step_avg:60.83ms
step:1510/2285 train_time:91849ms step_avg:60.83ms
step:1511/2285 train_time:91911ms step_avg:60.83ms
step:1512/2285 train_time:91971ms step_avg:60.83ms
step:1513/2285 train_time:92034ms step_avg:60.83ms
step:1514/2285 train_time:92094ms step_avg:60.83ms
step:1515/2285 train_time:92156ms step_avg:60.83ms
step:1516/2285 train_time:92218ms step_avg:60.83ms
step:1517/2285 train_time:92282ms step_avg:60.83ms
step:1518/2285 train_time:92344ms step_avg:60.83ms
step:1519/2285 train_time:92409ms step_avg:60.84ms
step:1520/2285 train_time:92470ms step_avg:60.84ms
step:1521/2285 train_time:92533ms step_avg:60.84ms
step:1522/2285 train_time:92594ms step_avg:60.84ms
step:1523/2285 train_time:92656ms step_avg:60.84ms
step:1524/2285 train_time:92717ms step_avg:60.84ms
step:1525/2285 train_time:92780ms step_avg:60.84ms
step:1526/2285 train_time:92841ms step_avg:60.84ms
step:1527/2285 train_time:92904ms step_avg:60.84ms
step:1528/2285 train_time:92963ms step_avg:60.84ms
step:1529/2285 train_time:93025ms step_avg:60.84ms
step:1530/2285 train_time:93085ms step_avg:60.84ms
step:1531/2285 train_time:93147ms step_avg:60.84ms
step:1532/2285 train_time:93208ms step_avg:60.84ms
step:1533/2285 train_time:93272ms step_avg:60.84ms
step:1534/2285 train_time:93333ms step_avg:60.84ms
step:1535/2285 train_time:93397ms step_avg:60.84ms
step:1536/2285 train_time:93457ms step_avg:60.84ms
step:1537/2285 train_time:93520ms step_avg:60.85ms
step:1538/2285 train_time:93581ms step_avg:60.85ms
step:1539/2285 train_time:93643ms step_avg:60.85ms
step:1540/2285 train_time:93704ms step_avg:60.85ms
step:1541/2285 train_time:93767ms step_avg:60.85ms
step:1542/2285 train_time:93827ms step_avg:60.85ms
step:1543/2285 train_time:93890ms step_avg:60.85ms
step:1544/2285 train_time:93950ms step_avg:60.85ms
step:1545/2285 train_time:94013ms step_avg:60.85ms
step:1546/2285 train_time:94073ms step_avg:60.85ms
step:1547/2285 train_time:94135ms step_avg:60.85ms
step:1548/2285 train_time:94196ms step_avg:60.85ms
step:1549/2285 train_time:94259ms step_avg:60.85ms
step:1550/2285 train_time:94321ms step_avg:60.85ms
step:1551/2285 train_time:94383ms step_avg:60.85ms
step:1552/2285 train_time:94444ms step_avg:60.85ms
step:1553/2285 train_time:94508ms step_avg:60.85ms
step:1554/2285 train_time:94569ms step_avg:60.85ms
step:1555/2285 train_time:94631ms step_avg:60.86ms
step:1556/2285 train_time:94692ms step_avg:60.86ms
step:1557/2285 train_time:94754ms step_avg:60.86ms
step:1558/2285 train_time:94815ms step_avg:60.86ms
step:1559/2285 train_time:94879ms step_avg:60.86ms
step:1560/2285 train_time:94939ms step_avg:60.86ms
step:1561/2285 train_time:95001ms step_avg:60.86ms
step:1562/2285 train_time:95062ms step_avg:60.86ms
step:1563/2285 train_time:95125ms step_avg:60.86ms
step:1564/2285 train_time:95184ms step_avg:60.86ms
step:1565/2285 train_time:95247ms step_avg:60.86ms
step:1566/2285 train_time:95307ms step_avg:60.86ms
step:1567/2285 train_time:95371ms step_avg:60.86ms
step:1568/2285 train_time:95431ms step_avg:60.86ms
step:1569/2285 train_time:95494ms step_avg:60.86ms
step:1570/2285 train_time:95555ms step_avg:60.86ms
step:1571/2285 train_time:95619ms step_avg:60.86ms
step:1572/2285 train_time:95679ms step_avg:60.86ms
step:1573/2285 train_time:95743ms step_avg:60.87ms
step:1574/2285 train_time:95803ms step_avg:60.87ms
step:1575/2285 train_time:95866ms step_avg:60.87ms
step:1576/2285 train_time:95926ms step_avg:60.87ms
step:1577/2285 train_time:95990ms step_avg:60.87ms
step:1578/2285 train_time:96050ms step_avg:60.87ms
step:1579/2285 train_time:96112ms step_avg:60.87ms
step:1580/2285 train_time:96172ms step_avg:60.87ms
step:1581/2285 train_time:96235ms step_avg:60.87ms
step:1582/2285 train_time:96295ms step_avg:60.87ms
step:1583/2285 train_time:96358ms step_avg:60.87ms
step:1584/2285 train_time:96420ms step_avg:60.87ms
step:1585/2285 train_time:96482ms step_avg:60.87ms
step:1586/2285 train_time:96543ms step_avg:60.87ms
step:1587/2285 train_time:96605ms step_avg:60.87ms
step:1588/2285 train_time:96665ms step_avg:60.87ms
step:1589/2285 train_time:96728ms step_avg:60.87ms
step:1590/2285 train_time:96788ms step_avg:60.87ms
step:1591/2285 train_time:96851ms step_avg:60.87ms
step:1592/2285 train_time:96911ms step_avg:60.87ms
step:1593/2285 train_time:96974ms step_avg:60.88ms
step:1594/2285 train_time:97035ms step_avg:60.87ms
step:1595/2285 train_time:97097ms step_avg:60.88ms
step:1596/2285 train_time:97158ms step_avg:60.88ms
step:1597/2285 train_time:97221ms step_avg:60.88ms
step:1598/2285 train_time:97281ms step_avg:60.88ms
step:1599/2285 train_time:97344ms step_avg:60.88ms
step:1600/2285 train_time:97404ms step_avg:60.88ms
step:1601/2285 train_time:97467ms step_avg:60.88ms
step:1602/2285 train_time:97529ms step_avg:60.88ms
step:1603/2285 train_time:97591ms step_avg:60.88ms
step:1604/2285 train_time:97652ms step_avg:60.88ms
step:1605/2285 train_time:97716ms step_avg:60.88ms
step:1606/2285 train_time:97777ms step_avg:60.88ms
step:1607/2285 train_time:97840ms step_avg:60.88ms
step:1608/2285 train_time:97901ms step_avg:60.88ms
step:1609/2285 train_time:97964ms step_avg:60.89ms
step:1610/2285 train_time:98024ms step_avg:60.88ms
step:1611/2285 train_time:98086ms step_avg:60.89ms
step:1612/2285 train_time:98146ms step_avg:60.88ms
step:1613/2285 train_time:98209ms step_avg:60.89ms
step:1614/2285 train_time:98270ms step_avg:60.89ms
step:1615/2285 train_time:98333ms step_avg:60.89ms
step:1616/2285 train_time:98393ms step_avg:60.89ms
step:1617/2285 train_time:98457ms step_avg:60.89ms
step:1618/2285 train_time:98518ms step_avg:60.89ms
step:1619/2285 train_time:98581ms step_avg:60.89ms
step:1620/2285 train_time:98641ms step_avg:60.89ms
step:1621/2285 train_time:98705ms step_avg:60.89ms
step:1622/2285 train_time:98765ms step_avg:60.89ms
step:1623/2285 train_time:98828ms step_avg:60.89ms
step:1624/2285 train_time:98888ms step_avg:60.89ms
step:1625/2285 train_time:98951ms step_avg:60.89ms
step:1626/2285 train_time:99010ms step_avg:60.89ms
step:1627/2285 train_time:99074ms step_avg:60.89ms
step:1628/2285 train_time:99134ms step_avg:60.89ms
step:1629/2285 train_time:99197ms step_avg:60.89ms
step:1630/2285 train_time:99258ms step_avg:60.89ms
step:1631/2285 train_time:99321ms step_avg:60.90ms
step:1632/2285 train_time:99382ms step_avg:60.90ms
step:1633/2285 train_time:99445ms step_avg:60.90ms
step:1634/2285 train_time:99506ms step_avg:60.90ms
step:1635/2285 train_time:99569ms step_avg:60.90ms
step:1636/2285 train_time:99629ms step_avg:60.90ms
step:1637/2285 train_time:99691ms step_avg:60.90ms
step:1638/2285 train_time:99752ms step_avg:60.90ms
step:1639/2285 train_time:99815ms step_avg:60.90ms
step:1640/2285 train_time:99875ms step_avg:60.90ms
step:1641/2285 train_time:99939ms step_avg:60.90ms
step:1642/2285 train_time:99999ms step_avg:60.90ms
step:1643/2285 train_time:100062ms step_avg:60.90ms
step:1644/2285 train_time:100122ms step_avg:60.90ms
step:1645/2285 train_time:100185ms step_avg:60.90ms
step:1646/2285 train_time:100245ms step_avg:60.90ms
step:1647/2285 train_time:100308ms step_avg:60.90ms
step:1648/2285 train_time:100368ms step_avg:60.90ms
step:1649/2285 train_time:100431ms step_avg:60.90ms
step:1650/2285 train_time:100491ms step_avg:60.90ms
step:1651/2285 train_time:100554ms step_avg:60.91ms
step:1652/2285 train_time:100615ms step_avg:60.91ms
step:1653/2285 train_time:100678ms step_avg:60.91ms
step:1654/2285 train_time:100739ms step_avg:60.91ms
step:1655/2285 train_time:100803ms step_avg:60.91ms
step:1656/2285 train_time:100863ms step_avg:60.91ms
step:1657/2285 train_time:100926ms step_avg:60.91ms
step:1658/2285 train_time:100986ms step_avg:60.91ms
step:1659/2285 train_time:101049ms step_avg:60.91ms
step:1660/2285 train_time:101108ms step_avg:60.91ms
step:1661/2285 train_time:101171ms step_avg:60.91ms
step:1662/2285 train_time:101231ms step_avg:60.91ms
step:1663/2285 train_time:101294ms step_avg:60.91ms
step:1664/2285 train_time:101355ms step_avg:60.91ms
step:1665/2285 train_time:101419ms step_avg:60.91ms
step:1666/2285 train_time:101480ms step_avg:60.91ms
step:1667/2285 train_time:101543ms step_avg:60.91ms
step:1668/2285 train_time:101603ms step_avg:60.91ms
step:1669/2285 train_time:101666ms step_avg:60.91ms
step:1670/2285 train_time:101727ms step_avg:60.91ms
step:1671/2285 train_time:101789ms step_avg:60.92ms
step:1672/2285 train_time:101850ms step_avg:60.92ms
step:1673/2285 train_time:101914ms step_avg:60.92ms
step:1674/2285 train_time:101974ms step_avg:60.92ms
step:1675/2285 train_time:102038ms step_avg:60.92ms
step:1676/2285 train_time:102099ms step_avg:60.92ms
step:1677/2285 train_time:102161ms step_avg:60.92ms
step:1678/2285 train_time:102221ms step_avg:60.92ms
step:1679/2285 train_time:102285ms step_avg:60.92ms
step:1680/2285 train_time:102345ms step_avg:60.92ms
step:1681/2285 train_time:102408ms step_avg:60.92ms
step:1682/2285 train_time:102469ms step_avg:60.92ms
step:1683/2285 train_time:102533ms step_avg:60.92ms
step:1684/2285 train_time:102593ms step_avg:60.92ms
step:1685/2285 train_time:102656ms step_avg:60.92ms
step:1686/2285 train_time:102717ms step_avg:60.92ms
step:1687/2285 train_time:102780ms step_avg:60.92ms
step:1688/2285 train_time:102841ms step_avg:60.92ms
step:1689/2285 train_time:102904ms step_avg:60.93ms
step:1690/2285 train_time:102964ms step_avg:60.93ms
step:1691/2285 train_time:103026ms step_avg:60.93ms
step:1692/2285 train_time:103087ms step_avg:60.93ms
step:1693/2285 train_time:103149ms step_avg:60.93ms
step:1694/2285 train_time:103209ms step_avg:60.93ms
step:1695/2285 train_time:103272ms step_avg:60.93ms
step:1696/2285 train_time:103332ms step_avg:60.93ms
step:1697/2285 train_time:103395ms step_avg:60.93ms
step:1698/2285 train_time:103456ms step_avg:60.93ms
step:1699/2285 train_time:103519ms step_avg:60.93ms
step:1700/2285 train_time:103580ms step_avg:60.93ms
step:1701/2285 train_time:103643ms step_avg:60.93ms
step:1702/2285 train_time:103703ms step_avg:60.93ms
step:1703/2285 train_time:103766ms step_avg:60.93ms
step:1704/2285 train_time:103827ms step_avg:60.93ms
step:1705/2285 train_time:103890ms step_avg:60.93ms
step:1706/2285 train_time:103950ms step_avg:60.93ms
step:1707/2285 train_time:104012ms step_avg:60.93ms
step:1708/2285 train_time:104072ms step_avg:60.93ms
step:1709/2285 train_time:104135ms step_avg:60.93ms
step:1710/2285 train_time:104196ms step_avg:60.93ms
step:1711/2285 train_time:104259ms step_avg:60.93ms
step:1712/2285 train_time:104321ms step_avg:60.93ms
step:1713/2285 train_time:104383ms step_avg:60.94ms
step:1714/2285 train_time:104444ms step_avg:60.94ms
step:1715/2285 train_time:104506ms step_avg:60.94ms
step:1716/2285 train_time:104567ms step_avg:60.94ms
step:1717/2285 train_time:104629ms step_avg:60.94ms
step:1718/2285 train_time:104689ms step_avg:60.94ms
step:1719/2285 train_time:104752ms step_avg:60.94ms
step:1720/2285 train_time:104813ms step_avg:60.94ms
step:1721/2285 train_time:104876ms step_avg:60.94ms
step:1722/2285 train_time:104936ms step_avg:60.94ms
step:1723/2285 train_time:105000ms step_avg:60.94ms
step:1724/2285 train_time:105061ms step_avg:60.94ms
step:1725/2285 train_time:105123ms step_avg:60.94ms
step:1726/2285 train_time:105183ms step_avg:60.94ms
step:1727/2285 train_time:105246ms step_avg:60.94ms
step:1728/2285 train_time:105307ms step_avg:60.94ms
step:1729/2285 train_time:105371ms step_avg:60.94ms
step:1730/2285 train_time:105431ms step_avg:60.94ms
step:1731/2285 train_time:105494ms step_avg:60.94ms
step:1732/2285 train_time:105555ms step_avg:60.94ms
step:1733/2285 train_time:105618ms step_avg:60.95ms
step:1734/2285 train_time:105679ms step_avg:60.95ms
step:1735/2285 train_time:105743ms step_avg:60.95ms
step:1736/2285 train_time:105803ms step_avg:60.95ms
step:1737/2285 train_time:105866ms step_avg:60.95ms
step:1738/2285 train_time:105926ms step_avg:60.95ms
step:1739/2285 train_time:105989ms step_avg:60.95ms
step:1740/2285 train_time:106049ms step_avg:60.95ms
step:1741/2285 train_time:106112ms step_avg:60.95ms
step:1742/2285 train_time:106172ms step_avg:60.95ms
step:1743/2285 train_time:106235ms step_avg:60.95ms
step:1744/2285 train_time:106296ms step_avg:60.95ms
step:1745/2285 train_time:106359ms step_avg:60.95ms
step:1746/2285 train_time:106420ms step_avg:60.95ms
step:1747/2285 train_time:106483ms step_avg:60.95ms
step:1748/2285 train_time:106543ms step_avg:60.95ms
step:1749/2285 train_time:106606ms step_avg:60.95ms
step:1750/2285 train_time:106666ms step_avg:60.95ms
step:1750/2285 val_loss:3.3645 train_time:106730ms step_avg:60.99ms
step:1751/2285 train_time:106749ms step_avg:60.96ms
step:1752/2285 train_time:106792ms step_avg:60.95ms
step:1753/2285 train_time:106858ms step_avg:60.96ms
step:1754/2285 train_time:106919ms step_avg:60.96ms
step:1755/2285 train_time:106981ms step_avg:60.96ms
step:1756/2285 train_time:107041ms step_avg:60.96ms
step:1757/2285 train_time:107103ms step_avg:60.96ms
step:1758/2285 train_time:107163ms step_avg:60.96ms
step:1759/2285 train_time:107224ms step_avg:60.96ms
step:1760/2285 train_time:107284ms step_avg:60.96ms
step:1761/2285 train_time:107348ms step_avg:60.96ms
step:1762/2285 train_time:107408ms step_avg:60.96ms
step:1763/2285 train_time:107470ms step_avg:60.96ms
step:1764/2285 train_time:107531ms step_avg:60.96ms
step:1765/2285 train_time:107593ms step_avg:60.96ms
step:1766/2285 train_time:107654ms step_avg:60.96ms
step:1767/2285 train_time:107717ms step_avg:60.96ms
step:1768/2285 train_time:107779ms step_avg:60.96ms
step:1769/2285 train_time:107843ms step_avg:60.96ms
step:1770/2285 train_time:107904ms step_avg:60.96ms
step:1771/2285 train_time:107967ms step_avg:60.96ms
step:1772/2285 train_time:108027ms step_avg:60.96ms
step:1773/2285 train_time:108090ms step_avg:60.96ms
step:1774/2285 train_time:108150ms step_avg:60.96ms
step:1775/2285 train_time:108212ms step_avg:60.96ms
step:1776/2285 train_time:108272ms step_avg:60.96ms
step:1777/2285 train_time:108336ms step_avg:60.97ms
step:1778/2285 train_time:108396ms step_avg:60.96ms
step:1779/2285 train_time:108458ms step_avg:60.97ms
step:1780/2285 train_time:108518ms step_avg:60.97ms
step:1781/2285 train_time:108582ms step_avg:60.97ms
step:1782/2285 train_time:108643ms step_avg:60.97ms
step:1783/2285 train_time:108706ms step_avg:60.97ms
step:1784/2285 train_time:108767ms step_avg:60.97ms
step:1785/2285 train_time:108831ms step_avg:60.97ms
step:1786/2285 train_time:108891ms step_avg:60.97ms
step:1787/2285 train_time:108955ms step_avg:60.97ms
step:1788/2285 train_time:109015ms step_avg:60.97ms
step:1789/2285 train_time:109078ms step_avg:60.97ms
step:1790/2285 train_time:109138ms step_avg:60.97ms
step:1791/2285 train_time:109200ms step_avg:60.97ms
step:1792/2285 train_time:109260ms step_avg:60.97ms
step:1793/2285 train_time:109323ms step_avg:60.97ms
step:1794/2285 train_time:109383ms step_avg:60.97ms
step:1795/2285 train_time:109445ms step_avg:60.97ms
step:1796/2285 train_time:109506ms step_avg:60.97ms
step:1797/2285 train_time:109569ms step_avg:60.97ms
step:1798/2285 train_time:109629ms step_avg:60.97ms
step:1799/2285 train_time:109692ms step_avg:60.97ms
step:1800/2285 train_time:109754ms step_avg:60.97ms
step:1801/2285 train_time:109817ms step_avg:60.98ms
step:1802/2285 train_time:109877ms step_avg:60.98ms
step:1803/2285 train_time:109941ms step_avg:60.98ms
step:1804/2285 train_time:110001ms step_avg:60.98ms
step:1805/2285 train_time:110064ms step_avg:60.98ms
step:1806/2285 train_time:110124ms step_avg:60.98ms
step:1807/2285 train_time:110187ms step_avg:60.98ms
step:1808/2285 train_time:110247ms step_avg:60.98ms
step:1809/2285 train_time:110310ms step_avg:60.98ms
step:1810/2285 train_time:110370ms step_avg:60.98ms
step:1811/2285 train_time:110432ms step_avg:60.98ms
step:1812/2285 train_time:110492ms step_avg:60.98ms
step:1813/2285 train_time:110555ms step_avg:60.98ms
step:1814/2285 train_time:110615ms step_avg:60.98ms
step:1815/2285 train_time:110678ms step_avg:60.98ms
step:1816/2285 train_time:110739ms step_avg:60.98ms
step:1817/2285 train_time:110803ms step_avg:60.98ms
step:1818/2285 train_time:110863ms step_avg:60.98ms
step:1819/2285 train_time:110926ms step_avg:60.98ms
step:1820/2285 train_time:110987ms step_avg:60.98ms
step:1821/2285 train_time:111050ms step_avg:60.98ms
step:1822/2285 train_time:111110ms step_avg:60.98ms
step:1823/2285 train_time:111172ms step_avg:60.98ms
step:1824/2285 train_time:111232ms step_avg:60.98ms
step:1825/2285 train_time:111295ms step_avg:60.98ms
step:1826/2285 train_time:111355ms step_avg:60.98ms
step:1827/2285 train_time:111417ms step_avg:60.98ms
step:1828/2285 train_time:111477ms step_avg:60.98ms
step:1829/2285 train_time:111540ms step_avg:60.98ms
step:1830/2285 train_time:111601ms step_avg:60.98ms
step:1831/2285 train_time:111664ms step_avg:60.99ms
step:1832/2285 train_time:111724ms step_avg:60.98ms
step:1833/2285 train_time:111787ms step_avg:60.99ms
step:1834/2285 train_time:111848ms step_avg:60.99ms
step:1835/2285 train_time:111911ms step_avg:60.99ms
step:1836/2285 train_time:111972ms step_avg:60.99ms
step:1837/2285 train_time:112035ms step_avg:60.99ms
step:1838/2285 train_time:112096ms step_avg:60.99ms
step:1839/2285 train_time:112159ms step_avg:60.99ms
step:1840/2285 train_time:112219ms step_avg:60.99ms
step:1841/2285 train_time:112282ms step_avg:60.99ms
step:1842/2285 train_time:112343ms step_avg:60.99ms
step:1843/2285 train_time:112406ms step_avg:60.99ms
step:1844/2285 train_time:112466ms step_avg:60.99ms
step:1845/2285 train_time:112529ms step_avg:60.99ms
step:1846/2285 train_time:112589ms step_avg:60.99ms
step:1847/2285 train_time:112652ms step_avg:60.99ms
step:1848/2285 train_time:112712ms step_avg:60.99ms
step:1849/2285 train_time:112775ms step_avg:60.99ms
step:1850/2285 train_time:112835ms step_avg:60.99ms
step:1851/2285 train_time:112899ms step_avg:60.99ms
step:1852/2285 train_time:112959ms step_avg:60.99ms
step:1853/2285 train_time:113022ms step_avg:60.99ms
step:1854/2285 train_time:113083ms step_avg:60.99ms
step:1855/2285 train_time:113146ms step_avg:61.00ms
step:1856/2285 train_time:113206ms step_avg:60.99ms
step:1857/2285 train_time:113269ms step_avg:61.00ms
step:1858/2285 train_time:113330ms step_avg:61.00ms
step:1859/2285 train_time:113393ms step_avg:61.00ms
step:1860/2285 train_time:113453ms step_avg:61.00ms
step:1861/2285 train_time:113516ms step_avg:61.00ms
step:1862/2285 train_time:113576ms step_avg:61.00ms
step:1863/2285 train_time:113639ms step_avg:61.00ms
step:1864/2285 train_time:113699ms step_avg:61.00ms
step:1865/2285 train_time:113761ms step_avg:61.00ms
step:1866/2285 train_time:113822ms step_avg:61.00ms
step:1867/2285 train_time:113885ms step_avg:61.00ms
step:1868/2285 train_time:113946ms step_avg:61.00ms
step:1869/2285 train_time:114008ms step_avg:61.00ms
step:1870/2285 train_time:114068ms step_avg:61.00ms
step:1871/2285 train_time:114131ms step_avg:61.00ms
step:1872/2285 train_time:114192ms step_avg:61.00ms
step:1873/2285 train_time:114255ms step_avg:61.00ms
step:1874/2285 train_time:114315ms step_avg:61.00ms
step:1875/2285 train_time:114377ms step_avg:61.00ms
step:1876/2285 train_time:114438ms step_avg:61.00ms
step:1877/2285 train_time:114500ms step_avg:61.00ms
step:1878/2285 train_time:114561ms step_avg:61.00ms
step:1879/2285 train_time:114623ms step_avg:61.00ms
step:1880/2285 train_time:114684ms step_avg:61.00ms
step:1881/2285 train_time:114747ms step_avg:61.00ms
step:1882/2285 train_time:114808ms step_avg:61.00ms
step:1883/2285 train_time:114871ms step_avg:61.00ms
step:1884/2285 train_time:114930ms step_avg:61.00ms
step:1885/2285 train_time:114994ms step_avg:61.00ms
step:1886/2285 train_time:115054ms step_avg:61.00ms
step:1887/2285 train_time:115116ms step_avg:61.00ms
step:1888/2285 train_time:115176ms step_avg:61.00ms
step:1889/2285 train_time:115239ms step_avg:61.01ms
step:1890/2285 train_time:115300ms step_avg:61.01ms
step:1891/2285 train_time:115363ms step_avg:61.01ms
step:1892/2285 train_time:115423ms step_avg:61.01ms
step:1893/2285 train_time:115486ms step_avg:61.01ms
step:1894/2285 train_time:115547ms step_avg:61.01ms
step:1895/2285 train_time:115609ms step_avg:61.01ms
step:1896/2285 train_time:115669ms step_avg:61.01ms
step:1897/2285 train_time:115732ms step_avg:61.01ms
step:1898/2285 train_time:115792ms step_avg:61.01ms
step:1899/2285 train_time:115855ms step_avg:61.01ms
step:1900/2285 train_time:115915ms step_avg:61.01ms
step:1901/2285 train_time:115979ms step_avg:61.01ms
step:1902/2285 train_time:116040ms step_avg:61.01ms
step:1903/2285 train_time:116102ms step_avg:61.01ms
step:1904/2285 train_time:116163ms step_avg:61.01ms
step:1905/2285 train_time:116226ms step_avg:61.01ms
step:1906/2285 train_time:116286ms step_avg:61.01ms
step:1907/2285 train_time:116349ms step_avg:61.01ms
step:1908/2285 train_time:116409ms step_avg:61.01ms
step:1909/2285 train_time:116472ms step_avg:61.01ms
step:1910/2285 train_time:116532ms step_avg:61.01ms
step:1911/2285 train_time:116595ms step_avg:61.01ms
step:1912/2285 train_time:116655ms step_avg:61.01ms
step:1913/2285 train_time:116717ms step_avg:61.01ms
step:1914/2285 train_time:116777ms step_avg:61.01ms
step:1915/2285 train_time:116840ms step_avg:61.01ms
step:1916/2285 train_time:116901ms step_avg:61.01ms
step:1917/2285 train_time:116964ms step_avg:61.01ms
step:1918/2285 train_time:117024ms step_avg:61.01ms
step:1919/2285 train_time:117087ms step_avg:61.01ms
step:1920/2285 train_time:117147ms step_avg:61.01ms
step:1921/2285 train_time:117210ms step_avg:61.02ms
step:1922/2285 train_time:117270ms step_avg:61.01ms
step:1923/2285 train_time:117332ms step_avg:61.02ms
step:1924/2285 train_time:117393ms step_avg:61.02ms
step:1925/2285 train_time:117456ms step_avg:61.02ms
step:1926/2285 train_time:117516ms step_avg:61.02ms
step:1927/2285 train_time:117579ms step_avg:61.02ms
step:1928/2285 train_time:117640ms step_avg:61.02ms
step:1929/2285 train_time:117702ms step_avg:61.02ms
step:1930/2285 train_time:117763ms step_avg:61.02ms
step:1931/2285 train_time:117826ms step_avg:61.02ms
step:1932/2285 train_time:117887ms step_avg:61.02ms
step:1933/2285 train_time:117949ms step_avg:61.02ms
step:1934/2285 train_time:118010ms step_avg:61.02ms
step:1935/2285 train_time:118073ms step_avg:61.02ms
step:1936/2285 train_time:118133ms step_avg:61.02ms
step:1937/2285 train_time:118196ms step_avg:61.02ms
step:1938/2285 train_time:118256ms step_avg:61.02ms
step:1939/2285 train_time:118319ms step_avg:61.02ms
step:1940/2285 train_time:118379ms step_avg:61.02ms
step:1941/2285 train_time:118442ms step_avg:61.02ms
step:1942/2285 train_time:118503ms step_avg:61.02ms
step:1943/2285 train_time:118567ms step_avg:61.02ms
step:1944/2285 train_time:118627ms step_avg:61.02ms
step:1945/2285 train_time:118689ms step_avg:61.02ms
step:1946/2285 train_time:118750ms step_avg:61.02ms
step:1947/2285 train_time:118812ms step_avg:61.02ms
step:1948/2285 train_time:118874ms step_avg:61.02ms
step:1949/2285 train_time:118936ms step_avg:61.02ms
step:1950/2285 train_time:118997ms step_avg:61.02ms
step:1951/2285 train_time:119060ms step_avg:61.03ms
step:1952/2285 train_time:119121ms step_avg:61.02ms
step:1953/2285 train_time:119184ms step_avg:61.03ms
step:1954/2285 train_time:119244ms step_avg:61.03ms
step:1955/2285 train_time:119307ms step_avg:61.03ms
step:1956/2285 train_time:119368ms step_avg:61.03ms
step:1957/2285 train_time:119430ms step_avg:61.03ms
step:1958/2285 train_time:119491ms step_avg:61.03ms
step:1959/2285 train_time:119554ms step_avg:61.03ms
step:1960/2285 train_time:119615ms step_avg:61.03ms
step:1961/2285 train_time:119677ms step_avg:61.03ms
step:1962/2285 train_time:119737ms step_avg:61.03ms
step:1963/2285 train_time:119800ms step_avg:61.03ms
step:1964/2285 train_time:119860ms step_avg:61.03ms
step:1965/2285 train_time:119923ms step_avg:61.03ms
step:1966/2285 train_time:119984ms step_avg:61.03ms
step:1967/2285 train_time:120048ms step_avg:61.03ms
step:1968/2285 train_time:120108ms step_avg:61.03ms
step:1969/2285 train_time:120170ms step_avg:61.03ms
step:1970/2285 train_time:120230ms step_avg:61.03ms
step:1971/2285 train_time:120293ms step_avg:61.03ms
step:1972/2285 train_time:120354ms step_avg:61.03ms
step:1973/2285 train_time:120417ms step_avg:61.03ms
step:1974/2285 train_time:120477ms step_avg:61.03ms
step:1975/2285 train_time:120540ms step_avg:61.03ms
step:1976/2285 train_time:120602ms step_avg:61.03ms
step:1977/2285 train_time:120664ms step_avg:61.03ms
step:1978/2285 train_time:120724ms step_avg:61.03ms
step:1979/2285 train_time:120788ms step_avg:61.03ms
step:1980/2285 train_time:120848ms step_avg:61.03ms
step:1981/2285 train_time:120911ms step_avg:61.04ms
step:1982/2285 train_time:120972ms step_avg:61.04ms
step:1983/2285 train_time:121034ms step_avg:61.04ms
step:1984/2285 train_time:121094ms step_avg:61.04ms
step:1985/2285 train_time:121157ms step_avg:61.04ms
step:1986/2285 train_time:121217ms step_avg:61.04ms
step:1987/2285 train_time:121280ms step_avg:61.04ms
step:1988/2285 train_time:121342ms step_avg:61.04ms
step:1989/2285 train_time:121404ms step_avg:61.04ms
step:1990/2285 train_time:121465ms step_avg:61.04ms
step:1991/2285 train_time:121529ms step_avg:61.04ms
step:1992/2285 train_time:121589ms step_avg:61.04ms
step:1993/2285 train_time:121652ms step_avg:61.04ms
step:1994/2285 train_time:121713ms step_avg:61.04ms
step:1995/2285 train_time:121776ms step_avg:61.04ms
step:1996/2285 train_time:121837ms step_avg:61.04ms
step:1997/2285 train_time:121900ms step_avg:61.04ms
step:1998/2285 train_time:121961ms step_avg:61.04ms
step:1999/2285 train_time:122024ms step_avg:61.04ms
step:2000/2285 train_time:122085ms step_avg:61.04ms
step:2000/2285 val_loss:3.3187 train_time:122149ms step_avg:61.07ms
step:2001/2285 train_time:122168ms step_avg:61.05ms
step:2002/2285 train_time:122212ms step_avg:61.04ms
step:2003/2285 train_time:122279ms step_avg:61.05ms
step:2004/2285 train_time:122341ms step_avg:61.05ms
step:2005/2285 train_time:122404ms step_avg:61.05ms
step:2006/2285 train_time:122466ms step_avg:61.05ms
step:2007/2285 train_time:122528ms step_avg:61.05ms
step:2008/2285 train_time:122588ms step_avg:61.05ms
step:2009/2285 train_time:122651ms step_avg:61.05ms
step:2010/2285 train_time:122710ms step_avg:61.05ms
step:2011/2285 train_time:122772ms step_avg:61.05ms
step:2012/2285 train_time:122831ms step_avg:61.05ms
step:2013/2285 train_time:122894ms step_avg:61.05ms
step:2014/2285 train_time:122953ms step_avg:61.05ms
step:2015/2285 train_time:123015ms step_avg:61.05ms
step:2016/2285 train_time:123075ms step_avg:61.05ms
step:2017/2285 train_time:123140ms step_avg:61.05ms
step:2018/2285 train_time:123202ms step_avg:61.05ms
step:2019/2285 train_time:123266ms step_avg:61.05ms
step:2020/2285 train_time:123327ms step_avg:61.05ms
step:2021/2285 train_time:123391ms step_avg:61.05ms
step:2022/2285 train_time:123451ms step_avg:61.05ms
step:2023/2285 train_time:123515ms step_avg:61.06ms
step:2024/2285 train_time:123575ms step_avg:61.05ms
step:2025/2285 train_time:123637ms step_avg:61.06ms
step:2026/2285 train_time:123697ms step_avg:61.05ms
step:2027/2285 train_time:123760ms step_avg:61.06ms
step:2028/2285 train_time:123820ms step_avg:61.06ms
step:2029/2285 train_time:123882ms step_avg:61.06ms
step:2030/2285 train_time:123942ms step_avg:61.06ms
step:2031/2285 train_time:124005ms step_avg:61.06ms
step:2032/2285 train_time:124066ms step_avg:61.06ms
step:2033/2285 train_time:124130ms step_avg:61.06ms
step:2034/2285 train_time:124190ms step_avg:61.06ms
step:2035/2285 train_time:124253ms step_avg:61.06ms
step:2036/2285 train_time:124314ms step_avg:61.06ms
step:2037/2285 train_time:124378ms step_avg:61.06ms
step:2038/2285 train_time:124438ms step_avg:61.06ms
step:2039/2285 train_time:124501ms step_avg:61.06ms
step:2040/2285 train_time:124563ms step_avg:61.06ms
step:2041/2285 train_time:124626ms step_avg:61.06ms
step:2042/2285 train_time:124686ms step_avg:61.06ms
step:2043/2285 train_time:124749ms step_avg:61.06ms
step:2044/2285 train_time:124809ms step_avg:61.06ms
step:2045/2285 train_time:124872ms step_avg:61.06ms
step:2046/2285 train_time:124932ms step_avg:61.06ms
step:2047/2285 train_time:124994ms step_avg:61.06ms
step:2048/2285 train_time:125054ms step_avg:61.06ms
step:2049/2285 train_time:125117ms step_avg:61.06ms
step:2050/2285 train_time:125178ms step_avg:61.06ms
step:2051/2285 train_time:125241ms step_avg:61.06ms
step:2052/2285 train_time:125302ms step_avg:61.06ms
step:2053/2285 train_time:125365ms step_avg:61.06ms
step:2054/2285 train_time:125426ms step_avg:61.06ms
step:2055/2285 train_time:125489ms step_avg:61.07ms
step:2056/2285 train_time:125550ms step_avg:61.07ms
step:2057/2285 train_time:125612ms step_avg:61.07ms
step:2058/2285 train_time:125672ms step_avg:61.07ms
step:2059/2285 train_time:125735ms step_avg:61.07ms
step:2060/2285 train_time:125795ms step_avg:61.07ms
step:2061/2285 train_time:125858ms step_avg:61.07ms
step:2062/2285 train_time:125918ms step_avg:61.07ms
step:2063/2285 train_time:125980ms step_avg:61.07ms
step:2064/2285 train_time:126041ms step_avg:61.07ms
step:2065/2285 train_time:126104ms step_avg:61.07ms
step:2066/2285 train_time:126166ms step_avg:61.07ms
step:2067/2285 train_time:126229ms step_avg:61.07ms
step:2068/2285 train_time:126290ms step_avg:61.07ms
step:2069/2285 train_time:126353ms step_avg:61.07ms
step:2070/2285 train_time:126415ms step_avg:61.07ms
step:2071/2285 train_time:126478ms step_avg:61.07ms
step:2072/2285 train_time:126538ms step_avg:61.07ms
step:2073/2285 train_time:126601ms step_avg:61.07ms
step:2074/2285 train_time:126662ms step_avg:61.07ms
step:2075/2285 train_time:126726ms step_avg:61.07ms
step:2076/2285 train_time:126787ms step_avg:61.07ms
step:2077/2285 train_time:126851ms step_avg:61.07ms
step:2078/2285 train_time:126911ms step_avg:61.07ms
step:2079/2285 train_time:126974ms step_avg:61.07ms
step:2080/2285 train_time:127033ms step_avg:61.07ms
step:2081/2285 train_time:127097ms step_avg:61.07ms
step:2082/2285 train_time:127158ms step_avg:61.07ms
step:2083/2285 train_time:127221ms step_avg:61.08ms
step:2084/2285 train_time:127282ms step_avg:61.08ms
step:2085/2285 train_time:127345ms step_avg:61.08ms
step:2086/2285 train_time:127406ms step_avg:61.08ms
step:2087/2285 train_time:127469ms step_avg:61.08ms
step:2088/2285 train_time:127529ms step_avg:61.08ms
step:2089/2285 train_time:127592ms step_avg:61.08ms
step:2090/2285 train_time:127652ms step_avg:61.08ms
step:2091/2285 train_time:127715ms step_avg:61.08ms
step:2092/2285 train_time:127775ms step_avg:61.08ms
step:2093/2285 train_time:127837ms step_avg:61.08ms
step:2094/2285 train_time:127898ms step_avg:61.08ms
step:2095/2285 train_time:127960ms step_avg:61.08ms
step:2096/2285 train_time:128021ms step_avg:61.08ms
step:2097/2285 train_time:128084ms step_avg:61.08ms
step:2098/2285 train_time:128144ms step_avg:61.08ms
step:2099/2285 train_time:128208ms step_avg:61.08ms
step:2100/2285 train_time:128269ms step_avg:61.08ms
step:2101/2285 train_time:128331ms step_avg:61.08ms
step:2102/2285 train_time:128392ms step_avg:61.08ms
step:2103/2285 train_time:128455ms step_avg:61.08ms
step:2104/2285 train_time:128516ms step_avg:61.08ms
step:2105/2285 train_time:128579ms step_avg:61.08ms
step:2106/2285 train_time:128639ms step_avg:61.08ms
step:2107/2285 train_time:128702ms step_avg:61.08ms
step:2108/2285 train_time:128764ms step_avg:61.08ms
step:2109/2285 train_time:128827ms step_avg:61.08ms
step:2110/2285 train_time:128887ms step_avg:61.08ms
step:2111/2285 train_time:128950ms step_avg:61.08ms
step:2112/2285 train_time:129010ms step_avg:61.08ms
step:2113/2285 train_time:129073ms step_avg:61.09ms
step:2114/2285 train_time:129133ms step_avg:61.08ms
step:2115/2285 train_time:129196ms step_avg:61.09ms
step:2116/2285 train_time:129256ms step_avg:61.09ms
step:2117/2285 train_time:129320ms step_avg:61.09ms
step:2118/2285 train_time:129381ms step_avg:61.09ms
step:2119/2285 train_time:129444ms step_avg:61.09ms
step:2120/2285 train_time:129505ms step_avg:61.09ms
step:2121/2285 train_time:129568ms step_avg:61.09ms
step:2122/2285 train_time:129629ms step_avg:61.09ms
step:2123/2285 train_time:129692ms step_avg:61.09ms
step:2124/2285 train_time:129753ms step_avg:61.09ms
step:2125/2285 train_time:129815ms step_avg:61.09ms
step:2126/2285 train_time:129875ms step_avg:61.09ms
step:2127/2285 train_time:129937ms step_avg:61.09ms
step:2128/2285 train_time:129998ms step_avg:61.09ms
step:2129/2285 train_time:130061ms step_avg:61.09ms
step:2130/2285 train_time:130121ms step_avg:61.09ms
step:2131/2285 train_time:130185ms step_avg:61.09ms
step:2132/2285 train_time:130246ms step_avg:61.09ms
step:2133/2285 train_time:130309ms step_avg:61.09ms
step:2134/2285 train_time:130369ms step_avg:61.09ms
step:2135/2285 train_time:130432ms step_avg:61.09ms
step:2136/2285 train_time:130492ms step_avg:61.09ms
step:2137/2285 train_time:130556ms step_avg:61.09ms
step:2138/2285 train_time:130617ms step_avg:61.09ms
step:2139/2285 train_time:130680ms step_avg:61.09ms
step:2140/2285 train_time:130741ms step_avg:61.09ms
step:2141/2285 train_time:130804ms step_avg:61.09ms
step:2142/2285 train_time:130865ms step_avg:61.09ms
step:2143/2285 train_time:130928ms step_avg:61.10ms
step:2144/2285 train_time:130988ms step_avg:61.10ms
step:2145/2285 train_time:131052ms step_avg:61.10ms
step:2146/2285 train_time:131111ms step_avg:61.10ms
step:2147/2285 train_time:131174ms step_avg:61.10ms
step:2148/2285 train_time:131235ms step_avg:61.10ms
step:2149/2285 train_time:131298ms step_avg:61.10ms
step:2150/2285 train_time:131359ms step_avg:61.10ms
step:2151/2285 train_time:131422ms step_avg:61.10ms
step:2152/2285 train_time:131483ms step_avg:61.10ms
step:2153/2285 train_time:131546ms step_avg:61.10ms
step:2154/2285 train_time:131607ms step_avg:61.10ms
step:2155/2285 train_time:131670ms step_avg:61.10ms
step:2156/2285 train_time:131730ms step_avg:61.10ms
step:2157/2285 train_time:131793ms step_avg:61.10ms
step:2158/2285 train_time:131854ms step_avg:61.10ms
step:2159/2285 train_time:131918ms step_avg:61.10ms
step:2160/2285 train_time:131978ms step_avg:61.10ms
step:2161/2285 train_time:132040ms step_avg:61.10ms
step:2162/2285 train_time:132101ms step_avg:61.10ms
step:2163/2285 train_time:132165ms step_avg:61.10ms
step:2164/2285 train_time:132225ms step_avg:61.10ms
step:2165/2285 train_time:132289ms step_avg:61.10ms
step:2166/2285 train_time:132349ms step_avg:61.10ms
step:2167/2285 train_time:132412ms step_avg:61.10ms
step:2168/2285 train_time:132472ms step_avg:61.10ms
step:2169/2285 train_time:132536ms step_avg:61.10ms
step:2170/2285 train_time:132596ms step_avg:61.10ms
step:2171/2285 train_time:132659ms step_avg:61.10ms
step:2172/2285 train_time:132719ms step_avg:61.10ms
step:2173/2285 train_time:132782ms step_avg:61.11ms
step:2174/2285 train_time:132843ms step_avg:61.11ms
step:2175/2285 train_time:132906ms step_avg:61.11ms
step:2176/2285 train_time:132967ms step_avg:61.11ms
step:2177/2285 train_time:133031ms step_avg:61.11ms
step:2178/2285 train_time:133091ms step_avg:61.11ms
step:2179/2285 train_time:133154ms step_avg:61.11ms
step:2180/2285 train_time:133215ms step_avg:61.11ms
step:2181/2285 train_time:133279ms step_avg:61.11ms
step:2182/2285 train_time:133339ms step_avg:61.11ms
step:2183/2285 train_time:133402ms step_avg:61.11ms
step:2184/2285 train_time:133463ms step_avg:61.11ms
step:2185/2285 train_time:133526ms step_avg:61.11ms
step:2186/2285 train_time:133587ms step_avg:61.11ms
step:2187/2285 train_time:133651ms step_avg:61.11ms
step:2188/2285 train_time:133711ms step_avg:61.11ms
step:2189/2285 train_time:133774ms step_avg:61.11ms
step:2190/2285 train_time:133834ms step_avg:61.11ms
step:2191/2285 train_time:133898ms step_avg:61.11ms
step:2192/2285 train_time:133959ms step_avg:61.11ms
step:2193/2285 train_time:134021ms step_avg:61.11ms
step:2194/2285 train_time:134082ms step_avg:61.11ms
step:2195/2285 train_time:134145ms step_avg:61.11ms
step:2196/2285 train_time:134206ms step_avg:61.11ms
step:2197/2285 train_time:134269ms step_avg:61.11ms
step:2198/2285 train_time:134329ms step_avg:61.11ms
step:2199/2285 train_time:134392ms step_avg:61.11ms
step:2200/2285 train_time:134452ms step_avg:61.11ms
step:2201/2285 train_time:134516ms step_avg:61.12ms
step:2202/2285 train_time:134576ms step_avg:61.12ms
step:2203/2285 train_time:134639ms step_avg:61.12ms
step:2204/2285 train_time:134700ms step_avg:61.12ms
step:2205/2285 train_time:134763ms step_avg:61.12ms
step:2206/2285 train_time:134823ms step_avg:61.12ms
step:2207/2285 train_time:134886ms step_avg:61.12ms
step:2208/2285 train_time:134947ms step_avg:61.12ms
step:2209/2285 train_time:135011ms step_avg:61.12ms
step:2210/2285 train_time:135071ms step_avg:61.12ms
step:2211/2285 train_time:135133ms step_avg:61.12ms
step:2212/2285 train_time:135194ms step_avg:61.12ms
step:2213/2285 train_time:135257ms step_avg:61.12ms
step:2214/2285 train_time:135318ms step_avg:61.12ms
step:2215/2285 train_time:135381ms step_avg:61.12ms
step:2216/2285 train_time:135441ms step_avg:61.12ms
step:2217/2285 train_time:135504ms step_avg:61.12ms
step:2218/2285 train_time:135565ms step_avg:61.12ms
step:2219/2285 train_time:135628ms step_avg:61.12ms
step:2220/2285 train_time:135689ms step_avg:61.12ms
step:2221/2285 train_time:135752ms step_avg:61.12ms
step:2222/2285 train_time:135812ms step_avg:61.12ms
step:2223/2285 train_time:135874ms step_avg:61.12ms
step:2224/2285 train_time:135935ms step_avg:61.12ms
step:2225/2285 train_time:135997ms step_avg:61.12ms
step:2226/2285 train_time:136058ms step_avg:61.12ms
step:2227/2285 train_time:136121ms step_avg:61.12ms
step:2228/2285 train_time:136181ms step_avg:61.12ms
step:2229/2285 train_time:136244ms step_avg:61.12ms
step:2230/2285 train_time:136305ms step_avg:61.12ms
step:2231/2285 train_time:136368ms step_avg:61.12ms
step:2232/2285 train_time:136429ms step_avg:61.12ms
step:2233/2285 train_time:136492ms step_avg:61.13ms
step:2234/2285 train_time:136552ms step_avg:61.12ms
step:2235/2285 train_time:136616ms step_avg:61.13ms
step:2236/2285 train_time:136677ms step_avg:61.13ms
step:2237/2285 train_time:136739ms step_avg:61.13ms
step:2238/2285 train_time:136800ms step_avg:61.13ms
step:2239/2285 train_time:136863ms step_avg:61.13ms
step:2240/2285 train_time:136923ms step_avg:61.13ms
step:2241/2285 train_time:136986ms step_avg:61.13ms
step:2242/2285 train_time:137047ms step_avg:61.13ms
step:2243/2285 train_time:137110ms step_avg:61.13ms
step:2244/2285 train_time:137170ms step_avg:61.13ms
step:2245/2285 train_time:137233ms step_avg:61.13ms
step:2246/2285 train_time:137294ms step_avg:61.13ms
step:2247/2285 train_time:137357ms step_avg:61.13ms
step:2248/2285 train_time:137417ms step_avg:61.13ms
step:2249/2285 train_time:137480ms step_avg:61.13ms
step:2250/2285 train_time:137541ms step_avg:61.13ms
step:2250/2285 val_loss:3.2819 train_time:137605ms step_avg:61.16ms
step:2251/2285 train_time:137629ms step_avg:61.14ms
step:2252/2285 train_time:137667ms step_avg:61.13ms
step:2253/2285 train_time:137732ms step_avg:61.13ms
step:2254/2285 train_time:137795ms step_avg:61.13ms
step:2255/2285 train_time:137860ms step_avg:61.14ms
step:2256/2285 train_time:137920ms step_avg:61.13ms
step:2257/2285 train_time:137982ms step_avg:61.14ms
step:2258/2285 train_time:138042ms step_avg:61.13ms
step:2259/2285 train_time:138104ms step_avg:61.13ms
step:2260/2285 train_time:138163ms step_avg:61.13ms
step:2261/2285 train_time:138225ms step_avg:61.13ms
step:2262/2285 train_time:138285ms step_avg:61.13ms
step:2263/2285 train_time:138347ms step_avg:61.13ms
step:2264/2285 train_time:138407ms step_avg:61.13ms
step:2265/2285 train_time:138470ms step_avg:61.13ms
step:2266/2285 train_time:138535ms step_avg:61.14ms
step:2267/2285 train_time:138602ms step_avg:61.14ms
step:2268/2285 train_time:138664ms step_avg:61.14ms
step:2269/2285 train_time:138728ms step_avg:61.14ms
step:2270/2285 train_time:138788ms step_avg:61.14ms
step:2271/2285 train_time:138852ms step_avg:61.14ms
step:2272/2285 train_time:138913ms step_avg:61.14ms
step:2273/2285 train_time:138976ms step_avg:61.14ms
step:2274/2285 train_time:139036ms step_avg:61.14ms
step:2275/2285 train_time:139099ms step_avg:61.14ms
step:2276/2285 train_time:139158ms step_avg:61.14ms
step:2277/2285 train_time:139220ms step_avg:61.14ms
step:2278/2285 train_time:139280ms step_avg:61.14ms
step:2279/2285 train_time:139342ms step_avg:61.14ms
step:2280/2285 train_time:139402ms step_avg:61.14ms
step:2281/2285 train_time:139466ms step_avg:61.14ms
step:2282/2285 train_time:139527ms step_avg:61.14ms
step:2283/2285 train_time:139592ms step_avg:61.14ms
step:2284/2285 train_time:139654ms step_avg:61.14ms
step:2285/2285 train_time:139718ms step_avg:61.15ms
step:2285/2285 val_loss:3.2754 train_time:139780ms step_avg:61.17ms
peak memory allocated: 29626 MiB reserved: 50528 MiB

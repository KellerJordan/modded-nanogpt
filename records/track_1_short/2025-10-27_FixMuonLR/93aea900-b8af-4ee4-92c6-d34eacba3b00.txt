import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Thu Nov  6 05:09:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     92537      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     92538      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     92539      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     92540      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     92541      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     92542      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     92543      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     92544      C   /root/.venv/bin/python3                         0MiB |
|    1   N/A  N/A     92538      C   /root/.venv/bin/python3                         0MiB |
|    2   N/A  N/A     92539      C   /root/.venv/bin/python3                         0MiB |
|    3   N/A  N/A     92540      C   /root/.venv/bin/python3                         0MiB |
|    4   N/A  N/A     92541      C   /root/.venv/bin/python3                         0MiB |
|    5   N/A  N/A     92542      C   /root/.venv/bin/python3                         0MiB |
|    6   N/A  N/A     92543      C   /root/.venv/bin/python3                         0MiB |
|    7   N/A  N/A     92544      C   /root/.venv/bin/python3                         0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:113ms step_avg:112.52ms
step:2/2285 train_time:137ms step_avg:68.74ms
step:3/2285 train_time:172ms step_avg:57.35ms
step:4/2285 train_time:228ms step_avg:57.09ms
step:5/2285 train_time:288ms step_avg:57.53ms
step:6/2285 train_time:346ms step_avg:57.71ms
step:7/2285 train_time:407ms step_avg:58.18ms
step:8/2285 train_time:466ms step_avg:58.22ms
step:9/2285 train_time:527ms step_avg:58.55ms
step:10/2285 train_time:585ms step_avg:58.53ms
step:11/2285 train_time:646ms step_avg:58.75ms
step:12/2285 train_time:705ms step_avg:58.74ms
step:13/2285 train_time:766ms step_avg:58.95ms
step:14/2285 train_time:825ms step_avg:58.96ms
step:15/2285 train_time:886ms step_avg:59.08ms
step:16/2285 train_time:945ms step_avg:59.05ms
step:17/2285 train_time:1008ms step_avg:59.28ms
step:18/2285 train_time:1070ms step_avg:59.47ms
step:19/2285 train_time:1136ms step_avg:59.79ms
step:20/2285 train_time:1196ms step_avg:59.78ms
step:21/2285 train_time:1258ms step_avg:59.88ms
step:22/2285 train_time:1317ms step_avg:59.88ms
step:23/2285 train_time:1379ms step_avg:59.97ms
step:24/2285 train_time:1439ms step_avg:59.94ms
step:25/2285 train_time:1500ms step_avg:60.02ms
step:26/2285 train_time:1560ms step_avg:60.01ms
step:27/2285 train_time:1622ms step_avg:60.07ms
step:28/2285 train_time:1681ms step_avg:60.04ms
step:29/2285 train_time:1742ms step_avg:60.07ms
step:30/2285 train_time:1801ms step_avg:60.04ms
step:31/2285 train_time:1863ms step_avg:60.11ms
step:32/2285 train_time:1922ms step_avg:60.07ms
step:33/2285 train_time:1985ms step_avg:60.16ms
step:34/2285 train_time:2045ms step_avg:60.16ms
step:35/2285 train_time:2109ms step_avg:60.27ms
step:36/2285 train_time:2169ms step_avg:60.26ms
step:37/2285 train_time:2232ms step_avg:60.34ms
step:38/2285 train_time:2291ms step_avg:60.29ms
step:39/2285 train_time:2353ms step_avg:60.33ms
step:40/2285 train_time:2412ms step_avg:60.30ms
step:41/2285 train_time:2473ms step_avg:60.33ms
step:42/2285 train_time:2533ms step_avg:60.31ms
step:43/2285 train_time:2595ms step_avg:60.34ms
step:44/2285 train_time:2654ms step_avg:60.32ms
step:45/2285 train_time:2715ms step_avg:60.34ms
step:46/2285 train_time:2774ms step_avg:60.31ms
step:47/2285 train_time:2836ms step_avg:60.34ms
step:48/2285 train_time:2896ms step_avg:60.33ms
step:49/2285 train_time:2958ms step_avg:60.37ms
step:50/2285 train_time:3019ms step_avg:60.37ms
step:51/2285 train_time:3080ms step_avg:60.40ms
step:52/2285 train_time:3140ms step_avg:60.39ms
step:53/2285 train_time:3203ms step_avg:60.44ms
step:54/2285 train_time:3264ms step_avg:60.44ms
step:55/2285 train_time:3326ms step_avg:60.48ms
step:56/2285 train_time:3385ms step_avg:60.45ms
step:57/2285 train_time:3447ms step_avg:60.48ms
step:58/2285 train_time:3507ms step_avg:60.46ms
step:59/2285 train_time:3569ms step_avg:60.49ms
step:60/2285 train_time:3628ms step_avg:60.46ms
step:61/2285 train_time:3689ms step_avg:60.48ms
step:62/2285 train_time:3748ms step_avg:60.45ms
step:63/2285 train_time:3809ms step_avg:60.47ms
step:64/2285 train_time:3868ms step_avg:60.44ms
step:65/2285 train_time:3930ms step_avg:60.46ms
step:66/2285 train_time:3988ms step_avg:60.43ms
step:67/2285 train_time:4050ms step_avg:60.45ms
step:68/2285 train_time:4109ms step_avg:60.43ms
step:69/2285 train_time:4171ms step_avg:60.45ms
step:70/2285 train_time:4230ms step_avg:60.43ms
step:71/2285 train_time:4291ms step_avg:60.44ms
step:72/2285 train_time:4351ms step_avg:60.43ms
step:73/2285 train_time:4412ms step_avg:60.43ms
step:74/2285 train_time:4470ms step_avg:60.41ms
step:75/2285 train_time:4532ms step_avg:60.43ms
step:76/2285 train_time:4591ms step_avg:60.41ms
step:77/2285 train_time:4652ms step_avg:60.42ms
step:78/2285 train_time:4712ms step_avg:60.40ms
step:79/2285 train_time:4773ms step_avg:60.42ms
step:80/2285 train_time:4833ms step_avg:60.41ms
step:81/2285 train_time:4894ms step_avg:60.42ms
step:82/2285 train_time:4953ms step_avg:60.40ms
step:83/2285 train_time:5015ms step_avg:60.42ms
step:84/2285 train_time:5074ms step_avg:60.41ms
step:85/2285 train_time:5136ms step_avg:60.43ms
step:86/2285 train_time:5195ms step_avg:60.41ms
step:87/2285 train_time:5258ms step_avg:60.43ms
step:88/2285 train_time:5318ms step_avg:60.43ms
step:89/2285 train_time:5380ms step_avg:60.45ms
step:90/2285 train_time:5439ms step_avg:60.43ms
step:91/2285 train_time:5502ms step_avg:60.46ms
step:92/2285 train_time:5561ms step_avg:60.44ms
step:93/2285 train_time:5623ms step_avg:60.46ms
step:94/2285 train_time:5682ms step_avg:60.45ms
step:95/2285 train_time:5744ms step_avg:60.46ms
step:96/2285 train_time:5804ms step_avg:60.45ms
step:97/2285 train_time:5866ms step_avg:60.47ms
step:98/2285 train_time:5925ms step_avg:60.46ms
step:99/2285 train_time:5987ms step_avg:60.48ms
step:100/2285 train_time:6046ms step_avg:60.46ms
step:101/2285 train_time:6109ms step_avg:60.48ms
step:102/2285 train_time:6167ms step_avg:60.46ms
step:103/2285 train_time:6229ms step_avg:60.47ms
step:104/2285 train_time:6287ms step_avg:60.46ms
step:105/2285 train_time:6349ms step_avg:60.47ms
step:106/2285 train_time:6408ms step_avg:60.45ms
step:107/2285 train_time:6469ms step_avg:60.45ms
step:108/2285 train_time:6529ms step_avg:60.45ms
step:109/2285 train_time:6590ms step_avg:60.46ms
step:110/2285 train_time:6648ms step_avg:60.44ms
step:111/2285 train_time:6710ms step_avg:60.45ms
step:112/2285 train_time:6769ms step_avg:60.43ms
step:113/2285 train_time:6831ms step_avg:60.45ms
step:114/2285 train_time:6889ms step_avg:60.43ms
step:115/2285 train_time:6950ms step_avg:60.43ms
step:116/2285 train_time:7008ms step_avg:60.42ms
step:117/2285 train_time:7070ms step_avg:60.43ms
step:118/2285 train_time:7129ms step_avg:60.41ms
step:119/2285 train_time:7190ms step_avg:60.42ms
step:120/2285 train_time:7248ms step_avg:60.40ms
step:121/2285 train_time:7309ms step_avg:60.41ms
step:122/2285 train_time:7368ms step_avg:60.40ms
step:123/2285 train_time:7430ms step_avg:60.41ms
step:124/2285 train_time:7489ms step_avg:60.39ms
step:125/2285 train_time:7550ms step_avg:60.40ms
step:126/2285 train_time:7608ms step_avg:60.38ms
step:127/2285 train_time:7669ms step_avg:60.39ms
step:128/2285 train_time:7728ms step_avg:60.37ms
step:129/2285 train_time:7789ms step_avg:60.38ms
step:130/2285 train_time:7848ms step_avg:60.37ms
step:131/2285 train_time:7909ms step_avg:60.37ms
step:132/2285 train_time:7968ms step_avg:60.36ms
step:133/2285 train_time:8029ms step_avg:60.37ms
step:134/2285 train_time:8088ms step_avg:60.36ms
step:135/2285 train_time:8149ms step_avg:60.36ms
step:136/2285 train_time:8207ms step_avg:60.34ms
step:137/2285 train_time:8268ms step_avg:60.35ms
step:138/2285 train_time:8327ms step_avg:60.34ms
step:139/2285 train_time:8388ms step_avg:60.35ms
step:140/2285 train_time:8447ms step_avg:60.34ms
step:141/2285 train_time:8509ms step_avg:60.35ms
step:142/2285 train_time:8567ms step_avg:60.33ms
step:143/2285 train_time:8629ms step_avg:60.34ms
step:144/2285 train_time:8687ms step_avg:60.33ms
step:145/2285 train_time:8749ms step_avg:60.34ms
step:146/2285 train_time:8808ms step_avg:60.33ms
step:147/2285 train_time:8869ms step_avg:60.33ms
step:148/2285 train_time:8928ms step_avg:60.32ms
step:149/2285 train_time:8989ms step_avg:60.33ms
step:150/2285 train_time:9047ms step_avg:60.31ms
step:151/2285 train_time:9109ms step_avg:60.32ms
step:152/2285 train_time:9167ms step_avg:60.31ms
step:153/2285 train_time:9228ms step_avg:60.32ms
step:154/2285 train_time:9287ms step_avg:60.31ms
step:155/2285 train_time:9350ms step_avg:60.32ms
step:156/2285 train_time:9407ms step_avg:60.30ms
step:157/2285 train_time:9468ms step_avg:60.31ms
step:158/2285 train_time:9527ms step_avg:60.30ms
step:159/2285 train_time:9589ms step_avg:60.31ms
step:160/2285 train_time:9647ms step_avg:60.29ms
step:161/2285 train_time:9708ms step_avg:60.30ms
step:162/2285 train_time:9767ms step_avg:60.29ms
step:163/2285 train_time:9828ms step_avg:60.30ms
step:164/2285 train_time:9886ms step_avg:60.28ms
step:165/2285 train_time:9947ms step_avg:60.29ms
step:166/2285 train_time:10006ms step_avg:60.28ms
step:167/2285 train_time:10068ms step_avg:60.28ms
step:168/2285 train_time:10126ms step_avg:60.28ms
step:169/2285 train_time:10188ms step_avg:60.28ms
step:170/2285 train_time:10246ms step_avg:60.27ms
step:171/2285 train_time:10307ms step_avg:60.28ms
step:172/2285 train_time:10366ms step_avg:60.27ms
step:173/2285 train_time:10428ms step_avg:60.28ms
step:174/2285 train_time:10487ms step_avg:60.27ms
step:175/2285 train_time:10548ms step_avg:60.27ms
step:176/2285 train_time:10606ms step_avg:60.26ms
step:177/2285 train_time:10668ms step_avg:60.27ms
step:178/2285 train_time:10726ms step_avg:60.26ms
step:179/2285 train_time:10788ms step_avg:60.27ms
step:180/2285 train_time:10846ms step_avg:60.26ms
step:181/2285 train_time:10908ms step_avg:60.26ms
step:182/2285 train_time:10966ms step_avg:60.25ms
step:183/2285 train_time:11027ms step_avg:60.26ms
step:184/2285 train_time:11086ms step_avg:60.25ms
step:185/2285 train_time:11147ms step_avg:60.26ms
step:186/2285 train_time:11206ms step_avg:60.25ms
step:187/2285 train_time:11268ms step_avg:60.26ms
step:188/2285 train_time:11327ms step_avg:60.25ms
step:189/2285 train_time:11388ms step_avg:60.25ms
step:190/2285 train_time:11446ms step_avg:60.24ms
step:191/2285 train_time:11509ms step_avg:60.26ms
step:192/2285 train_time:11567ms step_avg:60.24ms
step:193/2285 train_time:11628ms step_avg:60.25ms
step:194/2285 train_time:11687ms step_avg:60.24ms
step:195/2285 train_time:11748ms step_avg:60.25ms
step:196/2285 train_time:11807ms step_avg:60.24ms
step:197/2285 train_time:11868ms step_avg:60.24ms
step:198/2285 train_time:11927ms step_avg:60.24ms
step:199/2285 train_time:11988ms step_avg:60.24ms
step:200/2285 train_time:12047ms step_avg:60.24ms
step:201/2285 train_time:12108ms step_avg:60.24ms
step:202/2285 train_time:12166ms step_avg:60.23ms
step:203/2285 train_time:12228ms step_avg:60.24ms
step:204/2285 train_time:12286ms step_avg:60.23ms
step:205/2285 train_time:12347ms step_avg:60.23ms
step:206/2285 train_time:12406ms step_avg:60.22ms
step:207/2285 train_time:12467ms step_avg:60.23ms
step:208/2285 train_time:12527ms step_avg:60.22ms
step:209/2285 train_time:12588ms step_avg:60.23ms
step:210/2285 train_time:12646ms step_avg:60.22ms
step:211/2285 train_time:12708ms step_avg:60.23ms
step:212/2285 train_time:12767ms step_avg:60.22ms
step:213/2285 train_time:12828ms step_avg:60.23ms
step:214/2285 train_time:12886ms step_avg:60.22ms
step:215/2285 train_time:12948ms step_avg:60.22ms
step:216/2285 train_time:13006ms step_avg:60.21ms
step:217/2285 train_time:13067ms step_avg:60.22ms
step:218/2285 train_time:13126ms step_avg:60.21ms
step:219/2285 train_time:13187ms step_avg:60.22ms
step:220/2285 train_time:13246ms step_avg:60.21ms
step:221/2285 train_time:13306ms step_avg:60.21ms
step:222/2285 train_time:13365ms step_avg:60.20ms
step:223/2285 train_time:13427ms step_avg:60.21ms
step:224/2285 train_time:13486ms step_avg:60.21ms
step:225/2285 train_time:13548ms step_avg:60.21ms
step:226/2285 train_time:13606ms step_avg:60.20ms
step:227/2285 train_time:13667ms step_avg:60.21ms
step:228/2285 train_time:13726ms step_avg:60.20ms
step:229/2285 train_time:13788ms step_avg:60.21ms
step:230/2285 train_time:13846ms step_avg:60.20ms
step:231/2285 train_time:13908ms step_avg:60.21ms
step:232/2285 train_time:13966ms step_avg:60.20ms
step:233/2285 train_time:14028ms step_avg:60.20ms
step:234/2285 train_time:14087ms step_avg:60.20ms
step:235/2285 train_time:14148ms step_avg:60.20ms
step:236/2285 train_time:14206ms step_avg:60.19ms
step:237/2285 train_time:14267ms step_avg:60.20ms
step:238/2285 train_time:14326ms step_avg:60.19ms
step:239/2285 train_time:14387ms step_avg:60.20ms
step:240/2285 train_time:14446ms step_avg:60.19ms
step:241/2285 train_time:14507ms step_avg:60.20ms
step:242/2285 train_time:14566ms step_avg:60.19ms
step:243/2285 train_time:14628ms step_avg:60.20ms
step:244/2285 train_time:14687ms step_avg:60.19ms
step:245/2285 train_time:14748ms step_avg:60.20ms
step:246/2285 train_time:14807ms step_avg:60.19ms
step:247/2285 train_time:14868ms step_avg:60.20ms
step:248/2285 train_time:14927ms step_avg:60.19ms
step:249/2285 train_time:14989ms step_avg:60.19ms
step:250/2285 train_time:15047ms step_avg:60.19ms
step:250/2285 val_loss:4.0756 train_time:15109ms step_avg:60.44ms
step:251/2285 train_time:15132ms step_avg:60.29ms
step:252/2285 train_time:15173ms step_avg:60.21ms
step:253/2285 train_time:15238ms step_avg:60.23ms
step:254/2285 train_time:15302ms step_avg:60.24ms
step:255/2285 train_time:15366ms step_avg:60.26ms
step:256/2285 train_time:15426ms step_avg:60.26ms
step:257/2285 train_time:15487ms step_avg:60.26ms
step:258/2285 train_time:15545ms step_avg:60.25ms
step:259/2285 train_time:15607ms step_avg:60.26ms
step:260/2285 train_time:15665ms step_avg:60.25ms
step:261/2285 train_time:15726ms step_avg:60.25ms
step:262/2285 train_time:15784ms step_avg:60.24ms
step:263/2285 train_time:15845ms step_avg:60.25ms
step:264/2285 train_time:15903ms step_avg:60.24ms
step:265/2285 train_time:15963ms step_avg:60.24ms
step:266/2285 train_time:16022ms step_avg:60.23ms
step:267/2285 train_time:16083ms step_avg:60.24ms
step:268/2285 train_time:16143ms step_avg:60.23ms
step:269/2285 train_time:16207ms step_avg:60.25ms
step:270/2285 train_time:16268ms step_avg:60.25ms
step:271/2285 train_time:16332ms step_avg:60.26ms
step:272/2285 train_time:16391ms step_avg:60.26ms
step:273/2285 train_time:16453ms step_avg:60.27ms
step:274/2285 train_time:16511ms step_avg:60.26ms
step:275/2285 train_time:16573ms step_avg:60.26ms
step:276/2285 train_time:16631ms step_avg:60.26ms
step:277/2285 train_time:16692ms step_avg:60.26ms
step:278/2285 train_time:16750ms step_avg:60.25ms
step:279/2285 train_time:16811ms step_avg:60.25ms
step:280/2285 train_time:16869ms step_avg:60.25ms
step:281/2285 train_time:16930ms step_avg:60.25ms
step:282/2285 train_time:16988ms step_avg:60.24ms
step:283/2285 train_time:17049ms step_avg:60.24ms
step:284/2285 train_time:17108ms step_avg:60.24ms
step:285/2285 train_time:17171ms step_avg:60.25ms
step:286/2285 train_time:17230ms step_avg:60.25ms
step:287/2285 train_time:17293ms step_avg:60.25ms
step:288/2285 train_time:17352ms step_avg:60.25ms
step:289/2285 train_time:17414ms step_avg:60.26ms
step:290/2285 train_time:17473ms step_avg:60.25ms
step:291/2285 train_time:17534ms step_avg:60.26ms
step:292/2285 train_time:17593ms step_avg:60.25ms
step:293/2285 train_time:17654ms step_avg:60.25ms
step:294/2285 train_time:17712ms step_avg:60.25ms
step:295/2285 train_time:17773ms step_avg:60.25ms
step:296/2285 train_time:17832ms step_avg:60.24ms
step:297/2285 train_time:17893ms step_avg:60.24ms
step:298/2285 train_time:17951ms step_avg:60.24ms
step:299/2285 train_time:18012ms step_avg:60.24ms
step:300/2285 train_time:18071ms step_avg:60.24ms
step:301/2285 train_time:18132ms step_avg:60.24ms
step:302/2285 train_time:18191ms step_avg:60.23ms
step:303/2285 train_time:18253ms step_avg:60.24ms
step:304/2285 train_time:18312ms step_avg:60.24ms
step:305/2285 train_time:18374ms step_avg:60.24ms
step:306/2285 train_time:18432ms step_avg:60.24ms
step:307/2285 train_time:18494ms step_avg:60.24ms
step:308/2285 train_time:18553ms step_avg:60.24ms
step:309/2285 train_time:18614ms step_avg:60.24ms
step:310/2285 train_time:18673ms step_avg:60.24ms
step:311/2285 train_time:18734ms step_avg:60.24ms
step:312/2285 train_time:18793ms step_avg:60.23ms
step:313/2285 train_time:18853ms step_avg:60.23ms
step:314/2285 train_time:18912ms step_avg:60.23ms
step:315/2285 train_time:18973ms step_avg:60.23ms
step:316/2285 train_time:19031ms step_avg:60.23ms
step:317/2285 train_time:19093ms step_avg:60.23ms
step:318/2285 train_time:19152ms step_avg:60.23ms
step:319/2285 train_time:19213ms step_avg:60.23ms
step:320/2285 train_time:19272ms step_avg:60.23ms
step:321/2285 train_time:19334ms step_avg:60.23ms
step:322/2285 train_time:19393ms step_avg:60.23ms
step:323/2285 train_time:19455ms step_avg:60.23ms
step:324/2285 train_time:19513ms step_avg:60.23ms
step:325/2285 train_time:19575ms step_avg:60.23ms
step:326/2285 train_time:19633ms step_avg:60.22ms
step:327/2285 train_time:19694ms step_avg:60.23ms
step:328/2285 train_time:19752ms step_avg:60.22ms
step:329/2285 train_time:19813ms step_avg:60.22ms
step:330/2285 train_time:19872ms step_avg:60.22ms
step:331/2285 train_time:19933ms step_avg:60.22ms
step:332/2285 train_time:19992ms step_avg:60.22ms
step:333/2285 train_time:20053ms step_avg:60.22ms
step:334/2285 train_time:20112ms step_avg:60.22ms
step:335/2285 train_time:20174ms step_avg:60.22ms
step:336/2285 train_time:20233ms step_avg:60.22ms
step:337/2285 train_time:20295ms step_avg:60.22ms
step:338/2285 train_time:20353ms step_avg:60.22ms
step:339/2285 train_time:20414ms step_avg:60.22ms
step:340/2285 train_time:20473ms step_avg:60.22ms
step:341/2285 train_time:20535ms step_avg:60.22ms
step:342/2285 train_time:20593ms step_avg:60.21ms
step:343/2285 train_time:20654ms step_avg:60.22ms
step:344/2285 train_time:20712ms step_avg:60.21ms
step:345/2285 train_time:20773ms step_avg:60.21ms
step:346/2285 train_time:20832ms step_avg:60.21ms
step:347/2285 train_time:20893ms step_avg:60.21ms
step:348/2285 train_time:20951ms step_avg:60.21ms
step:349/2285 train_time:21013ms step_avg:60.21ms
step:350/2285 train_time:21072ms step_avg:60.20ms
step:351/2285 train_time:21132ms step_avg:60.21ms
step:352/2285 train_time:21191ms step_avg:60.20ms
step:353/2285 train_time:21253ms step_avg:60.21ms
step:354/2285 train_time:21312ms step_avg:60.20ms
step:355/2285 train_time:21373ms step_avg:60.21ms
step:356/2285 train_time:21431ms step_avg:60.20ms
step:357/2285 train_time:21493ms step_avg:60.20ms
step:358/2285 train_time:21552ms step_avg:60.20ms
step:359/2285 train_time:21612ms step_avg:60.20ms
step:360/2285 train_time:21671ms step_avg:60.20ms
step:361/2285 train_time:21732ms step_avg:60.20ms
step:362/2285 train_time:21791ms step_avg:60.20ms
step:363/2285 train_time:21852ms step_avg:60.20ms
step:364/2285 train_time:21911ms step_avg:60.19ms
step:365/2285 train_time:21972ms step_avg:60.20ms
step:366/2285 train_time:22030ms step_avg:60.19ms
step:367/2285 train_time:22092ms step_avg:60.20ms
step:368/2285 train_time:22150ms step_avg:60.19ms
step:369/2285 train_time:22211ms step_avg:60.19ms
step:370/2285 train_time:22270ms step_avg:60.19ms
step:371/2285 train_time:22332ms step_avg:60.19ms
step:372/2285 train_time:22391ms step_avg:60.19ms
step:373/2285 train_time:22452ms step_avg:60.19ms
step:374/2285 train_time:22511ms step_avg:60.19ms
step:375/2285 train_time:22573ms step_avg:60.19ms
step:376/2285 train_time:22632ms step_avg:60.19ms
step:377/2285 train_time:22693ms step_avg:60.19ms
step:378/2285 train_time:22752ms step_avg:60.19ms
step:379/2285 train_time:22813ms step_avg:60.19ms
step:380/2285 train_time:22872ms step_avg:60.19ms
step:381/2285 train_time:22934ms step_avg:60.19ms
step:382/2285 train_time:22992ms step_avg:60.19ms
step:383/2285 train_time:23053ms step_avg:60.19ms
step:384/2285 train_time:23112ms step_avg:60.19ms
step:385/2285 train_time:23174ms step_avg:60.19ms
step:386/2285 train_time:23232ms step_avg:60.19ms
step:387/2285 train_time:23293ms step_avg:60.19ms
step:388/2285 train_time:23352ms step_avg:60.19ms
step:389/2285 train_time:23413ms step_avg:60.19ms
step:390/2285 train_time:23472ms step_avg:60.19ms
step:391/2285 train_time:23533ms step_avg:60.19ms
step:392/2285 train_time:23592ms step_avg:60.18ms
step:393/2285 train_time:23653ms step_avg:60.19ms
step:394/2285 train_time:23711ms step_avg:60.18ms
step:395/2285 train_time:23773ms step_avg:60.18ms
step:396/2285 train_time:23832ms step_avg:60.18ms
step:397/2285 train_time:23894ms step_avg:60.19ms
step:398/2285 train_time:23951ms step_avg:60.18ms
step:399/2285 train_time:24013ms step_avg:60.18ms
step:400/2285 train_time:24072ms step_avg:60.18ms
step:401/2285 train_time:24134ms step_avg:60.18ms
step:402/2285 train_time:24192ms step_avg:60.18ms
step:403/2285 train_time:24253ms step_avg:60.18ms
step:404/2285 train_time:24311ms step_avg:60.18ms
step:405/2285 train_time:24373ms step_avg:60.18ms
step:406/2285 train_time:24432ms step_avg:60.18ms
step:407/2285 train_time:24493ms step_avg:60.18ms
step:408/2285 train_time:24551ms step_avg:60.17ms
step:409/2285 train_time:24613ms step_avg:60.18ms
step:410/2285 train_time:24672ms step_avg:60.17ms
step:411/2285 train_time:24733ms step_avg:60.18ms
step:412/2285 train_time:24791ms step_avg:60.17ms
step:413/2285 train_time:24853ms step_avg:60.18ms
step:414/2285 train_time:24912ms step_avg:60.17ms
step:415/2285 train_time:24973ms step_avg:60.18ms
step:416/2285 train_time:25032ms step_avg:60.17ms
step:417/2285 train_time:25093ms step_avg:60.18ms
step:418/2285 train_time:25151ms step_avg:60.17ms
step:419/2285 train_time:25213ms step_avg:60.17ms
step:420/2285 train_time:25272ms step_avg:60.17ms
step:421/2285 train_time:25333ms step_avg:60.17ms
step:422/2285 train_time:25392ms step_avg:60.17ms
step:423/2285 train_time:25452ms step_avg:60.17ms
step:424/2285 train_time:25511ms step_avg:60.17ms
step:425/2285 train_time:25572ms step_avg:60.17ms
step:426/2285 train_time:25631ms step_avg:60.17ms
step:427/2285 train_time:25692ms step_avg:60.17ms
step:428/2285 train_time:25751ms step_avg:60.17ms
step:429/2285 train_time:25813ms step_avg:60.17ms
step:430/2285 train_time:25871ms step_avg:60.17ms
step:431/2285 train_time:25933ms step_avg:60.17ms
step:432/2285 train_time:25991ms step_avg:60.16ms
step:433/2285 train_time:26052ms step_avg:60.17ms
step:434/2285 train_time:26111ms step_avg:60.16ms
step:435/2285 train_time:26173ms step_avg:60.17ms
step:436/2285 train_time:26231ms step_avg:60.16ms
step:437/2285 train_time:26293ms step_avg:60.17ms
step:438/2285 train_time:26352ms step_avg:60.16ms
step:439/2285 train_time:26413ms step_avg:60.17ms
step:440/2285 train_time:26472ms step_avg:60.16ms
step:441/2285 train_time:26534ms step_avg:60.17ms
step:442/2285 train_time:26592ms step_avg:60.16ms
step:443/2285 train_time:26653ms step_avg:60.16ms
step:444/2285 train_time:26712ms step_avg:60.16ms
step:445/2285 train_time:26773ms step_avg:60.16ms
step:446/2285 train_time:26832ms step_avg:60.16ms
step:447/2285 train_time:26893ms step_avg:60.16ms
step:448/2285 train_time:26952ms step_avg:60.16ms
step:449/2285 train_time:27013ms step_avg:60.16ms
step:450/2285 train_time:27072ms step_avg:60.16ms
step:451/2285 train_time:27133ms step_avg:60.16ms
step:452/2285 train_time:27192ms step_avg:60.16ms
step:453/2285 train_time:27253ms step_avg:60.16ms
step:454/2285 train_time:27311ms step_avg:60.16ms
step:455/2285 train_time:27373ms step_avg:60.16ms
step:456/2285 train_time:27431ms step_avg:60.16ms
step:457/2285 train_time:27492ms step_avg:60.16ms
step:458/2285 train_time:27550ms step_avg:60.15ms
step:459/2285 train_time:27612ms step_avg:60.16ms
step:460/2285 train_time:27671ms step_avg:60.15ms
step:461/2285 train_time:27732ms step_avg:60.16ms
step:462/2285 train_time:27791ms step_avg:60.15ms
step:463/2285 train_time:27852ms step_avg:60.16ms
step:464/2285 train_time:27911ms step_avg:60.15ms
step:465/2285 train_time:27973ms step_avg:60.16ms
step:466/2285 train_time:28031ms step_avg:60.15ms
step:467/2285 train_time:28093ms step_avg:60.16ms
step:468/2285 train_time:28151ms step_avg:60.15ms
step:469/2285 train_time:28213ms step_avg:60.16ms
step:470/2285 train_time:28271ms step_avg:60.15ms
step:471/2285 train_time:28333ms step_avg:60.15ms
step:472/2285 train_time:28391ms step_avg:60.15ms
step:473/2285 train_time:28452ms step_avg:60.15ms
step:474/2285 train_time:28511ms step_avg:60.15ms
step:475/2285 train_time:28572ms step_avg:60.15ms
step:476/2285 train_time:28631ms step_avg:60.15ms
step:477/2285 train_time:28693ms step_avg:60.15ms
step:478/2285 train_time:28751ms step_avg:60.15ms
step:479/2285 train_time:28813ms step_avg:60.15ms
step:480/2285 train_time:28871ms step_avg:60.15ms
step:481/2285 train_time:28933ms step_avg:60.15ms
step:482/2285 train_time:28992ms step_avg:60.15ms
step:483/2285 train_time:29053ms step_avg:60.15ms
step:484/2285 train_time:29112ms step_avg:60.15ms
step:485/2285 train_time:29173ms step_avg:60.15ms
step:486/2285 train_time:29231ms step_avg:60.15ms
step:487/2285 train_time:29292ms step_avg:60.15ms
step:488/2285 train_time:29351ms step_avg:60.14ms
step:489/2285 train_time:29412ms step_avg:60.15ms
step:490/2285 train_time:29471ms step_avg:60.15ms
step:491/2285 train_time:29532ms step_avg:60.15ms
step:492/2285 train_time:29591ms step_avg:60.14ms
step:493/2285 train_time:29652ms step_avg:60.15ms
step:494/2285 train_time:29711ms step_avg:60.14ms
step:495/2285 train_time:29773ms step_avg:60.15ms
step:496/2285 train_time:29831ms step_avg:60.14ms
step:497/2285 train_time:29892ms step_avg:60.15ms
step:498/2285 train_time:29951ms step_avg:60.14ms
step:499/2285 train_time:30013ms step_avg:60.15ms
step:500/2285 train_time:30072ms step_avg:60.14ms
step:500/2285 val_loss:3.8114 train_time:30134ms step_avg:60.27ms
step:501/2285 train_time:30152ms step_avg:60.18ms
step:502/2285 train_time:30195ms step_avg:60.15ms
step:503/2285 train_time:30258ms step_avg:60.16ms
step:504/2285 train_time:30320ms step_avg:60.16ms
step:505/2285 train_time:30382ms step_avg:60.16ms
step:506/2285 train_time:30441ms step_avg:60.16ms
step:507/2285 train_time:30502ms step_avg:60.16ms
step:508/2285 train_time:30560ms step_avg:60.16ms
step:509/2285 train_time:30622ms step_avg:60.16ms
step:510/2285 train_time:30680ms step_avg:60.16ms
step:511/2285 train_time:30741ms step_avg:60.16ms
step:512/2285 train_time:30799ms step_avg:60.15ms
step:513/2285 train_time:30860ms step_avg:60.16ms
step:514/2285 train_time:30919ms step_avg:60.15ms
step:515/2285 train_time:30980ms step_avg:60.16ms
step:516/2285 train_time:31039ms step_avg:60.15ms
step:517/2285 train_time:31103ms step_avg:60.16ms
step:518/2285 train_time:31165ms step_avg:60.16ms
step:519/2285 train_time:31228ms step_avg:60.17ms
step:520/2285 train_time:31288ms step_avg:60.17ms
step:521/2285 train_time:31350ms step_avg:60.17ms
step:522/2285 train_time:31409ms step_avg:60.17ms
step:523/2285 train_time:31471ms step_avg:60.17ms
step:524/2285 train_time:31530ms step_avg:60.17ms
step:525/2285 train_time:31591ms step_avg:60.17ms
step:526/2285 train_time:31649ms step_avg:60.17ms
step:527/2285 train_time:31710ms step_avg:60.17ms
step:528/2285 train_time:31768ms step_avg:60.17ms
step:529/2285 train_time:31830ms step_avg:60.17ms
step:530/2285 train_time:31888ms step_avg:60.17ms
step:531/2285 train_time:31950ms step_avg:60.17ms
step:532/2285 train_time:32009ms step_avg:60.17ms
step:533/2285 train_time:32071ms step_avg:60.17ms
step:534/2285 train_time:32131ms step_avg:60.17ms
step:535/2285 train_time:32192ms step_avg:60.17ms
step:536/2285 train_time:32251ms step_avg:60.17ms
step:537/2285 train_time:32313ms step_avg:60.17ms
step:538/2285 train_time:32371ms step_avg:60.17ms
step:539/2285 train_time:32433ms step_avg:60.17ms
step:540/2285 train_time:32491ms step_avg:60.17ms
step:541/2285 train_time:32552ms step_avg:60.17ms
step:542/2285 train_time:32610ms step_avg:60.17ms
step:543/2285 train_time:32672ms step_avg:60.17ms
step:544/2285 train_time:32731ms step_avg:60.17ms
step:545/2285 train_time:32792ms step_avg:60.17ms
step:546/2285 train_time:32850ms step_avg:60.16ms
step:547/2285 train_time:32911ms step_avg:60.17ms
step:548/2285 train_time:32970ms step_avg:60.16ms
step:549/2285 train_time:33031ms step_avg:60.17ms
step:550/2285 train_time:33090ms step_avg:60.16ms
step:551/2285 train_time:33151ms step_avg:60.17ms
step:552/2285 train_time:33210ms step_avg:60.16ms
step:553/2285 train_time:33272ms step_avg:60.17ms
step:554/2285 train_time:33331ms step_avg:60.16ms
step:555/2285 train_time:33392ms step_avg:60.17ms
step:556/2285 train_time:33450ms step_avg:60.16ms
step:557/2285 train_time:33512ms step_avg:60.17ms
step:558/2285 train_time:33570ms step_avg:60.16ms
step:559/2285 train_time:33632ms step_avg:60.16ms
step:560/2285 train_time:33690ms step_avg:60.16ms
step:561/2285 train_time:33751ms step_avg:60.16ms
step:562/2285 train_time:33809ms step_avg:60.16ms
step:563/2285 train_time:33871ms step_avg:60.16ms
step:564/2285 train_time:33930ms step_avg:60.16ms
step:565/2285 train_time:33991ms step_avg:60.16ms
step:566/2285 train_time:34050ms step_avg:60.16ms
step:567/2285 train_time:34111ms step_avg:60.16ms
step:568/2285 train_time:34170ms step_avg:60.16ms
step:569/2285 train_time:34232ms step_avg:60.16ms
step:570/2285 train_time:34290ms step_avg:60.16ms
step:571/2285 train_time:34351ms step_avg:60.16ms
step:572/2285 train_time:34410ms step_avg:60.16ms
step:573/2285 train_time:34471ms step_avg:60.16ms
step:574/2285 train_time:34530ms step_avg:60.16ms
step:575/2285 train_time:34591ms step_avg:60.16ms
step:576/2285 train_time:34650ms step_avg:60.16ms
step:577/2285 train_time:34711ms step_avg:60.16ms
step:578/2285 train_time:34769ms step_avg:60.15ms
step:579/2285 train_time:34830ms step_avg:60.16ms
step:580/2285 train_time:34889ms step_avg:60.15ms
step:581/2285 train_time:34950ms step_avg:60.16ms
step:582/2285 train_time:35009ms step_avg:60.15ms
step:583/2285 train_time:35070ms step_avg:60.15ms
step:584/2285 train_time:35129ms step_avg:60.15ms
step:585/2285 train_time:35191ms step_avg:60.15ms
step:586/2285 train_time:35249ms step_avg:60.15ms
step:587/2285 train_time:35310ms step_avg:60.15ms
step:588/2285 train_time:35369ms step_avg:60.15ms
step:589/2285 train_time:35431ms step_avg:60.15ms
step:590/2285 train_time:35489ms step_avg:60.15ms
step:591/2285 train_time:35551ms step_avg:60.15ms
step:592/2285 train_time:35609ms step_avg:60.15ms
step:593/2285 train_time:35670ms step_avg:60.15ms
step:594/2285 train_time:35729ms step_avg:60.15ms
step:595/2285 train_time:35791ms step_avg:60.15ms
step:596/2285 train_time:35849ms step_avg:60.15ms
step:597/2285 train_time:35911ms step_avg:60.15ms
step:598/2285 train_time:35970ms step_avg:60.15ms
step:599/2285 train_time:36031ms step_avg:60.15ms
step:600/2285 train_time:36090ms step_avg:60.15ms
step:601/2285 train_time:36151ms step_avg:60.15ms
step:602/2285 train_time:36209ms step_avg:60.15ms
step:603/2285 train_time:36271ms step_avg:60.15ms
step:604/2285 train_time:36330ms step_avg:60.15ms
step:605/2285 train_time:36392ms step_avg:60.15ms
step:606/2285 train_time:36450ms step_avg:60.15ms
step:607/2285 train_time:36511ms step_avg:60.15ms
step:608/2285 train_time:36570ms step_avg:60.15ms
step:609/2285 train_time:36631ms step_avg:60.15ms
step:610/2285 train_time:36690ms step_avg:60.15ms
step:611/2285 train_time:36751ms step_avg:60.15ms
step:612/2285 train_time:36810ms step_avg:60.15ms
step:613/2285 train_time:36871ms step_avg:60.15ms
step:614/2285 train_time:36930ms step_avg:60.15ms
step:615/2285 train_time:36991ms step_avg:60.15ms
step:616/2285 train_time:37050ms step_avg:60.15ms
step:617/2285 train_time:37111ms step_avg:60.15ms
step:618/2285 train_time:37170ms step_avg:60.15ms
step:619/2285 train_time:37232ms step_avg:60.15ms
step:620/2285 train_time:37290ms step_avg:60.15ms
step:621/2285 train_time:37351ms step_avg:60.15ms
step:622/2285 train_time:37410ms step_avg:60.14ms
step:623/2285 train_time:37472ms step_avg:60.15ms
step:624/2285 train_time:37530ms step_avg:60.14ms
step:625/2285 train_time:37592ms step_avg:60.15ms
step:626/2285 train_time:37650ms step_avg:60.14ms
step:627/2285 train_time:37711ms step_avg:60.15ms
step:628/2285 train_time:37770ms step_avg:60.14ms
step:629/2285 train_time:37832ms step_avg:60.15ms
step:630/2285 train_time:37890ms step_avg:60.14ms
step:631/2285 train_time:37951ms step_avg:60.14ms
step:632/2285 train_time:38010ms step_avg:60.14ms
step:633/2285 train_time:38072ms step_avg:60.15ms
step:634/2285 train_time:38131ms step_avg:60.14ms
step:635/2285 train_time:38192ms step_avg:60.15ms
step:636/2285 train_time:38251ms step_avg:60.14ms
step:637/2285 train_time:38313ms step_avg:60.15ms
step:638/2285 train_time:38371ms step_avg:60.14ms
step:639/2285 train_time:38433ms step_avg:60.15ms
step:640/2285 train_time:38492ms step_avg:60.14ms
step:641/2285 train_time:38553ms step_avg:60.14ms
step:642/2285 train_time:38611ms step_avg:60.14ms
step:643/2285 train_time:38673ms step_avg:60.14ms
step:644/2285 train_time:38732ms step_avg:60.14ms
step:645/2285 train_time:38793ms step_avg:60.14ms
step:646/2285 train_time:38852ms step_avg:60.14ms
step:647/2285 train_time:38913ms step_avg:60.14ms
step:648/2285 train_time:38972ms step_avg:60.14ms
step:649/2285 train_time:39034ms step_avg:60.14ms
step:650/2285 train_time:39092ms step_avg:60.14ms
step:651/2285 train_time:39154ms step_avg:60.14ms
step:652/2285 train_time:39212ms step_avg:60.14ms
step:653/2285 train_time:39274ms step_avg:60.14ms
step:654/2285 train_time:39333ms step_avg:60.14ms
step:655/2285 train_time:39394ms step_avg:60.14ms
step:656/2285 train_time:39452ms step_avg:60.14ms
step:657/2285 train_time:39514ms step_avg:60.14ms
step:658/2285 train_time:39573ms step_avg:60.14ms
step:659/2285 train_time:39634ms step_avg:60.14ms
step:660/2285 train_time:39692ms step_avg:60.14ms
step:661/2285 train_time:39754ms step_avg:60.14ms
step:662/2285 train_time:39813ms step_avg:60.14ms
step:663/2285 train_time:39874ms step_avg:60.14ms
step:664/2285 train_time:39933ms step_avg:60.14ms
step:665/2285 train_time:39994ms step_avg:60.14ms
step:666/2285 train_time:40053ms step_avg:60.14ms
step:667/2285 train_time:40115ms step_avg:60.14ms
step:668/2285 train_time:40174ms step_avg:60.14ms
step:669/2285 train_time:40235ms step_avg:60.14ms
step:670/2285 train_time:40294ms step_avg:60.14ms
step:671/2285 train_time:40355ms step_avg:60.14ms
step:672/2285 train_time:40414ms step_avg:60.14ms
step:673/2285 train_time:40475ms step_avg:60.14ms
step:674/2285 train_time:40535ms step_avg:60.14ms
step:675/2285 train_time:40596ms step_avg:60.14ms
step:676/2285 train_time:40656ms step_avg:60.14ms
step:677/2285 train_time:40718ms step_avg:60.14ms
step:678/2285 train_time:40776ms step_avg:60.14ms
step:679/2285 train_time:40837ms step_avg:60.14ms
step:680/2285 train_time:40896ms step_avg:60.14ms
step:681/2285 train_time:40957ms step_avg:60.14ms
step:682/2285 train_time:41017ms step_avg:60.14ms
step:683/2285 train_time:41079ms step_avg:60.14ms
step:684/2285 train_time:41138ms step_avg:60.14ms
step:685/2285 train_time:41199ms step_avg:60.14ms
step:686/2285 train_time:41258ms step_avg:60.14ms
step:687/2285 train_time:41320ms step_avg:60.15ms
step:688/2285 train_time:41379ms step_avg:60.14ms
step:689/2285 train_time:41441ms step_avg:60.15ms
step:690/2285 train_time:41501ms step_avg:60.15ms
step:691/2285 train_time:41562ms step_avg:60.15ms
step:692/2285 train_time:41621ms step_avg:60.15ms
step:693/2285 train_time:41683ms step_avg:60.15ms
step:694/2285 train_time:41742ms step_avg:60.15ms
step:695/2285 train_time:41804ms step_avg:60.15ms
step:696/2285 train_time:41863ms step_avg:60.15ms
step:697/2285 train_time:41925ms step_avg:60.15ms
step:698/2285 train_time:41984ms step_avg:60.15ms
step:699/2285 train_time:42045ms step_avg:60.15ms
step:700/2285 train_time:42104ms step_avg:60.15ms
step:701/2285 train_time:42166ms step_avg:60.15ms
step:702/2285 train_time:42225ms step_avg:60.15ms
step:703/2285 train_time:42286ms step_avg:60.15ms
step:704/2285 train_time:42345ms step_avg:60.15ms
step:705/2285 train_time:42407ms step_avg:60.15ms
step:706/2285 train_time:42466ms step_avg:60.15ms
step:707/2285 train_time:42527ms step_avg:60.15ms
step:708/2285 train_time:42586ms step_avg:60.15ms
step:709/2285 train_time:42647ms step_avg:60.15ms
step:710/2285 train_time:42706ms step_avg:60.15ms
step:711/2285 train_time:42768ms step_avg:60.15ms
step:712/2285 train_time:42827ms step_avg:60.15ms
step:713/2285 train_time:42888ms step_avg:60.15ms
step:714/2285 train_time:42947ms step_avg:60.15ms
step:715/2285 train_time:43008ms step_avg:60.15ms
step:716/2285 train_time:43067ms step_avg:60.15ms
step:717/2285 train_time:43129ms step_avg:60.15ms
step:718/2285 train_time:43188ms step_avg:60.15ms
step:719/2285 train_time:43249ms step_avg:60.15ms
step:720/2285 train_time:43308ms step_avg:60.15ms
step:721/2285 train_time:43369ms step_avg:60.15ms
step:722/2285 train_time:43428ms step_avg:60.15ms
step:723/2285 train_time:43489ms step_avg:60.15ms
step:724/2285 train_time:43548ms step_avg:60.15ms
step:725/2285 train_time:43609ms step_avg:60.15ms
step:726/2285 train_time:43668ms step_avg:60.15ms
step:727/2285 train_time:43729ms step_avg:60.15ms
step:728/2285 train_time:43789ms step_avg:60.15ms
step:729/2285 train_time:43850ms step_avg:60.15ms
step:730/2285 train_time:43909ms step_avg:60.15ms
step:731/2285 train_time:43970ms step_avg:60.15ms
step:732/2285 train_time:44029ms step_avg:60.15ms
step:733/2285 train_time:44090ms step_avg:60.15ms
step:734/2285 train_time:44149ms step_avg:60.15ms
step:735/2285 train_time:44210ms step_avg:60.15ms
step:736/2285 train_time:44269ms step_avg:60.15ms
step:737/2285 train_time:44330ms step_avg:60.15ms
step:738/2285 train_time:44389ms step_avg:60.15ms
step:739/2285 train_time:44450ms step_avg:60.15ms
step:740/2285 train_time:44509ms step_avg:60.15ms
step:741/2285 train_time:44570ms step_avg:60.15ms
step:742/2285 train_time:44629ms step_avg:60.15ms
step:743/2285 train_time:44690ms step_avg:60.15ms
step:744/2285 train_time:44749ms step_avg:60.15ms
step:745/2285 train_time:44810ms step_avg:60.15ms
step:746/2285 train_time:44869ms step_avg:60.15ms
step:747/2285 train_time:44930ms step_avg:60.15ms
step:748/2285 train_time:44989ms step_avg:60.15ms
step:749/2285 train_time:45050ms step_avg:60.15ms
step:750/2285 train_time:45109ms step_avg:60.15ms
step:750/2285 val_loss:3.6723 train_time:45173ms step_avg:60.23ms
step:751/2285 train_time:45196ms step_avg:60.18ms
step:752/2285 train_time:45233ms step_avg:60.15ms
step:753/2285 train_time:45296ms step_avg:60.15ms
step:754/2285 train_time:45357ms step_avg:60.16ms
step:755/2285 train_time:45420ms step_avg:60.16ms
step:756/2285 train_time:45479ms step_avg:60.16ms
step:757/2285 train_time:45540ms step_avg:60.16ms
step:758/2285 train_time:45599ms step_avg:60.16ms
step:759/2285 train_time:45661ms step_avg:60.16ms
step:760/2285 train_time:45719ms step_avg:60.16ms
step:761/2285 train_time:45781ms step_avg:60.16ms
step:762/2285 train_time:45839ms step_avg:60.16ms
step:763/2285 train_time:45901ms step_avg:60.16ms
step:764/2285 train_time:45960ms step_avg:60.16ms
step:765/2285 train_time:46021ms step_avg:60.16ms
step:766/2285 train_time:46083ms step_avg:60.16ms
step:767/2285 train_time:46150ms step_avg:60.17ms
step:768/2285 train_time:46212ms step_avg:60.17ms
step:769/2285 train_time:46275ms step_avg:60.18ms
step:770/2285 train_time:46336ms step_avg:60.18ms
step:771/2285 train_time:46399ms step_avg:60.18ms
step:772/2285 train_time:46458ms step_avg:60.18ms
step:773/2285 train_time:46520ms step_avg:60.18ms
step:774/2285 train_time:46579ms step_avg:60.18ms
step:775/2285 train_time:46640ms step_avg:60.18ms
step:776/2285 train_time:46700ms step_avg:60.18ms
step:777/2285 train_time:46761ms step_avg:60.18ms
step:778/2285 train_time:46820ms step_avg:60.18ms
step:779/2285 train_time:46881ms step_avg:60.18ms
step:780/2285 train_time:46941ms step_avg:60.18ms
step:781/2285 train_time:47003ms step_avg:60.18ms
step:782/2285 train_time:47064ms step_avg:60.18ms
step:783/2285 train_time:47127ms step_avg:60.19ms
step:784/2285 train_time:47188ms step_avg:60.19ms
step:785/2285 train_time:47250ms step_avg:60.19ms
step:786/2285 train_time:47311ms step_avg:60.19ms
step:787/2285 train_time:47373ms step_avg:60.19ms
step:788/2285 train_time:47433ms step_avg:60.19ms
step:789/2285 train_time:47495ms step_avg:60.20ms
step:790/2285 train_time:47555ms step_avg:60.20ms
step:791/2285 train_time:47617ms step_avg:60.20ms
step:792/2285 train_time:47676ms step_avg:60.20ms
step:793/2285 train_time:47738ms step_avg:60.20ms
step:794/2285 train_time:47797ms step_avg:60.20ms
step:795/2285 train_time:47859ms step_avg:60.20ms
step:796/2285 train_time:47918ms step_avg:60.20ms
step:797/2285 train_time:47982ms step_avg:60.20ms
step:798/2285 train_time:48042ms step_avg:60.20ms
step:799/2285 train_time:48105ms step_avg:60.21ms
step:800/2285 train_time:48165ms step_avg:60.21ms
step:801/2285 train_time:48228ms step_avg:60.21ms
step:802/2285 train_time:48287ms step_avg:60.21ms
step:803/2285 train_time:48350ms step_avg:60.21ms
step:804/2285 train_time:48410ms step_avg:60.21ms
step:805/2285 train_time:48472ms step_avg:60.21ms
step:806/2285 train_time:48532ms step_avg:60.21ms
step:807/2285 train_time:48594ms step_avg:60.22ms
step:808/2285 train_time:48654ms step_avg:60.22ms
step:809/2285 train_time:48716ms step_avg:60.22ms
step:810/2285 train_time:48775ms step_avg:60.22ms
step:811/2285 train_time:48837ms step_avg:60.22ms
step:812/2285 train_time:48896ms step_avg:60.22ms
step:813/2285 train_time:48959ms step_avg:60.22ms
step:814/2285 train_time:49019ms step_avg:60.22ms
step:815/2285 train_time:49082ms step_avg:60.22ms
step:816/2285 train_time:49142ms step_avg:60.22ms
step:817/2285 train_time:49205ms step_avg:60.23ms
step:818/2285 train_time:49264ms step_avg:60.23ms
step:819/2285 train_time:49326ms step_avg:60.23ms
step:820/2285 train_time:49386ms step_avg:60.23ms
step:821/2285 train_time:49448ms step_avg:60.23ms
step:822/2285 train_time:49508ms step_avg:60.23ms
step:823/2285 train_time:49570ms step_avg:60.23ms
step:824/2285 train_time:49629ms step_avg:60.23ms
step:825/2285 train_time:49691ms step_avg:60.23ms
step:826/2285 train_time:49751ms step_avg:60.23ms
step:827/2285 train_time:49813ms step_avg:60.23ms
step:828/2285 train_time:49872ms step_avg:60.23ms
step:829/2285 train_time:49935ms step_avg:60.23ms
step:830/2285 train_time:49995ms step_avg:60.23ms
step:831/2285 train_time:50058ms step_avg:60.24ms
step:832/2285 train_time:50118ms step_avg:60.24ms
step:833/2285 train_time:50181ms step_avg:60.24ms
step:834/2285 train_time:50241ms step_avg:60.24ms
step:835/2285 train_time:50305ms step_avg:60.25ms
step:836/2285 train_time:50364ms step_avg:60.24ms
step:837/2285 train_time:50426ms step_avg:60.25ms
step:838/2285 train_time:50485ms step_avg:60.25ms
step:839/2285 train_time:50547ms step_avg:60.25ms
step:840/2285 train_time:50607ms step_avg:60.25ms
step:841/2285 train_time:50668ms step_avg:60.25ms
step:842/2285 train_time:50728ms step_avg:60.25ms
step:843/2285 train_time:50790ms step_avg:60.25ms
step:844/2285 train_time:50850ms step_avg:60.25ms
step:845/2285 train_time:50912ms step_avg:60.25ms
step:846/2285 train_time:50972ms step_avg:60.25ms
step:847/2285 train_time:51036ms step_avg:60.25ms
step:848/2285 train_time:51096ms step_avg:60.25ms
step:849/2285 train_time:51159ms step_avg:60.26ms
step:850/2285 train_time:51218ms step_avg:60.26ms
step:851/2285 train_time:51281ms step_avg:60.26ms
step:852/2285 train_time:51341ms step_avg:60.26ms
step:853/2285 train_time:51404ms step_avg:60.26ms
step:854/2285 train_time:51463ms step_avg:60.26ms
step:855/2285 train_time:51525ms step_avg:60.26ms
step:856/2285 train_time:51584ms step_avg:60.26ms
step:857/2285 train_time:51646ms step_avg:60.26ms
step:858/2285 train_time:51705ms step_avg:60.26ms
step:859/2285 train_time:51768ms step_avg:60.27ms
step:860/2285 train_time:51827ms step_avg:60.26ms
step:861/2285 train_time:51889ms step_avg:60.27ms
step:862/2285 train_time:51949ms step_avg:60.27ms
step:863/2285 train_time:52012ms step_avg:60.27ms
step:864/2285 train_time:52071ms step_avg:60.27ms
step:865/2285 train_time:52134ms step_avg:60.27ms
step:866/2285 train_time:52194ms step_avg:60.27ms
step:867/2285 train_time:52257ms step_avg:60.27ms
step:868/2285 train_time:52317ms step_avg:60.27ms
step:869/2285 train_time:52380ms step_avg:60.28ms
step:870/2285 train_time:52441ms step_avg:60.28ms
step:871/2285 train_time:52503ms step_avg:60.28ms
step:872/2285 train_time:52562ms step_avg:60.28ms
step:873/2285 train_time:52624ms step_avg:60.28ms
step:874/2285 train_time:52683ms step_avg:60.28ms
step:875/2285 train_time:52745ms step_avg:60.28ms
step:876/2285 train_time:52805ms step_avg:60.28ms
step:877/2285 train_time:52867ms step_avg:60.28ms
step:878/2285 train_time:52926ms step_avg:60.28ms
step:879/2285 train_time:52988ms step_avg:60.28ms
step:880/2285 train_time:53048ms step_avg:60.28ms
step:881/2285 train_time:53110ms step_avg:60.28ms
step:882/2285 train_time:53170ms step_avg:60.28ms
step:883/2285 train_time:53232ms step_avg:60.29ms
step:884/2285 train_time:53293ms step_avg:60.29ms
step:885/2285 train_time:53356ms step_avg:60.29ms
step:886/2285 train_time:53416ms step_avg:60.29ms
step:887/2285 train_time:53479ms step_avg:60.29ms
step:888/2285 train_time:53539ms step_avg:60.29ms
step:889/2285 train_time:53602ms step_avg:60.29ms
step:890/2285 train_time:53661ms step_avg:60.29ms
step:891/2285 train_time:53723ms step_avg:60.30ms
step:892/2285 train_time:53782ms step_avg:60.29ms
step:893/2285 train_time:53845ms step_avg:60.30ms
step:894/2285 train_time:53904ms step_avg:60.30ms
step:895/2285 train_time:53967ms step_avg:60.30ms
step:896/2285 train_time:54026ms step_avg:60.30ms
step:897/2285 train_time:54088ms step_avg:60.30ms
step:898/2285 train_time:54147ms step_avg:60.30ms
step:899/2285 train_time:54209ms step_avg:60.30ms
step:900/2285 train_time:54269ms step_avg:60.30ms
step:901/2285 train_time:54332ms step_avg:60.30ms
step:902/2285 train_time:54392ms step_avg:60.30ms
step:903/2285 train_time:54455ms step_avg:60.30ms
step:904/2285 train_time:54515ms step_avg:60.30ms
step:905/2285 train_time:54578ms step_avg:60.31ms
step:906/2285 train_time:54638ms step_avg:60.31ms
step:907/2285 train_time:54700ms step_avg:60.31ms
step:908/2285 train_time:54759ms step_avg:60.31ms
step:909/2285 train_time:54822ms step_avg:60.31ms
step:910/2285 train_time:54881ms step_avg:60.31ms
step:911/2285 train_time:54944ms step_avg:60.31ms
step:912/2285 train_time:55003ms step_avg:60.31ms
step:913/2285 train_time:55065ms step_avg:60.31ms
step:914/2285 train_time:55124ms step_avg:60.31ms
step:915/2285 train_time:55187ms step_avg:60.31ms
step:916/2285 train_time:55246ms step_avg:60.31ms
step:917/2285 train_time:55309ms step_avg:60.31ms
step:918/2285 train_time:55368ms step_avg:60.31ms
step:919/2285 train_time:55431ms step_avg:60.32ms
step:920/2285 train_time:55491ms step_avg:60.32ms
step:921/2285 train_time:55554ms step_avg:60.32ms
step:922/2285 train_time:55614ms step_avg:60.32ms
step:923/2285 train_time:55676ms step_avg:60.32ms
step:924/2285 train_time:55736ms step_avg:60.32ms
step:925/2285 train_time:55798ms step_avg:60.32ms
step:926/2285 train_time:55858ms step_avg:60.32ms
step:927/2285 train_time:55921ms step_avg:60.32ms
step:928/2285 train_time:55981ms step_avg:60.32ms
step:929/2285 train_time:56043ms step_avg:60.33ms
step:930/2285 train_time:56103ms step_avg:60.33ms
step:931/2285 train_time:56165ms step_avg:60.33ms
step:932/2285 train_time:56225ms step_avg:60.33ms
step:933/2285 train_time:56287ms step_avg:60.33ms
step:934/2285 train_time:56346ms step_avg:60.33ms
step:935/2285 train_time:56408ms step_avg:60.33ms
step:936/2285 train_time:56468ms step_avg:60.33ms
step:937/2285 train_time:56530ms step_avg:60.33ms
step:938/2285 train_time:56590ms step_avg:60.33ms
step:939/2285 train_time:56653ms step_avg:60.33ms
step:940/2285 train_time:56713ms step_avg:60.33ms
step:941/2285 train_time:56776ms step_avg:60.34ms
step:942/2285 train_time:56835ms step_avg:60.33ms
step:943/2285 train_time:56899ms step_avg:60.34ms
step:944/2285 train_time:56959ms step_avg:60.34ms
step:945/2285 train_time:57021ms step_avg:60.34ms
step:946/2285 train_time:57081ms step_avg:60.34ms
step:947/2285 train_time:57144ms step_avg:60.34ms
step:948/2285 train_time:57203ms step_avg:60.34ms
step:949/2285 train_time:57266ms step_avg:60.34ms
step:950/2285 train_time:57325ms step_avg:60.34ms
step:951/2285 train_time:57387ms step_avg:60.34ms
step:952/2285 train_time:57446ms step_avg:60.34ms
step:953/2285 train_time:57509ms step_avg:60.35ms
step:954/2285 train_time:57568ms step_avg:60.34ms
step:955/2285 train_time:57630ms step_avg:60.35ms
step:956/2285 train_time:57691ms step_avg:60.35ms
step:957/2285 train_time:57753ms step_avg:60.35ms
step:958/2285 train_time:57813ms step_avg:60.35ms
step:959/2285 train_time:57876ms step_avg:60.35ms
step:960/2285 train_time:57937ms step_avg:60.35ms
step:961/2285 train_time:57999ms step_avg:60.35ms
step:962/2285 train_time:58059ms step_avg:60.35ms
step:963/2285 train_time:58122ms step_avg:60.35ms
step:964/2285 train_time:58181ms step_avg:60.35ms
step:965/2285 train_time:58243ms step_avg:60.36ms
step:966/2285 train_time:58303ms step_avg:60.35ms
step:967/2285 train_time:58365ms step_avg:60.36ms
step:968/2285 train_time:58424ms step_avg:60.36ms
step:969/2285 train_time:58487ms step_avg:60.36ms
step:970/2285 train_time:58546ms step_avg:60.36ms
step:971/2285 train_time:58609ms step_avg:60.36ms
step:972/2285 train_time:58668ms step_avg:60.36ms
step:973/2285 train_time:58730ms step_avg:60.36ms
step:974/2285 train_time:58790ms step_avg:60.36ms
step:975/2285 train_time:58853ms step_avg:60.36ms
step:976/2285 train_time:58914ms step_avg:60.36ms
step:977/2285 train_time:58976ms step_avg:60.36ms
step:978/2285 train_time:59036ms step_avg:60.36ms
step:979/2285 train_time:59099ms step_avg:60.37ms
step:980/2285 train_time:59159ms step_avg:60.37ms
step:981/2285 train_time:59221ms step_avg:60.37ms
step:982/2285 train_time:59280ms step_avg:60.37ms
step:983/2285 train_time:59343ms step_avg:60.37ms
step:984/2285 train_time:59402ms step_avg:60.37ms
step:985/2285 train_time:59464ms step_avg:60.37ms
step:986/2285 train_time:59524ms step_avg:60.37ms
step:987/2285 train_time:59587ms step_avg:60.37ms
step:988/2285 train_time:59646ms step_avg:60.37ms
step:989/2285 train_time:59708ms step_avg:60.37ms
step:990/2285 train_time:59768ms step_avg:60.37ms
step:991/2285 train_time:59830ms step_avg:60.37ms
step:992/2285 train_time:59890ms step_avg:60.37ms
step:993/2285 train_time:59954ms step_avg:60.38ms
step:994/2285 train_time:60014ms step_avg:60.38ms
step:995/2285 train_time:60077ms step_avg:60.38ms
step:996/2285 train_time:60137ms step_avg:60.38ms
step:997/2285 train_time:60199ms step_avg:60.38ms
step:998/2285 train_time:60260ms step_avg:60.38ms
step:999/2285 train_time:60321ms step_avg:60.38ms
step:1000/2285 train_time:60381ms step_avg:60.38ms
step:1000/2285 val_loss:3.5719 train_time:60444ms step_avg:60.44ms
step:1001/2285 train_time:60463ms step_avg:60.40ms
step:1002/2285 train_time:60508ms step_avg:60.39ms
step:1003/2285 train_time:60573ms step_avg:60.39ms
step:1004/2285 train_time:60634ms step_avg:60.39ms
step:1005/2285 train_time:60696ms step_avg:60.39ms
step:1006/2285 train_time:60756ms step_avg:60.39ms
step:1007/2285 train_time:60818ms step_avg:60.40ms
step:1008/2285 train_time:60877ms step_avg:60.39ms
step:1009/2285 train_time:60939ms step_avg:60.40ms
step:1010/2285 train_time:60999ms step_avg:60.39ms
step:1011/2285 train_time:61061ms step_avg:60.40ms
step:1012/2285 train_time:61120ms step_avg:60.40ms
step:1013/2285 train_time:61182ms step_avg:60.40ms
step:1014/2285 train_time:61242ms step_avg:60.40ms
step:1015/2285 train_time:61304ms step_avg:60.40ms
step:1016/2285 train_time:61365ms step_avg:60.40ms
step:1017/2285 train_time:61427ms step_avg:60.40ms
step:1018/2285 train_time:61487ms step_avg:60.40ms
step:1019/2285 train_time:61551ms step_avg:60.40ms
step:1020/2285 train_time:61612ms step_avg:60.40ms
step:1021/2285 train_time:61674ms step_avg:60.41ms
step:1022/2285 train_time:61734ms step_avg:60.40ms
step:1023/2285 train_time:61796ms step_avg:60.41ms
step:1024/2285 train_time:61855ms step_avg:60.41ms
step:1025/2285 train_time:61917ms step_avg:60.41ms
step:1026/2285 train_time:61977ms step_avg:60.41ms
step:1027/2285 train_time:62039ms step_avg:60.41ms
step:1028/2285 train_time:62098ms step_avg:60.41ms
step:1029/2285 train_time:62160ms step_avg:60.41ms
step:1030/2285 train_time:62220ms step_avg:60.41ms
step:1031/2285 train_time:62282ms step_avg:60.41ms
step:1032/2285 train_time:62343ms step_avg:60.41ms
step:1033/2285 train_time:62406ms step_avg:60.41ms
step:1034/2285 train_time:62466ms step_avg:60.41ms
step:1035/2285 train_time:62529ms step_avg:60.41ms
step:1036/2285 train_time:62590ms step_avg:60.42ms
step:1037/2285 train_time:62653ms step_avg:60.42ms
step:1038/2285 train_time:62712ms step_avg:60.42ms
step:1039/2285 train_time:62775ms step_avg:60.42ms
step:1040/2285 train_time:62835ms step_avg:60.42ms
step:1041/2285 train_time:62897ms step_avg:60.42ms
step:1042/2285 train_time:62956ms step_avg:60.42ms
step:1043/2285 train_time:63019ms step_avg:60.42ms
step:1044/2285 train_time:63078ms step_avg:60.42ms
step:1045/2285 train_time:63140ms step_avg:60.42ms
step:1046/2285 train_time:63200ms step_avg:60.42ms
step:1047/2285 train_time:63262ms step_avg:60.42ms
step:1048/2285 train_time:63323ms step_avg:60.42ms
step:1049/2285 train_time:63386ms step_avg:60.43ms
step:1050/2285 train_time:63446ms step_avg:60.42ms
step:1051/2285 train_time:63508ms step_avg:60.43ms
step:1052/2285 train_time:63568ms step_avg:60.43ms
step:1053/2285 train_time:63631ms step_avg:60.43ms
step:1054/2285 train_time:63690ms step_avg:60.43ms
step:1055/2285 train_time:63753ms step_avg:60.43ms
step:1056/2285 train_time:63812ms step_avg:60.43ms
step:1057/2285 train_time:63874ms step_avg:60.43ms
step:1058/2285 train_time:63934ms step_avg:60.43ms
step:1059/2285 train_time:63997ms step_avg:60.43ms
step:1060/2285 train_time:64056ms step_avg:60.43ms
step:1061/2285 train_time:64118ms step_avg:60.43ms
step:1062/2285 train_time:64178ms step_avg:60.43ms
step:1063/2285 train_time:64241ms step_avg:60.43ms
step:1064/2285 train_time:64301ms step_avg:60.43ms
step:1065/2285 train_time:64363ms step_avg:60.44ms
step:1066/2285 train_time:64423ms step_avg:60.43ms
step:1067/2285 train_time:64486ms step_avg:60.44ms
step:1068/2285 train_time:64546ms step_avg:60.44ms
step:1069/2285 train_time:64608ms step_avg:60.44ms
step:1070/2285 train_time:64667ms step_avg:60.44ms
step:1071/2285 train_time:64730ms step_avg:60.44ms
step:1072/2285 train_time:64790ms step_avg:60.44ms
step:1073/2285 train_time:64852ms step_avg:60.44ms
step:1074/2285 train_time:64911ms step_avg:60.44ms
step:1075/2285 train_time:64973ms step_avg:60.44ms
step:1076/2285 train_time:65033ms step_avg:60.44ms
step:1077/2285 train_time:65095ms step_avg:60.44ms
step:1078/2285 train_time:65155ms step_avg:60.44ms
step:1079/2285 train_time:65218ms step_avg:60.44ms
step:1080/2285 train_time:65278ms step_avg:60.44ms
step:1081/2285 train_time:65341ms step_avg:60.44ms
step:1082/2285 train_time:65401ms step_avg:60.44ms
step:1083/2285 train_time:65464ms step_avg:60.45ms
step:1084/2285 train_time:65523ms step_avg:60.45ms
step:1085/2285 train_time:65586ms step_avg:60.45ms
step:1086/2285 train_time:65646ms step_avg:60.45ms
step:1087/2285 train_time:65709ms step_avg:60.45ms
step:1088/2285 train_time:65768ms step_avg:60.45ms
step:1089/2285 train_time:65830ms step_avg:60.45ms
step:1090/2285 train_time:65889ms step_avg:60.45ms
step:1091/2285 train_time:65951ms step_avg:60.45ms
step:1092/2285 train_time:66011ms step_avg:60.45ms
step:1093/2285 train_time:66073ms step_avg:60.45ms
step:1094/2285 train_time:66132ms step_avg:60.45ms
step:1095/2285 train_time:66195ms step_avg:60.45ms
step:1096/2285 train_time:66255ms step_avg:60.45ms
step:1097/2285 train_time:66319ms step_avg:60.45ms
step:1098/2285 train_time:66379ms step_avg:60.45ms
step:1099/2285 train_time:66442ms step_avg:60.46ms
step:1100/2285 train_time:66502ms step_avg:60.46ms
step:1101/2285 train_time:66565ms step_avg:60.46ms
step:1102/2285 train_time:66625ms step_avg:60.46ms
step:1103/2285 train_time:66687ms step_avg:60.46ms
step:1104/2285 train_time:66746ms step_avg:60.46ms
step:1105/2285 train_time:66809ms step_avg:60.46ms
step:1106/2285 train_time:66868ms step_avg:60.46ms
step:1107/2285 train_time:66930ms step_avg:60.46ms
step:1108/2285 train_time:66989ms step_avg:60.46ms
step:1109/2285 train_time:67052ms step_avg:60.46ms
step:1110/2285 train_time:67112ms step_avg:60.46ms
step:1111/2285 train_time:67174ms step_avg:60.46ms
step:1112/2285 train_time:67234ms step_avg:60.46ms
step:1113/2285 train_time:67297ms step_avg:60.46ms
step:1114/2285 train_time:67357ms step_avg:60.46ms
step:1115/2285 train_time:67421ms step_avg:60.47ms
step:1116/2285 train_time:67481ms step_avg:60.47ms
step:1117/2285 train_time:67543ms step_avg:60.47ms
step:1118/2285 train_time:67603ms step_avg:60.47ms
step:1119/2285 train_time:67666ms step_avg:60.47ms
step:1120/2285 train_time:67726ms step_avg:60.47ms
step:1121/2285 train_time:67788ms step_avg:60.47ms
step:1122/2285 train_time:67847ms step_avg:60.47ms
step:1123/2285 train_time:67909ms step_avg:60.47ms
step:1124/2285 train_time:67969ms step_avg:60.47ms
step:1125/2285 train_time:68031ms step_avg:60.47ms
step:1126/2285 train_time:68090ms step_avg:60.47ms
step:1127/2285 train_time:68153ms step_avg:60.47ms
step:1128/2285 train_time:68213ms step_avg:60.47ms
step:1129/2285 train_time:68276ms step_avg:60.47ms
step:1130/2285 train_time:68336ms step_avg:60.47ms
step:1131/2285 train_time:68399ms step_avg:60.48ms
step:1132/2285 train_time:68460ms step_avg:60.48ms
step:1133/2285 train_time:68522ms step_avg:60.48ms
step:1134/2285 train_time:68581ms step_avg:60.48ms
step:1135/2285 train_time:68644ms step_avg:60.48ms
step:1136/2285 train_time:68704ms step_avg:60.48ms
step:1137/2285 train_time:68766ms step_avg:60.48ms
step:1138/2285 train_time:68826ms step_avg:60.48ms
step:1139/2285 train_time:68889ms step_avg:60.48ms
step:1140/2285 train_time:68948ms step_avg:60.48ms
step:1141/2285 train_time:69010ms step_avg:60.48ms
step:1142/2285 train_time:69070ms step_avg:60.48ms
step:1143/2285 train_time:69132ms step_avg:60.48ms
step:1144/2285 train_time:69191ms step_avg:60.48ms
step:1145/2285 train_time:69254ms step_avg:60.48ms
step:1146/2285 train_time:69314ms step_avg:60.48ms
step:1147/2285 train_time:69377ms step_avg:60.49ms
step:1148/2285 train_time:69438ms step_avg:60.49ms
step:1149/2285 train_time:69501ms step_avg:60.49ms
step:1150/2285 train_time:69560ms step_avg:60.49ms
step:1151/2285 train_time:69622ms step_avg:60.49ms
step:1152/2285 train_time:69683ms step_avg:60.49ms
step:1153/2285 train_time:69746ms step_avg:60.49ms
step:1154/2285 train_time:69806ms step_avg:60.49ms
step:1155/2285 train_time:69868ms step_avg:60.49ms
step:1156/2285 train_time:69927ms step_avg:60.49ms
step:1157/2285 train_time:69989ms step_avg:60.49ms
step:1158/2285 train_time:70049ms step_avg:60.49ms
step:1159/2285 train_time:70111ms step_avg:60.49ms
step:1160/2285 train_time:70171ms step_avg:60.49ms
step:1161/2285 train_time:70233ms step_avg:60.49ms
step:1162/2285 train_time:70293ms step_avg:60.49ms
step:1163/2285 train_time:70355ms step_avg:60.49ms
step:1164/2285 train_time:70415ms step_avg:60.49ms
step:1165/2285 train_time:70478ms step_avg:60.50ms
step:1166/2285 train_time:70538ms step_avg:60.50ms
step:1167/2285 train_time:70602ms step_avg:60.50ms
step:1168/2285 train_time:70662ms step_avg:60.50ms
step:1169/2285 train_time:70724ms step_avg:60.50ms
step:1170/2285 train_time:70784ms step_avg:60.50ms
step:1171/2285 train_time:70847ms step_avg:60.50ms
step:1172/2285 train_time:70907ms step_avg:60.50ms
step:1173/2285 train_time:70969ms step_avg:60.50ms
step:1174/2285 train_time:71029ms step_avg:60.50ms
step:1175/2285 train_time:71091ms step_avg:60.50ms
step:1176/2285 train_time:71151ms step_avg:60.50ms
step:1177/2285 train_time:71213ms step_avg:60.50ms
step:1178/2285 train_time:71273ms step_avg:60.50ms
step:1179/2285 train_time:71335ms step_avg:60.50ms
step:1180/2285 train_time:71395ms step_avg:60.50ms
step:1181/2285 train_time:71457ms step_avg:60.51ms
step:1182/2285 train_time:71517ms step_avg:60.51ms
step:1183/2285 train_time:71580ms step_avg:60.51ms
step:1184/2285 train_time:71640ms step_avg:60.51ms
step:1185/2285 train_time:71702ms step_avg:60.51ms
step:1186/2285 train_time:71762ms step_avg:60.51ms
step:1187/2285 train_time:71825ms step_avg:60.51ms
step:1188/2285 train_time:71885ms step_avg:60.51ms
step:1189/2285 train_time:71948ms step_avg:60.51ms
step:1190/2285 train_time:72007ms step_avg:60.51ms
step:1191/2285 train_time:72069ms step_avg:60.51ms
step:1192/2285 train_time:72129ms step_avg:60.51ms
step:1193/2285 train_time:72191ms step_avg:60.51ms
step:1194/2285 train_time:72251ms step_avg:60.51ms
step:1195/2285 train_time:72313ms step_avg:60.51ms
step:1196/2285 train_time:72372ms step_avg:60.51ms
step:1197/2285 train_time:72434ms step_avg:60.51ms
step:1198/2285 train_time:72494ms step_avg:60.51ms
step:1199/2285 train_time:72558ms step_avg:60.52ms
step:1200/2285 train_time:72619ms step_avg:60.52ms
step:1201/2285 train_time:72682ms step_avg:60.52ms
step:1202/2285 train_time:72742ms step_avg:60.52ms
step:1203/2285 train_time:72805ms step_avg:60.52ms
step:1204/2285 train_time:72865ms step_avg:60.52ms
step:1205/2285 train_time:72927ms step_avg:60.52ms
step:1206/2285 train_time:72987ms step_avg:60.52ms
step:1207/2285 train_time:73049ms step_avg:60.52ms
step:1208/2285 train_time:73108ms step_avg:60.52ms
step:1209/2285 train_time:73171ms step_avg:60.52ms
step:1210/2285 train_time:73230ms step_avg:60.52ms
step:1211/2285 train_time:73292ms step_avg:60.52ms
step:1212/2285 train_time:73352ms step_avg:60.52ms
step:1213/2285 train_time:73414ms step_avg:60.52ms
step:1214/2285 train_time:73473ms step_avg:60.52ms
step:1215/2285 train_time:73536ms step_avg:60.52ms
step:1216/2285 train_time:73597ms step_avg:60.52ms
step:1217/2285 train_time:73661ms step_avg:60.53ms
step:1218/2285 train_time:73721ms step_avg:60.53ms
step:1219/2285 train_time:73784ms step_avg:60.53ms
step:1220/2285 train_time:73844ms step_avg:60.53ms
step:1221/2285 train_time:73907ms step_avg:60.53ms
step:1222/2285 train_time:73966ms step_avg:60.53ms
step:1223/2285 train_time:74028ms step_avg:60.53ms
step:1224/2285 train_time:74089ms step_avg:60.53ms
step:1225/2285 train_time:74150ms step_avg:60.53ms
step:1226/2285 train_time:74210ms step_avg:60.53ms
step:1227/2285 train_time:74272ms step_avg:60.53ms
step:1228/2285 train_time:74331ms step_avg:60.53ms
step:1229/2285 train_time:74394ms step_avg:60.53ms
step:1230/2285 train_time:74453ms step_avg:60.53ms
step:1231/2285 train_time:74516ms step_avg:60.53ms
step:1232/2285 train_time:74576ms step_avg:60.53ms
step:1233/2285 train_time:74639ms step_avg:60.53ms
step:1234/2285 train_time:74699ms step_avg:60.53ms
step:1235/2285 train_time:74762ms step_avg:60.54ms
step:1236/2285 train_time:74822ms step_avg:60.54ms
step:1237/2285 train_time:74885ms step_avg:60.54ms
step:1238/2285 train_time:74944ms step_avg:60.54ms
step:1239/2285 train_time:75007ms step_avg:60.54ms
step:1240/2285 train_time:75066ms step_avg:60.54ms
step:1241/2285 train_time:75129ms step_avg:60.54ms
step:1242/2285 train_time:75189ms step_avg:60.54ms
step:1243/2285 train_time:75251ms step_avg:60.54ms
step:1244/2285 train_time:75311ms step_avg:60.54ms
step:1245/2285 train_time:75373ms step_avg:60.54ms
step:1246/2285 train_time:75432ms step_avg:60.54ms
step:1247/2285 train_time:75495ms step_avg:60.54ms
step:1248/2285 train_time:75554ms step_avg:60.54ms
step:1249/2285 train_time:75618ms step_avg:60.54ms
step:1250/2285 train_time:75678ms step_avg:60.54ms
step:1250/2285 val_loss:3.5031 train_time:75742ms step_avg:60.59ms
step:1251/2285 train_time:75762ms step_avg:60.56ms
step:1252/2285 train_time:75803ms step_avg:60.55ms
step:1253/2285 train_time:75867ms step_avg:60.55ms
step:1254/2285 train_time:75927ms step_avg:60.55ms
step:1255/2285 train_time:75989ms step_avg:60.55ms
step:1256/2285 train_time:76050ms step_avg:60.55ms
step:1257/2285 train_time:76111ms step_avg:60.55ms
step:1258/2285 train_time:76171ms step_avg:60.55ms
step:1259/2285 train_time:76233ms step_avg:60.55ms
step:1260/2285 train_time:76293ms step_avg:60.55ms
step:1261/2285 train_time:76354ms step_avg:60.55ms
step:1262/2285 train_time:76413ms step_avg:60.55ms
step:1263/2285 train_time:76475ms step_avg:60.55ms
step:1264/2285 train_time:76535ms step_avg:60.55ms
step:1265/2285 train_time:76597ms step_avg:60.55ms
step:1266/2285 train_time:76657ms step_avg:60.55ms
step:1267/2285 train_time:76721ms step_avg:60.55ms
step:1268/2285 train_time:76782ms step_avg:60.55ms
step:1269/2285 train_time:76845ms step_avg:60.56ms
step:1270/2285 train_time:76905ms step_avg:60.56ms
step:1271/2285 train_time:76968ms step_avg:60.56ms
step:1272/2285 train_time:77028ms step_avg:60.56ms
step:1273/2285 train_time:77090ms step_avg:60.56ms
step:1274/2285 train_time:77149ms step_avg:60.56ms
step:1275/2285 train_time:77211ms step_avg:60.56ms
step:1276/2285 train_time:77270ms step_avg:60.56ms
step:1277/2285 train_time:77332ms step_avg:60.56ms
step:1278/2285 train_time:77392ms step_avg:60.56ms
step:1279/2285 train_time:77454ms step_avg:60.56ms
step:1280/2285 train_time:77513ms step_avg:60.56ms
step:1281/2285 train_time:77575ms step_avg:60.56ms
step:1282/2285 train_time:77636ms step_avg:60.56ms
step:1283/2285 train_time:77699ms step_avg:60.56ms
step:1284/2285 train_time:77760ms step_avg:60.56ms
step:1285/2285 train_time:77824ms step_avg:60.56ms
step:1286/2285 train_time:77884ms step_avg:60.56ms
step:1287/2285 train_time:77946ms step_avg:60.56ms
step:1288/2285 train_time:78005ms step_avg:60.56ms
step:1289/2285 train_time:78068ms step_avg:60.56ms
step:1290/2285 train_time:78127ms step_avg:60.56ms
step:1291/2285 train_time:78190ms step_avg:60.57ms
step:1292/2285 train_time:78249ms step_avg:60.56ms
step:1293/2285 train_time:78311ms step_avg:60.57ms
step:1294/2285 train_time:78370ms step_avg:60.56ms
step:1295/2285 train_time:78432ms step_avg:60.57ms
step:1296/2285 train_time:78492ms step_avg:60.56ms
step:1297/2285 train_time:78553ms step_avg:60.57ms
step:1298/2285 train_time:78613ms step_avg:60.56ms
step:1299/2285 train_time:78677ms step_avg:60.57ms
step:1300/2285 train_time:78738ms step_avg:60.57ms
step:1301/2285 train_time:78800ms step_avg:60.57ms
step:1302/2285 train_time:78861ms step_avg:60.57ms
step:1303/2285 train_time:78924ms step_avg:60.57ms
step:1304/2285 train_time:78984ms step_avg:60.57ms
step:1305/2285 train_time:79046ms step_avg:60.57ms
step:1306/2285 train_time:79105ms step_avg:60.57ms
step:1307/2285 train_time:79166ms step_avg:60.57ms
step:1308/2285 train_time:79226ms step_avg:60.57ms
step:1309/2285 train_time:79289ms step_avg:60.57ms
step:1310/2285 train_time:79348ms step_avg:60.57ms
step:1311/2285 train_time:79410ms step_avg:60.57ms
step:1312/2285 train_time:79469ms step_avg:60.57ms
step:1313/2285 train_time:79531ms step_avg:60.57ms
step:1314/2285 train_time:79591ms step_avg:60.57ms
step:1315/2285 train_time:79654ms step_avg:60.57ms
step:1316/2285 train_time:79715ms step_avg:60.57ms
step:1317/2285 train_time:79778ms step_avg:60.58ms
step:1318/2285 train_time:79838ms step_avg:60.57ms
step:1319/2285 train_time:79900ms step_avg:60.58ms
step:1320/2285 train_time:79960ms step_avg:60.58ms
step:1321/2285 train_time:80023ms step_avg:60.58ms
step:1322/2285 train_time:80083ms step_avg:60.58ms
step:1323/2285 train_time:80145ms step_avg:60.58ms
step:1324/2285 train_time:80204ms step_avg:60.58ms
step:1325/2285 train_time:80266ms step_avg:60.58ms
step:1326/2285 train_time:80326ms step_avg:60.58ms
step:1327/2285 train_time:80388ms step_avg:60.58ms
step:1328/2285 train_time:80447ms step_avg:60.58ms
step:1329/2285 train_time:80509ms step_avg:60.58ms
step:1330/2285 train_time:80569ms step_avg:60.58ms
step:1331/2285 train_time:80632ms step_avg:60.58ms
step:1332/2285 train_time:80692ms step_avg:60.58ms
step:1333/2285 train_time:80755ms step_avg:60.58ms
step:1334/2285 train_time:80815ms step_avg:60.58ms
step:1335/2285 train_time:80878ms step_avg:60.58ms
step:1336/2285 train_time:80938ms step_avg:60.58ms
step:1337/2285 train_time:81001ms step_avg:60.58ms
step:1338/2285 train_time:81061ms step_avg:60.58ms
step:1339/2285 train_time:81124ms step_avg:60.59ms
step:1340/2285 train_time:81183ms step_avg:60.58ms
step:1341/2285 train_time:81246ms step_avg:60.59ms
step:1342/2285 train_time:81305ms step_avg:60.59ms
step:1343/2285 train_time:81367ms step_avg:60.59ms
step:1344/2285 train_time:81426ms step_avg:60.58ms
step:1345/2285 train_time:81488ms step_avg:60.59ms
step:1346/2285 train_time:81548ms step_avg:60.59ms
step:1347/2285 train_time:81610ms step_avg:60.59ms
step:1348/2285 train_time:81669ms step_avg:60.59ms
step:1349/2285 train_time:81732ms step_avg:60.59ms
step:1350/2285 train_time:81793ms step_avg:60.59ms
step:1351/2285 train_time:81855ms step_avg:60.59ms
step:1352/2285 train_time:81916ms step_avg:60.59ms
step:1353/2285 train_time:81979ms step_avg:60.59ms
step:1354/2285 train_time:82039ms step_avg:60.59ms
step:1355/2285 train_time:82101ms step_avg:60.59ms
step:1356/2285 train_time:82161ms step_avg:60.59ms
step:1357/2285 train_time:82224ms step_avg:60.59ms
step:1358/2285 train_time:82284ms step_avg:60.59ms
step:1359/2285 train_time:82347ms step_avg:60.59ms
step:1360/2285 train_time:82406ms step_avg:60.59ms
step:1361/2285 train_time:82468ms step_avg:60.59ms
step:1362/2285 train_time:82527ms step_avg:60.59ms
step:1363/2285 train_time:82590ms step_avg:60.59ms
step:1364/2285 train_time:82649ms step_avg:60.59ms
step:1365/2285 train_time:82711ms step_avg:60.59ms
step:1366/2285 train_time:82771ms step_avg:60.59ms
step:1367/2285 train_time:82834ms step_avg:60.60ms
step:1368/2285 train_time:82894ms step_avg:60.60ms
step:1369/2285 train_time:82957ms step_avg:60.60ms
step:1370/2285 train_time:83018ms step_avg:60.60ms
step:1371/2285 train_time:83080ms step_avg:60.60ms
step:1372/2285 train_time:83141ms step_avg:60.60ms
step:1373/2285 train_time:83203ms step_avg:60.60ms
step:1374/2285 train_time:83262ms step_avg:60.60ms
step:1375/2285 train_time:83326ms step_avg:60.60ms
step:1376/2285 train_time:83386ms step_avg:60.60ms
step:1377/2285 train_time:83448ms step_avg:60.60ms
step:1378/2285 train_time:83507ms step_avg:60.60ms
step:1379/2285 train_time:83569ms step_avg:60.60ms
step:1380/2285 train_time:83629ms step_avg:60.60ms
step:1381/2285 train_time:83691ms step_avg:60.60ms
step:1382/2285 train_time:83751ms step_avg:60.60ms
step:1383/2285 train_time:83814ms step_avg:60.60ms
step:1384/2285 train_time:83874ms step_avg:60.60ms
step:1385/2285 train_time:83938ms step_avg:60.61ms
step:1386/2285 train_time:83998ms step_avg:60.60ms
step:1387/2285 train_time:84060ms step_avg:60.61ms
step:1388/2285 train_time:84120ms step_avg:60.61ms
step:1389/2285 train_time:84182ms step_avg:60.61ms
step:1390/2285 train_time:84242ms step_avg:60.61ms
step:1391/2285 train_time:84304ms step_avg:60.61ms
step:1392/2285 train_time:84364ms step_avg:60.61ms
step:1393/2285 train_time:84426ms step_avg:60.61ms
step:1394/2285 train_time:84486ms step_avg:60.61ms
step:1395/2285 train_time:84549ms step_avg:60.61ms
step:1396/2285 train_time:84608ms step_avg:60.61ms
step:1397/2285 train_time:84669ms step_avg:60.61ms
step:1398/2285 train_time:84729ms step_avg:60.61ms
step:1399/2285 train_time:84791ms step_avg:60.61ms
step:1400/2285 train_time:84851ms step_avg:60.61ms
step:1401/2285 train_time:84914ms step_avg:60.61ms
step:1402/2285 train_time:84975ms step_avg:60.61ms
step:1403/2285 train_time:85039ms step_avg:60.61ms
step:1404/2285 train_time:85099ms step_avg:60.61ms
step:1405/2285 train_time:85161ms step_avg:60.61ms
step:1406/2285 train_time:85221ms step_avg:60.61ms
step:1407/2285 train_time:85284ms step_avg:60.61ms
step:1408/2285 train_time:85343ms step_avg:60.61ms
step:1409/2285 train_time:85406ms step_avg:60.61ms
step:1410/2285 train_time:85465ms step_avg:60.61ms
step:1411/2285 train_time:85528ms step_avg:60.62ms
step:1412/2285 train_time:85587ms step_avg:60.61ms
step:1413/2285 train_time:85649ms step_avg:60.61ms
step:1414/2285 train_time:85709ms step_avg:60.61ms
step:1415/2285 train_time:85771ms step_avg:60.62ms
step:1416/2285 train_time:85831ms step_avg:60.61ms
step:1417/2285 train_time:85893ms step_avg:60.62ms
step:1418/2285 train_time:85953ms step_avg:60.62ms
step:1419/2285 train_time:86015ms step_avg:60.62ms
step:1420/2285 train_time:86076ms step_avg:60.62ms
step:1421/2285 train_time:86139ms step_avg:60.62ms
step:1422/2285 train_time:86199ms step_avg:60.62ms
step:1423/2285 train_time:86261ms step_avg:60.62ms
step:1424/2285 train_time:86321ms step_avg:60.62ms
step:1425/2285 train_time:86383ms step_avg:60.62ms
step:1426/2285 train_time:86443ms step_avg:60.62ms
step:1427/2285 train_time:86506ms step_avg:60.62ms
step:1428/2285 train_time:86565ms step_avg:60.62ms
step:1429/2285 train_time:86627ms step_avg:60.62ms
step:1430/2285 train_time:86686ms step_avg:60.62ms
step:1431/2285 train_time:86748ms step_avg:60.62ms
step:1432/2285 train_time:86807ms step_avg:60.62ms
step:1433/2285 train_time:86869ms step_avg:60.62ms
step:1434/2285 train_time:86929ms step_avg:60.62ms
step:1435/2285 train_time:86992ms step_avg:60.62ms
step:1436/2285 train_time:87052ms step_avg:60.62ms
step:1437/2285 train_time:87115ms step_avg:60.62ms
step:1438/2285 train_time:87176ms step_avg:60.62ms
step:1439/2285 train_time:87239ms step_avg:60.62ms
step:1440/2285 train_time:87299ms step_avg:60.62ms
step:1441/2285 train_time:87362ms step_avg:60.63ms
step:1442/2285 train_time:87422ms step_avg:60.63ms
step:1443/2285 train_time:87485ms step_avg:60.63ms
step:1444/2285 train_time:87544ms step_avg:60.63ms
step:1445/2285 train_time:87606ms step_avg:60.63ms
step:1446/2285 train_time:87666ms step_avg:60.63ms
step:1447/2285 train_time:87728ms step_avg:60.63ms
step:1448/2285 train_time:87787ms step_avg:60.63ms
step:1449/2285 train_time:87850ms step_avg:60.63ms
step:1450/2285 train_time:87909ms step_avg:60.63ms
step:1451/2285 train_time:87971ms step_avg:60.63ms
step:1452/2285 train_time:88031ms step_avg:60.63ms
step:1453/2285 train_time:88094ms step_avg:60.63ms
step:1454/2285 train_time:88155ms step_avg:60.63ms
step:1455/2285 train_time:88218ms step_avg:60.63ms
step:1456/2285 train_time:88278ms step_avg:60.63ms
step:1457/2285 train_time:88341ms step_avg:60.63ms
step:1458/2285 train_time:88401ms step_avg:60.63ms
step:1459/2285 train_time:88463ms step_avg:60.63ms
step:1460/2285 train_time:88523ms step_avg:60.63ms
step:1461/2285 train_time:88585ms step_avg:60.63ms
step:1462/2285 train_time:88645ms step_avg:60.63ms
step:1463/2285 train_time:88707ms step_avg:60.63ms
step:1464/2285 train_time:88766ms step_avg:60.63ms
step:1465/2285 train_time:88828ms step_avg:60.63ms
step:1466/2285 train_time:88888ms step_avg:60.63ms
step:1467/2285 train_time:88950ms step_avg:60.63ms
step:1468/2285 train_time:89010ms step_avg:60.63ms
step:1469/2285 train_time:89072ms step_avg:60.63ms
step:1470/2285 train_time:89132ms step_avg:60.63ms
step:1471/2285 train_time:89195ms step_avg:60.64ms
step:1472/2285 train_time:89256ms step_avg:60.64ms
step:1473/2285 train_time:89318ms step_avg:60.64ms
step:1474/2285 train_time:89379ms step_avg:60.64ms
step:1475/2285 train_time:89441ms step_avg:60.64ms
step:1476/2285 train_time:89501ms step_avg:60.64ms
step:1477/2285 train_time:89563ms step_avg:60.64ms
step:1478/2285 train_time:89623ms step_avg:60.64ms
step:1479/2285 train_time:89686ms step_avg:60.64ms
step:1480/2285 train_time:89745ms step_avg:60.64ms
step:1481/2285 train_time:89808ms step_avg:60.64ms
step:1482/2285 train_time:89868ms step_avg:60.64ms
step:1483/2285 train_time:89930ms step_avg:60.64ms
step:1484/2285 train_time:89990ms step_avg:60.64ms
step:1485/2285 train_time:90051ms step_avg:60.64ms
step:1486/2285 train_time:90112ms step_avg:60.64ms
step:1487/2285 train_time:90174ms step_avg:60.64ms
step:1488/2285 train_time:90234ms step_avg:60.64ms
step:1489/2285 train_time:90298ms step_avg:60.64ms
step:1490/2285 train_time:90358ms step_avg:60.64ms
step:1491/2285 train_time:90421ms step_avg:60.64ms
step:1492/2285 train_time:90480ms step_avg:60.64ms
step:1493/2285 train_time:90543ms step_avg:60.65ms
step:1494/2285 train_time:90603ms step_avg:60.64ms
step:1495/2285 train_time:90665ms step_avg:60.65ms
step:1496/2285 train_time:90724ms step_avg:60.64ms
step:1497/2285 train_time:90787ms step_avg:60.65ms
step:1498/2285 train_time:90847ms step_avg:60.65ms
step:1499/2285 train_time:90910ms step_avg:60.65ms
step:1500/2285 train_time:90970ms step_avg:60.65ms
step:1500/2285 val_loss:3.4293 train_time:91033ms step_avg:60.69ms
step:1501/2285 train_time:91054ms step_avg:60.66ms
step:1502/2285 train_time:91097ms step_avg:60.65ms
step:1503/2285 train_time:91160ms step_avg:60.65ms
step:1504/2285 train_time:91221ms step_avg:60.65ms
step:1505/2285 train_time:91285ms step_avg:60.65ms
step:1506/2285 train_time:91346ms step_avg:60.65ms
step:1507/2285 train_time:91409ms step_avg:60.66ms
step:1508/2285 train_time:91469ms step_avg:60.66ms
step:1509/2285 train_time:91531ms step_avg:60.66ms
step:1510/2285 train_time:91590ms step_avg:60.66ms
step:1511/2285 train_time:91652ms step_avg:60.66ms
step:1512/2285 train_time:91712ms step_avg:60.66ms
step:1513/2285 train_time:91774ms step_avg:60.66ms
step:1514/2285 train_time:91835ms step_avg:60.66ms
step:1515/2285 train_time:91897ms step_avg:60.66ms
step:1516/2285 train_time:91958ms step_avg:60.66ms
step:1517/2285 train_time:92023ms step_avg:60.66ms
step:1518/2285 train_time:92085ms step_avg:60.66ms
step:1519/2285 train_time:92149ms step_avg:60.66ms
step:1520/2285 train_time:92211ms step_avg:60.67ms
step:1521/2285 train_time:92275ms step_avg:60.67ms
step:1522/2285 train_time:92335ms step_avg:60.67ms
step:1523/2285 train_time:92399ms step_avg:60.67ms
step:1524/2285 train_time:92460ms step_avg:60.67ms
step:1525/2285 train_time:92522ms step_avg:60.67ms
step:1526/2285 train_time:92581ms step_avg:60.67ms
step:1527/2285 train_time:92643ms step_avg:60.67ms
step:1528/2285 train_time:92703ms step_avg:60.67ms
step:1529/2285 train_time:92765ms step_avg:60.67ms
step:1530/2285 train_time:92825ms step_avg:60.67ms
step:1531/2285 train_time:92888ms step_avg:60.67ms
step:1532/2285 train_time:92949ms step_avg:60.67ms
step:1533/2285 train_time:93013ms step_avg:60.67ms
step:1534/2285 train_time:93073ms step_avg:60.67ms
step:1535/2285 train_time:93137ms step_avg:60.68ms
step:1536/2285 train_time:93198ms step_avg:60.68ms
step:1537/2285 train_time:93261ms step_avg:60.68ms
step:1538/2285 train_time:93321ms step_avg:60.68ms
step:1539/2285 train_time:93384ms step_avg:60.68ms
step:1540/2285 train_time:93444ms step_avg:60.68ms
step:1541/2285 train_time:93507ms step_avg:60.68ms
step:1542/2285 train_time:93567ms step_avg:60.68ms
step:1543/2285 train_time:93629ms step_avg:60.68ms
step:1544/2285 train_time:93689ms step_avg:60.68ms
step:1545/2285 train_time:93751ms step_avg:60.68ms
step:1546/2285 train_time:93811ms step_avg:60.68ms
step:1547/2285 train_time:93874ms step_avg:60.68ms
step:1548/2285 train_time:93934ms step_avg:60.68ms
step:1549/2285 train_time:93997ms step_avg:60.68ms
step:1550/2285 train_time:94058ms step_avg:60.68ms
step:1551/2285 train_time:94121ms step_avg:60.68ms
step:1552/2285 train_time:94181ms step_avg:60.68ms
step:1553/2285 train_time:94244ms step_avg:60.68ms
step:1554/2285 train_time:94304ms step_avg:60.68ms
step:1555/2285 train_time:94367ms step_avg:60.69ms
step:1556/2285 train_time:94428ms step_avg:60.69ms
step:1557/2285 train_time:94490ms step_avg:60.69ms
step:1558/2285 train_time:94551ms step_avg:60.69ms
step:1559/2285 train_time:94614ms step_avg:60.69ms
step:1560/2285 train_time:94674ms step_avg:60.69ms
step:1561/2285 train_time:94737ms step_avg:60.69ms
step:1562/2285 train_time:94797ms step_avg:60.69ms
step:1563/2285 train_time:94859ms step_avg:60.69ms
step:1564/2285 train_time:94919ms step_avg:60.69ms
step:1565/2285 train_time:94981ms step_avg:60.69ms
step:1566/2285 train_time:95042ms step_avg:60.69ms
step:1567/2285 train_time:95105ms step_avg:60.69ms
step:1568/2285 train_time:95166ms step_avg:60.69ms
step:1569/2285 train_time:95229ms step_avg:60.69ms
step:1570/2285 train_time:95289ms step_avg:60.69ms
step:1571/2285 train_time:95353ms step_avg:60.70ms
step:1572/2285 train_time:95413ms step_avg:60.70ms
step:1573/2285 train_time:95476ms step_avg:60.70ms
step:1574/2285 train_time:95536ms step_avg:60.70ms
step:1575/2285 train_time:95599ms step_avg:60.70ms
step:1576/2285 train_time:95659ms step_avg:60.70ms
step:1577/2285 train_time:95722ms step_avg:60.70ms
step:1578/2285 train_time:95782ms step_avg:60.70ms
step:1579/2285 train_time:95845ms step_avg:60.70ms
step:1580/2285 train_time:95905ms step_avg:60.70ms
step:1581/2285 train_time:95968ms step_avg:60.70ms
step:1582/2285 train_time:96029ms step_avg:60.70ms
step:1583/2285 train_time:96092ms step_avg:60.70ms
step:1584/2285 train_time:96153ms step_avg:60.70ms
step:1585/2285 train_time:96216ms step_avg:60.70ms
step:1586/2285 train_time:96276ms step_avg:60.70ms
step:1587/2285 train_time:96339ms step_avg:60.71ms
step:1588/2285 train_time:96399ms step_avg:60.70ms
step:1589/2285 train_time:96462ms step_avg:60.71ms
step:1590/2285 train_time:96522ms step_avg:60.71ms
step:1591/2285 train_time:96586ms step_avg:60.71ms
step:1592/2285 train_time:96646ms step_avg:60.71ms
step:1593/2285 train_time:96709ms step_avg:60.71ms
step:1594/2285 train_time:96769ms step_avg:60.71ms
step:1595/2285 train_time:96832ms step_avg:60.71ms
step:1596/2285 train_time:96892ms step_avg:60.71ms
step:1597/2285 train_time:96955ms step_avg:60.71ms
step:1598/2285 train_time:97016ms step_avg:60.71ms
step:1599/2285 train_time:97078ms step_avg:60.71ms
step:1600/2285 train_time:97139ms step_avg:60.71ms
step:1601/2285 train_time:97201ms step_avg:60.71ms
step:1602/2285 train_time:97261ms step_avg:60.71ms
step:1603/2285 train_time:97324ms step_avg:60.71ms
step:1604/2285 train_time:97384ms step_avg:60.71ms
step:1605/2285 train_time:97447ms step_avg:60.71ms
step:1606/2285 train_time:97507ms step_avg:60.71ms
step:1607/2285 train_time:97570ms step_avg:60.72ms
step:1608/2285 train_time:97630ms step_avg:60.72ms
step:1609/2285 train_time:97693ms step_avg:60.72ms
step:1610/2285 train_time:97754ms step_avg:60.72ms
step:1611/2285 train_time:97816ms step_avg:60.72ms
step:1612/2285 train_time:97876ms step_avg:60.72ms
step:1613/2285 train_time:97939ms step_avg:60.72ms
step:1614/2285 train_time:97999ms step_avg:60.72ms
step:1615/2285 train_time:98063ms step_avg:60.72ms
step:1616/2285 train_time:98123ms step_avg:60.72ms
step:1617/2285 train_time:98185ms step_avg:60.72ms
step:1618/2285 train_time:98246ms step_avg:60.72ms
step:1619/2285 train_time:98308ms step_avg:60.72ms
step:1620/2285 train_time:98368ms step_avg:60.72ms
step:1621/2285 train_time:98431ms step_avg:60.72ms
step:1622/2285 train_time:98491ms step_avg:60.72ms
step:1623/2285 train_time:98555ms step_avg:60.72ms
step:1624/2285 train_time:98615ms step_avg:60.72ms
step:1625/2285 train_time:98678ms step_avg:60.73ms
step:1626/2285 train_time:98738ms step_avg:60.72ms
step:1627/2285 train_time:98801ms step_avg:60.73ms
step:1628/2285 train_time:98861ms step_avg:60.73ms
step:1629/2285 train_time:98924ms step_avg:60.73ms
step:1630/2285 train_time:98983ms step_avg:60.73ms
step:1631/2285 train_time:99046ms step_avg:60.73ms
step:1632/2285 train_time:99106ms step_avg:60.73ms
step:1633/2285 train_time:99169ms step_avg:60.73ms
step:1634/2285 train_time:99230ms step_avg:60.73ms
step:1635/2285 train_time:99293ms step_avg:60.73ms
step:1636/2285 train_time:99353ms step_avg:60.73ms
step:1637/2285 train_time:99416ms step_avg:60.73ms
step:1638/2285 train_time:99476ms step_avg:60.73ms
step:1639/2285 train_time:99540ms step_avg:60.73ms
step:1640/2285 train_time:99600ms step_avg:60.73ms
step:1641/2285 train_time:99662ms step_avg:60.73ms
step:1642/2285 train_time:99722ms step_avg:60.73ms
step:1643/2285 train_time:99785ms step_avg:60.73ms
step:1644/2285 train_time:99845ms step_avg:60.73ms
step:1645/2285 train_time:99908ms step_avg:60.73ms
step:1646/2285 train_time:99968ms step_avg:60.73ms
step:1647/2285 train_time:100031ms step_avg:60.74ms
step:1648/2285 train_time:100092ms step_avg:60.74ms
step:1649/2285 train_time:100154ms step_avg:60.74ms
step:1650/2285 train_time:100215ms step_avg:60.74ms
step:1651/2285 train_time:100278ms step_avg:60.74ms
step:1652/2285 train_time:100338ms step_avg:60.74ms
step:1653/2285 train_time:100401ms step_avg:60.74ms
step:1654/2285 train_time:100461ms step_avg:60.74ms
step:1655/2285 train_time:100523ms step_avg:60.74ms
step:1656/2285 train_time:100583ms step_avg:60.74ms
step:1657/2285 train_time:100646ms step_avg:60.74ms
step:1658/2285 train_time:100706ms step_avg:60.74ms
step:1659/2285 train_time:100769ms step_avg:60.74ms
step:1660/2285 train_time:100829ms step_avg:60.74ms
step:1661/2285 train_time:100892ms step_avg:60.74ms
step:1662/2285 train_time:100953ms step_avg:60.74ms
step:1663/2285 train_time:101015ms step_avg:60.74ms
step:1664/2285 train_time:101075ms step_avg:60.74ms
step:1665/2285 train_time:101138ms step_avg:60.74ms
step:1666/2285 train_time:101197ms step_avg:60.74ms
step:1667/2285 train_time:101261ms step_avg:60.74ms
step:1668/2285 train_time:101321ms step_avg:60.74ms
step:1669/2285 train_time:101384ms step_avg:60.75ms
step:1670/2285 train_time:101444ms step_avg:60.74ms
step:1671/2285 train_time:101508ms step_avg:60.75ms
step:1672/2285 train_time:101569ms step_avg:60.75ms
step:1673/2285 train_time:101632ms step_avg:60.75ms
step:1674/2285 train_time:101692ms step_avg:60.75ms
step:1675/2285 train_time:101756ms step_avg:60.75ms
step:1676/2285 train_time:101816ms step_avg:60.75ms
step:1677/2285 train_time:101879ms step_avg:60.75ms
step:1678/2285 train_time:101939ms step_avg:60.75ms
step:1679/2285 train_time:102001ms step_avg:60.75ms
step:1680/2285 train_time:102061ms step_avg:60.75ms
step:1681/2285 train_time:102124ms step_avg:60.75ms
step:1682/2285 train_time:102184ms step_avg:60.75ms
step:1683/2285 train_time:102247ms step_avg:60.75ms
step:1684/2285 train_time:102307ms step_avg:60.75ms
step:1685/2285 train_time:102370ms step_avg:60.75ms
step:1686/2285 train_time:102431ms step_avg:60.75ms
step:1687/2285 train_time:102495ms step_avg:60.76ms
step:1688/2285 train_time:102555ms step_avg:60.76ms
step:1689/2285 train_time:102619ms step_avg:60.76ms
step:1690/2285 train_time:102679ms step_avg:60.76ms
step:1691/2285 train_time:102741ms step_avg:60.76ms
step:1692/2285 train_time:102800ms step_avg:60.76ms
step:1693/2285 train_time:102863ms step_avg:60.76ms
step:1694/2285 train_time:102923ms step_avg:60.76ms
step:1695/2285 train_time:102986ms step_avg:60.76ms
step:1696/2285 train_time:103046ms step_avg:60.76ms
step:1697/2285 train_time:103109ms step_avg:60.76ms
step:1698/2285 train_time:103170ms step_avg:60.76ms
step:1699/2285 train_time:103233ms step_avg:60.76ms
step:1700/2285 train_time:103294ms step_avg:60.76ms
step:1701/2285 train_time:103356ms step_avg:60.76ms
step:1702/2285 train_time:103416ms step_avg:60.76ms
step:1703/2285 train_time:103479ms step_avg:60.76ms
step:1704/2285 train_time:103539ms step_avg:60.76ms
step:1705/2285 train_time:103602ms step_avg:60.76ms
step:1706/2285 train_time:103662ms step_avg:60.76ms
step:1707/2285 train_time:103725ms step_avg:60.76ms
step:1708/2285 train_time:103786ms step_avg:60.76ms
step:1709/2285 train_time:103849ms step_avg:60.77ms
step:1710/2285 train_time:103909ms step_avg:60.77ms
step:1711/2285 train_time:103972ms step_avg:60.77ms
step:1712/2285 train_time:104032ms step_avg:60.77ms
step:1713/2285 train_time:104096ms step_avg:60.77ms
step:1714/2285 train_time:104157ms step_avg:60.77ms
step:1715/2285 train_time:104219ms step_avg:60.77ms
step:1716/2285 train_time:104279ms step_avg:60.77ms
step:1717/2285 train_time:104342ms step_avg:60.77ms
step:1718/2285 train_time:104402ms step_avg:60.77ms
step:1719/2285 train_time:104465ms step_avg:60.77ms
step:1720/2285 train_time:104525ms step_avg:60.77ms
step:1721/2285 train_time:104588ms step_avg:60.77ms
step:1722/2285 train_time:104648ms step_avg:60.77ms
step:1723/2285 train_time:104711ms step_avg:60.77ms
step:1724/2285 train_time:104772ms step_avg:60.77ms
step:1725/2285 train_time:104835ms step_avg:60.77ms
step:1726/2285 train_time:104895ms step_avg:60.77ms
step:1727/2285 train_time:104959ms step_avg:60.78ms
step:1728/2285 train_time:105019ms step_avg:60.77ms
step:1729/2285 train_time:105082ms step_avg:60.78ms
step:1730/2285 train_time:105142ms step_avg:60.78ms
step:1731/2285 train_time:105205ms step_avg:60.78ms
step:1732/2285 train_time:105265ms step_avg:60.78ms
step:1733/2285 train_time:105327ms step_avg:60.78ms
step:1734/2285 train_time:105388ms step_avg:60.78ms
step:1735/2285 train_time:105451ms step_avg:60.78ms
step:1736/2285 train_time:105512ms step_avg:60.78ms
step:1737/2285 train_time:105574ms step_avg:60.78ms
step:1738/2285 train_time:105635ms step_avg:60.78ms
step:1739/2285 train_time:105698ms step_avg:60.78ms
step:1740/2285 train_time:105758ms step_avg:60.78ms
step:1741/2285 train_time:105821ms step_avg:60.78ms
step:1742/2285 train_time:105880ms step_avg:60.78ms
step:1743/2285 train_time:105943ms step_avg:60.78ms
step:1744/2285 train_time:106003ms step_avg:60.78ms
step:1745/2285 train_time:106066ms step_avg:60.78ms
step:1746/2285 train_time:106126ms step_avg:60.78ms
step:1747/2285 train_time:106189ms step_avg:60.78ms
step:1748/2285 train_time:106249ms step_avg:60.78ms
step:1749/2285 train_time:106312ms step_avg:60.78ms
step:1750/2285 train_time:106373ms step_avg:60.78ms
step:1750/2285 val_loss:3.3706 train_time:106437ms step_avg:60.82ms
step:1751/2285 train_time:106457ms step_avg:60.80ms
step:1752/2285 train_time:106501ms step_avg:60.79ms
step:1753/2285 train_time:106568ms step_avg:60.79ms
step:1754/2285 train_time:106629ms step_avg:60.79ms
step:1755/2285 train_time:106692ms step_avg:60.79ms
step:1756/2285 train_time:106752ms step_avg:60.79ms
step:1757/2285 train_time:106814ms step_avg:60.79ms
step:1758/2285 train_time:106874ms step_avg:60.79ms
step:1759/2285 train_time:106936ms step_avg:60.79ms
step:1760/2285 train_time:106996ms step_avg:60.79ms
step:1761/2285 train_time:107058ms step_avg:60.79ms
step:1762/2285 train_time:107118ms step_avg:60.79ms
step:1763/2285 train_time:107180ms step_avg:60.79ms
step:1764/2285 train_time:107239ms step_avg:60.79ms
step:1765/2285 train_time:107301ms step_avg:60.79ms
step:1766/2285 train_time:107362ms step_avg:60.79ms
step:1767/2285 train_time:107427ms step_avg:60.80ms
step:1768/2285 train_time:107488ms step_avg:60.80ms
step:1769/2285 train_time:107552ms step_avg:60.80ms
step:1770/2285 train_time:107612ms step_avg:60.80ms
step:1771/2285 train_time:107676ms step_avg:60.80ms
step:1772/2285 train_time:107736ms step_avg:60.80ms
step:1773/2285 train_time:107799ms step_avg:60.80ms
step:1774/2285 train_time:107859ms step_avg:60.80ms
step:1775/2285 train_time:107922ms step_avg:60.80ms
step:1776/2285 train_time:107982ms step_avg:60.80ms
step:1777/2285 train_time:108044ms step_avg:60.80ms
step:1778/2285 train_time:108104ms step_avg:60.80ms
step:1779/2285 train_time:108167ms step_avg:60.80ms
step:1780/2285 train_time:108226ms step_avg:60.80ms
step:1781/2285 train_time:108289ms step_avg:60.80ms
step:1782/2285 train_time:108349ms step_avg:60.80ms
step:1783/2285 train_time:108412ms step_avg:60.80ms
step:1784/2285 train_time:108473ms step_avg:60.80ms
step:1785/2285 train_time:108536ms step_avg:60.80ms
step:1786/2285 train_time:108597ms step_avg:60.80ms
step:1787/2285 train_time:108661ms step_avg:60.81ms
step:1788/2285 train_time:108721ms step_avg:60.81ms
step:1789/2285 train_time:108784ms step_avg:60.81ms
step:1790/2285 train_time:108844ms step_avg:60.81ms
step:1791/2285 train_time:108907ms step_avg:60.81ms
step:1792/2285 train_time:108967ms step_avg:60.81ms
step:1793/2285 train_time:109029ms step_avg:60.81ms
step:1794/2285 train_time:109089ms step_avg:60.81ms
step:1795/2285 train_time:109151ms step_avg:60.81ms
step:1796/2285 train_time:109210ms step_avg:60.81ms
step:1797/2285 train_time:109273ms step_avg:60.81ms
step:1798/2285 train_time:109333ms step_avg:60.81ms
step:1799/2285 train_time:109396ms step_avg:60.81ms
step:1800/2285 train_time:109457ms step_avg:60.81ms
step:1801/2285 train_time:109521ms step_avg:60.81ms
step:1802/2285 train_time:109582ms step_avg:60.81ms
step:1803/2285 train_time:109645ms step_avg:60.81ms
step:1804/2285 train_time:109705ms step_avg:60.81ms
step:1805/2285 train_time:109768ms step_avg:60.81ms
step:1806/2285 train_time:109828ms step_avg:60.81ms
step:1807/2285 train_time:109890ms step_avg:60.81ms
step:1808/2285 train_time:109950ms step_avg:60.81ms
step:1809/2285 train_time:110013ms step_avg:60.81ms
step:1810/2285 train_time:110073ms step_avg:60.81ms
step:1811/2285 train_time:110135ms step_avg:60.81ms
step:1812/2285 train_time:110195ms step_avg:60.81ms
step:1813/2285 train_time:110258ms step_avg:60.82ms
step:1814/2285 train_time:110318ms step_avg:60.81ms
step:1815/2285 train_time:110381ms step_avg:60.82ms
step:1816/2285 train_time:110441ms step_avg:60.82ms
step:1817/2285 train_time:110505ms step_avg:60.82ms
step:1818/2285 train_time:110565ms step_avg:60.82ms
step:1819/2285 train_time:110628ms step_avg:60.82ms
step:1820/2285 train_time:110689ms step_avg:60.82ms
step:1821/2285 train_time:110752ms step_avg:60.82ms
step:1822/2285 train_time:110813ms step_avg:60.82ms
step:1823/2285 train_time:110875ms step_avg:60.82ms
step:1824/2285 train_time:110936ms step_avg:60.82ms
step:1825/2285 train_time:110998ms step_avg:60.82ms
step:1826/2285 train_time:111059ms step_avg:60.82ms
step:1827/2285 train_time:111122ms step_avg:60.82ms
step:1828/2285 train_time:111182ms step_avg:60.82ms
step:1829/2285 train_time:111245ms step_avg:60.82ms
step:1830/2285 train_time:111305ms step_avg:60.82ms
step:1831/2285 train_time:111367ms step_avg:60.82ms
step:1832/2285 train_time:111427ms step_avg:60.82ms
step:1833/2285 train_time:111490ms step_avg:60.82ms
step:1834/2285 train_time:111551ms step_avg:60.82ms
step:1835/2285 train_time:111613ms step_avg:60.82ms
step:1836/2285 train_time:111675ms step_avg:60.83ms
step:1837/2285 train_time:111738ms step_avg:60.83ms
step:1838/2285 train_time:111799ms step_avg:60.83ms
step:1839/2285 train_time:111862ms step_avg:60.83ms
step:1840/2285 train_time:111922ms step_avg:60.83ms
step:1841/2285 train_time:111984ms step_avg:60.83ms
step:1842/2285 train_time:112044ms step_avg:60.83ms
step:1843/2285 train_time:112106ms step_avg:60.83ms
step:1844/2285 train_time:112167ms step_avg:60.83ms
step:1845/2285 train_time:112229ms step_avg:60.83ms
step:1846/2285 train_time:112289ms step_avg:60.83ms
step:1847/2285 train_time:112351ms step_avg:60.83ms
step:1848/2285 train_time:112411ms step_avg:60.83ms
step:1849/2285 train_time:112474ms step_avg:60.83ms
step:1850/2285 train_time:112534ms step_avg:60.83ms
step:1851/2285 train_time:112597ms step_avg:60.83ms
step:1852/2285 train_time:112657ms step_avg:60.83ms
step:1853/2285 train_time:112720ms step_avg:60.83ms
step:1854/2285 train_time:112780ms step_avg:60.83ms
step:1855/2285 train_time:112843ms step_avg:60.83ms
step:1856/2285 train_time:112904ms step_avg:60.83ms
step:1857/2285 train_time:112967ms step_avg:60.83ms
step:1858/2285 train_time:113027ms step_avg:60.83ms
step:1859/2285 train_time:113089ms step_avg:60.83ms
step:1860/2285 train_time:113148ms step_avg:60.83ms
step:1861/2285 train_time:113211ms step_avg:60.83ms
step:1862/2285 train_time:113271ms step_avg:60.83ms
step:1863/2285 train_time:113333ms step_avg:60.83ms
step:1864/2285 train_time:113394ms step_avg:60.83ms
step:1865/2285 train_time:113456ms step_avg:60.83ms
step:1866/2285 train_time:113517ms step_avg:60.83ms
step:1867/2285 train_time:113581ms step_avg:60.84ms
step:1868/2285 train_time:113641ms step_avg:60.84ms
step:1869/2285 train_time:113703ms step_avg:60.84ms
step:1870/2285 train_time:113764ms step_avg:60.84ms
step:1871/2285 train_time:113827ms step_avg:60.84ms
step:1872/2285 train_time:113887ms step_avg:60.84ms
step:1873/2285 train_time:113950ms step_avg:60.84ms
step:1874/2285 train_time:114010ms step_avg:60.84ms
step:1875/2285 train_time:114073ms step_avg:60.84ms
step:1876/2285 train_time:114133ms step_avg:60.84ms
step:1877/2285 train_time:114195ms step_avg:60.84ms
step:1878/2285 train_time:114256ms step_avg:60.84ms
step:1879/2285 train_time:114319ms step_avg:60.84ms
step:1880/2285 train_time:114380ms step_avg:60.84ms
step:1881/2285 train_time:114443ms step_avg:60.84ms
step:1882/2285 train_time:114503ms step_avg:60.84ms
step:1883/2285 train_time:114566ms step_avg:60.84ms
step:1884/2285 train_time:114626ms step_avg:60.84ms
step:1885/2285 train_time:114689ms step_avg:60.84ms
step:1886/2285 train_time:114749ms step_avg:60.84ms
step:1887/2285 train_time:114812ms step_avg:60.84ms
step:1888/2285 train_time:114872ms step_avg:60.84ms
step:1889/2285 train_time:114935ms step_avg:60.84ms
step:1890/2285 train_time:114995ms step_avg:60.84ms
step:1891/2285 train_time:115058ms step_avg:60.85ms
step:1892/2285 train_time:115119ms step_avg:60.85ms
step:1893/2285 train_time:115182ms step_avg:60.85ms
step:1894/2285 train_time:115242ms step_avg:60.85ms
step:1895/2285 train_time:115305ms step_avg:60.85ms
step:1896/2285 train_time:115365ms step_avg:60.85ms
step:1897/2285 train_time:115428ms step_avg:60.85ms
step:1898/2285 train_time:115488ms step_avg:60.85ms
step:1899/2285 train_time:115550ms step_avg:60.85ms
step:1900/2285 train_time:115610ms step_avg:60.85ms
step:1901/2285 train_time:115673ms step_avg:60.85ms
step:1902/2285 train_time:115734ms step_avg:60.85ms
step:1903/2285 train_time:115797ms step_avg:60.85ms
step:1904/2285 train_time:115858ms step_avg:60.85ms
step:1905/2285 train_time:115920ms step_avg:60.85ms
step:1906/2285 train_time:115981ms step_avg:60.85ms
step:1907/2285 train_time:116044ms step_avg:60.85ms
step:1908/2285 train_time:116104ms step_avg:60.85ms
step:1909/2285 train_time:116167ms step_avg:60.85ms
step:1910/2285 train_time:116227ms step_avg:60.85ms
step:1911/2285 train_time:116290ms step_avg:60.85ms
step:1912/2285 train_time:116350ms step_avg:60.85ms
step:1913/2285 train_time:116412ms step_avg:60.85ms
step:1914/2285 train_time:116474ms step_avg:60.85ms
step:1915/2285 train_time:116537ms step_avg:60.85ms
step:1916/2285 train_time:116598ms step_avg:60.85ms
step:1917/2285 train_time:116661ms step_avg:60.86ms
step:1918/2285 train_time:116721ms step_avg:60.86ms
step:1919/2285 train_time:116783ms step_avg:60.86ms
step:1920/2285 train_time:116844ms step_avg:60.86ms
step:1921/2285 train_time:116907ms step_avg:60.86ms
step:1922/2285 train_time:116967ms step_avg:60.86ms
step:1923/2285 train_time:117030ms step_avg:60.86ms
step:1924/2285 train_time:117090ms step_avg:60.86ms
step:1925/2285 train_time:117153ms step_avg:60.86ms
step:1926/2285 train_time:117213ms step_avg:60.86ms
step:1927/2285 train_time:117276ms step_avg:60.86ms
step:1928/2285 train_time:117336ms step_avg:60.86ms
step:1929/2285 train_time:117399ms step_avg:60.86ms
step:1930/2285 train_time:117460ms step_avg:60.86ms
step:1931/2285 train_time:117523ms step_avg:60.86ms
step:1932/2285 train_time:117583ms step_avg:60.86ms
step:1933/2285 train_time:117647ms step_avg:60.86ms
step:1934/2285 train_time:117707ms step_avg:60.86ms
step:1935/2285 train_time:117769ms step_avg:60.86ms
step:1936/2285 train_time:117830ms step_avg:60.86ms
step:1937/2285 train_time:117892ms step_avg:60.86ms
step:1938/2285 train_time:117951ms step_avg:60.86ms
step:1939/2285 train_time:118014ms step_avg:60.86ms
step:1940/2285 train_time:118075ms step_avg:60.86ms
step:1941/2285 train_time:118138ms step_avg:60.86ms
step:1942/2285 train_time:118198ms step_avg:60.86ms
step:1943/2285 train_time:118260ms step_avg:60.86ms
step:1944/2285 train_time:118321ms step_avg:60.86ms
step:1945/2285 train_time:118384ms step_avg:60.87ms
step:1946/2285 train_time:118444ms step_avg:60.87ms
step:1947/2285 train_time:118507ms step_avg:60.87ms
step:1948/2285 train_time:118568ms step_avg:60.87ms
step:1949/2285 train_time:118630ms step_avg:60.87ms
step:1950/2285 train_time:118690ms step_avg:60.87ms
step:1951/2285 train_time:118752ms step_avg:60.87ms
step:1952/2285 train_time:118813ms step_avg:60.87ms
step:1953/2285 train_time:118876ms step_avg:60.87ms
step:1954/2285 train_time:118936ms step_avg:60.87ms
step:1955/2285 train_time:118999ms step_avg:60.87ms
step:1956/2285 train_time:119060ms step_avg:60.87ms
step:1957/2285 train_time:119123ms step_avg:60.87ms
step:1958/2285 train_time:119183ms step_avg:60.87ms
step:1959/2285 train_time:119246ms step_avg:60.87ms
step:1960/2285 train_time:119306ms step_avg:60.87ms
step:1961/2285 train_time:119369ms step_avg:60.87ms
step:1962/2285 train_time:119428ms step_avg:60.87ms
step:1963/2285 train_time:119491ms step_avg:60.87ms
step:1964/2285 train_time:119551ms step_avg:60.87ms
step:1965/2285 train_time:119614ms step_avg:60.87ms
step:1966/2285 train_time:119674ms step_avg:60.87ms
step:1967/2285 train_time:119737ms step_avg:60.87ms
step:1968/2285 train_time:119798ms step_avg:60.87ms
step:1969/2285 train_time:119861ms step_avg:60.87ms
step:1970/2285 train_time:119922ms step_avg:60.87ms
step:1971/2285 train_time:119986ms step_avg:60.88ms
step:1972/2285 train_time:120046ms step_avg:60.88ms
step:1973/2285 train_time:120108ms step_avg:60.88ms
step:1974/2285 train_time:120168ms step_avg:60.88ms
step:1975/2285 train_time:120231ms step_avg:60.88ms
step:1976/2285 train_time:120290ms step_avg:60.88ms
step:1977/2285 train_time:120353ms step_avg:60.88ms
step:1978/2285 train_time:120413ms step_avg:60.88ms
step:1979/2285 train_time:120477ms step_avg:60.88ms
step:1980/2285 train_time:120538ms step_avg:60.88ms
step:1981/2285 train_time:120600ms step_avg:60.88ms
step:1982/2285 train_time:120661ms step_avg:60.88ms
step:1983/2285 train_time:120725ms step_avg:60.88ms
step:1984/2285 train_time:120785ms step_avg:60.88ms
step:1985/2285 train_time:120847ms step_avg:60.88ms
step:1986/2285 train_time:120907ms step_avg:60.88ms
step:1987/2285 train_time:120969ms step_avg:60.88ms
step:1988/2285 train_time:121029ms step_avg:60.88ms
step:1989/2285 train_time:121092ms step_avg:60.88ms
step:1990/2285 train_time:121152ms step_avg:60.88ms
step:1991/2285 train_time:121215ms step_avg:60.88ms
step:1992/2285 train_time:121275ms step_avg:60.88ms
step:1993/2285 train_time:121338ms step_avg:60.88ms
step:1994/2285 train_time:121399ms step_avg:60.88ms
step:1995/2285 train_time:121461ms step_avg:60.88ms
step:1996/2285 train_time:121521ms step_avg:60.88ms
step:1997/2285 train_time:121584ms step_avg:60.88ms
step:1998/2285 train_time:121645ms step_avg:60.88ms
step:1999/2285 train_time:121708ms step_avg:60.88ms
step:2000/2285 train_time:121767ms step_avg:60.88ms
step:2000/2285 val_loss:3.3245 train_time:121831ms step_avg:60.92ms
step:2001/2285 train_time:121850ms step_avg:60.89ms
step:2002/2285 train_time:121892ms step_avg:60.89ms
step:2003/2285 train_time:121957ms step_avg:60.89ms
step:2004/2285 train_time:122019ms step_avg:60.89ms
step:2005/2285 train_time:122082ms step_avg:60.89ms
step:2006/2285 train_time:122142ms step_avg:60.89ms
step:2007/2285 train_time:122204ms step_avg:60.89ms
step:2008/2285 train_time:122264ms step_avg:60.89ms
step:2009/2285 train_time:122326ms step_avg:60.89ms
step:2010/2285 train_time:122386ms step_avg:60.89ms
step:2011/2285 train_time:122449ms step_avg:60.89ms
step:2012/2285 train_time:122509ms step_avg:60.89ms
step:2013/2285 train_time:122572ms step_avg:60.89ms
step:2014/2285 train_time:122632ms step_avg:60.89ms
step:2015/2285 train_time:122695ms step_avg:60.89ms
step:2016/2285 train_time:122755ms step_avg:60.89ms
step:2017/2285 train_time:122819ms step_avg:60.89ms
step:2018/2285 train_time:122880ms step_avg:60.89ms
step:2019/2285 train_time:122943ms step_avg:60.89ms
step:2020/2285 train_time:123004ms step_avg:60.89ms
step:2021/2285 train_time:123067ms step_avg:60.89ms
step:2022/2285 train_time:123127ms step_avg:60.89ms
step:2023/2285 train_time:123190ms step_avg:60.89ms
step:2024/2285 train_time:123250ms step_avg:60.89ms
step:2025/2285 train_time:123313ms step_avg:60.90ms
step:2026/2285 train_time:123373ms step_avg:60.89ms
step:2027/2285 train_time:123435ms step_avg:60.90ms
step:2028/2285 train_time:123494ms step_avg:60.89ms
step:2029/2285 train_time:123557ms step_avg:60.90ms
step:2030/2285 train_time:123616ms step_avg:60.89ms
step:2031/2285 train_time:123679ms step_avg:60.90ms
step:2032/2285 train_time:123740ms step_avg:60.90ms
step:2033/2285 train_time:123804ms step_avg:60.90ms
step:2034/2285 train_time:123864ms step_avg:60.90ms
step:2035/2285 train_time:123928ms step_avg:60.90ms
step:2036/2285 train_time:123989ms step_avg:60.90ms
step:2037/2285 train_time:124053ms step_avg:60.90ms
step:2038/2285 train_time:124113ms step_avg:60.90ms
step:2039/2285 train_time:124176ms step_avg:60.90ms
step:2040/2285 train_time:124236ms step_avg:60.90ms
step:2041/2285 train_time:124298ms step_avg:60.90ms
step:2042/2285 train_time:124358ms step_avg:60.90ms
step:2043/2285 train_time:124421ms step_avg:60.90ms
step:2044/2285 train_time:124481ms step_avg:60.90ms
step:2045/2285 train_time:124543ms step_avg:60.90ms
step:2046/2285 train_time:124604ms step_avg:60.90ms
step:2047/2285 train_time:124667ms step_avg:60.90ms
step:2048/2285 train_time:124728ms step_avg:60.90ms
step:2049/2285 train_time:124791ms step_avg:60.90ms
step:2050/2285 train_time:124852ms step_avg:60.90ms
step:2051/2285 train_time:124915ms step_avg:60.90ms
step:2052/2285 train_time:124975ms step_avg:60.90ms
step:2053/2285 train_time:125038ms step_avg:60.91ms
step:2054/2285 train_time:125098ms step_avg:60.90ms
step:2055/2285 train_time:125161ms step_avg:60.91ms
step:2056/2285 train_time:125222ms step_avg:60.91ms
step:2057/2285 train_time:125285ms step_avg:60.91ms
step:2058/2285 train_time:125345ms step_avg:60.91ms
step:2059/2285 train_time:125407ms step_avg:60.91ms
step:2060/2285 train_time:125467ms step_avg:60.91ms
step:2061/2285 train_time:125531ms step_avg:60.91ms
step:2062/2285 train_time:125592ms step_avg:60.91ms
step:2063/2285 train_time:125655ms step_avg:60.91ms
step:2064/2285 train_time:125715ms step_avg:60.91ms
step:2065/2285 train_time:125778ms step_avg:60.91ms
step:2066/2285 train_time:125838ms step_avg:60.91ms
step:2067/2285 train_time:125900ms step_avg:60.91ms
step:2068/2285 train_time:125961ms step_avg:60.91ms
step:2069/2285 train_time:126025ms step_avg:60.91ms
step:2070/2285 train_time:126085ms step_avg:60.91ms
step:2071/2285 train_time:126149ms step_avg:60.91ms
step:2072/2285 train_time:126210ms step_avg:60.91ms
step:2073/2285 train_time:126272ms step_avg:60.91ms
step:2074/2285 train_time:126332ms step_avg:60.91ms
step:2075/2285 train_time:126395ms step_avg:60.91ms
step:2076/2285 train_time:126456ms step_avg:60.91ms
step:2077/2285 train_time:126518ms step_avg:60.91ms
step:2078/2285 train_time:126578ms step_avg:60.91ms
step:2079/2285 train_time:126641ms step_avg:60.91ms
step:2080/2285 train_time:126701ms step_avg:60.91ms
step:2081/2285 train_time:126764ms step_avg:60.92ms
step:2082/2285 train_time:126824ms step_avg:60.91ms
step:2083/2285 train_time:126887ms step_avg:60.92ms
step:2084/2285 train_time:126947ms step_avg:60.92ms
step:2085/2285 train_time:127011ms step_avg:60.92ms
step:2086/2285 train_time:127072ms step_avg:60.92ms
step:2087/2285 train_time:127135ms step_avg:60.92ms
step:2088/2285 train_time:127196ms step_avg:60.92ms
step:2089/2285 train_time:127258ms step_avg:60.92ms
step:2090/2285 train_time:127318ms step_avg:60.92ms
step:2091/2285 train_time:127381ms step_avg:60.92ms
step:2092/2285 train_time:127442ms step_avg:60.92ms
step:2093/2285 train_time:127505ms step_avg:60.92ms
step:2094/2285 train_time:127566ms step_avg:60.92ms
step:2095/2285 train_time:127629ms step_avg:60.92ms
step:2096/2285 train_time:127689ms step_avg:60.92ms
step:2097/2285 train_time:127753ms step_avg:60.92ms
step:2098/2285 train_time:127813ms step_avg:60.92ms
step:2099/2285 train_time:127875ms step_avg:60.92ms
step:2100/2285 train_time:127935ms step_avg:60.92ms
step:2101/2285 train_time:127998ms step_avg:60.92ms
step:2102/2285 train_time:128059ms step_avg:60.92ms
step:2103/2285 train_time:128122ms step_avg:60.92ms
step:2104/2285 train_time:128182ms step_avg:60.92ms
step:2105/2285 train_time:128246ms step_avg:60.92ms
step:2106/2285 train_time:128307ms step_avg:60.92ms
step:2107/2285 train_time:128370ms step_avg:60.93ms
step:2108/2285 train_time:128431ms step_avg:60.93ms
step:2109/2285 train_time:128494ms step_avg:60.93ms
step:2110/2285 train_time:128554ms step_avg:60.93ms
step:2111/2285 train_time:128616ms step_avg:60.93ms
step:2112/2285 train_time:128677ms step_avg:60.93ms
step:2113/2285 train_time:128739ms step_avg:60.93ms
step:2114/2285 train_time:128799ms step_avg:60.93ms
step:2115/2285 train_time:128862ms step_avg:60.93ms
step:2116/2285 train_time:128922ms step_avg:60.93ms
step:2117/2285 train_time:128985ms step_avg:60.93ms
step:2118/2285 train_time:129046ms step_avg:60.93ms
step:2119/2285 train_time:129110ms step_avg:60.93ms
step:2120/2285 train_time:129170ms step_avg:60.93ms
step:2121/2285 train_time:129233ms step_avg:60.93ms
step:2122/2285 train_time:129293ms step_avg:60.93ms
step:2123/2285 train_time:129356ms step_avg:60.93ms
step:2124/2285 train_time:129416ms step_avg:60.93ms
step:2125/2285 train_time:129479ms step_avg:60.93ms
step:2126/2285 train_time:129538ms step_avg:60.93ms
step:2127/2285 train_time:129601ms step_avg:60.93ms
step:2128/2285 train_time:129662ms step_avg:60.93ms
step:2129/2285 train_time:129725ms step_avg:60.93ms
step:2130/2285 train_time:129785ms step_avg:60.93ms
step:2131/2285 train_time:129848ms step_avg:60.93ms
step:2132/2285 train_time:129908ms step_avg:60.93ms
step:2133/2285 train_time:129972ms step_avg:60.93ms
step:2134/2285 train_time:130032ms step_avg:60.93ms
step:2135/2285 train_time:130096ms step_avg:60.93ms
step:2136/2285 train_time:130155ms step_avg:60.93ms
step:2137/2285 train_time:130218ms step_avg:60.93ms
step:2138/2285 train_time:130278ms step_avg:60.93ms
step:2139/2285 train_time:130341ms step_avg:60.94ms
step:2140/2285 train_time:130402ms step_avg:60.94ms
step:2141/2285 train_time:130465ms step_avg:60.94ms
step:2142/2285 train_time:130525ms step_avg:60.94ms
step:2143/2285 train_time:130588ms step_avg:60.94ms
step:2144/2285 train_time:130649ms step_avg:60.94ms
step:2145/2285 train_time:130712ms step_avg:60.94ms
step:2146/2285 train_time:130772ms step_avg:60.94ms
step:2147/2285 train_time:130835ms step_avg:60.94ms
step:2148/2285 train_time:130896ms step_avg:60.94ms
step:2149/2285 train_time:130958ms step_avg:60.94ms
step:2150/2285 train_time:131018ms step_avg:60.94ms
step:2151/2285 train_time:131081ms step_avg:60.94ms
step:2152/2285 train_time:131142ms step_avg:60.94ms
step:2153/2285 train_time:131205ms step_avg:60.94ms
step:2154/2285 train_time:131265ms step_avg:60.94ms
step:2155/2285 train_time:131329ms step_avg:60.94ms
step:2156/2285 train_time:131390ms step_avg:60.94ms
step:2157/2285 train_time:131453ms step_avg:60.94ms
step:2158/2285 train_time:131513ms step_avg:60.94ms
step:2159/2285 train_time:131576ms step_avg:60.94ms
step:2160/2285 train_time:131635ms step_avg:60.94ms
step:2161/2285 train_time:131698ms step_avg:60.94ms
step:2162/2285 train_time:131758ms step_avg:60.94ms
step:2163/2285 train_time:131821ms step_avg:60.94ms
step:2164/2285 train_time:131882ms step_avg:60.94ms
step:2165/2285 train_time:131945ms step_avg:60.94ms
step:2166/2285 train_time:132006ms step_avg:60.94ms
step:2167/2285 train_time:132068ms step_avg:60.95ms
step:2168/2285 train_time:132128ms step_avg:60.94ms
step:2169/2285 train_time:132192ms step_avg:60.95ms
step:2170/2285 train_time:132253ms step_avg:60.95ms
step:2171/2285 train_time:132315ms step_avg:60.95ms
step:2172/2285 train_time:132375ms step_avg:60.95ms
step:2173/2285 train_time:132438ms step_avg:60.95ms
step:2174/2285 train_time:132498ms step_avg:60.95ms
step:2175/2285 train_time:132561ms step_avg:60.95ms
step:2176/2285 train_time:132621ms step_avg:60.95ms
step:2177/2285 train_time:132684ms step_avg:60.95ms
step:2178/2285 train_time:132745ms step_avg:60.95ms
step:2179/2285 train_time:132808ms step_avg:60.95ms
step:2180/2285 train_time:132868ms step_avg:60.95ms
step:2181/2285 train_time:132931ms step_avg:60.95ms
step:2182/2285 train_time:132992ms step_avg:60.95ms
step:2183/2285 train_time:133055ms step_avg:60.95ms
step:2184/2285 train_time:133115ms step_avg:60.95ms
step:2185/2285 train_time:133177ms step_avg:60.95ms
step:2186/2285 train_time:133237ms step_avg:60.95ms
step:2187/2285 train_time:133301ms step_avg:60.95ms
step:2188/2285 train_time:133361ms step_avg:60.95ms
step:2189/2285 train_time:133424ms step_avg:60.95ms
step:2190/2285 train_time:133484ms step_avg:60.95ms
step:2191/2285 train_time:133547ms step_avg:60.95ms
step:2192/2285 train_time:133607ms step_avg:60.95ms
step:2193/2285 train_time:133670ms step_avg:60.95ms
step:2194/2285 train_time:133730ms step_avg:60.95ms
step:2195/2285 train_time:133793ms step_avg:60.95ms
step:2196/2285 train_time:133854ms step_avg:60.95ms
step:2197/2285 train_time:133916ms step_avg:60.95ms
step:2198/2285 train_time:133976ms step_avg:60.95ms
step:2199/2285 train_time:134039ms step_avg:60.95ms
step:2200/2285 train_time:134099ms step_avg:60.95ms
step:2201/2285 train_time:134162ms step_avg:60.95ms
step:2202/2285 train_time:134222ms step_avg:60.95ms
step:2203/2285 train_time:134285ms step_avg:60.96ms
step:2204/2285 train_time:134346ms step_avg:60.96ms
step:2205/2285 train_time:134410ms step_avg:60.96ms
step:2206/2285 train_time:134470ms step_avg:60.96ms
step:2207/2285 train_time:134533ms step_avg:60.96ms
step:2208/2285 train_time:134594ms step_avg:60.96ms
step:2209/2285 train_time:134657ms step_avg:60.96ms
step:2210/2285 train_time:134717ms step_avg:60.96ms
step:2211/2285 train_time:134780ms step_avg:60.96ms
step:2212/2285 train_time:134840ms step_avg:60.96ms
step:2213/2285 train_time:134903ms step_avg:60.96ms
step:2214/2285 train_time:134964ms step_avg:60.96ms
step:2215/2285 train_time:135027ms step_avg:60.96ms
step:2216/2285 train_time:135087ms step_avg:60.96ms
step:2217/2285 train_time:135151ms step_avg:60.96ms
step:2218/2285 train_time:135211ms step_avg:60.96ms
step:2219/2285 train_time:135274ms step_avg:60.96ms
step:2220/2285 train_time:135334ms step_avg:60.96ms
step:2221/2285 train_time:135397ms step_avg:60.96ms
step:2222/2285 train_time:135457ms step_avg:60.96ms
step:2223/2285 train_time:135519ms step_avg:60.96ms
step:2224/2285 train_time:135579ms step_avg:60.96ms
step:2225/2285 train_time:135641ms step_avg:60.96ms
step:2226/2285 train_time:135702ms step_avg:60.96ms
step:2227/2285 train_time:135765ms step_avg:60.96ms
step:2228/2285 train_time:135826ms step_avg:60.96ms
step:2229/2285 train_time:135889ms step_avg:60.96ms
step:2230/2285 train_time:135949ms step_avg:60.96ms
step:2231/2285 train_time:136012ms step_avg:60.96ms
step:2232/2285 train_time:136072ms step_avg:60.96ms
step:2233/2285 train_time:136135ms step_avg:60.97ms
step:2234/2285 train_time:136196ms step_avg:60.96ms
step:2235/2285 train_time:136258ms step_avg:60.97ms
step:2236/2285 train_time:136318ms step_avg:60.97ms
step:2237/2285 train_time:136381ms step_avg:60.97ms
step:2238/2285 train_time:136441ms step_avg:60.97ms
step:2239/2285 train_time:136504ms step_avg:60.97ms
step:2240/2285 train_time:136565ms step_avg:60.97ms
step:2241/2285 train_time:136628ms step_avg:60.97ms
step:2242/2285 train_time:136689ms step_avg:60.97ms
step:2243/2285 train_time:136751ms step_avg:60.97ms
step:2244/2285 train_time:136811ms step_avg:60.97ms
step:2245/2285 train_time:136874ms step_avg:60.97ms
step:2246/2285 train_time:136935ms step_avg:60.97ms
step:2247/2285 train_time:136997ms step_avg:60.97ms
step:2248/2285 train_time:137057ms step_avg:60.97ms
step:2249/2285 train_time:137120ms step_avg:60.97ms
step:2250/2285 train_time:137181ms step_avg:60.97ms
step:2250/2285 val_loss:3.2875 train_time:137244ms step_avg:61.00ms
step:2251/2285 train_time:137263ms step_avg:60.98ms
step:2252/2285 train_time:137306ms step_avg:60.97ms
step:2253/2285 train_time:137370ms step_avg:60.97ms
step:2254/2285 train_time:137432ms step_avg:60.97ms
step:2255/2285 train_time:137496ms step_avg:60.97ms
step:2256/2285 train_time:137556ms step_avg:60.97ms
step:2257/2285 train_time:137618ms step_avg:60.97ms
step:2258/2285 train_time:137678ms step_avg:60.97ms
step:2259/2285 train_time:137739ms step_avg:60.97ms
step:2260/2285 train_time:137800ms step_avg:60.97ms
step:2261/2285 train_time:137862ms step_avg:60.97ms
step:2262/2285 train_time:137921ms step_avg:60.97ms
step:2263/2285 train_time:137983ms step_avg:60.97ms
step:2264/2285 train_time:138043ms step_avg:60.97ms
step:2265/2285 train_time:138106ms step_avg:60.97ms
step:2266/2285 train_time:138172ms step_avg:60.98ms
step:2267/2285 train_time:138238ms step_avg:60.98ms
step:2268/2285 train_time:138300ms step_avg:60.98ms
step:2269/2285 train_time:138362ms step_avg:60.98ms
step:2270/2285 train_time:138423ms step_avg:60.98ms
step:2271/2285 train_time:138487ms step_avg:60.98ms
step:2272/2285 train_time:138547ms step_avg:60.98ms
step:2273/2285 train_time:138610ms step_avg:60.98ms
step:2274/2285 train_time:138670ms step_avg:60.98ms
step:2275/2285 train_time:138732ms step_avg:60.98ms
step:2276/2285 train_time:138792ms step_avg:60.98ms
step:2277/2285 train_time:138855ms step_avg:60.98ms
step:2278/2285 train_time:138915ms step_avg:60.98ms
step:2279/2285 train_time:138977ms step_avg:60.98ms
step:2280/2285 train_time:139037ms step_avg:60.98ms
step:2281/2285 train_time:139100ms step_avg:60.98ms
step:2282/2285 train_time:139161ms step_avg:60.98ms
step:2283/2285 train_time:139224ms step_avg:60.98ms
step:2284/2285 train_time:139286ms step_avg:60.98ms
step:2285/2285 train_time:139350ms step_avg:60.98ms
step:2285/2285 val_loss:3.2813 train_time:139411ms step_avg:61.01ms
peak memory allocated: 29248 MiB reserved: 50528 MiB

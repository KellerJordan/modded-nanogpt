import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2285
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 01:55:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:116ms step_avg:115.76ms
step:2/2285 train_time:137ms step_avg:68.68ms
step:3/2285 train_time:174ms step_avg:58.06ms
step:4/2285 train_time:230ms step_avg:57.53ms
step:5/2285 train_time:289ms step_avg:57.89ms
step:6/2285 train_time:348ms step_avg:57.93ms
step:7/2285 train_time:408ms step_avg:58.30ms
step:8/2285 train_time:466ms step_avg:58.28ms
step:9/2285 train_time:527ms step_avg:58.60ms
step:10/2285 train_time:585ms step_avg:58.55ms
step:11/2285 train_time:646ms step_avg:58.73ms
step:12/2285 train_time:704ms step_avg:58.68ms
step:13/2285 train_time:765ms step_avg:58.87ms
step:14/2285 train_time:824ms step_avg:58.83ms
step:15/2285 train_time:884ms step_avg:58.96ms
step:16/2285 train_time:943ms step_avg:58.95ms
step:17/2285 train_time:1006ms step_avg:59.20ms
step:18/2285 train_time:1070ms step_avg:59.42ms
step:19/2285 train_time:1134ms step_avg:59.70ms
step:20/2285 train_time:1195ms step_avg:59.73ms
step:21/2285 train_time:1257ms step_avg:59.85ms
step:22/2285 train_time:1315ms step_avg:59.79ms
step:23/2285 train_time:1377ms step_avg:59.89ms
step:24/2285 train_time:1436ms step_avg:59.85ms
step:25/2285 train_time:1498ms step_avg:59.91ms
step:26/2285 train_time:1557ms step_avg:59.88ms
step:27/2285 train_time:1618ms step_avg:59.94ms
step:28/2285 train_time:1677ms step_avg:59.90ms
step:29/2285 train_time:1738ms step_avg:59.93ms
step:30/2285 train_time:1796ms step_avg:59.88ms
step:31/2285 train_time:1858ms step_avg:59.93ms
step:32/2285 train_time:1917ms step_avg:59.90ms
step:33/2285 train_time:1979ms step_avg:59.97ms
step:34/2285 train_time:2039ms step_avg:59.98ms
step:35/2285 train_time:2103ms step_avg:60.07ms
step:36/2285 train_time:2164ms step_avg:60.11ms
step:37/2285 train_time:2227ms step_avg:60.18ms
step:38/2285 train_time:2286ms step_avg:60.15ms
step:39/2285 train_time:2347ms step_avg:60.19ms
step:40/2285 train_time:2407ms step_avg:60.18ms
step:41/2285 train_time:2467ms step_avg:60.18ms
step:42/2285 train_time:2526ms step_avg:60.15ms
step:43/2285 train_time:2588ms step_avg:60.18ms
step:44/2285 train_time:2646ms step_avg:60.14ms
step:45/2285 train_time:2708ms step_avg:60.18ms
step:46/2285 train_time:2768ms step_avg:60.18ms
step:47/2285 train_time:2830ms step_avg:60.20ms
step:48/2285 train_time:2888ms step_avg:60.17ms
step:49/2285 train_time:2950ms step_avg:60.20ms
step:50/2285 train_time:3009ms step_avg:60.18ms
step:51/2285 train_time:3070ms step_avg:60.21ms
step:52/2285 train_time:3130ms step_avg:60.18ms
step:53/2285 train_time:3191ms step_avg:60.21ms
step:54/2285 train_time:3250ms step_avg:60.19ms
step:55/2285 train_time:3312ms step_avg:60.21ms
step:56/2285 train_time:3371ms step_avg:60.20ms
step:57/2285 train_time:3432ms step_avg:60.21ms
step:58/2285 train_time:3490ms step_avg:60.18ms
step:59/2285 train_time:3551ms step_avg:60.19ms
step:60/2285 train_time:3610ms step_avg:60.17ms
step:61/2285 train_time:3672ms step_avg:60.19ms
step:62/2285 train_time:3730ms step_avg:60.16ms
step:63/2285 train_time:3792ms step_avg:60.19ms
step:64/2285 train_time:3850ms step_avg:60.16ms
step:65/2285 train_time:3911ms step_avg:60.17ms
step:66/2285 train_time:3970ms step_avg:60.15ms
step:67/2285 train_time:4032ms step_avg:60.18ms
step:68/2285 train_time:4091ms step_avg:60.15ms
step:69/2285 train_time:4152ms step_avg:60.17ms
step:70/2285 train_time:4211ms step_avg:60.15ms
step:71/2285 train_time:4271ms step_avg:60.16ms
step:72/2285 train_time:4330ms step_avg:60.14ms
step:73/2285 train_time:4391ms step_avg:60.15ms
step:74/2285 train_time:4450ms step_avg:60.13ms
step:75/2285 train_time:4511ms step_avg:60.15ms
step:76/2285 train_time:4570ms step_avg:60.13ms
step:77/2285 train_time:4631ms step_avg:60.14ms
step:78/2285 train_time:4690ms step_avg:60.13ms
step:79/2285 train_time:4751ms step_avg:60.14ms
step:80/2285 train_time:4809ms step_avg:60.12ms
step:81/2285 train_time:4871ms step_avg:60.14ms
step:82/2285 train_time:4930ms step_avg:60.12ms
step:83/2285 train_time:4992ms step_avg:60.14ms
step:84/2285 train_time:5050ms step_avg:60.12ms
step:85/2285 train_time:5112ms step_avg:60.14ms
step:86/2285 train_time:5170ms step_avg:60.12ms
step:87/2285 train_time:5232ms step_avg:60.14ms
step:88/2285 train_time:5290ms step_avg:60.12ms
step:89/2285 train_time:5351ms step_avg:60.13ms
step:90/2285 train_time:5410ms step_avg:60.11ms
step:91/2285 train_time:5471ms step_avg:60.12ms
step:92/2285 train_time:5530ms step_avg:60.11ms
step:93/2285 train_time:5590ms step_avg:60.11ms
step:94/2285 train_time:5649ms step_avg:60.10ms
step:95/2285 train_time:5710ms step_avg:60.11ms
step:96/2285 train_time:5769ms step_avg:60.09ms
step:97/2285 train_time:5830ms step_avg:60.10ms
step:98/2285 train_time:5889ms step_avg:60.09ms
step:99/2285 train_time:5950ms step_avg:60.10ms
step:100/2285 train_time:6009ms step_avg:60.09ms
step:101/2285 train_time:6070ms step_avg:60.10ms
step:102/2285 train_time:6129ms step_avg:60.09ms
step:103/2285 train_time:6190ms step_avg:60.10ms
step:104/2285 train_time:6249ms step_avg:60.09ms
step:105/2285 train_time:6310ms step_avg:60.10ms
step:106/2285 train_time:6369ms step_avg:60.08ms
step:107/2285 train_time:6430ms step_avg:60.09ms
step:108/2285 train_time:6489ms step_avg:60.08ms
step:109/2285 train_time:6549ms step_avg:60.09ms
step:110/2285 train_time:6608ms step_avg:60.07ms
step:111/2285 train_time:6669ms step_avg:60.08ms
step:112/2285 train_time:6727ms step_avg:60.07ms
step:113/2285 train_time:6788ms step_avg:60.07ms
step:114/2285 train_time:6847ms step_avg:60.06ms
step:115/2285 train_time:6908ms step_avg:60.07ms
step:116/2285 train_time:6967ms step_avg:60.06ms
step:117/2285 train_time:7029ms step_avg:60.08ms
step:118/2285 train_time:7088ms step_avg:60.07ms
step:119/2285 train_time:7149ms step_avg:60.08ms
step:120/2285 train_time:7208ms step_avg:60.07ms
step:121/2285 train_time:7269ms step_avg:60.08ms
step:122/2285 train_time:7328ms step_avg:60.07ms
step:123/2285 train_time:7389ms step_avg:60.07ms
step:124/2285 train_time:7447ms step_avg:60.06ms
step:125/2285 train_time:7509ms step_avg:60.07ms
step:126/2285 train_time:7567ms step_avg:60.06ms
step:127/2285 train_time:7628ms step_avg:60.06ms
step:128/2285 train_time:7687ms step_avg:60.05ms
step:129/2285 train_time:7748ms step_avg:60.06ms
step:130/2285 train_time:7807ms step_avg:60.05ms
step:131/2285 train_time:7868ms step_avg:60.06ms
step:132/2285 train_time:7927ms step_avg:60.05ms
step:133/2285 train_time:7988ms step_avg:60.06ms
step:134/2285 train_time:8047ms step_avg:60.05ms
step:135/2285 train_time:8108ms step_avg:60.06ms
step:136/2285 train_time:8167ms step_avg:60.05ms
step:137/2285 train_time:8228ms step_avg:60.06ms
step:138/2285 train_time:8287ms step_avg:60.05ms
step:139/2285 train_time:8348ms step_avg:60.06ms
step:140/2285 train_time:8407ms step_avg:60.05ms
step:141/2285 train_time:8468ms step_avg:60.06ms
step:142/2285 train_time:8527ms step_avg:60.05ms
step:143/2285 train_time:8588ms step_avg:60.06ms
step:144/2285 train_time:8646ms step_avg:60.04ms
step:145/2285 train_time:8708ms step_avg:60.05ms
step:146/2285 train_time:8766ms step_avg:60.04ms
step:147/2285 train_time:8828ms step_avg:60.05ms
step:148/2285 train_time:8886ms step_avg:60.04ms
step:149/2285 train_time:8947ms step_avg:60.05ms
step:150/2285 train_time:9006ms step_avg:60.04ms
step:151/2285 train_time:9067ms step_avg:60.05ms
step:152/2285 train_time:9127ms step_avg:60.04ms
step:153/2285 train_time:9188ms step_avg:60.05ms
step:154/2285 train_time:9246ms step_avg:60.04ms
step:155/2285 train_time:9307ms step_avg:60.05ms
step:156/2285 train_time:9366ms step_avg:60.04ms
step:157/2285 train_time:9428ms step_avg:60.05ms
step:158/2285 train_time:9486ms step_avg:60.04ms
step:159/2285 train_time:9547ms step_avg:60.04ms
step:160/2285 train_time:9605ms step_avg:60.03ms
step:161/2285 train_time:9667ms step_avg:60.04ms
step:162/2285 train_time:9726ms step_avg:60.04ms
step:163/2285 train_time:9787ms step_avg:60.04ms
step:164/2285 train_time:9845ms step_avg:60.03ms
step:165/2285 train_time:9906ms step_avg:60.04ms
step:166/2285 train_time:9965ms step_avg:60.03ms
step:167/2285 train_time:10026ms step_avg:60.04ms
step:168/2285 train_time:10086ms step_avg:60.03ms
step:169/2285 train_time:10146ms step_avg:60.04ms
step:170/2285 train_time:10205ms step_avg:60.03ms
step:171/2285 train_time:10266ms step_avg:60.04ms
step:172/2285 train_time:10325ms step_avg:60.03ms
step:173/2285 train_time:10387ms step_avg:60.04ms
step:174/2285 train_time:10445ms step_avg:60.03ms
step:175/2285 train_time:10506ms step_avg:60.04ms
step:176/2285 train_time:10565ms step_avg:60.03ms
step:177/2285 train_time:10627ms step_avg:60.04ms
step:178/2285 train_time:10685ms step_avg:60.03ms
step:179/2285 train_time:10746ms step_avg:60.03ms
step:180/2285 train_time:10805ms step_avg:60.03ms
step:181/2285 train_time:10866ms step_avg:60.03ms
step:182/2285 train_time:10926ms step_avg:60.04ms
step:183/2285 train_time:10987ms step_avg:60.04ms
step:184/2285 train_time:11045ms step_avg:60.03ms
step:185/2285 train_time:11106ms step_avg:60.03ms
step:186/2285 train_time:11165ms step_avg:60.03ms
step:187/2285 train_time:11226ms step_avg:60.03ms
step:188/2285 train_time:11285ms step_avg:60.03ms
step:189/2285 train_time:11346ms step_avg:60.03ms
step:190/2285 train_time:11405ms step_avg:60.03ms
step:191/2285 train_time:11466ms step_avg:60.03ms
step:192/2285 train_time:11525ms step_avg:60.02ms
step:193/2285 train_time:11586ms step_avg:60.03ms
step:194/2285 train_time:11644ms step_avg:60.02ms
step:195/2285 train_time:11705ms step_avg:60.03ms
step:196/2285 train_time:11764ms step_avg:60.02ms
step:197/2285 train_time:11825ms step_avg:60.03ms
step:198/2285 train_time:11884ms step_avg:60.02ms
step:199/2285 train_time:11946ms step_avg:60.03ms
step:200/2285 train_time:12005ms step_avg:60.02ms
step:201/2285 train_time:12066ms step_avg:60.03ms
step:202/2285 train_time:12125ms step_avg:60.03ms
step:203/2285 train_time:12186ms step_avg:60.03ms
step:204/2285 train_time:12245ms step_avg:60.02ms
step:205/2285 train_time:12306ms step_avg:60.03ms
step:206/2285 train_time:12366ms step_avg:60.03ms
step:207/2285 train_time:12427ms step_avg:60.03ms
step:208/2285 train_time:12485ms step_avg:60.02ms
step:209/2285 train_time:12546ms step_avg:60.03ms
step:210/2285 train_time:12606ms step_avg:60.03ms
step:211/2285 train_time:12667ms step_avg:60.04ms
step:212/2285 train_time:12726ms step_avg:60.03ms
step:213/2285 train_time:12787ms step_avg:60.03ms
step:214/2285 train_time:12846ms step_avg:60.03ms
step:215/2285 train_time:12907ms step_avg:60.03ms
step:216/2285 train_time:12966ms step_avg:60.03ms
step:217/2285 train_time:13027ms step_avg:60.03ms
step:218/2285 train_time:13086ms step_avg:60.03ms
step:219/2285 train_time:13147ms step_avg:60.03ms
step:220/2285 train_time:13206ms step_avg:60.03ms
step:221/2285 train_time:13267ms step_avg:60.03ms
step:222/2285 train_time:13326ms step_avg:60.03ms
step:223/2285 train_time:13387ms step_avg:60.03ms
step:224/2285 train_time:13445ms step_avg:60.02ms
step:225/2285 train_time:13507ms step_avg:60.03ms
step:226/2285 train_time:13565ms step_avg:60.02ms
step:227/2285 train_time:13627ms step_avg:60.03ms
step:228/2285 train_time:13685ms step_avg:60.02ms
step:229/2285 train_time:13746ms step_avg:60.03ms
step:230/2285 train_time:13805ms step_avg:60.02ms
step:231/2285 train_time:13866ms step_avg:60.03ms
step:232/2285 train_time:13925ms step_avg:60.02ms
step:233/2285 train_time:13986ms step_avg:60.03ms
step:234/2285 train_time:14045ms step_avg:60.02ms
step:235/2285 train_time:14107ms step_avg:60.03ms
step:236/2285 train_time:14166ms step_avg:60.02ms
step:237/2285 train_time:14227ms step_avg:60.03ms
step:238/2285 train_time:14286ms step_avg:60.02ms
step:239/2285 train_time:14346ms step_avg:60.03ms
step:240/2285 train_time:14406ms step_avg:60.02ms
step:241/2285 train_time:14467ms step_avg:60.03ms
step:242/2285 train_time:14526ms step_avg:60.02ms
step:243/2285 train_time:14586ms step_avg:60.03ms
step:244/2285 train_time:14645ms step_avg:60.02ms
step:245/2285 train_time:14706ms step_avg:60.02ms
step:246/2285 train_time:14765ms step_avg:60.02ms
step:247/2285 train_time:14826ms step_avg:60.02ms
step:248/2285 train_time:14884ms step_avg:60.02ms
step:249/2285 train_time:14945ms step_avg:60.02ms
step:250/2285 train_time:15004ms step_avg:60.01ms
step:250/2285 val_loss:4.0876 train_time:15067ms step_avg:60.27ms
step:251/2285 train_time:15086ms step_avg:60.10ms
step:252/2285 train_time:15126ms step_avg:60.02ms
step:253/2285 train_time:15194ms step_avg:60.06ms
step:254/2285 train_time:15259ms step_avg:60.08ms
step:255/2285 train_time:15320ms step_avg:60.08ms
step:256/2285 train_time:15379ms step_avg:60.07ms
step:257/2285 train_time:15439ms step_avg:60.07ms
step:258/2285 train_time:15497ms step_avg:60.07ms
step:259/2285 train_time:15557ms step_avg:60.07ms
step:260/2285 train_time:15615ms step_avg:60.06ms
step:261/2285 train_time:15676ms step_avg:60.06ms
step:262/2285 train_time:15733ms step_avg:60.05ms
step:263/2285 train_time:15793ms step_avg:60.05ms
step:264/2285 train_time:15851ms step_avg:60.04ms
step:265/2285 train_time:15911ms step_avg:60.04ms
step:266/2285 train_time:15969ms step_avg:60.03ms
step:267/2285 train_time:16031ms step_avg:60.04ms
step:268/2285 train_time:16090ms step_avg:60.04ms
step:269/2285 train_time:16153ms step_avg:60.05ms
step:270/2285 train_time:16216ms step_avg:60.06ms
step:271/2285 train_time:16277ms step_avg:60.06ms
step:272/2285 train_time:16335ms step_avg:60.06ms
step:273/2285 train_time:16397ms step_avg:60.06ms
step:274/2285 train_time:16455ms step_avg:60.06ms
step:275/2285 train_time:16516ms step_avg:60.06ms
step:276/2285 train_time:16576ms step_avg:60.06ms
step:277/2285 train_time:16635ms step_avg:60.05ms
step:278/2285 train_time:16693ms step_avg:60.05ms
step:279/2285 train_time:16753ms step_avg:60.05ms
step:280/2285 train_time:16811ms step_avg:60.04ms
step:281/2285 train_time:16870ms step_avg:60.04ms
step:282/2285 train_time:16928ms step_avg:60.03ms
step:283/2285 train_time:16989ms step_avg:60.03ms
step:284/2285 train_time:17047ms step_avg:60.02ms
step:285/2285 train_time:17109ms step_avg:60.03ms
step:286/2285 train_time:17169ms step_avg:60.03ms
step:287/2285 train_time:17231ms step_avg:60.04ms
step:288/2285 train_time:17290ms step_avg:60.04ms
step:289/2285 train_time:17352ms step_avg:60.04ms
step:290/2285 train_time:17411ms step_avg:60.04ms
step:291/2285 train_time:17472ms step_avg:60.04ms
step:292/2285 train_time:17531ms step_avg:60.04ms
step:293/2285 train_time:17592ms step_avg:60.04ms
step:294/2285 train_time:17651ms step_avg:60.04ms
step:295/2285 train_time:17712ms step_avg:60.04ms
step:296/2285 train_time:17770ms step_avg:60.03ms
step:297/2285 train_time:17829ms step_avg:60.03ms
step:298/2285 train_time:17887ms step_avg:60.02ms
step:299/2285 train_time:17948ms step_avg:60.03ms
step:300/2285 train_time:18006ms step_avg:60.02ms
step:301/2285 train_time:18067ms step_avg:60.02ms
step:302/2285 train_time:18126ms step_avg:60.02ms
step:303/2285 train_time:18187ms step_avg:60.02ms
step:304/2285 train_time:18246ms step_avg:60.02ms
step:305/2285 train_time:18309ms step_avg:60.03ms
step:306/2285 train_time:18367ms step_avg:60.02ms
step:307/2285 train_time:18429ms step_avg:60.03ms
step:308/2285 train_time:18488ms step_avg:60.03ms
step:309/2285 train_time:18549ms step_avg:60.03ms
step:310/2285 train_time:18609ms step_avg:60.03ms
step:311/2285 train_time:18669ms step_avg:60.03ms
step:312/2285 train_time:18728ms step_avg:60.02ms
step:313/2285 train_time:18788ms step_avg:60.03ms
step:314/2285 train_time:18846ms step_avg:60.02ms
step:315/2285 train_time:18907ms step_avg:60.02ms
step:316/2285 train_time:18965ms step_avg:60.02ms
step:317/2285 train_time:19027ms step_avg:60.02ms
step:318/2285 train_time:19085ms step_avg:60.01ms
step:319/2285 train_time:19147ms step_avg:60.02ms
step:320/2285 train_time:19206ms step_avg:60.02ms
step:321/2285 train_time:19268ms step_avg:60.03ms
step:322/2285 train_time:19327ms step_avg:60.02ms
step:323/2285 train_time:19389ms step_avg:60.03ms
step:324/2285 train_time:19448ms step_avg:60.02ms
step:325/2285 train_time:19510ms step_avg:60.03ms
step:326/2285 train_time:19569ms step_avg:60.03ms
step:327/2285 train_time:19629ms step_avg:60.03ms
step:328/2285 train_time:19688ms step_avg:60.02ms
step:329/2285 train_time:19748ms step_avg:60.02ms
step:330/2285 train_time:19807ms step_avg:60.02ms
step:331/2285 train_time:19867ms step_avg:60.02ms
step:332/2285 train_time:19925ms step_avg:60.02ms
step:333/2285 train_time:19986ms step_avg:60.02ms
step:334/2285 train_time:20045ms step_avg:60.01ms
step:335/2285 train_time:20105ms step_avg:60.02ms
step:336/2285 train_time:20164ms step_avg:60.01ms
step:337/2285 train_time:20225ms step_avg:60.01ms
step:338/2285 train_time:20284ms step_avg:60.01ms
step:339/2285 train_time:20345ms step_avg:60.01ms
step:340/2285 train_time:20404ms step_avg:60.01ms
step:341/2285 train_time:20467ms step_avg:60.02ms
step:342/2285 train_time:20526ms step_avg:60.02ms
step:343/2285 train_time:20587ms step_avg:60.02ms
step:344/2285 train_time:20646ms step_avg:60.02ms
step:345/2285 train_time:20707ms step_avg:60.02ms
step:346/2285 train_time:20766ms step_avg:60.02ms
step:347/2285 train_time:20827ms step_avg:60.02ms
step:348/2285 train_time:20885ms step_avg:60.01ms
step:349/2285 train_time:20946ms step_avg:60.02ms
step:350/2285 train_time:21004ms step_avg:60.01ms
step:351/2285 train_time:21065ms step_avg:60.01ms
step:352/2285 train_time:21123ms step_avg:60.01ms
step:353/2285 train_time:21184ms step_avg:60.01ms
step:354/2285 train_time:21242ms step_avg:60.01ms
step:355/2285 train_time:21304ms step_avg:60.01ms
step:356/2285 train_time:21362ms step_avg:60.01ms
step:357/2285 train_time:21425ms step_avg:60.01ms
step:358/2285 train_time:21483ms step_avg:60.01ms
step:359/2285 train_time:21544ms step_avg:60.01ms
step:360/2285 train_time:21603ms step_avg:60.01ms
step:361/2285 train_time:21664ms step_avg:60.01ms
step:362/2285 train_time:21723ms step_avg:60.01ms
step:363/2285 train_time:21783ms step_avg:60.01ms
step:364/2285 train_time:21842ms step_avg:60.01ms
step:365/2285 train_time:21902ms step_avg:60.01ms
step:366/2285 train_time:21960ms step_avg:60.00ms
step:367/2285 train_time:22021ms step_avg:60.00ms
step:368/2285 train_time:22079ms step_avg:60.00ms
step:369/2285 train_time:22140ms step_avg:60.00ms
step:370/2285 train_time:22197ms step_avg:59.99ms
step:371/2285 train_time:22259ms step_avg:60.00ms
step:372/2285 train_time:22317ms step_avg:59.99ms
step:373/2285 train_time:22378ms step_avg:60.00ms
step:374/2285 train_time:22437ms step_avg:59.99ms
step:375/2285 train_time:22497ms step_avg:59.99ms
step:376/2285 train_time:22556ms step_avg:59.99ms
step:377/2285 train_time:22617ms step_avg:59.99ms
step:378/2285 train_time:22676ms step_avg:59.99ms
step:379/2285 train_time:22736ms step_avg:59.99ms
step:380/2285 train_time:22794ms step_avg:59.99ms
step:381/2285 train_time:22855ms step_avg:59.99ms
step:382/2285 train_time:22914ms step_avg:59.98ms
step:383/2285 train_time:22975ms step_avg:59.99ms
step:384/2285 train_time:23034ms step_avg:59.98ms
step:385/2285 train_time:23095ms step_avg:59.99ms
step:386/2285 train_time:23154ms step_avg:59.98ms
step:387/2285 train_time:23215ms step_avg:59.99ms
step:388/2285 train_time:23274ms step_avg:59.98ms
step:389/2285 train_time:23335ms step_avg:59.99ms
step:390/2285 train_time:23394ms step_avg:59.99ms
step:391/2285 train_time:23456ms step_avg:59.99ms
step:392/2285 train_time:23515ms step_avg:59.99ms
step:393/2285 train_time:23576ms step_avg:59.99ms
step:394/2285 train_time:23635ms step_avg:59.99ms
step:395/2285 train_time:23696ms step_avg:59.99ms
step:396/2285 train_time:23755ms step_avg:59.99ms
step:397/2285 train_time:23817ms step_avg:59.99ms
step:398/2285 train_time:23875ms step_avg:59.99ms
step:399/2285 train_time:23936ms step_avg:59.99ms
step:400/2285 train_time:23995ms step_avg:59.99ms
step:401/2285 train_time:24055ms step_avg:59.99ms
step:402/2285 train_time:24114ms step_avg:59.99ms
step:403/2285 train_time:24175ms step_avg:59.99ms
step:404/2285 train_time:24234ms step_avg:59.98ms
step:405/2285 train_time:24296ms step_avg:59.99ms
step:406/2285 train_time:24355ms step_avg:59.99ms
step:407/2285 train_time:24417ms step_avg:59.99ms
step:408/2285 train_time:24475ms step_avg:59.99ms
step:409/2285 train_time:24536ms step_avg:59.99ms
step:410/2285 train_time:24595ms step_avg:59.99ms
step:411/2285 train_time:24657ms step_avg:59.99ms
step:412/2285 train_time:24715ms step_avg:59.99ms
step:413/2285 train_time:24776ms step_avg:59.99ms
step:414/2285 train_time:24835ms step_avg:59.99ms
step:415/2285 train_time:24896ms step_avg:59.99ms
step:416/2285 train_time:24955ms step_avg:59.99ms
step:417/2285 train_time:25017ms step_avg:59.99ms
step:418/2285 train_time:25075ms step_avg:59.99ms
step:419/2285 train_time:25136ms step_avg:59.99ms
step:420/2285 train_time:25195ms step_avg:59.99ms
step:421/2285 train_time:25256ms step_avg:59.99ms
step:422/2285 train_time:25315ms step_avg:59.99ms
step:423/2285 train_time:25376ms step_avg:59.99ms
step:424/2285 train_time:25435ms step_avg:59.99ms
step:425/2285 train_time:25496ms step_avg:59.99ms
step:426/2285 train_time:25555ms step_avg:59.99ms
step:427/2285 train_time:25617ms step_avg:59.99ms
step:428/2285 train_time:25675ms step_avg:59.99ms
step:429/2285 train_time:25736ms step_avg:59.99ms
step:430/2285 train_time:25795ms step_avg:59.99ms
step:431/2285 train_time:25856ms step_avg:59.99ms
step:432/2285 train_time:25915ms step_avg:59.99ms
step:433/2285 train_time:25976ms step_avg:59.99ms
step:434/2285 train_time:26035ms step_avg:59.99ms
step:435/2285 train_time:26096ms step_avg:59.99ms
step:436/2285 train_time:26155ms step_avg:59.99ms
step:437/2285 train_time:26216ms step_avg:59.99ms
step:438/2285 train_time:26275ms step_avg:59.99ms
step:439/2285 train_time:26336ms step_avg:59.99ms
step:440/2285 train_time:26394ms step_avg:59.99ms
step:441/2285 train_time:26456ms step_avg:59.99ms
step:442/2285 train_time:26515ms step_avg:59.99ms
step:443/2285 train_time:26576ms step_avg:59.99ms
step:444/2285 train_time:26635ms step_avg:59.99ms
step:445/2285 train_time:26696ms step_avg:59.99ms
step:446/2285 train_time:26755ms step_avg:59.99ms
step:447/2285 train_time:26816ms step_avg:59.99ms
step:448/2285 train_time:26875ms step_avg:59.99ms
step:449/2285 train_time:26936ms step_avg:59.99ms
step:450/2285 train_time:26995ms step_avg:59.99ms
step:451/2285 train_time:27056ms step_avg:59.99ms
step:452/2285 train_time:27115ms step_avg:59.99ms
step:453/2285 train_time:27177ms step_avg:59.99ms
step:454/2285 train_time:27236ms step_avg:59.99ms
step:455/2285 train_time:27297ms step_avg:59.99ms
step:456/2285 train_time:27356ms step_avg:59.99ms
step:457/2285 train_time:27417ms step_avg:59.99ms
step:458/2285 train_time:27476ms step_avg:59.99ms
step:459/2285 train_time:27537ms step_avg:59.99ms
step:460/2285 train_time:27595ms step_avg:59.99ms
step:461/2285 train_time:27656ms step_avg:59.99ms
step:462/2285 train_time:27715ms step_avg:59.99ms
step:463/2285 train_time:27779ms step_avg:60.00ms
step:464/2285 train_time:27834ms step_avg:59.99ms
step:465/2285 train_time:27895ms step_avg:59.99ms
step:466/2285 train_time:27954ms step_avg:59.99ms
step:467/2285 train_time:28015ms step_avg:59.99ms
step:468/2285 train_time:28074ms step_avg:59.99ms
step:469/2285 train_time:28136ms step_avg:59.99ms
step:470/2285 train_time:28195ms step_avg:59.99ms
step:471/2285 train_time:28256ms step_avg:59.99ms
step:472/2285 train_time:28315ms step_avg:59.99ms
step:473/2285 train_time:28376ms step_avg:59.99ms
step:474/2285 train_time:28435ms step_avg:59.99ms
step:475/2285 train_time:28496ms step_avg:59.99ms
step:476/2285 train_time:28555ms step_avg:59.99ms
step:477/2285 train_time:28616ms step_avg:59.99ms
step:478/2285 train_time:28675ms step_avg:59.99ms
step:479/2285 train_time:28736ms step_avg:59.99ms
step:480/2285 train_time:28795ms step_avg:59.99ms
step:481/2285 train_time:28856ms step_avg:59.99ms
step:482/2285 train_time:28915ms step_avg:59.99ms
step:483/2285 train_time:28975ms step_avg:59.99ms
step:484/2285 train_time:29034ms step_avg:59.99ms
step:485/2285 train_time:29095ms step_avg:59.99ms
step:486/2285 train_time:29154ms step_avg:59.99ms
step:487/2285 train_time:29216ms step_avg:59.99ms
step:488/2285 train_time:29275ms step_avg:59.99ms
step:489/2285 train_time:29336ms step_avg:59.99ms
step:490/2285 train_time:29395ms step_avg:59.99ms
step:491/2285 train_time:29457ms step_avg:59.99ms
step:492/2285 train_time:29516ms step_avg:59.99ms
step:493/2285 train_time:29578ms step_avg:60.00ms
step:494/2285 train_time:29636ms step_avg:59.99ms
step:495/2285 train_time:29697ms step_avg:59.99ms
step:496/2285 train_time:29757ms step_avg:59.99ms
step:497/2285 train_time:29818ms step_avg:60.00ms
step:498/2285 train_time:29877ms step_avg:59.99ms
step:499/2285 train_time:29938ms step_avg:60.00ms
step:500/2285 train_time:29996ms step_avg:59.99ms
step:500/2285 val_loss:3.7874 train_time:30059ms step_avg:60.12ms
step:501/2285 train_time:30087ms step_avg:60.05ms
step:502/2285 train_time:30120ms step_avg:60.00ms
step:503/2285 train_time:30180ms step_avg:60.00ms
step:504/2285 train_time:30241ms step_avg:60.00ms
step:505/2285 train_time:30303ms step_avg:60.01ms
step:506/2285 train_time:30362ms step_avg:60.00ms
step:507/2285 train_time:30423ms step_avg:60.01ms
step:508/2285 train_time:30481ms step_avg:60.00ms
step:509/2285 train_time:30542ms step_avg:60.00ms
step:510/2285 train_time:30600ms step_avg:60.00ms
step:511/2285 train_time:30660ms step_avg:60.00ms
step:512/2285 train_time:30719ms step_avg:60.00ms
step:513/2285 train_time:30779ms step_avg:60.00ms
step:514/2285 train_time:30838ms step_avg:60.00ms
step:515/2285 train_time:30898ms step_avg:60.00ms
step:516/2285 train_time:30957ms step_avg:60.00ms
step:517/2285 train_time:31023ms step_avg:60.01ms
step:518/2285 train_time:31085ms step_avg:60.01ms
step:519/2285 train_time:31147ms step_avg:60.01ms
step:520/2285 train_time:31206ms step_avg:60.01ms
step:521/2285 train_time:31267ms step_avg:60.01ms
step:522/2285 train_time:31326ms step_avg:60.01ms
step:523/2285 train_time:31387ms step_avg:60.01ms
step:524/2285 train_time:31446ms step_avg:60.01ms
step:525/2285 train_time:31507ms step_avg:60.01ms
step:526/2285 train_time:31566ms step_avg:60.01ms
step:527/2285 train_time:31627ms step_avg:60.01ms
step:528/2285 train_time:31686ms step_avg:60.01ms
step:529/2285 train_time:31747ms step_avg:60.01ms
step:530/2285 train_time:31806ms step_avg:60.01ms
step:531/2285 train_time:31867ms step_avg:60.01ms
step:532/2285 train_time:31926ms step_avg:60.01ms
step:533/2285 train_time:31989ms step_avg:60.02ms
step:534/2285 train_time:32048ms step_avg:60.02ms
step:535/2285 train_time:32110ms step_avg:60.02ms
step:536/2285 train_time:32170ms step_avg:60.02ms
step:537/2285 train_time:32233ms step_avg:60.02ms
step:538/2285 train_time:32293ms step_avg:60.02ms
step:539/2285 train_time:32354ms step_avg:60.03ms
step:540/2285 train_time:32413ms step_avg:60.02ms
step:541/2285 train_time:32474ms step_avg:60.03ms
step:542/2285 train_time:32533ms step_avg:60.02ms
step:543/2285 train_time:32594ms step_avg:60.03ms
step:544/2285 train_time:32653ms step_avg:60.02ms
step:545/2285 train_time:32714ms step_avg:60.03ms
step:546/2285 train_time:32773ms step_avg:60.02ms
step:547/2285 train_time:32835ms step_avg:60.03ms
step:548/2285 train_time:32894ms step_avg:60.03ms
step:549/2285 train_time:32957ms step_avg:60.03ms
step:550/2285 train_time:33016ms step_avg:60.03ms
step:551/2285 train_time:33077ms step_avg:60.03ms
step:552/2285 train_time:33137ms step_avg:60.03ms
step:553/2285 train_time:33199ms step_avg:60.03ms
step:554/2285 train_time:33258ms step_avg:60.03ms
step:555/2285 train_time:33320ms step_avg:60.04ms
step:556/2285 train_time:33378ms step_avg:60.03ms
step:557/2285 train_time:33439ms step_avg:60.03ms
step:558/2285 train_time:33498ms step_avg:60.03ms
step:559/2285 train_time:33559ms step_avg:60.03ms
step:560/2285 train_time:33618ms step_avg:60.03ms
step:561/2285 train_time:33680ms step_avg:60.04ms
step:562/2285 train_time:33739ms step_avg:60.03ms
step:563/2285 train_time:33800ms step_avg:60.04ms
step:564/2285 train_time:33859ms step_avg:60.03ms
step:565/2285 train_time:33921ms step_avg:60.04ms
step:566/2285 train_time:33979ms step_avg:60.03ms
step:567/2285 train_time:34041ms step_avg:60.04ms
step:568/2285 train_time:34100ms step_avg:60.04ms
step:569/2285 train_time:34162ms step_avg:60.04ms
step:570/2285 train_time:34220ms step_avg:60.04ms
step:571/2285 train_time:34282ms step_avg:60.04ms
step:572/2285 train_time:34341ms step_avg:60.04ms
step:573/2285 train_time:34402ms step_avg:60.04ms
step:574/2285 train_time:34461ms step_avg:60.04ms
step:575/2285 train_time:34522ms step_avg:60.04ms
step:576/2285 train_time:34580ms step_avg:60.04ms
step:577/2285 train_time:34642ms step_avg:60.04ms
step:578/2285 train_time:34700ms step_avg:60.04ms
step:579/2285 train_time:34762ms step_avg:60.04ms
step:580/2285 train_time:34820ms step_avg:60.04ms
step:581/2285 train_time:34881ms step_avg:60.04ms
step:582/2285 train_time:34940ms step_avg:60.03ms
step:583/2285 train_time:35002ms step_avg:60.04ms
step:584/2285 train_time:35061ms step_avg:60.04ms
step:585/2285 train_time:35122ms step_avg:60.04ms
step:586/2285 train_time:35181ms step_avg:60.04ms
step:587/2285 train_time:35242ms step_avg:60.04ms
step:588/2285 train_time:35301ms step_avg:60.04ms
step:589/2285 train_time:35362ms step_avg:60.04ms
step:590/2285 train_time:35421ms step_avg:60.04ms
step:591/2285 train_time:35482ms step_avg:60.04ms
step:592/2285 train_time:35541ms step_avg:60.04ms
step:593/2285 train_time:35602ms step_avg:60.04ms
step:594/2285 train_time:35661ms step_avg:60.04ms
step:595/2285 train_time:35722ms step_avg:60.04ms
step:596/2285 train_time:35781ms step_avg:60.03ms
step:597/2285 train_time:35842ms step_avg:60.04ms
step:598/2285 train_time:35901ms step_avg:60.03ms
step:599/2285 train_time:35962ms step_avg:60.04ms
step:600/2285 train_time:36021ms step_avg:60.03ms
step:601/2285 train_time:36082ms step_avg:60.04ms
step:602/2285 train_time:36141ms step_avg:60.03ms
step:603/2285 train_time:36202ms step_avg:60.04ms
step:604/2285 train_time:36261ms step_avg:60.03ms
step:605/2285 train_time:36322ms step_avg:60.04ms
step:606/2285 train_time:36381ms step_avg:60.03ms
step:607/2285 train_time:36443ms step_avg:60.04ms
step:608/2285 train_time:36502ms step_avg:60.04ms
step:609/2285 train_time:36562ms step_avg:60.04ms
step:610/2285 train_time:36621ms step_avg:60.03ms
step:611/2285 train_time:36682ms step_avg:60.04ms
step:612/2285 train_time:36741ms step_avg:60.03ms
step:613/2285 train_time:36802ms step_avg:60.04ms
step:614/2285 train_time:36861ms step_avg:60.03ms
step:615/2285 train_time:36922ms step_avg:60.04ms
step:616/2285 train_time:36981ms step_avg:60.03ms
step:617/2285 train_time:37043ms step_avg:60.04ms
step:618/2285 train_time:37102ms step_avg:60.04ms
step:619/2285 train_time:37163ms step_avg:60.04ms
step:620/2285 train_time:37221ms step_avg:60.03ms
step:621/2285 train_time:37282ms step_avg:60.04ms
step:622/2285 train_time:37341ms step_avg:60.03ms
step:623/2285 train_time:37402ms step_avg:60.04ms
step:624/2285 train_time:37461ms step_avg:60.03ms
step:625/2285 train_time:37522ms step_avg:60.04ms
step:626/2285 train_time:37581ms step_avg:60.03ms
step:627/2285 train_time:37642ms step_avg:60.04ms
step:628/2285 train_time:37701ms step_avg:60.03ms
step:629/2285 train_time:37763ms step_avg:60.04ms
step:630/2285 train_time:37821ms step_avg:60.03ms
step:631/2285 train_time:37882ms step_avg:60.04ms
step:632/2285 train_time:37941ms step_avg:60.03ms
step:633/2285 train_time:38003ms step_avg:60.04ms
step:634/2285 train_time:38061ms step_avg:60.03ms
step:635/2285 train_time:38122ms step_avg:60.03ms
step:636/2285 train_time:38181ms step_avg:60.03ms
step:637/2285 train_time:38242ms step_avg:60.03ms
step:638/2285 train_time:38301ms step_avg:60.03ms
step:639/2285 train_time:38362ms step_avg:60.03ms
step:640/2285 train_time:38420ms step_avg:60.03ms
step:641/2285 train_time:38482ms step_avg:60.03ms
step:642/2285 train_time:38541ms step_avg:60.03ms
step:643/2285 train_time:38602ms step_avg:60.03ms
step:644/2285 train_time:38661ms step_avg:60.03ms
step:645/2285 train_time:38723ms step_avg:60.03ms
step:646/2285 train_time:38781ms step_avg:60.03ms
step:647/2285 train_time:38842ms step_avg:60.03ms
step:648/2285 train_time:38901ms step_avg:60.03ms
step:649/2285 train_time:38962ms step_avg:60.03ms
step:650/2285 train_time:39021ms step_avg:60.03ms
step:651/2285 train_time:39081ms step_avg:60.03ms
step:652/2285 train_time:39140ms step_avg:60.03ms
step:653/2285 train_time:39202ms step_avg:60.03ms
step:654/2285 train_time:39260ms step_avg:60.03ms
step:655/2285 train_time:39322ms step_avg:60.03ms
step:656/2285 train_time:39381ms step_avg:60.03ms
step:657/2285 train_time:39442ms step_avg:60.03ms
step:658/2285 train_time:39501ms step_avg:60.03ms
step:659/2285 train_time:39563ms step_avg:60.03ms
step:660/2285 train_time:39622ms step_avg:60.03ms
step:661/2285 train_time:39684ms step_avg:60.04ms
step:662/2285 train_time:39743ms step_avg:60.03ms
step:663/2285 train_time:39804ms step_avg:60.04ms
step:664/2285 train_time:39862ms step_avg:60.03ms
step:665/2285 train_time:39923ms step_avg:60.03ms
step:666/2285 train_time:39982ms step_avg:60.03ms
step:667/2285 train_time:40042ms step_avg:60.03ms
step:668/2285 train_time:40101ms step_avg:60.03ms
step:669/2285 train_time:40163ms step_avg:60.03ms
step:670/2285 train_time:40221ms step_avg:60.03ms
step:671/2285 train_time:40282ms step_avg:60.03ms
step:672/2285 train_time:40341ms step_avg:60.03ms
step:673/2285 train_time:40402ms step_avg:60.03ms
step:674/2285 train_time:40461ms step_avg:60.03ms
step:675/2285 train_time:40522ms step_avg:60.03ms
step:676/2285 train_time:40582ms step_avg:60.03ms
step:677/2285 train_time:40643ms step_avg:60.03ms
step:678/2285 train_time:40702ms step_avg:60.03ms
step:679/2285 train_time:40764ms step_avg:60.03ms
step:680/2285 train_time:40822ms step_avg:60.03ms
step:681/2285 train_time:40884ms step_avg:60.03ms
step:682/2285 train_time:40942ms step_avg:60.03ms
step:683/2285 train_time:41003ms step_avg:60.03ms
step:684/2285 train_time:41062ms step_avg:60.03ms
step:685/2285 train_time:41123ms step_avg:60.03ms
step:686/2285 train_time:41181ms step_avg:60.03ms
step:687/2285 train_time:41242ms step_avg:60.03ms
step:688/2285 train_time:41301ms step_avg:60.03ms
step:689/2285 train_time:41364ms step_avg:60.03ms
step:690/2285 train_time:41422ms step_avg:60.03ms
step:691/2285 train_time:41483ms step_avg:60.03ms
step:692/2285 train_time:41542ms step_avg:60.03ms
step:693/2285 train_time:41603ms step_avg:60.03ms
step:694/2285 train_time:41662ms step_avg:60.03ms
step:695/2285 train_time:41724ms step_avg:60.03ms
step:696/2285 train_time:41783ms step_avg:60.03ms
step:697/2285 train_time:41843ms step_avg:60.03ms
step:698/2285 train_time:41902ms step_avg:60.03ms
step:699/2285 train_time:41964ms step_avg:60.03ms
step:700/2285 train_time:42022ms step_avg:60.03ms
step:701/2285 train_time:42083ms step_avg:60.03ms
step:702/2285 train_time:42142ms step_avg:60.03ms
step:703/2285 train_time:42203ms step_avg:60.03ms
step:704/2285 train_time:42262ms step_avg:60.03ms
step:705/2285 train_time:42323ms step_avg:60.03ms
step:706/2285 train_time:42382ms step_avg:60.03ms
step:707/2285 train_time:42443ms step_avg:60.03ms
step:708/2285 train_time:42502ms step_avg:60.03ms
step:709/2285 train_time:42563ms step_avg:60.03ms
step:710/2285 train_time:42622ms step_avg:60.03ms
step:711/2285 train_time:42684ms step_avg:60.03ms
step:712/2285 train_time:42742ms step_avg:60.03ms
step:713/2285 train_time:42804ms step_avg:60.03ms
step:714/2285 train_time:42863ms step_avg:60.03ms
step:715/2285 train_time:42925ms step_avg:60.03ms
step:716/2285 train_time:42983ms step_avg:60.03ms
step:717/2285 train_time:43044ms step_avg:60.03ms
step:718/2285 train_time:43104ms step_avg:60.03ms
step:719/2285 train_time:43165ms step_avg:60.03ms
step:720/2285 train_time:43223ms step_avg:60.03ms
step:721/2285 train_time:43284ms step_avg:60.03ms
step:722/2285 train_time:43343ms step_avg:60.03ms
step:723/2285 train_time:43404ms step_avg:60.03ms
step:724/2285 train_time:43463ms step_avg:60.03ms
step:725/2285 train_time:43524ms step_avg:60.03ms
step:726/2285 train_time:43583ms step_avg:60.03ms
step:727/2285 train_time:43645ms step_avg:60.03ms
step:728/2285 train_time:43703ms step_avg:60.03ms
step:729/2285 train_time:43765ms step_avg:60.03ms
step:730/2285 train_time:43823ms step_avg:60.03ms
step:731/2285 train_time:43885ms step_avg:60.03ms
step:732/2285 train_time:43944ms step_avg:60.03ms
step:733/2285 train_time:44005ms step_avg:60.03ms
step:734/2285 train_time:44064ms step_avg:60.03ms
step:735/2285 train_time:44125ms step_avg:60.03ms
step:736/2285 train_time:44184ms step_avg:60.03ms
step:737/2285 train_time:44245ms step_avg:60.03ms
step:738/2285 train_time:44304ms step_avg:60.03ms
step:739/2285 train_time:44366ms step_avg:60.04ms
step:740/2285 train_time:44424ms step_avg:60.03ms
step:741/2285 train_time:44486ms step_avg:60.04ms
step:742/2285 train_time:44545ms step_avg:60.03ms
step:743/2285 train_time:44607ms step_avg:60.04ms
step:744/2285 train_time:44666ms step_avg:60.03ms
step:745/2285 train_time:44727ms step_avg:60.04ms
step:746/2285 train_time:44786ms step_avg:60.04ms
step:747/2285 train_time:44848ms step_avg:60.04ms
step:748/2285 train_time:44907ms step_avg:60.04ms
step:749/2285 train_time:44968ms step_avg:60.04ms
step:750/2285 train_time:45028ms step_avg:60.04ms
step:750/2285 val_loss:3.6604 train_time:45092ms step_avg:60.12ms
step:751/2285 train_time:45116ms step_avg:60.08ms
step:752/2285 train_time:45153ms step_avg:60.04ms
step:753/2285 train_time:45216ms step_avg:60.05ms
step:754/2285 train_time:45281ms step_avg:60.06ms
step:755/2285 train_time:45343ms step_avg:60.06ms
step:756/2285 train_time:45401ms step_avg:60.05ms
step:757/2285 train_time:45462ms step_avg:60.06ms
step:758/2285 train_time:45521ms step_avg:60.05ms
step:759/2285 train_time:45583ms step_avg:60.06ms
step:760/2285 train_time:45641ms step_avg:60.05ms
step:761/2285 train_time:45702ms step_avg:60.06ms
step:762/2285 train_time:45760ms step_avg:60.05ms
step:763/2285 train_time:45821ms step_avg:60.05ms
step:764/2285 train_time:45880ms step_avg:60.05ms
step:765/2285 train_time:45941ms step_avg:60.05ms
step:766/2285 train_time:46001ms step_avg:60.05ms
step:767/2285 train_time:46065ms step_avg:60.06ms
step:768/2285 train_time:46126ms step_avg:60.06ms
step:769/2285 train_time:46190ms step_avg:60.06ms
step:770/2285 train_time:46251ms step_avg:60.07ms
step:771/2285 train_time:46313ms step_avg:60.07ms
step:772/2285 train_time:46372ms step_avg:60.07ms
step:773/2285 train_time:46433ms step_avg:60.07ms
step:774/2285 train_time:46492ms step_avg:60.07ms
step:775/2285 train_time:46553ms step_avg:60.07ms
step:776/2285 train_time:46613ms step_avg:60.07ms
step:777/2285 train_time:46674ms step_avg:60.07ms
step:778/2285 train_time:46733ms step_avg:60.07ms
step:779/2285 train_time:46795ms step_avg:60.07ms
step:780/2285 train_time:46854ms step_avg:60.07ms
step:781/2285 train_time:46915ms step_avg:60.07ms
step:782/2285 train_time:46975ms step_avg:60.07ms
step:783/2285 train_time:47037ms step_avg:60.07ms
step:784/2285 train_time:47097ms step_avg:60.07ms
step:785/2285 train_time:47161ms step_avg:60.08ms
step:786/2285 train_time:47221ms step_avg:60.08ms
step:787/2285 train_time:47284ms step_avg:60.08ms
step:788/2285 train_time:47343ms step_avg:60.08ms
step:789/2285 train_time:47405ms step_avg:60.08ms
step:790/2285 train_time:47465ms step_avg:60.08ms
step:791/2285 train_time:47526ms step_avg:60.08ms
step:792/2285 train_time:47586ms step_avg:60.08ms
step:793/2285 train_time:47648ms step_avg:60.09ms
step:794/2285 train_time:47706ms step_avg:60.08ms
step:795/2285 train_time:47768ms step_avg:60.09ms
step:796/2285 train_time:47827ms step_avg:60.08ms
step:797/2285 train_time:47888ms step_avg:60.09ms
step:798/2285 train_time:47947ms step_avg:60.08ms
step:799/2285 train_time:48010ms step_avg:60.09ms
step:800/2285 train_time:48069ms step_avg:60.09ms
step:801/2285 train_time:48132ms step_avg:60.09ms
step:802/2285 train_time:48191ms step_avg:60.09ms
step:803/2285 train_time:48254ms step_avg:60.09ms
step:804/2285 train_time:48312ms step_avg:60.09ms
step:805/2285 train_time:48374ms step_avg:60.09ms
step:806/2285 train_time:48434ms step_avg:60.09ms
step:807/2285 train_time:48496ms step_avg:60.09ms
step:808/2285 train_time:48555ms step_avg:60.09ms
step:809/2285 train_time:48617ms step_avg:60.10ms
step:810/2285 train_time:48677ms step_avg:60.09ms
step:811/2285 train_time:48738ms step_avg:60.10ms
step:812/2285 train_time:48797ms step_avg:60.10ms
step:813/2285 train_time:48859ms step_avg:60.10ms
step:814/2285 train_time:48919ms step_avg:60.10ms
step:815/2285 train_time:48981ms step_avg:60.10ms
step:816/2285 train_time:49040ms step_avg:60.10ms
step:817/2285 train_time:49102ms step_avg:60.10ms
step:818/2285 train_time:49162ms step_avg:60.10ms
step:819/2285 train_time:49224ms step_avg:60.10ms
step:820/2285 train_time:49283ms step_avg:60.10ms
step:821/2285 train_time:49345ms step_avg:60.10ms
step:822/2285 train_time:49405ms step_avg:60.10ms
step:823/2285 train_time:49467ms step_avg:60.11ms
step:824/2285 train_time:49527ms step_avg:60.11ms
step:825/2285 train_time:49589ms step_avg:60.11ms
step:826/2285 train_time:49648ms step_avg:60.11ms
step:827/2285 train_time:49709ms step_avg:60.11ms
step:828/2285 train_time:49768ms step_avg:60.11ms
step:829/2285 train_time:49830ms step_avg:60.11ms
step:830/2285 train_time:49889ms step_avg:60.11ms
step:831/2285 train_time:49951ms step_avg:60.11ms
step:832/2285 train_time:50011ms step_avg:60.11ms
step:833/2285 train_time:50073ms step_avg:60.11ms
step:834/2285 train_time:50132ms step_avg:60.11ms
step:835/2285 train_time:50194ms step_avg:60.11ms
step:836/2285 train_time:50253ms step_avg:60.11ms
step:837/2285 train_time:50315ms step_avg:60.11ms
step:838/2285 train_time:50374ms step_avg:60.11ms
step:839/2285 train_time:50437ms step_avg:60.12ms
step:840/2285 train_time:50496ms step_avg:60.11ms
step:841/2285 train_time:50560ms step_avg:60.12ms
step:842/2285 train_time:50618ms step_avg:60.12ms
step:843/2285 train_time:50680ms step_avg:60.12ms
step:844/2285 train_time:50739ms step_avg:60.12ms
step:845/2285 train_time:50801ms step_avg:60.12ms
step:846/2285 train_time:50861ms step_avg:60.12ms
step:847/2285 train_time:50923ms step_avg:60.12ms
step:848/2285 train_time:50982ms step_avg:60.12ms
step:849/2285 train_time:51045ms step_avg:60.12ms
step:850/2285 train_time:51104ms step_avg:60.12ms
step:851/2285 train_time:51166ms step_avg:60.12ms
step:852/2285 train_time:51225ms step_avg:60.12ms
step:853/2285 train_time:51287ms step_avg:60.13ms
step:854/2285 train_time:51347ms step_avg:60.13ms
step:855/2285 train_time:51408ms step_avg:60.13ms
step:856/2285 train_time:51468ms step_avg:60.13ms
step:857/2285 train_time:51530ms step_avg:60.13ms
step:858/2285 train_time:51589ms step_avg:60.13ms
step:859/2285 train_time:51651ms step_avg:60.13ms
step:860/2285 train_time:51710ms step_avg:60.13ms
step:861/2285 train_time:51771ms step_avg:60.13ms
step:862/2285 train_time:51831ms step_avg:60.13ms
step:863/2285 train_time:51894ms step_avg:60.13ms
step:864/2285 train_time:51952ms step_avg:60.13ms
step:865/2285 train_time:52013ms step_avg:60.13ms
step:866/2285 train_time:52072ms step_avg:60.13ms
step:867/2285 train_time:52134ms step_avg:60.13ms
step:868/2285 train_time:52193ms step_avg:60.13ms
step:869/2285 train_time:52255ms step_avg:60.13ms
step:870/2285 train_time:52314ms step_avg:60.13ms
step:871/2285 train_time:52376ms step_avg:60.13ms
step:872/2285 train_time:52436ms step_avg:60.13ms
step:873/2285 train_time:52498ms step_avg:60.14ms
step:874/2285 train_time:52558ms step_avg:60.13ms
step:875/2285 train_time:52620ms step_avg:60.14ms
step:876/2285 train_time:52680ms step_avg:60.14ms
step:877/2285 train_time:52743ms step_avg:60.14ms
step:878/2285 train_time:52802ms step_avg:60.14ms
step:879/2285 train_time:52864ms step_avg:60.14ms
step:880/2285 train_time:52924ms step_avg:60.14ms
step:881/2285 train_time:52986ms step_avg:60.14ms
step:882/2285 train_time:53045ms step_avg:60.14ms
step:883/2285 train_time:53107ms step_avg:60.14ms
step:884/2285 train_time:53166ms step_avg:60.14ms
step:885/2285 train_time:53228ms step_avg:60.14ms
step:886/2285 train_time:53288ms step_avg:60.14ms
step:887/2285 train_time:53350ms step_avg:60.15ms
step:888/2285 train_time:53409ms step_avg:60.15ms
step:889/2285 train_time:53472ms step_avg:60.15ms
step:890/2285 train_time:53531ms step_avg:60.15ms
step:891/2285 train_time:53593ms step_avg:60.15ms
step:892/2285 train_time:53652ms step_avg:60.15ms
step:893/2285 train_time:53714ms step_avg:60.15ms
step:894/2285 train_time:53773ms step_avg:60.15ms
step:895/2285 train_time:53835ms step_avg:60.15ms
step:896/2285 train_time:53894ms step_avg:60.15ms
step:897/2285 train_time:53956ms step_avg:60.15ms
step:898/2285 train_time:54015ms step_avg:60.15ms
step:899/2285 train_time:54078ms step_avg:60.15ms
step:900/2285 train_time:54137ms step_avg:60.15ms
step:901/2285 train_time:54200ms step_avg:60.16ms
step:902/2285 train_time:54260ms step_avg:60.15ms
step:903/2285 train_time:54322ms step_avg:60.16ms
step:904/2285 train_time:54381ms step_avg:60.16ms
step:905/2285 train_time:54444ms step_avg:60.16ms
step:906/2285 train_time:54504ms step_avg:60.16ms
step:907/2285 train_time:54565ms step_avg:60.16ms
step:908/2285 train_time:54625ms step_avg:60.16ms
step:909/2285 train_time:54686ms step_avg:60.16ms
step:910/2285 train_time:54746ms step_avg:60.16ms
step:911/2285 train_time:54807ms step_avg:60.16ms
step:912/2285 train_time:54866ms step_avg:60.16ms
step:913/2285 train_time:54928ms step_avg:60.16ms
step:914/2285 train_time:54988ms step_avg:60.16ms
step:915/2285 train_time:55050ms step_avg:60.16ms
step:916/2285 train_time:55109ms step_avg:60.16ms
step:917/2285 train_time:55170ms step_avg:60.16ms
step:918/2285 train_time:55230ms step_avg:60.16ms
step:919/2285 train_time:55292ms step_avg:60.16ms
step:920/2285 train_time:55350ms step_avg:60.16ms
step:921/2285 train_time:55412ms step_avg:60.17ms
step:922/2285 train_time:55471ms step_avg:60.16ms
step:923/2285 train_time:55533ms step_avg:60.17ms
step:924/2285 train_time:55592ms step_avg:60.16ms
step:925/2285 train_time:55654ms step_avg:60.17ms
step:926/2285 train_time:55713ms step_avg:60.16ms
step:927/2285 train_time:55775ms step_avg:60.17ms
step:928/2285 train_time:55834ms step_avg:60.17ms
step:929/2285 train_time:55896ms step_avg:60.17ms
step:930/2285 train_time:55955ms step_avg:60.17ms
step:931/2285 train_time:56017ms step_avg:60.17ms
step:932/2285 train_time:56077ms step_avg:60.17ms
step:933/2285 train_time:56139ms step_avg:60.17ms
step:934/2285 train_time:56199ms step_avg:60.17ms
step:935/2285 train_time:56261ms step_avg:60.17ms
step:936/2285 train_time:56321ms step_avg:60.17ms
step:937/2285 train_time:56382ms step_avg:60.17ms
step:938/2285 train_time:56443ms step_avg:60.17ms
step:939/2285 train_time:56505ms step_avg:60.18ms
step:940/2285 train_time:56564ms step_avg:60.17ms
step:941/2285 train_time:56626ms step_avg:60.18ms
step:942/2285 train_time:56686ms step_avg:60.18ms
step:943/2285 train_time:56748ms step_avg:60.18ms
step:944/2285 train_time:56808ms step_avg:60.18ms
step:945/2285 train_time:56869ms step_avg:60.18ms
step:946/2285 train_time:56928ms step_avg:60.18ms
step:947/2285 train_time:56990ms step_avg:60.18ms
step:948/2285 train_time:57050ms step_avg:60.18ms
step:949/2285 train_time:57111ms step_avg:60.18ms
step:950/2285 train_time:57171ms step_avg:60.18ms
step:951/2285 train_time:57232ms step_avg:60.18ms
step:952/2285 train_time:57292ms step_avg:60.18ms
step:953/2285 train_time:57354ms step_avg:60.18ms
step:954/2285 train_time:57413ms step_avg:60.18ms
step:955/2285 train_time:57475ms step_avg:60.18ms
step:956/2285 train_time:57534ms step_avg:60.18ms
step:957/2285 train_time:57596ms step_avg:60.18ms
step:958/2285 train_time:57655ms step_avg:60.18ms
step:959/2285 train_time:57717ms step_avg:60.18ms
step:960/2285 train_time:57776ms step_avg:60.18ms
step:961/2285 train_time:57839ms step_avg:60.19ms
step:962/2285 train_time:57898ms step_avg:60.18ms
step:963/2285 train_time:57960ms step_avg:60.19ms
step:964/2285 train_time:58019ms step_avg:60.19ms
step:965/2285 train_time:58081ms step_avg:60.19ms
step:966/2285 train_time:58142ms step_avg:60.19ms
step:967/2285 train_time:58204ms step_avg:60.19ms
step:968/2285 train_time:58264ms step_avg:60.19ms
step:969/2285 train_time:58325ms step_avg:60.19ms
step:970/2285 train_time:58385ms step_avg:60.19ms
step:971/2285 train_time:58447ms step_avg:60.19ms
step:972/2285 train_time:58506ms step_avg:60.19ms
step:973/2285 train_time:58569ms step_avg:60.19ms
step:974/2285 train_time:58628ms step_avg:60.19ms
step:975/2285 train_time:58690ms step_avg:60.19ms
step:976/2285 train_time:58749ms step_avg:60.19ms
step:977/2285 train_time:58810ms step_avg:60.19ms
step:978/2285 train_time:58870ms step_avg:60.19ms
step:979/2285 train_time:58931ms step_avg:60.20ms
step:980/2285 train_time:58991ms step_avg:60.20ms
step:981/2285 train_time:59054ms step_avg:60.20ms
step:982/2285 train_time:59113ms step_avg:60.20ms
step:983/2285 train_time:59175ms step_avg:60.20ms
step:984/2285 train_time:59235ms step_avg:60.20ms
step:985/2285 train_time:59297ms step_avg:60.20ms
step:986/2285 train_time:59357ms step_avg:60.20ms
step:987/2285 train_time:59419ms step_avg:60.20ms
step:988/2285 train_time:59478ms step_avg:60.20ms
step:989/2285 train_time:59540ms step_avg:60.20ms
step:990/2285 train_time:59600ms step_avg:60.20ms
step:991/2285 train_time:59662ms step_avg:60.20ms
step:992/2285 train_time:59721ms step_avg:60.20ms
step:993/2285 train_time:59784ms step_avg:60.21ms
step:994/2285 train_time:59844ms step_avg:60.20ms
step:995/2285 train_time:59906ms step_avg:60.21ms
step:996/2285 train_time:59965ms step_avg:60.21ms
step:997/2285 train_time:60027ms step_avg:60.21ms
step:998/2285 train_time:60087ms step_avg:60.21ms
step:999/2285 train_time:60149ms step_avg:60.21ms
step:1000/2285 train_time:60209ms step_avg:60.21ms
step:1000/2285 val_loss:3.5730 train_time:60272ms step_avg:60.27ms
step:1001/2285 train_time:60293ms step_avg:60.23ms
step:1002/2285 train_time:60334ms step_avg:60.21ms
step:1003/2285 train_time:60398ms step_avg:60.22ms
step:1004/2285 train_time:60459ms step_avg:60.22ms
step:1005/2285 train_time:60522ms step_avg:60.22ms
step:1006/2285 train_time:60583ms step_avg:60.22ms
step:1007/2285 train_time:60644ms step_avg:60.22ms
step:1008/2285 train_time:60702ms step_avg:60.22ms
step:1009/2285 train_time:60764ms step_avg:60.22ms
step:1010/2285 train_time:60822ms step_avg:60.22ms
step:1011/2285 train_time:60883ms step_avg:60.22ms
step:1012/2285 train_time:60942ms step_avg:60.22ms
step:1013/2285 train_time:61003ms step_avg:60.22ms
step:1014/2285 train_time:61062ms step_avg:60.22ms
step:1015/2285 train_time:61123ms step_avg:60.22ms
step:1016/2285 train_time:61184ms step_avg:60.22ms
step:1017/2285 train_time:61250ms step_avg:60.23ms
step:1018/2285 train_time:61310ms step_avg:60.23ms
step:1019/2285 train_time:61372ms step_avg:60.23ms
step:1020/2285 train_time:61432ms step_avg:60.23ms
step:1021/2285 train_time:61495ms step_avg:60.23ms
step:1022/2285 train_time:61555ms step_avg:60.23ms
step:1023/2285 train_time:61617ms step_avg:60.23ms
step:1024/2285 train_time:61677ms step_avg:60.23ms
step:1025/2285 train_time:61738ms step_avg:60.23ms
step:1026/2285 train_time:61797ms step_avg:60.23ms
step:1027/2285 train_time:61859ms step_avg:60.23ms
step:1028/2285 train_time:61918ms step_avg:60.23ms
step:1029/2285 train_time:61979ms step_avg:60.23ms
step:1030/2285 train_time:62038ms step_avg:60.23ms
step:1031/2285 train_time:62100ms step_avg:60.23ms
step:1032/2285 train_time:62160ms step_avg:60.23ms
step:1033/2285 train_time:62223ms step_avg:60.23ms
step:1034/2285 train_time:62283ms step_avg:60.23ms
step:1035/2285 train_time:62345ms step_avg:60.24ms
step:1036/2285 train_time:62405ms step_avg:60.24ms
step:1037/2285 train_time:62467ms step_avg:60.24ms
step:1038/2285 train_time:62527ms step_avg:60.24ms
step:1039/2285 train_time:62588ms step_avg:60.24ms
step:1040/2285 train_time:62647ms step_avg:60.24ms
step:1041/2285 train_time:62709ms step_avg:60.24ms
step:1042/2285 train_time:62768ms step_avg:60.24ms
step:1043/2285 train_time:62830ms step_avg:60.24ms
step:1044/2285 train_time:62889ms step_avg:60.24ms
step:1045/2285 train_time:62950ms step_avg:60.24ms
step:1046/2285 train_time:63010ms step_avg:60.24ms
step:1047/2285 train_time:63072ms step_avg:60.24ms
step:1048/2285 train_time:63131ms step_avg:60.24ms
step:1049/2285 train_time:63193ms step_avg:60.24ms
step:1050/2285 train_time:63253ms step_avg:60.24ms
step:1051/2285 train_time:63316ms step_avg:60.24ms
step:1052/2285 train_time:63377ms step_avg:60.24ms
step:1053/2285 train_time:63439ms step_avg:60.25ms
step:1054/2285 train_time:63499ms step_avg:60.25ms
step:1055/2285 train_time:63561ms step_avg:60.25ms
step:1056/2285 train_time:63620ms step_avg:60.25ms
step:1057/2285 train_time:63682ms step_avg:60.25ms
step:1058/2285 train_time:63742ms step_avg:60.25ms
step:1059/2285 train_time:63803ms step_avg:60.25ms
step:1060/2285 train_time:63863ms step_avg:60.25ms
step:1061/2285 train_time:63925ms step_avg:60.25ms
step:1062/2285 train_time:63984ms step_avg:60.25ms
step:1063/2285 train_time:64046ms step_avg:60.25ms
step:1064/2285 train_time:64105ms step_avg:60.25ms
step:1065/2285 train_time:64167ms step_avg:60.25ms
step:1066/2285 train_time:64226ms step_avg:60.25ms
step:1067/2285 train_time:64288ms step_avg:60.25ms
step:1068/2285 train_time:64348ms step_avg:60.25ms
step:1069/2285 train_time:64410ms step_avg:60.25ms
step:1070/2285 train_time:64469ms step_avg:60.25ms
step:1071/2285 train_time:64531ms step_avg:60.25ms
step:1072/2285 train_time:64591ms step_avg:60.25ms
step:1073/2285 train_time:64653ms step_avg:60.25ms
step:1074/2285 train_time:64712ms step_avg:60.25ms
step:1075/2285 train_time:64774ms step_avg:60.26ms
step:1076/2285 train_time:64834ms step_avg:60.25ms
step:1077/2285 train_time:64897ms step_avg:60.26ms
step:1078/2285 train_time:64956ms step_avg:60.26ms
step:1079/2285 train_time:65019ms step_avg:60.26ms
step:1080/2285 train_time:65078ms step_avg:60.26ms
step:1081/2285 train_time:65140ms step_avg:60.26ms
step:1082/2285 train_time:65200ms step_avg:60.26ms
step:1083/2285 train_time:65262ms step_avg:60.26ms
step:1084/2285 train_time:65321ms step_avg:60.26ms
step:1085/2285 train_time:65383ms step_avg:60.26ms
step:1086/2285 train_time:65443ms step_avg:60.26ms
step:1087/2285 train_time:65505ms step_avg:60.26ms
step:1088/2285 train_time:65565ms step_avg:60.26ms
step:1089/2285 train_time:65627ms step_avg:60.26ms
step:1090/2285 train_time:65686ms step_avg:60.26ms
step:1091/2285 train_time:65747ms step_avg:60.26ms
step:1092/2285 train_time:65807ms step_avg:60.26ms
step:1093/2285 train_time:65868ms step_avg:60.26ms
step:1094/2285 train_time:65928ms step_avg:60.26ms
step:1095/2285 train_time:65990ms step_avg:60.26ms
step:1096/2285 train_time:66049ms step_avg:60.26ms
step:1097/2285 train_time:66111ms step_avg:60.27ms
step:1098/2285 train_time:66171ms step_avg:60.27ms
step:1099/2285 train_time:66234ms step_avg:60.27ms
step:1100/2285 train_time:66293ms step_avg:60.27ms
step:1101/2285 train_time:66355ms step_avg:60.27ms
step:1102/2285 train_time:66415ms step_avg:60.27ms
step:1103/2285 train_time:66478ms step_avg:60.27ms
step:1104/2285 train_time:66538ms step_avg:60.27ms
step:1105/2285 train_time:66600ms step_avg:60.27ms
step:1106/2285 train_time:66659ms step_avg:60.27ms
step:1107/2285 train_time:66722ms step_avg:60.27ms
step:1108/2285 train_time:66781ms step_avg:60.27ms
step:1109/2285 train_time:66843ms step_avg:60.27ms
step:1110/2285 train_time:66903ms step_avg:60.27ms
step:1111/2285 train_time:66965ms step_avg:60.27ms
step:1112/2285 train_time:67024ms step_avg:60.27ms
step:1113/2285 train_time:67086ms step_avg:60.27ms
step:1114/2285 train_time:67145ms step_avg:60.27ms
step:1115/2285 train_time:67207ms step_avg:60.28ms
step:1116/2285 train_time:67266ms step_avg:60.27ms
step:1117/2285 train_time:67328ms step_avg:60.28ms
step:1118/2285 train_time:67388ms step_avg:60.28ms
step:1119/2285 train_time:67449ms step_avg:60.28ms
step:1120/2285 train_time:67508ms step_avg:60.28ms
step:1121/2285 train_time:67571ms step_avg:60.28ms
step:1122/2285 train_time:67630ms step_avg:60.28ms
step:1123/2285 train_time:67692ms step_avg:60.28ms
step:1124/2285 train_time:67752ms step_avg:60.28ms
step:1125/2285 train_time:67814ms step_avg:60.28ms
step:1126/2285 train_time:67873ms step_avg:60.28ms
step:1127/2285 train_time:67936ms step_avg:60.28ms
step:1128/2285 train_time:67996ms step_avg:60.28ms
step:1129/2285 train_time:68058ms step_avg:60.28ms
step:1130/2285 train_time:68117ms step_avg:60.28ms
step:1131/2285 train_time:68180ms step_avg:60.28ms
step:1132/2285 train_time:68239ms step_avg:60.28ms
step:1133/2285 train_time:68301ms step_avg:60.28ms
step:1134/2285 train_time:68361ms step_avg:60.28ms
step:1135/2285 train_time:68423ms step_avg:60.28ms
step:1136/2285 train_time:68483ms step_avg:60.28ms
step:1137/2285 train_time:68544ms step_avg:60.29ms
step:1138/2285 train_time:68604ms step_avg:60.28ms
step:1139/2285 train_time:68666ms step_avg:60.29ms
step:1140/2285 train_time:68726ms step_avg:60.29ms
step:1141/2285 train_time:68788ms step_avg:60.29ms
step:1142/2285 train_time:68847ms step_avg:60.29ms
step:1143/2285 train_time:68909ms step_avg:60.29ms
step:1144/2285 train_time:68968ms step_avg:60.29ms
step:1145/2285 train_time:69030ms step_avg:60.29ms
step:1146/2285 train_time:69089ms step_avg:60.29ms
step:1147/2285 train_time:69152ms step_avg:60.29ms
step:1148/2285 train_time:69211ms step_avg:60.29ms
step:1149/2285 train_time:69274ms step_avg:60.29ms
step:1150/2285 train_time:69334ms step_avg:60.29ms
step:1151/2285 train_time:69397ms step_avg:60.29ms
step:1152/2285 train_time:69457ms step_avg:60.29ms
step:1153/2285 train_time:69520ms step_avg:60.29ms
step:1154/2285 train_time:69581ms step_avg:60.30ms
step:1155/2285 train_time:69643ms step_avg:60.30ms
step:1156/2285 train_time:69703ms step_avg:60.30ms
step:1157/2285 train_time:69765ms step_avg:60.30ms
step:1158/2285 train_time:69826ms step_avg:60.30ms
step:1159/2285 train_time:69887ms step_avg:60.30ms
step:1160/2285 train_time:69946ms step_avg:60.30ms
step:1161/2285 train_time:70008ms step_avg:60.30ms
step:1162/2285 train_time:70067ms step_avg:60.30ms
step:1163/2285 train_time:70129ms step_avg:60.30ms
step:1164/2285 train_time:70189ms step_avg:60.30ms
step:1165/2285 train_time:70251ms step_avg:60.30ms
step:1166/2285 train_time:70310ms step_avg:60.30ms
step:1167/2285 train_time:70373ms step_avg:60.30ms
step:1168/2285 train_time:70433ms step_avg:60.30ms
step:1169/2285 train_time:70496ms step_avg:60.30ms
step:1170/2285 train_time:70556ms step_avg:60.30ms
step:1171/2285 train_time:70620ms step_avg:60.31ms
step:1172/2285 train_time:70679ms step_avg:60.31ms
step:1173/2285 train_time:70741ms step_avg:60.31ms
step:1174/2285 train_time:70801ms step_avg:60.31ms
step:1175/2285 train_time:70863ms step_avg:60.31ms
step:1176/2285 train_time:70923ms step_avg:60.31ms
step:1177/2285 train_time:70986ms step_avg:60.31ms
step:1178/2285 train_time:71045ms step_avg:60.31ms
step:1179/2285 train_time:71106ms step_avg:60.31ms
step:1180/2285 train_time:71166ms step_avg:60.31ms
step:1181/2285 train_time:71228ms step_avg:60.31ms
step:1182/2285 train_time:71288ms step_avg:60.31ms
step:1183/2285 train_time:71350ms step_avg:60.31ms
step:1184/2285 train_time:71410ms step_avg:60.31ms
step:1185/2285 train_time:71472ms step_avg:60.31ms
step:1186/2285 train_time:71532ms step_avg:60.31ms
step:1187/2285 train_time:71595ms step_avg:60.32ms
step:1188/2285 train_time:71655ms step_avg:60.32ms
step:1189/2285 train_time:71717ms step_avg:60.32ms
step:1190/2285 train_time:71778ms step_avg:60.32ms
step:1191/2285 train_time:71840ms step_avg:60.32ms
step:1192/2285 train_time:71900ms step_avg:60.32ms
step:1193/2285 train_time:71962ms step_avg:60.32ms
step:1194/2285 train_time:72021ms step_avg:60.32ms
step:1195/2285 train_time:72084ms step_avg:60.32ms
step:1196/2285 train_time:72144ms step_avg:60.32ms
step:1197/2285 train_time:72205ms step_avg:60.32ms
step:1198/2285 train_time:72264ms step_avg:60.32ms
step:1199/2285 train_time:72326ms step_avg:60.32ms
step:1200/2285 train_time:72386ms step_avg:60.32ms
step:1201/2285 train_time:72448ms step_avg:60.32ms
step:1202/2285 train_time:72507ms step_avg:60.32ms
step:1203/2285 train_time:72570ms step_avg:60.32ms
step:1204/2285 train_time:72630ms step_avg:60.32ms
step:1205/2285 train_time:72692ms step_avg:60.33ms
step:1206/2285 train_time:72751ms step_avg:60.32ms
step:1207/2285 train_time:72814ms step_avg:60.33ms
step:1208/2285 train_time:72875ms step_avg:60.33ms
step:1209/2285 train_time:72937ms step_avg:60.33ms
step:1210/2285 train_time:72997ms step_avg:60.33ms
step:1211/2285 train_time:73059ms step_avg:60.33ms
step:1212/2285 train_time:73119ms step_avg:60.33ms
step:1213/2285 train_time:73182ms step_avg:60.33ms
step:1214/2285 train_time:73242ms step_avg:60.33ms
step:1215/2285 train_time:73304ms step_avg:60.33ms
step:1216/2285 train_time:73364ms step_avg:60.33ms
step:1217/2285 train_time:73426ms step_avg:60.33ms
step:1218/2285 train_time:73485ms step_avg:60.33ms
step:1219/2285 train_time:73547ms step_avg:60.33ms
step:1220/2285 train_time:73606ms step_avg:60.33ms
step:1221/2285 train_time:73668ms step_avg:60.33ms
step:1222/2285 train_time:73728ms step_avg:60.33ms
step:1223/2285 train_time:73791ms step_avg:60.34ms
step:1224/2285 train_time:73850ms step_avg:60.34ms
step:1225/2285 train_time:73912ms step_avg:60.34ms
step:1226/2285 train_time:73972ms step_avg:60.34ms
step:1227/2285 train_time:74034ms step_avg:60.34ms
step:1228/2285 train_time:74095ms step_avg:60.34ms
step:1229/2285 train_time:74158ms step_avg:60.34ms
step:1230/2285 train_time:74218ms step_avg:60.34ms
step:1231/2285 train_time:74280ms step_avg:60.34ms
step:1232/2285 train_time:74340ms step_avg:60.34ms
step:1233/2285 train_time:74402ms step_avg:60.34ms
step:1234/2285 train_time:74462ms step_avg:60.34ms
step:1235/2285 train_time:74525ms step_avg:60.34ms
step:1236/2285 train_time:74585ms step_avg:60.34ms
step:1237/2285 train_time:74646ms step_avg:60.34ms
step:1238/2285 train_time:74706ms step_avg:60.34ms
step:1239/2285 train_time:74767ms step_avg:60.34ms
step:1240/2285 train_time:74827ms step_avg:60.34ms
step:1241/2285 train_time:74889ms step_avg:60.35ms
step:1242/2285 train_time:74949ms step_avg:60.35ms
step:1243/2285 train_time:75012ms step_avg:60.35ms
step:1244/2285 train_time:75071ms step_avg:60.35ms
step:1245/2285 train_time:75134ms step_avg:60.35ms
step:1246/2285 train_time:75195ms step_avg:60.35ms
step:1247/2285 train_time:75256ms step_avg:60.35ms
step:1248/2285 train_time:75315ms step_avg:60.35ms
step:1249/2285 train_time:75378ms step_avg:60.35ms
step:1250/2285 train_time:75439ms step_avg:60.35ms
step:1250/2285 val_loss:3.4968 train_time:75503ms step_avg:60.40ms
step:1251/2285 train_time:75531ms step_avg:60.38ms
step:1252/2285 train_time:75563ms step_avg:60.35ms
step:1253/2285 train_time:75624ms step_avg:60.35ms
step:1254/2285 train_time:75684ms step_avg:60.35ms
step:1255/2285 train_time:75748ms step_avg:60.36ms
step:1256/2285 train_time:75808ms step_avg:60.36ms
step:1257/2285 train_time:75870ms step_avg:60.36ms
step:1258/2285 train_time:75929ms step_avg:60.36ms
step:1259/2285 train_time:75990ms step_avg:60.36ms
step:1260/2285 train_time:76048ms step_avg:60.36ms
step:1261/2285 train_time:76109ms step_avg:60.36ms
step:1262/2285 train_time:76168ms step_avg:60.35ms
step:1263/2285 train_time:76229ms step_avg:60.36ms
step:1264/2285 train_time:76287ms step_avg:60.35ms
step:1265/2285 train_time:76348ms step_avg:60.35ms
step:1266/2285 train_time:76410ms step_avg:60.36ms
step:1267/2285 train_time:76477ms step_avg:60.36ms
step:1268/2285 train_time:76537ms step_avg:60.36ms
step:1269/2285 train_time:76599ms step_avg:60.36ms
step:1270/2285 train_time:76659ms step_avg:60.36ms
step:1271/2285 train_time:76721ms step_avg:60.36ms
step:1272/2285 train_time:76781ms step_avg:60.36ms
step:1273/2285 train_time:76842ms step_avg:60.36ms
step:1274/2285 train_time:76902ms step_avg:60.36ms
step:1275/2285 train_time:76965ms step_avg:60.36ms
step:1276/2285 train_time:77024ms step_avg:60.36ms
step:1277/2285 train_time:77085ms step_avg:60.36ms
step:1278/2285 train_time:77144ms step_avg:60.36ms
step:1279/2285 train_time:77205ms step_avg:60.36ms
step:1280/2285 train_time:77265ms step_avg:60.36ms
step:1281/2285 train_time:77327ms step_avg:60.36ms
step:1282/2285 train_time:77388ms step_avg:60.37ms
step:1283/2285 train_time:77452ms step_avg:60.37ms
step:1284/2285 train_time:77511ms step_avg:60.37ms
step:1285/2285 train_time:77573ms step_avg:60.37ms
step:1286/2285 train_time:77633ms step_avg:60.37ms
step:1287/2285 train_time:77696ms step_avg:60.37ms
step:1288/2285 train_time:77755ms step_avg:60.37ms
step:1289/2285 train_time:77818ms step_avg:60.37ms
step:1290/2285 train_time:77878ms step_avg:60.37ms
step:1291/2285 train_time:77939ms step_avg:60.37ms
step:1292/2285 train_time:77999ms step_avg:60.37ms
step:1293/2285 train_time:78061ms step_avg:60.37ms
step:1294/2285 train_time:78120ms step_avg:60.37ms
step:1295/2285 train_time:78182ms step_avg:60.37ms
step:1296/2285 train_time:78241ms step_avg:60.37ms
step:1297/2285 train_time:78304ms step_avg:60.37ms
step:1298/2285 train_time:78364ms step_avg:60.37ms
step:1299/2285 train_time:78426ms step_avg:60.37ms
step:1300/2285 train_time:78487ms step_avg:60.37ms
step:1301/2285 train_time:78550ms step_avg:60.38ms
step:1302/2285 train_time:78609ms step_avg:60.38ms
step:1303/2285 train_time:78671ms step_avg:60.38ms
step:1304/2285 train_time:78731ms step_avg:60.38ms
step:1305/2285 train_time:78793ms step_avg:60.38ms
step:1306/2285 train_time:78853ms step_avg:60.38ms
step:1307/2285 train_time:78915ms step_avg:60.38ms
step:1308/2285 train_time:78975ms step_avg:60.38ms
step:1309/2285 train_time:79036ms step_avg:60.38ms
step:1310/2285 train_time:79096ms step_avg:60.38ms
step:1311/2285 train_time:79158ms step_avg:60.38ms
step:1312/2285 train_time:79218ms step_avg:60.38ms
step:1313/2285 train_time:79281ms step_avg:60.38ms
step:1314/2285 train_time:79341ms step_avg:60.38ms
step:1315/2285 train_time:79404ms step_avg:60.38ms
step:1316/2285 train_time:79464ms step_avg:60.38ms
step:1317/2285 train_time:79526ms step_avg:60.38ms
step:1318/2285 train_time:79586ms step_avg:60.38ms
step:1319/2285 train_time:79648ms step_avg:60.39ms
step:1320/2285 train_time:79708ms step_avg:60.39ms
step:1321/2285 train_time:79770ms step_avg:60.39ms
step:1322/2285 train_time:79830ms step_avg:60.39ms
step:1323/2285 train_time:79891ms step_avg:60.39ms
step:1324/2285 train_time:79950ms step_avg:60.39ms
step:1325/2285 train_time:80013ms step_avg:60.39ms
step:1326/2285 train_time:80073ms step_avg:60.39ms
step:1327/2285 train_time:80135ms step_avg:60.39ms
step:1328/2285 train_time:80194ms step_avg:60.39ms
step:1329/2285 train_time:80256ms step_avg:60.39ms
step:1330/2285 train_time:80316ms step_avg:60.39ms
step:1331/2285 train_time:80378ms step_avg:60.39ms
step:1332/2285 train_time:80438ms step_avg:60.39ms
step:1333/2285 train_time:80501ms step_avg:60.39ms
step:1334/2285 train_time:80562ms step_avg:60.39ms
step:1335/2285 train_time:80624ms step_avg:60.39ms
step:1336/2285 train_time:80684ms step_avg:60.39ms
step:1337/2285 train_time:80746ms step_avg:60.39ms
step:1338/2285 train_time:80805ms step_avg:60.39ms
step:1339/2285 train_time:80867ms step_avg:60.39ms
step:1340/2285 train_time:80927ms step_avg:60.39ms
step:1341/2285 train_time:80989ms step_avg:60.39ms
step:1342/2285 train_time:81048ms step_avg:60.39ms
step:1343/2285 train_time:81110ms step_avg:60.39ms
step:1344/2285 train_time:81170ms step_avg:60.39ms
step:1345/2285 train_time:81232ms step_avg:60.40ms
step:1346/2285 train_time:81292ms step_avg:60.40ms
step:1347/2285 train_time:81354ms step_avg:60.40ms
step:1348/2285 train_time:81414ms step_avg:60.40ms
step:1349/2285 train_time:81476ms step_avg:60.40ms
step:1350/2285 train_time:81535ms step_avg:60.40ms
step:1351/2285 train_time:81598ms step_avg:60.40ms
step:1352/2285 train_time:81658ms step_avg:60.40ms
step:1353/2285 train_time:81720ms step_avg:60.40ms
step:1354/2285 train_time:81780ms step_avg:60.40ms
step:1355/2285 train_time:81843ms step_avg:60.40ms
step:1356/2285 train_time:81902ms step_avg:60.40ms
step:1357/2285 train_time:81966ms step_avg:60.40ms
step:1358/2285 train_time:82025ms step_avg:60.40ms
step:1359/2285 train_time:82088ms step_avg:60.40ms
step:1360/2285 train_time:82147ms step_avg:60.40ms
step:1361/2285 train_time:82209ms step_avg:60.40ms
step:1362/2285 train_time:82269ms step_avg:60.40ms
step:1363/2285 train_time:82330ms step_avg:60.40ms
step:1364/2285 train_time:82390ms step_avg:60.40ms
step:1365/2285 train_time:82452ms step_avg:60.40ms
step:1366/2285 train_time:82512ms step_avg:60.40ms
step:1367/2285 train_time:82574ms step_avg:60.41ms
step:1368/2285 train_time:82634ms step_avg:60.41ms
step:1369/2285 train_time:82697ms step_avg:60.41ms
step:1370/2285 train_time:82756ms step_avg:60.41ms
step:1371/2285 train_time:82818ms step_avg:60.41ms
step:1372/2285 train_time:82878ms step_avg:60.41ms
step:1373/2285 train_time:82940ms step_avg:60.41ms
step:1374/2285 train_time:83000ms step_avg:60.41ms
step:1375/2285 train_time:83063ms step_avg:60.41ms
step:1376/2285 train_time:83123ms step_avg:60.41ms
step:1377/2285 train_time:83185ms step_avg:60.41ms
step:1378/2285 train_time:83244ms step_avg:60.41ms
step:1379/2285 train_time:83306ms step_avg:60.41ms
step:1380/2285 train_time:83367ms step_avg:60.41ms
step:1381/2285 train_time:83429ms step_avg:60.41ms
step:1382/2285 train_time:83489ms step_avg:60.41ms
step:1383/2285 train_time:83551ms step_avg:60.41ms
step:1384/2285 train_time:83610ms step_avg:60.41ms
step:1385/2285 train_time:83672ms step_avg:60.41ms
step:1386/2285 train_time:83732ms step_avg:60.41ms
step:1387/2285 train_time:83794ms step_avg:60.41ms
step:1388/2285 train_time:83853ms step_avg:60.41ms
step:1389/2285 train_time:83915ms step_avg:60.41ms
step:1390/2285 train_time:83975ms step_avg:60.41ms
step:1391/2285 train_time:84037ms step_avg:60.41ms
step:1392/2285 train_time:84096ms step_avg:60.41ms
step:1393/2285 train_time:84159ms step_avg:60.42ms
step:1394/2285 train_time:84219ms step_avg:60.42ms
step:1395/2285 train_time:84281ms step_avg:60.42ms
step:1396/2285 train_time:84341ms step_avg:60.42ms
step:1397/2285 train_time:84404ms step_avg:60.42ms
step:1398/2285 train_time:84464ms step_avg:60.42ms
step:1399/2285 train_time:84526ms step_avg:60.42ms
step:1400/2285 train_time:84586ms step_avg:60.42ms
step:1401/2285 train_time:84648ms step_avg:60.42ms
step:1402/2285 train_time:84707ms step_avg:60.42ms
step:1403/2285 train_time:84770ms step_avg:60.42ms
step:1404/2285 train_time:84830ms step_avg:60.42ms
step:1405/2285 train_time:84891ms step_avg:60.42ms
step:1406/2285 train_time:84951ms step_avg:60.42ms
step:1407/2285 train_time:85013ms step_avg:60.42ms
step:1408/2285 train_time:85073ms step_avg:60.42ms
step:1409/2285 train_time:85136ms step_avg:60.42ms
step:1410/2285 train_time:85194ms step_avg:60.42ms
step:1411/2285 train_time:85257ms step_avg:60.42ms
step:1412/2285 train_time:85317ms step_avg:60.42ms
step:1413/2285 train_time:85379ms step_avg:60.42ms
step:1414/2285 train_time:85439ms step_avg:60.42ms
step:1415/2285 train_time:85501ms step_avg:60.42ms
step:1416/2285 train_time:85562ms step_avg:60.43ms
step:1417/2285 train_time:85625ms step_avg:60.43ms
step:1418/2285 train_time:85684ms step_avg:60.43ms
step:1419/2285 train_time:85747ms step_avg:60.43ms
step:1420/2285 train_time:85807ms step_avg:60.43ms
step:1421/2285 train_time:85870ms step_avg:60.43ms
step:1422/2285 train_time:85929ms step_avg:60.43ms
step:1423/2285 train_time:85991ms step_avg:60.43ms
step:1424/2285 train_time:86050ms step_avg:60.43ms
step:1425/2285 train_time:86112ms step_avg:60.43ms
step:1426/2285 train_time:86171ms step_avg:60.43ms
step:1427/2285 train_time:86233ms step_avg:60.43ms
step:1428/2285 train_time:86292ms step_avg:60.43ms
step:1429/2285 train_time:86354ms step_avg:60.43ms
step:1430/2285 train_time:86414ms step_avg:60.43ms
step:1431/2285 train_time:86476ms step_avg:60.43ms
step:1432/2285 train_time:86536ms step_avg:60.43ms
step:1433/2285 train_time:86598ms step_avg:60.43ms
step:1434/2285 train_time:86658ms step_avg:60.43ms
step:1435/2285 train_time:86721ms step_avg:60.43ms
step:1436/2285 train_time:86781ms step_avg:60.43ms
step:1437/2285 train_time:86844ms step_avg:60.43ms
step:1438/2285 train_time:86903ms step_avg:60.43ms
step:1439/2285 train_time:86965ms step_avg:60.43ms
step:1440/2285 train_time:87025ms step_avg:60.43ms
step:1441/2285 train_time:87087ms step_avg:60.44ms
step:1442/2285 train_time:87147ms step_avg:60.43ms
step:1443/2285 train_time:87209ms step_avg:60.44ms
step:1444/2285 train_time:87268ms step_avg:60.44ms
step:1445/2285 train_time:87330ms step_avg:60.44ms
step:1446/2285 train_time:87390ms step_avg:60.44ms
step:1447/2285 train_time:87452ms step_avg:60.44ms
step:1448/2285 train_time:87512ms step_avg:60.44ms
step:1449/2285 train_time:87575ms step_avg:60.44ms
step:1450/2285 train_time:87635ms step_avg:60.44ms
step:1451/2285 train_time:87697ms step_avg:60.44ms
step:1452/2285 train_time:87757ms step_avg:60.44ms
step:1453/2285 train_time:87819ms step_avg:60.44ms
step:1454/2285 train_time:87879ms step_avg:60.44ms
step:1455/2285 train_time:87941ms step_avg:60.44ms
step:1456/2285 train_time:88001ms step_avg:60.44ms
step:1457/2285 train_time:88063ms step_avg:60.44ms
step:1458/2285 train_time:88123ms step_avg:60.44ms
step:1459/2285 train_time:88185ms step_avg:60.44ms
step:1460/2285 train_time:88245ms step_avg:60.44ms
step:1461/2285 train_time:88307ms step_avg:60.44ms
step:1462/2285 train_time:88367ms step_avg:60.44ms
step:1463/2285 train_time:88430ms step_avg:60.44ms
step:1464/2285 train_time:88489ms step_avg:60.44ms
step:1465/2285 train_time:88551ms step_avg:60.44ms
step:1466/2285 train_time:88611ms step_avg:60.44ms
step:1467/2285 train_time:88673ms step_avg:60.45ms
step:1468/2285 train_time:88733ms step_avg:60.44ms
step:1469/2285 train_time:88795ms step_avg:60.45ms
step:1470/2285 train_time:88855ms step_avg:60.45ms
step:1471/2285 train_time:88917ms step_avg:60.45ms
step:1472/2285 train_time:88976ms step_avg:60.45ms
step:1473/2285 train_time:89039ms step_avg:60.45ms
step:1474/2285 train_time:89099ms step_avg:60.45ms
step:1475/2285 train_time:89162ms step_avg:60.45ms
step:1476/2285 train_time:89221ms step_avg:60.45ms
step:1477/2285 train_time:89283ms step_avg:60.45ms
step:1478/2285 train_time:89343ms step_avg:60.45ms
step:1479/2285 train_time:89405ms step_avg:60.45ms
step:1480/2285 train_time:89466ms step_avg:60.45ms
step:1481/2285 train_time:89528ms step_avg:60.45ms
step:1482/2285 train_time:89588ms step_avg:60.45ms
step:1483/2285 train_time:89650ms step_avg:60.45ms
step:1484/2285 train_time:89709ms step_avg:60.45ms
step:1485/2285 train_time:89771ms step_avg:60.45ms
step:1486/2285 train_time:89831ms step_avg:60.45ms
step:1487/2285 train_time:89893ms step_avg:60.45ms
step:1488/2285 train_time:89953ms step_avg:60.45ms
step:1489/2285 train_time:90015ms step_avg:60.45ms
step:1490/2285 train_time:90075ms step_avg:60.45ms
step:1491/2285 train_time:90137ms step_avg:60.45ms
step:1492/2285 train_time:90197ms step_avg:60.45ms
step:1493/2285 train_time:90260ms step_avg:60.46ms
step:1494/2285 train_time:90320ms step_avg:60.45ms
step:1495/2285 train_time:90382ms step_avg:60.46ms
step:1496/2285 train_time:90442ms step_avg:60.46ms
step:1497/2285 train_time:90505ms step_avg:60.46ms
step:1498/2285 train_time:90565ms step_avg:60.46ms
step:1499/2285 train_time:90627ms step_avg:60.46ms
step:1500/2285 train_time:90687ms step_avg:60.46ms
step:1500/2285 val_loss:3.4294 train_time:90751ms step_avg:60.50ms
step:1501/2285 train_time:90773ms step_avg:60.48ms
step:1502/2285 train_time:90814ms step_avg:60.46ms
step:1503/2285 train_time:90879ms step_avg:60.47ms
step:1504/2285 train_time:90940ms step_avg:60.47ms
step:1505/2285 train_time:91002ms step_avg:60.47ms
step:1506/2285 train_time:91062ms step_avg:60.47ms
step:1507/2285 train_time:91123ms step_avg:60.47ms
step:1508/2285 train_time:91182ms step_avg:60.47ms
step:1509/2285 train_time:91244ms step_avg:60.47ms
step:1510/2285 train_time:91303ms step_avg:60.47ms
step:1511/2285 train_time:91364ms step_avg:60.47ms
step:1512/2285 train_time:91423ms step_avg:60.47ms
step:1513/2285 train_time:91484ms step_avg:60.47ms
step:1514/2285 train_time:91544ms step_avg:60.46ms
step:1515/2285 train_time:91606ms step_avg:60.47ms
step:1516/2285 train_time:91666ms step_avg:60.47ms
step:1517/2285 train_time:91729ms step_avg:60.47ms
step:1518/2285 train_time:91790ms step_avg:60.47ms
step:1519/2285 train_time:91853ms step_avg:60.47ms
step:1520/2285 train_time:91914ms step_avg:60.47ms
step:1521/2285 train_time:91976ms step_avg:60.47ms
step:1522/2285 train_time:92036ms step_avg:60.47ms
step:1523/2285 train_time:92098ms step_avg:60.47ms
step:1524/2285 train_time:92157ms step_avg:60.47ms
step:1525/2285 train_time:92220ms step_avg:60.47ms
step:1526/2285 train_time:92280ms step_avg:60.47ms
step:1527/2285 train_time:92342ms step_avg:60.47ms
step:1528/2285 train_time:92402ms step_avg:60.47ms
step:1529/2285 train_time:92464ms step_avg:60.47ms
step:1530/2285 train_time:92523ms step_avg:60.47ms
step:1531/2285 train_time:92585ms step_avg:60.47ms
step:1532/2285 train_time:92645ms step_avg:60.47ms
step:1533/2285 train_time:92708ms step_avg:60.47ms
step:1534/2285 train_time:92768ms step_avg:60.47ms
step:1535/2285 train_time:92831ms step_avg:60.48ms
step:1536/2285 train_time:92892ms step_avg:60.48ms
step:1537/2285 train_time:92954ms step_avg:60.48ms
step:1538/2285 train_time:93015ms step_avg:60.48ms
step:1539/2285 train_time:93077ms step_avg:60.48ms
step:1540/2285 train_time:93137ms step_avg:60.48ms
step:1541/2285 train_time:93200ms step_avg:60.48ms
step:1542/2285 train_time:93260ms step_avg:60.48ms
step:1543/2285 train_time:93322ms step_avg:60.48ms
step:1544/2285 train_time:93381ms step_avg:60.48ms
step:1545/2285 train_time:93443ms step_avg:60.48ms
step:1546/2285 train_time:93503ms step_avg:60.48ms
step:1547/2285 train_time:93565ms step_avg:60.48ms
step:1548/2285 train_time:93625ms step_avg:60.48ms
step:1549/2285 train_time:93688ms step_avg:60.48ms
step:1550/2285 train_time:93748ms step_avg:60.48ms
step:1551/2285 train_time:93811ms step_avg:60.48ms
step:1552/2285 train_time:93871ms step_avg:60.48ms
step:1553/2285 train_time:93934ms step_avg:60.49ms
step:1554/2285 train_time:93993ms step_avg:60.48ms
step:1555/2285 train_time:94056ms step_avg:60.49ms
step:1556/2285 train_time:94116ms step_avg:60.49ms
step:1557/2285 train_time:94178ms step_avg:60.49ms
step:1558/2285 train_time:94238ms step_avg:60.49ms
step:1559/2285 train_time:94301ms step_avg:60.49ms
step:1560/2285 train_time:94361ms step_avg:60.49ms
step:1561/2285 train_time:94423ms step_avg:60.49ms
step:1562/2285 train_time:94483ms step_avg:60.49ms
step:1563/2285 train_time:94544ms step_avg:60.49ms
step:1564/2285 train_time:94604ms step_avg:60.49ms
step:1565/2285 train_time:94667ms step_avg:60.49ms
step:1566/2285 train_time:94727ms step_avg:60.49ms
step:1567/2285 train_time:94789ms step_avg:60.49ms
step:1568/2285 train_time:94849ms step_avg:60.49ms
step:1569/2285 train_time:94912ms step_avg:60.49ms
step:1570/2285 train_time:94972ms step_avg:60.49ms
step:1571/2285 train_time:95034ms step_avg:60.49ms
step:1572/2285 train_time:95094ms step_avg:60.49ms
step:1573/2285 train_time:95156ms step_avg:60.49ms
step:1574/2285 train_time:95216ms step_avg:60.49ms
step:1575/2285 train_time:95279ms step_avg:60.49ms
step:1576/2285 train_time:95339ms step_avg:60.49ms
step:1577/2285 train_time:95402ms step_avg:60.50ms
step:1578/2285 train_time:95462ms step_avg:60.50ms
step:1579/2285 train_time:95524ms step_avg:60.50ms
step:1580/2285 train_time:95584ms step_avg:60.50ms
step:1581/2285 train_time:95646ms step_avg:60.50ms
step:1582/2285 train_time:95707ms step_avg:60.50ms
step:1583/2285 train_time:95769ms step_avg:60.50ms
step:1584/2285 train_time:95829ms step_avg:60.50ms
step:1585/2285 train_time:95891ms step_avg:60.50ms
step:1586/2285 train_time:95952ms step_avg:60.50ms
step:1587/2285 train_time:96014ms step_avg:60.50ms
step:1588/2285 train_time:96073ms step_avg:60.50ms
step:1589/2285 train_time:96136ms step_avg:60.50ms
step:1590/2285 train_time:96196ms step_avg:60.50ms
step:1591/2285 train_time:96258ms step_avg:60.50ms
step:1592/2285 train_time:96319ms step_avg:60.50ms
step:1593/2285 train_time:96381ms step_avg:60.50ms
step:1594/2285 train_time:96441ms step_avg:60.50ms
step:1595/2285 train_time:96504ms step_avg:60.50ms
step:1596/2285 train_time:96564ms step_avg:60.50ms
step:1597/2285 train_time:96626ms step_avg:60.50ms
step:1598/2285 train_time:96686ms step_avg:60.50ms
step:1599/2285 train_time:96749ms step_avg:60.51ms
step:1600/2285 train_time:96810ms step_avg:60.51ms
step:1601/2285 train_time:96872ms step_avg:60.51ms
step:1602/2285 train_time:96932ms step_avg:60.51ms
step:1603/2285 train_time:96994ms step_avg:60.51ms
step:1604/2285 train_time:97053ms step_avg:60.51ms
step:1605/2285 train_time:97116ms step_avg:60.51ms
step:1606/2285 train_time:97175ms step_avg:60.51ms
step:1607/2285 train_time:97237ms step_avg:60.51ms
step:1608/2285 train_time:97298ms step_avg:60.51ms
step:1609/2285 train_time:97360ms step_avg:60.51ms
step:1610/2285 train_time:97420ms step_avg:60.51ms
step:1611/2285 train_time:97482ms step_avg:60.51ms
step:1612/2285 train_time:97542ms step_avg:60.51ms
step:1613/2285 train_time:97605ms step_avg:60.51ms
step:1614/2285 train_time:97665ms step_avg:60.51ms
step:1615/2285 train_time:97728ms step_avg:60.51ms
step:1616/2285 train_time:97788ms step_avg:60.51ms
step:1617/2285 train_time:97851ms step_avg:60.51ms
step:1618/2285 train_time:97911ms step_avg:60.51ms
step:1619/2285 train_time:97973ms step_avg:60.51ms
step:1620/2285 train_time:98032ms step_avg:60.51ms
step:1621/2285 train_time:98094ms step_avg:60.51ms
step:1622/2285 train_time:98154ms step_avg:60.51ms
step:1623/2285 train_time:98216ms step_avg:60.52ms
step:1624/2285 train_time:98276ms step_avg:60.51ms
step:1625/2285 train_time:98339ms step_avg:60.52ms
step:1626/2285 train_time:98400ms step_avg:60.52ms
step:1627/2285 train_time:98463ms step_avg:60.52ms
step:1628/2285 train_time:98523ms step_avg:60.52ms
step:1629/2285 train_time:98585ms step_avg:60.52ms
step:1630/2285 train_time:98645ms step_avg:60.52ms
step:1631/2285 train_time:98708ms step_avg:60.52ms
step:1632/2285 train_time:98767ms step_avg:60.52ms
step:1633/2285 train_time:98830ms step_avg:60.52ms
step:1634/2285 train_time:98890ms step_avg:60.52ms
step:1635/2285 train_time:98952ms step_avg:60.52ms
step:1636/2285 train_time:99012ms step_avg:60.52ms
step:1637/2285 train_time:99074ms step_avg:60.52ms
step:1638/2285 train_time:99133ms step_avg:60.52ms
step:1639/2285 train_time:99195ms step_avg:60.52ms
step:1640/2285 train_time:99255ms step_avg:60.52ms
step:1641/2285 train_time:99318ms step_avg:60.52ms
step:1642/2285 train_time:99378ms step_avg:60.52ms
step:1643/2285 train_time:99441ms step_avg:60.52ms
step:1644/2285 train_time:99502ms step_avg:60.52ms
step:1645/2285 train_time:99564ms step_avg:60.53ms
step:1646/2285 train_time:99624ms step_avg:60.52ms
step:1647/2285 train_time:99686ms step_avg:60.53ms
step:1648/2285 train_time:99746ms step_avg:60.53ms
step:1649/2285 train_time:99808ms step_avg:60.53ms
step:1650/2285 train_time:99869ms step_avg:60.53ms
step:1651/2285 train_time:99930ms step_avg:60.53ms
step:1652/2285 train_time:99990ms step_avg:60.53ms
step:1653/2285 train_time:100052ms step_avg:60.53ms
step:1654/2285 train_time:100112ms step_avg:60.53ms
step:1655/2285 train_time:100175ms step_avg:60.53ms
step:1656/2285 train_time:100234ms step_avg:60.53ms
step:1657/2285 train_time:100297ms step_avg:60.53ms
step:1658/2285 train_time:100357ms step_avg:60.53ms
step:1659/2285 train_time:100421ms step_avg:60.53ms
step:1660/2285 train_time:100480ms step_avg:60.53ms
step:1661/2285 train_time:100543ms step_avg:60.53ms
step:1662/2285 train_time:100603ms step_avg:60.53ms
step:1663/2285 train_time:100665ms step_avg:60.53ms
step:1664/2285 train_time:100725ms step_avg:60.53ms
step:1665/2285 train_time:100788ms step_avg:60.53ms
step:1666/2285 train_time:100847ms step_avg:60.53ms
step:1667/2285 train_time:100910ms step_avg:60.53ms
step:1668/2285 train_time:100969ms step_avg:60.53ms
step:1669/2285 train_time:101031ms step_avg:60.53ms
step:1670/2285 train_time:101092ms step_avg:60.53ms
step:1671/2285 train_time:101154ms step_avg:60.54ms
step:1672/2285 train_time:101215ms step_avg:60.54ms
step:1673/2285 train_time:101277ms step_avg:60.54ms
step:1674/2285 train_time:101337ms step_avg:60.54ms
step:1675/2285 train_time:101399ms step_avg:60.54ms
step:1676/2285 train_time:101459ms step_avg:60.54ms
step:1677/2285 train_time:101522ms step_avg:60.54ms
step:1678/2285 train_time:101582ms step_avg:60.54ms
step:1679/2285 train_time:101645ms step_avg:60.54ms
step:1680/2285 train_time:101705ms step_avg:60.54ms
step:1681/2285 train_time:101768ms step_avg:60.54ms
step:1682/2285 train_time:101828ms step_avg:60.54ms
step:1683/2285 train_time:101890ms step_avg:60.54ms
step:1684/2285 train_time:101949ms step_avg:60.54ms
step:1685/2285 train_time:102012ms step_avg:60.54ms
step:1686/2285 train_time:102071ms step_avg:60.54ms
step:1687/2285 train_time:102133ms step_avg:60.54ms
step:1688/2285 train_time:102193ms step_avg:60.54ms
step:1689/2285 train_time:102255ms step_avg:60.54ms
step:1690/2285 train_time:102315ms step_avg:60.54ms
step:1691/2285 train_time:102378ms step_avg:60.54ms
step:1692/2285 train_time:102439ms step_avg:60.54ms
step:1693/2285 train_time:102502ms step_avg:60.54ms
step:1694/2285 train_time:102562ms step_avg:60.54ms
step:1695/2285 train_time:102624ms step_avg:60.55ms
step:1696/2285 train_time:102684ms step_avg:60.54ms
step:1697/2285 train_time:102746ms step_avg:60.55ms
step:1698/2285 train_time:102807ms step_avg:60.55ms
step:1699/2285 train_time:102869ms step_avg:60.55ms
step:1700/2285 train_time:102929ms step_avg:60.55ms
step:1701/2285 train_time:102991ms step_avg:60.55ms
step:1702/2285 train_time:103050ms step_avg:60.55ms
step:1703/2285 train_time:103113ms step_avg:60.55ms
step:1704/2285 train_time:103172ms step_avg:60.55ms
step:1705/2285 train_time:103235ms step_avg:60.55ms
step:1706/2285 train_time:103294ms step_avg:60.55ms
step:1707/2285 train_time:103358ms step_avg:60.55ms
step:1708/2285 train_time:103418ms step_avg:60.55ms
step:1709/2285 train_time:103481ms step_avg:60.55ms
step:1710/2285 train_time:103541ms step_avg:60.55ms
step:1711/2285 train_time:103604ms step_avg:60.55ms
step:1712/2285 train_time:103663ms step_avg:60.55ms
step:1713/2285 train_time:103726ms step_avg:60.55ms
step:1714/2285 train_time:103786ms step_avg:60.55ms
step:1715/2285 train_time:103848ms step_avg:60.55ms
step:1716/2285 train_time:103909ms step_avg:60.55ms
step:1717/2285 train_time:103970ms step_avg:60.55ms
step:1718/2285 train_time:104030ms step_avg:60.55ms
step:1719/2285 train_time:104092ms step_avg:60.55ms
step:1720/2285 train_time:104152ms step_avg:60.55ms
step:1721/2285 train_time:104214ms step_avg:60.55ms
step:1722/2285 train_time:104274ms step_avg:60.55ms
step:1723/2285 train_time:104336ms step_avg:60.56ms
step:1724/2285 train_time:104396ms step_avg:60.55ms
step:1725/2285 train_time:104459ms step_avg:60.56ms
step:1726/2285 train_time:104520ms step_avg:60.56ms
step:1727/2285 train_time:104582ms step_avg:60.56ms
step:1728/2285 train_time:104642ms step_avg:60.56ms
step:1729/2285 train_time:104705ms step_avg:60.56ms
step:1730/2285 train_time:104765ms step_avg:60.56ms
step:1731/2285 train_time:104827ms step_avg:60.56ms
step:1732/2285 train_time:104887ms step_avg:60.56ms
step:1733/2285 train_time:104949ms step_avg:60.56ms
step:1734/2285 train_time:105009ms step_avg:60.56ms
step:1735/2285 train_time:105072ms step_avg:60.56ms
step:1736/2285 train_time:105131ms step_avg:60.56ms
step:1737/2285 train_time:105193ms step_avg:60.56ms
step:1738/2285 train_time:105253ms step_avg:60.56ms
step:1739/2285 train_time:105315ms step_avg:60.56ms
step:1740/2285 train_time:105374ms step_avg:60.56ms
step:1741/2285 train_time:105437ms step_avg:60.56ms
step:1742/2285 train_time:105497ms step_avg:60.56ms
step:1743/2285 train_time:105560ms step_avg:60.56ms
step:1744/2285 train_time:105621ms step_avg:60.56ms
step:1745/2285 train_time:105683ms step_avg:60.56ms
step:1746/2285 train_time:105743ms step_avg:60.56ms
step:1747/2285 train_time:105806ms step_avg:60.56ms
step:1748/2285 train_time:105866ms step_avg:60.56ms
step:1749/2285 train_time:105928ms step_avg:60.57ms
step:1750/2285 train_time:105988ms step_avg:60.56ms
step:1750/2285 val_loss:3.3689 train_time:106052ms step_avg:60.60ms
step:1751/2285 train_time:106072ms step_avg:60.58ms
step:1752/2285 train_time:106112ms step_avg:60.57ms
step:1753/2285 train_time:106176ms step_avg:60.57ms
step:1754/2285 train_time:106237ms step_avg:60.57ms
step:1755/2285 train_time:106301ms step_avg:60.57ms
step:1756/2285 train_time:106362ms step_avg:60.57ms
step:1757/2285 train_time:106424ms step_avg:60.57ms
step:1758/2285 train_time:106483ms step_avg:60.57ms
step:1759/2285 train_time:106544ms step_avg:60.57ms
step:1760/2285 train_time:106603ms step_avg:60.57ms
step:1761/2285 train_time:106664ms step_avg:60.57ms
step:1762/2285 train_time:106723ms step_avg:60.57ms
step:1763/2285 train_time:106784ms step_avg:60.57ms
step:1764/2285 train_time:106843ms step_avg:60.57ms
step:1765/2285 train_time:106904ms step_avg:60.57ms
step:1766/2285 train_time:106966ms step_avg:60.57ms
step:1767/2285 train_time:107033ms step_avg:60.57ms
step:1768/2285 train_time:107094ms step_avg:60.57ms
step:1769/2285 train_time:107156ms step_avg:60.57ms
step:1770/2285 train_time:107217ms step_avg:60.57ms
step:1771/2285 train_time:107280ms step_avg:60.58ms
step:1772/2285 train_time:107340ms step_avg:60.58ms
step:1773/2285 train_time:107402ms step_avg:60.58ms
step:1774/2285 train_time:107462ms step_avg:60.58ms
step:1775/2285 train_time:107523ms step_avg:60.58ms
step:1776/2285 train_time:107583ms step_avg:60.58ms
step:1777/2285 train_time:107644ms step_avg:60.58ms
step:1778/2285 train_time:107703ms step_avg:60.58ms
step:1779/2285 train_time:107765ms step_avg:60.58ms
step:1780/2285 train_time:107824ms step_avg:60.58ms
step:1781/2285 train_time:107885ms step_avg:60.58ms
step:1782/2285 train_time:107946ms step_avg:60.58ms
step:1783/2285 train_time:108010ms step_avg:60.58ms
step:1784/2285 train_time:108071ms step_avg:60.58ms
step:1785/2285 train_time:108133ms step_avg:60.58ms
step:1786/2285 train_time:108193ms step_avg:60.58ms
step:1787/2285 train_time:108256ms step_avg:60.58ms
step:1788/2285 train_time:108316ms step_avg:60.58ms
step:1789/2285 train_time:108379ms step_avg:60.58ms
step:1790/2285 train_time:108439ms step_avg:60.58ms
step:1791/2285 train_time:108501ms step_avg:60.58ms
step:1792/2285 train_time:108561ms step_avg:60.58ms
step:1793/2285 train_time:108623ms step_avg:60.58ms
step:1794/2285 train_time:108682ms step_avg:60.58ms
step:1795/2285 train_time:108744ms step_avg:60.58ms
step:1796/2285 train_time:108803ms step_avg:60.58ms
step:1797/2285 train_time:108865ms step_avg:60.58ms
step:1798/2285 train_time:108926ms step_avg:60.58ms
step:1799/2285 train_time:108988ms step_avg:60.58ms
step:1800/2285 train_time:109048ms step_avg:60.58ms
step:1801/2285 train_time:109111ms step_avg:60.58ms
step:1802/2285 train_time:109172ms step_avg:60.58ms
step:1803/2285 train_time:109234ms step_avg:60.58ms
step:1804/2285 train_time:109294ms step_avg:60.58ms
step:1805/2285 train_time:109358ms step_avg:60.59ms
step:1806/2285 train_time:109418ms step_avg:60.59ms
step:1807/2285 train_time:109480ms step_avg:60.59ms
step:1808/2285 train_time:109540ms step_avg:60.59ms
step:1809/2285 train_time:109602ms step_avg:60.59ms
step:1810/2285 train_time:109662ms step_avg:60.59ms
step:1811/2285 train_time:109724ms step_avg:60.59ms
step:1812/2285 train_time:109783ms step_avg:60.59ms
step:1813/2285 train_time:109845ms step_avg:60.59ms
step:1814/2285 train_time:109905ms step_avg:60.59ms
step:1815/2285 train_time:109967ms step_avg:60.59ms
step:1816/2285 train_time:110027ms step_avg:60.59ms
step:1817/2285 train_time:110090ms step_avg:60.59ms
step:1818/2285 train_time:110150ms step_avg:60.59ms
step:1819/2285 train_time:110213ms step_avg:60.59ms
step:1820/2285 train_time:110272ms step_avg:60.59ms
step:1821/2285 train_time:110334ms step_avg:60.59ms
step:1822/2285 train_time:110394ms step_avg:60.59ms
step:1823/2285 train_time:110457ms step_avg:60.59ms
step:1824/2285 train_time:110516ms step_avg:60.59ms
step:1825/2285 train_time:110579ms step_avg:60.59ms
step:1826/2285 train_time:110639ms step_avg:60.59ms
step:1827/2285 train_time:110701ms step_avg:60.59ms
step:1828/2285 train_time:110761ms step_avg:60.59ms
step:1829/2285 train_time:110824ms step_avg:60.59ms
step:1830/2285 train_time:110883ms step_avg:60.59ms
step:1831/2285 train_time:110945ms step_avg:60.59ms
step:1832/2285 train_time:111005ms step_avg:60.59ms
step:1833/2285 train_time:111067ms step_avg:60.59ms
step:1834/2285 train_time:111127ms step_avg:60.59ms
step:1835/2285 train_time:111189ms step_avg:60.59ms
step:1836/2285 train_time:111249ms step_avg:60.59ms
step:1837/2285 train_time:111311ms step_avg:60.59ms
step:1838/2285 train_time:111371ms step_avg:60.59ms
step:1839/2285 train_time:111434ms step_avg:60.59ms
step:1840/2285 train_time:111494ms step_avg:60.59ms
step:1841/2285 train_time:111558ms step_avg:60.60ms
step:1842/2285 train_time:111618ms step_avg:60.60ms
step:1843/2285 train_time:111680ms step_avg:60.60ms
step:1844/2285 train_time:111740ms step_avg:60.60ms
step:1845/2285 train_time:111802ms step_avg:60.60ms
step:1846/2285 train_time:111862ms step_avg:60.60ms
step:1847/2285 train_time:111925ms step_avg:60.60ms
step:1848/2285 train_time:111985ms step_avg:60.60ms
step:1849/2285 train_time:112046ms step_avg:60.60ms
step:1850/2285 train_time:112107ms step_avg:60.60ms
step:1851/2285 train_time:112169ms step_avg:60.60ms
step:1852/2285 train_time:112228ms step_avg:60.60ms
step:1853/2285 train_time:112291ms step_avg:60.60ms
step:1854/2285 train_time:112350ms step_avg:60.60ms
step:1855/2285 train_time:112413ms step_avg:60.60ms
step:1856/2285 train_time:112473ms step_avg:60.60ms
step:1857/2285 train_time:112536ms step_avg:60.60ms
step:1858/2285 train_time:112596ms step_avg:60.60ms
step:1859/2285 train_time:112660ms step_avg:60.60ms
step:1860/2285 train_time:112720ms step_avg:60.60ms
step:1861/2285 train_time:112782ms step_avg:60.60ms
step:1862/2285 train_time:112841ms step_avg:60.60ms
step:1863/2285 train_time:112904ms step_avg:60.60ms
step:1864/2285 train_time:112964ms step_avg:60.60ms
step:1865/2285 train_time:113026ms step_avg:60.60ms
step:1866/2285 train_time:113085ms step_avg:60.60ms
step:1867/2285 train_time:113147ms step_avg:60.60ms
step:1868/2285 train_time:113207ms step_avg:60.60ms
step:1869/2285 train_time:113270ms step_avg:60.60ms
step:1870/2285 train_time:113330ms step_avg:60.60ms
step:1871/2285 train_time:113392ms step_avg:60.60ms
step:1872/2285 train_time:113452ms step_avg:60.60ms
step:1873/2285 train_time:113515ms step_avg:60.61ms
step:1874/2285 train_time:113575ms step_avg:60.61ms
step:1875/2285 train_time:113638ms step_avg:60.61ms
step:1876/2285 train_time:113698ms step_avg:60.61ms
step:1877/2285 train_time:113761ms step_avg:60.61ms
step:1878/2285 train_time:113821ms step_avg:60.61ms
step:1879/2285 train_time:113883ms step_avg:60.61ms
step:1880/2285 train_time:113944ms step_avg:60.61ms
step:1881/2285 train_time:114006ms step_avg:60.61ms
step:1882/2285 train_time:114065ms step_avg:60.61ms
step:1883/2285 train_time:114127ms step_avg:60.61ms
step:1884/2285 train_time:114187ms step_avg:60.61ms
step:1885/2285 train_time:114249ms step_avg:60.61ms
step:1886/2285 train_time:114309ms step_avg:60.61ms
step:1887/2285 train_time:114372ms step_avg:60.61ms
step:1888/2285 train_time:114432ms step_avg:60.61ms
step:1889/2285 train_time:114495ms step_avg:60.61ms
step:1890/2285 train_time:114554ms step_avg:60.61ms
step:1891/2285 train_time:114618ms step_avg:60.61ms
step:1892/2285 train_time:114678ms step_avg:60.61ms
step:1893/2285 train_time:114741ms step_avg:60.61ms
step:1894/2285 train_time:114800ms step_avg:60.61ms
step:1895/2285 train_time:114863ms step_avg:60.61ms
step:1896/2285 train_time:114923ms step_avg:60.61ms
step:1897/2285 train_time:114985ms step_avg:60.61ms
step:1898/2285 train_time:115044ms step_avg:60.61ms
step:1899/2285 train_time:115106ms step_avg:60.61ms
step:1900/2285 train_time:115166ms step_avg:60.61ms
step:1901/2285 train_time:115229ms step_avg:60.61ms
step:1902/2285 train_time:115289ms step_avg:60.61ms
step:1903/2285 train_time:115351ms step_avg:60.62ms
step:1904/2285 train_time:115411ms step_avg:60.62ms
step:1905/2285 train_time:115473ms step_avg:60.62ms
step:1906/2285 train_time:115533ms step_avg:60.62ms
step:1907/2285 train_time:115596ms step_avg:60.62ms
step:1908/2285 train_time:115656ms step_avg:60.62ms
step:1909/2285 train_time:115720ms step_avg:60.62ms
step:1910/2285 train_time:115780ms step_avg:60.62ms
step:1911/2285 train_time:115842ms step_avg:60.62ms
step:1912/2285 train_time:115902ms step_avg:60.62ms
step:1913/2285 train_time:115964ms step_avg:60.62ms
step:1914/2285 train_time:116025ms step_avg:60.62ms
step:1915/2285 train_time:116087ms step_avg:60.62ms
step:1916/2285 train_time:116146ms step_avg:60.62ms
step:1917/2285 train_time:116209ms step_avg:60.62ms
step:1918/2285 train_time:116269ms step_avg:60.62ms
step:1919/2285 train_time:116331ms step_avg:60.62ms
step:1920/2285 train_time:116391ms step_avg:60.62ms
step:1921/2285 train_time:116453ms step_avg:60.62ms
step:1922/2285 train_time:116514ms step_avg:60.62ms
step:1923/2285 train_time:116577ms step_avg:60.62ms
step:1924/2285 train_time:116638ms step_avg:60.62ms
step:1925/2285 train_time:116701ms step_avg:60.62ms
step:1926/2285 train_time:116761ms step_avg:60.62ms
step:1927/2285 train_time:116824ms step_avg:60.62ms
step:1928/2285 train_time:116884ms step_avg:60.62ms
step:1929/2285 train_time:116947ms step_avg:60.63ms
step:1930/2285 train_time:117006ms step_avg:60.63ms
step:1931/2285 train_time:117069ms step_avg:60.63ms
step:1932/2285 train_time:117129ms step_avg:60.63ms
step:1933/2285 train_time:117192ms step_avg:60.63ms
step:1934/2285 train_time:117251ms step_avg:60.63ms
step:1935/2285 train_time:117314ms step_avg:60.63ms
step:1936/2285 train_time:117374ms step_avg:60.63ms
step:1937/2285 train_time:117436ms step_avg:60.63ms
step:1938/2285 train_time:117496ms step_avg:60.63ms
step:1939/2285 train_time:117559ms step_avg:60.63ms
step:1940/2285 train_time:117619ms step_avg:60.63ms
step:1941/2285 train_time:117682ms step_avg:60.63ms
step:1942/2285 train_time:117742ms step_avg:60.63ms
step:1943/2285 train_time:117805ms step_avg:60.63ms
step:1944/2285 train_time:117865ms step_avg:60.63ms
step:1945/2285 train_time:117927ms step_avg:60.63ms
step:1946/2285 train_time:117987ms step_avg:60.63ms
step:1947/2285 train_time:118049ms step_avg:60.63ms
step:1948/2285 train_time:118109ms step_avg:60.63ms
step:1949/2285 train_time:118171ms step_avg:60.63ms
step:1950/2285 train_time:118231ms step_avg:60.63ms
step:1951/2285 train_time:118293ms step_avg:60.63ms
step:1952/2285 train_time:118353ms step_avg:60.63ms
step:1953/2285 train_time:118416ms step_avg:60.63ms
step:1954/2285 train_time:118476ms step_avg:60.63ms
step:1955/2285 train_time:118539ms step_avg:60.63ms
step:1956/2285 train_time:118598ms step_avg:60.63ms
step:1957/2285 train_time:118661ms step_avg:60.63ms
step:1958/2285 train_time:118721ms step_avg:60.63ms
step:1959/2285 train_time:118784ms step_avg:60.63ms
step:1960/2285 train_time:118844ms step_avg:60.63ms
step:1961/2285 train_time:118906ms step_avg:60.64ms
step:1962/2285 train_time:118966ms step_avg:60.63ms
step:1963/2285 train_time:119028ms step_avg:60.64ms
step:1964/2285 train_time:119088ms step_avg:60.64ms
step:1965/2285 train_time:119150ms step_avg:60.64ms
step:1966/2285 train_time:119210ms step_avg:60.64ms
step:1967/2285 train_time:119272ms step_avg:60.64ms
step:1968/2285 train_time:119332ms step_avg:60.64ms
step:1969/2285 train_time:119394ms step_avg:60.64ms
step:1970/2285 train_time:119455ms step_avg:60.64ms
step:1971/2285 train_time:119517ms step_avg:60.64ms
step:1972/2285 train_time:119577ms step_avg:60.64ms
step:1973/2285 train_time:119641ms step_avg:60.64ms
step:1974/2285 train_time:119701ms step_avg:60.64ms
step:1975/2285 train_time:119764ms step_avg:60.64ms
step:1976/2285 train_time:119824ms step_avg:60.64ms
step:1977/2285 train_time:119886ms step_avg:60.64ms
step:1978/2285 train_time:119945ms step_avg:60.64ms
step:1979/2285 train_time:120008ms step_avg:60.64ms
step:1980/2285 train_time:120068ms step_avg:60.64ms
step:1981/2285 train_time:120130ms step_avg:60.64ms
step:1982/2285 train_time:120190ms step_avg:60.64ms
step:1983/2285 train_time:120252ms step_avg:60.64ms
step:1984/2285 train_time:120312ms step_avg:60.64ms
step:1985/2285 train_time:120375ms step_avg:60.64ms
step:1986/2285 train_time:120435ms step_avg:60.64ms
step:1987/2285 train_time:120497ms step_avg:60.64ms
step:1988/2285 train_time:120558ms step_avg:60.64ms
step:1989/2285 train_time:120620ms step_avg:60.64ms
step:1990/2285 train_time:120680ms step_avg:60.64ms
step:1991/2285 train_time:120744ms step_avg:60.64ms
step:1992/2285 train_time:120803ms step_avg:60.64ms
step:1993/2285 train_time:120865ms step_avg:60.64ms
step:1994/2285 train_time:120925ms step_avg:60.64ms
step:1995/2285 train_time:120988ms step_avg:60.65ms
step:1996/2285 train_time:121048ms step_avg:60.65ms
step:1997/2285 train_time:121110ms step_avg:60.65ms
step:1998/2285 train_time:121170ms step_avg:60.65ms
step:1999/2285 train_time:121232ms step_avg:60.65ms
step:2000/2285 train_time:121292ms step_avg:60.65ms
step:2000/2285 val_loss:3.3201 train_time:121356ms step_avg:60.68ms
step:2001/2285 train_time:121381ms step_avg:60.66ms
step:2002/2285 train_time:121417ms step_avg:60.65ms
step:2003/2285 train_time:121479ms step_avg:60.65ms
step:2004/2285 train_time:121540ms step_avg:60.65ms
step:2005/2285 train_time:121605ms step_avg:60.65ms
step:2006/2285 train_time:121664ms step_avg:60.65ms
step:2007/2285 train_time:121726ms step_avg:60.65ms
step:2008/2285 train_time:121786ms step_avg:60.65ms
step:2009/2285 train_time:121848ms step_avg:60.65ms
step:2010/2285 train_time:121907ms step_avg:60.65ms
step:2011/2285 train_time:121968ms step_avg:60.65ms
step:2012/2285 train_time:122028ms step_avg:60.65ms
step:2013/2285 train_time:122092ms step_avg:60.65ms
step:2014/2285 train_time:122152ms step_avg:60.65ms
step:2015/2285 train_time:122213ms step_avg:60.65ms
step:2016/2285 train_time:122276ms step_avg:60.65ms
step:2017/2285 train_time:122340ms step_avg:60.65ms
step:2018/2285 train_time:122401ms step_avg:60.65ms
step:2019/2285 train_time:122465ms step_avg:60.66ms
step:2020/2285 train_time:122527ms step_avg:60.66ms
step:2021/2285 train_time:122590ms step_avg:60.66ms
step:2022/2285 train_time:122650ms step_avg:60.66ms
step:2023/2285 train_time:122713ms step_avg:60.66ms
step:2024/2285 train_time:122774ms step_avg:60.66ms
step:2025/2285 train_time:122835ms step_avg:60.66ms
step:2026/2285 train_time:122896ms step_avg:60.66ms
step:2027/2285 train_time:122957ms step_avg:60.66ms
step:2028/2285 train_time:123017ms step_avg:60.66ms
step:2029/2285 train_time:123079ms step_avg:60.66ms
step:2030/2285 train_time:123138ms step_avg:60.66ms
step:2031/2285 train_time:123200ms step_avg:60.66ms
step:2032/2285 train_time:123260ms step_avg:60.66ms
step:2033/2285 train_time:123323ms step_avg:60.66ms
step:2034/2285 train_time:123383ms step_avg:60.66ms
step:2035/2285 train_time:123447ms step_avg:60.66ms
step:2036/2285 train_time:123507ms step_avg:60.66ms
step:2037/2285 train_time:123570ms step_avg:60.66ms
step:2038/2285 train_time:123631ms step_avg:60.66ms
step:2039/2285 train_time:123694ms step_avg:60.66ms
step:2040/2285 train_time:123754ms step_avg:60.66ms
step:2041/2285 train_time:123817ms step_avg:60.66ms
step:2042/2285 train_time:123876ms step_avg:60.66ms
step:2043/2285 train_time:123939ms step_avg:60.67ms
step:2044/2285 train_time:123998ms step_avg:60.66ms
step:2045/2285 train_time:124060ms step_avg:60.67ms
step:2046/2285 train_time:124120ms step_avg:60.66ms
step:2047/2285 train_time:124182ms step_avg:60.67ms
step:2048/2285 train_time:124242ms step_avg:60.67ms
step:2049/2285 train_time:124305ms step_avg:60.67ms
step:2050/2285 train_time:124365ms step_avg:60.67ms
step:2051/2285 train_time:124429ms step_avg:60.67ms
step:2052/2285 train_time:124489ms step_avg:60.67ms
step:2053/2285 train_time:124552ms step_avg:60.67ms
step:2054/2285 train_time:124612ms step_avg:60.67ms
step:2055/2285 train_time:124675ms step_avg:60.67ms
step:2056/2285 train_time:124735ms step_avg:60.67ms
step:2057/2285 train_time:124798ms step_avg:60.67ms
step:2058/2285 train_time:124857ms step_avg:60.67ms
step:2059/2285 train_time:124920ms step_avg:60.67ms
step:2060/2285 train_time:124979ms step_avg:60.67ms
step:2061/2285 train_time:125041ms step_avg:60.67ms
step:2062/2285 train_time:125101ms step_avg:60.67ms
step:2063/2285 train_time:125163ms step_avg:60.67ms
step:2064/2285 train_time:125223ms step_avg:60.67ms
step:2065/2285 train_time:125285ms step_avg:60.67ms
step:2066/2285 train_time:125346ms step_avg:60.67ms
step:2067/2285 train_time:125409ms step_avg:60.67ms
step:2068/2285 train_time:125469ms step_avg:60.67ms
step:2069/2285 train_time:125533ms step_avg:60.67ms
step:2070/2285 train_time:125593ms step_avg:60.67ms
step:2071/2285 train_time:125655ms step_avg:60.67ms
step:2072/2285 train_time:125715ms step_avg:60.67ms
step:2073/2285 train_time:125778ms step_avg:60.67ms
step:2074/2285 train_time:125838ms step_avg:60.67ms
step:2075/2285 train_time:125900ms step_avg:60.67ms
step:2076/2285 train_time:125960ms step_avg:60.67ms
step:2077/2285 train_time:126022ms step_avg:60.68ms
step:2078/2285 train_time:126082ms step_avg:60.67ms
step:2079/2285 train_time:126145ms step_avg:60.68ms
step:2080/2285 train_time:126205ms step_avg:60.68ms
step:2081/2285 train_time:126268ms step_avg:60.68ms
step:2082/2285 train_time:126328ms step_avg:60.68ms
step:2083/2285 train_time:126391ms step_avg:60.68ms
step:2084/2285 train_time:126450ms step_avg:60.68ms
step:2085/2285 train_time:126513ms step_avg:60.68ms
step:2086/2285 train_time:126574ms step_avg:60.68ms
step:2087/2285 train_time:126637ms step_avg:60.68ms
step:2088/2285 train_time:126696ms step_avg:60.68ms
step:2089/2285 train_time:126759ms step_avg:60.68ms
step:2090/2285 train_time:126818ms step_avg:60.68ms
step:2091/2285 train_time:126881ms step_avg:60.68ms
step:2092/2285 train_time:126940ms step_avg:60.68ms
step:2093/2285 train_time:127003ms step_avg:60.68ms
step:2094/2285 train_time:127063ms step_avg:60.68ms
step:2095/2285 train_time:127125ms step_avg:60.68ms
step:2096/2285 train_time:127185ms step_avg:60.68ms
step:2097/2285 train_time:127248ms step_avg:60.68ms
step:2098/2285 train_time:127308ms step_avg:60.68ms
step:2099/2285 train_time:127371ms step_avg:60.68ms
step:2100/2285 train_time:127431ms step_avg:60.68ms
step:2101/2285 train_time:127493ms step_avg:60.68ms
step:2102/2285 train_time:127553ms step_avg:60.68ms
step:2103/2285 train_time:127616ms step_avg:60.68ms
step:2104/2285 train_time:127676ms step_avg:60.68ms
step:2105/2285 train_time:127740ms step_avg:60.68ms
step:2106/2285 train_time:127799ms step_avg:60.68ms
step:2107/2285 train_time:127862ms step_avg:60.68ms
step:2108/2285 train_time:127922ms step_avg:60.68ms
step:2109/2285 train_time:127985ms step_avg:60.69ms
step:2110/2285 train_time:128045ms step_avg:60.68ms
step:2111/2285 train_time:128108ms step_avg:60.69ms
step:2112/2285 train_time:128168ms step_avg:60.69ms
step:2113/2285 train_time:128231ms step_avg:60.69ms
step:2114/2285 train_time:128291ms step_avg:60.69ms
step:2115/2285 train_time:128354ms step_avg:60.69ms
step:2116/2285 train_time:128413ms step_avg:60.69ms
step:2117/2285 train_time:128475ms step_avg:60.69ms
step:2118/2285 train_time:128536ms step_avg:60.69ms
step:2119/2285 train_time:128598ms step_avg:60.69ms
step:2120/2285 train_time:128658ms step_avg:60.69ms
step:2121/2285 train_time:128720ms step_avg:60.69ms
step:2122/2285 train_time:128780ms step_avg:60.69ms
step:2123/2285 train_time:128843ms step_avg:60.69ms
step:2124/2285 train_time:128903ms step_avg:60.69ms
step:2125/2285 train_time:128965ms step_avg:60.69ms
step:2126/2285 train_time:129026ms step_avg:60.69ms
step:2127/2285 train_time:129089ms step_avg:60.69ms
step:2128/2285 train_time:129149ms step_avg:60.69ms
step:2129/2285 train_time:129211ms step_avg:60.69ms
step:2130/2285 train_time:129272ms step_avg:60.69ms
step:2131/2285 train_time:129334ms step_avg:60.69ms
step:2132/2285 train_time:129395ms step_avg:60.69ms
step:2133/2285 train_time:129457ms step_avg:60.69ms
step:2134/2285 train_time:129517ms step_avg:60.69ms
step:2135/2285 train_time:129579ms step_avg:60.69ms
step:2136/2285 train_time:129640ms step_avg:60.69ms
step:2137/2285 train_time:129703ms step_avg:60.69ms
step:2138/2285 train_time:129763ms step_avg:60.69ms
step:2139/2285 train_time:129825ms step_avg:60.69ms
step:2140/2285 train_time:129885ms step_avg:60.69ms
step:2141/2285 train_time:129947ms step_avg:60.69ms
step:2142/2285 train_time:130007ms step_avg:60.69ms
step:2143/2285 train_time:130070ms step_avg:60.70ms
step:2144/2285 train_time:130130ms step_avg:60.69ms
step:2145/2285 train_time:130193ms step_avg:60.70ms
step:2146/2285 train_time:130252ms step_avg:60.70ms
step:2147/2285 train_time:130315ms step_avg:60.70ms
step:2148/2285 train_time:130376ms step_avg:60.70ms
step:2149/2285 train_time:130438ms step_avg:60.70ms
step:2150/2285 train_time:130498ms step_avg:60.70ms
step:2151/2285 train_time:130561ms step_avg:60.70ms
step:2152/2285 train_time:130620ms step_avg:60.70ms
step:2153/2285 train_time:130683ms step_avg:60.70ms
step:2154/2285 train_time:130743ms step_avg:60.70ms
step:2155/2285 train_time:130805ms step_avg:60.70ms
step:2156/2285 train_time:130867ms step_avg:60.70ms
step:2157/2285 train_time:130929ms step_avg:60.70ms
step:2158/2285 train_time:130990ms step_avg:60.70ms
step:2159/2285 train_time:131052ms step_avg:60.70ms
step:2160/2285 train_time:131112ms step_avg:60.70ms
step:2161/2285 train_time:131174ms step_avg:60.70ms
step:2162/2285 train_time:131235ms step_avg:60.70ms
step:2163/2285 train_time:131297ms step_avg:60.70ms
step:2164/2285 train_time:131358ms step_avg:60.70ms
step:2165/2285 train_time:131421ms step_avg:60.70ms
step:2166/2285 train_time:131481ms step_avg:60.70ms
step:2167/2285 train_time:131544ms step_avg:60.70ms
step:2168/2285 train_time:131604ms step_avg:60.70ms
step:2169/2285 train_time:131667ms step_avg:60.70ms
step:2170/2285 train_time:131727ms step_avg:60.70ms
step:2171/2285 train_time:131789ms step_avg:60.70ms
step:2172/2285 train_time:131850ms step_avg:60.70ms
step:2173/2285 train_time:131912ms step_avg:60.71ms
step:2174/2285 train_time:131972ms step_avg:60.70ms
step:2175/2285 train_time:132035ms step_avg:60.71ms
step:2176/2285 train_time:132094ms step_avg:60.71ms
step:2177/2285 train_time:132157ms step_avg:60.71ms
step:2178/2285 train_time:132217ms step_avg:60.71ms
step:2179/2285 train_time:132279ms step_avg:60.71ms
step:2180/2285 train_time:132339ms step_avg:60.71ms
step:2181/2285 train_time:132401ms step_avg:60.71ms
step:2182/2285 train_time:132461ms step_avg:60.71ms
step:2183/2285 train_time:132523ms step_avg:60.71ms
step:2184/2285 train_time:132584ms step_avg:60.71ms
step:2185/2285 train_time:132646ms step_avg:60.71ms
step:2186/2285 train_time:132706ms step_avg:60.71ms
step:2187/2285 train_time:132769ms step_avg:60.71ms
step:2188/2285 train_time:132829ms step_avg:60.71ms
step:2189/2285 train_time:132892ms step_avg:60.71ms
step:2190/2285 train_time:132952ms step_avg:60.71ms
step:2191/2285 train_time:133014ms step_avg:60.71ms
step:2192/2285 train_time:133075ms step_avg:60.71ms
step:2193/2285 train_time:133137ms step_avg:60.71ms
step:2194/2285 train_time:133197ms step_avg:60.71ms
step:2195/2285 train_time:133259ms step_avg:60.71ms
step:2196/2285 train_time:133319ms step_avg:60.71ms
step:2197/2285 train_time:133382ms step_avg:60.71ms
step:2198/2285 train_time:133441ms step_avg:60.71ms
step:2199/2285 train_time:133504ms step_avg:60.71ms
step:2200/2285 train_time:133564ms step_avg:60.71ms
step:2201/2285 train_time:133626ms step_avg:60.71ms
step:2202/2285 train_time:133686ms step_avg:60.71ms
step:2203/2285 train_time:133750ms step_avg:60.71ms
step:2204/2285 train_time:133810ms step_avg:60.71ms
step:2205/2285 train_time:133873ms step_avg:60.71ms
step:2206/2285 train_time:133933ms step_avg:60.71ms
step:2207/2285 train_time:133996ms step_avg:60.71ms
step:2208/2285 train_time:134056ms step_avg:60.71ms
step:2209/2285 train_time:134118ms step_avg:60.71ms
step:2210/2285 train_time:134178ms step_avg:60.71ms
step:2211/2285 train_time:134240ms step_avg:60.71ms
step:2212/2285 train_time:134301ms step_avg:60.71ms
step:2213/2285 train_time:134363ms step_avg:60.72ms
step:2214/2285 train_time:134424ms step_avg:60.72ms
step:2215/2285 train_time:134487ms step_avg:60.72ms
step:2216/2285 train_time:134547ms step_avg:60.72ms
step:2217/2285 train_time:134610ms step_avg:60.72ms
step:2218/2285 train_time:134670ms step_avg:60.72ms
step:2219/2285 train_time:134733ms step_avg:60.72ms
step:2220/2285 train_time:134792ms step_avg:60.72ms
step:2221/2285 train_time:134854ms step_avg:60.72ms
step:2222/2285 train_time:134914ms step_avg:60.72ms
step:2223/2285 train_time:134976ms step_avg:60.72ms
step:2224/2285 train_time:135037ms step_avg:60.72ms
step:2225/2285 train_time:135099ms step_avg:60.72ms
step:2226/2285 train_time:135159ms step_avg:60.72ms
step:2227/2285 train_time:135221ms step_avg:60.72ms
step:2228/2285 train_time:135281ms step_avg:60.72ms
step:2229/2285 train_time:135343ms step_avg:60.72ms
step:2230/2285 train_time:135403ms step_avg:60.72ms
step:2231/2285 train_time:135465ms step_avg:60.72ms
step:2232/2285 train_time:135525ms step_avg:60.72ms
step:2233/2285 train_time:135588ms step_avg:60.72ms
step:2234/2285 train_time:135649ms step_avg:60.72ms
step:2235/2285 train_time:135711ms step_avg:60.72ms
step:2236/2285 train_time:135771ms step_avg:60.72ms
step:2237/2285 train_time:135833ms step_avg:60.72ms
step:2238/2285 train_time:135894ms step_avg:60.72ms
step:2239/2285 train_time:135955ms step_avg:60.72ms
step:2240/2285 train_time:136015ms step_avg:60.72ms
step:2241/2285 train_time:136078ms step_avg:60.72ms
step:2242/2285 train_time:136138ms step_avg:60.72ms
step:2243/2285 train_time:136200ms step_avg:60.72ms
step:2244/2285 train_time:136260ms step_avg:60.72ms
step:2245/2285 train_time:136323ms step_avg:60.72ms
step:2246/2285 train_time:136383ms step_avg:60.72ms
step:2247/2285 train_time:136446ms step_avg:60.72ms
step:2248/2285 train_time:136505ms step_avg:60.72ms
step:2249/2285 train_time:136568ms step_avg:60.72ms
step:2250/2285 train_time:136628ms step_avg:60.72ms
step:2250/2285 val_loss:3.2856 train_time:136692ms step_avg:60.75ms
step:2251/2285 train_time:136714ms step_avg:60.73ms
step:2252/2285 train_time:136754ms step_avg:60.73ms
step:2253/2285 train_time:136819ms step_avg:60.73ms
step:2254/2285 train_time:136880ms step_avg:60.73ms
step:2255/2285 train_time:136942ms step_avg:60.73ms
step:2256/2285 train_time:137002ms step_avg:60.73ms
step:2257/2285 train_time:137064ms step_avg:60.73ms
step:2258/2285 train_time:137123ms step_avg:60.73ms
step:2259/2285 train_time:137184ms step_avg:60.73ms
step:2260/2285 train_time:137244ms step_avg:60.73ms
step:2261/2285 train_time:137306ms step_avg:60.73ms
step:2262/2285 train_time:137366ms step_avg:60.73ms
step:2263/2285 train_time:137428ms step_avg:60.73ms
step:2264/2285 train_time:137488ms step_avg:60.73ms
step:2265/2285 train_time:137550ms step_avg:60.73ms
step:2266/2285 train_time:137610ms step_avg:60.73ms
step:2267/2285 train_time:137674ms step_avg:60.73ms
step:2268/2285 train_time:137735ms step_avg:60.73ms
step:2269/2285 train_time:137799ms step_avg:60.73ms
step:2270/2285 train_time:137859ms step_avg:60.73ms
step:2271/2285 train_time:137923ms step_avg:60.73ms
step:2272/2285 train_time:137983ms step_avg:60.73ms
step:2273/2285 train_time:138045ms step_avg:60.73ms
step:2274/2285 train_time:138104ms step_avg:60.73ms
step:2275/2285 train_time:138166ms step_avg:60.73ms
step:2276/2285 train_time:138225ms step_avg:60.73ms
step:2277/2285 train_time:138287ms step_avg:60.73ms
step:2278/2285 train_time:138346ms step_avg:60.73ms
step:2279/2285 train_time:138408ms step_avg:60.73ms
step:2280/2285 train_time:138468ms step_avg:60.73ms
step:2281/2285 train_time:138530ms step_avg:60.73ms
step:2282/2285 train_time:138590ms step_avg:60.73ms
step:2283/2285 train_time:138653ms step_avg:60.73ms
step:2284/2285 train_time:138714ms step_avg:60.73ms
step:2285/2285 train_time:138777ms step_avg:60.73ms
step:2285/2285 val_loss:3.2794 train_time:138838ms step_avg:60.76ms
peak memory allocated: 29626 MiB reserved: 50528 MiB

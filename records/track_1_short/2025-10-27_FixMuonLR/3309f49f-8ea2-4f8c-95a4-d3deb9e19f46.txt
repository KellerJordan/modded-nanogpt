import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2285
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 02:17:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:121ms step_avg:121.14ms
step:2/2285 train_time:142ms step_avg:70.78ms
step:3/2285 train_time:180ms step_avg:60.11ms
step:4/2285 train_time:237ms step_avg:59.15ms
step:5/2285 train_time:296ms step_avg:59.16ms
step:6/2285 train_time:354ms step_avg:58.97ms
step:7/2285 train_time:414ms step_avg:59.19ms
step:8/2285 train_time:473ms step_avg:59.14ms
step:9/2285 train_time:534ms step_avg:59.34ms
step:10/2285 train_time:593ms step_avg:59.28ms
step:11/2285 train_time:653ms step_avg:59.39ms
step:12/2285 train_time:712ms step_avg:59.35ms
step:13/2285 train_time:773ms step_avg:59.45ms
step:14/2285 train_time:831ms step_avg:59.37ms
step:15/2285 train_time:892ms step_avg:59.49ms
step:16/2285 train_time:951ms step_avg:59.42ms
step:17/2285 train_time:1014ms step_avg:59.62ms
step:18/2285 train_time:1077ms step_avg:59.81ms
step:19/2285 train_time:1141ms step_avg:60.08ms
step:20/2285 train_time:1204ms step_avg:60.18ms
step:21/2285 train_time:1265ms step_avg:60.24ms
step:22/2285 train_time:1323ms step_avg:60.15ms
step:23/2285 train_time:1385ms step_avg:60.20ms
step:24/2285 train_time:1444ms step_avg:60.16ms
step:25/2285 train_time:1505ms step_avg:60.19ms
step:26/2285 train_time:1563ms step_avg:60.13ms
step:27/2285 train_time:1625ms step_avg:60.19ms
step:28/2285 train_time:1683ms step_avg:60.12ms
step:29/2285 train_time:1744ms step_avg:60.14ms
step:30/2285 train_time:1803ms step_avg:60.10ms
step:31/2285 train_time:1864ms step_avg:60.14ms
step:32/2285 train_time:1923ms step_avg:60.11ms
step:33/2285 train_time:1985ms step_avg:60.16ms
step:34/2285 train_time:2045ms step_avg:60.15ms
step:35/2285 train_time:2107ms step_avg:60.21ms
step:36/2285 train_time:2166ms step_avg:60.18ms
step:37/2285 train_time:2228ms step_avg:60.22ms
step:38/2285 train_time:2287ms step_avg:60.19ms
step:39/2285 train_time:2348ms step_avg:60.21ms
step:40/2285 train_time:2408ms step_avg:60.19ms
step:41/2285 train_time:2469ms step_avg:60.22ms
step:42/2285 train_time:2528ms step_avg:60.18ms
step:43/2285 train_time:2589ms step_avg:60.21ms
step:44/2285 train_time:2648ms step_avg:60.17ms
step:45/2285 train_time:2709ms step_avg:60.21ms
step:46/2285 train_time:2769ms step_avg:60.19ms
step:47/2285 train_time:2830ms step_avg:60.22ms
step:48/2285 train_time:2889ms step_avg:60.20ms
step:49/2285 train_time:2952ms step_avg:60.24ms
step:50/2285 train_time:3011ms step_avg:60.22ms
step:51/2285 train_time:3074ms step_avg:60.27ms
step:52/2285 train_time:3133ms step_avg:60.25ms
step:53/2285 train_time:3195ms step_avg:60.28ms
step:54/2285 train_time:3254ms step_avg:60.26ms
step:55/2285 train_time:3316ms step_avg:60.29ms
step:56/2285 train_time:3375ms step_avg:60.27ms
step:57/2285 train_time:3436ms step_avg:60.29ms
step:58/2285 train_time:3496ms step_avg:60.27ms
step:59/2285 train_time:3557ms step_avg:60.30ms
step:60/2285 train_time:3618ms step_avg:60.29ms
step:61/2285 train_time:3679ms step_avg:60.31ms
step:62/2285 train_time:3739ms step_avg:60.30ms
step:63/2285 train_time:3800ms step_avg:60.32ms
step:64/2285 train_time:3859ms step_avg:60.30ms
step:65/2285 train_time:3922ms step_avg:60.33ms
step:66/2285 train_time:3980ms step_avg:60.31ms
step:67/2285 train_time:4042ms step_avg:60.33ms
step:68/2285 train_time:4101ms step_avg:60.32ms
step:69/2285 train_time:4163ms step_avg:60.33ms
step:70/2285 train_time:4221ms step_avg:60.30ms
step:71/2285 train_time:4283ms step_avg:60.32ms
step:72/2285 train_time:4341ms step_avg:60.29ms
step:73/2285 train_time:4403ms step_avg:60.31ms
step:74/2285 train_time:4461ms step_avg:60.29ms
step:75/2285 train_time:4523ms step_avg:60.30ms
step:76/2285 train_time:4581ms step_avg:60.28ms
step:77/2285 train_time:4643ms step_avg:60.29ms
step:78/2285 train_time:4701ms step_avg:60.27ms
step:79/2285 train_time:4763ms step_avg:60.29ms
step:80/2285 train_time:4821ms step_avg:60.27ms
step:81/2285 train_time:4882ms step_avg:60.28ms
step:82/2285 train_time:4941ms step_avg:60.26ms
step:83/2285 train_time:5002ms step_avg:60.27ms
step:84/2285 train_time:5062ms step_avg:60.26ms
step:85/2285 train_time:5123ms step_avg:60.27ms
step:86/2285 train_time:5182ms step_avg:60.25ms
step:87/2285 train_time:5243ms step_avg:60.26ms
step:88/2285 train_time:5301ms step_avg:60.24ms
step:89/2285 train_time:5363ms step_avg:60.26ms
step:90/2285 train_time:5421ms step_avg:60.24ms
step:91/2285 train_time:5483ms step_avg:60.25ms
step:92/2285 train_time:5541ms step_avg:60.23ms
step:93/2285 train_time:5602ms step_avg:60.24ms
step:94/2285 train_time:5661ms step_avg:60.22ms
step:95/2285 train_time:5722ms step_avg:60.24ms
step:96/2285 train_time:5781ms step_avg:60.22ms
step:97/2285 train_time:5843ms step_avg:60.23ms
step:98/2285 train_time:5902ms step_avg:60.22ms
step:99/2285 train_time:5963ms step_avg:60.23ms
step:100/2285 train_time:6022ms step_avg:60.22ms
step:101/2285 train_time:6083ms step_avg:60.22ms
step:102/2285 train_time:6142ms step_avg:60.22ms
step:103/2285 train_time:6203ms step_avg:60.22ms
step:104/2285 train_time:6261ms step_avg:60.21ms
step:105/2285 train_time:6322ms step_avg:60.21ms
step:106/2285 train_time:6381ms step_avg:60.20ms
step:107/2285 train_time:6442ms step_avg:60.21ms
step:108/2285 train_time:6501ms step_avg:60.20ms
step:109/2285 train_time:6562ms step_avg:60.20ms
step:110/2285 train_time:6621ms step_avg:60.19ms
step:111/2285 train_time:6682ms step_avg:60.20ms
step:112/2285 train_time:6741ms step_avg:60.18ms
step:113/2285 train_time:6802ms step_avg:60.19ms
step:114/2285 train_time:6860ms step_avg:60.18ms
step:115/2285 train_time:6922ms step_avg:60.19ms
step:116/2285 train_time:6980ms step_avg:60.17ms
step:117/2285 train_time:7041ms step_avg:60.18ms
step:118/2285 train_time:7099ms step_avg:60.16ms
step:119/2285 train_time:7160ms step_avg:60.17ms
step:120/2285 train_time:7219ms step_avg:60.16ms
step:121/2285 train_time:7280ms step_avg:60.17ms
step:122/2285 train_time:7339ms step_avg:60.16ms
step:123/2285 train_time:7400ms step_avg:60.16ms
step:124/2285 train_time:7459ms step_avg:60.15ms
step:125/2285 train_time:7520ms step_avg:60.16ms
step:126/2285 train_time:7579ms step_avg:60.15ms
step:127/2285 train_time:7639ms step_avg:60.15ms
step:128/2285 train_time:7698ms step_avg:60.14ms
step:129/2285 train_time:7759ms step_avg:60.15ms
step:130/2285 train_time:7818ms step_avg:60.14ms
step:131/2285 train_time:7879ms step_avg:60.15ms
step:132/2285 train_time:7938ms step_avg:60.14ms
step:133/2285 train_time:8000ms step_avg:60.15ms
step:134/2285 train_time:8059ms step_avg:60.14ms
step:135/2285 train_time:8120ms step_avg:60.15ms
step:136/2285 train_time:8179ms step_avg:60.14ms
step:137/2285 train_time:8240ms step_avg:60.14ms
step:138/2285 train_time:8298ms step_avg:60.13ms
step:139/2285 train_time:8359ms step_avg:60.14ms
step:140/2285 train_time:8418ms step_avg:60.13ms
step:141/2285 train_time:8479ms step_avg:60.14ms
step:142/2285 train_time:8538ms step_avg:60.13ms
step:143/2285 train_time:8599ms step_avg:60.14ms
step:144/2285 train_time:8658ms step_avg:60.13ms
step:145/2285 train_time:8719ms step_avg:60.13ms
step:146/2285 train_time:8778ms step_avg:60.12ms
step:147/2285 train_time:8839ms step_avg:60.13ms
step:148/2285 train_time:8898ms step_avg:60.12ms
step:149/2285 train_time:8959ms step_avg:60.13ms
step:150/2285 train_time:9018ms step_avg:60.12ms
step:151/2285 train_time:9079ms step_avg:60.13ms
step:152/2285 train_time:9138ms step_avg:60.12ms
step:153/2285 train_time:9200ms step_avg:60.13ms
step:154/2285 train_time:9258ms step_avg:60.12ms
step:155/2285 train_time:9319ms step_avg:60.12ms
step:156/2285 train_time:9378ms step_avg:60.12ms
step:157/2285 train_time:9439ms step_avg:60.12ms
step:158/2285 train_time:9498ms step_avg:60.11ms
step:159/2285 train_time:9559ms step_avg:60.12ms
step:160/2285 train_time:9617ms step_avg:60.11ms
step:161/2285 train_time:9679ms step_avg:60.12ms
step:162/2285 train_time:9737ms step_avg:60.11ms
step:163/2285 train_time:9799ms step_avg:60.11ms
step:164/2285 train_time:9857ms step_avg:60.11ms
step:165/2285 train_time:9919ms step_avg:60.11ms
step:166/2285 train_time:9977ms step_avg:60.10ms
step:167/2285 train_time:10038ms step_avg:60.11ms
step:168/2285 train_time:10096ms step_avg:60.10ms
step:169/2285 train_time:10158ms step_avg:60.10ms
step:170/2285 train_time:10217ms step_avg:60.10ms
step:171/2285 train_time:10278ms step_avg:60.11ms
step:172/2285 train_time:10336ms step_avg:60.09ms
step:173/2285 train_time:10398ms step_avg:60.10ms
step:174/2285 train_time:10456ms step_avg:60.09ms
step:175/2285 train_time:10517ms step_avg:60.10ms
step:176/2285 train_time:10576ms step_avg:60.09ms
step:177/2285 train_time:10637ms step_avg:60.10ms
step:178/2285 train_time:10696ms step_avg:60.09ms
step:179/2285 train_time:10756ms step_avg:60.09ms
step:180/2285 train_time:10815ms step_avg:60.08ms
step:181/2285 train_time:10876ms step_avg:60.09ms
step:182/2285 train_time:10935ms step_avg:60.08ms
step:183/2285 train_time:10997ms step_avg:60.09ms
step:184/2285 train_time:11055ms step_avg:60.08ms
step:185/2285 train_time:11117ms step_avg:60.09ms
step:186/2285 train_time:11175ms step_avg:60.08ms
step:187/2285 train_time:11236ms step_avg:60.08ms
step:188/2285 train_time:11294ms step_avg:60.08ms
step:189/2285 train_time:11356ms step_avg:60.08ms
step:190/2285 train_time:11415ms step_avg:60.08ms
step:191/2285 train_time:11476ms step_avg:60.09ms
step:192/2285 train_time:11535ms step_avg:60.08ms
step:193/2285 train_time:11596ms step_avg:60.08ms
step:194/2285 train_time:11654ms step_avg:60.07ms
step:195/2285 train_time:11715ms step_avg:60.08ms
step:196/2285 train_time:11774ms step_avg:60.07ms
step:197/2285 train_time:11836ms step_avg:60.08ms
step:198/2285 train_time:11894ms step_avg:60.07ms
step:199/2285 train_time:11956ms step_avg:60.08ms
step:200/2285 train_time:12015ms step_avg:60.07ms
step:201/2285 train_time:12076ms step_avg:60.08ms
step:202/2285 train_time:12134ms step_avg:60.07ms
step:203/2285 train_time:12195ms step_avg:60.07ms
step:204/2285 train_time:12254ms step_avg:60.07ms
step:205/2285 train_time:12315ms step_avg:60.07ms
step:206/2285 train_time:12375ms step_avg:60.07ms
step:207/2285 train_time:12435ms step_avg:60.07ms
step:208/2285 train_time:12494ms step_avg:60.07ms
step:209/2285 train_time:12555ms step_avg:60.07ms
step:210/2285 train_time:12614ms step_avg:60.07ms
step:211/2285 train_time:12675ms step_avg:60.07ms
step:212/2285 train_time:12734ms step_avg:60.06ms
step:213/2285 train_time:12795ms step_avg:60.07ms
step:214/2285 train_time:12853ms step_avg:60.06ms
step:215/2285 train_time:12914ms step_avg:60.07ms
step:216/2285 train_time:12973ms step_avg:60.06ms
step:217/2285 train_time:13034ms step_avg:60.07ms
step:218/2285 train_time:13093ms step_avg:60.06ms
step:219/2285 train_time:13154ms step_avg:60.06ms
step:220/2285 train_time:13213ms step_avg:60.06ms
step:221/2285 train_time:13274ms step_avg:60.06ms
step:222/2285 train_time:13334ms step_avg:60.06ms
step:223/2285 train_time:13395ms step_avg:60.07ms
step:224/2285 train_time:13454ms step_avg:60.06ms
step:225/2285 train_time:13515ms step_avg:60.07ms
step:226/2285 train_time:13575ms step_avg:60.06ms
step:227/2285 train_time:13636ms step_avg:60.07ms
step:228/2285 train_time:13695ms step_avg:60.07ms
step:229/2285 train_time:13756ms step_avg:60.07ms
step:230/2285 train_time:13814ms step_avg:60.06ms
step:231/2285 train_time:13876ms step_avg:60.07ms
step:232/2285 train_time:13934ms step_avg:60.06ms
step:233/2285 train_time:13996ms step_avg:60.07ms
step:234/2285 train_time:14054ms step_avg:60.06ms
step:235/2285 train_time:14115ms step_avg:60.07ms
step:236/2285 train_time:14174ms step_avg:60.06ms
step:237/2285 train_time:14235ms step_avg:60.06ms
step:238/2285 train_time:14294ms step_avg:60.06ms
step:239/2285 train_time:14355ms step_avg:60.06ms
step:240/2285 train_time:14414ms step_avg:60.06ms
step:241/2285 train_time:14476ms step_avg:60.07ms
step:242/2285 train_time:14534ms step_avg:60.06ms
step:243/2285 train_time:14596ms step_avg:60.06ms
step:244/2285 train_time:14654ms step_avg:60.06ms
step:245/2285 train_time:14715ms step_avg:60.06ms
step:246/2285 train_time:14774ms step_avg:60.06ms
step:247/2285 train_time:14835ms step_avg:60.06ms
step:248/2285 train_time:14894ms step_avg:60.06ms
step:249/2285 train_time:14956ms step_avg:60.06ms
step:250/2285 train_time:15015ms step_avg:60.06ms
step:250/2285 val_loss:4.0863 train_time:15078ms step_avg:60.31ms
step:251/2285 train_time:15096ms step_avg:60.14ms
step:252/2285 train_time:15139ms step_avg:60.08ms
step:253/2285 train_time:15207ms step_avg:60.11ms
step:254/2285 train_time:15271ms step_avg:60.12ms
step:255/2285 train_time:15336ms step_avg:60.14ms
step:256/2285 train_time:15395ms step_avg:60.13ms
step:257/2285 train_time:15455ms step_avg:60.14ms
step:258/2285 train_time:15514ms step_avg:60.13ms
step:259/2285 train_time:15574ms step_avg:60.13ms
step:260/2285 train_time:15632ms step_avg:60.12ms
step:261/2285 train_time:15692ms step_avg:60.12ms
step:262/2285 train_time:15750ms step_avg:60.12ms
step:263/2285 train_time:15810ms step_avg:60.12ms
step:264/2285 train_time:15868ms step_avg:60.11ms
step:265/2285 train_time:15928ms step_avg:60.11ms
step:266/2285 train_time:15986ms step_avg:60.10ms
step:267/2285 train_time:16048ms step_avg:60.10ms
step:268/2285 train_time:16107ms step_avg:60.10ms
step:269/2285 train_time:16169ms step_avg:60.11ms
step:270/2285 train_time:16230ms step_avg:60.11ms
step:271/2285 train_time:16292ms step_avg:60.12ms
step:272/2285 train_time:16351ms step_avg:60.11ms
step:273/2285 train_time:16412ms step_avg:60.12ms
step:274/2285 train_time:16471ms step_avg:60.11ms
step:275/2285 train_time:16531ms step_avg:60.11ms
step:276/2285 train_time:16590ms step_avg:60.11ms
step:277/2285 train_time:16650ms step_avg:60.11ms
step:278/2285 train_time:16708ms step_avg:60.10ms
step:279/2285 train_time:16768ms step_avg:60.10ms
step:280/2285 train_time:16826ms step_avg:60.09ms
step:281/2285 train_time:16887ms step_avg:60.10ms
step:282/2285 train_time:16945ms step_avg:60.09ms
step:283/2285 train_time:17005ms step_avg:60.09ms
step:284/2285 train_time:17064ms step_avg:60.08ms
step:285/2285 train_time:17125ms step_avg:60.09ms
step:286/2285 train_time:17184ms step_avg:60.08ms
step:287/2285 train_time:17246ms step_avg:60.09ms
step:288/2285 train_time:17305ms step_avg:60.09ms
step:289/2285 train_time:17366ms step_avg:60.09ms
step:290/2285 train_time:17425ms step_avg:60.09ms
step:291/2285 train_time:17487ms step_avg:60.09ms
step:292/2285 train_time:17546ms step_avg:60.09ms
step:293/2285 train_time:17607ms step_avg:60.09ms
step:294/2285 train_time:17665ms step_avg:60.09ms
step:295/2285 train_time:17726ms step_avg:60.09ms
step:296/2285 train_time:17784ms step_avg:60.08ms
step:297/2285 train_time:17844ms step_avg:60.08ms
step:298/2285 train_time:17902ms step_avg:60.07ms
step:299/2285 train_time:17962ms step_avg:60.07ms
step:300/2285 train_time:18020ms step_avg:60.07ms
step:301/2285 train_time:18081ms step_avg:60.07ms
step:302/2285 train_time:18141ms step_avg:60.07ms
step:303/2285 train_time:18202ms step_avg:60.07ms
step:304/2285 train_time:18261ms step_avg:60.07ms
step:305/2285 train_time:18323ms step_avg:60.08ms
step:306/2285 train_time:18382ms step_avg:60.07ms
step:307/2285 train_time:18443ms step_avg:60.08ms
step:308/2285 train_time:18502ms step_avg:60.07ms
step:309/2285 train_time:18563ms step_avg:60.07ms
step:310/2285 train_time:18622ms step_avg:60.07ms
step:311/2285 train_time:18683ms step_avg:60.07ms
step:312/2285 train_time:18742ms step_avg:60.07ms
step:313/2285 train_time:18803ms step_avg:60.07ms
step:314/2285 train_time:18861ms step_avg:60.07ms
step:315/2285 train_time:18922ms step_avg:60.07ms
step:316/2285 train_time:18980ms step_avg:60.06ms
step:317/2285 train_time:19041ms step_avg:60.07ms
step:318/2285 train_time:19099ms step_avg:60.06ms
step:319/2285 train_time:19161ms step_avg:60.06ms
step:320/2285 train_time:19220ms step_avg:60.06ms
step:321/2285 train_time:19281ms step_avg:60.07ms
step:322/2285 train_time:19340ms step_avg:60.06ms
step:323/2285 train_time:19401ms step_avg:60.07ms
step:324/2285 train_time:19460ms step_avg:60.06ms
step:325/2285 train_time:19521ms step_avg:60.07ms
step:326/2285 train_time:19580ms step_avg:60.06ms
step:327/2285 train_time:19641ms step_avg:60.06ms
step:328/2285 train_time:19700ms step_avg:60.06ms
step:329/2285 train_time:19761ms step_avg:60.06ms
step:330/2285 train_time:19819ms step_avg:60.06ms
step:331/2285 train_time:19880ms step_avg:60.06ms
step:332/2285 train_time:19939ms step_avg:60.06ms
step:333/2285 train_time:20000ms step_avg:60.06ms
step:334/2285 train_time:20058ms step_avg:60.05ms
step:335/2285 train_time:20119ms step_avg:60.06ms
step:336/2285 train_time:20178ms step_avg:60.05ms
step:337/2285 train_time:20239ms step_avg:60.06ms
step:338/2285 train_time:20298ms step_avg:60.05ms
step:339/2285 train_time:20359ms step_avg:60.06ms
step:340/2285 train_time:20418ms step_avg:60.05ms
step:341/2285 train_time:20480ms step_avg:60.06ms
step:342/2285 train_time:20539ms step_avg:60.06ms
step:343/2285 train_time:20600ms step_avg:60.06ms
step:344/2285 train_time:20659ms step_avg:60.05ms
step:345/2285 train_time:20720ms step_avg:60.06ms
step:346/2285 train_time:20779ms step_avg:60.05ms
step:347/2285 train_time:20840ms step_avg:60.06ms
step:348/2285 train_time:20899ms step_avg:60.05ms
step:349/2285 train_time:20959ms step_avg:60.06ms
step:350/2285 train_time:21018ms step_avg:60.05ms
step:351/2285 train_time:21078ms step_avg:60.05ms
step:352/2285 train_time:21137ms step_avg:60.05ms
step:353/2285 train_time:21198ms step_avg:60.05ms
step:354/2285 train_time:21257ms step_avg:60.05ms
step:355/2285 train_time:21319ms step_avg:60.05ms
step:356/2285 train_time:21378ms step_avg:60.05ms
step:357/2285 train_time:21440ms step_avg:60.06ms
step:358/2285 train_time:21499ms step_avg:60.05ms
step:359/2285 train_time:21560ms step_avg:60.06ms
step:360/2285 train_time:21620ms step_avg:60.05ms
step:361/2285 train_time:21681ms step_avg:60.06ms
step:362/2285 train_time:21739ms step_avg:60.05ms
step:363/2285 train_time:21800ms step_avg:60.06ms
step:364/2285 train_time:21859ms step_avg:60.05ms
step:365/2285 train_time:21920ms step_avg:60.05ms
step:366/2285 train_time:21978ms step_avg:60.05ms
step:367/2285 train_time:22040ms step_avg:60.05ms
step:368/2285 train_time:22098ms step_avg:60.05ms
step:369/2285 train_time:22159ms step_avg:60.05ms
step:370/2285 train_time:22218ms step_avg:60.05ms
step:371/2285 train_time:22279ms step_avg:60.05ms
step:372/2285 train_time:22339ms step_avg:60.05ms
step:373/2285 train_time:22400ms step_avg:60.05ms
step:374/2285 train_time:22458ms step_avg:60.05ms
step:375/2285 train_time:22520ms step_avg:60.05ms
step:376/2285 train_time:22579ms step_avg:60.05ms
step:377/2285 train_time:22639ms step_avg:60.05ms
step:378/2285 train_time:22698ms step_avg:60.05ms
step:379/2285 train_time:22759ms step_avg:60.05ms
step:380/2285 train_time:22818ms step_avg:60.05ms
step:381/2285 train_time:22879ms step_avg:60.05ms
step:382/2285 train_time:22938ms step_avg:60.05ms
step:383/2285 train_time:23000ms step_avg:60.05ms
step:384/2285 train_time:23059ms step_avg:60.05ms
step:385/2285 train_time:23121ms step_avg:60.05ms
step:386/2285 train_time:23180ms step_avg:60.05ms
step:387/2285 train_time:23242ms step_avg:60.06ms
step:388/2285 train_time:23301ms step_avg:60.05ms
step:389/2285 train_time:23363ms step_avg:60.06ms
step:390/2285 train_time:23422ms step_avg:60.06ms
step:391/2285 train_time:23483ms step_avg:60.06ms
step:392/2285 train_time:23543ms step_avg:60.06ms
step:393/2285 train_time:23603ms step_avg:60.06ms
step:394/2285 train_time:23662ms step_avg:60.06ms
step:395/2285 train_time:23723ms step_avg:60.06ms
step:396/2285 train_time:23782ms step_avg:60.06ms
step:397/2285 train_time:23843ms step_avg:60.06ms
step:398/2285 train_time:23902ms step_avg:60.06ms
step:399/2285 train_time:23963ms step_avg:60.06ms
step:400/2285 train_time:24022ms step_avg:60.06ms
step:401/2285 train_time:24083ms step_avg:60.06ms
step:402/2285 train_time:24142ms step_avg:60.06ms
step:403/2285 train_time:24204ms step_avg:60.06ms
step:404/2285 train_time:24263ms step_avg:60.06ms
step:405/2285 train_time:24324ms step_avg:60.06ms
step:406/2285 train_time:24383ms step_avg:60.06ms
step:407/2285 train_time:24445ms step_avg:60.06ms
step:408/2285 train_time:24504ms step_avg:60.06ms
step:409/2285 train_time:24566ms step_avg:60.06ms
step:410/2285 train_time:24625ms step_avg:60.06ms
step:411/2285 train_time:24686ms step_avg:60.06ms
step:412/2285 train_time:24745ms step_avg:60.06ms
step:413/2285 train_time:24806ms step_avg:60.06ms
step:414/2285 train_time:24864ms step_avg:60.06ms
step:415/2285 train_time:24926ms step_avg:60.06ms
step:416/2285 train_time:24985ms step_avg:60.06ms
step:417/2285 train_time:25046ms step_avg:60.06ms
step:418/2285 train_time:25105ms step_avg:60.06ms
step:419/2285 train_time:25166ms step_avg:60.06ms
step:420/2285 train_time:25225ms step_avg:60.06ms
step:421/2285 train_time:25287ms step_avg:60.06ms
step:422/2285 train_time:25346ms step_avg:60.06ms
step:423/2285 train_time:25408ms step_avg:60.07ms
step:424/2285 train_time:25467ms step_avg:60.06ms
step:425/2285 train_time:25528ms step_avg:60.07ms
step:426/2285 train_time:25587ms step_avg:60.06ms
step:427/2285 train_time:25648ms step_avg:60.07ms
step:428/2285 train_time:25707ms step_avg:60.06ms
step:429/2285 train_time:25768ms step_avg:60.07ms
step:430/2285 train_time:25827ms step_avg:60.06ms
step:431/2285 train_time:25888ms step_avg:60.06ms
step:432/2285 train_time:25947ms step_avg:60.06ms
step:433/2285 train_time:26008ms step_avg:60.07ms
step:434/2285 train_time:26067ms step_avg:60.06ms
step:435/2285 train_time:26129ms step_avg:60.07ms
step:436/2285 train_time:26188ms step_avg:60.07ms
step:437/2285 train_time:26250ms step_avg:60.07ms
step:438/2285 train_time:26309ms step_avg:60.07ms
step:439/2285 train_time:26370ms step_avg:60.07ms
step:440/2285 train_time:26429ms step_avg:60.07ms
step:441/2285 train_time:26491ms step_avg:60.07ms
step:442/2285 train_time:26550ms step_avg:60.07ms
step:443/2285 train_time:26610ms step_avg:60.07ms
step:444/2285 train_time:26669ms step_avg:60.07ms
step:445/2285 train_time:26730ms step_avg:60.07ms
step:446/2285 train_time:26790ms step_avg:60.07ms
step:447/2285 train_time:26851ms step_avg:60.07ms
step:448/2285 train_time:26909ms step_avg:60.07ms
step:449/2285 train_time:26971ms step_avg:60.07ms
step:450/2285 train_time:27029ms step_avg:60.07ms
step:451/2285 train_time:27091ms step_avg:60.07ms
step:452/2285 train_time:27149ms step_avg:60.07ms
step:453/2285 train_time:27211ms step_avg:60.07ms
step:454/2285 train_time:27271ms step_avg:60.07ms
step:455/2285 train_time:27332ms step_avg:60.07ms
step:456/2285 train_time:27392ms step_avg:60.07ms
step:457/2285 train_time:27453ms step_avg:60.07ms
step:458/2285 train_time:27512ms step_avg:60.07ms
step:459/2285 train_time:27573ms step_avg:60.07ms
step:460/2285 train_time:27632ms step_avg:60.07ms
step:461/2285 train_time:27694ms step_avg:60.07ms
step:462/2285 train_time:27753ms step_avg:60.07ms
step:463/2285 train_time:27814ms step_avg:60.07ms
step:464/2285 train_time:27873ms step_avg:60.07ms
step:465/2285 train_time:27934ms step_avg:60.07ms
step:466/2285 train_time:27993ms step_avg:60.07ms
step:467/2285 train_time:28054ms step_avg:60.07ms
step:468/2285 train_time:28113ms step_avg:60.07ms
step:469/2285 train_time:28175ms step_avg:60.07ms
step:470/2285 train_time:28235ms step_avg:60.07ms
step:471/2285 train_time:28296ms step_avg:60.08ms
step:472/2285 train_time:28355ms step_avg:60.07ms
step:473/2285 train_time:28416ms step_avg:60.08ms
step:474/2285 train_time:28475ms step_avg:60.07ms
step:475/2285 train_time:28537ms step_avg:60.08ms
step:476/2285 train_time:28596ms step_avg:60.08ms
step:477/2285 train_time:28657ms step_avg:60.08ms
step:478/2285 train_time:28717ms step_avg:60.08ms
step:479/2285 train_time:28778ms step_avg:60.08ms
step:480/2285 train_time:28837ms step_avg:60.08ms
step:481/2285 train_time:28898ms step_avg:60.08ms
step:482/2285 train_time:28957ms step_avg:60.08ms
step:483/2285 train_time:29019ms step_avg:60.08ms
step:484/2285 train_time:29078ms step_avg:60.08ms
step:485/2285 train_time:29140ms step_avg:60.08ms
step:486/2285 train_time:29199ms step_avg:60.08ms
step:487/2285 train_time:29260ms step_avg:60.08ms
step:488/2285 train_time:29320ms step_avg:60.08ms
step:489/2285 train_time:29382ms step_avg:60.09ms
step:490/2285 train_time:29441ms step_avg:60.08ms
step:491/2285 train_time:29502ms step_avg:60.09ms
step:492/2285 train_time:29561ms step_avg:60.08ms
step:493/2285 train_time:29623ms step_avg:60.09ms
step:494/2285 train_time:29682ms step_avg:60.08ms
step:495/2285 train_time:29744ms step_avg:60.09ms
step:496/2285 train_time:29803ms step_avg:60.09ms
step:497/2285 train_time:29864ms step_avg:60.09ms
step:498/2285 train_time:29923ms step_avg:60.09ms
step:499/2285 train_time:29984ms step_avg:60.09ms
step:500/2285 train_time:30043ms step_avg:60.09ms
step:500/2285 val_loss:3.7891 train_time:30106ms step_avg:60.21ms
step:501/2285 train_time:30125ms step_avg:60.13ms
step:502/2285 train_time:30166ms step_avg:60.09ms
step:503/2285 train_time:30227ms step_avg:60.09ms
step:504/2285 train_time:30287ms step_avg:60.09ms
step:505/2285 train_time:30349ms step_avg:60.10ms
step:506/2285 train_time:30408ms step_avg:60.10ms
step:507/2285 train_time:30469ms step_avg:60.10ms
step:508/2285 train_time:30528ms step_avg:60.10ms
step:509/2285 train_time:30591ms step_avg:60.10ms
step:510/2285 train_time:30649ms step_avg:60.10ms
step:511/2285 train_time:30711ms step_avg:60.10ms
step:512/2285 train_time:30769ms step_avg:60.10ms
step:513/2285 train_time:30830ms step_avg:60.10ms
step:514/2285 train_time:30890ms step_avg:60.10ms
step:515/2285 train_time:30952ms step_avg:60.10ms
step:516/2285 train_time:31012ms step_avg:60.10ms
step:517/2285 train_time:31078ms step_avg:60.11ms
step:518/2285 train_time:31138ms step_avg:60.11ms
step:519/2285 train_time:31199ms step_avg:60.11ms
step:520/2285 train_time:31258ms step_avg:60.11ms
step:521/2285 train_time:31321ms step_avg:60.12ms
step:522/2285 train_time:31380ms step_avg:60.12ms
step:523/2285 train_time:31441ms step_avg:60.12ms
step:524/2285 train_time:31500ms step_avg:60.11ms
step:525/2285 train_time:31561ms step_avg:60.12ms
step:526/2285 train_time:31620ms step_avg:60.11ms
step:527/2285 train_time:31681ms step_avg:60.12ms
step:528/2285 train_time:31740ms step_avg:60.11ms
step:529/2285 train_time:31801ms step_avg:60.12ms
step:530/2285 train_time:31860ms step_avg:60.11ms
step:531/2285 train_time:31921ms step_avg:60.12ms
step:532/2285 train_time:31981ms step_avg:60.11ms
step:533/2285 train_time:32043ms step_avg:60.12ms
step:534/2285 train_time:32103ms step_avg:60.12ms
step:535/2285 train_time:32164ms step_avg:60.12ms
step:536/2285 train_time:32224ms step_avg:60.12ms
step:537/2285 train_time:32286ms step_avg:60.12ms
step:538/2285 train_time:32345ms step_avg:60.12ms
step:539/2285 train_time:32406ms step_avg:60.12ms
step:540/2285 train_time:32465ms step_avg:60.12ms
step:541/2285 train_time:32527ms step_avg:60.12ms
step:542/2285 train_time:32585ms step_avg:60.12ms
step:543/2285 train_time:32647ms step_avg:60.12ms
step:544/2285 train_time:32705ms step_avg:60.12ms
step:545/2285 train_time:32767ms step_avg:60.12ms
step:546/2285 train_time:32826ms step_avg:60.12ms
step:547/2285 train_time:32887ms step_avg:60.12ms
step:548/2285 train_time:32946ms step_avg:60.12ms
step:549/2285 train_time:33008ms step_avg:60.12ms
step:550/2285 train_time:33069ms step_avg:60.13ms
step:551/2285 train_time:33130ms step_avg:60.13ms
step:552/2285 train_time:33190ms step_avg:60.13ms
step:553/2285 train_time:33251ms step_avg:60.13ms
step:554/2285 train_time:33310ms step_avg:60.13ms
step:555/2285 train_time:33372ms step_avg:60.13ms
step:556/2285 train_time:33430ms step_avg:60.13ms
step:557/2285 train_time:33492ms step_avg:60.13ms
step:558/2285 train_time:33551ms step_avg:60.13ms
step:559/2285 train_time:33612ms step_avg:60.13ms
step:560/2285 train_time:33672ms step_avg:60.13ms
step:561/2285 train_time:33733ms step_avg:60.13ms
step:562/2285 train_time:33792ms step_avg:60.13ms
step:563/2285 train_time:33854ms step_avg:60.13ms
step:564/2285 train_time:33914ms step_avg:60.13ms
step:565/2285 train_time:33975ms step_avg:60.13ms
step:566/2285 train_time:34035ms step_avg:60.13ms
step:567/2285 train_time:34097ms step_avg:60.14ms
step:568/2285 train_time:34156ms step_avg:60.13ms
step:569/2285 train_time:34217ms step_avg:60.14ms
step:570/2285 train_time:34276ms step_avg:60.13ms
step:571/2285 train_time:34338ms step_avg:60.14ms
step:572/2285 train_time:34397ms step_avg:60.13ms
step:573/2285 train_time:34458ms step_avg:60.14ms
step:574/2285 train_time:34517ms step_avg:60.13ms
step:575/2285 train_time:34578ms step_avg:60.14ms
step:576/2285 train_time:34637ms step_avg:60.13ms
step:577/2285 train_time:34698ms step_avg:60.14ms
step:578/2285 train_time:34757ms step_avg:60.13ms
step:579/2285 train_time:34819ms step_avg:60.14ms
step:580/2285 train_time:34878ms step_avg:60.13ms
step:581/2285 train_time:34940ms step_avg:60.14ms
step:582/2285 train_time:34998ms step_avg:60.13ms
step:583/2285 train_time:35060ms step_avg:60.14ms
step:584/2285 train_time:35119ms step_avg:60.14ms
step:585/2285 train_time:35180ms step_avg:60.14ms
step:586/2285 train_time:35239ms step_avg:60.14ms
step:587/2285 train_time:35301ms step_avg:60.14ms
step:588/2285 train_time:35360ms step_avg:60.14ms
step:589/2285 train_time:35421ms step_avg:60.14ms
step:590/2285 train_time:35480ms step_avg:60.14ms
step:591/2285 train_time:35542ms step_avg:60.14ms
step:592/2285 train_time:35601ms step_avg:60.14ms
step:593/2285 train_time:35662ms step_avg:60.14ms
step:594/2285 train_time:35721ms step_avg:60.14ms
step:595/2285 train_time:35782ms step_avg:60.14ms
step:596/2285 train_time:35841ms step_avg:60.14ms
step:597/2285 train_time:35902ms step_avg:60.14ms
step:598/2285 train_time:35961ms step_avg:60.14ms
step:599/2285 train_time:36022ms step_avg:60.14ms
step:600/2285 train_time:36081ms step_avg:60.14ms
step:601/2285 train_time:36142ms step_avg:60.14ms
step:602/2285 train_time:36201ms step_avg:60.13ms
step:603/2285 train_time:36263ms step_avg:60.14ms
step:604/2285 train_time:36321ms step_avg:60.13ms
step:605/2285 train_time:36383ms step_avg:60.14ms
step:606/2285 train_time:36443ms step_avg:60.14ms
step:607/2285 train_time:36504ms step_avg:60.14ms
step:608/2285 train_time:36564ms step_avg:60.14ms
step:609/2285 train_time:36624ms step_avg:60.14ms
step:610/2285 train_time:36683ms step_avg:60.14ms
step:611/2285 train_time:36744ms step_avg:60.14ms
step:612/2285 train_time:36803ms step_avg:60.14ms
step:613/2285 train_time:36865ms step_avg:60.14ms
step:614/2285 train_time:36924ms step_avg:60.14ms
step:615/2285 train_time:36986ms step_avg:60.14ms
step:616/2285 train_time:37045ms step_avg:60.14ms
step:617/2285 train_time:37106ms step_avg:60.14ms
step:618/2285 train_time:37165ms step_avg:60.14ms
step:619/2285 train_time:37227ms step_avg:60.14ms
step:620/2285 train_time:37286ms step_avg:60.14ms
step:621/2285 train_time:37348ms step_avg:60.14ms
step:622/2285 train_time:37407ms step_avg:60.14ms
step:623/2285 train_time:37469ms step_avg:60.14ms
step:624/2285 train_time:37529ms step_avg:60.14ms
step:625/2285 train_time:37590ms step_avg:60.14ms
step:626/2285 train_time:37649ms step_avg:60.14ms
step:627/2285 train_time:37710ms step_avg:60.14ms
step:628/2285 train_time:37770ms step_avg:60.14ms
step:629/2285 train_time:37831ms step_avg:60.15ms
step:630/2285 train_time:37891ms step_avg:60.14ms
step:631/2285 train_time:37953ms step_avg:60.15ms
step:632/2285 train_time:38012ms step_avg:60.15ms
step:633/2285 train_time:38074ms step_avg:60.15ms
step:634/2285 train_time:38133ms step_avg:60.15ms
step:635/2285 train_time:38195ms step_avg:60.15ms
step:636/2285 train_time:38254ms step_avg:60.15ms
step:637/2285 train_time:38316ms step_avg:60.15ms
step:638/2285 train_time:38376ms step_avg:60.15ms
step:639/2285 train_time:38437ms step_avg:60.15ms
step:640/2285 train_time:38496ms step_avg:60.15ms
step:641/2285 train_time:38558ms step_avg:60.15ms
step:642/2285 train_time:38617ms step_avg:60.15ms
step:643/2285 train_time:38679ms step_avg:60.15ms
step:644/2285 train_time:38738ms step_avg:60.15ms
step:645/2285 train_time:38799ms step_avg:60.15ms
step:646/2285 train_time:38858ms step_avg:60.15ms
step:647/2285 train_time:38920ms step_avg:60.15ms
step:648/2285 train_time:38979ms step_avg:60.15ms
step:649/2285 train_time:39040ms step_avg:60.15ms
step:650/2285 train_time:39099ms step_avg:60.15ms
step:651/2285 train_time:39161ms step_avg:60.15ms
step:652/2285 train_time:39220ms step_avg:60.15ms
step:653/2285 train_time:39281ms step_avg:60.15ms
step:654/2285 train_time:39340ms step_avg:60.15ms
step:655/2285 train_time:39401ms step_avg:60.15ms
step:656/2285 train_time:39460ms step_avg:60.15ms
step:657/2285 train_time:39522ms step_avg:60.15ms
step:658/2285 train_time:39581ms step_avg:60.15ms
step:659/2285 train_time:39642ms step_avg:60.16ms
step:660/2285 train_time:39702ms step_avg:60.15ms
step:661/2285 train_time:39764ms step_avg:60.16ms
step:662/2285 train_time:39823ms step_avg:60.15ms
step:663/2285 train_time:39884ms step_avg:60.16ms
step:664/2285 train_time:39943ms step_avg:60.16ms
step:665/2285 train_time:40004ms step_avg:60.16ms
step:666/2285 train_time:40063ms step_avg:60.16ms
step:667/2285 train_time:40125ms step_avg:60.16ms
step:668/2285 train_time:40184ms step_avg:60.16ms
step:669/2285 train_time:40245ms step_avg:60.16ms
step:670/2285 train_time:40304ms step_avg:60.16ms
step:671/2285 train_time:40366ms step_avg:60.16ms
step:672/2285 train_time:40426ms step_avg:60.16ms
step:673/2285 train_time:40488ms step_avg:60.16ms
step:674/2285 train_time:40546ms step_avg:60.16ms
step:675/2285 train_time:40608ms step_avg:60.16ms
step:676/2285 train_time:40668ms step_avg:60.16ms
step:677/2285 train_time:40729ms step_avg:60.16ms
step:678/2285 train_time:40788ms step_avg:60.16ms
step:679/2285 train_time:40850ms step_avg:60.16ms
step:680/2285 train_time:40910ms step_avg:60.16ms
step:681/2285 train_time:40972ms step_avg:60.16ms
step:682/2285 train_time:41031ms step_avg:60.16ms
step:683/2285 train_time:41093ms step_avg:60.16ms
step:684/2285 train_time:41152ms step_avg:60.16ms
step:685/2285 train_time:41214ms step_avg:60.17ms
step:686/2285 train_time:41273ms step_avg:60.17ms
step:687/2285 train_time:41335ms step_avg:60.17ms
step:688/2285 train_time:41394ms step_avg:60.17ms
step:689/2285 train_time:41455ms step_avg:60.17ms
step:690/2285 train_time:41515ms step_avg:60.17ms
step:691/2285 train_time:41577ms step_avg:60.17ms
step:692/2285 train_time:41636ms step_avg:60.17ms
step:693/2285 train_time:41698ms step_avg:60.17ms
step:694/2285 train_time:41757ms step_avg:60.17ms
step:695/2285 train_time:41819ms step_avg:60.17ms
step:696/2285 train_time:41878ms step_avg:60.17ms
step:697/2285 train_time:41939ms step_avg:60.17ms
step:698/2285 train_time:41998ms step_avg:60.17ms
step:699/2285 train_time:42060ms step_avg:60.17ms
step:700/2285 train_time:42119ms step_avg:60.17ms
step:701/2285 train_time:42180ms step_avg:60.17ms
step:702/2285 train_time:42239ms step_avg:60.17ms
step:703/2285 train_time:42300ms step_avg:60.17ms
step:704/2285 train_time:42359ms step_avg:60.17ms
step:705/2285 train_time:42421ms step_avg:60.17ms
step:706/2285 train_time:42480ms step_avg:60.17ms
step:707/2285 train_time:42541ms step_avg:60.17ms
step:708/2285 train_time:42600ms step_avg:60.17ms
step:709/2285 train_time:42662ms step_avg:60.17ms
step:710/2285 train_time:42721ms step_avg:60.17ms
step:711/2285 train_time:42783ms step_avg:60.17ms
step:712/2285 train_time:42841ms step_avg:60.17ms
step:713/2285 train_time:42903ms step_avg:60.17ms
step:714/2285 train_time:42962ms step_avg:60.17ms
step:715/2285 train_time:43023ms step_avg:60.17ms
step:716/2285 train_time:43082ms step_avg:60.17ms
step:717/2285 train_time:43144ms step_avg:60.17ms
step:718/2285 train_time:43203ms step_avg:60.17ms
step:719/2285 train_time:43264ms step_avg:60.17ms
step:720/2285 train_time:43323ms step_avg:60.17ms
step:721/2285 train_time:43384ms step_avg:60.17ms
step:722/2285 train_time:43443ms step_avg:60.17ms
step:723/2285 train_time:43505ms step_avg:60.17ms
step:724/2285 train_time:43564ms step_avg:60.17ms
step:725/2285 train_time:43626ms step_avg:60.17ms
step:726/2285 train_time:43685ms step_avg:60.17ms
step:727/2285 train_time:43746ms step_avg:60.17ms
step:728/2285 train_time:43806ms step_avg:60.17ms
step:729/2285 train_time:43868ms step_avg:60.17ms
step:730/2285 train_time:43927ms step_avg:60.17ms
step:731/2285 train_time:43988ms step_avg:60.18ms
step:732/2285 train_time:44048ms step_avg:60.17ms
step:733/2285 train_time:44110ms step_avg:60.18ms
step:734/2285 train_time:44170ms step_avg:60.18ms
step:735/2285 train_time:44231ms step_avg:60.18ms
step:736/2285 train_time:44290ms step_avg:60.18ms
step:737/2285 train_time:44353ms step_avg:60.18ms
step:738/2285 train_time:44412ms step_avg:60.18ms
step:739/2285 train_time:44473ms step_avg:60.18ms
step:740/2285 train_time:44532ms step_avg:60.18ms
step:741/2285 train_time:44595ms step_avg:60.18ms
step:742/2285 train_time:44654ms step_avg:60.18ms
step:743/2285 train_time:44715ms step_avg:60.18ms
step:744/2285 train_time:44775ms step_avg:60.18ms
step:745/2285 train_time:44836ms step_avg:60.18ms
step:746/2285 train_time:44895ms step_avg:60.18ms
step:747/2285 train_time:44957ms step_avg:60.18ms
step:748/2285 train_time:45016ms step_avg:60.18ms
step:749/2285 train_time:45078ms step_avg:60.18ms
step:750/2285 train_time:45138ms step_avg:60.18ms
step:750/2285 val_loss:3.6571 train_time:45200ms step_avg:60.27ms
step:751/2285 train_time:45218ms step_avg:60.21ms
step:752/2285 train_time:45260ms step_avg:60.19ms
step:753/2285 train_time:45324ms step_avg:60.19ms
step:754/2285 train_time:45387ms step_avg:60.20ms
step:755/2285 train_time:45449ms step_avg:60.20ms
step:756/2285 train_time:45508ms step_avg:60.20ms
step:757/2285 train_time:45570ms step_avg:60.20ms
step:758/2285 train_time:45628ms step_avg:60.20ms
step:759/2285 train_time:45689ms step_avg:60.20ms
step:760/2285 train_time:45747ms step_avg:60.19ms
step:761/2285 train_time:45808ms step_avg:60.19ms
step:762/2285 train_time:45866ms step_avg:60.19ms
step:763/2285 train_time:45927ms step_avg:60.19ms
step:764/2285 train_time:45986ms step_avg:60.19ms
step:765/2285 train_time:46047ms step_avg:60.19ms
step:766/2285 train_time:46107ms step_avg:60.19ms
step:767/2285 train_time:46170ms step_avg:60.20ms
step:768/2285 train_time:46231ms step_avg:60.20ms
step:769/2285 train_time:46295ms step_avg:60.20ms
step:770/2285 train_time:46355ms step_avg:60.20ms
step:771/2285 train_time:46418ms step_avg:60.20ms
step:772/2285 train_time:46477ms step_avg:60.20ms
step:773/2285 train_time:46539ms step_avg:60.21ms
step:774/2285 train_time:46598ms step_avg:60.20ms
step:775/2285 train_time:46661ms step_avg:60.21ms
step:776/2285 train_time:46720ms step_avg:60.21ms
step:777/2285 train_time:46781ms step_avg:60.21ms
step:778/2285 train_time:46840ms step_avg:60.21ms
step:779/2285 train_time:46902ms step_avg:60.21ms
step:780/2285 train_time:46960ms step_avg:60.21ms
step:781/2285 train_time:47021ms step_avg:60.21ms
step:782/2285 train_time:47081ms step_avg:60.21ms
step:783/2285 train_time:47143ms step_avg:60.21ms
step:784/2285 train_time:47203ms step_avg:60.21ms
step:785/2285 train_time:47265ms step_avg:60.21ms
step:786/2285 train_time:47325ms step_avg:60.21ms
step:787/2285 train_time:47388ms step_avg:60.21ms
step:788/2285 train_time:47447ms step_avg:60.21ms
step:789/2285 train_time:47509ms step_avg:60.21ms
step:790/2285 train_time:47569ms step_avg:60.21ms
step:791/2285 train_time:47631ms step_avg:60.22ms
step:792/2285 train_time:47690ms step_avg:60.22ms
step:793/2285 train_time:47752ms step_avg:60.22ms
step:794/2285 train_time:47812ms step_avg:60.22ms
step:795/2285 train_time:47873ms step_avg:60.22ms
step:796/2285 train_time:47933ms step_avg:60.22ms
step:797/2285 train_time:47995ms step_avg:60.22ms
step:798/2285 train_time:48055ms step_avg:60.22ms
step:799/2285 train_time:48118ms step_avg:60.22ms
step:800/2285 train_time:48177ms step_avg:60.22ms
step:801/2285 train_time:48240ms step_avg:60.22ms
step:802/2285 train_time:48299ms step_avg:60.22ms
step:803/2285 train_time:48361ms step_avg:60.23ms
step:804/2285 train_time:48421ms step_avg:60.22ms
step:805/2285 train_time:48483ms step_avg:60.23ms
step:806/2285 train_time:48542ms step_avg:60.23ms
step:807/2285 train_time:48605ms step_avg:60.23ms
step:808/2285 train_time:48664ms step_avg:60.23ms
step:809/2285 train_time:48725ms step_avg:60.23ms
step:810/2285 train_time:48785ms step_avg:60.23ms
step:811/2285 train_time:48846ms step_avg:60.23ms
step:812/2285 train_time:48906ms step_avg:60.23ms
step:813/2285 train_time:48968ms step_avg:60.23ms
step:814/2285 train_time:49028ms step_avg:60.23ms
step:815/2285 train_time:49090ms step_avg:60.23ms
step:816/2285 train_time:49149ms step_avg:60.23ms
step:817/2285 train_time:49212ms step_avg:60.23ms
step:818/2285 train_time:49271ms step_avg:60.23ms
step:819/2285 train_time:49333ms step_avg:60.24ms
step:820/2285 train_time:49393ms step_avg:60.24ms
step:821/2285 train_time:49455ms step_avg:60.24ms
step:822/2285 train_time:49514ms step_avg:60.24ms
step:823/2285 train_time:49576ms step_avg:60.24ms
step:824/2285 train_time:49636ms step_avg:60.24ms
step:825/2285 train_time:49699ms step_avg:60.24ms
step:826/2285 train_time:49758ms step_avg:60.24ms
step:827/2285 train_time:49820ms step_avg:60.24ms
step:828/2285 train_time:49879ms step_avg:60.24ms
step:829/2285 train_time:49941ms step_avg:60.24ms
step:830/2285 train_time:50000ms step_avg:60.24ms
step:831/2285 train_time:50062ms step_avg:60.24ms
step:832/2285 train_time:50121ms step_avg:60.24ms
step:833/2285 train_time:50184ms step_avg:60.24ms
step:834/2285 train_time:50243ms step_avg:60.24ms
step:835/2285 train_time:50305ms step_avg:60.25ms
step:836/2285 train_time:50365ms step_avg:60.24ms
step:837/2285 train_time:50426ms step_avg:60.25ms
step:838/2285 train_time:50486ms step_avg:60.25ms
step:839/2285 train_time:50547ms step_avg:60.25ms
step:840/2285 train_time:50607ms step_avg:60.25ms
step:841/2285 train_time:50669ms step_avg:60.25ms
step:842/2285 train_time:50729ms step_avg:60.25ms
step:843/2285 train_time:50791ms step_avg:60.25ms
step:844/2285 train_time:50851ms step_avg:60.25ms
step:845/2285 train_time:50912ms step_avg:60.25ms
step:846/2285 train_time:50972ms step_avg:60.25ms
step:847/2285 train_time:51034ms step_avg:60.25ms
step:848/2285 train_time:51094ms step_avg:60.25ms
step:849/2285 train_time:51157ms step_avg:60.26ms
step:850/2285 train_time:51217ms step_avg:60.26ms
step:851/2285 train_time:51279ms step_avg:60.26ms
step:852/2285 train_time:51338ms step_avg:60.26ms
step:853/2285 train_time:51400ms step_avg:60.26ms
step:854/2285 train_time:51460ms step_avg:60.26ms
step:855/2285 train_time:51521ms step_avg:60.26ms
step:856/2285 train_time:51581ms step_avg:60.26ms
step:857/2285 train_time:51643ms step_avg:60.26ms
step:858/2285 train_time:51702ms step_avg:60.26ms
step:859/2285 train_time:51764ms step_avg:60.26ms
step:860/2285 train_time:51824ms step_avg:60.26ms
step:861/2285 train_time:51886ms step_avg:60.26ms
step:862/2285 train_time:51945ms step_avg:60.26ms
step:863/2285 train_time:52007ms step_avg:60.26ms
step:864/2285 train_time:52066ms step_avg:60.26ms
step:865/2285 train_time:52128ms step_avg:60.26ms
step:866/2285 train_time:52188ms step_avg:60.26ms
step:867/2285 train_time:52250ms step_avg:60.27ms
step:868/2285 train_time:52310ms step_avg:60.26ms
step:869/2285 train_time:52372ms step_avg:60.27ms
step:870/2285 train_time:52431ms step_avg:60.27ms
step:871/2285 train_time:52494ms step_avg:60.27ms
step:872/2285 train_time:52554ms step_avg:60.27ms
step:873/2285 train_time:52616ms step_avg:60.27ms
step:874/2285 train_time:52676ms step_avg:60.27ms
step:875/2285 train_time:52738ms step_avg:60.27ms
step:876/2285 train_time:52798ms step_avg:60.27ms
step:877/2285 train_time:52860ms step_avg:60.27ms
step:878/2285 train_time:52919ms step_avg:60.27ms
step:879/2285 train_time:52980ms step_avg:60.27ms
step:880/2285 train_time:53040ms step_avg:60.27ms
step:881/2285 train_time:53102ms step_avg:60.27ms
step:882/2285 train_time:53161ms step_avg:60.27ms
step:883/2285 train_time:53223ms step_avg:60.28ms
step:884/2285 train_time:53283ms step_avg:60.27ms
step:885/2285 train_time:53345ms step_avg:60.28ms
step:886/2285 train_time:53405ms step_avg:60.28ms
step:887/2285 train_time:53466ms step_avg:60.28ms
step:888/2285 train_time:53526ms step_avg:60.28ms
step:889/2285 train_time:53588ms step_avg:60.28ms
step:890/2285 train_time:53647ms step_avg:60.28ms
step:891/2285 train_time:53709ms step_avg:60.28ms
step:892/2285 train_time:53769ms step_avg:60.28ms
step:893/2285 train_time:53831ms step_avg:60.28ms
step:894/2285 train_time:53890ms step_avg:60.28ms
step:895/2285 train_time:53953ms step_avg:60.28ms
step:896/2285 train_time:54013ms step_avg:60.28ms
step:897/2285 train_time:54074ms step_avg:60.28ms
step:898/2285 train_time:54134ms step_avg:60.28ms
step:899/2285 train_time:54196ms step_avg:60.28ms
step:900/2285 train_time:54255ms step_avg:60.28ms
step:901/2285 train_time:54317ms step_avg:60.29ms
step:902/2285 train_time:54377ms step_avg:60.28ms
step:903/2285 train_time:54439ms step_avg:60.29ms
step:904/2285 train_time:54499ms step_avg:60.29ms
step:905/2285 train_time:54560ms step_avg:60.29ms
step:906/2285 train_time:54620ms step_avg:60.29ms
step:907/2285 train_time:54681ms step_avg:60.29ms
step:908/2285 train_time:54741ms step_avg:60.29ms
step:909/2285 train_time:54803ms step_avg:60.29ms
step:910/2285 train_time:54863ms step_avg:60.29ms
step:911/2285 train_time:54924ms step_avg:60.29ms
step:912/2285 train_time:54984ms step_avg:60.29ms
step:913/2285 train_time:55045ms step_avg:60.29ms
step:914/2285 train_time:55104ms step_avg:60.29ms
step:915/2285 train_time:55166ms step_avg:60.29ms
step:916/2285 train_time:55226ms step_avg:60.29ms
step:917/2285 train_time:55288ms step_avg:60.29ms
step:918/2285 train_time:55347ms step_avg:60.29ms
step:919/2285 train_time:55409ms step_avg:60.29ms
step:920/2285 train_time:55469ms step_avg:60.29ms
step:921/2285 train_time:55531ms step_avg:60.29ms
step:922/2285 train_time:55591ms step_avg:60.29ms
step:923/2285 train_time:55653ms step_avg:60.30ms
step:924/2285 train_time:55714ms step_avg:60.30ms
step:925/2285 train_time:55775ms step_avg:60.30ms
step:926/2285 train_time:55835ms step_avg:60.30ms
step:927/2285 train_time:55897ms step_avg:60.30ms
step:928/2285 train_time:55956ms step_avg:60.30ms
step:929/2285 train_time:56019ms step_avg:60.30ms
step:930/2285 train_time:56078ms step_avg:60.30ms
step:931/2285 train_time:56140ms step_avg:60.30ms
step:932/2285 train_time:56199ms step_avg:60.30ms
step:933/2285 train_time:56261ms step_avg:60.30ms
step:934/2285 train_time:56321ms step_avg:60.30ms
step:935/2285 train_time:56383ms step_avg:60.30ms
step:936/2285 train_time:56442ms step_avg:60.30ms
step:937/2285 train_time:56505ms step_avg:60.30ms
step:938/2285 train_time:56564ms step_avg:60.30ms
step:939/2285 train_time:56627ms step_avg:60.31ms
step:940/2285 train_time:56686ms step_avg:60.30ms
step:941/2285 train_time:56748ms step_avg:60.31ms
step:942/2285 train_time:56808ms step_avg:60.31ms
step:943/2285 train_time:56869ms step_avg:60.31ms
step:944/2285 train_time:56929ms step_avg:60.31ms
step:945/2285 train_time:56990ms step_avg:60.31ms
step:946/2285 train_time:57051ms step_avg:60.31ms
step:947/2285 train_time:57113ms step_avg:60.31ms
step:948/2285 train_time:57173ms step_avg:60.31ms
step:949/2285 train_time:57235ms step_avg:60.31ms
step:950/2285 train_time:57294ms step_avg:60.31ms
step:951/2285 train_time:57356ms step_avg:60.31ms
step:952/2285 train_time:57416ms step_avg:60.31ms
step:953/2285 train_time:57478ms step_avg:60.31ms
step:954/2285 train_time:57538ms step_avg:60.31ms
step:955/2285 train_time:57600ms step_avg:60.31ms
step:956/2285 train_time:57659ms step_avg:60.31ms
step:957/2285 train_time:57722ms step_avg:60.32ms
step:958/2285 train_time:57781ms step_avg:60.31ms
step:959/2285 train_time:57843ms step_avg:60.32ms
step:960/2285 train_time:57903ms step_avg:60.32ms
step:961/2285 train_time:57965ms step_avg:60.32ms
step:962/2285 train_time:58024ms step_avg:60.32ms
step:963/2285 train_time:58086ms step_avg:60.32ms
step:964/2285 train_time:58145ms step_avg:60.32ms
step:965/2285 train_time:58207ms step_avg:60.32ms
step:966/2285 train_time:58267ms step_avg:60.32ms
step:967/2285 train_time:58329ms step_avg:60.32ms
step:968/2285 train_time:58389ms step_avg:60.32ms
step:969/2285 train_time:58451ms step_avg:60.32ms
step:970/2285 train_time:58511ms step_avg:60.32ms
step:971/2285 train_time:58573ms step_avg:60.32ms
step:972/2285 train_time:58632ms step_avg:60.32ms
step:973/2285 train_time:58694ms step_avg:60.32ms
step:974/2285 train_time:58754ms step_avg:60.32ms
step:975/2285 train_time:58817ms step_avg:60.32ms
step:976/2285 train_time:58877ms step_avg:60.32ms
step:977/2285 train_time:58938ms step_avg:60.33ms
step:978/2285 train_time:58998ms step_avg:60.33ms
step:979/2285 train_time:59059ms step_avg:60.33ms
step:980/2285 train_time:59119ms step_avg:60.33ms
step:981/2285 train_time:59181ms step_avg:60.33ms
step:982/2285 train_time:59241ms step_avg:60.33ms
step:983/2285 train_time:59303ms step_avg:60.33ms
step:984/2285 train_time:59363ms step_avg:60.33ms
step:985/2285 train_time:59425ms step_avg:60.33ms
step:986/2285 train_time:59484ms step_avg:60.33ms
step:987/2285 train_time:59546ms step_avg:60.33ms
step:988/2285 train_time:59605ms step_avg:60.33ms
step:989/2285 train_time:59667ms step_avg:60.33ms
step:990/2285 train_time:59727ms step_avg:60.33ms
step:991/2285 train_time:59789ms step_avg:60.33ms
step:992/2285 train_time:59848ms step_avg:60.33ms
step:993/2285 train_time:59910ms step_avg:60.33ms
step:994/2285 train_time:59970ms step_avg:60.33ms
step:995/2285 train_time:60032ms step_avg:60.33ms
step:996/2285 train_time:60091ms step_avg:60.33ms
step:997/2285 train_time:60154ms step_avg:60.33ms
step:998/2285 train_time:60214ms step_avg:60.33ms
step:999/2285 train_time:60275ms step_avg:60.34ms
step:1000/2285 train_time:60335ms step_avg:60.33ms
step:1000/2285 val_loss:3.5674 train_time:60398ms step_avg:60.40ms
step:1001/2285 train_time:60418ms step_avg:60.36ms
step:1002/2285 train_time:60458ms step_avg:60.34ms
step:1003/2285 train_time:60519ms step_avg:60.34ms
step:1004/2285 train_time:60578ms step_avg:60.34ms
step:1005/2285 train_time:60641ms step_avg:60.34ms
step:1006/2285 train_time:60701ms step_avg:60.34ms
step:1007/2285 train_time:60762ms step_avg:60.34ms
step:1008/2285 train_time:60820ms step_avg:60.34ms
step:1009/2285 train_time:60881ms step_avg:60.34ms
step:1010/2285 train_time:60939ms step_avg:60.34ms
step:1011/2285 train_time:61000ms step_avg:60.34ms
step:1012/2285 train_time:61058ms step_avg:60.33ms
step:1013/2285 train_time:61121ms step_avg:60.34ms
step:1014/2285 train_time:61180ms step_avg:60.34ms
step:1015/2285 train_time:61241ms step_avg:60.34ms
step:1016/2285 train_time:61302ms step_avg:60.34ms
step:1017/2285 train_time:61368ms step_avg:60.34ms
step:1018/2285 train_time:61429ms step_avg:60.34ms
step:1019/2285 train_time:61491ms step_avg:60.34ms
step:1020/2285 train_time:61551ms step_avg:60.34ms
step:1021/2285 train_time:61613ms step_avg:60.35ms
step:1022/2285 train_time:61672ms step_avg:60.34ms
step:1023/2285 train_time:61734ms step_avg:60.35ms
step:1024/2285 train_time:61793ms step_avg:60.34ms
step:1025/2285 train_time:61855ms step_avg:60.35ms
step:1026/2285 train_time:61914ms step_avg:60.34ms
step:1027/2285 train_time:61975ms step_avg:60.35ms
step:1028/2285 train_time:62034ms step_avg:60.34ms
step:1029/2285 train_time:62096ms step_avg:60.35ms
step:1030/2285 train_time:62156ms step_avg:60.35ms
step:1031/2285 train_time:62217ms step_avg:60.35ms
step:1032/2285 train_time:62278ms step_avg:60.35ms
step:1033/2285 train_time:62340ms step_avg:60.35ms
step:1034/2285 train_time:62400ms step_avg:60.35ms
step:1035/2285 train_time:62462ms step_avg:60.35ms
step:1036/2285 train_time:62522ms step_avg:60.35ms
step:1037/2285 train_time:62584ms step_avg:60.35ms
step:1038/2285 train_time:62643ms step_avg:60.35ms
step:1039/2285 train_time:62706ms step_avg:60.35ms
step:1040/2285 train_time:62766ms step_avg:60.35ms
step:1041/2285 train_time:62827ms step_avg:60.35ms
step:1042/2285 train_time:62887ms step_avg:60.35ms
step:1043/2285 train_time:62949ms step_avg:60.35ms
step:1044/2285 train_time:63008ms step_avg:60.35ms
step:1045/2285 train_time:63070ms step_avg:60.35ms
step:1046/2285 train_time:63130ms step_avg:60.35ms
step:1047/2285 train_time:63192ms step_avg:60.35ms
step:1048/2285 train_time:63252ms step_avg:60.35ms
step:1049/2285 train_time:63314ms step_avg:60.36ms
step:1050/2285 train_time:63374ms step_avg:60.36ms
step:1051/2285 train_time:63436ms step_avg:60.36ms
step:1052/2285 train_time:63495ms step_avg:60.36ms
step:1053/2285 train_time:63557ms step_avg:60.36ms
step:1054/2285 train_time:63617ms step_avg:60.36ms
step:1055/2285 train_time:63679ms step_avg:60.36ms
step:1056/2285 train_time:63738ms step_avg:60.36ms
step:1057/2285 train_time:63800ms step_avg:60.36ms
step:1058/2285 train_time:63860ms step_avg:60.36ms
step:1059/2285 train_time:63922ms step_avg:60.36ms
step:1060/2285 train_time:63981ms step_avg:60.36ms
step:1061/2285 train_time:64043ms step_avg:60.36ms
step:1062/2285 train_time:64103ms step_avg:60.36ms
step:1063/2285 train_time:64165ms step_avg:60.36ms
step:1064/2285 train_time:64224ms step_avg:60.36ms
step:1065/2285 train_time:64286ms step_avg:60.36ms
step:1066/2285 train_time:64347ms step_avg:60.36ms
step:1067/2285 train_time:64409ms step_avg:60.36ms
step:1068/2285 train_time:64468ms step_avg:60.36ms
step:1069/2285 train_time:64530ms step_avg:60.37ms
step:1070/2285 train_time:64590ms step_avg:60.36ms
step:1071/2285 train_time:64652ms step_avg:60.37ms
step:1072/2285 train_time:64712ms step_avg:60.37ms
step:1073/2285 train_time:64774ms step_avg:60.37ms
step:1074/2285 train_time:64833ms step_avg:60.37ms
step:1075/2285 train_time:64896ms step_avg:60.37ms
step:1076/2285 train_time:64955ms step_avg:60.37ms
step:1077/2285 train_time:65017ms step_avg:60.37ms
step:1078/2285 train_time:65076ms step_avg:60.37ms
step:1079/2285 train_time:65137ms step_avg:60.37ms
step:1080/2285 train_time:65197ms step_avg:60.37ms
step:1081/2285 train_time:65259ms step_avg:60.37ms
step:1082/2285 train_time:65319ms step_avg:60.37ms
step:1083/2285 train_time:65382ms step_avg:60.37ms
step:1084/2285 train_time:65441ms step_avg:60.37ms
step:1085/2285 train_time:65503ms step_avg:60.37ms
step:1086/2285 train_time:65562ms step_avg:60.37ms
step:1087/2285 train_time:65624ms step_avg:60.37ms
step:1088/2285 train_time:65683ms step_avg:60.37ms
step:1089/2285 train_time:65745ms step_avg:60.37ms
step:1090/2285 train_time:65805ms step_avg:60.37ms
step:1091/2285 train_time:65867ms step_avg:60.37ms
step:1092/2285 train_time:65927ms step_avg:60.37ms
step:1093/2285 train_time:65990ms step_avg:60.38ms
step:1094/2285 train_time:66051ms step_avg:60.38ms
step:1095/2285 train_time:66112ms step_avg:60.38ms
step:1096/2285 train_time:66172ms step_avg:60.38ms
step:1097/2285 train_time:66235ms step_avg:60.38ms
step:1098/2285 train_time:66294ms step_avg:60.38ms
step:1099/2285 train_time:66356ms step_avg:60.38ms
step:1100/2285 train_time:66416ms step_avg:60.38ms
step:1101/2285 train_time:66478ms step_avg:60.38ms
step:1102/2285 train_time:66537ms step_avg:60.38ms
step:1103/2285 train_time:66598ms step_avg:60.38ms
step:1104/2285 train_time:66658ms step_avg:60.38ms
step:1105/2285 train_time:66720ms step_avg:60.38ms
step:1106/2285 train_time:66779ms step_avg:60.38ms
step:1107/2285 train_time:66842ms step_avg:60.38ms
step:1108/2285 train_time:66901ms step_avg:60.38ms
step:1109/2285 train_time:66963ms step_avg:60.38ms
step:1110/2285 train_time:67023ms step_avg:60.38ms
step:1111/2285 train_time:67084ms step_avg:60.38ms
step:1112/2285 train_time:67144ms step_avg:60.38ms
step:1113/2285 train_time:67206ms step_avg:60.38ms
step:1114/2285 train_time:67265ms step_avg:60.38ms
step:1115/2285 train_time:67327ms step_avg:60.38ms
step:1116/2285 train_time:67386ms step_avg:60.38ms
step:1117/2285 train_time:67449ms step_avg:60.38ms
step:1118/2285 train_time:67509ms step_avg:60.38ms
step:1119/2285 train_time:67571ms step_avg:60.38ms
step:1120/2285 train_time:67631ms step_avg:60.39ms
step:1121/2285 train_time:67693ms step_avg:60.39ms
step:1122/2285 train_time:67753ms step_avg:60.39ms
step:1123/2285 train_time:67815ms step_avg:60.39ms
step:1124/2285 train_time:67874ms step_avg:60.39ms
step:1125/2285 train_time:67937ms step_avg:60.39ms
step:1126/2285 train_time:67996ms step_avg:60.39ms
step:1127/2285 train_time:68058ms step_avg:60.39ms
step:1128/2285 train_time:68117ms step_avg:60.39ms
step:1129/2285 train_time:68179ms step_avg:60.39ms
step:1130/2285 train_time:68239ms step_avg:60.39ms
step:1131/2285 train_time:68301ms step_avg:60.39ms
step:1132/2285 train_time:68361ms step_avg:60.39ms
step:1133/2285 train_time:68422ms step_avg:60.39ms
step:1134/2285 train_time:68481ms step_avg:60.39ms
step:1135/2285 train_time:68543ms step_avg:60.39ms
step:1136/2285 train_time:68603ms step_avg:60.39ms
step:1137/2285 train_time:68664ms step_avg:60.39ms
step:1138/2285 train_time:68723ms step_avg:60.39ms
step:1139/2285 train_time:68785ms step_avg:60.39ms
step:1140/2285 train_time:68846ms step_avg:60.39ms
step:1141/2285 train_time:68908ms step_avg:60.39ms
step:1142/2285 train_time:68967ms step_avg:60.39ms
step:1143/2285 train_time:69029ms step_avg:60.39ms
step:1144/2285 train_time:69089ms step_avg:60.39ms
step:1145/2285 train_time:69152ms step_avg:60.39ms
step:1146/2285 train_time:69211ms step_avg:60.39ms
step:1147/2285 train_time:69274ms step_avg:60.40ms
step:1148/2285 train_time:69334ms step_avg:60.40ms
step:1149/2285 train_time:69396ms step_avg:60.40ms
step:1150/2285 train_time:69456ms step_avg:60.40ms
step:1151/2285 train_time:69517ms step_avg:60.40ms
step:1152/2285 train_time:69577ms step_avg:60.40ms
step:1153/2285 train_time:69639ms step_avg:60.40ms
step:1154/2285 train_time:69699ms step_avg:60.40ms
step:1155/2285 train_time:69761ms step_avg:60.40ms
step:1156/2285 train_time:69821ms step_avg:60.40ms
step:1157/2285 train_time:69883ms step_avg:60.40ms
step:1158/2285 train_time:69942ms step_avg:60.40ms
step:1159/2285 train_time:70004ms step_avg:60.40ms
step:1160/2285 train_time:70064ms step_avg:60.40ms
step:1161/2285 train_time:70126ms step_avg:60.40ms
step:1162/2285 train_time:70186ms step_avg:60.40ms
step:1163/2285 train_time:70248ms step_avg:60.40ms
step:1164/2285 train_time:70309ms step_avg:60.40ms
step:1165/2285 train_time:70371ms step_avg:60.40ms
step:1166/2285 train_time:70432ms step_avg:60.40ms
step:1167/2285 train_time:70495ms step_avg:60.41ms
step:1168/2285 train_time:70555ms step_avg:60.41ms
step:1169/2285 train_time:70617ms step_avg:60.41ms
step:1170/2285 train_time:70676ms step_avg:60.41ms
step:1171/2285 train_time:70738ms step_avg:60.41ms
step:1172/2285 train_time:70797ms step_avg:60.41ms
step:1173/2285 train_time:70859ms step_avg:60.41ms
step:1174/2285 train_time:70919ms step_avg:60.41ms
step:1175/2285 train_time:70982ms step_avg:60.41ms
step:1176/2285 train_time:71041ms step_avg:60.41ms
step:1177/2285 train_time:71103ms step_avg:60.41ms
step:1178/2285 train_time:71162ms step_avg:60.41ms
step:1179/2285 train_time:71225ms step_avg:60.41ms
step:1180/2285 train_time:71284ms step_avg:60.41ms
step:1181/2285 train_time:71347ms step_avg:60.41ms
step:1182/2285 train_time:71407ms step_avg:60.41ms
step:1183/2285 train_time:71469ms step_avg:60.41ms
step:1184/2285 train_time:71529ms step_avg:60.41ms
step:1185/2285 train_time:71592ms step_avg:60.42ms
step:1186/2285 train_time:71652ms step_avg:60.41ms
step:1187/2285 train_time:71714ms step_avg:60.42ms
step:1188/2285 train_time:71774ms step_avg:60.42ms
step:1189/2285 train_time:71836ms step_avg:60.42ms
step:1190/2285 train_time:71896ms step_avg:60.42ms
step:1191/2285 train_time:71958ms step_avg:60.42ms
step:1192/2285 train_time:72018ms step_avg:60.42ms
step:1193/2285 train_time:72080ms step_avg:60.42ms
step:1194/2285 train_time:72139ms step_avg:60.42ms
step:1195/2285 train_time:72202ms step_avg:60.42ms
step:1196/2285 train_time:72261ms step_avg:60.42ms
step:1197/2285 train_time:72323ms step_avg:60.42ms
step:1198/2285 train_time:72383ms step_avg:60.42ms
step:1199/2285 train_time:72445ms step_avg:60.42ms
step:1200/2285 train_time:72504ms step_avg:60.42ms
step:1201/2285 train_time:72567ms step_avg:60.42ms
step:1202/2285 train_time:72627ms step_avg:60.42ms
step:1203/2285 train_time:72689ms step_avg:60.42ms
step:1204/2285 train_time:72748ms step_avg:60.42ms
step:1205/2285 train_time:72810ms step_avg:60.42ms
step:1206/2285 train_time:72872ms step_avg:60.42ms
step:1207/2285 train_time:72935ms step_avg:60.43ms
step:1208/2285 train_time:72994ms step_avg:60.43ms
step:1209/2285 train_time:73056ms step_avg:60.43ms
step:1210/2285 train_time:73116ms step_avg:60.43ms
step:1211/2285 train_time:73178ms step_avg:60.43ms
step:1212/2285 train_time:73238ms step_avg:60.43ms
step:1213/2285 train_time:73301ms step_avg:60.43ms
step:1214/2285 train_time:73361ms step_avg:60.43ms
step:1215/2285 train_time:73423ms step_avg:60.43ms
step:1216/2285 train_time:73482ms step_avg:60.43ms
step:1217/2285 train_time:73544ms step_avg:60.43ms
step:1218/2285 train_time:73604ms step_avg:60.43ms
step:1219/2285 train_time:73667ms step_avg:60.43ms
step:1220/2285 train_time:73726ms step_avg:60.43ms
step:1221/2285 train_time:73788ms step_avg:60.43ms
step:1222/2285 train_time:73849ms step_avg:60.43ms
step:1223/2285 train_time:73912ms step_avg:60.43ms
step:1224/2285 train_time:73972ms step_avg:60.43ms
step:1225/2285 train_time:74034ms step_avg:60.44ms
step:1226/2285 train_time:74094ms step_avg:60.44ms
step:1227/2285 train_time:74157ms step_avg:60.44ms
step:1228/2285 train_time:74217ms step_avg:60.44ms
step:1229/2285 train_time:74279ms step_avg:60.44ms
step:1230/2285 train_time:74339ms step_avg:60.44ms
step:1231/2285 train_time:74402ms step_avg:60.44ms
step:1232/2285 train_time:74462ms step_avg:60.44ms
step:1233/2285 train_time:74524ms step_avg:60.44ms
step:1234/2285 train_time:74583ms step_avg:60.44ms
step:1235/2285 train_time:74645ms step_avg:60.44ms
step:1236/2285 train_time:74705ms step_avg:60.44ms
step:1237/2285 train_time:74768ms step_avg:60.44ms
step:1238/2285 train_time:74828ms step_avg:60.44ms
step:1239/2285 train_time:74890ms step_avg:60.44ms
step:1240/2285 train_time:74950ms step_avg:60.44ms
step:1241/2285 train_time:75012ms step_avg:60.44ms
step:1242/2285 train_time:75072ms step_avg:60.44ms
step:1243/2285 train_time:75135ms step_avg:60.45ms
step:1244/2285 train_time:75195ms step_avg:60.45ms
step:1245/2285 train_time:75257ms step_avg:60.45ms
step:1246/2285 train_time:75316ms step_avg:60.45ms
step:1247/2285 train_time:75378ms step_avg:60.45ms
step:1248/2285 train_time:75438ms step_avg:60.45ms
step:1249/2285 train_time:75499ms step_avg:60.45ms
step:1250/2285 train_time:75560ms step_avg:60.45ms
step:1250/2285 val_loss:3.4957 train_time:75624ms step_avg:60.50ms
step:1251/2285 train_time:75642ms step_avg:60.47ms
step:1252/2285 train_time:75684ms step_avg:60.45ms
step:1253/2285 train_time:75747ms step_avg:60.45ms
step:1254/2285 train_time:75806ms step_avg:60.45ms
step:1255/2285 train_time:75869ms step_avg:60.45ms
step:1256/2285 train_time:75928ms step_avg:60.45ms
step:1257/2285 train_time:75990ms step_avg:60.45ms
step:1258/2285 train_time:76049ms step_avg:60.45ms
step:1259/2285 train_time:76110ms step_avg:60.45ms
step:1260/2285 train_time:76169ms step_avg:60.45ms
step:1261/2285 train_time:76230ms step_avg:60.45ms
step:1262/2285 train_time:76289ms step_avg:60.45ms
step:1263/2285 train_time:76350ms step_avg:60.45ms
step:1264/2285 train_time:76409ms step_avg:60.45ms
step:1265/2285 train_time:76470ms step_avg:60.45ms
step:1266/2285 train_time:76536ms step_avg:60.45ms
step:1267/2285 train_time:76602ms step_avg:60.46ms
step:1268/2285 train_time:76663ms step_avg:60.46ms
step:1269/2285 train_time:76726ms step_avg:60.46ms
step:1270/2285 train_time:76785ms step_avg:60.46ms
step:1271/2285 train_time:76848ms step_avg:60.46ms
step:1272/2285 train_time:76907ms step_avg:60.46ms
step:1273/2285 train_time:76968ms step_avg:60.46ms
step:1274/2285 train_time:77028ms step_avg:60.46ms
step:1275/2285 train_time:77089ms step_avg:60.46ms
step:1276/2285 train_time:77148ms step_avg:60.46ms
step:1277/2285 train_time:77210ms step_avg:60.46ms
step:1278/2285 train_time:77268ms step_avg:60.46ms
step:1279/2285 train_time:77330ms step_avg:60.46ms
step:1280/2285 train_time:77389ms step_avg:60.46ms
step:1281/2285 train_time:77451ms step_avg:60.46ms
step:1282/2285 train_time:77513ms step_avg:60.46ms
step:1283/2285 train_time:77578ms step_avg:60.47ms
step:1284/2285 train_time:77638ms step_avg:60.47ms
step:1285/2285 train_time:77700ms step_avg:60.47ms
step:1286/2285 train_time:77760ms step_avg:60.47ms
step:1287/2285 train_time:77822ms step_avg:60.47ms
step:1288/2285 train_time:77882ms step_avg:60.47ms
step:1289/2285 train_time:77944ms step_avg:60.47ms
step:1290/2285 train_time:78003ms step_avg:60.47ms
step:1291/2285 train_time:78065ms step_avg:60.47ms
step:1292/2285 train_time:78124ms step_avg:60.47ms
step:1293/2285 train_time:78186ms step_avg:60.47ms
step:1294/2285 train_time:78246ms step_avg:60.47ms
step:1295/2285 train_time:78307ms step_avg:60.47ms
step:1296/2285 train_time:78366ms step_avg:60.47ms
step:1297/2285 train_time:78429ms step_avg:60.47ms
step:1298/2285 train_time:78489ms step_avg:60.47ms
step:1299/2285 train_time:78553ms step_avg:60.47ms
step:1300/2285 train_time:78613ms step_avg:60.47ms
step:1301/2285 train_time:78677ms step_avg:60.47ms
step:1302/2285 train_time:78737ms step_avg:60.47ms
step:1303/2285 train_time:78799ms step_avg:60.48ms
step:1304/2285 train_time:78859ms step_avg:60.47ms
step:1305/2285 train_time:78921ms step_avg:60.48ms
step:1306/2285 train_time:78980ms step_avg:60.48ms
step:1307/2285 train_time:79043ms step_avg:60.48ms
step:1308/2285 train_time:79102ms step_avg:60.48ms
step:1309/2285 train_time:79164ms step_avg:60.48ms
step:1310/2285 train_time:79223ms step_avg:60.48ms
step:1311/2285 train_time:79285ms step_avg:60.48ms
step:1312/2285 train_time:79345ms step_avg:60.48ms
step:1313/2285 train_time:79408ms step_avg:60.48ms
step:1314/2285 train_time:79468ms step_avg:60.48ms
step:1315/2285 train_time:79530ms step_avg:60.48ms
step:1316/2285 train_time:79590ms step_avg:60.48ms
step:1317/2285 train_time:79653ms step_avg:60.48ms
step:1318/2285 train_time:79714ms step_avg:60.48ms
step:1319/2285 train_time:79777ms step_avg:60.48ms
step:1320/2285 train_time:79837ms step_avg:60.48ms
step:1321/2285 train_time:79899ms step_avg:60.48ms
step:1322/2285 train_time:79958ms step_avg:60.48ms
step:1323/2285 train_time:80020ms step_avg:60.48ms
step:1324/2285 train_time:80080ms step_avg:60.48ms
step:1325/2285 train_time:80142ms step_avg:60.48ms
step:1326/2285 train_time:80202ms step_avg:60.48ms
step:1327/2285 train_time:80264ms step_avg:60.49ms
step:1328/2285 train_time:80323ms step_avg:60.48ms
step:1329/2285 train_time:80385ms step_avg:60.49ms
step:1330/2285 train_time:80445ms step_avg:60.48ms
step:1331/2285 train_time:80507ms step_avg:60.49ms
step:1332/2285 train_time:80568ms step_avg:60.49ms
step:1333/2285 train_time:80630ms step_avg:60.49ms
step:1334/2285 train_time:80691ms step_avg:60.49ms
step:1335/2285 train_time:80753ms step_avg:60.49ms
step:1336/2285 train_time:80813ms step_avg:60.49ms
step:1337/2285 train_time:80875ms step_avg:60.49ms
step:1338/2285 train_time:80936ms step_avg:60.49ms
step:1339/2285 train_time:80998ms step_avg:60.49ms
step:1340/2285 train_time:81057ms step_avg:60.49ms
step:1341/2285 train_time:81120ms step_avg:60.49ms
step:1342/2285 train_time:81180ms step_avg:60.49ms
step:1343/2285 train_time:81242ms step_avg:60.49ms
step:1344/2285 train_time:81302ms step_avg:60.49ms
step:1345/2285 train_time:81364ms step_avg:60.49ms
step:1346/2285 train_time:81424ms step_avg:60.49ms
step:1347/2285 train_time:81485ms step_avg:60.49ms
step:1348/2285 train_time:81545ms step_avg:60.49ms
step:1349/2285 train_time:81608ms step_avg:60.49ms
step:1350/2285 train_time:81668ms step_avg:60.49ms
step:1351/2285 train_time:81730ms step_avg:60.50ms
step:1352/2285 train_time:81790ms step_avg:60.50ms
step:1353/2285 train_time:81853ms step_avg:60.50ms
step:1354/2285 train_time:81912ms step_avg:60.50ms
step:1355/2285 train_time:81975ms step_avg:60.50ms
step:1356/2285 train_time:82035ms step_avg:60.50ms
step:1357/2285 train_time:82098ms step_avg:60.50ms
step:1358/2285 train_time:82158ms step_avg:60.50ms
step:1359/2285 train_time:82220ms step_avg:60.50ms
step:1360/2285 train_time:82280ms step_avg:60.50ms
step:1361/2285 train_time:82342ms step_avg:60.50ms
step:1362/2285 train_time:82402ms step_avg:60.50ms
step:1363/2285 train_time:82464ms step_avg:60.50ms
step:1364/2285 train_time:82524ms step_avg:60.50ms
step:1365/2285 train_time:82585ms step_avg:60.50ms
step:1366/2285 train_time:82645ms step_avg:60.50ms
step:1367/2285 train_time:82707ms step_avg:60.50ms
step:1368/2285 train_time:82767ms step_avg:60.50ms
step:1369/2285 train_time:82829ms step_avg:60.50ms
step:1370/2285 train_time:82889ms step_avg:60.50ms
step:1371/2285 train_time:82952ms step_avg:60.50ms
step:1372/2285 train_time:83012ms step_avg:60.50ms
step:1373/2285 train_time:83075ms step_avg:60.51ms
step:1374/2285 train_time:83135ms step_avg:60.51ms
step:1375/2285 train_time:83197ms step_avg:60.51ms
step:1376/2285 train_time:83257ms step_avg:60.51ms
step:1377/2285 train_time:83319ms step_avg:60.51ms
step:1378/2285 train_time:83378ms step_avg:60.51ms
step:1379/2285 train_time:83440ms step_avg:60.51ms
step:1380/2285 train_time:83500ms step_avg:60.51ms
step:1381/2285 train_time:83563ms step_avg:60.51ms
step:1382/2285 train_time:83623ms step_avg:60.51ms
step:1383/2285 train_time:83685ms step_avg:60.51ms
step:1384/2285 train_time:83745ms step_avg:60.51ms
step:1385/2285 train_time:83807ms step_avg:60.51ms
step:1386/2285 train_time:83867ms step_avg:60.51ms
step:1387/2285 train_time:83929ms step_avg:60.51ms
step:1388/2285 train_time:83989ms step_avg:60.51ms
step:1389/2285 train_time:84052ms step_avg:60.51ms
step:1390/2285 train_time:84112ms step_avg:60.51ms
step:1391/2285 train_time:84175ms step_avg:60.51ms
step:1392/2285 train_time:84236ms step_avg:60.51ms
step:1393/2285 train_time:84298ms step_avg:60.52ms
step:1394/2285 train_time:84358ms step_avg:60.52ms
step:1395/2285 train_time:84420ms step_avg:60.52ms
step:1396/2285 train_time:84479ms step_avg:60.52ms
step:1397/2285 train_time:84541ms step_avg:60.52ms
step:1398/2285 train_time:84601ms step_avg:60.52ms
step:1399/2285 train_time:84664ms step_avg:60.52ms
step:1400/2285 train_time:84724ms step_avg:60.52ms
step:1401/2285 train_time:84785ms step_avg:60.52ms
step:1402/2285 train_time:84845ms step_avg:60.52ms
step:1403/2285 train_time:84907ms step_avg:60.52ms
step:1404/2285 train_time:84967ms step_avg:60.52ms
step:1405/2285 train_time:85029ms step_avg:60.52ms
step:1406/2285 train_time:85089ms step_avg:60.52ms
step:1407/2285 train_time:85152ms step_avg:60.52ms
step:1408/2285 train_time:85212ms step_avg:60.52ms
step:1409/2285 train_time:85274ms step_avg:60.52ms
step:1410/2285 train_time:85334ms step_avg:60.52ms
step:1411/2285 train_time:85397ms step_avg:60.52ms
step:1412/2285 train_time:85457ms step_avg:60.52ms
step:1413/2285 train_time:85519ms step_avg:60.52ms
step:1414/2285 train_time:85578ms step_avg:60.52ms
step:1415/2285 train_time:85640ms step_avg:60.52ms
step:1416/2285 train_time:85700ms step_avg:60.52ms
step:1417/2285 train_time:85762ms step_avg:60.52ms
step:1418/2285 train_time:85822ms step_avg:60.52ms
step:1419/2285 train_time:85884ms step_avg:60.52ms
step:1420/2285 train_time:85944ms step_avg:60.52ms
step:1421/2285 train_time:86007ms step_avg:60.53ms
step:1422/2285 train_time:86067ms step_avg:60.53ms
step:1423/2285 train_time:86129ms step_avg:60.53ms
step:1424/2285 train_time:86188ms step_avg:60.53ms
step:1425/2285 train_time:86250ms step_avg:60.53ms
step:1426/2285 train_time:86311ms step_avg:60.53ms
step:1427/2285 train_time:86373ms step_avg:60.53ms
step:1428/2285 train_time:86433ms step_avg:60.53ms
step:1429/2285 train_time:86496ms step_avg:60.53ms
step:1430/2285 train_time:86556ms step_avg:60.53ms
step:1431/2285 train_time:86618ms step_avg:60.53ms
step:1432/2285 train_time:86677ms step_avg:60.53ms
step:1433/2285 train_time:86739ms step_avg:60.53ms
step:1434/2285 train_time:86799ms step_avg:60.53ms
step:1435/2285 train_time:86863ms step_avg:60.53ms
step:1436/2285 train_time:86922ms step_avg:60.53ms
step:1437/2285 train_time:86984ms step_avg:60.53ms
step:1438/2285 train_time:87044ms step_avg:60.53ms
step:1439/2285 train_time:87106ms step_avg:60.53ms
step:1440/2285 train_time:87165ms step_avg:60.53ms
step:1441/2285 train_time:87228ms step_avg:60.53ms
step:1442/2285 train_time:87288ms step_avg:60.53ms
step:1443/2285 train_time:87350ms step_avg:60.53ms
step:1444/2285 train_time:87410ms step_avg:60.53ms
step:1445/2285 train_time:87473ms step_avg:60.54ms
step:1446/2285 train_time:87534ms step_avg:60.54ms
step:1447/2285 train_time:87597ms step_avg:60.54ms
step:1448/2285 train_time:87657ms step_avg:60.54ms
step:1449/2285 train_time:87719ms step_avg:60.54ms
step:1450/2285 train_time:87778ms step_avg:60.54ms
step:1451/2285 train_time:87840ms step_avg:60.54ms
step:1452/2285 train_time:87900ms step_avg:60.54ms
step:1453/2285 train_time:87963ms step_avg:60.54ms
step:1454/2285 train_time:88022ms step_avg:60.54ms
step:1455/2285 train_time:88084ms step_avg:60.54ms
step:1456/2285 train_time:88144ms step_avg:60.54ms
step:1457/2285 train_time:88206ms step_avg:60.54ms
step:1458/2285 train_time:88266ms step_avg:60.54ms
step:1459/2285 train_time:88328ms step_avg:60.54ms
step:1460/2285 train_time:88388ms step_avg:60.54ms
step:1461/2285 train_time:88451ms step_avg:60.54ms
step:1462/2285 train_time:88511ms step_avg:60.54ms
step:1463/2285 train_time:88574ms step_avg:60.54ms
step:1464/2285 train_time:88635ms step_avg:60.54ms
step:1465/2285 train_time:88697ms step_avg:60.54ms
step:1466/2285 train_time:88757ms step_avg:60.54ms
step:1467/2285 train_time:88818ms step_avg:60.54ms
step:1468/2285 train_time:88878ms step_avg:60.54ms
step:1469/2285 train_time:88940ms step_avg:60.54ms
step:1470/2285 train_time:89000ms step_avg:60.54ms
step:1471/2285 train_time:89062ms step_avg:60.55ms
step:1472/2285 train_time:89122ms step_avg:60.54ms
step:1473/2285 train_time:89184ms step_avg:60.55ms
step:1474/2285 train_time:89244ms step_avg:60.55ms
step:1475/2285 train_time:89306ms step_avg:60.55ms
step:1476/2285 train_time:89365ms step_avg:60.55ms
step:1477/2285 train_time:89428ms step_avg:60.55ms
step:1478/2285 train_time:89488ms step_avg:60.55ms
step:1479/2285 train_time:89550ms step_avg:60.55ms
step:1480/2285 train_time:89611ms step_avg:60.55ms
step:1481/2285 train_time:89674ms step_avg:60.55ms
step:1482/2285 train_time:89735ms step_avg:60.55ms
step:1483/2285 train_time:89797ms step_avg:60.55ms
step:1484/2285 train_time:89857ms step_avg:60.55ms
step:1485/2285 train_time:89919ms step_avg:60.55ms
step:1486/2285 train_time:89978ms step_avg:60.55ms
step:1487/2285 train_time:90040ms step_avg:60.55ms
step:1488/2285 train_time:90100ms step_avg:60.55ms
step:1489/2285 train_time:90162ms step_avg:60.55ms
step:1490/2285 train_time:90222ms step_avg:60.55ms
step:1491/2285 train_time:90284ms step_avg:60.55ms
step:1492/2285 train_time:90344ms step_avg:60.55ms
step:1493/2285 train_time:90406ms step_avg:60.55ms
step:1494/2285 train_time:90466ms step_avg:60.55ms
step:1495/2285 train_time:90529ms step_avg:60.55ms
step:1496/2285 train_time:90589ms step_avg:60.55ms
step:1497/2285 train_time:90652ms step_avg:60.56ms
step:1498/2285 train_time:90712ms step_avg:60.56ms
step:1499/2285 train_time:90774ms step_avg:60.56ms
step:1500/2285 train_time:90834ms step_avg:60.56ms
step:1500/2285 val_loss:3.4280 train_time:90899ms step_avg:60.60ms
step:1501/2285 train_time:90917ms step_avg:60.57ms
step:1502/2285 train_time:90960ms step_avg:60.56ms
step:1503/2285 train_time:91026ms step_avg:60.56ms
step:1504/2285 train_time:91089ms step_avg:60.56ms
step:1505/2285 train_time:91151ms step_avg:60.57ms
step:1506/2285 train_time:91211ms step_avg:60.57ms
step:1507/2285 train_time:91272ms step_avg:60.57ms
step:1508/2285 train_time:91331ms step_avg:60.56ms
step:1509/2285 train_time:91393ms step_avg:60.57ms
step:1510/2285 train_time:91452ms step_avg:60.56ms
step:1511/2285 train_time:91514ms step_avg:60.57ms
step:1512/2285 train_time:91573ms step_avg:60.56ms
step:1513/2285 train_time:91635ms step_avg:60.57ms
step:1514/2285 train_time:91694ms step_avg:60.56ms
step:1515/2285 train_time:91756ms step_avg:60.56ms
step:1516/2285 train_time:91815ms step_avg:60.56ms
step:1517/2285 train_time:91878ms step_avg:60.57ms
step:1518/2285 train_time:91940ms step_avg:60.57ms
step:1519/2285 train_time:92004ms step_avg:60.57ms
step:1520/2285 train_time:92064ms step_avg:60.57ms
step:1521/2285 train_time:92128ms step_avg:60.57ms
step:1522/2285 train_time:92188ms step_avg:60.57ms
step:1523/2285 train_time:92250ms step_avg:60.57ms
step:1524/2285 train_time:92309ms step_avg:60.57ms
step:1525/2285 train_time:92372ms step_avg:60.57ms
step:1526/2285 train_time:92431ms step_avg:60.57ms
step:1527/2285 train_time:92493ms step_avg:60.57ms
step:1528/2285 train_time:92552ms step_avg:60.57ms
step:1529/2285 train_time:92614ms step_avg:60.57ms
step:1530/2285 train_time:92674ms step_avg:60.57ms
step:1531/2285 train_time:92735ms step_avg:60.57ms
step:1532/2285 train_time:92795ms step_avg:60.57ms
step:1533/2285 train_time:92858ms step_avg:60.57ms
step:1534/2285 train_time:92919ms step_avg:60.57ms
step:1535/2285 train_time:92982ms step_avg:60.57ms
step:1536/2285 train_time:93042ms step_avg:60.57ms
step:1537/2285 train_time:93105ms step_avg:60.58ms
step:1538/2285 train_time:93165ms step_avg:60.58ms
step:1539/2285 train_time:93227ms step_avg:60.58ms
step:1540/2285 train_time:93287ms step_avg:60.58ms
step:1541/2285 train_time:93349ms step_avg:60.58ms
step:1542/2285 train_time:93409ms step_avg:60.58ms
step:1543/2285 train_time:93472ms step_avg:60.58ms
step:1544/2285 train_time:93532ms step_avg:60.58ms
step:1545/2285 train_time:93594ms step_avg:60.58ms
step:1546/2285 train_time:93654ms step_avg:60.58ms
step:1547/2285 train_time:93716ms step_avg:60.58ms
step:1548/2285 train_time:93776ms step_avg:60.58ms
step:1549/2285 train_time:93838ms step_avg:60.58ms
step:1550/2285 train_time:93899ms step_avg:60.58ms
step:1551/2285 train_time:93962ms step_avg:60.58ms
step:1552/2285 train_time:94021ms step_avg:60.58ms
step:1553/2285 train_time:94084ms step_avg:60.58ms
step:1554/2285 train_time:94144ms step_avg:60.58ms
step:1555/2285 train_time:94207ms step_avg:60.58ms
step:1556/2285 train_time:94267ms step_avg:60.58ms
step:1557/2285 train_time:94330ms step_avg:60.58ms
step:1558/2285 train_time:94390ms step_avg:60.58ms
step:1559/2285 train_time:94452ms step_avg:60.58ms
step:1560/2285 train_time:94511ms step_avg:60.58ms
step:1561/2285 train_time:94574ms step_avg:60.59ms
step:1562/2285 train_time:94634ms step_avg:60.59ms
step:1563/2285 train_time:94696ms step_avg:60.59ms
step:1564/2285 train_time:94756ms step_avg:60.59ms
step:1565/2285 train_time:94818ms step_avg:60.59ms
step:1566/2285 train_time:94878ms step_avg:60.59ms
step:1567/2285 train_time:94940ms step_avg:60.59ms
step:1568/2285 train_time:95001ms step_avg:60.59ms
step:1569/2285 train_time:95064ms step_avg:60.59ms
step:1570/2285 train_time:95124ms step_avg:60.59ms
step:1571/2285 train_time:95186ms step_avg:60.59ms
step:1572/2285 train_time:95246ms step_avg:60.59ms
step:1573/2285 train_time:95309ms step_avg:60.59ms
step:1574/2285 train_time:95369ms step_avg:60.59ms
step:1575/2285 train_time:95431ms step_avg:60.59ms
step:1576/2285 train_time:95491ms step_avg:60.59ms
step:1577/2285 train_time:95553ms step_avg:60.59ms
step:1578/2285 train_time:95613ms step_avg:60.59ms
step:1579/2285 train_time:95676ms step_avg:60.59ms
step:1580/2285 train_time:95736ms step_avg:60.59ms
step:1581/2285 train_time:95798ms step_avg:60.59ms
step:1582/2285 train_time:95859ms step_avg:60.59ms
step:1583/2285 train_time:95921ms step_avg:60.59ms
step:1584/2285 train_time:95981ms step_avg:60.59ms
step:1585/2285 train_time:96043ms step_avg:60.60ms
step:1586/2285 train_time:96103ms step_avg:60.59ms
step:1587/2285 train_time:96166ms step_avg:60.60ms
step:1588/2285 train_time:96226ms step_avg:60.60ms
step:1589/2285 train_time:96288ms step_avg:60.60ms
step:1590/2285 train_time:96348ms step_avg:60.60ms
step:1591/2285 train_time:96411ms step_avg:60.60ms
step:1592/2285 train_time:96471ms step_avg:60.60ms
step:1593/2285 train_time:96534ms step_avg:60.60ms
step:1594/2285 train_time:96594ms step_avg:60.60ms
step:1595/2285 train_time:96656ms step_avg:60.60ms
step:1596/2285 train_time:96716ms step_avg:60.60ms
step:1597/2285 train_time:96779ms step_avg:60.60ms
step:1598/2285 train_time:96839ms step_avg:60.60ms
step:1599/2285 train_time:96901ms step_avg:60.60ms
step:1600/2285 train_time:96960ms step_avg:60.60ms
step:1601/2285 train_time:97023ms step_avg:60.60ms
step:1602/2285 train_time:97083ms step_avg:60.60ms
step:1603/2285 train_time:97145ms step_avg:60.60ms
step:1604/2285 train_time:97205ms step_avg:60.60ms
step:1605/2285 train_time:97267ms step_avg:60.60ms
step:1606/2285 train_time:97327ms step_avg:60.60ms
step:1607/2285 train_time:97390ms step_avg:60.60ms
step:1608/2285 train_time:97449ms step_avg:60.60ms
step:1609/2285 train_time:97512ms step_avg:60.60ms
step:1610/2285 train_time:97573ms step_avg:60.60ms
step:1611/2285 train_time:97636ms step_avg:60.61ms
step:1612/2285 train_time:97696ms step_avg:60.61ms
step:1613/2285 train_time:97759ms step_avg:60.61ms
step:1614/2285 train_time:97818ms step_avg:60.61ms
step:1615/2285 train_time:97881ms step_avg:60.61ms
step:1616/2285 train_time:97941ms step_avg:60.61ms
step:1617/2285 train_time:98003ms step_avg:60.61ms
step:1618/2285 train_time:98062ms step_avg:60.61ms
step:1619/2285 train_time:98124ms step_avg:60.61ms
step:1620/2285 train_time:98184ms step_avg:60.61ms
step:1621/2285 train_time:98246ms step_avg:60.61ms
step:1622/2285 train_time:98306ms step_avg:60.61ms
step:1623/2285 train_time:98368ms step_avg:60.61ms
step:1624/2285 train_time:98429ms step_avg:60.61ms
step:1625/2285 train_time:98492ms step_avg:60.61ms
step:1626/2285 train_time:98552ms step_avg:60.61ms
step:1627/2285 train_time:98614ms step_avg:60.61ms
step:1628/2285 train_time:98674ms step_avg:60.61ms
step:1629/2285 train_time:98737ms step_avg:60.61ms
step:1630/2285 train_time:98797ms step_avg:60.61ms
step:1631/2285 train_time:98859ms step_avg:60.61ms
step:1632/2285 train_time:98919ms step_avg:60.61ms
step:1633/2285 train_time:98982ms step_avg:60.61ms
step:1634/2285 train_time:99041ms step_avg:60.61ms
step:1635/2285 train_time:99103ms step_avg:60.61ms
step:1636/2285 train_time:99163ms step_avg:60.61ms
step:1637/2285 train_time:99225ms step_avg:60.61ms
step:1638/2285 train_time:99285ms step_avg:60.61ms
step:1639/2285 train_time:99347ms step_avg:60.61ms
step:1640/2285 train_time:99408ms step_avg:60.61ms
step:1641/2285 train_time:99470ms step_avg:60.62ms
step:1642/2285 train_time:99531ms step_avg:60.62ms
step:1643/2285 train_time:99594ms step_avg:60.62ms
step:1644/2285 train_time:99654ms step_avg:60.62ms
step:1645/2285 train_time:99716ms step_avg:60.62ms
step:1646/2285 train_time:99777ms step_avg:60.62ms
step:1647/2285 train_time:99839ms step_avg:60.62ms
step:1648/2285 train_time:99899ms step_avg:60.62ms
step:1649/2285 train_time:99961ms step_avg:60.62ms
step:1650/2285 train_time:100021ms step_avg:60.62ms
step:1651/2285 train_time:100084ms step_avg:60.62ms
step:1652/2285 train_time:100144ms step_avg:60.62ms
step:1653/2285 train_time:100206ms step_avg:60.62ms
step:1654/2285 train_time:100265ms step_avg:60.62ms
step:1655/2285 train_time:100327ms step_avg:60.62ms
step:1656/2285 train_time:100387ms step_avg:60.62ms
step:1657/2285 train_time:100450ms step_avg:60.62ms
step:1658/2285 train_time:100511ms step_avg:60.62ms
step:1659/2285 train_time:100574ms step_avg:60.62ms
step:1660/2285 train_time:100634ms step_avg:60.62ms
step:1661/2285 train_time:100696ms step_avg:60.62ms
step:1662/2285 train_time:100756ms step_avg:60.62ms
step:1663/2285 train_time:100818ms step_avg:60.62ms
step:1664/2285 train_time:100878ms step_avg:60.62ms
step:1665/2285 train_time:100941ms step_avg:60.63ms
step:1666/2285 train_time:101001ms step_avg:60.62ms
step:1667/2285 train_time:101063ms step_avg:60.63ms
step:1668/2285 train_time:101122ms step_avg:60.62ms
step:1669/2285 train_time:101184ms step_avg:60.63ms
step:1670/2285 train_time:101244ms step_avg:60.63ms
step:1671/2285 train_time:101306ms step_avg:60.63ms
step:1672/2285 train_time:101367ms step_avg:60.63ms
step:1673/2285 train_time:101429ms step_avg:60.63ms
step:1674/2285 train_time:101489ms step_avg:60.63ms
step:1675/2285 train_time:101553ms step_avg:60.63ms
step:1676/2285 train_time:101613ms step_avg:60.63ms
step:1677/2285 train_time:101676ms step_avg:60.63ms
step:1678/2285 train_time:101736ms step_avg:60.63ms
step:1679/2285 train_time:101798ms step_avg:60.63ms
step:1680/2285 train_time:101858ms step_avg:60.63ms
step:1681/2285 train_time:101921ms step_avg:60.63ms
step:1682/2285 train_time:101980ms step_avg:60.63ms
step:1683/2285 train_time:102043ms step_avg:60.63ms
step:1684/2285 train_time:102103ms step_avg:60.63ms
step:1685/2285 train_time:102165ms step_avg:60.63ms
step:1686/2285 train_time:102225ms step_avg:60.63ms
step:1687/2285 train_time:102287ms step_avg:60.63ms
step:1688/2285 train_time:102347ms step_avg:60.63ms
step:1689/2285 train_time:102410ms step_avg:60.63ms
step:1690/2285 train_time:102471ms step_avg:60.63ms
step:1691/2285 train_time:102533ms step_avg:60.63ms
step:1692/2285 train_time:102594ms step_avg:60.63ms
step:1693/2285 train_time:102655ms step_avg:60.64ms
step:1694/2285 train_time:102715ms step_avg:60.63ms
step:1695/2285 train_time:102778ms step_avg:60.64ms
step:1696/2285 train_time:102838ms step_avg:60.64ms
step:1697/2285 train_time:102900ms step_avg:60.64ms
step:1698/2285 train_time:102960ms step_avg:60.64ms
step:1699/2285 train_time:103022ms step_avg:60.64ms
step:1700/2285 train_time:103082ms step_avg:60.64ms
step:1701/2285 train_time:103144ms step_avg:60.64ms
step:1702/2285 train_time:103204ms step_avg:60.64ms
step:1703/2285 train_time:103266ms step_avg:60.64ms
step:1704/2285 train_time:103325ms step_avg:60.64ms
step:1705/2285 train_time:103388ms step_avg:60.64ms
step:1706/2285 train_time:103448ms step_avg:60.64ms
step:1707/2285 train_time:103511ms step_avg:60.64ms
step:1708/2285 train_time:103571ms step_avg:60.64ms
step:1709/2285 train_time:103634ms step_avg:60.64ms
step:1710/2285 train_time:103694ms step_avg:60.64ms
step:1711/2285 train_time:103756ms step_avg:60.64ms
step:1712/2285 train_time:103816ms step_avg:60.64ms
step:1713/2285 train_time:103879ms step_avg:60.64ms
step:1714/2285 train_time:103939ms step_avg:60.64ms
step:1715/2285 train_time:104002ms step_avg:60.64ms
step:1716/2285 train_time:104061ms step_avg:60.64ms
step:1717/2285 train_time:104124ms step_avg:60.64ms
step:1718/2285 train_time:104184ms step_avg:60.64ms
step:1719/2285 train_time:104246ms step_avg:60.64ms
step:1720/2285 train_time:104306ms step_avg:60.64ms
step:1721/2285 train_time:104368ms step_avg:60.64ms
step:1722/2285 train_time:104428ms step_avg:60.64ms
step:1723/2285 train_time:104491ms step_avg:60.64ms
step:1724/2285 train_time:104551ms step_avg:60.64ms
step:1725/2285 train_time:104615ms step_avg:60.65ms
step:1726/2285 train_time:104675ms step_avg:60.65ms
step:1727/2285 train_time:104737ms step_avg:60.65ms
step:1728/2285 train_time:104797ms step_avg:60.65ms
step:1729/2285 train_time:104859ms step_avg:60.65ms
step:1730/2285 train_time:104919ms step_avg:60.65ms
step:1731/2285 train_time:104981ms step_avg:60.65ms
step:1732/2285 train_time:105041ms step_avg:60.65ms
step:1733/2285 train_time:105103ms step_avg:60.65ms
step:1734/2285 train_time:105163ms step_avg:60.65ms
step:1735/2285 train_time:105225ms step_avg:60.65ms
step:1736/2285 train_time:105284ms step_avg:60.65ms
step:1737/2285 train_time:105347ms step_avg:60.65ms
step:1738/2285 train_time:105407ms step_avg:60.65ms
step:1739/2285 train_time:105470ms step_avg:60.65ms
step:1740/2285 train_time:105530ms step_avg:60.65ms
step:1741/2285 train_time:105593ms step_avg:60.65ms
step:1742/2285 train_time:105653ms step_avg:60.65ms
step:1743/2285 train_time:105716ms step_avg:60.65ms
step:1744/2285 train_time:105776ms step_avg:60.65ms
step:1745/2285 train_time:105838ms step_avg:60.65ms
step:1746/2285 train_time:105898ms step_avg:60.65ms
step:1747/2285 train_time:105960ms step_avg:60.65ms
step:1748/2285 train_time:106019ms step_avg:60.65ms
step:1749/2285 train_time:106082ms step_avg:60.65ms
step:1750/2285 train_time:106141ms step_avg:60.65ms
step:1750/2285 val_loss:3.3665 train_time:106205ms step_avg:60.69ms
step:1751/2285 train_time:106223ms step_avg:60.66ms
step:1752/2285 train_time:106266ms step_avg:60.65ms
step:1753/2285 train_time:106329ms step_avg:60.66ms
step:1754/2285 train_time:106389ms step_avg:60.66ms
step:1755/2285 train_time:106454ms step_avg:60.66ms
step:1756/2285 train_time:106515ms step_avg:60.66ms
step:1757/2285 train_time:106576ms step_avg:60.66ms
step:1758/2285 train_time:106635ms step_avg:60.66ms
step:1759/2285 train_time:106697ms step_avg:60.66ms
step:1760/2285 train_time:106756ms step_avg:60.66ms
step:1761/2285 train_time:106818ms step_avg:60.66ms
step:1762/2285 train_time:106877ms step_avg:60.66ms
step:1763/2285 train_time:106939ms step_avg:60.66ms
step:1764/2285 train_time:106998ms step_avg:60.66ms
step:1765/2285 train_time:107059ms step_avg:60.66ms
step:1766/2285 train_time:107119ms step_avg:60.66ms
step:1767/2285 train_time:107183ms step_avg:60.66ms
step:1768/2285 train_time:107244ms step_avg:60.66ms
step:1769/2285 train_time:107306ms step_avg:60.66ms
step:1770/2285 train_time:107367ms step_avg:60.66ms
step:1771/2285 train_time:107429ms step_avg:60.66ms
step:1772/2285 train_time:107489ms step_avg:60.66ms
step:1773/2285 train_time:107552ms step_avg:60.66ms
step:1774/2285 train_time:107612ms step_avg:60.66ms
step:1775/2285 train_time:107675ms step_avg:60.66ms
step:1776/2285 train_time:107735ms step_avg:60.66ms
step:1777/2285 train_time:107796ms step_avg:60.66ms
step:1778/2285 train_time:107855ms step_avg:60.66ms
step:1779/2285 train_time:107917ms step_avg:60.66ms
step:1780/2285 train_time:107976ms step_avg:60.66ms
step:1781/2285 train_time:108038ms step_avg:60.66ms
step:1782/2285 train_time:108098ms step_avg:60.66ms
step:1783/2285 train_time:108161ms step_avg:60.66ms
step:1784/2285 train_time:108221ms step_avg:60.66ms
step:1785/2285 train_time:108284ms step_avg:60.66ms
step:1786/2285 train_time:108345ms step_avg:60.66ms
step:1787/2285 train_time:108407ms step_avg:60.66ms
step:1788/2285 train_time:108467ms step_avg:60.66ms
step:1789/2285 train_time:108530ms step_avg:60.67ms
step:1790/2285 train_time:108590ms step_avg:60.66ms
step:1791/2285 train_time:108653ms step_avg:60.67ms
step:1792/2285 train_time:108713ms step_avg:60.67ms
step:1793/2285 train_time:108774ms step_avg:60.67ms
step:1794/2285 train_time:108834ms step_avg:60.67ms
step:1795/2285 train_time:108896ms step_avg:60.67ms
step:1796/2285 train_time:108955ms step_avg:60.67ms
step:1797/2285 train_time:109017ms step_avg:60.67ms
step:1798/2285 train_time:109077ms step_avg:60.67ms
step:1799/2285 train_time:109140ms step_avg:60.67ms
step:1800/2285 train_time:109200ms step_avg:60.67ms
step:1801/2285 train_time:109263ms step_avg:60.67ms
step:1802/2285 train_time:109324ms step_avg:60.67ms
step:1803/2285 train_time:109386ms step_avg:60.67ms
step:1804/2285 train_time:109446ms step_avg:60.67ms
step:1805/2285 train_time:109508ms step_avg:60.67ms
step:1806/2285 train_time:109568ms step_avg:60.67ms
step:1807/2285 train_time:109631ms step_avg:60.67ms
step:1808/2285 train_time:109691ms step_avg:60.67ms
step:1809/2285 train_time:109753ms step_avg:60.67ms
step:1810/2285 train_time:109813ms step_avg:60.67ms
step:1811/2285 train_time:109876ms step_avg:60.67ms
step:1812/2285 train_time:109936ms step_avg:60.67ms
step:1813/2285 train_time:109998ms step_avg:60.67ms
step:1814/2285 train_time:110057ms step_avg:60.67ms
step:1815/2285 train_time:110120ms step_avg:60.67ms
step:1816/2285 train_time:110180ms step_avg:60.67ms
step:1817/2285 train_time:110244ms step_avg:60.67ms
step:1818/2285 train_time:110303ms step_avg:60.67ms
step:1819/2285 train_time:110366ms step_avg:60.67ms
step:1820/2285 train_time:110425ms step_avg:60.67ms
step:1821/2285 train_time:110488ms step_avg:60.67ms
step:1822/2285 train_time:110548ms step_avg:60.67ms
step:1823/2285 train_time:110610ms step_avg:60.67ms
step:1824/2285 train_time:110670ms step_avg:60.67ms
step:1825/2285 train_time:110733ms step_avg:60.68ms
step:1826/2285 train_time:110793ms step_avg:60.68ms
step:1827/2285 train_time:110856ms step_avg:60.68ms
step:1828/2285 train_time:110916ms step_avg:60.68ms
step:1829/2285 train_time:110978ms step_avg:60.68ms
step:1830/2285 train_time:111037ms step_avg:60.68ms
step:1831/2285 train_time:111100ms step_avg:60.68ms
step:1832/2285 train_time:111160ms step_avg:60.68ms
step:1833/2285 train_time:111223ms step_avg:60.68ms
step:1834/2285 train_time:111282ms step_avg:60.68ms
step:1835/2285 train_time:111344ms step_avg:60.68ms
step:1836/2285 train_time:111404ms step_avg:60.68ms
step:1837/2285 train_time:111466ms step_avg:60.68ms
step:1838/2285 train_time:111526ms step_avg:60.68ms
step:1839/2285 train_time:111588ms step_avg:60.68ms
step:1840/2285 train_time:111648ms step_avg:60.68ms
step:1841/2285 train_time:111710ms step_avg:60.68ms
step:1842/2285 train_time:111771ms step_avg:60.68ms
step:1843/2285 train_time:111834ms step_avg:60.68ms
step:1844/2285 train_time:111894ms step_avg:60.68ms
step:1845/2285 train_time:111957ms step_avg:60.68ms
step:1846/2285 train_time:112017ms step_avg:60.68ms
step:1847/2285 train_time:112079ms step_avg:60.68ms
step:1848/2285 train_time:112139ms step_avg:60.68ms
step:1849/2285 train_time:112201ms step_avg:60.68ms
step:1850/2285 train_time:112261ms step_avg:60.68ms
step:1851/2285 train_time:112324ms step_avg:60.68ms
step:1852/2285 train_time:112384ms step_avg:60.68ms
step:1853/2285 train_time:112445ms step_avg:60.68ms
step:1854/2285 train_time:112505ms step_avg:60.68ms
step:1855/2285 train_time:112567ms step_avg:60.68ms
step:1856/2285 train_time:112627ms step_avg:60.68ms
step:1857/2285 train_time:112689ms step_avg:60.68ms
step:1858/2285 train_time:112749ms step_avg:60.68ms
step:1859/2285 train_time:112812ms step_avg:60.68ms
step:1860/2285 train_time:112872ms step_avg:60.68ms
step:1861/2285 train_time:112936ms step_avg:60.69ms
step:1862/2285 train_time:112997ms step_avg:60.69ms
step:1863/2285 train_time:113058ms step_avg:60.69ms
step:1864/2285 train_time:113118ms step_avg:60.69ms
step:1865/2285 train_time:113181ms step_avg:60.69ms
step:1866/2285 train_time:113241ms step_avg:60.69ms
step:1867/2285 train_time:113303ms step_avg:60.69ms
step:1868/2285 train_time:113363ms step_avg:60.69ms
step:1869/2285 train_time:113424ms step_avg:60.69ms
step:1870/2285 train_time:113484ms step_avg:60.69ms
step:1871/2285 train_time:113546ms step_avg:60.69ms
step:1872/2285 train_time:113606ms step_avg:60.69ms
step:1873/2285 train_time:113669ms step_avg:60.69ms
step:1874/2285 train_time:113729ms step_avg:60.69ms
step:1875/2285 train_time:113792ms step_avg:60.69ms
step:1876/2285 train_time:113852ms step_avg:60.69ms
step:1877/2285 train_time:113916ms step_avg:60.69ms
step:1878/2285 train_time:113976ms step_avg:60.69ms
step:1879/2285 train_time:114038ms step_avg:60.69ms
step:1880/2285 train_time:114098ms step_avg:60.69ms
step:1881/2285 train_time:114160ms step_avg:60.69ms
step:1882/2285 train_time:114220ms step_avg:60.69ms
step:1883/2285 train_time:114282ms step_avg:60.69ms
step:1884/2285 train_time:114342ms step_avg:60.69ms
step:1885/2285 train_time:114404ms step_avg:60.69ms
step:1886/2285 train_time:114464ms step_avg:60.69ms
step:1887/2285 train_time:114526ms step_avg:60.69ms
step:1888/2285 train_time:114586ms step_avg:60.69ms
step:1889/2285 train_time:114648ms step_avg:60.69ms
step:1890/2285 train_time:114708ms step_avg:60.69ms
step:1891/2285 train_time:114770ms step_avg:60.69ms
step:1892/2285 train_time:114831ms step_avg:60.69ms
step:1893/2285 train_time:114893ms step_avg:60.69ms
step:1894/2285 train_time:114953ms step_avg:60.69ms
step:1895/2285 train_time:115016ms step_avg:60.69ms
step:1896/2285 train_time:115076ms step_avg:60.69ms
step:1897/2285 train_time:115138ms step_avg:60.69ms
step:1898/2285 train_time:115198ms step_avg:60.69ms
step:1899/2285 train_time:115260ms step_avg:60.70ms
step:1900/2285 train_time:115320ms step_avg:60.69ms
step:1901/2285 train_time:115382ms step_avg:60.70ms
step:1902/2285 train_time:115442ms step_avg:60.70ms
step:1903/2285 train_time:115505ms step_avg:60.70ms
step:1904/2285 train_time:115565ms step_avg:60.70ms
step:1905/2285 train_time:115628ms step_avg:60.70ms
step:1906/2285 train_time:115688ms step_avg:60.70ms
step:1907/2285 train_time:115751ms step_avg:60.70ms
step:1908/2285 train_time:115811ms step_avg:60.70ms
step:1909/2285 train_time:115873ms step_avg:60.70ms
step:1910/2285 train_time:115934ms step_avg:60.70ms
step:1911/2285 train_time:115996ms step_avg:60.70ms
step:1912/2285 train_time:116057ms step_avg:60.70ms
step:1913/2285 train_time:116119ms step_avg:60.70ms
step:1914/2285 train_time:116179ms step_avg:60.70ms
step:1915/2285 train_time:116242ms step_avg:60.70ms
step:1916/2285 train_time:116302ms step_avg:60.70ms
step:1917/2285 train_time:116364ms step_avg:60.70ms
step:1918/2285 train_time:116424ms step_avg:60.70ms
step:1919/2285 train_time:116487ms step_avg:60.70ms
step:1920/2285 train_time:116547ms step_avg:60.70ms
step:1921/2285 train_time:116609ms step_avg:60.70ms
step:1922/2285 train_time:116669ms step_avg:60.70ms
step:1923/2285 train_time:116732ms step_avg:60.70ms
step:1924/2285 train_time:116792ms step_avg:60.70ms
step:1925/2285 train_time:116855ms step_avg:60.70ms
step:1926/2285 train_time:116915ms step_avg:60.70ms
step:1927/2285 train_time:116977ms step_avg:60.70ms
step:1928/2285 train_time:117038ms step_avg:60.70ms
step:1929/2285 train_time:117100ms step_avg:60.70ms
step:1930/2285 train_time:117160ms step_avg:60.70ms
step:1931/2285 train_time:117222ms step_avg:60.71ms
step:1932/2285 train_time:117282ms step_avg:60.71ms
step:1933/2285 train_time:117344ms step_avg:60.71ms
step:1934/2285 train_time:117404ms step_avg:60.71ms
step:1935/2285 train_time:117467ms step_avg:60.71ms
step:1936/2285 train_time:117527ms step_avg:60.71ms
step:1937/2285 train_time:117589ms step_avg:60.71ms
step:1938/2285 train_time:117649ms step_avg:60.71ms
step:1939/2285 train_time:117712ms step_avg:60.71ms
step:1940/2285 train_time:117772ms step_avg:60.71ms
step:1941/2285 train_time:117835ms step_avg:60.71ms
step:1942/2285 train_time:117894ms step_avg:60.71ms
step:1943/2285 train_time:117957ms step_avg:60.71ms
step:1944/2285 train_time:118017ms step_avg:60.71ms
step:1945/2285 train_time:118079ms step_avg:60.71ms
step:1946/2285 train_time:118140ms step_avg:60.71ms
step:1947/2285 train_time:118203ms step_avg:60.71ms
step:1948/2285 train_time:118263ms step_avg:60.71ms
step:1949/2285 train_time:118325ms step_avg:60.71ms
step:1950/2285 train_time:118385ms step_avg:60.71ms
step:1951/2285 train_time:118447ms step_avg:60.71ms
step:1952/2285 train_time:118507ms step_avg:60.71ms
step:1953/2285 train_time:118569ms step_avg:60.71ms
step:1954/2285 train_time:118629ms step_avg:60.71ms
step:1955/2285 train_time:118692ms step_avg:60.71ms
step:1956/2285 train_time:118752ms step_avg:60.71ms
step:1957/2285 train_time:118814ms step_avg:60.71ms
step:1958/2285 train_time:118874ms step_avg:60.71ms
step:1959/2285 train_time:118937ms step_avg:60.71ms
step:1960/2285 train_time:118997ms step_avg:60.71ms
step:1961/2285 train_time:119059ms step_avg:60.71ms
step:1962/2285 train_time:119119ms step_avg:60.71ms
step:1963/2285 train_time:119182ms step_avg:60.71ms
step:1964/2285 train_time:119242ms step_avg:60.71ms
step:1965/2285 train_time:119304ms step_avg:60.71ms
step:1966/2285 train_time:119364ms step_avg:60.71ms
step:1967/2285 train_time:119426ms step_avg:60.72ms
step:1968/2285 train_time:119486ms step_avg:60.71ms
step:1969/2285 train_time:119549ms step_avg:60.72ms
step:1970/2285 train_time:119609ms step_avg:60.72ms
step:1971/2285 train_time:119672ms step_avg:60.72ms
step:1972/2285 train_time:119732ms step_avg:60.72ms
step:1973/2285 train_time:119794ms step_avg:60.72ms
step:1974/2285 train_time:119855ms step_avg:60.72ms
step:1975/2285 train_time:119917ms step_avg:60.72ms
step:1976/2285 train_time:119977ms step_avg:60.72ms
step:1977/2285 train_time:120039ms step_avg:60.72ms
step:1978/2285 train_time:120099ms step_avg:60.72ms
step:1979/2285 train_time:120162ms step_avg:60.72ms
step:1980/2285 train_time:120222ms step_avg:60.72ms
step:1981/2285 train_time:120284ms step_avg:60.72ms
step:1982/2285 train_time:120344ms step_avg:60.72ms
step:1983/2285 train_time:120407ms step_avg:60.72ms
step:1984/2285 train_time:120466ms step_avg:60.72ms
step:1985/2285 train_time:120528ms step_avg:60.72ms
step:1986/2285 train_time:120589ms step_avg:60.72ms
step:1987/2285 train_time:120652ms step_avg:60.72ms
step:1988/2285 train_time:120713ms step_avg:60.72ms
step:1989/2285 train_time:120775ms step_avg:60.72ms
step:1990/2285 train_time:120835ms step_avg:60.72ms
step:1991/2285 train_time:120898ms step_avg:60.72ms
step:1992/2285 train_time:120958ms step_avg:60.72ms
step:1993/2285 train_time:121021ms step_avg:60.72ms
step:1994/2285 train_time:121081ms step_avg:60.72ms
step:1995/2285 train_time:121144ms step_avg:60.72ms
step:1996/2285 train_time:121204ms step_avg:60.72ms
step:1997/2285 train_time:121266ms step_avg:60.72ms
step:1998/2285 train_time:121326ms step_avg:60.72ms
step:1999/2285 train_time:121388ms step_avg:60.72ms
step:2000/2285 train_time:121448ms step_avg:60.72ms
step:2000/2285 val_loss:3.3174 train_time:121512ms step_avg:60.76ms
step:2001/2285 train_time:121530ms step_avg:60.73ms
step:2002/2285 train_time:121574ms step_avg:60.73ms
step:2003/2285 train_time:121636ms step_avg:60.73ms
step:2004/2285 train_time:121697ms step_avg:60.73ms
step:2005/2285 train_time:121762ms step_avg:60.73ms
step:2006/2285 train_time:121822ms step_avg:60.73ms
step:2007/2285 train_time:121884ms step_avg:60.73ms
step:2008/2285 train_time:121945ms step_avg:60.73ms
step:2009/2285 train_time:122007ms step_avg:60.73ms
step:2010/2285 train_time:122066ms step_avg:60.73ms
step:2011/2285 train_time:122127ms step_avg:60.73ms
step:2012/2285 train_time:122186ms step_avg:60.73ms
step:2013/2285 train_time:122248ms step_avg:60.73ms
step:2014/2285 train_time:122309ms step_avg:60.73ms
step:2015/2285 train_time:122371ms step_avg:60.73ms
step:2016/2285 train_time:122432ms step_avg:60.73ms
step:2017/2285 train_time:122496ms step_avg:60.73ms
step:2018/2285 train_time:122557ms step_avg:60.73ms
step:2019/2285 train_time:122620ms step_avg:60.73ms
step:2020/2285 train_time:122681ms step_avg:60.73ms
step:2021/2285 train_time:122744ms step_avg:60.73ms
step:2022/2285 train_time:122804ms step_avg:60.73ms
step:2023/2285 train_time:122867ms step_avg:60.73ms
step:2024/2285 train_time:122927ms step_avg:60.73ms
step:2025/2285 train_time:122988ms step_avg:60.74ms
step:2026/2285 train_time:123048ms step_avg:60.73ms
step:2027/2285 train_time:123109ms step_avg:60.73ms
step:2028/2285 train_time:123169ms step_avg:60.73ms
step:2029/2285 train_time:123230ms step_avg:60.73ms
step:2030/2285 train_time:123290ms step_avg:60.73ms
step:2031/2285 train_time:123352ms step_avg:60.73ms
step:2032/2285 train_time:123413ms step_avg:60.73ms
step:2033/2285 train_time:123476ms step_avg:60.74ms
step:2034/2285 train_time:123536ms step_avg:60.74ms
step:2035/2285 train_time:123599ms step_avg:60.74ms
step:2036/2285 train_time:123660ms step_avg:60.74ms
step:2037/2285 train_time:123723ms step_avg:60.74ms
step:2038/2285 train_time:123784ms step_avg:60.74ms
step:2039/2285 train_time:123847ms step_avg:60.74ms
step:2040/2285 train_time:123907ms step_avg:60.74ms
step:2041/2285 train_time:123969ms step_avg:60.74ms
step:2042/2285 train_time:124029ms step_avg:60.74ms
step:2043/2285 train_time:124091ms step_avg:60.74ms
step:2044/2285 train_time:124151ms step_avg:60.74ms
step:2045/2285 train_time:124213ms step_avg:60.74ms
step:2046/2285 train_time:124272ms step_avg:60.74ms
step:2047/2285 train_time:124335ms step_avg:60.74ms
step:2048/2285 train_time:124395ms step_avg:60.74ms
step:2049/2285 train_time:124457ms step_avg:60.74ms
step:2050/2285 train_time:124517ms step_avg:60.74ms
step:2051/2285 train_time:124579ms step_avg:60.74ms
step:2052/2285 train_time:124640ms step_avg:60.74ms
step:2053/2285 train_time:124703ms step_avg:60.74ms
step:2054/2285 train_time:124764ms step_avg:60.74ms
step:2055/2285 train_time:124826ms step_avg:60.74ms
step:2056/2285 train_time:124886ms step_avg:60.74ms
step:2057/2285 train_time:124948ms step_avg:60.74ms
step:2058/2285 train_time:125008ms step_avg:60.74ms
step:2059/2285 train_time:125070ms step_avg:60.74ms
step:2060/2285 train_time:125130ms step_avg:60.74ms
step:2061/2285 train_time:125191ms step_avg:60.74ms
step:2062/2285 train_time:125251ms step_avg:60.74ms
step:2063/2285 train_time:125313ms step_avg:60.74ms
step:2064/2285 train_time:125373ms step_avg:60.74ms
step:2065/2285 train_time:125436ms step_avg:60.74ms
step:2066/2285 train_time:125496ms step_avg:60.74ms
step:2067/2285 train_time:125558ms step_avg:60.74ms
step:2068/2285 train_time:125619ms step_avg:60.74ms
step:2069/2285 train_time:125682ms step_avg:60.75ms
step:2070/2285 train_time:125742ms step_avg:60.75ms
step:2071/2285 train_time:125805ms step_avg:60.75ms
step:2072/2285 train_time:125866ms step_avg:60.75ms
step:2073/2285 train_time:125928ms step_avg:60.75ms
step:2074/2285 train_time:125988ms step_avg:60.75ms
step:2075/2285 train_time:126050ms step_avg:60.75ms
step:2076/2285 train_time:126110ms step_avg:60.75ms
step:2077/2285 train_time:126172ms step_avg:60.75ms
step:2078/2285 train_time:126232ms step_avg:60.75ms
step:2079/2285 train_time:126295ms step_avg:60.75ms
step:2080/2285 train_time:126355ms step_avg:60.75ms
step:2081/2285 train_time:126417ms step_avg:60.75ms
step:2082/2285 train_time:126477ms step_avg:60.75ms
step:2083/2285 train_time:126539ms step_avg:60.75ms
step:2084/2285 train_time:126600ms step_avg:60.75ms
step:2085/2285 train_time:126663ms step_avg:60.75ms
step:2086/2285 train_time:126723ms step_avg:60.75ms
step:2087/2285 train_time:126787ms step_avg:60.75ms
step:2088/2285 train_time:126847ms step_avg:60.75ms
step:2089/2285 train_time:126909ms step_avg:60.75ms
step:2090/2285 train_time:126969ms step_avg:60.75ms
step:2091/2285 train_time:127031ms step_avg:60.75ms
step:2092/2285 train_time:127091ms step_avg:60.75ms
step:2093/2285 train_time:127154ms step_avg:60.75ms
step:2094/2285 train_time:127214ms step_avg:60.75ms
step:2095/2285 train_time:127275ms step_avg:60.75ms
step:2096/2285 train_time:127335ms step_avg:60.75ms
step:2097/2285 train_time:127398ms step_avg:60.75ms
step:2098/2285 train_time:127457ms step_avg:60.75ms
step:2099/2285 train_time:127519ms step_avg:60.75ms
step:2100/2285 train_time:127580ms step_avg:60.75ms
step:2101/2285 train_time:127642ms step_avg:60.75ms
step:2102/2285 train_time:127703ms step_avg:60.75ms
step:2103/2285 train_time:127766ms step_avg:60.75ms
step:2104/2285 train_time:127826ms step_avg:60.75ms
step:2105/2285 train_time:127888ms step_avg:60.75ms
step:2106/2285 train_time:127948ms step_avg:60.75ms
step:2107/2285 train_time:128010ms step_avg:60.75ms
step:2108/2285 train_time:128071ms step_avg:60.75ms
step:2109/2285 train_time:128132ms step_avg:60.76ms
step:2110/2285 train_time:128192ms step_avg:60.75ms
step:2111/2285 train_time:128254ms step_avg:60.76ms
step:2112/2285 train_time:128314ms step_avg:60.75ms
step:2113/2285 train_time:128376ms step_avg:60.76ms
step:2114/2285 train_time:128436ms step_avg:60.75ms
step:2115/2285 train_time:128497ms step_avg:60.76ms
step:2116/2285 train_time:128558ms step_avg:60.76ms
step:2117/2285 train_time:128621ms step_avg:60.76ms
step:2118/2285 train_time:128681ms step_avg:60.76ms
step:2119/2285 train_time:128744ms step_avg:60.76ms
step:2120/2285 train_time:128805ms step_avg:60.76ms
step:2121/2285 train_time:128867ms step_avg:60.76ms
step:2122/2285 train_time:128928ms step_avg:60.76ms
step:2123/2285 train_time:128990ms step_avg:60.76ms
step:2124/2285 train_time:129050ms step_avg:60.76ms
step:2125/2285 train_time:129112ms step_avg:60.76ms
step:2126/2285 train_time:129173ms step_avg:60.76ms
step:2127/2285 train_time:129235ms step_avg:60.76ms
step:2128/2285 train_time:129294ms step_avg:60.76ms
step:2129/2285 train_time:129356ms step_avg:60.76ms
step:2130/2285 train_time:129416ms step_avg:60.76ms
step:2131/2285 train_time:129479ms step_avg:60.76ms
step:2132/2285 train_time:129539ms step_avg:60.76ms
step:2133/2285 train_time:129601ms step_avg:60.76ms
step:2134/2285 train_time:129663ms step_avg:60.76ms
step:2135/2285 train_time:129725ms step_avg:60.76ms
step:2136/2285 train_time:129785ms step_avg:60.76ms
step:2137/2285 train_time:129848ms step_avg:60.76ms
step:2138/2285 train_time:129908ms step_avg:60.76ms
step:2139/2285 train_time:129971ms step_avg:60.76ms
step:2140/2285 train_time:130031ms step_avg:60.76ms
step:2141/2285 train_time:130093ms step_avg:60.76ms
step:2142/2285 train_time:130153ms step_avg:60.76ms
step:2143/2285 train_time:130215ms step_avg:60.76ms
step:2144/2285 train_time:130275ms step_avg:60.76ms
step:2145/2285 train_time:130337ms step_avg:60.76ms
step:2146/2285 train_time:130397ms step_avg:60.76ms
step:2147/2285 train_time:130459ms step_avg:60.76ms
step:2148/2285 train_time:130520ms step_avg:60.76ms
step:2149/2285 train_time:130582ms step_avg:60.76ms
step:2150/2285 train_time:130642ms step_avg:60.76ms
step:2151/2285 train_time:130705ms step_avg:60.76ms
step:2152/2285 train_time:130766ms step_avg:60.76ms
step:2153/2285 train_time:130828ms step_avg:60.77ms
step:2154/2285 train_time:130888ms step_avg:60.77ms
step:2155/2285 train_time:130951ms step_avg:60.77ms
step:2156/2285 train_time:131011ms step_avg:60.77ms
step:2157/2285 train_time:131074ms step_avg:60.77ms
step:2158/2285 train_time:131134ms step_avg:60.77ms
step:2159/2285 train_time:131196ms step_avg:60.77ms
step:2160/2285 train_time:131256ms step_avg:60.77ms
step:2161/2285 train_time:131318ms step_avg:60.77ms
step:2162/2285 train_time:131378ms step_avg:60.77ms
step:2163/2285 train_time:131441ms step_avg:60.77ms
step:2164/2285 train_time:131501ms step_avg:60.77ms
step:2165/2285 train_time:131564ms step_avg:60.77ms
step:2166/2285 train_time:131624ms step_avg:60.77ms
step:2167/2285 train_time:131686ms step_avg:60.77ms
step:2168/2285 train_time:131746ms step_avg:60.77ms
step:2169/2285 train_time:131808ms step_avg:60.77ms
step:2170/2285 train_time:131868ms step_avg:60.77ms
step:2171/2285 train_time:131931ms step_avg:60.77ms
step:2172/2285 train_time:131991ms step_avg:60.77ms
step:2173/2285 train_time:132054ms step_avg:60.77ms
step:2174/2285 train_time:132113ms step_avg:60.77ms
step:2175/2285 train_time:132175ms step_avg:60.77ms
step:2176/2285 train_time:132235ms step_avg:60.77ms
step:2177/2285 train_time:132297ms step_avg:60.77ms
step:2178/2285 train_time:132357ms step_avg:60.77ms
step:2179/2285 train_time:132419ms step_avg:60.77ms
step:2180/2285 train_time:132479ms step_avg:60.77ms
step:2181/2285 train_time:132541ms step_avg:60.77ms
step:2182/2285 train_time:132602ms step_avg:60.77ms
step:2183/2285 train_time:132665ms step_avg:60.77ms
step:2184/2285 train_time:132725ms step_avg:60.77ms
step:2185/2285 train_time:132788ms step_avg:60.77ms
step:2186/2285 train_time:132848ms step_avg:60.77ms
step:2187/2285 train_time:132911ms step_avg:60.77ms
step:2188/2285 train_time:132971ms step_avg:60.77ms
step:2189/2285 train_time:133034ms step_avg:60.77ms
step:2190/2285 train_time:133093ms step_avg:60.77ms
step:2191/2285 train_time:133156ms step_avg:60.77ms
step:2192/2285 train_time:133216ms step_avg:60.77ms
step:2193/2285 train_time:133277ms step_avg:60.77ms
step:2194/2285 train_time:133337ms step_avg:60.77ms
step:2195/2285 train_time:133399ms step_avg:60.77ms
step:2196/2285 train_time:133459ms step_avg:60.77ms
step:2197/2285 train_time:133522ms step_avg:60.77ms
step:2198/2285 train_time:133582ms step_avg:60.77ms
step:2199/2285 train_time:133645ms step_avg:60.78ms
step:2200/2285 train_time:133705ms step_avg:60.78ms
step:2201/2285 train_time:133768ms step_avg:60.78ms
step:2202/2285 train_time:133828ms step_avg:60.78ms
step:2203/2285 train_time:133890ms step_avg:60.78ms
step:2204/2285 train_time:133951ms step_avg:60.78ms
step:2205/2285 train_time:134013ms step_avg:60.78ms
step:2206/2285 train_time:134073ms step_avg:60.78ms
step:2207/2285 train_time:134135ms step_avg:60.78ms
step:2208/2285 train_time:134195ms step_avg:60.78ms
step:2209/2285 train_time:134257ms step_avg:60.78ms
step:2210/2285 train_time:134317ms step_avg:60.78ms
step:2211/2285 train_time:134380ms step_avg:60.78ms
step:2212/2285 train_time:134439ms step_avg:60.78ms
step:2213/2285 train_time:134502ms step_avg:60.78ms
step:2214/2285 train_time:134563ms step_avg:60.78ms
step:2215/2285 train_time:134626ms step_avg:60.78ms
step:2216/2285 train_time:134685ms step_avg:60.78ms
step:2217/2285 train_time:134748ms step_avg:60.78ms
step:2218/2285 train_time:134808ms step_avg:60.78ms
step:2219/2285 train_time:134870ms step_avg:60.78ms
step:2220/2285 train_time:134930ms step_avg:60.78ms
step:2221/2285 train_time:134992ms step_avg:60.78ms
step:2222/2285 train_time:135052ms step_avg:60.78ms
step:2223/2285 train_time:135114ms step_avg:60.78ms
step:2224/2285 train_time:135175ms step_avg:60.78ms
step:2225/2285 train_time:135237ms step_avg:60.78ms
step:2226/2285 train_time:135297ms step_avg:60.78ms
step:2227/2285 train_time:135360ms step_avg:60.78ms
step:2228/2285 train_time:135420ms step_avg:60.78ms
step:2229/2285 train_time:135482ms step_avg:60.78ms
step:2230/2285 train_time:135543ms step_avg:60.78ms
step:2231/2285 train_time:135606ms step_avg:60.78ms
step:2232/2285 train_time:135666ms step_avg:60.78ms
step:2233/2285 train_time:135729ms step_avg:60.78ms
step:2234/2285 train_time:135788ms step_avg:60.78ms
step:2235/2285 train_time:135850ms step_avg:60.78ms
step:2236/2285 train_time:135910ms step_avg:60.78ms
step:2237/2285 train_time:135973ms step_avg:60.78ms
step:2238/2285 train_time:136032ms step_avg:60.78ms
step:2239/2285 train_time:136095ms step_avg:60.78ms
step:2240/2285 train_time:136155ms step_avg:60.78ms
step:2241/2285 train_time:136217ms step_avg:60.78ms
step:2242/2285 train_time:136277ms step_avg:60.78ms
step:2243/2285 train_time:136340ms step_avg:60.78ms
step:2244/2285 train_time:136400ms step_avg:60.78ms
step:2245/2285 train_time:136463ms step_avg:60.79ms
step:2246/2285 train_time:136523ms step_avg:60.79ms
step:2247/2285 train_time:136586ms step_avg:60.79ms
step:2248/2285 train_time:136646ms step_avg:60.79ms
step:2249/2285 train_time:136708ms step_avg:60.79ms
step:2250/2285 train_time:136769ms step_avg:60.79ms
step:2250/2285 val_loss:3.2821 train_time:136832ms step_avg:60.81ms
step:2251/2285 train_time:136851ms step_avg:60.80ms
step:2252/2285 train_time:136896ms step_avg:60.79ms
step:2253/2285 train_time:136961ms step_avg:60.79ms
step:2254/2285 train_time:137022ms step_avg:60.79ms
step:2255/2285 train_time:137085ms step_avg:60.79ms
step:2256/2285 train_time:137145ms step_avg:60.79ms
step:2257/2285 train_time:137206ms step_avg:60.79ms
step:2258/2285 train_time:137266ms step_avg:60.79ms
step:2259/2285 train_time:137327ms step_avg:60.79ms
step:2260/2285 train_time:137387ms step_avg:60.79ms
step:2261/2285 train_time:137449ms step_avg:60.79ms
step:2262/2285 train_time:137508ms step_avg:60.79ms
step:2263/2285 train_time:137570ms step_avg:60.79ms
step:2264/2285 train_time:137630ms step_avg:60.79ms
step:2265/2285 train_time:137692ms step_avg:60.79ms
step:2266/2285 train_time:137753ms step_avg:60.79ms
step:2267/2285 train_time:137818ms step_avg:60.79ms
step:2268/2285 train_time:137880ms step_avg:60.79ms
step:2269/2285 train_time:137943ms step_avg:60.79ms
step:2270/2285 train_time:138004ms step_avg:60.79ms
step:2271/2285 train_time:138067ms step_avg:60.80ms
step:2272/2285 train_time:138128ms step_avg:60.80ms
step:2273/2285 train_time:138190ms step_avg:60.80ms
step:2274/2285 train_time:138250ms step_avg:60.80ms
step:2275/2285 train_time:138312ms step_avg:60.80ms
step:2276/2285 train_time:138371ms step_avg:60.80ms
step:2277/2285 train_time:138433ms step_avg:60.80ms
step:2278/2285 train_time:138493ms step_avg:60.80ms
step:2279/2285 train_time:138555ms step_avg:60.80ms
step:2280/2285 train_time:138615ms step_avg:60.80ms
step:2281/2285 train_time:138677ms step_avg:60.80ms
step:2282/2285 train_time:138738ms step_avg:60.80ms
step:2283/2285 train_time:138802ms step_avg:60.80ms
step:2284/2285 train_time:138863ms step_avg:60.80ms
step:2285/2285 train_time:138925ms step_avg:60.80ms
step:2285/2285 val_loss:3.2766 train_time:138986ms step_avg:60.83ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

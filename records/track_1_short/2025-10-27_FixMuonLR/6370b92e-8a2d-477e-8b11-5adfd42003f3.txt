import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Thu Nov  6 05:17:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     95378      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     95379      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     95380      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     95381      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     95382      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     95383      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     95384      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     95385      C   /root/.venv/bin/python3                         0MiB |
|    1   N/A  N/A     95379      C   /root/.venv/bin/python3                         0MiB |
|    2   N/A  N/A     95380      C   /root/.venv/bin/python3                         0MiB |
|    3   N/A  N/A     95381      C   /root/.venv/bin/python3                         0MiB |
|    4   N/A  N/A     95382      C   /root/.venv/bin/python3                         0MiB |
|    5   N/A  N/A     95383      C   /root/.venv/bin/python3                         0MiB |
|    6   N/A  N/A     95384      C   /root/.venv/bin/python3                         0MiB |
|    7   N/A  N/A     95385      C   /root/.venv/bin/python3                         0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/2285 train_time:116ms step_avg:115.84ms
step:2/2285 train_time:138ms step_avg:68.83ms
step:3/2285 train_time:175ms step_avg:58.39ms
step:4/2285 train_time:231ms step_avg:57.86ms
step:5/2285 train_time:291ms step_avg:58.20ms
step:6/2285 train_time:350ms step_avg:58.26ms
step:7/2285 train_time:411ms step_avg:58.65ms
step:8/2285 train_time:469ms step_avg:58.65ms
step:9/2285 train_time:530ms step_avg:58.91ms
step:10/2285 train_time:589ms step_avg:58.88ms
step:11/2285 train_time:650ms step_avg:59.06ms
step:12/2285 train_time:709ms step_avg:59.05ms
step:13/2285 train_time:769ms step_avg:59.18ms
step:14/2285 train_time:828ms step_avg:59.13ms
step:15/2285 train_time:889ms step_avg:59.28ms
step:16/2285 train_time:948ms step_avg:59.27ms
step:17/2285 train_time:1011ms step_avg:59.50ms
step:18/2285 train_time:1072ms step_avg:59.57ms
step:19/2285 train_time:1137ms step_avg:59.86ms
step:20/2285 train_time:1199ms step_avg:59.96ms
step:21/2285 train_time:1262ms step_avg:60.09ms
step:22/2285 train_time:1321ms step_avg:60.06ms
step:23/2285 train_time:1384ms step_avg:60.16ms
step:24/2285 train_time:1443ms step_avg:60.12ms
step:25/2285 train_time:1505ms step_avg:60.21ms
step:26/2285 train_time:1565ms step_avg:60.17ms
step:27/2285 train_time:1626ms step_avg:60.24ms
step:28/2285 train_time:1687ms step_avg:60.25ms
step:29/2285 train_time:1747ms step_avg:60.24ms
step:30/2285 train_time:1806ms step_avg:60.20ms
step:31/2285 train_time:1867ms step_avg:60.23ms
step:32/2285 train_time:1926ms step_avg:60.20ms
step:33/2285 train_time:1989ms step_avg:60.27ms
step:34/2285 train_time:2049ms step_avg:60.26ms
step:35/2285 train_time:2113ms step_avg:60.36ms
step:36/2285 train_time:2172ms step_avg:60.33ms
step:37/2285 train_time:2234ms step_avg:60.37ms
step:38/2285 train_time:2293ms step_avg:60.35ms
step:39/2285 train_time:2356ms step_avg:60.40ms
step:40/2285 train_time:2416ms step_avg:60.39ms
step:41/2285 train_time:2477ms step_avg:60.42ms
step:42/2285 train_time:2538ms step_avg:60.42ms
step:43/2285 train_time:2600ms step_avg:60.46ms
step:44/2285 train_time:2659ms step_avg:60.42ms
step:45/2285 train_time:2720ms step_avg:60.44ms
step:46/2285 train_time:2779ms step_avg:60.42ms
step:47/2285 train_time:2841ms step_avg:60.44ms
step:48/2285 train_time:2901ms step_avg:60.43ms
step:49/2285 train_time:2963ms step_avg:60.47ms
step:50/2285 train_time:3023ms step_avg:60.46ms
step:51/2285 train_time:3087ms step_avg:60.53ms
step:52/2285 train_time:3147ms step_avg:60.52ms
step:53/2285 train_time:3209ms step_avg:60.56ms
step:54/2285 train_time:3269ms step_avg:60.54ms
step:55/2285 train_time:3331ms step_avg:60.55ms
step:56/2285 train_time:3390ms step_avg:60.53ms
step:57/2285 train_time:3452ms step_avg:60.55ms
step:58/2285 train_time:3511ms step_avg:60.53ms
step:59/2285 train_time:3572ms step_avg:60.54ms
step:60/2285 train_time:3631ms step_avg:60.52ms
step:61/2285 train_time:3692ms step_avg:60.53ms
step:62/2285 train_time:3751ms step_avg:60.50ms
step:63/2285 train_time:3812ms step_avg:60.51ms
step:64/2285 train_time:3871ms step_avg:60.48ms
step:65/2285 train_time:3932ms step_avg:60.49ms
step:66/2285 train_time:3992ms step_avg:60.48ms
step:67/2285 train_time:4054ms step_avg:60.51ms
step:68/2285 train_time:4114ms step_avg:60.50ms
step:69/2285 train_time:4177ms step_avg:60.53ms
step:70/2285 train_time:4237ms step_avg:60.52ms
step:71/2285 train_time:4299ms step_avg:60.56ms
step:72/2285 train_time:4359ms step_avg:60.54ms
step:73/2285 train_time:4421ms step_avg:60.56ms
step:74/2285 train_time:4481ms step_avg:60.55ms
step:75/2285 train_time:4543ms step_avg:60.57ms
step:76/2285 train_time:4602ms step_avg:60.55ms
step:77/2285 train_time:4664ms step_avg:60.58ms
step:78/2285 train_time:4724ms step_avg:60.56ms
step:79/2285 train_time:4786ms step_avg:60.58ms
step:80/2285 train_time:4845ms step_avg:60.56ms
step:81/2285 train_time:4906ms step_avg:60.57ms
step:82/2285 train_time:4965ms step_avg:60.55ms
step:83/2285 train_time:5027ms step_avg:60.57ms
step:84/2285 train_time:5086ms step_avg:60.55ms
step:85/2285 train_time:5148ms step_avg:60.57ms
step:86/2285 train_time:5207ms step_avg:60.55ms
step:87/2285 train_time:5269ms step_avg:60.57ms
step:88/2285 train_time:5328ms step_avg:60.54ms
step:89/2285 train_time:5389ms step_avg:60.55ms
step:90/2285 train_time:5447ms step_avg:60.52ms
step:91/2285 train_time:5509ms step_avg:60.53ms
step:92/2285 train_time:5567ms step_avg:60.51ms
step:93/2285 train_time:5628ms step_avg:60.52ms
step:94/2285 train_time:5687ms step_avg:60.50ms
step:95/2285 train_time:5749ms step_avg:60.51ms
step:96/2285 train_time:5807ms step_avg:60.49ms
step:97/2285 train_time:5868ms step_avg:60.50ms
step:98/2285 train_time:5927ms step_avg:60.48ms
step:99/2285 train_time:5989ms step_avg:60.49ms
step:100/2285 train_time:6048ms step_avg:60.48ms
step:101/2285 train_time:6110ms step_avg:60.49ms
step:102/2285 train_time:6168ms step_avg:60.47ms
step:103/2285 train_time:6230ms step_avg:60.48ms
step:104/2285 train_time:6289ms step_avg:60.47ms
step:105/2285 train_time:6350ms step_avg:60.47ms
step:106/2285 train_time:6409ms step_avg:60.46ms
step:107/2285 train_time:6470ms step_avg:60.47ms
step:108/2285 train_time:6529ms step_avg:60.45ms
step:109/2285 train_time:6590ms step_avg:60.46ms
step:110/2285 train_time:6648ms step_avg:60.44ms
step:111/2285 train_time:6710ms step_avg:60.45ms
step:112/2285 train_time:6768ms step_avg:60.43ms
step:113/2285 train_time:6830ms step_avg:60.44ms
step:114/2285 train_time:6889ms step_avg:60.43ms
step:115/2285 train_time:6950ms step_avg:60.43ms
step:116/2285 train_time:7009ms step_avg:60.42ms
step:117/2285 train_time:7070ms step_avg:60.43ms
step:118/2285 train_time:7129ms step_avg:60.42ms
step:119/2285 train_time:7191ms step_avg:60.43ms
step:120/2285 train_time:7249ms step_avg:60.41ms
step:121/2285 train_time:7311ms step_avg:60.42ms
step:122/2285 train_time:7369ms step_avg:60.40ms
step:123/2285 train_time:7430ms step_avg:60.41ms
step:124/2285 train_time:7489ms step_avg:60.40ms
step:125/2285 train_time:7551ms step_avg:60.40ms
step:126/2285 train_time:7610ms step_avg:60.39ms
step:127/2285 train_time:7671ms step_avg:60.40ms
step:128/2285 train_time:7730ms step_avg:60.39ms
step:129/2285 train_time:7792ms step_avg:60.40ms
step:130/2285 train_time:7851ms step_avg:60.39ms
step:131/2285 train_time:7912ms step_avg:60.40ms
step:132/2285 train_time:7971ms step_avg:60.39ms
step:133/2285 train_time:8033ms step_avg:60.39ms
step:134/2285 train_time:8091ms step_avg:60.38ms
step:135/2285 train_time:8153ms step_avg:60.39ms
step:136/2285 train_time:8212ms step_avg:60.38ms
step:137/2285 train_time:8273ms step_avg:60.39ms
step:138/2285 train_time:8332ms step_avg:60.38ms
step:139/2285 train_time:8394ms step_avg:60.39ms
step:140/2285 train_time:8453ms step_avg:60.38ms
step:141/2285 train_time:8514ms step_avg:60.38ms
step:142/2285 train_time:8573ms step_avg:60.37ms
step:143/2285 train_time:8634ms step_avg:60.38ms
step:144/2285 train_time:8694ms step_avg:60.37ms
step:145/2285 train_time:8756ms step_avg:60.39ms
step:146/2285 train_time:8815ms step_avg:60.38ms
step:147/2285 train_time:8877ms step_avg:60.39ms
step:148/2285 train_time:8936ms step_avg:60.38ms
step:149/2285 train_time:8997ms step_avg:60.38ms
step:150/2285 train_time:9057ms step_avg:60.38ms
step:151/2285 train_time:9119ms step_avg:60.39ms
step:152/2285 train_time:9179ms step_avg:60.39ms
step:153/2285 train_time:9240ms step_avg:60.40ms
step:154/2285 train_time:9299ms step_avg:60.39ms
step:155/2285 train_time:9362ms step_avg:60.40ms
step:156/2285 train_time:9422ms step_avg:60.40ms
step:157/2285 train_time:9485ms step_avg:60.42ms
step:158/2285 train_time:9545ms step_avg:60.41ms
step:159/2285 train_time:9607ms step_avg:60.42ms
step:160/2285 train_time:9665ms step_avg:60.41ms
step:161/2285 train_time:9727ms step_avg:60.42ms
step:162/2285 train_time:9786ms step_avg:60.41ms
step:163/2285 train_time:9847ms step_avg:60.41ms
step:164/2285 train_time:9906ms step_avg:60.40ms
step:165/2285 train_time:9967ms step_avg:60.41ms
step:166/2285 train_time:10026ms step_avg:60.40ms
step:167/2285 train_time:10087ms step_avg:60.40ms
step:168/2285 train_time:10147ms step_avg:60.40ms
step:169/2285 train_time:10208ms step_avg:60.40ms
step:170/2285 train_time:10267ms step_avg:60.39ms
step:171/2285 train_time:10328ms step_avg:60.40ms
step:172/2285 train_time:10386ms step_avg:60.39ms
step:173/2285 train_time:10448ms step_avg:60.39ms
step:174/2285 train_time:10507ms step_avg:60.38ms
step:175/2285 train_time:10568ms step_avg:60.39ms
step:176/2285 train_time:10627ms step_avg:60.38ms
step:177/2285 train_time:10688ms step_avg:60.38ms
step:178/2285 train_time:10747ms step_avg:60.37ms
step:179/2285 train_time:10808ms step_avg:60.38ms
step:180/2285 train_time:10867ms step_avg:60.37ms
step:181/2285 train_time:10928ms step_avg:60.37ms
step:182/2285 train_time:10986ms step_avg:60.36ms
step:183/2285 train_time:11048ms step_avg:60.37ms
step:184/2285 train_time:11107ms step_avg:60.37ms
step:185/2285 train_time:11168ms step_avg:60.37ms
step:186/2285 train_time:11227ms step_avg:60.36ms
step:187/2285 train_time:11289ms step_avg:60.37ms
step:188/2285 train_time:11348ms step_avg:60.36ms
step:189/2285 train_time:11409ms step_avg:60.36ms
step:190/2285 train_time:11467ms step_avg:60.35ms
step:191/2285 train_time:11528ms step_avg:60.36ms
step:192/2285 train_time:11587ms step_avg:60.35ms
step:193/2285 train_time:11649ms step_avg:60.36ms
step:194/2285 train_time:11707ms step_avg:60.35ms
step:195/2285 train_time:11768ms step_avg:60.35ms
step:196/2285 train_time:11827ms step_avg:60.34ms
step:197/2285 train_time:11889ms step_avg:60.35ms
step:198/2285 train_time:11947ms step_avg:60.34ms
step:199/2285 train_time:12008ms step_avg:60.34ms
step:200/2285 train_time:12067ms step_avg:60.33ms
step:201/2285 train_time:12128ms step_avg:60.34ms
step:202/2285 train_time:12187ms step_avg:60.33ms
step:203/2285 train_time:12249ms step_avg:60.34ms
step:204/2285 train_time:12308ms step_avg:60.33ms
step:205/2285 train_time:12368ms step_avg:60.33ms
step:206/2285 train_time:12427ms step_avg:60.33ms
step:207/2285 train_time:12488ms step_avg:60.33ms
step:208/2285 train_time:12547ms step_avg:60.32ms
step:209/2285 train_time:12609ms step_avg:60.33ms
step:210/2285 train_time:12667ms step_avg:60.32ms
step:211/2285 train_time:12728ms step_avg:60.32ms
step:212/2285 train_time:12787ms step_avg:60.31ms
step:213/2285 train_time:12848ms step_avg:60.32ms
step:214/2285 train_time:12907ms step_avg:60.31ms
step:215/2285 train_time:12968ms step_avg:60.32ms
step:216/2285 train_time:13026ms step_avg:60.31ms
step:217/2285 train_time:13088ms step_avg:60.32ms
step:218/2285 train_time:13147ms step_avg:60.31ms
step:219/2285 train_time:13209ms step_avg:60.31ms
step:220/2285 train_time:13267ms step_avg:60.31ms
step:221/2285 train_time:13328ms step_avg:60.31ms
step:222/2285 train_time:13387ms step_avg:60.30ms
step:223/2285 train_time:13449ms step_avg:60.31ms
step:224/2285 train_time:13508ms step_avg:60.30ms
step:225/2285 train_time:13569ms step_avg:60.31ms
step:226/2285 train_time:13627ms step_avg:60.30ms
step:227/2285 train_time:13689ms step_avg:60.30ms
step:228/2285 train_time:13747ms step_avg:60.29ms
step:229/2285 train_time:13809ms step_avg:60.30ms
step:230/2285 train_time:13867ms step_avg:60.29ms
step:231/2285 train_time:13928ms step_avg:60.29ms
step:232/2285 train_time:13986ms step_avg:60.29ms
step:233/2285 train_time:14049ms step_avg:60.30ms
step:234/2285 train_time:14108ms step_avg:60.29ms
step:235/2285 train_time:14169ms step_avg:60.29ms
step:236/2285 train_time:14227ms step_avg:60.28ms
step:237/2285 train_time:14289ms step_avg:60.29ms
step:238/2285 train_time:14347ms step_avg:60.28ms
step:239/2285 train_time:14409ms step_avg:60.29ms
step:240/2285 train_time:14467ms step_avg:60.28ms
step:241/2285 train_time:14528ms step_avg:60.28ms
step:242/2285 train_time:14588ms step_avg:60.28ms
step:243/2285 train_time:14649ms step_avg:60.28ms
step:244/2285 train_time:14707ms step_avg:60.28ms
step:245/2285 train_time:14769ms step_avg:60.28ms
step:246/2285 train_time:14827ms step_avg:60.27ms
step:247/2285 train_time:14889ms step_avg:60.28ms
step:248/2285 train_time:14947ms step_avg:60.27ms
step:249/2285 train_time:15008ms step_avg:60.27ms
step:250/2285 train_time:15067ms step_avg:60.27ms
step:250/2285 val_loss:4.0818 train_time:15129ms step_avg:60.52ms
step:251/2285 train_time:15150ms step_avg:60.36ms
step:252/2285 train_time:15188ms step_avg:60.27ms
step:253/2285 train_time:15256ms step_avg:60.30ms
step:254/2285 train_time:15320ms step_avg:60.32ms
step:255/2285 train_time:15382ms step_avg:60.32ms
step:256/2285 train_time:15442ms step_avg:60.32ms
step:257/2285 train_time:15503ms step_avg:60.32ms
step:258/2285 train_time:15561ms step_avg:60.32ms
step:259/2285 train_time:15623ms step_avg:60.32ms
step:260/2285 train_time:15682ms step_avg:60.32ms
step:261/2285 train_time:15743ms step_avg:60.32ms
step:262/2285 train_time:15802ms step_avg:60.31ms
step:263/2285 train_time:15863ms step_avg:60.32ms
step:264/2285 train_time:15921ms step_avg:60.31ms
step:265/2285 train_time:15983ms step_avg:60.31ms
step:266/2285 train_time:16042ms step_avg:60.31ms
step:267/2285 train_time:16104ms step_avg:60.31ms
step:268/2285 train_time:16164ms step_avg:60.32ms
step:269/2285 train_time:16230ms step_avg:60.33ms
step:270/2285 train_time:16291ms step_avg:60.34ms
step:271/2285 train_time:16353ms step_avg:60.34ms
step:272/2285 train_time:16413ms step_avg:60.34ms
step:273/2285 train_time:16474ms step_avg:60.34ms
step:274/2285 train_time:16533ms step_avg:60.34ms
step:275/2285 train_time:16593ms step_avg:60.34ms
step:276/2285 train_time:16652ms step_avg:60.33ms
step:277/2285 train_time:16712ms step_avg:60.33ms
step:278/2285 train_time:16770ms step_avg:60.32ms
step:279/2285 train_time:16831ms step_avg:60.33ms
step:280/2285 train_time:16890ms step_avg:60.32ms
step:281/2285 train_time:16950ms step_avg:60.32ms
step:282/2285 train_time:17008ms step_avg:60.31ms
step:283/2285 train_time:17069ms step_avg:60.31ms
step:284/2285 train_time:17128ms step_avg:60.31ms
step:285/2285 train_time:17190ms step_avg:60.32ms
step:286/2285 train_time:17250ms step_avg:60.32ms
step:287/2285 train_time:17313ms step_avg:60.32ms
step:288/2285 train_time:17371ms step_avg:60.32ms
step:289/2285 train_time:17433ms step_avg:60.32ms
step:290/2285 train_time:17492ms step_avg:60.32ms
step:291/2285 train_time:17554ms step_avg:60.32ms
step:292/2285 train_time:17612ms step_avg:60.32ms
step:293/2285 train_time:17673ms step_avg:60.32ms
step:294/2285 train_time:17731ms step_avg:60.31ms
step:295/2285 train_time:17792ms step_avg:60.31ms
step:296/2285 train_time:17850ms step_avg:60.30ms
step:297/2285 train_time:17910ms step_avg:60.30ms
step:298/2285 train_time:17969ms step_avg:60.30ms
step:299/2285 train_time:18029ms step_avg:60.30ms
step:300/2285 train_time:18088ms step_avg:60.29ms
step:301/2285 train_time:18150ms step_avg:60.30ms
step:302/2285 train_time:18209ms step_avg:60.29ms
step:303/2285 train_time:18270ms step_avg:60.30ms
step:304/2285 train_time:18330ms step_avg:60.30ms
step:305/2285 train_time:18392ms step_avg:60.30ms
step:306/2285 train_time:18450ms step_avg:60.30ms
step:307/2285 train_time:18512ms step_avg:60.30ms
step:308/2285 train_time:18570ms step_avg:60.29ms
step:309/2285 train_time:18631ms step_avg:60.30ms
step:310/2285 train_time:18689ms step_avg:60.29ms
step:311/2285 train_time:18751ms step_avg:60.29ms
step:312/2285 train_time:18809ms step_avg:60.29ms
step:313/2285 train_time:18870ms step_avg:60.29ms
step:314/2285 train_time:18928ms step_avg:60.28ms
step:315/2285 train_time:18989ms step_avg:60.28ms
step:316/2285 train_time:19047ms step_avg:60.28ms
step:317/2285 train_time:19108ms step_avg:60.28ms
step:318/2285 train_time:19167ms step_avg:60.27ms
step:319/2285 train_time:19229ms step_avg:60.28ms
step:320/2285 train_time:19288ms step_avg:60.27ms
step:321/2285 train_time:19350ms step_avg:60.28ms
step:322/2285 train_time:19410ms step_avg:60.28ms
step:323/2285 train_time:19471ms step_avg:60.28ms
step:324/2285 train_time:19530ms step_avg:60.28ms
step:325/2285 train_time:19591ms step_avg:60.28ms
step:326/2285 train_time:19649ms step_avg:60.27ms
step:327/2285 train_time:19710ms step_avg:60.28ms
step:328/2285 train_time:19769ms step_avg:60.27ms
step:329/2285 train_time:19830ms step_avg:60.27ms
step:330/2285 train_time:19888ms step_avg:60.27ms
step:331/2285 train_time:19949ms step_avg:60.27ms
step:332/2285 train_time:20007ms step_avg:60.26ms
step:333/2285 train_time:20069ms step_avg:60.27ms
step:334/2285 train_time:20127ms step_avg:60.26ms
step:335/2285 train_time:20189ms step_avg:60.26ms
step:336/2285 train_time:20247ms step_avg:60.26ms
step:337/2285 train_time:20309ms step_avg:60.26ms
step:338/2285 train_time:20368ms step_avg:60.26ms
step:339/2285 train_time:20431ms step_avg:60.27ms
step:340/2285 train_time:20489ms step_avg:60.26ms
step:341/2285 train_time:20551ms step_avg:60.27ms
step:342/2285 train_time:20610ms step_avg:60.26ms
step:343/2285 train_time:20671ms step_avg:60.27ms
step:344/2285 train_time:20730ms step_avg:60.26ms
step:345/2285 train_time:20791ms step_avg:60.26ms
step:346/2285 train_time:20849ms step_avg:60.26ms
step:347/2285 train_time:20910ms step_avg:60.26ms
step:348/2285 train_time:20968ms step_avg:60.25ms
step:349/2285 train_time:21029ms step_avg:60.26ms
step:350/2285 train_time:21088ms step_avg:60.25ms
step:351/2285 train_time:21149ms step_avg:60.25ms
step:352/2285 train_time:21208ms step_avg:60.25ms
step:353/2285 train_time:21270ms step_avg:60.25ms
step:354/2285 train_time:21329ms step_avg:60.25ms
step:355/2285 train_time:21391ms step_avg:60.26ms
step:356/2285 train_time:21449ms step_avg:60.25ms
step:357/2285 train_time:21511ms step_avg:60.25ms
step:358/2285 train_time:21569ms step_avg:60.25ms
step:359/2285 train_time:21631ms step_avg:60.25ms
step:360/2285 train_time:21690ms step_avg:60.25ms
step:361/2285 train_time:21751ms step_avg:60.25ms
step:362/2285 train_time:21809ms step_avg:60.25ms
step:363/2285 train_time:21871ms step_avg:60.25ms
step:364/2285 train_time:21929ms step_avg:60.25ms
step:365/2285 train_time:21991ms step_avg:60.25ms
step:366/2285 train_time:22049ms step_avg:60.24ms
step:367/2285 train_time:22110ms step_avg:60.24ms
step:368/2285 train_time:22168ms step_avg:60.24ms
step:369/2285 train_time:22230ms step_avg:60.24ms
step:370/2285 train_time:22289ms step_avg:60.24ms
step:371/2285 train_time:22350ms step_avg:60.24ms
step:372/2285 train_time:22409ms step_avg:60.24ms
step:373/2285 train_time:22471ms step_avg:60.24ms
step:374/2285 train_time:22529ms step_avg:60.24ms
step:375/2285 train_time:22591ms step_avg:60.24ms
step:376/2285 train_time:22649ms step_avg:60.24ms
step:377/2285 train_time:22711ms step_avg:60.24ms
step:378/2285 train_time:22769ms step_avg:60.24ms
step:379/2285 train_time:22831ms step_avg:60.24ms
step:380/2285 train_time:22889ms step_avg:60.23ms
step:381/2285 train_time:22950ms step_avg:60.24ms
step:382/2285 train_time:23008ms step_avg:60.23ms
step:383/2285 train_time:23069ms step_avg:60.23ms
step:384/2285 train_time:23127ms step_avg:60.23ms
step:385/2285 train_time:23189ms step_avg:60.23ms
step:386/2285 train_time:23248ms step_avg:60.23ms
step:387/2285 train_time:23309ms step_avg:60.23ms
step:388/2285 train_time:23367ms step_avg:60.22ms
step:389/2285 train_time:23429ms step_avg:60.23ms
step:390/2285 train_time:23488ms step_avg:60.23ms
step:391/2285 train_time:23550ms step_avg:60.23ms
step:392/2285 train_time:23609ms step_avg:60.23ms
step:393/2285 train_time:23670ms step_avg:60.23ms
step:394/2285 train_time:23728ms step_avg:60.22ms
step:395/2285 train_time:23791ms step_avg:60.23ms
step:396/2285 train_time:23849ms step_avg:60.23ms
step:397/2285 train_time:23910ms step_avg:60.23ms
step:398/2285 train_time:23968ms step_avg:60.22ms
step:399/2285 train_time:24030ms step_avg:60.22ms
step:400/2285 train_time:24089ms step_avg:60.22ms
step:401/2285 train_time:24150ms step_avg:60.22ms
step:402/2285 train_time:24209ms step_avg:60.22ms
step:403/2285 train_time:24270ms step_avg:60.22ms
step:404/2285 train_time:24329ms step_avg:60.22ms
step:405/2285 train_time:24390ms step_avg:60.22ms
step:406/2285 train_time:24449ms step_avg:60.22ms
step:407/2285 train_time:24510ms step_avg:60.22ms
step:408/2285 train_time:24569ms step_avg:60.22ms
step:409/2285 train_time:24630ms step_avg:60.22ms
step:410/2285 train_time:24689ms step_avg:60.22ms
step:411/2285 train_time:24751ms step_avg:60.22ms
step:412/2285 train_time:24809ms step_avg:60.22ms
step:413/2285 train_time:24870ms step_avg:60.22ms
step:414/2285 train_time:24929ms step_avg:60.22ms
step:415/2285 train_time:24990ms step_avg:60.22ms
step:416/2285 train_time:25049ms step_avg:60.21ms
step:417/2285 train_time:25110ms step_avg:60.22ms
step:418/2285 train_time:25168ms step_avg:60.21ms
step:419/2285 train_time:25229ms step_avg:60.21ms
step:420/2285 train_time:25289ms step_avg:60.21ms
step:421/2285 train_time:25350ms step_avg:60.21ms
step:422/2285 train_time:25408ms step_avg:60.21ms
step:423/2285 train_time:25470ms step_avg:60.21ms
step:424/2285 train_time:25529ms step_avg:60.21ms
step:425/2285 train_time:25590ms step_avg:60.21ms
step:426/2285 train_time:25649ms step_avg:60.21ms
step:427/2285 train_time:25710ms step_avg:60.21ms
step:428/2285 train_time:25769ms step_avg:60.21ms
step:429/2285 train_time:25830ms step_avg:60.21ms
step:430/2285 train_time:25889ms step_avg:60.21ms
step:431/2285 train_time:25950ms step_avg:60.21ms
step:432/2285 train_time:26009ms step_avg:60.21ms
step:433/2285 train_time:26070ms step_avg:60.21ms
step:434/2285 train_time:26129ms step_avg:60.20ms
step:435/2285 train_time:26190ms step_avg:60.21ms
step:436/2285 train_time:26249ms step_avg:60.20ms
step:437/2285 train_time:26311ms step_avg:60.21ms
step:438/2285 train_time:26369ms step_avg:60.20ms
step:439/2285 train_time:26430ms step_avg:60.21ms
step:440/2285 train_time:26489ms step_avg:60.20ms
step:441/2285 train_time:26551ms step_avg:60.21ms
step:442/2285 train_time:26609ms step_avg:60.20ms
step:443/2285 train_time:26671ms step_avg:60.20ms
step:444/2285 train_time:26729ms step_avg:60.20ms
step:445/2285 train_time:26790ms step_avg:60.20ms
step:446/2285 train_time:26849ms step_avg:60.20ms
step:447/2285 train_time:26910ms step_avg:60.20ms
step:448/2285 train_time:26969ms step_avg:60.20ms
step:449/2285 train_time:27030ms step_avg:60.20ms
step:450/2285 train_time:27089ms step_avg:60.20ms
step:451/2285 train_time:27150ms step_avg:60.20ms
step:452/2285 train_time:27208ms step_avg:60.20ms
step:453/2285 train_time:27270ms step_avg:60.20ms
step:454/2285 train_time:27329ms step_avg:60.20ms
step:455/2285 train_time:27390ms step_avg:60.20ms
step:456/2285 train_time:27449ms step_avg:60.20ms
step:457/2285 train_time:27510ms step_avg:60.20ms
step:458/2285 train_time:27569ms step_avg:60.19ms
step:459/2285 train_time:27630ms step_avg:60.20ms
step:460/2285 train_time:27689ms step_avg:60.19ms
step:461/2285 train_time:27751ms step_avg:60.20ms
step:462/2285 train_time:27809ms step_avg:60.19ms
step:463/2285 train_time:27871ms step_avg:60.20ms
step:464/2285 train_time:27929ms step_avg:60.19ms
step:465/2285 train_time:27991ms step_avg:60.20ms
step:466/2285 train_time:28050ms step_avg:60.19ms
step:467/2285 train_time:28111ms step_avg:60.20ms
step:468/2285 train_time:28170ms step_avg:60.19ms
step:469/2285 train_time:28230ms step_avg:60.19ms
step:470/2285 train_time:28289ms step_avg:60.19ms
step:471/2285 train_time:28350ms step_avg:60.19ms
step:472/2285 train_time:28408ms step_avg:60.19ms
step:473/2285 train_time:28469ms step_avg:60.19ms
step:474/2285 train_time:28528ms step_avg:60.19ms
step:475/2285 train_time:28590ms step_avg:60.19ms
step:476/2285 train_time:28649ms step_avg:60.19ms
step:477/2285 train_time:28710ms step_avg:60.19ms
step:478/2285 train_time:28769ms step_avg:60.19ms
step:479/2285 train_time:28830ms step_avg:60.19ms
step:480/2285 train_time:28889ms step_avg:60.19ms
step:481/2285 train_time:28950ms step_avg:60.19ms
step:482/2285 train_time:29008ms step_avg:60.18ms
step:483/2285 train_time:29069ms step_avg:60.18ms
step:484/2285 train_time:29128ms step_avg:60.18ms
step:485/2285 train_time:29189ms step_avg:60.18ms
step:486/2285 train_time:29248ms step_avg:60.18ms
step:487/2285 train_time:29309ms step_avg:60.18ms
step:488/2285 train_time:29368ms step_avg:60.18ms
step:489/2285 train_time:29430ms step_avg:60.18ms
step:490/2285 train_time:29488ms step_avg:60.18ms
step:491/2285 train_time:29549ms step_avg:60.18ms
step:492/2285 train_time:29609ms step_avg:60.18ms
step:493/2285 train_time:29670ms step_avg:60.18ms
step:494/2285 train_time:29729ms step_avg:60.18ms
step:495/2285 train_time:29791ms step_avg:60.18ms
step:496/2285 train_time:29850ms step_avg:60.18ms
step:497/2285 train_time:29910ms step_avg:60.18ms
step:498/2285 train_time:29969ms step_avg:60.18ms
step:499/2285 train_time:30030ms step_avg:60.18ms
step:500/2285 train_time:30088ms step_avg:60.18ms
step:500/2285 val_loss:3.8093 train_time:30151ms step_avg:60.30ms
step:501/2285 train_time:30169ms step_avg:60.22ms
step:502/2285 train_time:30210ms step_avg:60.18ms
step:503/2285 train_time:30275ms step_avg:60.19ms
step:504/2285 train_time:30336ms step_avg:60.19ms
step:505/2285 train_time:30398ms step_avg:60.19ms
step:506/2285 train_time:30457ms step_avg:60.19ms
step:507/2285 train_time:30518ms step_avg:60.19ms
step:508/2285 train_time:30576ms step_avg:60.19ms
step:509/2285 train_time:30637ms step_avg:60.19ms
step:510/2285 train_time:30695ms step_avg:60.19ms
step:511/2285 train_time:30756ms step_avg:60.19ms
step:512/2285 train_time:30815ms step_avg:60.18ms
step:513/2285 train_time:30876ms step_avg:60.19ms
step:514/2285 train_time:30935ms step_avg:60.18ms
step:515/2285 train_time:30996ms step_avg:60.19ms
step:516/2285 train_time:31056ms step_avg:60.19ms
step:517/2285 train_time:31119ms step_avg:60.19ms
step:518/2285 train_time:31179ms step_avg:60.19ms
step:519/2285 train_time:31242ms step_avg:60.20ms
step:520/2285 train_time:31301ms step_avg:60.19ms
step:521/2285 train_time:31362ms step_avg:60.20ms
step:522/2285 train_time:31421ms step_avg:60.19ms
step:523/2285 train_time:31481ms step_avg:60.19ms
step:524/2285 train_time:31540ms step_avg:60.19ms
step:525/2285 train_time:31601ms step_avg:60.19ms
step:526/2285 train_time:31659ms step_avg:60.19ms
step:527/2285 train_time:31719ms step_avg:60.19ms
step:528/2285 train_time:31778ms step_avg:60.19ms
step:529/2285 train_time:31838ms step_avg:60.19ms
step:530/2285 train_time:31897ms step_avg:60.18ms
step:531/2285 train_time:31958ms step_avg:60.18ms
step:532/2285 train_time:32017ms step_avg:60.18ms
step:533/2285 train_time:32079ms step_avg:60.19ms
step:534/2285 train_time:32139ms step_avg:60.18ms
step:535/2285 train_time:32201ms step_avg:60.19ms
step:536/2285 train_time:32260ms step_avg:60.19ms
step:537/2285 train_time:32322ms step_avg:60.19ms
step:538/2285 train_time:32381ms step_avg:60.19ms
step:539/2285 train_time:32442ms step_avg:60.19ms
step:540/2285 train_time:32501ms step_avg:60.19ms
step:541/2285 train_time:32561ms step_avg:60.19ms
step:542/2285 train_time:32620ms step_avg:60.18ms
step:543/2285 train_time:32681ms step_avg:60.19ms
step:544/2285 train_time:32739ms step_avg:60.18ms
step:545/2285 train_time:32800ms step_avg:60.18ms
step:546/2285 train_time:32858ms step_avg:60.18ms
step:547/2285 train_time:32919ms step_avg:60.18ms
step:548/2285 train_time:32978ms step_avg:60.18ms
step:549/2285 train_time:33039ms step_avg:60.18ms
step:550/2285 train_time:33098ms step_avg:60.18ms
step:551/2285 train_time:33160ms step_avg:60.18ms
step:552/2285 train_time:33219ms step_avg:60.18ms
step:553/2285 train_time:33281ms step_avg:60.18ms
step:554/2285 train_time:33340ms step_avg:60.18ms
step:555/2285 train_time:33402ms step_avg:60.18ms
step:556/2285 train_time:33460ms step_avg:60.18ms
step:557/2285 train_time:33521ms step_avg:60.18ms
step:558/2285 train_time:33580ms step_avg:60.18ms
step:559/2285 train_time:33641ms step_avg:60.18ms
step:560/2285 train_time:33700ms step_avg:60.18ms
step:561/2285 train_time:33760ms step_avg:60.18ms
step:562/2285 train_time:33819ms step_avg:60.18ms
step:563/2285 train_time:33880ms step_avg:60.18ms
step:564/2285 train_time:33939ms step_avg:60.18ms
step:565/2285 train_time:34000ms step_avg:60.18ms
step:566/2285 train_time:34059ms step_avg:60.17ms
step:567/2285 train_time:34120ms step_avg:60.18ms
step:568/2285 train_time:34179ms step_avg:60.17ms
step:569/2285 train_time:34241ms step_avg:60.18ms
step:570/2285 train_time:34300ms step_avg:60.18ms
step:571/2285 train_time:34362ms step_avg:60.18ms
step:572/2285 train_time:34421ms step_avg:60.18ms
step:573/2285 train_time:34481ms step_avg:60.18ms
step:574/2285 train_time:34540ms step_avg:60.17ms
step:575/2285 train_time:34601ms step_avg:60.18ms
step:576/2285 train_time:34660ms step_avg:60.17ms
step:577/2285 train_time:34721ms step_avg:60.17ms
step:578/2285 train_time:34779ms step_avg:60.17ms
step:579/2285 train_time:34840ms step_avg:60.17ms
step:580/2285 train_time:34899ms step_avg:60.17ms
step:581/2285 train_time:34960ms step_avg:60.17ms
step:582/2285 train_time:35018ms step_avg:60.17ms
step:583/2285 train_time:35080ms step_avg:60.17ms
step:584/2285 train_time:35139ms step_avg:60.17ms
step:585/2285 train_time:35201ms step_avg:60.17ms
step:586/2285 train_time:35259ms step_avg:60.17ms
step:587/2285 train_time:35321ms step_avg:60.17ms
step:588/2285 train_time:35380ms step_avg:60.17ms
step:589/2285 train_time:35441ms step_avg:60.17ms
step:590/2285 train_time:35500ms step_avg:60.17ms
step:591/2285 train_time:35561ms step_avg:60.17ms
step:592/2285 train_time:35620ms step_avg:60.17ms
step:593/2285 train_time:35681ms step_avg:60.17ms
step:594/2285 train_time:35739ms step_avg:60.17ms
step:595/2285 train_time:35800ms step_avg:60.17ms
step:596/2285 train_time:35859ms step_avg:60.17ms
step:597/2285 train_time:35919ms step_avg:60.17ms
step:598/2285 train_time:35978ms step_avg:60.16ms
step:599/2285 train_time:36039ms step_avg:60.17ms
step:600/2285 train_time:36099ms step_avg:60.16ms
step:601/2285 train_time:36160ms step_avg:60.17ms
step:602/2285 train_time:36219ms step_avg:60.16ms
step:603/2285 train_time:36280ms step_avg:60.17ms
step:604/2285 train_time:36339ms step_avg:60.16ms
step:605/2285 train_time:36401ms step_avg:60.17ms
step:606/2285 train_time:36459ms step_avg:60.16ms
step:607/2285 train_time:36521ms step_avg:60.17ms
step:608/2285 train_time:36579ms step_avg:60.16ms
step:609/2285 train_time:36640ms step_avg:60.16ms
step:610/2285 train_time:36699ms step_avg:60.16ms
step:611/2285 train_time:36760ms step_avg:60.16ms
step:612/2285 train_time:36818ms step_avg:60.16ms
step:613/2285 train_time:36879ms step_avg:60.16ms
step:614/2285 train_time:36938ms step_avg:60.16ms
step:615/2285 train_time:36999ms step_avg:60.16ms
step:616/2285 train_time:37059ms step_avg:60.16ms
step:617/2285 train_time:37120ms step_avg:60.16ms
step:618/2285 train_time:37178ms step_avg:60.16ms
step:619/2285 train_time:37240ms step_avg:60.16ms
step:620/2285 train_time:37299ms step_avg:60.16ms
step:621/2285 train_time:37361ms step_avg:60.16ms
step:622/2285 train_time:37419ms step_avg:60.16ms
step:623/2285 train_time:37481ms step_avg:60.16ms
step:624/2285 train_time:37539ms step_avg:60.16ms
step:625/2285 train_time:37601ms step_avg:60.16ms
step:626/2285 train_time:37659ms step_avg:60.16ms
step:627/2285 train_time:37721ms step_avg:60.16ms
step:628/2285 train_time:37779ms step_avg:60.16ms
step:629/2285 train_time:37841ms step_avg:60.16ms
step:630/2285 train_time:37899ms step_avg:60.16ms
step:631/2285 train_time:37960ms step_avg:60.16ms
step:632/2285 train_time:38019ms step_avg:60.16ms
step:633/2285 train_time:38080ms step_avg:60.16ms
step:634/2285 train_time:38139ms step_avg:60.16ms
step:635/2285 train_time:38200ms step_avg:60.16ms
step:636/2285 train_time:38259ms step_avg:60.16ms
step:637/2285 train_time:38320ms step_avg:60.16ms
step:638/2285 train_time:38379ms step_avg:60.15ms
step:639/2285 train_time:38440ms step_avg:60.16ms
step:640/2285 train_time:38500ms step_avg:60.16ms
step:641/2285 train_time:38561ms step_avg:60.16ms
step:642/2285 train_time:38620ms step_avg:60.16ms
step:643/2285 train_time:38681ms step_avg:60.16ms
step:644/2285 train_time:38740ms step_avg:60.16ms
step:645/2285 train_time:38802ms step_avg:60.16ms
step:646/2285 train_time:38860ms step_avg:60.16ms
step:647/2285 train_time:38921ms step_avg:60.16ms
step:648/2285 train_time:38980ms step_avg:60.15ms
step:649/2285 train_time:39041ms step_avg:60.16ms
step:650/2285 train_time:39099ms step_avg:60.15ms
step:651/2285 train_time:39160ms step_avg:60.15ms
step:652/2285 train_time:39219ms step_avg:60.15ms
step:653/2285 train_time:39280ms step_avg:60.15ms
step:654/2285 train_time:39339ms step_avg:60.15ms
step:655/2285 train_time:39401ms step_avg:60.15ms
step:656/2285 train_time:39459ms step_avg:60.15ms
step:657/2285 train_time:39521ms step_avg:60.15ms
step:658/2285 train_time:39579ms step_avg:60.15ms
step:659/2285 train_time:39641ms step_avg:60.15ms
step:660/2285 train_time:39700ms step_avg:60.15ms
step:661/2285 train_time:39761ms step_avg:60.15ms
step:662/2285 train_time:39819ms step_avg:60.15ms
step:663/2285 train_time:39880ms step_avg:60.15ms
step:664/2285 train_time:39939ms step_avg:60.15ms
step:665/2285 train_time:40000ms step_avg:60.15ms
step:666/2285 train_time:40059ms step_avg:60.15ms
step:667/2285 train_time:40120ms step_avg:60.15ms
step:668/2285 train_time:40180ms step_avg:60.15ms
step:669/2285 train_time:40241ms step_avg:60.15ms
step:670/2285 train_time:40300ms step_avg:60.15ms
step:671/2285 train_time:40361ms step_avg:60.15ms
step:672/2285 train_time:40419ms step_avg:60.15ms
step:673/2285 train_time:40481ms step_avg:60.15ms
step:674/2285 train_time:40539ms step_avg:60.15ms
step:675/2285 train_time:40600ms step_avg:60.15ms
step:676/2285 train_time:40659ms step_avg:60.15ms
step:677/2285 train_time:40721ms step_avg:60.15ms
step:678/2285 train_time:40780ms step_avg:60.15ms
step:679/2285 train_time:40841ms step_avg:60.15ms
step:680/2285 train_time:40900ms step_avg:60.15ms
step:681/2285 train_time:40960ms step_avg:60.15ms
step:682/2285 train_time:41019ms step_avg:60.15ms
step:683/2285 train_time:41081ms step_avg:60.15ms
step:684/2285 train_time:41140ms step_avg:60.15ms
step:685/2285 train_time:41201ms step_avg:60.15ms
step:686/2285 train_time:41259ms step_avg:60.14ms
step:687/2285 train_time:41320ms step_avg:60.15ms
step:688/2285 train_time:41379ms step_avg:60.14ms
step:689/2285 train_time:41441ms step_avg:60.15ms
step:690/2285 train_time:41500ms step_avg:60.14ms
step:691/2285 train_time:41561ms step_avg:60.15ms
step:692/2285 train_time:41619ms step_avg:60.14ms
step:693/2285 train_time:41681ms step_avg:60.15ms
step:694/2285 train_time:41739ms step_avg:60.14ms
step:695/2285 train_time:41801ms step_avg:60.14ms
step:696/2285 train_time:41859ms step_avg:60.14ms
step:697/2285 train_time:41920ms step_avg:60.14ms
step:698/2285 train_time:41979ms step_avg:60.14ms
step:699/2285 train_time:42040ms step_avg:60.14ms
step:700/2285 train_time:42099ms step_avg:60.14ms
step:701/2285 train_time:42160ms step_avg:60.14ms
step:702/2285 train_time:42219ms step_avg:60.14ms
step:703/2285 train_time:42280ms step_avg:60.14ms
step:704/2285 train_time:42339ms step_avg:60.14ms
step:705/2285 train_time:42400ms step_avg:60.14ms
step:706/2285 train_time:42459ms step_avg:60.14ms
step:707/2285 train_time:42520ms step_avg:60.14ms
step:708/2285 train_time:42579ms step_avg:60.14ms
step:709/2285 train_time:42640ms step_avg:60.14ms
step:710/2285 train_time:42699ms step_avg:60.14ms
step:711/2285 train_time:42760ms step_avg:60.14ms
step:712/2285 train_time:42819ms step_avg:60.14ms
step:713/2285 train_time:42880ms step_avg:60.14ms
step:714/2285 train_time:42938ms step_avg:60.14ms
step:715/2285 train_time:43000ms step_avg:60.14ms
step:716/2285 train_time:43059ms step_avg:60.14ms
step:717/2285 train_time:43121ms step_avg:60.14ms
step:718/2285 train_time:43179ms step_avg:60.14ms
step:719/2285 train_time:43240ms step_avg:60.14ms
step:720/2285 train_time:43299ms step_avg:60.14ms
step:721/2285 train_time:43360ms step_avg:60.14ms
step:722/2285 train_time:43419ms step_avg:60.14ms
step:723/2285 train_time:43481ms step_avg:60.14ms
step:724/2285 train_time:43539ms step_avg:60.14ms
step:725/2285 train_time:43601ms step_avg:60.14ms
step:726/2285 train_time:43659ms step_avg:60.14ms
step:727/2285 train_time:43720ms step_avg:60.14ms
step:728/2285 train_time:43778ms step_avg:60.14ms
step:729/2285 train_time:43840ms step_avg:60.14ms
step:730/2285 train_time:43899ms step_avg:60.14ms
step:731/2285 train_time:43961ms step_avg:60.14ms
step:732/2285 train_time:44019ms step_avg:60.14ms
step:733/2285 train_time:44081ms step_avg:60.14ms
step:734/2285 train_time:44140ms step_avg:60.14ms
step:735/2285 train_time:44201ms step_avg:60.14ms
step:736/2285 train_time:44260ms step_avg:60.14ms
step:737/2285 train_time:44321ms step_avg:60.14ms
step:738/2285 train_time:44379ms step_avg:60.13ms
step:739/2285 train_time:44441ms step_avg:60.14ms
step:740/2285 train_time:44499ms step_avg:60.13ms
step:741/2285 train_time:44561ms step_avg:60.14ms
step:742/2285 train_time:44619ms step_avg:60.13ms
step:743/2285 train_time:44680ms step_avg:60.14ms
step:744/2285 train_time:44739ms step_avg:60.13ms
step:745/2285 train_time:44800ms step_avg:60.13ms
step:746/2285 train_time:44858ms step_avg:60.13ms
step:747/2285 train_time:44920ms step_avg:60.13ms
step:748/2285 train_time:44978ms step_avg:60.13ms
step:749/2285 train_time:45040ms step_avg:60.13ms
step:750/2285 train_time:45099ms step_avg:60.13ms
step:750/2285 val_loss:3.6781 train_time:45162ms step_avg:60.22ms
step:751/2285 train_time:45181ms step_avg:60.16ms
step:752/2285 train_time:45224ms step_avg:60.14ms
step:753/2285 train_time:45286ms step_avg:60.14ms
step:754/2285 train_time:45347ms step_avg:60.14ms
step:755/2285 train_time:45410ms step_avg:60.15ms
step:756/2285 train_time:45470ms step_avg:60.14ms
step:757/2285 train_time:45531ms step_avg:60.15ms
step:758/2285 train_time:45590ms step_avg:60.15ms
step:759/2285 train_time:45651ms step_avg:60.15ms
step:760/2285 train_time:45710ms step_avg:60.15ms
step:761/2285 train_time:45771ms step_avg:60.15ms
step:762/2285 train_time:45831ms step_avg:60.15ms
step:763/2285 train_time:45891ms step_avg:60.15ms
step:764/2285 train_time:45950ms step_avg:60.14ms
step:765/2285 train_time:46011ms step_avg:60.14ms
step:766/2285 train_time:46075ms step_avg:60.15ms
step:767/2285 train_time:46141ms step_avg:60.16ms
step:768/2285 train_time:46202ms step_avg:60.16ms
step:769/2285 train_time:46265ms step_avg:60.16ms
step:770/2285 train_time:46325ms step_avg:60.16ms
step:771/2285 train_time:46387ms step_avg:60.16ms
step:772/2285 train_time:46447ms step_avg:60.16ms
step:773/2285 train_time:46509ms step_avg:60.17ms
step:774/2285 train_time:46568ms step_avg:60.17ms
step:775/2285 train_time:46629ms step_avg:60.17ms
step:776/2285 train_time:46689ms step_avg:60.17ms
step:777/2285 train_time:46751ms step_avg:60.17ms
step:778/2285 train_time:46810ms step_avg:60.17ms
step:779/2285 train_time:46871ms step_avg:60.17ms
step:780/2285 train_time:46930ms step_avg:60.17ms
step:781/2285 train_time:46992ms step_avg:60.17ms
step:782/2285 train_time:47054ms step_avg:60.17ms
step:783/2285 train_time:47118ms step_avg:60.18ms
step:784/2285 train_time:47178ms step_avg:60.18ms
step:785/2285 train_time:47242ms step_avg:60.18ms
step:786/2285 train_time:47302ms step_avg:60.18ms
step:787/2285 train_time:47364ms step_avg:60.18ms
step:788/2285 train_time:47424ms step_avg:60.18ms
step:789/2285 train_time:47486ms step_avg:60.18ms
step:790/2285 train_time:47545ms step_avg:60.18ms
step:791/2285 train_time:47606ms step_avg:60.18ms
step:792/2285 train_time:47665ms step_avg:60.18ms
step:793/2285 train_time:47726ms step_avg:60.18ms
step:794/2285 train_time:47785ms step_avg:60.18ms
step:795/2285 train_time:47846ms step_avg:60.18ms
step:796/2285 train_time:47905ms step_avg:60.18ms
step:797/2285 train_time:47967ms step_avg:60.19ms
step:798/2285 train_time:48028ms step_avg:60.19ms
step:799/2285 train_time:48091ms step_avg:60.19ms
step:800/2285 train_time:48152ms step_avg:60.19ms
step:801/2285 train_time:48216ms step_avg:60.19ms
step:802/2285 train_time:48276ms step_avg:60.19ms
step:803/2285 train_time:48338ms step_avg:60.20ms
step:804/2285 train_time:48398ms step_avg:60.20ms
step:805/2285 train_time:48461ms step_avg:60.20ms
step:806/2285 train_time:48520ms step_avg:60.20ms
step:807/2285 train_time:48582ms step_avg:60.20ms
step:808/2285 train_time:48641ms step_avg:60.20ms
step:809/2285 train_time:48704ms step_avg:60.20ms
step:810/2285 train_time:48763ms step_avg:60.20ms
step:811/2285 train_time:48824ms step_avg:60.20ms
step:812/2285 train_time:48883ms step_avg:60.20ms
step:813/2285 train_time:48945ms step_avg:60.20ms
step:814/2285 train_time:49005ms step_avg:60.20ms
step:815/2285 train_time:49067ms step_avg:60.20ms
step:816/2285 train_time:49126ms step_avg:60.20ms
step:817/2285 train_time:49190ms step_avg:60.21ms
step:818/2285 train_time:49251ms step_avg:60.21ms
step:819/2285 train_time:49314ms step_avg:60.21ms
step:820/2285 train_time:49375ms step_avg:60.21ms
step:821/2285 train_time:49437ms step_avg:60.22ms
step:822/2285 train_time:49498ms step_avg:60.22ms
step:823/2285 train_time:49560ms step_avg:60.22ms
step:824/2285 train_time:49620ms step_avg:60.22ms
step:825/2285 train_time:49682ms step_avg:60.22ms
step:826/2285 train_time:49741ms step_avg:60.22ms
step:827/2285 train_time:49803ms step_avg:60.22ms
step:828/2285 train_time:49862ms step_avg:60.22ms
step:829/2285 train_time:49924ms step_avg:60.22ms
step:830/2285 train_time:49983ms step_avg:60.22ms
step:831/2285 train_time:50046ms step_avg:60.22ms
step:832/2285 train_time:50105ms step_avg:60.22ms
step:833/2285 train_time:50167ms step_avg:60.22ms
step:834/2285 train_time:50227ms step_avg:60.22ms
step:835/2285 train_time:50290ms step_avg:60.23ms
step:836/2285 train_time:50350ms step_avg:60.23ms
step:837/2285 train_time:50413ms step_avg:60.23ms
step:838/2285 train_time:50473ms step_avg:60.23ms
step:839/2285 train_time:50535ms step_avg:60.23ms
step:840/2285 train_time:50595ms step_avg:60.23ms
step:841/2285 train_time:50658ms step_avg:60.23ms
step:842/2285 train_time:50717ms step_avg:60.23ms
step:843/2285 train_time:50779ms step_avg:60.24ms
step:844/2285 train_time:50839ms step_avg:60.24ms
step:845/2285 train_time:50902ms step_avg:60.24ms
step:846/2285 train_time:50962ms step_avg:60.24ms
step:847/2285 train_time:51024ms step_avg:60.24ms
step:848/2285 train_time:51084ms step_avg:60.24ms
step:849/2285 train_time:51146ms step_avg:60.24ms
step:850/2285 train_time:51205ms step_avg:60.24ms
step:851/2285 train_time:51267ms step_avg:60.24ms
step:852/2285 train_time:51326ms step_avg:60.24ms
step:853/2285 train_time:51388ms step_avg:60.24ms
step:854/2285 train_time:51449ms step_avg:60.24ms
step:855/2285 train_time:51512ms step_avg:60.25ms
step:856/2285 train_time:51572ms step_avg:60.25ms
step:857/2285 train_time:51634ms step_avg:60.25ms
step:858/2285 train_time:51694ms step_avg:60.25ms
step:859/2285 train_time:51757ms step_avg:60.25ms
step:860/2285 train_time:51817ms step_avg:60.25ms
step:861/2285 train_time:51879ms step_avg:60.25ms
step:862/2285 train_time:51939ms step_avg:60.25ms
step:863/2285 train_time:52002ms step_avg:60.26ms
step:864/2285 train_time:52061ms step_avg:60.26ms
step:865/2285 train_time:52123ms step_avg:60.26ms
step:866/2285 train_time:52183ms step_avg:60.26ms
step:867/2285 train_time:52244ms step_avg:60.26ms
step:868/2285 train_time:52304ms step_avg:60.26ms
step:869/2285 train_time:52366ms step_avg:60.26ms
step:870/2285 train_time:52425ms step_avg:60.26ms
step:871/2285 train_time:52487ms step_avg:60.26ms
step:872/2285 train_time:52547ms step_avg:60.26ms
step:873/2285 train_time:52609ms step_avg:60.26ms
step:874/2285 train_time:52670ms step_avg:60.26ms
step:875/2285 train_time:52731ms step_avg:60.26ms
step:876/2285 train_time:52792ms step_avg:60.26ms
step:877/2285 train_time:52855ms step_avg:60.27ms
step:878/2285 train_time:52915ms step_avg:60.27ms
step:879/2285 train_time:52978ms step_avg:60.27ms
step:880/2285 train_time:53039ms step_avg:60.27ms
step:881/2285 train_time:53102ms step_avg:60.27ms
step:882/2285 train_time:53162ms step_avg:60.27ms
step:883/2285 train_time:53223ms step_avg:60.28ms
step:884/2285 train_time:53283ms step_avg:60.27ms
step:885/2285 train_time:53346ms step_avg:60.28ms
step:886/2285 train_time:53405ms step_avg:60.28ms
step:887/2285 train_time:53467ms step_avg:60.28ms
step:888/2285 train_time:53527ms step_avg:60.28ms
step:889/2285 train_time:53589ms step_avg:60.28ms
step:890/2285 train_time:53649ms step_avg:60.28ms
step:891/2285 train_time:53710ms step_avg:60.28ms
step:892/2285 train_time:53771ms step_avg:60.28ms
step:893/2285 train_time:53834ms step_avg:60.28ms
step:894/2285 train_time:53894ms step_avg:60.28ms
step:895/2285 train_time:53956ms step_avg:60.29ms
step:896/2285 train_time:54016ms step_avg:60.29ms
step:897/2285 train_time:54079ms step_avg:60.29ms
step:898/2285 train_time:54140ms step_avg:60.29ms
step:899/2285 train_time:54203ms step_avg:60.29ms
step:900/2285 train_time:54262ms step_avg:60.29ms
step:901/2285 train_time:54325ms step_avg:60.29ms
step:902/2285 train_time:54384ms step_avg:60.29ms
step:903/2285 train_time:54446ms step_avg:60.29ms
step:904/2285 train_time:54505ms step_avg:60.29ms
step:905/2285 train_time:54567ms step_avg:60.30ms
step:906/2285 train_time:54626ms step_avg:60.29ms
step:907/2285 train_time:54688ms step_avg:60.30ms
step:908/2285 train_time:54749ms step_avg:60.30ms
step:909/2285 train_time:54811ms step_avg:60.30ms
step:910/2285 train_time:54872ms step_avg:60.30ms
step:911/2285 train_time:54935ms step_avg:60.30ms
step:912/2285 train_time:54995ms step_avg:60.30ms
step:913/2285 train_time:55058ms step_avg:60.30ms
step:914/2285 train_time:55118ms step_avg:60.30ms
step:915/2285 train_time:55181ms step_avg:60.31ms
step:916/2285 train_time:55241ms step_avg:60.31ms
step:917/2285 train_time:55303ms step_avg:60.31ms
step:918/2285 train_time:55363ms step_avg:60.31ms
step:919/2285 train_time:55425ms step_avg:60.31ms
step:920/2285 train_time:55484ms step_avg:60.31ms
step:921/2285 train_time:55547ms step_avg:60.31ms
step:922/2285 train_time:55607ms step_avg:60.31ms
step:923/2285 train_time:55668ms step_avg:60.31ms
step:924/2285 train_time:55728ms step_avg:60.31ms
step:925/2285 train_time:55790ms step_avg:60.31ms
step:926/2285 train_time:55849ms step_avg:60.31ms
step:927/2285 train_time:55912ms step_avg:60.32ms
step:928/2285 train_time:55974ms step_avg:60.32ms
step:929/2285 train_time:56036ms step_avg:60.32ms
step:930/2285 train_time:56096ms step_avg:60.32ms
step:931/2285 train_time:56159ms step_avg:60.32ms
step:932/2285 train_time:56218ms step_avg:60.32ms
step:933/2285 train_time:56281ms step_avg:60.32ms
step:934/2285 train_time:56340ms step_avg:60.32ms
step:935/2285 train_time:56402ms step_avg:60.32ms
step:936/2285 train_time:56462ms step_avg:60.32ms
step:937/2285 train_time:56524ms step_avg:60.32ms
step:938/2285 train_time:56583ms step_avg:60.32ms
step:939/2285 train_time:56645ms step_avg:60.33ms
step:940/2285 train_time:56704ms step_avg:60.32ms
step:941/2285 train_time:56766ms step_avg:60.33ms
step:942/2285 train_time:56826ms step_avg:60.32ms
step:943/2285 train_time:56888ms step_avg:60.33ms
step:944/2285 train_time:56949ms step_avg:60.33ms
step:945/2285 train_time:57011ms step_avg:60.33ms
step:946/2285 train_time:57071ms step_avg:60.33ms
step:947/2285 train_time:57133ms step_avg:60.33ms
step:948/2285 train_time:57193ms step_avg:60.33ms
step:949/2285 train_time:57256ms step_avg:60.33ms
step:950/2285 train_time:57316ms step_avg:60.33ms
step:951/2285 train_time:57379ms step_avg:60.34ms
step:952/2285 train_time:57439ms step_avg:60.34ms
step:953/2285 train_time:57502ms step_avg:60.34ms
step:954/2285 train_time:57562ms step_avg:60.34ms
step:955/2285 train_time:57624ms step_avg:60.34ms
step:956/2285 train_time:57683ms step_avg:60.34ms
step:957/2285 train_time:57745ms step_avg:60.34ms
step:958/2285 train_time:57805ms step_avg:60.34ms
step:959/2285 train_time:57867ms step_avg:60.34ms
step:960/2285 train_time:57926ms step_avg:60.34ms
step:961/2285 train_time:57988ms step_avg:60.34ms
step:962/2285 train_time:58049ms step_avg:60.34ms
step:963/2285 train_time:58111ms step_avg:60.34ms
step:964/2285 train_time:58172ms step_avg:60.34ms
step:965/2285 train_time:58235ms step_avg:60.35ms
step:966/2285 train_time:58295ms step_avg:60.35ms
step:967/2285 train_time:58357ms step_avg:60.35ms
step:968/2285 train_time:58417ms step_avg:60.35ms
step:969/2285 train_time:58480ms step_avg:60.35ms
step:970/2285 train_time:58540ms step_avg:60.35ms
step:971/2285 train_time:58602ms step_avg:60.35ms
step:972/2285 train_time:58662ms step_avg:60.35ms
step:973/2285 train_time:58724ms step_avg:60.35ms
step:974/2285 train_time:58783ms step_avg:60.35ms
step:975/2285 train_time:58845ms step_avg:60.35ms
step:976/2285 train_time:58905ms step_avg:60.35ms
step:977/2285 train_time:58967ms step_avg:60.35ms
step:978/2285 train_time:59026ms step_avg:60.35ms
step:979/2285 train_time:59088ms step_avg:60.36ms
step:980/2285 train_time:59149ms step_avg:60.36ms
step:981/2285 train_time:59211ms step_avg:60.36ms
step:982/2285 train_time:59272ms step_avg:60.36ms
step:983/2285 train_time:59335ms step_avg:60.36ms
step:984/2285 train_time:59395ms step_avg:60.36ms
step:985/2285 train_time:59457ms step_avg:60.36ms
step:986/2285 train_time:59517ms step_avg:60.36ms
step:987/2285 train_time:59580ms step_avg:60.37ms
step:988/2285 train_time:59640ms step_avg:60.36ms
step:989/2285 train_time:59703ms step_avg:60.37ms
step:990/2285 train_time:59762ms step_avg:60.37ms
step:991/2285 train_time:59824ms step_avg:60.37ms
step:992/2285 train_time:59883ms step_avg:60.37ms
step:993/2285 train_time:59946ms step_avg:60.37ms
step:994/2285 train_time:60005ms step_avg:60.37ms
step:995/2285 train_time:60067ms step_avg:60.37ms
step:996/2285 train_time:60126ms step_avg:60.37ms
step:997/2285 train_time:60189ms step_avg:60.37ms
step:998/2285 train_time:60250ms step_avg:60.37ms
step:999/2285 train_time:60312ms step_avg:60.37ms
step:1000/2285 train_time:60373ms step_avg:60.37ms
step:1000/2285 val_loss:3.5692 train_time:60437ms step_avg:60.44ms
step:1001/2285 train_time:60455ms step_avg:60.39ms
step:1002/2285 train_time:60499ms step_avg:60.38ms
step:1003/2285 train_time:60566ms step_avg:60.39ms
step:1004/2285 train_time:60630ms step_avg:60.39ms
step:1005/2285 train_time:60694ms step_avg:60.39ms
step:1006/2285 train_time:60753ms step_avg:60.39ms
step:1007/2285 train_time:60814ms step_avg:60.39ms
step:1008/2285 train_time:60874ms step_avg:60.39ms
step:1009/2285 train_time:60936ms step_avg:60.39ms
step:1010/2285 train_time:60996ms step_avg:60.39ms
step:1011/2285 train_time:61057ms step_avg:60.39ms
step:1012/2285 train_time:61116ms step_avg:60.39ms
step:1013/2285 train_time:61177ms step_avg:60.39ms
step:1014/2285 train_time:61236ms step_avg:60.39ms
step:1015/2285 train_time:61298ms step_avg:60.39ms
step:1016/2285 train_time:61356ms step_avg:60.39ms
step:1017/2285 train_time:61419ms step_avg:60.39ms
step:1018/2285 train_time:61480ms step_avg:60.39ms
step:1019/2285 train_time:61543ms step_avg:60.40ms
step:1020/2285 train_time:61605ms step_avg:60.40ms
step:1021/2285 train_time:61668ms step_avg:60.40ms
step:1022/2285 train_time:61728ms step_avg:60.40ms
step:1023/2285 train_time:61790ms step_avg:60.40ms
step:1024/2285 train_time:61850ms step_avg:60.40ms
step:1025/2285 train_time:61912ms step_avg:60.40ms
step:1026/2285 train_time:61972ms step_avg:60.40ms
step:1027/2285 train_time:62035ms step_avg:60.40ms
step:1028/2285 train_time:62094ms step_avg:60.40ms
step:1029/2285 train_time:62156ms step_avg:60.40ms
step:1030/2285 train_time:62215ms step_avg:60.40ms
step:1031/2285 train_time:62277ms step_avg:60.40ms
step:1032/2285 train_time:62335ms step_avg:60.40ms
step:1033/2285 train_time:62398ms step_avg:60.40ms
step:1034/2285 train_time:62458ms step_avg:60.40ms
step:1035/2285 train_time:62520ms step_avg:60.41ms
step:1036/2285 train_time:62580ms step_avg:60.41ms
step:1037/2285 train_time:62643ms step_avg:60.41ms
step:1038/2285 train_time:62704ms step_avg:60.41ms
step:1039/2285 train_time:62767ms step_avg:60.41ms
step:1040/2285 train_time:62827ms step_avg:60.41ms
step:1041/2285 train_time:62890ms step_avg:60.41ms
step:1042/2285 train_time:62950ms step_avg:60.41ms
step:1043/2285 train_time:63012ms step_avg:60.41ms
step:1044/2285 train_time:63072ms step_avg:60.41ms
step:1045/2285 train_time:63134ms step_avg:60.42ms
step:1046/2285 train_time:63194ms step_avg:60.42ms
step:1047/2285 train_time:63256ms step_avg:60.42ms
step:1048/2285 train_time:63315ms step_avg:60.42ms
step:1049/2285 train_time:63378ms step_avg:60.42ms
step:1050/2285 train_time:63437ms step_avg:60.42ms
step:1051/2285 train_time:63500ms step_avg:60.42ms
step:1052/2285 train_time:63559ms step_avg:60.42ms
step:1053/2285 train_time:63621ms step_avg:60.42ms
step:1054/2285 train_time:63681ms step_avg:60.42ms
step:1055/2285 train_time:63744ms step_avg:60.42ms
step:1056/2285 train_time:63804ms step_avg:60.42ms
step:1057/2285 train_time:63867ms step_avg:60.42ms
step:1058/2285 train_time:63928ms step_avg:60.42ms
step:1059/2285 train_time:63991ms step_avg:60.43ms
step:1060/2285 train_time:64051ms step_avg:60.43ms
step:1061/2285 train_time:64113ms step_avg:60.43ms
step:1062/2285 train_time:64173ms step_avg:60.43ms
step:1063/2285 train_time:64235ms step_avg:60.43ms
step:1064/2285 train_time:64295ms step_avg:60.43ms
step:1065/2285 train_time:64357ms step_avg:60.43ms
step:1066/2285 train_time:64416ms step_avg:60.43ms
step:1067/2285 train_time:64479ms step_avg:60.43ms
step:1068/2285 train_time:64539ms step_avg:60.43ms
step:1069/2285 train_time:64601ms step_avg:60.43ms
step:1070/2285 train_time:64660ms step_avg:60.43ms
step:1071/2285 train_time:64723ms step_avg:60.43ms
step:1072/2285 train_time:64782ms step_avg:60.43ms
step:1073/2285 train_time:64845ms step_avg:60.43ms
step:1074/2285 train_time:64906ms step_avg:60.43ms
step:1075/2285 train_time:64968ms step_avg:60.44ms
step:1076/2285 train_time:65029ms step_avg:60.44ms
step:1077/2285 train_time:65091ms step_avg:60.44ms
step:1078/2285 train_time:65150ms step_avg:60.44ms
step:1079/2285 train_time:65213ms step_avg:60.44ms
step:1080/2285 train_time:65273ms step_avg:60.44ms
step:1081/2285 train_time:65336ms step_avg:60.44ms
step:1082/2285 train_time:65396ms step_avg:60.44ms
step:1083/2285 train_time:65458ms step_avg:60.44ms
step:1084/2285 train_time:65518ms step_avg:60.44ms
step:1085/2285 train_time:65581ms step_avg:60.44ms
step:1086/2285 train_time:65640ms step_avg:60.44ms
step:1087/2285 train_time:65703ms step_avg:60.44ms
step:1088/2285 train_time:65762ms step_avg:60.44ms
step:1089/2285 train_time:65824ms step_avg:60.44ms
step:1090/2285 train_time:65885ms step_avg:60.44ms
step:1091/2285 train_time:65948ms step_avg:60.45ms
step:1092/2285 train_time:66008ms step_avg:60.45ms
step:1093/2285 train_time:66070ms step_avg:60.45ms
step:1094/2285 train_time:66130ms step_avg:60.45ms
step:1095/2285 train_time:66193ms step_avg:60.45ms
step:1096/2285 train_time:66252ms step_avg:60.45ms
step:1097/2285 train_time:66315ms step_avg:60.45ms
step:1098/2285 train_time:66376ms step_avg:60.45ms
step:1099/2285 train_time:66438ms step_avg:60.45ms
step:1100/2285 train_time:66498ms step_avg:60.45ms
step:1101/2285 train_time:66561ms step_avg:60.45ms
step:1102/2285 train_time:66620ms step_avg:60.45ms
step:1103/2285 train_time:66683ms step_avg:60.46ms
step:1104/2285 train_time:66742ms step_avg:60.45ms
step:1105/2285 train_time:66804ms step_avg:60.46ms
step:1106/2285 train_time:66864ms step_avg:60.46ms
step:1107/2285 train_time:66927ms step_avg:60.46ms
step:1108/2285 train_time:66987ms step_avg:60.46ms
step:1109/2285 train_time:67050ms step_avg:60.46ms
step:1110/2285 train_time:67110ms step_avg:60.46ms
step:1111/2285 train_time:67173ms step_avg:60.46ms
step:1112/2285 train_time:67233ms step_avg:60.46ms
step:1113/2285 train_time:67295ms step_avg:60.46ms
step:1114/2285 train_time:67355ms step_avg:60.46ms
step:1115/2285 train_time:67417ms step_avg:60.46ms
step:1116/2285 train_time:67477ms step_avg:60.46ms
step:1117/2285 train_time:67539ms step_avg:60.46ms
step:1118/2285 train_time:67599ms step_avg:60.46ms
step:1119/2285 train_time:67661ms step_avg:60.47ms
step:1120/2285 train_time:67720ms step_avg:60.46ms
step:1121/2285 train_time:67782ms step_avg:60.47ms
step:1122/2285 train_time:67842ms step_avg:60.47ms
step:1123/2285 train_time:67905ms step_avg:60.47ms
step:1124/2285 train_time:67965ms step_avg:60.47ms
step:1125/2285 train_time:68027ms step_avg:60.47ms
step:1126/2285 train_time:68088ms step_avg:60.47ms
step:1127/2285 train_time:68151ms step_avg:60.47ms
step:1128/2285 train_time:68211ms step_avg:60.47ms
step:1129/2285 train_time:68274ms step_avg:60.47ms
step:1130/2285 train_time:68334ms step_avg:60.47ms
step:1131/2285 train_time:68396ms step_avg:60.47ms
step:1132/2285 train_time:68455ms step_avg:60.47ms
step:1133/2285 train_time:68518ms step_avg:60.47ms
step:1134/2285 train_time:68578ms step_avg:60.47ms
step:1135/2285 train_time:68640ms step_avg:60.48ms
step:1136/2285 train_time:68699ms step_avg:60.47ms
step:1137/2285 train_time:68761ms step_avg:60.48ms
step:1138/2285 train_time:68820ms step_avg:60.47ms
step:1139/2285 train_time:68882ms step_avg:60.48ms
step:1140/2285 train_time:68943ms step_avg:60.48ms
step:1141/2285 train_time:69005ms step_avg:60.48ms
step:1142/2285 train_time:69065ms step_avg:60.48ms
step:1143/2285 train_time:69129ms step_avg:60.48ms
step:1144/2285 train_time:69190ms step_avg:60.48ms
step:1145/2285 train_time:69252ms step_avg:60.48ms
step:1146/2285 train_time:69312ms step_avg:60.48ms
step:1147/2285 train_time:69375ms step_avg:60.48ms
step:1148/2285 train_time:69435ms step_avg:60.48ms
step:1149/2285 train_time:69498ms step_avg:60.49ms
step:1150/2285 train_time:69558ms step_avg:60.48ms
step:1151/2285 train_time:69620ms step_avg:60.49ms
step:1152/2285 train_time:69680ms step_avg:60.49ms
step:1153/2285 train_time:69742ms step_avg:60.49ms
step:1154/2285 train_time:69801ms step_avg:60.49ms
step:1155/2285 train_time:69863ms step_avg:60.49ms
step:1156/2285 train_time:69923ms step_avg:60.49ms
step:1157/2285 train_time:69986ms step_avg:60.49ms
step:1158/2285 train_time:70046ms step_avg:60.49ms
step:1159/2285 train_time:70109ms step_avg:60.49ms
step:1160/2285 train_time:70170ms step_avg:60.49ms
step:1161/2285 train_time:70232ms step_avg:60.49ms
step:1162/2285 train_time:70292ms step_avg:60.49ms
step:1163/2285 train_time:70355ms step_avg:60.49ms
step:1164/2285 train_time:70415ms step_avg:60.49ms
step:1165/2285 train_time:70477ms step_avg:60.50ms
step:1166/2285 train_time:70537ms step_avg:60.49ms
step:1167/2285 train_time:70599ms step_avg:60.50ms
step:1168/2285 train_time:70658ms step_avg:60.49ms
step:1169/2285 train_time:70720ms step_avg:60.50ms
step:1170/2285 train_time:70780ms step_avg:60.50ms
step:1171/2285 train_time:70842ms step_avg:60.50ms
step:1172/2285 train_time:70901ms step_avg:60.50ms
step:1173/2285 train_time:70963ms step_avg:60.50ms
step:1174/2285 train_time:71023ms step_avg:60.50ms
step:1175/2285 train_time:71086ms step_avg:60.50ms
step:1176/2285 train_time:71147ms step_avg:60.50ms
step:1177/2285 train_time:71210ms step_avg:60.50ms
step:1178/2285 train_time:71270ms step_avg:60.50ms
step:1179/2285 train_time:71334ms step_avg:60.50ms
step:1180/2285 train_time:71394ms step_avg:60.50ms
step:1181/2285 train_time:71456ms step_avg:60.50ms
step:1182/2285 train_time:71516ms step_avg:60.50ms
step:1183/2285 train_time:71578ms step_avg:60.51ms
step:1184/2285 train_time:71638ms step_avg:60.51ms
step:1185/2285 train_time:71700ms step_avg:60.51ms
step:1186/2285 train_time:71760ms step_avg:60.51ms
step:1187/2285 train_time:71823ms step_avg:60.51ms
step:1188/2285 train_time:71882ms step_avg:60.51ms
step:1189/2285 train_time:71945ms step_avg:60.51ms
step:1190/2285 train_time:72004ms step_avg:60.51ms
step:1191/2285 train_time:72066ms step_avg:60.51ms
step:1192/2285 train_time:72127ms step_avg:60.51ms
step:1193/2285 train_time:72190ms step_avg:60.51ms
step:1194/2285 train_time:72250ms step_avg:60.51ms
step:1195/2285 train_time:72313ms step_avg:60.51ms
step:1196/2285 train_time:72373ms step_avg:60.51ms
step:1197/2285 train_time:72436ms step_avg:60.51ms
step:1198/2285 train_time:72496ms step_avg:60.51ms
step:1199/2285 train_time:72558ms step_avg:60.52ms
step:1200/2285 train_time:72617ms step_avg:60.51ms
step:1201/2285 train_time:72679ms step_avg:60.52ms
step:1202/2285 train_time:72739ms step_avg:60.51ms
step:1203/2285 train_time:72801ms step_avg:60.52ms
step:1204/2285 train_time:72860ms step_avg:60.51ms
step:1205/2285 train_time:72922ms step_avg:60.52ms
step:1206/2285 train_time:72981ms step_avg:60.52ms
step:1207/2285 train_time:73044ms step_avg:60.52ms
step:1208/2285 train_time:73104ms step_avg:60.52ms
step:1209/2285 train_time:73167ms step_avg:60.52ms
step:1210/2285 train_time:73228ms step_avg:60.52ms
step:1211/2285 train_time:73291ms step_avg:60.52ms
step:1212/2285 train_time:73351ms step_avg:60.52ms
step:1213/2285 train_time:73413ms step_avg:60.52ms
step:1214/2285 train_time:73474ms step_avg:60.52ms
step:1215/2285 train_time:73536ms step_avg:60.52ms
step:1216/2285 train_time:73596ms step_avg:60.52ms
step:1217/2285 train_time:73658ms step_avg:60.52ms
step:1218/2285 train_time:73718ms step_avg:60.52ms
step:1219/2285 train_time:73780ms step_avg:60.53ms
step:1220/2285 train_time:73840ms step_avg:60.52ms
step:1221/2285 train_time:73901ms step_avg:60.52ms
step:1222/2285 train_time:73960ms step_avg:60.52ms
step:1223/2285 train_time:74022ms step_avg:60.53ms
step:1224/2285 train_time:74082ms step_avg:60.52ms
step:1225/2285 train_time:74145ms step_avg:60.53ms
step:1226/2285 train_time:74205ms step_avg:60.53ms
step:1227/2285 train_time:74268ms step_avg:60.53ms
step:1228/2285 train_time:74329ms step_avg:60.53ms
step:1229/2285 train_time:74392ms step_avg:60.53ms
step:1230/2285 train_time:74452ms step_avg:60.53ms
step:1231/2285 train_time:74515ms step_avg:60.53ms
step:1232/2285 train_time:74576ms step_avg:60.53ms
step:1233/2285 train_time:74638ms step_avg:60.53ms
step:1234/2285 train_time:74698ms step_avg:60.53ms
step:1235/2285 train_time:74761ms step_avg:60.53ms
step:1236/2285 train_time:74821ms step_avg:60.53ms
step:1237/2285 train_time:74883ms step_avg:60.54ms
step:1238/2285 train_time:74942ms step_avg:60.54ms
step:1239/2285 train_time:75004ms step_avg:60.54ms
step:1240/2285 train_time:75064ms step_avg:60.54ms
step:1241/2285 train_time:75126ms step_avg:60.54ms
step:1242/2285 train_time:75186ms step_avg:60.54ms
step:1243/2285 train_time:75250ms step_avg:60.54ms
step:1244/2285 train_time:75310ms step_avg:60.54ms
step:1245/2285 train_time:75372ms step_avg:60.54ms
step:1246/2285 train_time:75432ms step_avg:60.54ms
step:1247/2285 train_time:75495ms step_avg:60.54ms
step:1248/2285 train_time:75555ms step_avg:60.54ms
step:1249/2285 train_time:75619ms step_avg:60.54ms
step:1250/2285 train_time:75679ms step_avg:60.54ms
step:1250/2285 val_loss:3.5022 train_time:75742ms step_avg:60.59ms
step:1251/2285 train_time:75763ms step_avg:60.56ms
step:1252/2285 train_time:75804ms step_avg:60.55ms
step:1253/2285 train_time:75872ms step_avg:60.55ms
step:1254/2285 train_time:75932ms step_avg:60.55ms
step:1255/2285 train_time:75995ms step_avg:60.55ms
step:1256/2285 train_time:76054ms step_avg:60.55ms
step:1257/2285 train_time:76116ms step_avg:60.55ms
step:1258/2285 train_time:76175ms step_avg:60.55ms
step:1259/2285 train_time:76237ms step_avg:60.55ms
step:1260/2285 train_time:76296ms step_avg:60.55ms
step:1261/2285 train_time:76357ms step_avg:60.55ms
step:1262/2285 train_time:76416ms step_avg:60.55ms
step:1263/2285 train_time:76478ms step_avg:60.55ms
step:1264/2285 train_time:76538ms step_avg:60.55ms
step:1265/2285 train_time:76600ms step_avg:60.55ms
step:1266/2285 train_time:76660ms step_avg:60.55ms
step:1267/2285 train_time:76724ms step_avg:60.56ms
step:1268/2285 train_time:76786ms step_avg:60.56ms
step:1269/2285 train_time:76850ms step_avg:60.56ms
step:1270/2285 train_time:76912ms step_avg:60.56ms
step:1271/2285 train_time:76974ms step_avg:60.56ms
step:1272/2285 train_time:77034ms step_avg:60.56ms
step:1273/2285 train_time:77095ms step_avg:60.56ms
step:1274/2285 train_time:77155ms step_avg:60.56ms
step:1275/2285 train_time:77217ms step_avg:60.56ms
step:1276/2285 train_time:77276ms step_avg:60.56ms
step:1277/2285 train_time:77337ms step_avg:60.56ms
step:1278/2285 train_time:77397ms step_avg:60.56ms
step:1279/2285 train_time:77458ms step_avg:60.56ms
step:1280/2285 train_time:77518ms step_avg:60.56ms
step:1281/2285 train_time:77580ms step_avg:60.56ms
step:1282/2285 train_time:77641ms step_avg:60.56ms
step:1283/2285 train_time:77704ms step_avg:60.56ms
step:1284/2285 train_time:77765ms step_avg:60.56ms
step:1285/2285 train_time:77829ms step_avg:60.57ms
step:1286/2285 train_time:77889ms step_avg:60.57ms
step:1287/2285 train_time:77952ms step_avg:60.57ms
step:1288/2285 train_time:78012ms step_avg:60.57ms
step:1289/2285 train_time:78075ms step_avg:60.57ms
step:1290/2285 train_time:78134ms step_avg:60.57ms
step:1291/2285 train_time:78196ms step_avg:60.57ms
step:1292/2285 train_time:78255ms step_avg:60.57ms
step:1293/2285 train_time:78317ms step_avg:60.57ms
step:1294/2285 train_time:78376ms step_avg:60.57ms
step:1295/2285 train_time:78438ms step_avg:60.57ms
step:1296/2285 train_time:78498ms step_avg:60.57ms
step:1297/2285 train_time:78560ms step_avg:60.57ms
step:1298/2285 train_time:78621ms step_avg:60.57ms
step:1299/2285 train_time:78683ms step_avg:60.57ms
step:1300/2285 train_time:78744ms step_avg:60.57ms
step:1301/2285 train_time:78809ms step_avg:60.58ms
step:1302/2285 train_time:78869ms step_avg:60.57ms
step:1303/2285 train_time:78931ms step_avg:60.58ms
step:1304/2285 train_time:78991ms step_avg:60.58ms
step:1305/2285 train_time:79053ms step_avg:60.58ms
step:1306/2285 train_time:79113ms step_avg:60.58ms
step:1307/2285 train_time:79176ms step_avg:60.58ms
step:1308/2285 train_time:79235ms step_avg:60.58ms
step:1309/2285 train_time:79296ms step_avg:60.58ms
step:1310/2285 train_time:79356ms step_avg:60.58ms
step:1311/2285 train_time:79417ms step_avg:60.58ms
step:1312/2285 train_time:79477ms step_avg:60.58ms
step:1313/2285 train_time:79539ms step_avg:60.58ms
step:1314/2285 train_time:79599ms step_avg:60.58ms
step:1315/2285 train_time:79662ms step_avg:60.58ms
step:1316/2285 train_time:79723ms step_avg:60.58ms
step:1317/2285 train_time:79786ms step_avg:60.58ms
step:1318/2285 train_time:79846ms step_avg:60.58ms
step:1319/2285 train_time:79909ms step_avg:60.58ms
step:1320/2285 train_time:79969ms step_avg:60.58ms
step:1321/2285 train_time:80032ms step_avg:60.58ms
step:1322/2285 train_time:80091ms step_avg:60.58ms
step:1323/2285 train_time:80154ms step_avg:60.58ms
step:1324/2285 train_time:80213ms step_avg:60.58ms
step:1325/2285 train_time:80275ms step_avg:60.58ms
step:1326/2285 train_time:80335ms step_avg:60.58ms
step:1327/2285 train_time:80396ms step_avg:60.58ms
step:1328/2285 train_time:80456ms step_avg:60.58ms
step:1329/2285 train_time:80519ms step_avg:60.59ms
step:1330/2285 train_time:80578ms step_avg:60.59ms
step:1331/2285 train_time:80642ms step_avg:60.59ms
step:1332/2285 train_time:80702ms step_avg:60.59ms
step:1333/2285 train_time:80764ms step_avg:60.59ms
step:1334/2285 train_time:80825ms step_avg:60.59ms
step:1335/2285 train_time:80888ms step_avg:60.59ms
step:1336/2285 train_time:80948ms step_avg:60.59ms
step:1337/2285 train_time:81010ms step_avg:60.59ms
step:1338/2285 train_time:81070ms step_avg:60.59ms
step:1339/2285 train_time:81132ms step_avg:60.59ms
step:1340/2285 train_time:81191ms step_avg:60.59ms
step:1341/2285 train_time:81253ms step_avg:60.59ms
step:1342/2285 train_time:81312ms step_avg:60.59ms
step:1343/2285 train_time:81374ms step_avg:60.59ms
step:1344/2285 train_time:81433ms step_avg:60.59ms
step:1345/2285 train_time:81495ms step_avg:60.59ms
step:1346/2285 train_time:81555ms step_avg:60.59ms
step:1347/2285 train_time:81618ms step_avg:60.59ms
step:1348/2285 train_time:81679ms step_avg:60.59ms
step:1349/2285 train_time:81742ms step_avg:60.59ms
step:1350/2285 train_time:81803ms step_avg:60.59ms
step:1351/2285 train_time:81866ms step_avg:60.60ms
step:1352/2285 train_time:81926ms step_avg:60.60ms
step:1353/2285 train_time:81989ms step_avg:60.60ms
step:1354/2285 train_time:82048ms step_avg:60.60ms
step:1355/2285 train_time:82111ms step_avg:60.60ms
step:1356/2285 train_time:82170ms step_avg:60.60ms
step:1357/2285 train_time:82233ms step_avg:60.60ms
step:1358/2285 train_time:82292ms step_avg:60.60ms
step:1359/2285 train_time:82353ms step_avg:60.60ms
step:1360/2285 train_time:82413ms step_avg:60.60ms
step:1361/2285 train_time:82475ms step_avg:60.60ms
step:1362/2285 train_time:82535ms step_avg:60.60ms
step:1363/2285 train_time:82597ms step_avg:60.60ms
step:1364/2285 train_time:82658ms step_avg:60.60ms
step:1365/2285 train_time:82722ms step_avg:60.60ms
step:1366/2285 train_time:82781ms step_avg:60.60ms
step:1367/2285 train_time:82844ms step_avg:60.60ms
step:1368/2285 train_time:82905ms step_avg:60.60ms
step:1369/2285 train_time:82968ms step_avg:60.60ms
step:1370/2285 train_time:83028ms step_avg:60.60ms
step:1371/2285 train_time:83090ms step_avg:60.61ms
step:1372/2285 train_time:83149ms step_avg:60.60ms
step:1373/2285 train_time:83212ms step_avg:60.61ms
step:1374/2285 train_time:83271ms step_avg:60.61ms
step:1375/2285 train_time:83333ms step_avg:60.61ms
step:1376/2285 train_time:83393ms step_avg:60.61ms
step:1377/2285 train_time:83454ms step_avg:60.61ms
step:1378/2285 train_time:83514ms step_avg:60.61ms
step:1379/2285 train_time:83577ms step_avg:60.61ms
step:1380/2285 train_time:83637ms step_avg:60.61ms
step:1381/2285 train_time:83700ms step_avg:60.61ms
step:1382/2285 train_time:83761ms step_avg:60.61ms
step:1383/2285 train_time:83823ms step_avg:60.61ms
step:1384/2285 train_time:83883ms step_avg:60.61ms
step:1385/2285 train_time:83946ms step_avg:60.61ms
step:1386/2285 train_time:84007ms step_avg:60.61ms
step:1387/2285 train_time:84069ms step_avg:60.61ms
step:1388/2285 train_time:84129ms step_avg:60.61ms
step:1389/2285 train_time:84191ms step_avg:60.61ms
step:1390/2285 train_time:84251ms step_avg:60.61ms
step:1391/2285 train_time:84313ms step_avg:60.61ms
step:1392/2285 train_time:84373ms step_avg:60.61ms
step:1393/2285 train_time:84434ms step_avg:60.61ms
step:1394/2285 train_time:84493ms step_avg:60.61ms
step:1395/2285 train_time:84556ms step_avg:60.61ms
step:1396/2285 train_time:84616ms step_avg:60.61ms
step:1397/2285 train_time:84679ms step_avg:60.62ms
step:1398/2285 train_time:84740ms step_avg:60.61ms
step:1399/2285 train_time:84802ms step_avg:60.62ms
step:1400/2285 train_time:84862ms step_avg:60.62ms
step:1401/2285 train_time:84925ms step_avg:60.62ms
step:1402/2285 train_time:84985ms step_avg:60.62ms
step:1403/2285 train_time:85048ms step_avg:60.62ms
step:1404/2285 train_time:85108ms step_avg:60.62ms
step:1405/2285 train_time:85170ms step_avg:60.62ms
step:1406/2285 train_time:85230ms step_avg:60.62ms
step:1407/2285 train_time:85292ms step_avg:60.62ms
step:1408/2285 train_time:85351ms step_avg:60.62ms
step:1409/2285 train_time:85413ms step_avg:60.62ms
step:1410/2285 train_time:85472ms step_avg:60.62ms
step:1411/2285 train_time:85534ms step_avg:60.62ms
step:1412/2285 train_time:85594ms step_avg:60.62ms
step:1413/2285 train_time:85656ms step_avg:60.62ms
step:1414/2285 train_time:85717ms step_avg:60.62ms
step:1415/2285 train_time:85779ms step_avg:60.62ms
step:1416/2285 train_time:85840ms step_avg:60.62ms
step:1417/2285 train_time:85902ms step_avg:60.62ms
step:1418/2285 train_time:85963ms step_avg:60.62ms
step:1419/2285 train_time:86025ms step_avg:60.62ms
step:1420/2285 train_time:86086ms step_avg:60.62ms
step:1421/2285 train_time:86148ms step_avg:60.62ms
step:1422/2285 train_time:86208ms step_avg:60.62ms
step:1423/2285 train_time:86271ms step_avg:60.63ms
step:1424/2285 train_time:86330ms step_avg:60.63ms
step:1425/2285 train_time:86392ms step_avg:60.63ms
step:1426/2285 train_time:86451ms step_avg:60.63ms
step:1427/2285 train_time:86513ms step_avg:60.63ms
step:1428/2285 train_time:86573ms step_avg:60.63ms
step:1429/2285 train_time:86635ms step_avg:60.63ms
step:1430/2285 train_time:86695ms step_avg:60.63ms
step:1431/2285 train_time:86758ms step_avg:60.63ms
step:1432/2285 train_time:86819ms step_avg:60.63ms
step:1433/2285 train_time:86882ms step_avg:60.63ms
step:1434/2285 train_time:86942ms step_avg:60.63ms
step:1435/2285 train_time:87005ms step_avg:60.63ms
step:1436/2285 train_time:87065ms step_avg:60.63ms
step:1437/2285 train_time:87128ms step_avg:60.63ms
step:1438/2285 train_time:87187ms step_avg:60.63ms
step:1439/2285 train_time:87249ms step_avg:60.63ms
step:1440/2285 train_time:87310ms step_avg:60.63ms
step:1441/2285 train_time:87372ms step_avg:60.63ms
step:1442/2285 train_time:87431ms step_avg:60.63ms
step:1443/2285 train_time:87493ms step_avg:60.63ms
step:1444/2285 train_time:87552ms step_avg:60.63ms
step:1445/2285 train_time:87614ms step_avg:60.63ms
step:1446/2285 train_time:87674ms step_avg:60.63ms
step:1447/2285 train_time:87736ms step_avg:60.63ms
step:1448/2285 train_time:87796ms step_avg:60.63ms
step:1449/2285 train_time:87859ms step_avg:60.63ms
step:1450/2285 train_time:87920ms step_avg:60.63ms
step:1451/2285 train_time:87982ms step_avg:60.64ms
step:1452/2285 train_time:88043ms step_avg:60.64ms
step:1453/2285 train_time:88105ms step_avg:60.64ms
step:1454/2285 train_time:88165ms step_avg:60.64ms
step:1455/2285 train_time:88228ms step_avg:60.64ms
step:1456/2285 train_time:88287ms step_avg:60.64ms
step:1457/2285 train_time:88350ms step_avg:60.64ms
step:1458/2285 train_time:88409ms step_avg:60.64ms
step:1459/2285 train_time:88471ms step_avg:60.64ms
step:1460/2285 train_time:88531ms step_avg:60.64ms
step:1461/2285 train_time:88593ms step_avg:60.64ms
step:1462/2285 train_time:88652ms step_avg:60.64ms
step:1463/2285 train_time:88715ms step_avg:60.64ms
step:1464/2285 train_time:88775ms step_avg:60.64ms
step:1465/2285 train_time:88838ms step_avg:60.64ms
step:1466/2285 train_time:88899ms step_avg:60.64ms
step:1467/2285 train_time:88961ms step_avg:60.64ms
step:1468/2285 train_time:89021ms step_avg:60.64ms
step:1469/2285 train_time:89084ms step_avg:60.64ms
step:1470/2285 train_time:89144ms step_avg:60.64ms
step:1471/2285 train_time:89206ms step_avg:60.64ms
step:1472/2285 train_time:89266ms step_avg:60.64ms
step:1473/2285 train_time:89328ms step_avg:60.64ms
step:1474/2285 train_time:89388ms step_avg:60.64ms
step:1475/2285 train_time:89451ms step_avg:60.64ms
step:1476/2285 train_time:89511ms step_avg:60.64ms
step:1477/2285 train_time:89572ms step_avg:60.64ms
step:1478/2285 train_time:89632ms step_avg:60.64ms
step:1479/2285 train_time:89694ms step_avg:60.64ms
step:1480/2285 train_time:89753ms step_avg:60.64ms
step:1481/2285 train_time:89815ms step_avg:60.64ms
step:1482/2285 train_time:89876ms step_avg:60.64ms
step:1483/2285 train_time:89938ms step_avg:60.65ms
step:1484/2285 train_time:89998ms step_avg:60.65ms
step:1485/2285 train_time:90061ms step_avg:60.65ms
step:1486/2285 train_time:90122ms step_avg:60.65ms
step:1487/2285 train_time:90184ms step_avg:60.65ms
step:1488/2285 train_time:90245ms step_avg:60.65ms
step:1489/2285 train_time:90308ms step_avg:60.65ms
step:1490/2285 train_time:90368ms step_avg:60.65ms
step:1491/2285 train_time:90430ms step_avg:60.65ms
step:1492/2285 train_time:90490ms step_avg:60.65ms
step:1493/2285 train_time:90552ms step_avg:60.65ms
step:1494/2285 train_time:90612ms step_avg:60.65ms
step:1495/2285 train_time:90674ms step_avg:60.65ms
step:1496/2285 train_time:90734ms step_avg:60.65ms
step:1497/2285 train_time:90795ms step_avg:60.65ms
step:1498/2285 train_time:90856ms step_avg:60.65ms
step:1499/2285 train_time:90918ms step_avg:60.65ms
step:1500/2285 train_time:90979ms step_avg:60.65ms
step:1500/2285 val_loss:3.4277 train_time:91043ms step_avg:60.70ms
step:1501/2285 train_time:91063ms step_avg:60.67ms
step:1502/2285 train_time:91104ms step_avg:60.66ms
step:1503/2285 train_time:91168ms step_avg:60.66ms
step:1504/2285 train_time:91230ms step_avg:60.66ms
step:1505/2285 train_time:91295ms step_avg:60.66ms
step:1506/2285 train_time:91355ms step_avg:60.66ms
step:1507/2285 train_time:91417ms step_avg:60.66ms
step:1508/2285 train_time:91477ms step_avg:60.66ms
step:1509/2285 train_time:91539ms step_avg:60.66ms
step:1510/2285 train_time:91599ms step_avg:60.66ms
step:1511/2285 train_time:91661ms step_avg:60.66ms
step:1512/2285 train_time:91720ms step_avg:60.66ms
step:1513/2285 train_time:91782ms step_avg:60.66ms
step:1514/2285 train_time:91842ms step_avg:60.66ms
step:1515/2285 train_time:91904ms step_avg:60.66ms
step:1516/2285 train_time:91964ms step_avg:60.66ms
step:1517/2285 train_time:92028ms step_avg:60.66ms
step:1518/2285 train_time:92090ms step_avg:60.67ms
step:1519/2285 train_time:92154ms step_avg:60.67ms
step:1520/2285 train_time:92215ms step_avg:60.67ms
step:1521/2285 train_time:92279ms step_avg:60.67ms
step:1522/2285 train_time:92339ms step_avg:60.67ms
step:1523/2285 train_time:92401ms step_avg:60.67ms
step:1524/2285 train_time:92462ms step_avg:60.67ms
step:1525/2285 train_time:92523ms step_avg:60.67ms
step:1526/2285 train_time:92583ms step_avg:60.67ms
step:1527/2285 train_time:92646ms step_avg:60.67ms
step:1528/2285 train_time:92706ms step_avg:60.67ms
step:1529/2285 train_time:92769ms step_avg:60.67ms
step:1530/2285 train_time:92828ms step_avg:60.67ms
step:1531/2285 train_time:92892ms step_avg:60.67ms
step:1532/2285 train_time:92952ms step_avg:60.67ms
step:1533/2285 train_time:93015ms step_avg:60.68ms
step:1534/2285 train_time:93076ms step_avg:60.68ms
step:1535/2285 train_time:93141ms step_avg:60.68ms
step:1536/2285 train_time:93202ms step_avg:60.68ms
step:1537/2285 train_time:93264ms step_avg:60.68ms
step:1538/2285 train_time:93324ms step_avg:60.68ms
step:1539/2285 train_time:93388ms step_avg:60.68ms
step:1540/2285 train_time:93450ms step_avg:60.68ms
step:1541/2285 train_time:93512ms step_avg:60.68ms
step:1542/2285 train_time:93572ms step_avg:60.68ms
step:1543/2285 train_time:93635ms step_avg:60.68ms
step:1544/2285 train_time:93695ms step_avg:60.68ms
step:1545/2285 train_time:93758ms step_avg:60.68ms
step:1546/2285 train_time:93818ms step_avg:60.68ms
step:1547/2285 train_time:93880ms step_avg:60.69ms
step:1548/2285 train_time:93941ms step_avg:60.69ms
step:1549/2285 train_time:94003ms step_avg:60.69ms
step:1550/2285 train_time:94063ms step_avg:60.69ms
step:1551/2285 train_time:94126ms step_avg:60.69ms
step:1552/2285 train_time:94187ms step_avg:60.69ms
step:1553/2285 train_time:94251ms step_avg:60.69ms
step:1554/2285 train_time:94311ms step_avg:60.69ms
step:1555/2285 train_time:94375ms step_avg:60.69ms
step:1556/2285 train_time:94436ms step_avg:60.69ms
step:1557/2285 train_time:94499ms step_avg:60.69ms
step:1558/2285 train_time:94560ms step_avg:60.69ms
step:1559/2285 train_time:94622ms step_avg:60.69ms
step:1560/2285 train_time:94681ms step_avg:60.69ms
step:1561/2285 train_time:94743ms step_avg:60.69ms
step:1562/2285 train_time:94803ms step_avg:60.69ms
step:1563/2285 train_time:94865ms step_avg:60.69ms
step:1564/2285 train_time:94925ms step_avg:60.69ms
step:1565/2285 train_time:94988ms step_avg:60.70ms
step:1566/2285 train_time:95049ms step_avg:60.70ms
step:1567/2285 train_time:95113ms step_avg:60.70ms
step:1568/2285 train_time:95174ms step_avg:60.70ms
step:1569/2285 train_time:95237ms step_avg:60.70ms
step:1570/2285 train_time:95298ms step_avg:60.70ms
step:1571/2285 train_time:95361ms step_avg:60.70ms
step:1572/2285 train_time:95421ms step_avg:60.70ms
step:1573/2285 train_time:95483ms step_avg:60.70ms
step:1574/2285 train_time:95543ms step_avg:60.70ms
step:1575/2285 train_time:95606ms step_avg:60.70ms
step:1576/2285 train_time:95667ms step_avg:60.70ms
step:1577/2285 train_time:95729ms step_avg:60.70ms
step:1578/2285 train_time:95790ms step_avg:60.70ms
step:1579/2285 train_time:95853ms step_avg:60.70ms
step:1580/2285 train_time:95913ms step_avg:60.70ms
step:1581/2285 train_time:95976ms step_avg:60.71ms
step:1582/2285 train_time:96037ms step_avg:60.71ms
step:1583/2285 train_time:96101ms step_avg:60.71ms
step:1584/2285 train_time:96162ms step_avg:60.71ms
step:1585/2285 train_time:96224ms step_avg:60.71ms
step:1586/2285 train_time:96284ms step_avg:60.71ms
step:1587/2285 train_time:96347ms step_avg:60.71ms
step:1588/2285 train_time:96407ms step_avg:60.71ms
step:1589/2285 train_time:96470ms step_avg:60.71ms
step:1590/2285 train_time:96530ms step_avg:60.71ms
step:1591/2285 train_time:96593ms step_avg:60.71ms
step:1592/2285 train_time:96653ms step_avg:60.71ms
step:1593/2285 train_time:96716ms step_avg:60.71ms
step:1594/2285 train_time:96777ms step_avg:60.71ms
step:1595/2285 train_time:96840ms step_avg:60.71ms
step:1596/2285 train_time:96900ms step_avg:60.71ms
step:1597/2285 train_time:96962ms step_avg:60.72ms
step:1598/2285 train_time:97022ms step_avg:60.71ms
step:1599/2285 train_time:97086ms step_avg:60.72ms
step:1600/2285 train_time:97146ms step_avg:60.72ms
step:1601/2285 train_time:97209ms step_avg:60.72ms
step:1602/2285 train_time:97269ms step_avg:60.72ms
step:1603/2285 train_time:97332ms step_avg:60.72ms
step:1604/2285 train_time:97393ms step_avg:60.72ms
step:1605/2285 train_time:97456ms step_avg:60.72ms
step:1606/2285 train_time:97516ms step_avg:60.72ms
step:1607/2285 train_time:97579ms step_avg:60.72ms
step:1608/2285 train_time:97639ms step_avg:60.72ms
step:1609/2285 train_time:97702ms step_avg:60.72ms
step:1610/2285 train_time:97762ms step_avg:60.72ms
step:1611/2285 train_time:97824ms step_avg:60.72ms
step:1612/2285 train_time:97884ms step_avg:60.72ms
step:1613/2285 train_time:97947ms step_avg:60.72ms
step:1614/2285 train_time:98008ms step_avg:60.72ms
step:1615/2285 train_time:98071ms step_avg:60.72ms
step:1616/2285 train_time:98131ms step_avg:60.72ms
step:1617/2285 train_time:98194ms step_avg:60.73ms
step:1618/2285 train_time:98254ms step_avg:60.73ms
step:1619/2285 train_time:98317ms step_avg:60.73ms
step:1620/2285 train_time:98377ms step_avg:60.73ms
step:1621/2285 train_time:98440ms step_avg:60.73ms
step:1622/2285 train_time:98500ms step_avg:60.73ms
step:1623/2285 train_time:98562ms step_avg:60.73ms
step:1624/2285 train_time:98622ms step_avg:60.73ms
step:1625/2285 train_time:98684ms step_avg:60.73ms
step:1626/2285 train_time:98745ms step_avg:60.73ms
step:1627/2285 train_time:98807ms step_avg:60.73ms
step:1628/2285 train_time:98867ms step_avg:60.73ms
step:1629/2285 train_time:98930ms step_avg:60.73ms
step:1630/2285 train_time:98991ms step_avg:60.73ms
step:1631/2285 train_time:99054ms step_avg:60.73ms
step:1632/2285 train_time:99115ms step_avg:60.73ms
step:1633/2285 train_time:99178ms step_avg:60.73ms
step:1634/2285 train_time:99239ms step_avg:60.73ms
step:1635/2285 train_time:99301ms step_avg:60.73ms
step:1636/2285 train_time:99361ms step_avg:60.73ms
step:1637/2285 train_time:99424ms step_avg:60.74ms
step:1638/2285 train_time:99484ms step_avg:60.74ms
step:1639/2285 train_time:99546ms step_avg:60.74ms
step:1640/2285 train_time:99607ms step_avg:60.74ms
step:1641/2285 train_time:99669ms step_avg:60.74ms
step:1642/2285 train_time:99729ms step_avg:60.74ms
step:1643/2285 train_time:99792ms step_avg:60.74ms
step:1644/2285 train_time:99853ms step_avg:60.74ms
step:1645/2285 train_time:99915ms step_avg:60.74ms
step:1646/2285 train_time:99976ms step_avg:60.74ms
step:1647/2285 train_time:100041ms step_avg:60.74ms
step:1648/2285 train_time:100101ms step_avg:60.74ms
step:1649/2285 train_time:100163ms step_avg:60.74ms
step:1650/2285 train_time:100223ms step_avg:60.74ms
step:1651/2285 train_time:100286ms step_avg:60.74ms
step:1652/2285 train_time:100346ms step_avg:60.74ms
step:1653/2285 train_time:100409ms step_avg:60.74ms
step:1654/2285 train_time:100469ms step_avg:60.74ms
step:1655/2285 train_time:100532ms step_avg:60.74ms
step:1656/2285 train_time:100593ms step_avg:60.74ms
step:1657/2285 train_time:100656ms step_avg:60.75ms
step:1658/2285 train_time:100716ms step_avg:60.75ms
step:1659/2285 train_time:100778ms step_avg:60.75ms
step:1660/2285 train_time:100838ms step_avg:60.75ms
step:1661/2285 train_time:100901ms step_avg:60.75ms
step:1662/2285 train_time:100962ms step_avg:60.75ms
step:1663/2285 train_time:101024ms step_avg:60.75ms
step:1664/2285 train_time:101083ms step_avg:60.75ms
step:1665/2285 train_time:101146ms step_avg:60.75ms
step:1666/2285 train_time:101206ms step_avg:60.75ms
step:1667/2285 train_time:101269ms step_avg:60.75ms
step:1668/2285 train_time:101330ms step_avg:60.75ms
step:1669/2285 train_time:101393ms step_avg:60.75ms
step:1670/2285 train_time:101454ms step_avg:60.75ms
step:1671/2285 train_time:101517ms step_avg:60.75ms
step:1672/2285 train_time:101577ms step_avg:60.75ms
step:1673/2285 train_time:101641ms step_avg:60.75ms
step:1674/2285 train_time:101701ms step_avg:60.75ms
step:1675/2285 train_time:101763ms step_avg:60.75ms
step:1676/2285 train_time:101823ms step_avg:60.75ms
step:1677/2285 train_time:101885ms step_avg:60.75ms
step:1678/2285 train_time:101945ms step_avg:60.75ms
step:1679/2285 train_time:102009ms step_avg:60.76ms
step:1680/2285 train_time:102070ms step_avg:60.76ms
step:1681/2285 train_time:102132ms step_avg:60.76ms
step:1682/2285 train_time:102192ms step_avg:60.76ms
step:1683/2285 train_time:102255ms step_avg:60.76ms
step:1684/2285 train_time:102315ms step_avg:60.76ms
step:1685/2285 train_time:102380ms step_avg:60.76ms
step:1686/2285 train_time:102440ms step_avg:60.76ms
step:1687/2285 train_time:102502ms step_avg:60.76ms
step:1688/2285 train_time:102563ms step_avg:60.76ms
step:1689/2285 train_time:102626ms step_avg:60.76ms
step:1690/2285 train_time:102687ms step_avg:60.76ms
step:1691/2285 train_time:102750ms step_avg:60.76ms
step:1692/2285 train_time:102811ms step_avg:60.76ms
step:1693/2285 train_time:102873ms step_avg:60.76ms
step:1694/2285 train_time:102934ms step_avg:60.76ms
step:1695/2285 train_time:102997ms step_avg:60.77ms
step:1696/2285 train_time:103057ms step_avg:60.76ms
step:1697/2285 train_time:103120ms step_avg:60.77ms
step:1698/2285 train_time:103180ms step_avg:60.77ms
step:1699/2285 train_time:103243ms step_avg:60.77ms
step:1700/2285 train_time:103302ms step_avg:60.77ms
step:1701/2285 train_time:103365ms step_avg:60.77ms
step:1702/2285 train_time:103425ms step_avg:60.77ms
step:1703/2285 train_time:103487ms step_avg:60.77ms
step:1704/2285 train_time:103548ms step_avg:60.77ms
step:1705/2285 train_time:103611ms step_avg:60.77ms
step:1706/2285 train_time:103671ms step_avg:60.77ms
step:1707/2285 train_time:103734ms step_avg:60.77ms
step:1708/2285 train_time:103795ms step_avg:60.77ms
step:1709/2285 train_time:103858ms step_avg:60.77ms
step:1710/2285 train_time:103918ms step_avg:60.77ms
step:1711/2285 train_time:103981ms step_avg:60.77ms
step:1712/2285 train_time:104041ms step_avg:60.77ms
step:1713/2285 train_time:104103ms step_avg:60.77ms
step:1714/2285 train_time:104163ms step_avg:60.77ms
step:1715/2285 train_time:104226ms step_avg:60.77ms
step:1716/2285 train_time:104286ms step_avg:60.77ms
step:1717/2285 train_time:104349ms step_avg:60.77ms
step:1718/2285 train_time:104409ms step_avg:60.77ms
step:1719/2285 train_time:104472ms step_avg:60.77ms
step:1720/2285 train_time:104532ms step_avg:60.77ms
step:1721/2285 train_time:104595ms step_avg:60.78ms
step:1722/2285 train_time:104655ms step_avg:60.78ms
step:1723/2285 train_time:104719ms step_avg:60.78ms
step:1724/2285 train_time:104780ms step_avg:60.78ms
step:1725/2285 train_time:104842ms step_avg:60.78ms
step:1726/2285 train_time:104903ms step_avg:60.78ms
step:1727/2285 train_time:104966ms step_avg:60.78ms
step:1728/2285 train_time:105026ms step_avg:60.78ms
step:1729/2285 train_time:105089ms step_avg:60.78ms
step:1730/2285 train_time:105149ms step_avg:60.78ms
step:1731/2285 train_time:105212ms step_avg:60.78ms
step:1732/2285 train_time:105273ms step_avg:60.78ms
step:1733/2285 train_time:105336ms step_avg:60.78ms
step:1734/2285 train_time:105397ms step_avg:60.78ms
step:1735/2285 train_time:105461ms step_avg:60.78ms
step:1736/2285 train_time:105521ms step_avg:60.78ms
step:1737/2285 train_time:105583ms step_avg:60.78ms
step:1738/2285 train_time:105644ms step_avg:60.78ms
step:1739/2285 train_time:105707ms step_avg:60.79ms
step:1740/2285 train_time:105768ms step_avg:60.79ms
step:1741/2285 train_time:105831ms step_avg:60.79ms
step:1742/2285 train_time:105891ms step_avg:60.79ms
step:1743/2285 train_time:105955ms step_avg:60.79ms
step:1744/2285 train_time:106015ms step_avg:60.79ms
step:1745/2285 train_time:106079ms step_avg:60.79ms
step:1746/2285 train_time:106139ms step_avg:60.79ms
step:1747/2285 train_time:106201ms step_avg:60.79ms
step:1748/2285 train_time:106262ms step_avg:60.79ms
step:1749/2285 train_time:106324ms step_avg:60.79ms
step:1750/2285 train_time:106384ms step_avg:60.79ms
step:1750/2285 val_loss:3.3694 train_time:106448ms step_avg:60.83ms
step:1751/2285 train_time:106469ms step_avg:60.80ms
step:1752/2285 train_time:106509ms step_avg:60.79ms
step:1753/2285 train_time:106575ms step_avg:60.80ms
step:1754/2285 train_time:106637ms step_avg:60.80ms
step:1755/2285 train_time:106699ms step_avg:60.80ms
step:1756/2285 train_time:106759ms step_avg:60.80ms
step:1757/2285 train_time:106822ms step_avg:60.80ms
step:1758/2285 train_time:106882ms step_avg:60.80ms
step:1759/2285 train_time:106944ms step_avg:60.80ms
step:1760/2285 train_time:107003ms step_avg:60.80ms
step:1761/2285 train_time:107067ms step_avg:60.80ms
step:1762/2285 train_time:107127ms step_avg:60.80ms
step:1763/2285 train_time:107189ms step_avg:60.80ms
step:1764/2285 train_time:107249ms step_avg:60.80ms
step:1765/2285 train_time:107311ms step_avg:60.80ms
step:1766/2285 train_time:107371ms step_avg:60.80ms
step:1767/2285 train_time:107435ms step_avg:60.80ms
step:1768/2285 train_time:107496ms step_avg:60.80ms
step:1769/2285 train_time:107559ms step_avg:60.80ms
step:1770/2285 train_time:107621ms step_avg:60.80ms
step:1771/2285 train_time:107684ms step_avg:60.80ms
step:1772/2285 train_time:107744ms step_avg:60.80ms
step:1773/2285 train_time:107808ms step_avg:60.81ms
step:1774/2285 train_time:107867ms step_avg:60.80ms
step:1775/2285 train_time:107930ms step_avg:60.81ms
step:1776/2285 train_time:107990ms step_avg:60.80ms
step:1777/2285 train_time:108052ms step_avg:60.81ms
step:1778/2285 train_time:108112ms step_avg:60.81ms
step:1779/2285 train_time:108175ms step_avg:60.81ms
step:1780/2285 train_time:108236ms step_avg:60.81ms
step:1781/2285 train_time:108298ms step_avg:60.81ms
step:1782/2285 train_time:108358ms step_avg:60.81ms
step:1783/2285 train_time:108422ms step_avg:60.81ms
step:1784/2285 train_time:108484ms step_avg:60.81ms
step:1785/2285 train_time:108548ms step_avg:60.81ms
step:1786/2285 train_time:108608ms step_avg:60.81ms
step:1787/2285 train_time:108671ms step_avg:60.81ms
step:1788/2285 train_time:108732ms step_avg:60.81ms
step:1789/2285 train_time:108794ms step_avg:60.81ms
step:1790/2285 train_time:108854ms step_avg:60.81ms
step:1791/2285 train_time:108917ms step_avg:60.81ms
step:1792/2285 train_time:108978ms step_avg:60.81ms
step:1793/2285 train_time:109040ms step_avg:60.81ms
step:1794/2285 train_time:109100ms step_avg:60.81ms
step:1795/2285 train_time:109164ms step_avg:60.82ms
step:1796/2285 train_time:109224ms step_avg:60.82ms
step:1797/2285 train_time:109287ms step_avg:60.82ms
step:1798/2285 train_time:109347ms step_avg:60.82ms
step:1799/2285 train_time:109410ms step_avg:60.82ms
step:1800/2285 train_time:109470ms step_avg:60.82ms
step:1801/2285 train_time:109533ms step_avg:60.82ms
step:1802/2285 train_time:109594ms step_avg:60.82ms
step:1803/2285 train_time:109657ms step_avg:60.82ms
step:1804/2285 train_time:109718ms step_avg:60.82ms
step:1805/2285 train_time:109781ms step_avg:60.82ms
step:1806/2285 train_time:109841ms step_avg:60.82ms
step:1807/2285 train_time:109904ms step_avg:60.82ms
step:1808/2285 train_time:109964ms step_avg:60.82ms
step:1809/2285 train_time:110027ms step_avg:60.82ms
step:1810/2285 train_time:110087ms step_avg:60.82ms
step:1811/2285 train_time:110150ms step_avg:60.82ms
step:1812/2285 train_time:110210ms step_avg:60.82ms
step:1813/2285 train_time:110272ms step_avg:60.82ms
step:1814/2285 train_time:110332ms step_avg:60.82ms
step:1815/2285 train_time:110394ms step_avg:60.82ms
step:1816/2285 train_time:110455ms step_avg:60.82ms
step:1817/2285 train_time:110518ms step_avg:60.82ms
step:1818/2285 train_time:110579ms step_avg:60.82ms
step:1819/2285 train_time:110642ms step_avg:60.83ms
step:1820/2285 train_time:110702ms step_avg:60.83ms
step:1821/2285 train_time:110765ms step_avg:60.83ms
step:1822/2285 train_time:110826ms step_avg:60.83ms
step:1823/2285 train_time:110889ms step_avg:60.83ms
step:1824/2285 train_time:110948ms step_avg:60.83ms
step:1825/2285 train_time:111011ms step_avg:60.83ms
step:1826/2285 train_time:111071ms step_avg:60.83ms
step:1827/2285 train_time:111133ms step_avg:60.83ms
step:1828/2285 train_time:111193ms step_avg:60.83ms
step:1829/2285 train_time:111256ms step_avg:60.83ms
step:1830/2285 train_time:111316ms step_avg:60.83ms
step:1831/2285 train_time:111380ms step_avg:60.83ms
step:1832/2285 train_time:111441ms step_avg:60.83ms
step:1833/2285 train_time:111504ms step_avg:60.83ms
step:1834/2285 train_time:111565ms step_avg:60.83ms
step:1835/2285 train_time:111629ms step_avg:60.83ms
step:1836/2285 train_time:111689ms step_avg:60.83ms
step:1837/2285 train_time:111751ms step_avg:60.83ms
step:1838/2285 train_time:111811ms step_avg:60.83ms
step:1839/2285 train_time:111874ms step_avg:60.83ms
step:1840/2285 train_time:111935ms step_avg:60.83ms
step:1841/2285 train_time:111998ms step_avg:60.84ms
step:1842/2285 train_time:112058ms step_avg:60.84ms
step:1843/2285 train_time:112121ms step_avg:60.84ms
step:1844/2285 train_time:112182ms step_avg:60.84ms
step:1845/2285 train_time:112246ms step_avg:60.84ms
step:1846/2285 train_time:112306ms step_avg:60.84ms
step:1847/2285 train_time:112370ms step_avg:60.84ms
step:1848/2285 train_time:112430ms step_avg:60.84ms
step:1849/2285 train_time:112492ms step_avg:60.84ms
step:1850/2285 train_time:112552ms step_avg:60.84ms
step:1851/2285 train_time:112615ms step_avg:60.84ms
step:1852/2285 train_time:112675ms step_avg:60.84ms
step:1853/2285 train_time:112739ms step_avg:60.84ms
step:1854/2285 train_time:112799ms step_avg:60.84ms
step:1855/2285 train_time:112862ms step_avg:60.84ms
step:1856/2285 train_time:112924ms step_avg:60.84ms
step:1857/2285 train_time:112986ms step_avg:60.84ms
step:1858/2285 train_time:113047ms step_avg:60.84ms
step:1859/2285 train_time:113109ms step_avg:60.84ms
step:1860/2285 train_time:113169ms step_avg:60.84ms
step:1861/2285 train_time:113232ms step_avg:60.84ms
step:1862/2285 train_time:113293ms step_avg:60.84ms
step:1863/2285 train_time:113355ms step_avg:60.85ms
step:1864/2285 train_time:113416ms step_avg:60.85ms
step:1865/2285 train_time:113479ms step_avg:60.85ms
step:1866/2285 train_time:113540ms step_avg:60.85ms
step:1867/2285 train_time:113602ms step_avg:60.85ms
step:1868/2285 train_time:113663ms step_avg:60.85ms
step:1869/2285 train_time:113727ms step_avg:60.85ms
step:1870/2285 train_time:113787ms step_avg:60.85ms
step:1871/2285 train_time:113849ms step_avg:60.85ms
step:1872/2285 train_time:113909ms step_avg:60.85ms
step:1873/2285 train_time:113971ms step_avg:60.85ms
step:1874/2285 train_time:114031ms step_avg:60.85ms
step:1875/2285 train_time:114094ms step_avg:60.85ms
step:1876/2285 train_time:114155ms step_avg:60.85ms
step:1877/2285 train_time:114217ms step_avg:60.85ms
step:1878/2285 train_time:114278ms step_avg:60.85ms
step:1879/2285 train_time:114341ms step_avg:60.85ms
step:1880/2285 train_time:114402ms step_avg:60.85ms
step:1881/2285 train_time:114465ms step_avg:60.85ms
step:1882/2285 train_time:114526ms step_avg:60.85ms
step:1883/2285 train_time:114589ms step_avg:60.85ms
step:1884/2285 train_time:114649ms step_avg:60.85ms
step:1885/2285 train_time:114712ms step_avg:60.85ms
step:1886/2285 train_time:114772ms step_avg:60.85ms
step:1887/2285 train_time:114835ms step_avg:60.86ms
step:1888/2285 train_time:114895ms step_avg:60.86ms
step:1889/2285 train_time:114958ms step_avg:60.86ms
step:1890/2285 train_time:115018ms step_avg:60.86ms
step:1891/2285 train_time:115081ms step_avg:60.86ms
step:1892/2285 train_time:115142ms step_avg:60.86ms
step:1893/2285 train_time:115204ms step_avg:60.86ms
step:1894/2285 train_time:115265ms step_avg:60.86ms
step:1895/2285 train_time:115328ms step_avg:60.86ms
step:1896/2285 train_time:115388ms step_avg:60.86ms
step:1897/2285 train_time:115450ms step_avg:60.86ms
step:1898/2285 train_time:115510ms step_avg:60.86ms
step:1899/2285 train_time:115573ms step_avg:60.86ms
step:1900/2285 train_time:115634ms step_avg:60.86ms
step:1901/2285 train_time:115696ms step_avg:60.86ms
step:1902/2285 train_time:115756ms step_avg:60.86ms
step:1903/2285 train_time:115818ms step_avg:60.86ms
step:1904/2285 train_time:115879ms step_avg:60.86ms
step:1905/2285 train_time:115942ms step_avg:60.86ms
step:1906/2285 train_time:116002ms step_avg:60.86ms
step:1907/2285 train_time:116065ms step_avg:60.86ms
step:1908/2285 train_time:116125ms step_avg:60.86ms
step:1909/2285 train_time:116189ms step_avg:60.86ms
step:1910/2285 train_time:116249ms step_avg:60.86ms
step:1911/2285 train_time:116312ms step_avg:60.86ms
step:1912/2285 train_time:116372ms step_avg:60.86ms
step:1913/2285 train_time:116435ms step_avg:60.87ms
step:1914/2285 train_time:116496ms step_avg:60.87ms
step:1915/2285 train_time:116559ms step_avg:60.87ms
step:1916/2285 train_time:116620ms step_avg:60.87ms
step:1917/2285 train_time:116683ms step_avg:60.87ms
step:1918/2285 train_time:116743ms step_avg:60.87ms
step:1919/2285 train_time:116805ms step_avg:60.87ms
step:1920/2285 train_time:116867ms step_avg:60.87ms
step:1921/2285 train_time:116929ms step_avg:60.87ms
step:1922/2285 train_time:116989ms step_avg:60.87ms
step:1923/2285 train_time:117052ms step_avg:60.87ms
step:1924/2285 train_time:117112ms step_avg:60.87ms
step:1925/2285 train_time:117174ms step_avg:60.87ms
step:1926/2285 train_time:117235ms step_avg:60.87ms
step:1927/2285 train_time:117297ms step_avg:60.87ms
step:1928/2285 train_time:117358ms step_avg:60.87ms
step:1929/2285 train_time:117420ms step_avg:60.87ms
step:1930/2285 train_time:117481ms step_avg:60.87ms
step:1931/2285 train_time:117544ms step_avg:60.87ms
step:1932/2285 train_time:117605ms step_avg:60.87ms
step:1933/2285 train_time:117668ms step_avg:60.87ms
step:1934/2285 train_time:117728ms step_avg:60.87ms
step:1935/2285 train_time:117791ms step_avg:60.87ms
step:1936/2285 train_time:117852ms step_avg:60.87ms
step:1937/2285 train_time:117914ms step_avg:60.87ms
step:1938/2285 train_time:117974ms step_avg:60.87ms
step:1939/2285 train_time:118036ms step_avg:60.87ms
step:1940/2285 train_time:118096ms step_avg:60.87ms
step:1941/2285 train_time:118159ms step_avg:60.88ms
step:1942/2285 train_time:118219ms step_avg:60.88ms
step:1943/2285 train_time:118283ms step_avg:60.88ms
step:1944/2285 train_time:118343ms step_avg:60.88ms
step:1945/2285 train_time:118407ms step_avg:60.88ms
step:1946/2285 train_time:118468ms step_avg:60.88ms
step:1947/2285 train_time:118530ms step_avg:60.88ms
step:1948/2285 train_time:118590ms step_avg:60.88ms
step:1949/2285 train_time:118653ms step_avg:60.88ms
step:1950/2285 train_time:118713ms step_avg:60.88ms
step:1951/2285 train_time:118775ms step_avg:60.88ms
step:1952/2285 train_time:118835ms step_avg:60.88ms
step:1953/2285 train_time:118898ms step_avg:60.88ms
step:1954/2285 train_time:118958ms step_avg:60.88ms
step:1955/2285 train_time:119021ms step_avg:60.88ms
step:1956/2285 train_time:119081ms step_avg:60.88ms
step:1957/2285 train_time:119144ms step_avg:60.88ms
step:1958/2285 train_time:119204ms step_avg:60.88ms
step:1959/2285 train_time:119267ms step_avg:60.88ms
step:1960/2285 train_time:119328ms step_avg:60.88ms
step:1961/2285 train_time:119390ms step_avg:60.88ms
step:1962/2285 train_time:119450ms step_avg:60.88ms
step:1963/2285 train_time:119513ms step_avg:60.88ms
step:1964/2285 train_time:119574ms step_avg:60.88ms
step:1965/2285 train_time:119636ms step_avg:60.88ms
step:1966/2285 train_time:119697ms step_avg:60.88ms
step:1967/2285 train_time:119760ms step_avg:60.88ms
step:1968/2285 train_time:119821ms step_avg:60.88ms
step:1969/2285 train_time:119884ms step_avg:60.89ms
step:1970/2285 train_time:119945ms step_avg:60.89ms
step:1971/2285 train_time:120008ms step_avg:60.89ms
step:1972/2285 train_time:120069ms step_avg:60.89ms
step:1973/2285 train_time:120131ms step_avg:60.89ms
step:1974/2285 train_time:120191ms step_avg:60.89ms
step:1975/2285 train_time:120254ms step_avg:60.89ms
step:1976/2285 train_time:120314ms step_avg:60.89ms
step:1977/2285 train_time:120377ms step_avg:60.89ms
step:1978/2285 train_time:120437ms step_avg:60.89ms
step:1979/2285 train_time:120501ms step_avg:60.89ms
step:1980/2285 train_time:120561ms step_avg:60.89ms
step:1981/2285 train_time:120624ms step_avg:60.89ms
step:1982/2285 train_time:120685ms step_avg:60.89ms
step:1983/2285 train_time:120748ms step_avg:60.89ms
step:1984/2285 train_time:120809ms step_avg:60.89ms
step:1985/2285 train_time:120871ms step_avg:60.89ms
step:1986/2285 train_time:120931ms step_avg:60.89ms
step:1987/2285 train_time:120994ms step_avg:60.89ms
step:1988/2285 train_time:121054ms step_avg:60.89ms
step:1989/2285 train_time:121116ms step_avg:60.89ms
step:1990/2285 train_time:121177ms step_avg:60.89ms
step:1991/2285 train_time:121239ms step_avg:60.89ms
step:1992/2285 train_time:121300ms step_avg:60.89ms
step:1993/2285 train_time:121363ms step_avg:60.89ms
step:1994/2285 train_time:121424ms step_avg:60.89ms
step:1995/2285 train_time:121488ms step_avg:60.90ms
step:1996/2285 train_time:121548ms step_avg:60.90ms
step:1997/2285 train_time:121610ms step_avg:60.90ms
step:1998/2285 train_time:121670ms step_avg:60.90ms
step:1999/2285 train_time:121732ms step_avg:60.90ms
step:2000/2285 train_time:121792ms step_avg:60.90ms
step:2000/2285 val_loss:3.3229 train_time:121856ms step_avg:60.93ms
step:2001/2285 train_time:121874ms step_avg:60.91ms
step:2002/2285 train_time:121920ms step_avg:60.90ms
step:2003/2285 train_time:121986ms step_avg:60.90ms
step:2004/2285 train_time:122048ms step_avg:60.90ms
step:2005/2285 train_time:122112ms step_avg:60.90ms
step:2006/2285 train_time:122171ms step_avg:60.90ms
step:2007/2285 train_time:122233ms step_avg:60.90ms
step:2008/2285 train_time:122293ms step_avg:60.90ms
step:2009/2285 train_time:122355ms step_avg:60.90ms
step:2010/2285 train_time:122414ms step_avg:60.90ms
step:2011/2285 train_time:122476ms step_avg:60.90ms
step:2012/2285 train_time:122536ms step_avg:60.90ms
step:2013/2285 train_time:122598ms step_avg:60.90ms
step:2014/2285 train_time:122658ms step_avg:60.90ms
step:2015/2285 train_time:122720ms step_avg:60.90ms
step:2016/2285 train_time:122781ms step_avg:60.90ms
step:2017/2285 train_time:122845ms step_avg:60.90ms
step:2018/2285 train_time:122907ms step_avg:60.91ms
step:2019/2285 train_time:122971ms step_avg:60.91ms
step:2020/2285 train_time:123032ms step_avg:60.91ms
step:2021/2285 train_time:123095ms step_avg:60.91ms
step:2022/2285 train_time:123155ms step_avg:60.91ms
step:2023/2285 train_time:123218ms step_avg:60.91ms
step:2024/2285 train_time:123278ms step_avg:60.91ms
step:2025/2285 train_time:123340ms step_avg:60.91ms
step:2026/2285 train_time:123401ms step_avg:60.91ms
step:2027/2285 train_time:123462ms step_avg:60.91ms
step:2028/2285 train_time:123522ms step_avg:60.91ms
step:2029/2285 train_time:123585ms step_avg:60.91ms
step:2030/2285 train_time:123645ms step_avg:60.91ms
step:2031/2285 train_time:123707ms step_avg:60.91ms
step:2032/2285 train_time:123768ms step_avg:60.91ms
step:2033/2285 train_time:123832ms step_avg:60.91ms
step:2034/2285 train_time:123893ms step_avg:60.91ms
step:2035/2285 train_time:123957ms step_avg:60.91ms
step:2036/2285 train_time:124018ms step_avg:60.91ms
step:2037/2285 train_time:124082ms step_avg:60.91ms
step:2038/2285 train_time:124143ms step_avg:60.91ms
step:2039/2285 train_time:124206ms step_avg:60.92ms
step:2040/2285 train_time:124267ms step_avg:60.92ms
step:2041/2285 train_time:124330ms step_avg:60.92ms
step:2042/2285 train_time:124391ms step_avg:60.92ms
step:2043/2285 train_time:124453ms step_avg:60.92ms
step:2044/2285 train_time:124513ms step_avg:60.92ms
step:2045/2285 train_time:124575ms step_avg:60.92ms
step:2046/2285 train_time:124634ms step_avg:60.92ms
step:2047/2285 train_time:124697ms step_avg:60.92ms
step:2048/2285 train_time:124756ms step_avg:60.92ms
step:2049/2285 train_time:124819ms step_avg:60.92ms
step:2050/2285 train_time:124881ms step_avg:60.92ms
step:2051/2285 train_time:124944ms step_avg:60.92ms
step:2052/2285 train_time:125006ms step_avg:60.92ms
step:2053/2285 train_time:125069ms step_avg:60.92ms
step:2054/2285 train_time:125131ms step_avg:60.92ms
step:2055/2285 train_time:125194ms step_avg:60.92ms
step:2056/2285 train_time:125254ms step_avg:60.92ms
step:2057/2285 train_time:125317ms step_avg:60.92ms
step:2058/2285 train_time:125377ms step_avg:60.92ms
step:2059/2285 train_time:125440ms step_avg:60.92ms
step:2060/2285 train_time:125500ms step_avg:60.92ms
step:2061/2285 train_time:125563ms step_avg:60.92ms
step:2062/2285 train_time:125623ms step_avg:60.92ms
step:2063/2285 train_time:125686ms step_avg:60.92ms
step:2064/2285 train_time:125747ms step_avg:60.92ms
step:2065/2285 train_time:125811ms step_avg:60.93ms
step:2066/2285 train_time:125871ms step_avg:60.92ms
step:2067/2285 train_time:125933ms step_avg:60.93ms
step:2068/2285 train_time:125994ms step_avg:60.93ms
step:2069/2285 train_time:126057ms step_avg:60.93ms
step:2070/2285 train_time:126117ms step_avg:60.93ms
step:2071/2285 train_time:126181ms step_avg:60.93ms
step:2072/2285 train_time:126241ms step_avg:60.93ms
step:2073/2285 train_time:126304ms step_avg:60.93ms
step:2074/2285 train_time:126364ms step_avg:60.93ms
step:2075/2285 train_time:126428ms step_avg:60.93ms
step:2076/2285 train_time:126488ms step_avg:60.93ms
step:2077/2285 train_time:126551ms step_avg:60.93ms
step:2078/2285 train_time:126610ms step_avg:60.93ms
step:2079/2285 train_time:126673ms step_avg:60.93ms
step:2080/2285 train_time:126733ms step_avg:60.93ms
step:2081/2285 train_time:126796ms step_avg:60.93ms
step:2082/2285 train_time:126856ms step_avg:60.93ms
step:2083/2285 train_time:126919ms step_avg:60.93ms
step:2084/2285 train_time:126980ms step_avg:60.93ms
step:2085/2285 train_time:127043ms step_avg:60.93ms
step:2086/2285 train_time:127104ms step_avg:60.93ms
step:2087/2285 train_time:127167ms step_avg:60.93ms
step:2088/2285 train_time:127228ms step_avg:60.93ms
step:2089/2285 train_time:127291ms step_avg:60.93ms
step:2090/2285 train_time:127352ms step_avg:60.93ms
step:2091/2285 train_time:127414ms step_avg:60.93ms
step:2092/2285 train_time:127474ms step_avg:60.93ms
step:2093/2285 train_time:127538ms step_avg:60.94ms
step:2094/2285 train_time:127598ms step_avg:60.94ms
step:2095/2285 train_time:127661ms step_avg:60.94ms
step:2096/2285 train_time:127722ms step_avg:60.94ms
step:2097/2285 train_time:127784ms step_avg:60.94ms
step:2098/2285 train_time:127845ms step_avg:60.94ms
step:2099/2285 train_time:127908ms step_avg:60.94ms
step:2100/2285 train_time:127969ms step_avg:60.94ms
step:2101/2285 train_time:128032ms step_avg:60.94ms
step:2102/2285 train_time:128093ms step_avg:60.94ms
step:2103/2285 train_time:128155ms step_avg:60.94ms
step:2104/2285 train_time:128216ms step_avg:60.94ms
step:2105/2285 train_time:128278ms step_avg:60.94ms
step:2106/2285 train_time:128340ms step_avg:60.94ms
step:2107/2285 train_time:128402ms step_avg:60.94ms
step:2108/2285 train_time:128464ms step_avg:60.94ms
step:2109/2285 train_time:128526ms step_avg:60.94ms
step:2110/2285 train_time:128587ms step_avg:60.94ms
step:2111/2285 train_time:128650ms step_avg:60.94ms
step:2112/2285 train_time:128711ms step_avg:60.94ms
step:2113/2285 train_time:128773ms step_avg:60.94ms
step:2114/2285 train_time:128833ms step_avg:60.94ms
step:2115/2285 train_time:128895ms step_avg:60.94ms
step:2116/2285 train_time:128955ms step_avg:60.94ms
step:2117/2285 train_time:129018ms step_avg:60.94ms
step:2118/2285 train_time:129078ms step_avg:60.94ms
step:2119/2285 train_time:129141ms step_avg:60.94ms
step:2120/2285 train_time:129203ms step_avg:60.94ms
step:2121/2285 train_time:129266ms step_avg:60.95ms
step:2122/2285 train_time:129327ms step_avg:60.95ms
step:2123/2285 train_time:129391ms step_avg:60.95ms
step:2124/2285 train_time:129451ms step_avg:60.95ms
step:2125/2285 train_time:129514ms step_avg:60.95ms
step:2126/2285 train_time:129574ms step_avg:60.95ms
step:2127/2285 train_time:129637ms step_avg:60.95ms
step:2128/2285 train_time:129697ms step_avg:60.95ms
step:2129/2285 train_time:129759ms step_avg:60.95ms
step:2130/2285 train_time:129820ms step_avg:60.95ms
step:2131/2285 train_time:129883ms step_avg:60.95ms
step:2132/2285 train_time:129943ms step_avg:60.95ms
step:2133/2285 train_time:130007ms step_avg:60.95ms
step:2134/2285 train_time:130067ms step_avg:60.95ms
step:2135/2285 train_time:130131ms step_avg:60.95ms
step:2136/2285 train_time:130191ms step_avg:60.95ms
step:2137/2285 train_time:130253ms step_avg:60.95ms
step:2138/2285 train_time:130314ms step_avg:60.95ms
step:2139/2285 train_time:130377ms step_avg:60.95ms
step:2140/2285 train_time:130438ms step_avg:60.95ms
step:2141/2285 train_time:130502ms step_avg:60.95ms
step:2142/2285 train_time:130563ms step_avg:60.95ms
step:2143/2285 train_time:130626ms step_avg:60.95ms
step:2144/2285 train_time:130686ms step_avg:60.95ms
step:2145/2285 train_time:130749ms step_avg:60.96ms
step:2146/2285 train_time:130810ms step_avg:60.96ms
step:2147/2285 train_time:130873ms step_avg:60.96ms
step:2148/2285 train_time:130933ms step_avg:60.96ms
step:2149/2285 train_time:130997ms step_avg:60.96ms
step:2150/2285 train_time:131057ms step_avg:60.96ms
step:2151/2285 train_time:131120ms step_avg:60.96ms
step:2152/2285 train_time:131180ms step_avg:60.96ms
step:2153/2285 train_time:131243ms step_avg:60.96ms
step:2154/2285 train_time:131305ms step_avg:60.96ms
step:2155/2285 train_time:131369ms step_avg:60.96ms
step:2156/2285 train_time:131430ms step_avg:60.96ms
step:2157/2285 train_time:131493ms step_avg:60.96ms
step:2158/2285 train_time:131553ms step_avg:60.96ms
step:2159/2285 train_time:131616ms step_avg:60.96ms
step:2160/2285 train_time:131675ms step_avg:60.96ms
step:2161/2285 train_time:131738ms step_avg:60.96ms
step:2162/2285 train_time:131799ms step_avg:60.96ms
step:2163/2285 train_time:131862ms step_avg:60.96ms
step:2164/2285 train_time:131922ms step_avg:60.96ms
step:2165/2285 train_time:131985ms step_avg:60.96ms
step:2166/2285 train_time:132046ms step_avg:60.96ms
step:2167/2285 train_time:132109ms step_avg:60.96ms
step:2168/2285 train_time:132170ms step_avg:60.96ms
step:2169/2285 train_time:132233ms step_avg:60.97ms
step:2170/2285 train_time:132294ms step_avg:60.96ms
step:2171/2285 train_time:132357ms step_avg:60.97ms
step:2172/2285 train_time:132418ms step_avg:60.97ms
step:2173/2285 train_time:132481ms step_avg:60.97ms
step:2174/2285 train_time:132542ms step_avg:60.97ms
step:2175/2285 train_time:132605ms step_avg:60.97ms
step:2176/2285 train_time:132665ms step_avg:60.97ms
step:2177/2285 train_time:132728ms step_avg:60.97ms
step:2178/2285 train_time:132789ms step_avg:60.97ms
step:2179/2285 train_time:132852ms step_avg:60.97ms
step:2180/2285 train_time:132912ms step_avg:60.97ms
step:2181/2285 train_time:132975ms step_avg:60.97ms
step:2182/2285 train_time:133035ms step_avg:60.97ms
step:2183/2285 train_time:133098ms step_avg:60.97ms
step:2184/2285 train_time:133159ms step_avg:60.97ms
step:2185/2285 train_time:133222ms step_avg:60.97ms
step:2186/2285 train_time:133282ms step_avg:60.97ms
step:2187/2285 train_time:133346ms step_avg:60.97ms
step:2188/2285 train_time:133406ms step_avg:60.97ms
step:2189/2285 train_time:133469ms step_avg:60.97ms
step:2190/2285 train_time:133530ms step_avg:60.97ms
step:2191/2285 train_time:133594ms step_avg:60.97ms
step:2192/2285 train_time:133654ms step_avg:60.97ms
step:2193/2285 train_time:133716ms step_avg:60.97ms
step:2194/2285 train_time:133777ms step_avg:60.97ms
step:2195/2285 train_time:133839ms step_avg:60.97ms
step:2196/2285 train_time:133900ms step_avg:60.97ms
step:2197/2285 train_time:133963ms step_avg:60.98ms
step:2198/2285 train_time:134023ms step_avg:60.98ms
step:2199/2285 train_time:134087ms step_avg:60.98ms
step:2200/2285 train_time:134148ms step_avg:60.98ms
step:2201/2285 train_time:134212ms step_avg:60.98ms
step:2202/2285 train_time:134271ms step_avg:60.98ms
step:2203/2285 train_time:134334ms step_avg:60.98ms
step:2204/2285 train_time:134394ms step_avg:60.98ms
step:2205/2285 train_time:134457ms step_avg:60.98ms
step:2206/2285 train_time:134518ms step_avg:60.98ms
step:2207/2285 train_time:134580ms step_avg:60.98ms
step:2208/2285 train_time:134641ms step_avg:60.98ms
step:2209/2285 train_time:134705ms step_avg:60.98ms
step:2210/2285 train_time:134766ms step_avg:60.98ms
step:2211/2285 train_time:134829ms step_avg:60.98ms
step:2212/2285 train_time:134890ms step_avg:60.98ms
step:2213/2285 train_time:134953ms step_avg:60.98ms
step:2214/2285 train_time:135013ms step_avg:60.98ms
step:2215/2285 train_time:135075ms step_avg:60.98ms
step:2216/2285 train_time:135135ms step_avg:60.98ms
step:2217/2285 train_time:135198ms step_avg:60.98ms
step:2218/2285 train_time:135259ms step_avg:60.98ms
step:2219/2285 train_time:135321ms step_avg:60.98ms
step:2220/2285 train_time:135381ms step_avg:60.98ms
step:2221/2285 train_time:135445ms step_avg:60.98ms
step:2222/2285 train_time:135505ms step_avg:60.98ms
step:2223/2285 train_time:135568ms step_avg:60.98ms
step:2224/2285 train_time:135629ms step_avg:60.98ms
step:2225/2285 train_time:135692ms step_avg:60.99ms
step:2226/2285 train_time:135752ms step_avg:60.98ms
step:2227/2285 train_time:135815ms step_avg:60.99ms
step:2228/2285 train_time:135876ms step_avg:60.99ms
step:2229/2285 train_time:135938ms step_avg:60.99ms
step:2230/2285 train_time:135999ms step_avg:60.99ms
step:2231/2285 train_time:136062ms step_avg:60.99ms
step:2232/2285 train_time:136122ms step_avg:60.99ms
step:2233/2285 train_time:136185ms step_avg:60.99ms
step:2234/2285 train_time:136246ms step_avg:60.99ms
step:2235/2285 train_time:136309ms step_avg:60.99ms
step:2236/2285 train_time:136370ms step_avg:60.99ms
step:2237/2285 train_time:136432ms step_avg:60.99ms
step:2238/2285 train_time:136492ms step_avg:60.99ms
step:2239/2285 train_time:136555ms step_avg:60.99ms
step:2240/2285 train_time:136615ms step_avg:60.99ms
step:2241/2285 train_time:136678ms step_avg:60.99ms
step:2242/2285 train_time:136739ms step_avg:60.99ms
step:2243/2285 train_time:136802ms step_avg:60.99ms
step:2244/2285 train_time:136862ms step_avg:60.99ms
step:2245/2285 train_time:136925ms step_avg:60.99ms
step:2246/2285 train_time:136987ms step_avg:60.99ms
step:2247/2285 train_time:137051ms step_avg:60.99ms
step:2248/2285 train_time:137111ms step_avg:60.99ms
step:2249/2285 train_time:137174ms step_avg:60.99ms
step:2250/2285 train_time:137234ms step_avg:60.99ms
step:2250/2285 val_loss:3.2860 train_time:137298ms step_avg:61.02ms
step:2251/2285 train_time:137317ms step_avg:61.00ms
step:2252/2285 train_time:137359ms step_avg:60.99ms
step:2253/2285 train_time:137424ms step_avg:61.00ms
step:2254/2285 train_time:137486ms step_avg:61.00ms
step:2255/2285 train_time:137549ms step_avg:61.00ms
step:2256/2285 train_time:137609ms step_avg:61.00ms
step:2257/2285 train_time:137672ms step_avg:61.00ms
step:2258/2285 train_time:137731ms step_avg:61.00ms
step:2259/2285 train_time:137793ms step_avg:61.00ms
step:2260/2285 train_time:137853ms step_avg:61.00ms
step:2261/2285 train_time:137916ms step_avg:61.00ms
step:2262/2285 train_time:137975ms step_avg:61.00ms
step:2263/2285 train_time:138038ms step_avg:61.00ms
step:2264/2285 train_time:138098ms step_avg:61.00ms
step:2265/2285 train_time:138160ms step_avg:61.00ms
step:2266/2285 train_time:138224ms step_avg:61.00ms
step:2267/2285 train_time:138291ms step_avg:61.00ms
step:2268/2285 train_time:138353ms step_avg:61.00ms
step:2269/2285 train_time:138417ms step_avg:61.00ms
step:2270/2285 train_time:138479ms step_avg:61.00ms
step:2271/2285 train_time:138543ms step_avg:61.01ms
step:2272/2285 train_time:138603ms step_avg:61.01ms
step:2273/2285 train_time:138666ms step_avg:61.01ms
step:2274/2285 train_time:138726ms step_avg:61.01ms
step:2275/2285 train_time:138788ms step_avg:61.01ms
step:2276/2285 train_time:138847ms step_avg:61.00ms
step:2277/2285 train_time:138910ms step_avg:61.01ms
step:2278/2285 train_time:138969ms step_avg:61.01ms
step:2279/2285 train_time:139031ms step_avg:61.01ms
step:2280/2285 train_time:139090ms step_avg:61.00ms
step:2281/2285 train_time:139153ms step_avg:61.01ms
step:2282/2285 train_time:139214ms step_avg:61.01ms
step:2283/2285 train_time:139278ms step_avg:61.01ms
step:2284/2285 train_time:139339ms step_avg:61.01ms
step:2285/2285 train_time:139403ms step_avg:61.01ms
step:2285/2285 val_loss:3.2800 train_time:139464ms step_avg:61.03ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

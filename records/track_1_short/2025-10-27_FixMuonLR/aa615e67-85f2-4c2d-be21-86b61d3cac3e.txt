import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Thu Nov  6 04:29:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     79438      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     79439      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     79440      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     79441      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     79442      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     79443      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     79444      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     79445      C   /root/.venv/bin/python3                         0MiB |
|    1   N/A  N/A     79439      C   /root/.venv/bin/python3                         0MiB |
|    2   N/A  N/A     79440      C   /root/.venv/bin/python3                         0MiB |
|    3   N/A  N/A     79441      C   /root/.venv/bin/python3                         0MiB |
|    4   N/A  N/A     79442      C   /root/.venv/bin/python3                         0MiB |
|    5   N/A  N/A     79443      C   /root/.venv/bin/python3                         0MiB |
|    6   N/A  N/A     79444      C   /root/.venv/bin/python3                         0MiB |
|    7   N/A  N/A     79445      C   /root/.venv/bin/python3                         0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:133ms step_avg:132.75ms
step:2/2285 train_time:155ms step_avg:77.63ms
step:3/2285 train_time:192ms step_avg:64.15ms
step:4/2285 train_time:249ms step_avg:62.13ms
step:5/2285 train_time:308ms step_avg:61.60ms
step:6/2285 train_time:367ms step_avg:61.09ms
step:7/2285 train_time:428ms step_avg:61.11ms
step:8/2285 train_time:486ms step_avg:60.77ms
step:9/2285 train_time:547ms step_avg:60.80ms
step:10/2285 train_time:606ms step_avg:60.57ms
step:11/2285 train_time:667ms step_avg:60.62ms
step:12/2285 train_time:726ms step_avg:60.47ms
step:13/2285 train_time:787ms step_avg:60.57ms
step:14/2285 train_time:846ms step_avg:60.42ms
step:15/2285 train_time:907ms step_avg:60.47ms
step:16/2285 train_time:966ms step_avg:60.40ms
step:17/2285 train_time:1032ms step_avg:60.70ms
step:18/2285 train_time:1096ms step_avg:60.91ms
step:19/2285 train_time:1160ms step_avg:61.06ms
step:20/2285 train_time:1220ms step_avg:60.99ms
step:21/2285 train_time:1282ms step_avg:61.07ms
step:22/2285 train_time:1342ms step_avg:60.99ms
step:23/2285 train_time:1404ms step_avg:61.02ms
step:24/2285 train_time:1462ms step_avg:60.94ms
step:25/2285 train_time:1524ms step_avg:60.97ms
step:26/2285 train_time:1583ms step_avg:60.89ms
step:27/2285 train_time:1645ms step_avg:60.91ms
step:28/2285 train_time:1704ms step_avg:60.84ms
step:29/2285 train_time:1765ms step_avg:60.85ms
step:30/2285 train_time:1823ms step_avg:60.78ms
step:31/2285 train_time:1884ms step_avg:60.78ms
step:32/2285 train_time:1944ms step_avg:60.74ms
step:33/2285 train_time:2007ms step_avg:60.83ms
step:34/2285 train_time:2069ms step_avg:60.86ms
step:35/2285 train_time:2134ms step_avg:60.96ms
step:36/2285 train_time:2194ms step_avg:60.94ms
step:37/2285 train_time:2256ms step_avg:60.98ms
step:38/2285 train_time:2315ms step_avg:60.93ms
step:39/2285 train_time:2377ms step_avg:60.95ms
step:40/2285 train_time:2436ms step_avg:60.90ms
step:41/2285 train_time:2497ms step_avg:60.91ms
step:42/2285 train_time:2557ms step_avg:60.88ms
step:43/2285 train_time:2618ms step_avg:60.89ms
step:44/2285 train_time:2677ms step_avg:60.85ms
step:45/2285 train_time:2738ms step_avg:60.85ms
step:46/2285 train_time:2797ms step_avg:60.81ms
step:47/2285 train_time:2859ms step_avg:60.82ms
step:48/2285 train_time:2918ms step_avg:60.78ms
step:49/2285 train_time:2979ms step_avg:60.81ms
step:50/2285 train_time:3039ms step_avg:60.78ms
step:51/2285 train_time:3101ms step_avg:60.81ms
step:52/2285 train_time:3161ms step_avg:60.78ms
step:53/2285 train_time:3224ms step_avg:60.83ms
step:54/2285 train_time:3283ms step_avg:60.80ms
step:55/2285 train_time:3345ms step_avg:60.83ms
step:56/2285 train_time:3405ms step_avg:60.81ms
step:57/2285 train_time:3467ms step_avg:60.83ms
step:58/2285 train_time:3526ms step_avg:60.80ms
step:59/2285 train_time:3588ms step_avg:60.82ms
step:60/2285 train_time:3648ms step_avg:60.80ms
step:61/2285 train_time:3710ms step_avg:60.81ms
step:62/2285 train_time:3769ms step_avg:60.80ms
step:63/2285 train_time:3831ms step_avg:60.81ms
step:64/2285 train_time:3891ms step_avg:60.79ms
step:65/2285 train_time:3952ms step_avg:60.80ms
step:66/2285 train_time:4011ms step_avg:60.78ms
step:67/2285 train_time:4074ms step_avg:60.81ms
step:68/2285 train_time:4133ms step_avg:60.79ms
step:69/2285 train_time:4196ms step_avg:60.81ms
step:70/2285 train_time:4256ms step_avg:60.80ms
step:71/2285 train_time:4317ms step_avg:60.80ms
step:72/2285 train_time:4376ms step_avg:60.78ms
step:73/2285 train_time:4437ms step_avg:60.78ms
step:74/2285 train_time:4497ms step_avg:60.77ms
step:75/2285 train_time:4559ms step_avg:60.78ms
step:76/2285 train_time:4617ms step_avg:60.75ms
step:77/2285 train_time:4679ms step_avg:60.76ms
step:78/2285 train_time:4738ms step_avg:60.74ms
step:79/2285 train_time:4799ms step_avg:60.75ms
step:80/2285 train_time:4858ms step_avg:60.72ms
step:81/2285 train_time:4920ms step_avg:60.74ms
step:82/2285 train_time:4979ms step_avg:60.72ms
step:83/2285 train_time:5041ms step_avg:60.73ms
step:84/2285 train_time:5100ms step_avg:60.71ms
step:85/2285 train_time:5162ms step_avg:60.73ms
step:86/2285 train_time:5221ms step_avg:60.71ms
step:87/2285 train_time:5283ms step_avg:60.73ms
step:88/2285 train_time:5343ms step_avg:60.71ms
step:89/2285 train_time:5405ms step_avg:60.73ms
step:90/2285 train_time:5464ms step_avg:60.71ms
step:91/2285 train_time:5525ms step_avg:60.72ms
step:92/2285 train_time:5584ms step_avg:60.70ms
step:93/2285 train_time:5646ms step_avg:60.71ms
step:94/2285 train_time:5705ms step_avg:60.69ms
step:95/2285 train_time:5766ms step_avg:60.70ms
step:96/2285 train_time:5825ms step_avg:60.68ms
step:97/2285 train_time:5887ms step_avg:60.69ms
step:98/2285 train_time:5946ms step_avg:60.68ms
step:99/2285 train_time:6008ms step_avg:60.68ms
step:100/2285 train_time:6067ms step_avg:60.67ms
step:101/2285 train_time:6130ms step_avg:60.69ms
step:102/2285 train_time:6189ms step_avg:60.68ms
step:103/2285 train_time:6251ms step_avg:60.69ms
step:104/2285 train_time:6310ms step_avg:60.68ms
step:105/2285 train_time:6372ms step_avg:60.69ms
step:106/2285 train_time:6431ms step_avg:60.67ms
step:107/2285 train_time:6493ms step_avg:60.68ms
step:108/2285 train_time:6553ms step_avg:60.68ms
step:109/2285 train_time:6615ms step_avg:60.69ms
step:110/2285 train_time:6674ms step_avg:60.67ms
step:111/2285 train_time:6735ms step_avg:60.68ms
step:112/2285 train_time:6795ms step_avg:60.67ms
step:113/2285 train_time:6856ms step_avg:60.68ms
step:114/2285 train_time:6915ms step_avg:60.66ms
step:115/2285 train_time:6976ms step_avg:60.66ms
step:116/2285 train_time:7035ms step_avg:60.65ms
step:117/2285 train_time:7096ms step_avg:60.65ms
step:118/2285 train_time:7156ms step_avg:60.64ms
step:119/2285 train_time:7217ms step_avg:60.65ms
step:120/2285 train_time:7276ms step_avg:60.63ms
step:121/2285 train_time:7337ms step_avg:60.64ms
step:122/2285 train_time:7396ms step_avg:60.62ms
step:123/2285 train_time:7458ms step_avg:60.63ms
step:124/2285 train_time:7516ms step_avg:60.62ms
step:125/2285 train_time:7578ms step_avg:60.62ms
step:126/2285 train_time:7636ms step_avg:60.61ms
step:127/2285 train_time:7697ms step_avg:60.61ms
step:128/2285 train_time:7757ms step_avg:60.60ms
step:129/2285 train_time:7818ms step_avg:60.60ms
step:130/2285 train_time:7877ms step_avg:60.59ms
step:131/2285 train_time:7938ms step_avg:60.59ms
step:132/2285 train_time:7996ms step_avg:60.58ms
step:133/2285 train_time:8057ms step_avg:60.58ms
step:134/2285 train_time:8116ms step_avg:60.57ms
step:135/2285 train_time:8177ms step_avg:60.57ms
step:136/2285 train_time:8236ms step_avg:60.56ms
step:137/2285 train_time:8298ms step_avg:60.57ms
step:138/2285 train_time:8357ms step_avg:60.56ms
step:139/2285 train_time:8418ms step_avg:60.56ms
step:140/2285 train_time:8477ms step_avg:60.55ms
step:141/2285 train_time:8538ms step_avg:60.56ms
step:142/2285 train_time:8597ms step_avg:60.54ms
step:143/2285 train_time:8658ms step_avg:60.55ms
step:144/2285 train_time:8717ms step_avg:60.53ms
step:145/2285 train_time:8778ms step_avg:60.54ms
step:146/2285 train_time:8836ms step_avg:60.52ms
step:147/2285 train_time:8897ms step_avg:60.53ms
step:148/2285 train_time:8957ms step_avg:60.52ms
step:149/2285 train_time:9017ms step_avg:60.52ms
step:150/2285 train_time:9076ms step_avg:60.51ms
step:151/2285 train_time:9137ms step_avg:60.51ms
step:152/2285 train_time:9196ms step_avg:60.50ms
step:153/2285 train_time:9258ms step_avg:60.51ms
step:154/2285 train_time:9316ms step_avg:60.49ms
step:155/2285 train_time:9378ms step_avg:60.50ms
step:156/2285 train_time:9436ms step_avg:60.49ms
step:157/2285 train_time:9498ms step_avg:60.49ms
step:158/2285 train_time:9556ms step_avg:60.48ms
step:159/2285 train_time:9617ms step_avg:60.49ms
step:160/2285 train_time:9676ms step_avg:60.47ms
step:161/2285 train_time:9738ms step_avg:60.48ms
step:162/2285 train_time:9796ms step_avg:60.47ms
step:163/2285 train_time:9858ms step_avg:60.48ms
step:164/2285 train_time:9916ms step_avg:60.46ms
step:165/2285 train_time:9977ms step_avg:60.47ms
step:166/2285 train_time:10036ms step_avg:60.46ms
step:167/2285 train_time:10097ms step_avg:60.46ms
step:168/2285 train_time:10156ms step_avg:60.45ms
step:169/2285 train_time:10217ms step_avg:60.46ms
step:170/2285 train_time:10276ms step_avg:60.45ms
step:171/2285 train_time:10337ms step_avg:60.45ms
step:172/2285 train_time:10396ms step_avg:60.44ms
step:173/2285 train_time:10458ms step_avg:60.45ms
step:174/2285 train_time:10516ms step_avg:60.44ms
step:175/2285 train_time:10578ms step_avg:60.44ms
step:176/2285 train_time:10636ms step_avg:60.43ms
step:177/2285 train_time:10697ms step_avg:60.44ms
step:178/2285 train_time:10756ms step_avg:60.43ms
step:179/2285 train_time:10817ms step_avg:60.43ms
step:180/2285 train_time:10876ms step_avg:60.42ms
step:181/2285 train_time:10937ms step_avg:60.43ms
step:182/2285 train_time:10995ms step_avg:60.41ms
step:183/2285 train_time:11056ms step_avg:60.42ms
step:184/2285 train_time:11115ms step_avg:60.41ms
step:185/2285 train_time:11176ms step_avg:60.41ms
step:186/2285 train_time:11235ms step_avg:60.40ms
step:187/2285 train_time:11297ms step_avg:60.41ms
step:188/2285 train_time:11355ms step_avg:60.40ms
step:189/2285 train_time:11416ms step_avg:60.40ms
step:190/2285 train_time:11475ms step_avg:60.39ms
step:191/2285 train_time:11536ms step_avg:60.40ms
step:192/2285 train_time:11595ms step_avg:60.39ms
step:193/2285 train_time:11656ms step_avg:60.39ms
step:194/2285 train_time:11716ms step_avg:60.39ms
step:195/2285 train_time:11777ms step_avg:60.40ms
step:196/2285 train_time:11835ms step_avg:60.39ms
step:197/2285 train_time:11897ms step_avg:60.39ms
step:198/2285 train_time:11956ms step_avg:60.38ms
step:199/2285 train_time:12017ms step_avg:60.38ms
step:200/2285 train_time:12075ms step_avg:60.37ms
step:201/2285 train_time:12136ms step_avg:60.38ms
step:202/2285 train_time:12195ms step_avg:60.37ms
step:203/2285 train_time:12257ms step_avg:60.38ms
step:204/2285 train_time:12315ms step_avg:60.37ms
step:205/2285 train_time:12377ms step_avg:60.37ms
step:206/2285 train_time:12435ms step_avg:60.37ms
step:207/2285 train_time:12496ms step_avg:60.37ms
step:208/2285 train_time:12555ms step_avg:60.36ms
step:209/2285 train_time:12617ms step_avg:60.37ms
step:210/2285 train_time:12676ms step_avg:60.36ms
step:211/2285 train_time:12737ms step_avg:60.36ms
step:212/2285 train_time:12795ms step_avg:60.35ms
step:213/2285 train_time:12857ms step_avg:60.36ms
step:214/2285 train_time:12916ms step_avg:60.35ms
step:215/2285 train_time:12977ms step_avg:60.36ms
step:216/2285 train_time:13035ms step_avg:60.35ms
step:217/2285 train_time:13097ms step_avg:60.35ms
step:218/2285 train_time:13156ms step_avg:60.35ms
step:219/2285 train_time:13217ms step_avg:60.35ms
step:220/2285 train_time:13275ms step_avg:60.34ms
step:221/2285 train_time:13337ms step_avg:60.35ms
step:222/2285 train_time:13395ms step_avg:60.34ms
step:223/2285 train_time:13457ms step_avg:60.35ms
step:224/2285 train_time:13516ms step_avg:60.34ms
step:225/2285 train_time:13577ms step_avg:60.34ms
step:226/2285 train_time:13636ms step_avg:60.34ms
step:227/2285 train_time:13698ms step_avg:60.34ms
step:228/2285 train_time:13756ms step_avg:60.33ms
step:229/2285 train_time:13817ms step_avg:60.34ms
step:230/2285 train_time:13876ms step_avg:60.33ms
step:231/2285 train_time:13937ms step_avg:60.33ms
step:232/2285 train_time:13995ms step_avg:60.33ms
step:233/2285 train_time:14058ms step_avg:60.33ms
step:234/2285 train_time:14116ms step_avg:60.33ms
step:235/2285 train_time:14177ms step_avg:60.33ms
step:236/2285 train_time:14236ms step_avg:60.32ms
step:237/2285 train_time:14298ms step_avg:60.33ms
step:238/2285 train_time:14356ms step_avg:60.32ms
step:239/2285 train_time:14417ms step_avg:60.32ms
step:240/2285 train_time:14476ms step_avg:60.32ms
step:241/2285 train_time:14537ms step_avg:60.32ms
step:242/2285 train_time:14595ms step_avg:60.31ms
step:243/2285 train_time:14657ms step_avg:60.32ms
step:244/2285 train_time:14715ms step_avg:60.31ms
step:245/2285 train_time:14776ms step_avg:60.31ms
step:246/2285 train_time:14835ms step_avg:60.31ms
step:247/2285 train_time:14897ms step_avg:60.31ms
step:248/2285 train_time:14955ms step_avg:60.30ms
step:249/2285 train_time:15016ms step_avg:60.31ms
step:250/2285 train_time:15076ms step_avg:60.30ms
step:250/2285 val_loss:4.0675 train_time:15137ms step_avg:60.55ms
step:251/2285 train_time:15157ms step_avg:60.39ms
step:252/2285 train_time:15196ms step_avg:60.30ms
step:253/2285 train_time:15262ms step_avg:60.32ms
step:254/2285 train_time:15326ms step_avg:60.34ms
step:255/2285 train_time:15388ms step_avg:60.34ms
step:256/2285 train_time:15447ms step_avg:60.34ms
step:257/2285 train_time:15508ms step_avg:60.34ms
step:258/2285 train_time:15566ms step_avg:60.33ms
step:259/2285 train_time:15627ms step_avg:60.34ms
step:260/2285 train_time:15685ms step_avg:60.33ms
step:261/2285 train_time:15746ms step_avg:60.33ms
step:262/2285 train_time:15805ms step_avg:60.32ms
step:263/2285 train_time:15865ms step_avg:60.32ms
step:264/2285 train_time:15923ms step_avg:60.31ms
step:265/2285 train_time:15983ms step_avg:60.31ms
step:266/2285 train_time:16043ms step_avg:60.31ms
step:267/2285 train_time:16106ms step_avg:60.32ms
step:268/2285 train_time:16165ms step_avg:60.32ms
step:269/2285 train_time:16228ms step_avg:60.33ms
step:270/2285 train_time:16288ms step_avg:60.33ms
step:271/2285 train_time:16350ms step_avg:60.33ms
step:272/2285 train_time:16409ms step_avg:60.33ms
step:273/2285 train_time:16470ms step_avg:60.33ms
step:274/2285 train_time:16529ms step_avg:60.33ms
step:275/2285 train_time:16590ms step_avg:60.33ms
step:276/2285 train_time:16649ms step_avg:60.32ms
step:277/2285 train_time:16710ms step_avg:60.32ms
step:278/2285 train_time:16769ms step_avg:60.32ms
step:279/2285 train_time:16830ms step_avg:60.32ms
step:280/2285 train_time:16888ms step_avg:60.31ms
step:281/2285 train_time:16949ms step_avg:60.32ms
step:282/2285 train_time:17008ms step_avg:60.31ms
step:283/2285 train_time:17068ms step_avg:60.31ms
step:284/2285 train_time:17127ms step_avg:60.31ms
step:285/2285 train_time:17189ms step_avg:60.31ms
step:286/2285 train_time:17248ms step_avg:60.31ms
step:287/2285 train_time:17310ms step_avg:60.31ms
step:288/2285 train_time:17369ms step_avg:60.31ms
step:289/2285 train_time:17432ms step_avg:60.32ms
step:290/2285 train_time:17490ms step_avg:60.31ms
step:291/2285 train_time:17551ms step_avg:60.31ms
step:292/2285 train_time:17610ms step_avg:60.31ms
step:293/2285 train_time:17672ms step_avg:60.31ms
step:294/2285 train_time:17730ms step_avg:60.30ms
step:295/2285 train_time:17791ms step_avg:60.31ms
step:296/2285 train_time:17849ms step_avg:60.30ms
step:297/2285 train_time:17910ms step_avg:60.30ms
step:298/2285 train_time:17969ms step_avg:60.30ms
step:299/2285 train_time:18031ms step_avg:60.30ms
step:300/2285 train_time:18090ms step_avg:60.30ms
step:301/2285 train_time:18151ms step_avg:60.30ms
step:302/2285 train_time:18210ms step_avg:60.30ms
step:303/2285 train_time:18272ms step_avg:60.30ms
step:304/2285 train_time:18331ms step_avg:60.30ms
step:305/2285 train_time:18393ms step_avg:60.30ms
step:306/2285 train_time:18452ms step_avg:60.30ms
step:307/2285 train_time:18513ms step_avg:60.30ms
step:308/2285 train_time:18572ms step_avg:60.30ms
step:309/2285 train_time:18634ms step_avg:60.30ms
step:310/2285 train_time:18692ms step_avg:60.30ms
step:311/2285 train_time:18754ms step_avg:60.30ms
step:312/2285 train_time:18813ms step_avg:60.30ms
step:313/2285 train_time:18874ms step_avg:60.30ms
step:314/2285 train_time:18932ms step_avg:60.29ms
step:315/2285 train_time:18994ms step_avg:60.30ms
step:316/2285 train_time:19052ms step_avg:60.29ms
step:317/2285 train_time:19114ms step_avg:60.30ms
step:318/2285 train_time:19173ms step_avg:60.29ms
step:319/2285 train_time:19234ms step_avg:60.29ms
step:320/2285 train_time:19293ms step_avg:60.29ms
step:321/2285 train_time:19354ms step_avg:60.29ms
step:322/2285 train_time:19413ms step_avg:60.29ms
step:323/2285 train_time:19475ms step_avg:60.29ms
step:324/2285 train_time:19534ms step_avg:60.29ms
step:325/2285 train_time:19595ms step_avg:60.29ms
step:326/2285 train_time:19654ms step_avg:60.29ms
step:327/2285 train_time:19716ms step_avg:60.29ms
step:328/2285 train_time:19774ms step_avg:60.29ms
step:329/2285 train_time:19836ms step_avg:60.29ms
step:330/2285 train_time:19895ms step_avg:60.29ms
step:331/2285 train_time:19957ms step_avg:60.29ms
step:332/2285 train_time:20016ms step_avg:60.29ms
step:333/2285 train_time:20077ms step_avg:60.29ms
step:334/2285 train_time:20136ms step_avg:60.29ms
step:335/2285 train_time:20197ms step_avg:60.29ms
step:336/2285 train_time:20256ms step_avg:60.29ms
step:337/2285 train_time:20318ms step_avg:60.29ms
step:338/2285 train_time:20378ms step_avg:60.29ms
step:339/2285 train_time:20439ms step_avg:60.29ms
step:340/2285 train_time:20498ms step_avg:60.29ms
step:341/2285 train_time:20560ms step_avg:60.29ms
step:342/2285 train_time:20619ms step_avg:60.29ms
step:343/2285 train_time:20681ms step_avg:60.29ms
step:344/2285 train_time:20740ms step_avg:60.29ms
step:345/2285 train_time:20801ms step_avg:60.29ms
step:346/2285 train_time:20861ms step_avg:60.29ms
step:347/2285 train_time:20922ms step_avg:60.29ms
step:348/2285 train_time:20982ms step_avg:60.29ms
step:349/2285 train_time:21043ms step_avg:60.30ms
step:350/2285 train_time:21102ms step_avg:60.29ms
step:351/2285 train_time:21163ms step_avg:60.29ms
step:352/2285 train_time:21222ms step_avg:60.29ms
step:353/2285 train_time:21284ms step_avg:60.29ms
step:354/2285 train_time:21343ms step_avg:60.29ms
step:355/2285 train_time:21405ms step_avg:60.29ms
step:356/2285 train_time:21464ms step_avg:60.29ms
step:357/2285 train_time:21525ms step_avg:60.29ms
step:358/2285 train_time:21584ms step_avg:60.29ms
step:359/2285 train_time:21646ms step_avg:60.30ms
step:360/2285 train_time:21705ms step_avg:60.29ms
step:361/2285 train_time:21766ms step_avg:60.29ms
step:362/2285 train_time:21825ms step_avg:60.29ms
step:363/2285 train_time:21887ms step_avg:60.29ms
step:364/2285 train_time:21946ms step_avg:60.29ms
step:365/2285 train_time:22007ms step_avg:60.29ms
step:366/2285 train_time:22066ms step_avg:60.29ms
step:367/2285 train_time:22127ms step_avg:60.29ms
step:368/2285 train_time:22186ms step_avg:60.29ms
step:369/2285 train_time:22247ms step_avg:60.29ms
step:370/2285 train_time:22306ms step_avg:60.29ms
step:371/2285 train_time:22367ms step_avg:60.29ms
step:372/2285 train_time:22426ms step_avg:60.29ms
step:373/2285 train_time:22488ms step_avg:60.29ms
step:374/2285 train_time:22546ms step_avg:60.28ms
step:375/2285 train_time:22608ms step_avg:60.29ms
step:376/2285 train_time:22666ms step_avg:60.28ms
step:377/2285 train_time:22727ms step_avg:60.28ms
step:378/2285 train_time:22786ms step_avg:60.28ms
step:379/2285 train_time:22847ms step_avg:60.28ms
step:380/2285 train_time:22906ms step_avg:60.28ms
step:381/2285 train_time:22967ms step_avg:60.28ms
step:382/2285 train_time:23025ms step_avg:60.28ms
step:383/2285 train_time:23087ms step_avg:60.28ms
step:384/2285 train_time:23146ms step_avg:60.28ms
step:385/2285 train_time:23207ms step_avg:60.28ms
step:386/2285 train_time:23265ms step_avg:60.27ms
step:387/2285 train_time:23327ms step_avg:60.28ms
step:388/2285 train_time:23386ms step_avg:60.27ms
step:389/2285 train_time:23448ms step_avg:60.28ms
step:390/2285 train_time:23506ms step_avg:60.27ms
step:391/2285 train_time:23568ms step_avg:60.28ms
step:392/2285 train_time:23626ms step_avg:60.27ms
step:393/2285 train_time:23689ms step_avg:60.28ms
step:394/2285 train_time:23747ms step_avg:60.27ms
step:395/2285 train_time:23808ms step_avg:60.27ms
step:396/2285 train_time:23866ms step_avg:60.27ms
step:397/2285 train_time:23928ms step_avg:60.27ms
step:398/2285 train_time:23987ms step_avg:60.27ms
step:399/2285 train_time:24048ms step_avg:60.27ms
step:400/2285 train_time:24106ms step_avg:60.27ms
step:401/2285 train_time:24167ms step_avg:60.27ms
step:402/2285 train_time:24226ms step_avg:60.26ms
step:403/2285 train_time:24287ms step_avg:60.27ms
step:404/2285 train_time:24346ms step_avg:60.26ms
step:405/2285 train_time:24408ms step_avg:60.27ms
step:406/2285 train_time:24466ms step_avg:60.26ms
step:407/2285 train_time:24528ms step_avg:60.27ms
step:408/2285 train_time:24587ms step_avg:60.26ms
step:409/2285 train_time:24649ms step_avg:60.27ms
step:410/2285 train_time:24707ms step_avg:60.26ms
step:411/2285 train_time:24768ms step_avg:60.26ms
step:412/2285 train_time:24828ms step_avg:60.26ms
step:413/2285 train_time:24889ms step_avg:60.26ms
step:414/2285 train_time:24948ms step_avg:60.26ms
step:415/2285 train_time:25009ms step_avg:60.26ms
step:416/2285 train_time:25067ms step_avg:60.26ms
step:417/2285 train_time:25129ms step_avg:60.26ms
step:418/2285 train_time:25187ms step_avg:60.26ms
step:419/2285 train_time:25249ms step_avg:60.26ms
step:420/2285 train_time:25307ms step_avg:60.26ms
step:421/2285 train_time:25368ms step_avg:60.26ms
step:422/2285 train_time:25427ms step_avg:60.25ms
step:423/2285 train_time:25488ms step_avg:60.25ms
step:424/2285 train_time:25546ms step_avg:60.25ms
step:425/2285 train_time:25607ms step_avg:60.25ms
step:426/2285 train_time:25666ms step_avg:60.25ms
step:427/2285 train_time:25728ms step_avg:60.25ms
step:428/2285 train_time:25787ms step_avg:60.25ms
step:429/2285 train_time:25848ms step_avg:60.25ms
step:430/2285 train_time:25906ms step_avg:60.25ms
step:431/2285 train_time:25967ms step_avg:60.25ms
step:432/2285 train_time:26026ms step_avg:60.24ms
step:433/2285 train_time:26087ms step_avg:60.25ms
step:434/2285 train_time:26146ms step_avg:60.24ms
step:435/2285 train_time:26207ms step_avg:60.25ms
step:436/2285 train_time:26265ms step_avg:60.24ms
step:437/2285 train_time:26327ms step_avg:60.24ms
step:438/2285 train_time:26386ms step_avg:60.24ms
step:439/2285 train_time:26447ms step_avg:60.24ms
step:440/2285 train_time:26506ms step_avg:60.24ms
step:441/2285 train_time:26567ms step_avg:60.24ms
step:442/2285 train_time:26625ms step_avg:60.24ms
step:443/2285 train_time:26687ms step_avg:60.24ms
step:444/2285 train_time:26746ms step_avg:60.24ms
step:445/2285 train_time:26808ms step_avg:60.24ms
step:446/2285 train_time:26866ms step_avg:60.24ms
step:447/2285 train_time:26928ms step_avg:60.24ms
step:448/2285 train_time:26986ms step_avg:60.24ms
step:449/2285 train_time:27048ms step_avg:60.24ms
step:450/2285 train_time:27106ms step_avg:60.24ms
step:451/2285 train_time:27167ms step_avg:60.24ms
step:452/2285 train_time:27226ms step_avg:60.23ms
step:453/2285 train_time:27288ms step_avg:60.24ms
step:454/2285 train_time:27346ms step_avg:60.23ms
step:455/2285 train_time:27407ms step_avg:60.24ms
step:456/2285 train_time:27466ms step_avg:60.23ms
step:457/2285 train_time:27527ms step_avg:60.23ms
step:458/2285 train_time:27586ms step_avg:60.23ms
step:459/2285 train_time:27648ms step_avg:60.23ms
step:460/2285 train_time:27706ms step_avg:60.23ms
step:461/2285 train_time:27767ms step_avg:60.23ms
step:462/2285 train_time:27826ms step_avg:60.23ms
step:463/2285 train_time:27887ms step_avg:60.23ms
step:464/2285 train_time:27946ms step_avg:60.23ms
step:465/2285 train_time:28007ms step_avg:60.23ms
step:466/2285 train_time:28066ms step_avg:60.23ms
step:467/2285 train_time:28127ms step_avg:60.23ms
step:468/2285 train_time:28186ms step_avg:60.23ms
step:469/2285 train_time:28247ms step_avg:60.23ms
step:470/2285 train_time:28306ms step_avg:60.23ms
step:471/2285 train_time:28367ms step_avg:60.23ms
step:472/2285 train_time:28425ms step_avg:60.22ms
step:473/2285 train_time:28487ms step_avg:60.23ms
step:474/2285 train_time:28545ms step_avg:60.22ms
step:475/2285 train_time:28608ms step_avg:60.23ms
step:476/2285 train_time:28666ms step_avg:60.22ms
step:477/2285 train_time:28728ms step_avg:60.23ms
step:478/2285 train_time:28786ms step_avg:60.22ms
step:479/2285 train_time:28848ms step_avg:60.22ms
step:480/2285 train_time:28906ms step_avg:60.22ms
step:481/2285 train_time:28968ms step_avg:60.22ms
step:482/2285 train_time:29026ms step_avg:60.22ms
step:483/2285 train_time:29088ms step_avg:60.22ms
step:484/2285 train_time:29146ms step_avg:60.22ms
step:485/2285 train_time:29208ms step_avg:60.22ms
step:486/2285 train_time:29266ms step_avg:60.22ms
step:487/2285 train_time:29327ms step_avg:60.22ms
step:488/2285 train_time:29386ms step_avg:60.22ms
step:489/2285 train_time:29447ms step_avg:60.22ms
step:490/2285 train_time:29506ms step_avg:60.22ms
step:491/2285 train_time:29567ms step_avg:60.22ms
step:492/2285 train_time:29625ms step_avg:60.21ms
step:493/2285 train_time:29686ms step_avg:60.22ms
step:494/2285 train_time:29745ms step_avg:60.21ms
step:495/2285 train_time:29807ms step_avg:60.22ms
step:496/2285 train_time:29865ms step_avg:60.21ms
step:497/2285 train_time:29927ms step_avg:60.21ms
step:498/2285 train_time:29986ms step_avg:60.21ms
step:499/2285 train_time:30047ms step_avg:60.21ms
step:500/2285 train_time:30106ms step_avg:60.21ms
step:500/2285 val_loss:3.8068 train_time:30168ms step_avg:60.34ms
step:501/2285 train_time:30189ms step_avg:60.26ms
step:502/2285 train_time:30228ms step_avg:60.22ms
step:503/2285 train_time:30294ms step_avg:60.23ms
step:504/2285 train_time:30356ms step_avg:60.23ms
step:505/2285 train_time:30417ms step_avg:60.23ms
step:506/2285 train_time:30475ms step_avg:60.23ms
step:507/2285 train_time:30536ms step_avg:60.23ms
step:508/2285 train_time:30594ms step_avg:60.23ms
step:509/2285 train_time:30655ms step_avg:60.23ms
step:510/2285 train_time:30713ms step_avg:60.22ms
step:511/2285 train_time:30775ms step_avg:60.22ms
step:512/2285 train_time:30834ms step_avg:60.22ms
step:513/2285 train_time:30894ms step_avg:60.22ms
step:514/2285 train_time:30953ms step_avg:60.22ms
step:515/2285 train_time:31014ms step_avg:60.22ms
step:516/2285 train_time:31072ms step_avg:60.22ms
step:517/2285 train_time:31135ms step_avg:60.22ms
step:518/2285 train_time:31195ms step_avg:60.22ms
step:519/2285 train_time:31258ms step_avg:60.23ms
step:520/2285 train_time:31317ms step_avg:60.23ms
step:521/2285 train_time:31379ms step_avg:60.23ms
step:522/2285 train_time:31438ms step_avg:60.23ms
step:523/2285 train_time:31499ms step_avg:60.23ms
step:524/2285 train_time:31558ms step_avg:60.22ms
step:525/2285 train_time:31619ms step_avg:60.23ms
step:526/2285 train_time:31677ms step_avg:60.22ms
step:527/2285 train_time:31738ms step_avg:60.22ms
step:528/2285 train_time:31796ms step_avg:60.22ms
step:529/2285 train_time:31857ms step_avg:60.22ms
step:530/2285 train_time:31915ms step_avg:60.22ms
step:531/2285 train_time:31976ms step_avg:60.22ms
step:532/2285 train_time:32034ms step_avg:60.22ms
step:533/2285 train_time:32096ms step_avg:60.22ms
step:534/2285 train_time:32156ms step_avg:60.22ms
step:535/2285 train_time:32218ms step_avg:60.22ms
step:536/2285 train_time:32278ms step_avg:60.22ms
step:537/2285 train_time:32341ms step_avg:60.22ms
step:538/2285 train_time:32400ms step_avg:60.22ms
step:539/2285 train_time:32461ms step_avg:60.22ms
step:540/2285 train_time:32519ms step_avg:60.22ms
step:541/2285 train_time:32581ms step_avg:60.22ms
step:542/2285 train_time:32640ms step_avg:60.22ms
step:543/2285 train_time:32701ms step_avg:60.22ms
step:544/2285 train_time:32760ms step_avg:60.22ms
step:545/2285 train_time:32822ms step_avg:60.22ms
step:546/2285 train_time:32880ms step_avg:60.22ms
step:547/2285 train_time:32942ms step_avg:60.22ms
step:548/2285 train_time:33002ms step_avg:60.22ms
step:549/2285 train_time:33064ms step_avg:60.23ms
step:550/2285 train_time:33124ms step_avg:60.22ms
step:551/2285 train_time:33186ms step_avg:60.23ms
step:552/2285 train_time:33245ms step_avg:60.23ms
step:553/2285 train_time:33308ms step_avg:60.23ms
step:554/2285 train_time:33366ms step_avg:60.23ms
step:555/2285 train_time:33428ms step_avg:60.23ms
step:556/2285 train_time:33487ms step_avg:60.23ms
step:557/2285 train_time:33548ms step_avg:60.23ms
step:558/2285 train_time:33607ms step_avg:60.23ms
step:559/2285 train_time:33669ms step_avg:60.23ms
step:560/2285 train_time:33727ms step_avg:60.23ms
step:561/2285 train_time:33788ms step_avg:60.23ms
step:562/2285 train_time:33847ms step_avg:60.23ms
step:563/2285 train_time:33909ms step_avg:60.23ms
step:564/2285 train_time:33968ms step_avg:60.23ms
step:565/2285 train_time:34030ms step_avg:60.23ms
step:566/2285 train_time:34089ms step_avg:60.23ms
step:567/2285 train_time:34151ms step_avg:60.23ms
step:568/2285 train_time:34210ms step_avg:60.23ms
step:569/2285 train_time:34272ms step_avg:60.23ms
step:570/2285 train_time:34330ms step_avg:60.23ms
step:571/2285 train_time:34392ms step_avg:60.23ms
step:572/2285 train_time:34451ms step_avg:60.23ms
step:573/2285 train_time:34513ms step_avg:60.23ms
step:574/2285 train_time:34572ms step_avg:60.23ms
step:575/2285 train_time:34634ms step_avg:60.23ms
step:576/2285 train_time:34693ms step_avg:60.23ms
step:577/2285 train_time:34755ms step_avg:60.23ms
step:578/2285 train_time:34814ms step_avg:60.23ms
step:579/2285 train_time:34875ms step_avg:60.23ms
step:580/2285 train_time:34934ms step_avg:60.23ms
step:581/2285 train_time:34995ms step_avg:60.23ms
step:582/2285 train_time:35054ms step_avg:60.23ms
step:583/2285 train_time:35116ms step_avg:60.23ms
step:584/2285 train_time:35175ms step_avg:60.23ms
step:585/2285 train_time:35238ms step_avg:60.24ms
step:586/2285 train_time:35296ms step_avg:60.23ms
step:587/2285 train_time:35357ms step_avg:60.23ms
step:588/2285 train_time:35416ms step_avg:60.23ms
step:589/2285 train_time:35478ms step_avg:60.23ms
step:590/2285 train_time:35537ms step_avg:60.23ms
step:591/2285 train_time:35598ms step_avg:60.23ms
step:592/2285 train_time:35657ms step_avg:60.23ms
step:593/2285 train_time:35718ms step_avg:60.23ms
step:594/2285 train_time:35777ms step_avg:60.23ms
step:595/2285 train_time:35839ms step_avg:60.23ms
step:596/2285 train_time:35897ms step_avg:60.23ms
step:597/2285 train_time:35959ms step_avg:60.23ms
step:598/2285 train_time:36018ms step_avg:60.23ms
step:599/2285 train_time:36080ms step_avg:60.23ms
step:600/2285 train_time:36138ms step_avg:60.23ms
step:601/2285 train_time:36199ms step_avg:60.23ms
step:602/2285 train_time:36259ms step_avg:60.23ms
step:603/2285 train_time:36320ms step_avg:60.23ms
step:604/2285 train_time:36379ms step_avg:60.23ms
step:605/2285 train_time:36441ms step_avg:60.23ms
step:606/2285 train_time:36500ms step_avg:60.23ms
step:607/2285 train_time:36561ms step_avg:60.23ms
step:608/2285 train_time:36620ms step_avg:60.23ms
step:609/2285 train_time:36682ms step_avg:60.23ms
step:610/2285 train_time:36741ms step_avg:60.23ms
step:611/2285 train_time:36803ms step_avg:60.23ms
step:612/2285 train_time:36862ms step_avg:60.23ms
step:613/2285 train_time:36924ms step_avg:60.23ms
step:614/2285 train_time:36983ms step_avg:60.23ms
step:615/2285 train_time:37045ms step_avg:60.23ms
step:616/2285 train_time:37104ms step_avg:60.23ms
step:617/2285 train_time:37166ms step_avg:60.24ms
step:618/2285 train_time:37225ms step_avg:60.23ms
step:619/2285 train_time:37286ms step_avg:60.24ms
step:620/2285 train_time:37345ms step_avg:60.23ms
step:621/2285 train_time:37407ms step_avg:60.24ms
step:622/2285 train_time:37467ms step_avg:60.24ms
step:623/2285 train_time:37528ms step_avg:60.24ms
step:624/2285 train_time:37587ms step_avg:60.24ms
step:625/2285 train_time:37649ms step_avg:60.24ms
step:626/2285 train_time:37708ms step_avg:60.24ms
step:627/2285 train_time:37770ms step_avg:60.24ms
step:628/2285 train_time:37830ms step_avg:60.24ms
step:629/2285 train_time:37892ms step_avg:60.24ms
step:630/2285 train_time:37951ms step_avg:60.24ms
step:631/2285 train_time:38013ms step_avg:60.24ms
step:632/2285 train_time:38072ms step_avg:60.24ms
step:633/2285 train_time:38135ms step_avg:60.24ms
step:634/2285 train_time:38194ms step_avg:60.24ms
step:635/2285 train_time:38255ms step_avg:60.24ms
step:636/2285 train_time:38314ms step_avg:60.24ms
step:637/2285 train_time:38375ms step_avg:60.24ms
step:638/2285 train_time:38434ms step_avg:60.24ms
step:639/2285 train_time:38496ms step_avg:60.24ms
step:640/2285 train_time:38554ms step_avg:60.24ms
step:641/2285 train_time:38617ms step_avg:60.24ms
step:642/2285 train_time:38675ms step_avg:60.24ms
step:643/2285 train_time:38736ms step_avg:60.24ms
step:644/2285 train_time:38796ms step_avg:60.24ms
step:645/2285 train_time:38857ms step_avg:60.24ms
step:646/2285 train_time:38916ms step_avg:60.24ms
step:647/2285 train_time:38978ms step_avg:60.24ms
step:648/2285 train_time:39036ms step_avg:60.24ms
step:649/2285 train_time:39098ms step_avg:60.24ms
step:650/2285 train_time:39156ms step_avg:60.24ms
step:651/2285 train_time:39217ms step_avg:60.24ms
step:652/2285 train_time:39276ms step_avg:60.24ms
step:653/2285 train_time:39338ms step_avg:60.24ms
step:654/2285 train_time:39396ms step_avg:60.24ms
step:655/2285 train_time:39458ms step_avg:60.24ms
step:656/2285 train_time:39516ms step_avg:60.24ms
step:657/2285 train_time:39579ms step_avg:60.24ms
step:658/2285 train_time:39637ms step_avg:60.24ms
step:659/2285 train_time:39698ms step_avg:60.24ms
step:660/2285 train_time:39756ms step_avg:60.24ms
step:661/2285 train_time:39818ms step_avg:60.24ms
step:662/2285 train_time:39877ms step_avg:60.24ms
step:663/2285 train_time:39939ms step_avg:60.24ms
step:664/2285 train_time:39999ms step_avg:60.24ms
step:665/2285 train_time:40061ms step_avg:60.24ms
step:666/2285 train_time:40120ms step_avg:60.24ms
step:667/2285 train_time:40181ms step_avg:60.24ms
step:668/2285 train_time:40240ms step_avg:60.24ms
step:669/2285 train_time:40303ms step_avg:60.24ms
step:670/2285 train_time:40362ms step_avg:60.24ms
step:671/2285 train_time:40424ms step_avg:60.24ms
step:672/2285 train_time:40483ms step_avg:60.24ms
step:673/2285 train_time:40545ms step_avg:60.25ms
step:674/2285 train_time:40604ms step_avg:60.24ms
step:675/2285 train_time:40666ms step_avg:60.25ms
step:676/2285 train_time:40725ms step_avg:60.24ms
step:677/2285 train_time:40787ms step_avg:60.25ms
step:678/2285 train_time:40846ms step_avg:60.25ms
step:679/2285 train_time:40908ms step_avg:60.25ms
step:680/2285 train_time:40967ms step_avg:60.25ms
step:681/2285 train_time:41029ms step_avg:60.25ms
step:682/2285 train_time:41087ms step_avg:60.25ms
step:683/2285 train_time:41149ms step_avg:60.25ms
step:684/2285 train_time:41209ms step_avg:60.25ms
step:685/2285 train_time:41270ms step_avg:60.25ms
step:686/2285 train_time:41329ms step_avg:60.25ms
step:687/2285 train_time:41391ms step_avg:60.25ms
step:688/2285 train_time:41451ms step_avg:60.25ms
step:689/2285 train_time:41512ms step_avg:60.25ms
step:690/2285 train_time:41572ms step_avg:60.25ms
step:691/2285 train_time:41633ms step_avg:60.25ms
step:692/2285 train_time:41693ms step_avg:60.25ms
step:693/2285 train_time:41755ms step_avg:60.25ms
step:694/2285 train_time:41814ms step_avg:60.25ms
step:695/2285 train_time:41875ms step_avg:60.25ms
step:696/2285 train_time:41934ms step_avg:60.25ms
step:697/2285 train_time:41995ms step_avg:60.25ms
step:698/2285 train_time:42054ms step_avg:60.25ms
step:699/2285 train_time:42115ms step_avg:60.25ms
step:700/2285 train_time:42174ms step_avg:60.25ms
step:701/2285 train_time:42236ms step_avg:60.25ms
step:702/2285 train_time:42296ms step_avg:60.25ms
step:703/2285 train_time:42357ms step_avg:60.25ms
step:704/2285 train_time:42415ms step_avg:60.25ms
step:705/2285 train_time:42477ms step_avg:60.25ms
step:706/2285 train_time:42537ms step_avg:60.25ms
step:707/2285 train_time:42598ms step_avg:60.25ms
step:708/2285 train_time:42657ms step_avg:60.25ms
step:709/2285 train_time:42718ms step_avg:60.25ms
step:710/2285 train_time:42777ms step_avg:60.25ms
step:711/2285 train_time:42839ms step_avg:60.25ms
step:712/2285 train_time:42897ms step_avg:60.25ms
step:713/2285 train_time:42958ms step_avg:60.25ms
step:714/2285 train_time:43017ms step_avg:60.25ms
step:715/2285 train_time:43079ms step_avg:60.25ms
step:716/2285 train_time:43137ms step_avg:60.25ms
step:717/2285 train_time:43198ms step_avg:60.25ms
step:718/2285 train_time:43257ms step_avg:60.25ms
step:719/2285 train_time:43318ms step_avg:60.25ms
step:720/2285 train_time:43377ms step_avg:60.25ms
step:721/2285 train_time:43438ms step_avg:60.25ms
step:722/2285 train_time:43497ms step_avg:60.25ms
step:723/2285 train_time:43559ms step_avg:60.25ms
step:724/2285 train_time:43617ms step_avg:60.24ms
step:725/2285 train_time:43679ms step_avg:60.25ms
step:726/2285 train_time:43737ms step_avg:60.24ms
step:727/2285 train_time:43798ms step_avg:60.25ms
step:728/2285 train_time:43857ms step_avg:60.24ms
step:729/2285 train_time:43918ms step_avg:60.24ms
step:730/2285 train_time:43977ms step_avg:60.24ms
step:731/2285 train_time:44038ms step_avg:60.24ms
step:732/2285 train_time:44096ms step_avg:60.24ms
step:733/2285 train_time:44158ms step_avg:60.24ms
step:734/2285 train_time:44216ms step_avg:60.24ms
step:735/2285 train_time:44278ms step_avg:60.24ms
step:736/2285 train_time:44337ms step_avg:60.24ms
step:737/2285 train_time:44398ms step_avg:60.24ms
step:738/2285 train_time:44457ms step_avg:60.24ms
step:739/2285 train_time:44519ms step_avg:60.24ms
step:740/2285 train_time:44578ms step_avg:60.24ms
step:741/2285 train_time:44640ms step_avg:60.24ms
step:742/2285 train_time:44698ms step_avg:60.24ms
step:743/2285 train_time:44759ms step_avg:60.24ms
step:744/2285 train_time:44818ms step_avg:60.24ms
step:745/2285 train_time:44880ms step_avg:60.24ms
step:746/2285 train_time:44938ms step_avg:60.24ms
step:747/2285 train_time:44999ms step_avg:60.24ms
step:748/2285 train_time:45058ms step_avg:60.24ms
step:749/2285 train_time:45120ms step_avg:60.24ms
step:750/2285 train_time:45179ms step_avg:60.24ms
step:750/2285 val_loss:3.6692 train_time:45243ms step_avg:60.32ms
step:751/2285 train_time:45263ms step_avg:60.27ms
step:752/2285 train_time:45304ms step_avg:60.25ms
step:753/2285 train_time:45366ms step_avg:60.25ms
step:754/2285 train_time:45426ms step_avg:60.25ms
step:755/2285 train_time:45489ms step_avg:60.25ms
step:756/2285 train_time:45549ms step_avg:60.25ms
step:757/2285 train_time:45610ms step_avg:60.25ms
step:758/2285 train_time:45669ms step_avg:60.25ms
step:759/2285 train_time:45730ms step_avg:60.25ms
step:760/2285 train_time:45788ms step_avg:60.25ms
step:761/2285 train_time:45849ms step_avg:60.25ms
step:762/2285 train_time:45908ms step_avg:60.25ms
step:763/2285 train_time:45970ms step_avg:60.25ms
step:764/2285 train_time:46029ms step_avg:60.25ms
step:765/2285 train_time:46091ms step_avg:60.25ms
step:766/2285 train_time:46154ms step_avg:60.25ms
step:767/2285 train_time:46222ms step_avg:60.26ms
step:768/2285 train_time:46283ms step_avg:60.26ms
step:769/2285 train_time:46346ms step_avg:60.27ms
step:770/2285 train_time:46405ms step_avg:60.27ms
step:771/2285 train_time:46468ms step_avg:60.27ms
step:772/2285 train_time:46528ms step_avg:60.27ms
step:773/2285 train_time:46590ms step_avg:60.27ms
step:774/2285 train_time:46649ms step_avg:60.27ms
step:775/2285 train_time:46710ms step_avg:60.27ms
step:776/2285 train_time:46769ms step_avg:60.27ms
step:777/2285 train_time:46831ms step_avg:60.27ms
step:778/2285 train_time:46889ms step_avg:60.27ms
step:779/2285 train_time:46951ms step_avg:60.27ms
step:780/2285 train_time:47011ms step_avg:60.27ms
step:781/2285 train_time:47072ms step_avg:60.27ms
step:782/2285 train_time:47133ms step_avg:60.27ms
step:783/2285 train_time:47197ms step_avg:60.28ms
step:784/2285 train_time:47257ms step_avg:60.28ms
step:785/2285 train_time:47320ms step_avg:60.28ms
step:786/2285 train_time:47379ms step_avg:60.28ms
step:787/2285 train_time:47441ms step_avg:60.28ms
step:788/2285 train_time:47501ms step_avg:60.28ms
step:789/2285 train_time:47563ms step_avg:60.28ms
step:790/2285 train_time:47623ms step_avg:60.28ms
step:791/2285 train_time:47685ms step_avg:60.28ms
step:792/2285 train_time:47744ms step_avg:60.28ms
step:793/2285 train_time:47806ms step_avg:60.29ms
step:794/2285 train_time:47866ms step_avg:60.28ms
step:795/2285 train_time:47928ms step_avg:60.29ms
step:796/2285 train_time:47987ms step_avg:60.28ms
step:797/2285 train_time:48049ms step_avg:60.29ms
step:798/2285 train_time:48110ms step_avg:60.29ms
step:799/2285 train_time:48174ms step_avg:60.29ms
step:800/2285 train_time:48235ms step_avg:60.29ms
step:801/2285 train_time:48298ms step_avg:60.30ms
step:802/2285 train_time:48358ms step_avg:60.30ms
step:803/2285 train_time:48421ms step_avg:60.30ms
step:804/2285 train_time:48480ms step_avg:60.30ms
step:805/2285 train_time:48542ms step_avg:60.30ms
step:806/2285 train_time:48602ms step_avg:60.30ms
step:807/2285 train_time:48663ms step_avg:60.30ms
step:808/2285 train_time:48723ms step_avg:60.30ms
step:809/2285 train_time:48784ms step_avg:60.30ms
step:810/2285 train_time:48844ms step_avg:60.30ms
step:811/2285 train_time:48905ms step_avg:60.30ms
step:812/2285 train_time:48965ms step_avg:60.30ms
step:813/2285 train_time:49027ms step_avg:60.30ms
step:814/2285 train_time:49087ms step_avg:60.30ms
step:815/2285 train_time:49150ms step_avg:60.31ms
step:816/2285 train_time:49211ms step_avg:60.31ms
step:817/2285 train_time:49273ms step_avg:60.31ms
step:818/2285 train_time:49334ms step_avg:60.31ms
step:819/2285 train_time:49398ms step_avg:60.31ms
step:820/2285 train_time:49457ms step_avg:60.31ms
step:821/2285 train_time:49519ms step_avg:60.32ms
step:822/2285 train_time:49578ms step_avg:60.31ms
step:823/2285 train_time:49640ms step_avg:60.32ms
step:824/2285 train_time:49699ms step_avg:60.31ms
step:825/2285 train_time:49761ms step_avg:60.32ms
step:826/2285 train_time:49820ms step_avg:60.32ms
step:827/2285 train_time:49883ms step_avg:60.32ms
step:828/2285 train_time:49942ms step_avg:60.32ms
step:829/2285 train_time:50005ms step_avg:60.32ms
step:830/2285 train_time:50065ms step_avg:60.32ms
step:831/2285 train_time:50128ms step_avg:60.32ms
step:832/2285 train_time:50188ms step_avg:60.32ms
step:833/2285 train_time:50251ms step_avg:60.32ms
step:834/2285 train_time:50311ms step_avg:60.33ms
step:835/2285 train_time:50374ms step_avg:60.33ms
step:836/2285 train_time:50434ms step_avg:60.33ms
step:837/2285 train_time:50497ms step_avg:60.33ms
step:838/2285 train_time:50556ms step_avg:60.33ms
step:839/2285 train_time:50618ms step_avg:60.33ms
step:840/2285 train_time:50677ms step_avg:60.33ms
step:841/2285 train_time:50739ms step_avg:60.33ms
step:842/2285 train_time:50799ms step_avg:60.33ms
step:843/2285 train_time:50861ms step_avg:60.33ms
step:844/2285 train_time:50920ms step_avg:60.33ms
step:845/2285 train_time:50982ms step_avg:60.33ms
step:846/2285 train_time:51043ms step_avg:60.33ms
step:847/2285 train_time:51105ms step_avg:60.34ms
step:848/2285 train_time:51165ms step_avg:60.34ms
step:849/2285 train_time:51227ms step_avg:60.34ms
step:850/2285 train_time:51288ms step_avg:60.34ms
step:851/2285 train_time:51351ms step_avg:60.34ms
step:852/2285 train_time:51412ms step_avg:60.34ms
step:853/2285 train_time:51474ms step_avg:60.34ms
step:854/2285 train_time:51534ms step_avg:60.34ms
step:855/2285 train_time:51597ms step_avg:60.35ms
step:856/2285 train_time:51656ms step_avg:60.35ms
step:857/2285 train_time:51718ms step_avg:60.35ms
step:858/2285 train_time:51778ms step_avg:60.35ms
step:859/2285 train_time:51840ms step_avg:60.35ms
step:860/2285 train_time:51899ms step_avg:60.35ms
step:861/2285 train_time:51962ms step_avg:60.35ms
step:862/2285 train_time:52022ms step_avg:60.35ms
step:863/2285 train_time:52084ms step_avg:60.35ms
step:864/2285 train_time:52144ms step_avg:60.35ms
step:865/2285 train_time:52205ms step_avg:60.35ms
step:866/2285 train_time:52266ms step_avg:60.35ms
step:867/2285 train_time:52329ms step_avg:60.36ms
step:868/2285 train_time:52389ms step_avg:60.36ms
step:869/2285 train_time:52452ms step_avg:60.36ms
step:870/2285 train_time:52512ms step_avg:60.36ms
step:871/2285 train_time:52575ms step_avg:60.36ms
step:872/2285 train_time:52635ms step_avg:60.36ms
step:873/2285 train_time:52697ms step_avg:60.36ms
step:874/2285 train_time:52757ms step_avg:60.36ms
step:875/2285 train_time:52819ms step_avg:60.36ms
step:876/2285 train_time:52878ms step_avg:60.36ms
step:877/2285 train_time:52940ms step_avg:60.36ms
step:878/2285 train_time:53000ms step_avg:60.36ms
step:879/2285 train_time:53061ms step_avg:60.37ms
step:880/2285 train_time:53121ms step_avg:60.36ms
step:881/2285 train_time:53184ms step_avg:60.37ms
step:882/2285 train_time:53244ms step_avg:60.37ms
step:883/2285 train_time:53307ms step_avg:60.37ms
step:884/2285 train_time:53368ms step_avg:60.37ms
step:885/2285 train_time:53430ms step_avg:60.37ms
step:886/2285 train_time:53490ms step_avg:60.37ms
step:887/2285 train_time:53553ms step_avg:60.38ms
step:888/2285 train_time:53614ms step_avg:60.38ms
step:889/2285 train_time:53676ms step_avg:60.38ms
step:890/2285 train_time:53735ms step_avg:60.38ms
step:891/2285 train_time:53797ms step_avg:60.38ms
step:892/2285 train_time:53857ms step_avg:60.38ms
step:893/2285 train_time:53919ms step_avg:60.38ms
step:894/2285 train_time:53979ms step_avg:60.38ms
step:895/2285 train_time:54041ms step_avg:60.38ms
step:896/2285 train_time:54101ms step_avg:60.38ms
step:897/2285 train_time:54163ms step_avg:60.38ms
step:898/2285 train_time:54223ms step_avg:60.38ms
step:899/2285 train_time:54285ms step_avg:60.38ms
step:900/2285 train_time:54345ms step_avg:60.38ms
step:901/2285 train_time:54407ms step_avg:60.39ms
step:902/2285 train_time:54468ms step_avg:60.39ms
step:903/2285 train_time:54530ms step_avg:60.39ms
step:904/2285 train_time:54590ms step_avg:60.39ms
step:905/2285 train_time:54653ms step_avg:60.39ms
step:906/2285 train_time:54714ms step_avg:60.39ms
step:907/2285 train_time:54776ms step_avg:60.39ms
step:908/2285 train_time:54836ms step_avg:60.39ms
step:909/2285 train_time:54898ms step_avg:60.39ms
step:910/2285 train_time:54957ms step_avg:60.39ms
step:911/2285 train_time:55019ms step_avg:60.39ms
step:912/2285 train_time:55079ms step_avg:60.39ms
step:913/2285 train_time:55141ms step_avg:60.39ms
step:914/2285 train_time:55200ms step_avg:60.39ms
step:915/2285 train_time:55262ms step_avg:60.40ms
step:916/2285 train_time:55322ms step_avg:60.39ms
step:917/2285 train_time:55385ms step_avg:60.40ms
step:918/2285 train_time:55445ms step_avg:60.40ms
step:919/2285 train_time:55507ms step_avg:60.40ms
step:920/2285 train_time:55567ms step_avg:60.40ms
step:921/2285 train_time:55630ms step_avg:60.40ms
step:922/2285 train_time:55690ms step_avg:60.40ms
step:923/2285 train_time:55752ms step_avg:60.40ms
step:924/2285 train_time:55812ms step_avg:60.40ms
step:925/2285 train_time:55874ms step_avg:60.40ms
step:926/2285 train_time:55935ms step_avg:60.40ms
step:927/2285 train_time:55997ms step_avg:60.41ms
step:928/2285 train_time:56057ms step_avg:60.41ms
step:929/2285 train_time:56119ms step_avg:60.41ms
step:930/2285 train_time:56178ms step_avg:60.41ms
step:931/2285 train_time:56240ms step_avg:60.41ms
step:932/2285 train_time:56299ms step_avg:60.41ms
step:933/2285 train_time:56362ms step_avg:60.41ms
step:934/2285 train_time:56422ms step_avg:60.41ms
step:935/2285 train_time:56484ms step_avg:60.41ms
step:936/2285 train_time:56544ms step_avg:60.41ms
step:937/2285 train_time:56607ms step_avg:60.41ms
step:938/2285 train_time:56667ms step_avg:60.41ms
step:939/2285 train_time:56730ms step_avg:60.42ms
step:940/2285 train_time:56790ms step_avg:60.41ms
step:941/2285 train_time:56852ms step_avg:60.42ms
step:942/2285 train_time:56912ms step_avg:60.42ms
step:943/2285 train_time:56974ms step_avg:60.42ms
step:944/2285 train_time:57035ms step_avg:60.42ms
step:945/2285 train_time:57097ms step_avg:60.42ms
step:946/2285 train_time:57156ms step_avg:60.42ms
step:947/2285 train_time:57218ms step_avg:60.42ms
step:948/2285 train_time:57277ms step_avg:60.42ms
step:949/2285 train_time:57340ms step_avg:60.42ms
step:950/2285 train_time:57399ms step_avg:60.42ms
step:951/2285 train_time:57462ms step_avg:60.42ms
step:952/2285 train_time:57521ms step_avg:60.42ms
step:953/2285 train_time:57584ms step_avg:60.42ms
step:954/2285 train_time:57644ms step_avg:60.42ms
step:955/2285 train_time:57707ms step_avg:60.43ms
step:956/2285 train_time:57768ms step_avg:60.43ms
step:957/2285 train_time:57830ms step_avg:60.43ms
step:958/2285 train_time:57890ms step_avg:60.43ms
step:959/2285 train_time:57953ms step_avg:60.43ms
step:960/2285 train_time:58014ms step_avg:60.43ms
step:961/2285 train_time:58075ms step_avg:60.43ms
step:962/2285 train_time:58135ms step_avg:60.43ms
step:963/2285 train_time:58197ms step_avg:60.43ms
step:964/2285 train_time:58257ms step_avg:60.43ms
step:965/2285 train_time:58319ms step_avg:60.43ms
step:966/2285 train_time:58379ms step_avg:60.43ms
step:967/2285 train_time:58441ms step_avg:60.44ms
step:968/2285 train_time:58501ms step_avg:60.43ms
step:969/2285 train_time:58562ms step_avg:60.44ms
step:970/2285 train_time:58622ms step_avg:60.43ms
step:971/2285 train_time:58684ms step_avg:60.44ms
step:972/2285 train_time:58745ms step_avg:60.44ms
step:973/2285 train_time:58807ms step_avg:60.44ms
step:974/2285 train_time:58868ms step_avg:60.44ms
step:975/2285 train_time:58931ms step_avg:60.44ms
step:976/2285 train_time:58991ms step_avg:60.44ms
step:977/2285 train_time:59053ms step_avg:60.44ms
step:978/2285 train_time:59114ms step_avg:60.44ms
step:979/2285 train_time:59176ms step_avg:60.45ms
step:980/2285 train_time:59236ms step_avg:60.45ms
step:981/2285 train_time:59298ms step_avg:60.45ms
step:982/2285 train_time:59358ms step_avg:60.45ms
step:983/2285 train_time:59421ms step_avg:60.45ms
step:984/2285 train_time:59480ms step_avg:60.45ms
step:985/2285 train_time:59541ms step_avg:60.45ms
step:986/2285 train_time:59601ms step_avg:60.45ms
step:987/2285 train_time:59663ms step_avg:60.45ms
step:988/2285 train_time:59723ms step_avg:60.45ms
step:989/2285 train_time:59786ms step_avg:60.45ms
step:990/2285 train_time:59846ms step_avg:60.45ms
step:991/2285 train_time:59909ms step_avg:60.45ms
step:992/2285 train_time:59969ms step_avg:60.45ms
step:993/2285 train_time:60031ms step_avg:60.45ms
step:994/2285 train_time:60092ms step_avg:60.45ms
step:995/2285 train_time:60155ms step_avg:60.46ms
step:996/2285 train_time:60215ms step_avg:60.46ms
step:997/2285 train_time:60277ms step_avg:60.46ms
step:998/2285 train_time:60337ms step_avg:60.46ms
step:999/2285 train_time:60399ms step_avg:60.46ms
step:1000/2285 train_time:60459ms step_avg:60.46ms
step:1000/2285 val_loss:3.5669 train_time:60522ms step_avg:60.52ms
step:1001/2285 train_time:60541ms step_avg:60.48ms
step:1002/2285 train_time:60582ms step_avg:60.46ms
step:1003/2285 train_time:60648ms step_avg:60.47ms
step:1004/2285 train_time:60710ms step_avg:60.47ms
step:1005/2285 train_time:60773ms step_avg:60.47ms
step:1006/2285 train_time:60833ms step_avg:60.47ms
step:1007/2285 train_time:60895ms step_avg:60.47ms
step:1008/2285 train_time:60954ms step_avg:60.47ms
step:1009/2285 train_time:61016ms step_avg:60.47ms
step:1010/2285 train_time:61075ms step_avg:60.47ms
step:1011/2285 train_time:61137ms step_avg:60.47ms
step:1012/2285 train_time:61196ms step_avg:60.47ms
step:1013/2285 train_time:61258ms step_avg:60.47ms
step:1014/2285 train_time:61318ms step_avg:60.47ms
step:1015/2285 train_time:61379ms step_avg:60.47ms
step:1016/2285 train_time:61440ms step_avg:60.47ms
step:1017/2285 train_time:61504ms step_avg:60.48ms
step:1018/2285 train_time:61565ms step_avg:60.48ms
step:1019/2285 train_time:61629ms step_avg:60.48ms
step:1020/2285 train_time:61689ms step_avg:60.48ms
step:1021/2285 train_time:61752ms step_avg:60.48ms
step:1022/2285 train_time:61811ms step_avg:60.48ms
step:1023/2285 train_time:61874ms step_avg:60.48ms
step:1024/2285 train_time:61934ms step_avg:60.48ms
step:1025/2285 train_time:61997ms step_avg:60.48ms
step:1026/2285 train_time:62056ms step_avg:60.48ms
step:1027/2285 train_time:62118ms step_avg:60.48ms
step:1028/2285 train_time:62177ms step_avg:60.48ms
step:1029/2285 train_time:62239ms step_avg:60.49ms
step:1030/2285 train_time:62299ms step_avg:60.48ms
step:1031/2285 train_time:62361ms step_avg:60.49ms
step:1032/2285 train_time:62421ms step_avg:60.49ms
step:1033/2285 train_time:62485ms step_avg:60.49ms
step:1034/2285 train_time:62545ms step_avg:60.49ms
step:1035/2285 train_time:62607ms step_avg:60.49ms
step:1036/2285 train_time:62668ms step_avg:60.49ms
step:1037/2285 train_time:62731ms step_avg:60.49ms
step:1038/2285 train_time:62791ms step_avg:60.49ms
step:1039/2285 train_time:62853ms step_avg:60.49ms
step:1040/2285 train_time:62913ms step_avg:60.49ms
step:1041/2285 train_time:62975ms step_avg:60.50ms
step:1042/2285 train_time:63035ms step_avg:60.49ms
step:1043/2285 train_time:63097ms step_avg:60.50ms
step:1044/2285 train_time:63156ms step_avg:60.49ms
step:1045/2285 train_time:63217ms step_avg:60.50ms
step:1046/2285 train_time:63277ms step_avg:60.49ms
step:1047/2285 train_time:63340ms step_avg:60.50ms
step:1048/2285 train_time:63400ms step_avg:60.50ms
step:1049/2285 train_time:63464ms step_avg:60.50ms
step:1050/2285 train_time:63524ms step_avg:60.50ms
step:1051/2285 train_time:63587ms step_avg:60.50ms
step:1052/2285 train_time:63648ms step_avg:60.50ms
step:1053/2285 train_time:63711ms step_avg:60.50ms
step:1054/2285 train_time:63771ms step_avg:60.50ms
step:1055/2285 train_time:63833ms step_avg:60.51ms
step:1056/2285 train_time:63893ms step_avg:60.50ms
step:1057/2285 train_time:63955ms step_avg:60.51ms
step:1058/2285 train_time:64015ms step_avg:60.51ms
step:1059/2285 train_time:64077ms step_avg:60.51ms
step:1060/2285 train_time:64137ms step_avg:60.51ms
step:1061/2285 train_time:64199ms step_avg:60.51ms
step:1062/2285 train_time:64258ms step_avg:60.51ms
step:1063/2285 train_time:64320ms step_avg:60.51ms
step:1064/2285 train_time:64381ms step_avg:60.51ms
step:1065/2285 train_time:64443ms step_avg:60.51ms
step:1066/2285 train_time:64503ms step_avg:60.51ms
step:1067/2285 train_time:64566ms step_avg:60.51ms
step:1068/2285 train_time:64627ms step_avg:60.51ms
step:1069/2285 train_time:64689ms step_avg:60.51ms
step:1070/2285 train_time:64749ms step_avg:60.51ms
step:1071/2285 train_time:64811ms step_avg:60.51ms
step:1072/2285 train_time:64870ms step_avg:60.51ms
step:1073/2285 train_time:64933ms step_avg:60.52ms
step:1074/2285 train_time:64992ms step_avg:60.51ms
step:1075/2285 train_time:65055ms step_avg:60.52ms
step:1076/2285 train_time:65114ms step_avg:60.52ms
step:1077/2285 train_time:65177ms step_avg:60.52ms
step:1078/2285 train_time:65236ms step_avg:60.52ms
step:1079/2285 train_time:65299ms step_avg:60.52ms
step:1080/2285 train_time:65359ms step_avg:60.52ms
step:1081/2285 train_time:65422ms step_avg:60.52ms
step:1082/2285 train_time:65482ms step_avg:60.52ms
step:1083/2285 train_time:65545ms step_avg:60.52ms
step:1084/2285 train_time:65605ms step_avg:60.52ms
step:1085/2285 train_time:65667ms step_avg:60.52ms
step:1086/2285 train_time:65728ms step_avg:60.52ms
step:1087/2285 train_time:65790ms step_avg:60.52ms
step:1088/2285 train_time:65850ms step_avg:60.52ms
step:1089/2285 train_time:65911ms step_avg:60.52ms
step:1090/2285 train_time:65971ms step_avg:60.52ms
step:1091/2285 train_time:66034ms step_avg:60.53ms
step:1092/2285 train_time:66094ms step_avg:60.53ms
step:1093/2285 train_time:66157ms step_avg:60.53ms
step:1094/2285 train_time:66217ms step_avg:60.53ms
step:1095/2285 train_time:66279ms step_avg:60.53ms
step:1096/2285 train_time:66339ms step_avg:60.53ms
step:1097/2285 train_time:66401ms step_avg:60.53ms
step:1098/2285 train_time:66462ms step_avg:60.53ms
step:1099/2285 train_time:66525ms step_avg:60.53ms
step:1100/2285 train_time:66585ms step_avg:60.53ms
step:1101/2285 train_time:66648ms step_avg:60.53ms
step:1102/2285 train_time:66708ms step_avg:60.53ms
step:1103/2285 train_time:66771ms step_avg:60.54ms
step:1104/2285 train_time:66831ms step_avg:60.54ms
step:1105/2285 train_time:66893ms step_avg:60.54ms
step:1106/2285 train_time:66953ms step_avg:60.54ms
step:1107/2285 train_time:67015ms step_avg:60.54ms
step:1108/2285 train_time:67075ms step_avg:60.54ms
step:1109/2285 train_time:67137ms step_avg:60.54ms
step:1110/2285 train_time:67197ms step_avg:60.54ms
step:1111/2285 train_time:67259ms step_avg:60.54ms
step:1112/2285 train_time:67319ms step_avg:60.54ms
step:1113/2285 train_time:67381ms step_avg:60.54ms
step:1114/2285 train_time:67441ms step_avg:60.54ms
step:1115/2285 train_time:67504ms step_avg:60.54ms
step:1116/2285 train_time:67564ms step_avg:60.54ms
step:1117/2285 train_time:67627ms step_avg:60.54ms
step:1118/2285 train_time:67687ms step_avg:60.54ms
step:1119/2285 train_time:67750ms step_avg:60.54ms
step:1120/2285 train_time:67809ms step_avg:60.54ms
step:1121/2285 train_time:67871ms step_avg:60.55ms
step:1122/2285 train_time:67931ms step_avg:60.54ms
step:1123/2285 train_time:67992ms step_avg:60.55ms
step:1124/2285 train_time:68052ms step_avg:60.54ms
step:1125/2285 train_time:68114ms step_avg:60.55ms
step:1126/2285 train_time:68174ms step_avg:60.55ms
step:1127/2285 train_time:68237ms step_avg:60.55ms
step:1128/2285 train_time:68297ms step_avg:60.55ms
step:1129/2285 train_time:68359ms step_avg:60.55ms
step:1130/2285 train_time:68419ms step_avg:60.55ms
step:1131/2285 train_time:68482ms step_avg:60.55ms
step:1132/2285 train_time:68543ms step_avg:60.55ms
step:1133/2285 train_time:68606ms step_avg:60.55ms
step:1134/2285 train_time:68666ms step_avg:60.55ms
step:1135/2285 train_time:68728ms step_avg:60.55ms
step:1136/2285 train_time:68788ms step_avg:60.55ms
step:1137/2285 train_time:68850ms step_avg:60.55ms
step:1138/2285 train_time:68909ms step_avg:60.55ms
step:1139/2285 train_time:68971ms step_avg:60.55ms
step:1140/2285 train_time:69031ms step_avg:60.55ms
step:1141/2285 train_time:69093ms step_avg:60.55ms
step:1142/2285 train_time:69152ms step_avg:60.55ms
step:1143/2285 train_time:69215ms step_avg:60.56ms
step:1144/2285 train_time:69275ms step_avg:60.56ms
step:1145/2285 train_time:69338ms step_avg:60.56ms
step:1146/2285 train_time:69398ms step_avg:60.56ms
step:1147/2285 train_time:69461ms step_avg:60.56ms
step:1148/2285 train_time:69522ms step_avg:60.56ms
step:1149/2285 train_time:69585ms step_avg:60.56ms
step:1150/2285 train_time:69645ms step_avg:60.56ms
step:1151/2285 train_time:69707ms step_avg:60.56ms
step:1152/2285 train_time:69768ms step_avg:60.56ms
step:1153/2285 train_time:69830ms step_avg:60.56ms
step:1154/2285 train_time:69889ms step_avg:60.56ms
step:1155/2285 train_time:69952ms step_avg:60.56ms
step:1156/2285 train_time:70011ms step_avg:60.56ms
step:1157/2285 train_time:70073ms step_avg:60.56ms
step:1158/2285 train_time:70134ms step_avg:60.56ms
step:1159/2285 train_time:70197ms step_avg:60.57ms
step:1160/2285 train_time:70256ms step_avg:60.57ms
step:1161/2285 train_time:70318ms step_avg:60.57ms
step:1162/2285 train_time:70378ms step_avg:60.57ms
step:1163/2285 train_time:70441ms step_avg:60.57ms
step:1164/2285 train_time:70501ms step_avg:60.57ms
step:1165/2285 train_time:70564ms step_avg:60.57ms
step:1166/2285 train_time:70624ms step_avg:60.57ms
step:1167/2285 train_time:70688ms step_avg:60.57ms
step:1168/2285 train_time:70749ms step_avg:60.57ms
step:1169/2285 train_time:70811ms step_avg:60.57ms
step:1170/2285 train_time:70870ms step_avg:60.57ms
step:1171/2285 train_time:70933ms step_avg:60.57ms
step:1172/2285 train_time:70992ms step_avg:60.57ms
step:1173/2285 train_time:71054ms step_avg:60.58ms
step:1174/2285 train_time:71114ms step_avg:60.57ms
step:1175/2285 train_time:71176ms step_avg:60.58ms
step:1176/2285 train_time:71236ms step_avg:60.57ms
step:1177/2285 train_time:71298ms step_avg:60.58ms
step:1178/2285 train_time:71358ms step_avg:60.58ms
step:1179/2285 train_time:71419ms step_avg:60.58ms
step:1180/2285 train_time:71479ms step_avg:60.58ms
step:1181/2285 train_time:71542ms step_avg:60.58ms
step:1182/2285 train_time:71602ms step_avg:60.58ms
step:1183/2285 train_time:71665ms step_avg:60.58ms
step:1184/2285 train_time:71725ms step_avg:60.58ms
step:1185/2285 train_time:71788ms step_avg:60.58ms
step:1186/2285 train_time:71848ms step_avg:60.58ms
step:1187/2285 train_time:71909ms step_avg:60.58ms
step:1188/2285 train_time:71968ms step_avg:60.58ms
step:1189/2285 train_time:72031ms step_avg:60.58ms
step:1190/2285 train_time:72091ms step_avg:60.58ms
step:1191/2285 train_time:72153ms step_avg:60.58ms
step:1192/2285 train_time:72213ms step_avg:60.58ms
step:1193/2285 train_time:72276ms step_avg:60.58ms
step:1194/2285 train_time:72335ms step_avg:60.58ms
step:1195/2285 train_time:72398ms step_avg:60.58ms
step:1196/2285 train_time:72458ms step_avg:60.58ms
step:1197/2285 train_time:72520ms step_avg:60.58ms
step:1198/2285 train_time:72581ms step_avg:60.59ms
step:1199/2285 train_time:72644ms step_avg:60.59ms
step:1200/2285 train_time:72704ms step_avg:60.59ms
step:1201/2285 train_time:72766ms step_avg:60.59ms
step:1202/2285 train_time:72827ms step_avg:60.59ms
step:1203/2285 train_time:72889ms step_avg:60.59ms
step:1204/2285 train_time:72949ms step_avg:60.59ms
step:1205/2285 train_time:73011ms step_avg:60.59ms
step:1206/2285 train_time:73070ms step_avg:60.59ms
step:1207/2285 train_time:73132ms step_avg:60.59ms
step:1208/2285 train_time:73192ms step_avg:60.59ms
step:1209/2285 train_time:73254ms step_avg:60.59ms
step:1210/2285 train_time:73314ms step_avg:60.59ms
step:1211/2285 train_time:73377ms step_avg:60.59ms
step:1212/2285 train_time:73436ms step_avg:60.59ms
step:1213/2285 train_time:73498ms step_avg:60.59ms
step:1214/2285 train_time:73558ms step_avg:60.59ms
step:1215/2285 train_time:73621ms step_avg:60.59ms
step:1216/2285 train_time:73681ms step_avg:60.59ms
step:1217/2285 train_time:73745ms step_avg:60.60ms
step:1218/2285 train_time:73804ms step_avg:60.59ms
step:1219/2285 train_time:73867ms step_avg:60.60ms
step:1220/2285 train_time:73927ms step_avg:60.60ms
step:1221/2285 train_time:73989ms step_avg:60.60ms
step:1222/2285 train_time:74048ms step_avg:60.60ms
step:1223/2285 train_time:74110ms step_avg:60.60ms
step:1224/2285 train_time:74170ms step_avg:60.60ms
step:1225/2285 train_time:74232ms step_avg:60.60ms
step:1226/2285 train_time:74292ms step_avg:60.60ms
step:1227/2285 train_time:74355ms step_avg:60.60ms
step:1228/2285 train_time:74415ms step_avg:60.60ms
step:1229/2285 train_time:74478ms step_avg:60.60ms
step:1230/2285 train_time:74538ms step_avg:60.60ms
step:1231/2285 train_time:74600ms step_avg:60.60ms
step:1232/2285 train_time:74661ms step_avg:60.60ms
step:1233/2285 train_time:74724ms step_avg:60.60ms
step:1234/2285 train_time:74783ms step_avg:60.60ms
step:1235/2285 train_time:74846ms step_avg:60.60ms
step:1236/2285 train_time:74906ms step_avg:60.60ms
step:1237/2285 train_time:74968ms step_avg:60.60ms
step:1238/2285 train_time:75028ms step_avg:60.60ms
step:1239/2285 train_time:75090ms step_avg:60.61ms
step:1240/2285 train_time:75150ms step_avg:60.60ms
step:1241/2285 train_time:75212ms step_avg:60.61ms
step:1242/2285 train_time:75271ms step_avg:60.60ms
step:1243/2285 train_time:75334ms step_avg:60.61ms
step:1244/2285 train_time:75394ms step_avg:60.61ms
step:1245/2285 train_time:75457ms step_avg:60.61ms
step:1246/2285 train_time:75516ms step_avg:60.61ms
step:1247/2285 train_time:75579ms step_avg:60.61ms
step:1248/2285 train_time:75640ms step_avg:60.61ms
step:1249/2285 train_time:75703ms step_avg:60.61ms
step:1250/2285 train_time:75763ms step_avg:60.61ms
step:1250/2285 val_loss:3.4995 train_time:75826ms step_avg:60.66ms
step:1251/2285 train_time:75847ms step_avg:60.63ms
step:1252/2285 train_time:75888ms step_avg:60.61ms
step:1253/2285 train_time:75954ms step_avg:60.62ms
step:1254/2285 train_time:76018ms step_avg:60.62ms
step:1255/2285 train_time:76080ms step_avg:60.62ms
step:1256/2285 train_time:76140ms step_avg:60.62ms
step:1257/2285 train_time:76203ms step_avg:60.62ms
step:1258/2285 train_time:76262ms step_avg:60.62ms
step:1259/2285 train_time:76323ms step_avg:60.62ms
step:1260/2285 train_time:76383ms step_avg:60.62ms
step:1261/2285 train_time:76444ms step_avg:60.62ms
step:1262/2285 train_time:76503ms step_avg:60.62ms
step:1263/2285 train_time:76565ms step_avg:60.62ms
step:1264/2285 train_time:76626ms step_avg:60.62ms
step:1265/2285 train_time:76686ms step_avg:60.62ms
step:1266/2285 train_time:76748ms step_avg:60.62ms
step:1267/2285 train_time:76811ms step_avg:60.62ms
step:1268/2285 train_time:76873ms step_avg:60.63ms
step:1269/2285 train_time:76936ms step_avg:60.63ms
step:1270/2285 train_time:76997ms step_avg:60.63ms
step:1271/2285 train_time:77060ms step_avg:60.63ms
step:1272/2285 train_time:77120ms step_avg:60.63ms
step:1273/2285 train_time:77183ms step_avg:60.63ms
step:1274/2285 train_time:77242ms step_avg:60.63ms
step:1275/2285 train_time:77304ms step_avg:60.63ms
step:1276/2285 train_time:77363ms step_avg:60.63ms
step:1277/2285 train_time:77425ms step_avg:60.63ms
step:1278/2285 train_time:77485ms step_avg:60.63ms
step:1279/2285 train_time:77546ms step_avg:60.63ms
step:1280/2285 train_time:77606ms step_avg:60.63ms
step:1281/2285 train_time:77668ms step_avg:60.63ms
step:1282/2285 train_time:77729ms step_avg:60.63ms
step:1283/2285 train_time:77792ms step_avg:60.63ms
step:1284/2285 train_time:77853ms step_avg:60.63ms
step:1285/2285 train_time:77916ms step_avg:60.63ms
step:1286/2285 train_time:77977ms step_avg:60.64ms
step:1287/2285 train_time:78039ms step_avg:60.64ms
step:1288/2285 train_time:78099ms step_avg:60.64ms
step:1289/2285 train_time:78162ms step_avg:60.64ms
step:1290/2285 train_time:78222ms step_avg:60.64ms
step:1291/2285 train_time:78284ms step_avg:60.64ms
step:1292/2285 train_time:78343ms step_avg:60.64ms
step:1293/2285 train_time:78405ms step_avg:60.64ms
step:1294/2285 train_time:78464ms step_avg:60.64ms
step:1295/2285 train_time:78526ms step_avg:60.64ms
step:1296/2285 train_time:78585ms step_avg:60.64ms
step:1297/2285 train_time:78648ms step_avg:60.64ms
step:1298/2285 train_time:78708ms step_avg:60.64ms
step:1299/2285 train_time:78771ms step_avg:60.64ms
step:1300/2285 train_time:78831ms step_avg:60.64ms
step:1301/2285 train_time:78894ms step_avg:60.64ms
step:1302/2285 train_time:78954ms step_avg:60.64ms
step:1303/2285 train_time:79017ms step_avg:60.64ms
step:1304/2285 train_time:79076ms step_avg:60.64ms
step:1305/2285 train_time:79139ms step_avg:60.64ms
step:1306/2285 train_time:79198ms step_avg:60.64ms
step:1307/2285 train_time:79260ms step_avg:60.64ms
step:1308/2285 train_time:79321ms step_avg:60.64ms
step:1309/2285 train_time:79382ms step_avg:60.64ms
step:1310/2285 train_time:79442ms step_avg:60.64ms
step:1311/2285 train_time:79504ms step_avg:60.64ms
step:1312/2285 train_time:79564ms step_avg:60.64ms
step:1313/2285 train_time:79626ms step_avg:60.64ms
step:1314/2285 train_time:79686ms step_avg:60.64ms
step:1315/2285 train_time:79749ms step_avg:60.65ms
step:1316/2285 train_time:79810ms step_avg:60.65ms
step:1317/2285 train_time:79873ms step_avg:60.65ms
step:1318/2285 train_time:79933ms step_avg:60.65ms
step:1319/2285 train_time:79995ms step_avg:60.65ms
step:1320/2285 train_time:80055ms step_avg:60.65ms
step:1321/2285 train_time:80118ms step_avg:60.65ms
step:1322/2285 train_time:80177ms step_avg:60.65ms
step:1323/2285 train_time:80239ms step_avg:60.65ms
step:1324/2285 train_time:80299ms step_avg:60.65ms
step:1325/2285 train_time:80361ms step_avg:60.65ms
step:1326/2285 train_time:80420ms step_avg:60.65ms
step:1327/2285 train_time:80483ms step_avg:60.65ms
step:1328/2285 train_time:80543ms step_avg:60.65ms
step:1329/2285 train_time:80605ms step_avg:60.65ms
step:1330/2285 train_time:80665ms step_avg:60.65ms
step:1331/2285 train_time:80728ms step_avg:60.65ms
step:1332/2285 train_time:80789ms step_avg:60.65ms
step:1333/2285 train_time:80852ms step_avg:60.65ms
step:1334/2285 train_time:80912ms step_avg:60.65ms
step:1335/2285 train_time:80974ms step_avg:60.66ms
step:1336/2285 train_time:81034ms step_avg:60.65ms
step:1337/2285 train_time:81097ms step_avg:60.66ms
step:1338/2285 train_time:81156ms step_avg:60.65ms
step:1339/2285 train_time:81218ms step_avg:60.66ms
step:1340/2285 train_time:81277ms step_avg:60.65ms
step:1341/2285 train_time:81339ms step_avg:60.66ms
step:1342/2285 train_time:81399ms step_avg:60.65ms
step:1343/2285 train_time:81462ms step_avg:60.66ms
step:1344/2285 train_time:81521ms step_avg:60.66ms
step:1345/2285 train_time:81584ms step_avg:60.66ms
step:1346/2285 train_time:81643ms step_avg:60.66ms
step:1347/2285 train_time:81706ms step_avg:60.66ms
step:1348/2285 train_time:81767ms step_avg:60.66ms
step:1349/2285 train_time:81829ms step_avg:60.66ms
step:1350/2285 train_time:81889ms step_avg:60.66ms
step:1351/2285 train_time:81952ms step_avg:60.66ms
step:1352/2285 train_time:82012ms step_avg:60.66ms
step:1353/2285 train_time:82074ms step_avg:60.66ms
step:1354/2285 train_time:82134ms step_avg:60.66ms
step:1355/2285 train_time:82196ms step_avg:60.66ms
step:1356/2285 train_time:82255ms step_avg:60.66ms
step:1357/2285 train_time:82318ms step_avg:60.66ms
step:1358/2285 train_time:82377ms step_avg:60.66ms
step:1359/2285 train_time:82440ms step_avg:60.66ms
step:1360/2285 train_time:82500ms step_avg:60.66ms
step:1361/2285 train_time:82563ms step_avg:60.66ms
step:1362/2285 train_time:82623ms step_avg:60.66ms
step:1363/2285 train_time:82685ms step_avg:60.66ms
step:1364/2285 train_time:82745ms step_avg:60.66ms
step:1365/2285 train_time:82808ms step_avg:60.67ms
step:1366/2285 train_time:82869ms step_avg:60.67ms
step:1367/2285 train_time:82931ms step_avg:60.67ms
step:1368/2285 train_time:82991ms step_avg:60.67ms
step:1369/2285 train_time:83054ms step_avg:60.67ms
step:1370/2285 train_time:83114ms step_avg:60.67ms
step:1371/2285 train_time:83176ms step_avg:60.67ms
step:1372/2285 train_time:83235ms step_avg:60.67ms
step:1373/2285 train_time:83297ms step_avg:60.67ms
step:1374/2285 train_time:83358ms step_avg:60.67ms
step:1375/2285 train_time:83419ms step_avg:60.67ms
step:1376/2285 train_time:83479ms step_avg:60.67ms
step:1377/2285 train_time:83542ms step_avg:60.67ms
step:1378/2285 train_time:83602ms step_avg:60.67ms
step:1379/2285 train_time:83664ms step_avg:60.67ms
step:1380/2285 train_time:83724ms step_avg:60.67ms
step:1381/2285 train_time:83787ms step_avg:60.67ms
step:1382/2285 train_time:83848ms step_avg:60.67ms
step:1383/2285 train_time:83911ms step_avg:60.67ms
step:1384/2285 train_time:83971ms step_avg:60.67ms
step:1385/2285 train_time:84033ms step_avg:60.67ms
step:1386/2285 train_time:84093ms step_avg:60.67ms
step:1387/2285 train_time:84155ms step_avg:60.67ms
step:1388/2285 train_time:84214ms step_avg:60.67ms
step:1389/2285 train_time:84277ms step_avg:60.67ms
step:1390/2285 train_time:84337ms step_avg:60.67ms
step:1391/2285 train_time:84398ms step_avg:60.67ms
step:1392/2285 train_time:84458ms step_avg:60.67ms
step:1393/2285 train_time:84521ms step_avg:60.68ms
step:1394/2285 train_time:84580ms step_avg:60.67ms
step:1395/2285 train_time:84643ms step_avg:60.68ms
step:1396/2285 train_time:84703ms step_avg:60.68ms
step:1397/2285 train_time:84766ms step_avg:60.68ms
step:1398/2285 train_time:84826ms step_avg:60.68ms
step:1399/2285 train_time:84889ms step_avg:60.68ms
step:1400/2285 train_time:84949ms step_avg:60.68ms
step:1401/2285 train_time:85011ms step_avg:60.68ms
step:1402/2285 train_time:85071ms step_avg:60.68ms
step:1403/2285 train_time:85133ms step_avg:60.68ms
step:1404/2285 train_time:85193ms step_avg:60.68ms
step:1405/2285 train_time:85256ms step_avg:60.68ms
step:1406/2285 train_time:85315ms step_avg:60.68ms
step:1407/2285 train_time:85377ms step_avg:60.68ms
step:1408/2285 train_time:85436ms step_avg:60.68ms
step:1409/2285 train_time:85500ms step_avg:60.68ms
step:1410/2285 train_time:85560ms step_avg:60.68ms
step:1411/2285 train_time:85624ms step_avg:60.68ms
step:1412/2285 train_time:85683ms step_avg:60.68ms
step:1413/2285 train_time:85745ms step_avg:60.68ms
step:1414/2285 train_time:85805ms step_avg:60.68ms
step:1415/2285 train_time:85869ms step_avg:60.68ms
step:1416/2285 train_time:85928ms step_avg:60.68ms
step:1417/2285 train_time:85990ms step_avg:60.68ms
step:1418/2285 train_time:86050ms step_avg:60.68ms
step:1419/2285 train_time:86113ms step_avg:60.69ms
step:1420/2285 train_time:86173ms step_avg:60.68ms
step:1421/2285 train_time:86234ms step_avg:60.69ms
step:1422/2285 train_time:86294ms step_avg:60.69ms
step:1423/2285 train_time:86356ms step_avg:60.69ms
step:1424/2285 train_time:86416ms step_avg:60.69ms
step:1425/2285 train_time:86478ms step_avg:60.69ms
step:1426/2285 train_time:86538ms step_avg:60.69ms
step:1427/2285 train_time:86601ms step_avg:60.69ms
step:1428/2285 train_time:86661ms step_avg:60.69ms
step:1429/2285 train_time:86723ms step_avg:60.69ms
step:1430/2285 train_time:86784ms step_avg:60.69ms
step:1431/2285 train_time:86847ms step_avg:60.69ms
step:1432/2285 train_time:86907ms step_avg:60.69ms
step:1433/2285 train_time:86969ms step_avg:60.69ms
step:1434/2285 train_time:87029ms step_avg:60.69ms
step:1435/2285 train_time:87092ms step_avg:60.69ms
step:1436/2285 train_time:87152ms step_avg:60.69ms
step:1437/2285 train_time:87214ms step_avg:60.69ms
step:1438/2285 train_time:87274ms step_avg:60.69ms
step:1439/2285 train_time:87336ms step_avg:60.69ms
step:1440/2285 train_time:87396ms step_avg:60.69ms
step:1441/2285 train_time:87458ms step_avg:60.69ms
step:1442/2285 train_time:87518ms step_avg:60.69ms
step:1443/2285 train_time:87581ms step_avg:60.69ms
step:1444/2285 train_time:87641ms step_avg:60.69ms
step:1445/2285 train_time:87704ms step_avg:60.69ms
step:1446/2285 train_time:87764ms step_avg:60.69ms
step:1447/2285 train_time:87826ms step_avg:60.70ms
step:1448/2285 train_time:87886ms step_avg:60.69ms
step:1449/2285 train_time:87948ms step_avg:60.70ms
step:1450/2285 train_time:88008ms step_avg:60.70ms
step:1451/2285 train_time:88071ms step_avg:60.70ms
step:1452/2285 train_time:88130ms step_avg:60.70ms
step:1453/2285 train_time:88193ms step_avg:60.70ms
step:1454/2285 train_time:88253ms step_avg:60.70ms
step:1455/2285 train_time:88315ms step_avg:60.70ms
step:1456/2285 train_time:88375ms step_avg:60.70ms
step:1457/2285 train_time:88437ms step_avg:60.70ms
step:1458/2285 train_time:88497ms step_avg:60.70ms
step:1459/2285 train_time:88558ms step_avg:60.70ms
step:1460/2285 train_time:88619ms step_avg:60.70ms
step:1461/2285 train_time:88683ms step_avg:60.70ms
step:1462/2285 train_time:88742ms step_avg:60.70ms
step:1463/2285 train_time:88806ms step_avg:60.70ms
step:1464/2285 train_time:88866ms step_avg:60.70ms
step:1465/2285 train_time:88928ms step_avg:60.70ms
step:1466/2285 train_time:88988ms step_avg:60.70ms
step:1467/2285 train_time:89051ms step_avg:60.70ms
step:1468/2285 train_time:89110ms step_avg:60.70ms
step:1469/2285 train_time:89173ms step_avg:60.70ms
step:1470/2285 train_time:89232ms step_avg:60.70ms
step:1471/2285 train_time:89294ms step_avg:60.70ms
step:1472/2285 train_time:89354ms step_avg:60.70ms
step:1473/2285 train_time:89417ms step_avg:60.70ms
step:1474/2285 train_time:89477ms step_avg:60.70ms
step:1475/2285 train_time:89539ms step_avg:60.70ms
step:1476/2285 train_time:89599ms step_avg:60.70ms
step:1477/2285 train_time:89662ms step_avg:60.71ms
step:1478/2285 train_time:89723ms step_avg:60.71ms
step:1479/2285 train_time:89785ms step_avg:60.71ms
step:1480/2285 train_time:89845ms step_avg:60.71ms
step:1481/2285 train_time:89907ms step_avg:60.71ms
step:1482/2285 train_time:89968ms step_avg:60.71ms
step:1483/2285 train_time:90030ms step_avg:60.71ms
step:1484/2285 train_time:90090ms step_avg:60.71ms
step:1485/2285 train_time:90152ms step_avg:60.71ms
step:1486/2285 train_time:90212ms step_avg:60.71ms
step:1487/2285 train_time:90275ms step_avg:60.71ms
step:1488/2285 train_time:90334ms step_avg:60.71ms
step:1489/2285 train_time:90396ms step_avg:60.71ms
step:1490/2285 train_time:90455ms step_avg:60.71ms
step:1491/2285 train_time:90518ms step_avg:60.71ms
step:1492/2285 train_time:90577ms step_avg:60.71ms
step:1493/2285 train_time:90640ms step_avg:60.71ms
step:1494/2285 train_time:90700ms step_avg:60.71ms
step:1495/2285 train_time:90763ms step_avg:60.71ms
step:1496/2285 train_time:90823ms step_avg:60.71ms
step:1497/2285 train_time:90886ms step_avg:60.71ms
step:1498/2285 train_time:90946ms step_avg:60.71ms
step:1499/2285 train_time:91009ms step_avg:60.71ms
step:1500/2285 train_time:91069ms step_avg:60.71ms
step:1500/2285 val_loss:3.4260 train_time:91133ms step_avg:60.76ms
step:1501/2285 train_time:91154ms step_avg:60.73ms
step:1502/2285 train_time:91195ms step_avg:60.72ms
step:1503/2285 train_time:91258ms step_avg:60.72ms
step:1504/2285 train_time:91319ms step_avg:60.72ms
step:1505/2285 train_time:91384ms step_avg:60.72ms
step:1506/2285 train_time:91444ms step_avg:60.72ms
step:1507/2285 train_time:91505ms step_avg:60.72ms
step:1508/2285 train_time:91566ms step_avg:60.72ms
step:1509/2285 train_time:91627ms step_avg:60.72ms
step:1510/2285 train_time:91687ms step_avg:60.72ms
step:1511/2285 train_time:91748ms step_avg:60.72ms
step:1512/2285 train_time:91808ms step_avg:60.72ms
step:1513/2285 train_time:91870ms step_avg:60.72ms
step:1514/2285 train_time:91930ms step_avg:60.72ms
step:1515/2285 train_time:91992ms step_avg:60.72ms
step:1516/2285 train_time:92054ms step_avg:60.72ms
step:1517/2285 train_time:92119ms step_avg:60.72ms
step:1518/2285 train_time:92180ms step_avg:60.72ms
step:1519/2285 train_time:92243ms step_avg:60.73ms
step:1520/2285 train_time:92304ms step_avg:60.73ms
step:1521/2285 train_time:92368ms step_avg:60.73ms
step:1522/2285 train_time:92428ms step_avg:60.73ms
step:1523/2285 train_time:92490ms step_avg:60.73ms
step:1524/2285 train_time:92551ms step_avg:60.73ms
step:1525/2285 train_time:92613ms step_avg:60.73ms
step:1526/2285 train_time:92673ms step_avg:60.73ms
step:1527/2285 train_time:92736ms step_avg:60.73ms
step:1528/2285 train_time:92796ms step_avg:60.73ms
step:1529/2285 train_time:92858ms step_avg:60.73ms
step:1530/2285 train_time:92918ms step_avg:60.73ms
step:1531/2285 train_time:92982ms step_avg:60.73ms
step:1532/2285 train_time:93042ms step_avg:60.73ms
step:1533/2285 train_time:93106ms step_avg:60.73ms
step:1534/2285 train_time:93166ms step_avg:60.73ms
step:1535/2285 train_time:93229ms step_avg:60.74ms
step:1536/2285 train_time:93290ms step_avg:60.74ms
step:1537/2285 train_time:93353ms step_avg:60.74ms
step:1538/2285 train_time:93413ms step_avg:60.74ms
step:1539/2285 train_time:93476ms step_avg:60.74ms
step:1540/2285 train_time:93537ms step_avg:60.74ms
step:1541/2285 train_time:93599ms step_avg:60.74ms
step:1542/2285 train_time:93659ms step_avg:60.74ms
step:1543/2285 train_time:93722ms step_avg:60.74ms
step:1544/2285 train_time:93782ms step_avg:60.74ms
step:1545/2285 train_time:93845ms step_avg:60.74ms
step:1546/2285 train_time:93905ms step_avg:60.74ms
step:1547/2285 train_time:93968ms step_avg:60.74ms
step:1548/2285 train_time:94027ms step_avg:60.74ms
step:1549/2285 train_time:94090ms step_avg:60.74ms
step:1550/2285 train_time:94151ms step_avg:60.74ms
step:1551/2285 train_time:94214ms step_avg:60.74ms
step:1552/2285 train_time:94275ms step_avg:60.74ms
step:1553/2285 train_time:94338ms step_avg:60.75ms
step:1554/2285 train_time:94399ms step_avg:60.75ms
step:1555/2285 train_time:94462ms step_avg:60.75ms
step:1556/2285 train_time:94523ms step_avg:60.75ms
step:1557/2285 train_time:94586ms step_avg:60.75ms
step:1558/2285 train_time:94645ms step_avg:60.75ms
step:1559/2285 train_time:94708ms step_avg:60.75ms
step:1560/2285 train_time:94768ms step_avg:60.75ms
step:1561/2285 train_time:94830ms step_avg:60.75ms
step:1562/2285 train_time:94891ms step_avg:60.75ms
step:1563/2285 train_time:94953ms step_avg:60.75ms
step:1564/2285 train_time:95013ms step_avg:60.75ms
step:1565/2285 train_time:95076ms step_avg:60.75ms
step:1566/2285 train_time:95136ms step_avg:60.75ms
step:1567/2285 train_time:95199ms step_avg:60.75ms
step:1568/2285 train_time:95260ms step_avg:60.75ms
step:1569/2285 train_time:95324ms step_avg:60.75ms
step:1570/2285 train_time:95385ms step_avg:60.75ms
step:1571/2285 train_time:95448ms step_avg:60.76ms
step:1572/2285 train_time:95507ms step_avg:60.76ms
step:1573/2285 train_time:95571ms step_avg:60.76ms
step:1574/2285 train_time:95631ms step_avg:60.76ms
step:1575/2285 train_time:95693ms step_avg:60.76ms
step:1576/2285 train_time:95754ms step_avg:60.76ms
step:1577/2285 train_time:95816ms step_avg:60.76ms
step:1578/2285 train_time:95876ms step_avg:60.76ms
step:1579/2285 train_time:95939ms step_avg:60.76ms
step:1580/2285 train_time:95999ms step_avg:60.76ms
step:1581/2285 train_time:96062ms step_avg:60.76ms
step:1582/2285 train_time:96123ms step_avg:60.76ms
step:1583/2285 train_time:96186ms step_avg:60.76ms
step:1584/2285 train_time:96247ms step_avg:60.76ms
step:1585/2285 train_time:96309ms step_avg:60.76ms
step:1586/2285 train_time:96369ms step_avg:60.76ms
step:1587/2285 train_time:96432ms step_avg:60.76ms
step:1588/2285 train_time:96493ms step_avg:60.76ms
step:1589/2285 train_time:96556ms step_avg:60.77ms
step:1590/2285 train_time:96616ms step_avg:60.77ms
step:1591/2285 train_time:96679ms step_avg:60.77ms
step:1592/2285 train_time:96739ms step_avg:60.77ms
step:1593/2285 train_time:96802ms step_avg:60.77ms
step:1594/2285 train_time:96863ms step_avg:60.77ms
step:1595/2285 train_time:96926ms step_avg:60.77ms
step:1596/2285 train_time:96986ms step_avg:60.77ms
step:1597/2285 train_time:97048ms step_avg:60.77ms
step:1598/2285 train_time:97108ms step_avg:60.77ms
step:1599/2285 train_time:97171ms step_avg:60.77ms
step:1600/2285 train_time:97232ms step_avg:60.77ms
step:1601/2285 train_time:97295ms step_avg:60.77ms
step:1602/2285 train_time:97355ms step_avg:60.77ms
step:1603/2285 train_time:97418ms step_avg:60.77ms
step:1604/2285 train_time:97479ms step_avg:60.77ms
step:1605/2285 train_time:97542ms step_avg:60.77ms
step:1606/2285 train_time:97602ms step_avg:60.77ms
step:1607/2285 train_time:97665ms step_avg:60.77ms
step:1608/2285 train_time:97726ms step_avg:60.77ms
step:1609/2285 train_time:97789ms step_avg:60.78ms
step:1610/2285 train_time:97848ms step_avg:60.78ms
step:1611/2285 train_time:97910ms step_avg:60.78ms
step:1612/2285 train_time:97970ms step_avg:60.78ms
step:1613/2285 train_time:98034ms step_avg:60.78ms
step:1614/2285 train_time:98094ms step_avg:60.78ms
step:1615/2285 train_time:98158ms step_avg:60.78ms
step:1616/2285 train_time:98218ms step_avg:60.78ms
step:1617/2285 train_time:98281ms step_avg:60.78ms
step:1618/2285 train_time:98342ms step_avg:60.78ms
step:1619/2285 train_time:98405ms step_avg:60.78ms
step:1620/2285 train_time:98465ms step_avg:60.78ms
step:1621/2285 train_time:98527ms step_avg:60.78ms
step:1622/2285 train_time:98588ms step_avg:60.78ms
step:1623/2285 train_time:98651ms step_avg:60.78ms
step:1624/2285 train_time:98711ms step_avg:60.78ms
step:1625/2285 train_time:98773ms step_avg:60.78ms
step:1626/2285 train_time:98834ms step_avg:60.78ms
step:1627/2285 train_time:98897ms step_avg:60.78ms
step:1628/2285 train_time:98957ms step_avg:60.78ms
step:1629/2285 train_time:99020ms step_avg:60.79ms
step:1630/2285 train_time:99081ms step_avg:60.79ms
step:1631/2285 train_time:99144ms step_avg:60.79ms
step:1632/2285 train_time:99205ms step_avg:60.79ms
step:1633/2285 train_time:99268ms step_avg:60.79ms
step:1634/2285 train_time:99328ms step_avg:60.79ms
step:1635/2285 train_time:99391ms step_avg:60.79ms
step:1636/2285 train_time:99451ms step_avg:60.79ms
step:1637/2285 train_time:99513ms step_avg:60.79ms
step:1638/2285 train_time:99574ms step_avg:60.79ms
step:1639/2285 train_time:99636ms step_avg:60.79ms
step:1640/2285 train_time:99697ms step_avg:60.79ms
step:1641/2285 train_time:99759ms step_avg:60.79ms
step:1642/2285 train_time:99820ms step_avg:60.79ms
step:1643/2285 train_time:99883ms step_avg:60.79ms
step:1644/2285 train_time:99943ms step_avg:60.79ms
step:1645/2285 train_time:100006ms step_avg:60.79ms
step:1646/2285 train_time:100066ms step_avg:60.79ms
step:1647/2285 train_time:100129ms step_avg:60.79ms
step:1648/2285 train_time:100189ms step_avg:60.79ms
step:1649/2285 train_time:100252ms step_avg:60.80ms
step:1650/2285 train_time:100312ms step_avg:60.80ms
step:1651/2285 train_time:100376ms step_avg:60.80ms
step:1652/2285 train_time:100436ms step_avg:60.80ms
step:1653/2285 train_time:100499ms step_avg:60.80ms
step:1654/2285 train_time:100560ms step_avg:60.80ms
step:1655/2285 train_time:100623ms step_avg:60.80ms
step:1656/2285 train_time:100684ms step_avg:60.80ms
step:1657/2285 train_time:100747ms step_avg:60.80ms
step:1658/2285 train_time:100807ms step_avg:60.80ms
step:1659/2285 train_time:100869ms step_avg:60.80ms
step:1660/2285 train_time:100929ms step_avg:60.80ms
step:1661/2285 train_time:100992ms step_avg:60.80ms
step:1662/2285 train_time:101052ms step_avg:60.80ms
step:1663/2285 train_time:101115ms step_avg:60.80ms
step:1664/2285 train_time:101175ms step_avg:60.80ms
step:1665/2285 train_time:101238ms step_avg:60.80ms
step:1666/2285 train_time:101299ms step_avg:60.80ms
step:1667/2285 train_time:101362ms step_avg:60.80ms
step:1668/2285 train_time:101422ms step_avg:60.80ms
step:1669/2285 train_time:101485ms step_avg:60.81ms
step:1670/2285 train_time:101546ms step_avg:60.81ms
step:1671/2285 train_time:101609ms step_avg:60.81ms
step:1672/2285 train_time:101669ms step_avg:60.81ms
step:1673/2285 train_time:101732ms step_avg:60.81ms
step:1674/2285 train_time:101793ms step_avg:60.81ms
step:1675/2285 train_time:101856ms step_avg:60.81ms
step:1676/2285 train_time:101916ms step_avg:60.81ms
step:1677/2285 train_time:101979ms step_avg:60.81ms
step:1678/2285 train_time:102039ms step_avg:60.81ms
step:1679/2285 train_time:102103ms step_avg:60.81ms
step:1680/2285 train_time:102163ms step_avg:60.81ms
step:1681/2285 train_time:102226ms step_avg:60.81ms
step:1682/2285 train_time:102286ms step_avg:60.81ms
step:1683/2285 train_time:102349ms step_avg:60.81ms
step:1684/2285 train_time:102409ms step_avg:60.81ms
step:1685/2285 train_time:102472ms step_avg:60.81ms
step:1686/2285 train_time:102532ms step_avg:60.81ms
step:1687/2285 train_time:102595ms step_avg:60.81ms
step:1688/2285 train_time:102655ms step_avg:60.81ms
step:1689/2285 train_time:102719ms step_avg:60.82ms
step:1690/2285 train_time:102779ms step_avg:60.82ms
step:1691/2285 train_time:102842ms step_avg:60.82ms
step:1692/2285 train_time:102903ms step_avg:60.82ms
step:1693/2285 train_time:102966ms step_avg:60.82ms
step:1694/2285 train_time:103026ms step_avg:60.82ms
step:1695/2285 train_time:103089ms step_avg:60.82ms
step:1696/2285 train_time:103149ms step_avg:60.82ms
step:1697/2285 train_time:103212ms step_avg:60.82ms
step:1698/2285 train_time:103272ms step_avg:60.82ms
step:1699/2285 train_time:103334ms step_avg:60.82ms
step:1700/2285 train_time:103394ms step_avg:60.82ms
step:1701/2285 train_time:103457ms step_avg:60.82ms
step:1702/2285 train_time:103517ms step_avg:60.82ms
step:1703/2285 train_time:103580ms step_avg:60.82ms
step:1704/2285 train_time:103641ms step_avg:60.82ms
step:1705/2285 train_time:103704ms step_avg:60.82ms
step:1706/2285 train_time:103765ms step_avg:60.82ms
step:1707/2285 train_time:103828ms step_avg:60.82ms
step:1708/2285 train_time:103888ms step_avg:60.82ms
step:1709/2285 train_time:103951ms step_avg:60.83ms
step:1710/2285 train_time:104011ms step_avg:60.83ms
step:1711/2285 train_time:104074ms step_avg:60.83ms
step:1712/2285 train_time:104135ms step_avg:60.83ms
step:1713/2285 train_time:104198ms step_avg:60.83ms
step:1714/2285 train_time:104258ms step_avg:60.83ms
step:1715/2285 train_time:104321ms step_avg:60.83ms
step:1716/2285 train_time:104382ms step_avg:60.83ms
step:1717/2285 train_time:104445ms step_avg:60.83ms
step:1718/2285 train_time:104505ms step_avg:60.83ms
step:1719/2285 train_time:104568ms step_avg:60.83ms
step:1720/2285 train_time:104628ms step_avg:60.83ms
step:1721/2285 train_time:104692ms step_avg:60.83ms
step:1722/2285 train_time:104752ms step_avg:60.83ms
step:1723/2285 train_time:104815ms step_avg:60.83ms
step:1724/2285 train_time:104875ms step_avg:60.83ms
step:1725/2285 train_time:104938ms step_avg:60.83ms
step:1726/2285 train_time:104999ms step_avg:60.83ms
step:1727/2285 train_time:105062ms step_avg:60.83ms
step:1728/2285 train_time:105123ms step_avg:60.84ms
step:1729/2285 train_time:105187ms step_avg:60.84ms
step:1730/2285 train_time:105247ms step_avg:60.84ms
step:1731/2285 train_time:105309ms step_avg:60.84ms
step:1732/2285 train_time:105370ms step_avg:60.84ms
step:1733/2285 train_time:105433ms step_avg:60.84ms
step:1734/2285 train_time:105493ms step_avg:60.84ms
step:1735/2285 train_time:105556ms step_avg:60.84ms
step:1736/2285 train_time:105617ms step_avg:60.84ms
step:1737/2285 train_time:105680ms step_avg:60.84ms
step:1738/2285 train_time:105741ms step_avg:60.84ms
step:1739/2285 train_time:105804ms step_avg:60.84ms
step:1740/2285 train_time:105865ms step_avg:60.84ms
step:1741/2285 train_time:105928ms step_avg:60.84ms
step:1742/2285 train_time:105988ms step_avg:60.84ms
step:1743/2285 train_time:106051ms step_avg:60.84ms
step:1744/2285 train_time:106111ms step_avg:60.84ms
step:1745/2285 train_time:106174ms step_avg:60.84ms
step:1746/2285 train_time:106235ms step_avg:60.84ms
step:1747/2285 train_time:106298ms step_avg:60.85ms
step:1748/2285 train_time:106358ms step_avg:60.85ms
step:1749/2285 train_time:106422ms step_avg:60.85ms
step:1750/2285 train_time:106483ms step_avg:60.85ms
step:1750/2285 val_loss:3.3667 train_time:106547ms step_avg:60.88ms
step:1751/2285 train_time:106566ms step_avg:60.86ms
step:1752/2285 train_time:106610ms step_avg:60.85ms
step:1753/2285 train_time:106677ms step_avg:60.85ms
step:1754/2285 train_time:106738ms step_avg:60.85ms
step:1755/2285 train_time:106801ms step_avg:60.86ms
step:1756/2285 train_time:106860ms step_avg:60.85ms
step:1757/2285 train_time:106922ms step_avg:60.85ms
step:1758/2285 train_time:106982ms step_avg:60.85ms
step:1759/2285 train_time:107045ms step_avg:60.86ms
step:1760/2285 train_time:107106ms step_avg:60.86ms
step:1761/2285 train_time:107168ms step_avg:60.86ms
step:1762/2285 train_time:107229ms step_avg:60.86ms
step:1763/2285 train_time:107291ms step_avg:60.86ms
step:1764/2285 train_time:107351ms step_avg:60.86ms
step:1765/2285 train_time:107412ms step_avg:60.86ms
step:1766/2285 train_time:107473ms step_avg:60.86ms
step:1767/2285 train_time:107537ms step_avg:60.86ms
step:1768/2285 train_time:107599ms step_avg:60.86ms
step:1769/2285 train_time:107663ms step_avg:60.86ms
step:1770/2285 train_time:107723ms step_avg:60.86ms
step:1771/2285 train_time:107787ms step_avg:60.86ms
step:1772/2285 train_time:107847ms step_avg:60.86ms
step:1773/2285 train_time:107910ms step_avg:60.86ms
step:1774/2285 train_time:107969ms step_avg:60.86ms
step:1775/2285 train_time:108032ms step_avg:60.86ms
step:1776/2285 train_time:108091ms step_avg:60.86ms
step:1777/2285 train_time:108154ms step_avg:60.86ms
step:1778/2285 train_time:108214ms step_avg:60.86ms
step:1779/2285 train_time:108276ms step_avg:60.86ms
step:1780/2285 train_time:108336ms step_avg:60.86ms
step:1781/2285 train_time:108398ms step_avg:60.86ms
step:1782/2285 train_time:108459ms step_avg:60.86ms
step:1783/2285 train_time:108523ms step_avg:60.87ms
step:1784/2285 train_time:108584ms step_avg:60.87ms
step:1785/2285 train_time:108648ms step_avg:60.87ms
step:1786/2285 train_time:108709ms step_avg:60.87ms
step:1787/2285 train_time:108772ms step_avg:60.87ms
step:1788/2285 train_time:108833ms step_avg:60.87ms
step:1789/2285 train_time:108895ms step_avg:60.87ms
step:1790/2285 train_time:108955ms step_avg:60.87ms
step:1791/2285 train_time:109018ms step_avg:60.87ms
step:1792/2285 train_time:109078ms step_avg:60.87ms
step:1793/2285 train_time:109141ms step_avg:60.87ms
step:1794/2285 train_time:109201ms step_avg:60.87ms
step:1795/2285 train_time:109263ms step_avg:60.87ms
step:1796/2285 train_time:109324ms step_avg:60.87ms
step:1797/2285 train_time:109386ms step_avg:60.87ms
step:1798/2285 train_time:109447ms step_avg:60.87ms
step:1799/2285 train_time:109511ms step_avg:60.87ms
step:1800/2285 train_time:109570ms step_avg:60.87ms
step:1801/2285 train_time:109633ms step_avg:60.87ms
step:1802/2285 train_time:109694ms step_avg:60.87ms
step:1803/2285 train_time:109757ms step_avg:60.87ms
step:1804/2285 train_time:109817ms step_avg:60.87ms
step:1805/2285 train_time:109880ms step_avg:60.88ms
step:1806/2285 train_time:109940ms step_avg:60.88ms
step:1807/2285 train_time:110003ms step_avg:60.88ms
step:1808/2285 train_time:110063ms step_avg:60.88ms
step:1809/2285 train_time:110126ms step_avg:60.88ms
step:1810/2285 train_time:110187ms step_avg:60.88ms
step:1811/2285 train_time:110250ms step_avg:60.88ms
step:1812/2285 train_time:110310ms step_avg:60.88ms
step:1813/2285 train_time:110372ms step_avg:60.88ms
step:1814/2285 train_time:110432ms step_avg:60.88ms
step:1815/2285 train_time:110495ms step_avg:60.88ms
step:1816/2285 train_time:110555ms step_avg:60.88ms
step:1817/2285 train_time:110618ms step_avg:60.88ms
step:1818/2285 train_time:110679ms step_avg:60.88ms
step:1819/2285 train_time:110742ms step_avg:60.88ms
step:1820/2285 train_time:110803ms step_avg:60.88ms
step:1821/2285 train_time:110866ms step_avg:60.88ms
step:1822/2285 train_time:110926ms step_avg:60.88ms
step:1823/2285 train_time:110988ms step_avg:60.88ms
step:1824/2285 train_time:111048ms step_avg:60.88ms
step:1825/2285 train_time:111110ms step_avg:60.88ms
step:1826/2285 train_time:111171ms step_avg:60.88ms
step:1827/2285 train_time:111233ms step_avg:60.88ms
step:1828/2285 train_time:111293ms step_avg:60.88ms
step:1829/2285 train_time:111355ms step_avg:60.88ms
step:1830/2285 train_time:111415ms step_avg:60.88ms
step:1831/2285 train_time:111478ms step_avg:60.88ms
step:1832/2285 train_time:111539ms step_avg:60.88ms
step:1833/2285 train_time:111602ms step_avg:60.88ms
step:1834/2285 train_time:111662ms step_avg:60.88ms
step:1835/2285 train_time:111725ms step_avg:60.89ms
step:1836/2285 train_time:111786ms step_avg:60.89ms
step:1837/2285 train_time:111850ms step_avg:60.89ms
step:1838/2285 train_time:111910ms step_avg:60.89ms
step:1839/2285 train_time:111972ms step_avg:60.89ms
step:1840/2285 train_time:112032ms step_avg:60.89ms
step:1841/2285 train_time:112094ms step_avg:60.89ms
step:1842/2285 train_time:112154ms step_avg:60.89ms
step:1843/2285 train_time:112217ms step_avg:60.89ms
step:1844/2285 train_time:112277ms step_avg:60.89ms
step:1845/2285 train_time:112339ms step_avg:60.89ms
step:1846/2285 train_time:112399ms step_avg:60.89ms
step:1847/2285 train_time:112463ms step_avg:60.89ms
step:1848/2285 train_time:112523ms step_avg:60.89ms
step:1849/2285 train_time:112586ms step_avg:60.89ms
step:1850/2285 train_time:112646ms step_avg:60.89ms
step:1851/2285 train_time:112710ms step_avg:60.89ms
step:1852/2285 train_time:112770ms step_avg:60.89ms
step:1853/2285 train_time:112833ms step_avg:60.89ms
step:1854/2285 train_time:112893ms step_avg:60.89ms
step:1855/2285 train_time:112957ms step_avg:60.89ms
step:1856/2285 train_time:113017ms step_avg:60.89ms
step:1857/2285 train_time:113080ms step_avg:60.89ms
step:1858/2285 train_time:113141ms step_avg:60.89ms
step:1859/2285 train_time:113203ms step_avg:60.89ms
step:1860/2285 train_time:113264ms step_avg:60.89ms
step:1861/2285 train_time:113327ms step_avg:60.90ms
step:1862/2285 train_time:113388ms step_avg:60.90ms
step:1863/2285 train_time:113451ms step_avg:60.90ms
step:1864/2285 train_time:113511ms step_avg:60.90ms
step:1865/2285 train_time:113573ms step_avg:60.90ms
step:1866/2285 train_time:113633ms step_avg:60.90ms
step:1867/2285 train_time:113696ms step_avg:60.90ms
step:1868/2285 train_time:113756ms step_avg:60.90ms
step:1869/2285 train_time:113819ms step_avg:60.90ms
step:1870/2285 train_time:113879ms step_avg:60.90ms
step:1871/2285 train_time:113942ms step_avg:60.90ms
step:1872/2285 train_time:114003ms step_avg:60.90ms
step:1873/2285 train_time:114066ms step_avg:60.90ms
step:1874/2285 train_time:114127ms step_avg:60.90ms
step:1875/2285 train_time:114189ms step_avg:60.90ms
step:1876/2285 train_time:114250ms step_avg:60.90ms
step:1877/2285 train_time:114312ms step_avg:60.90ms
step:1878/2285 train_time:114371ms step_avg:60.90ms
step:1879/2285 train_time:114434ms step_avg:60.90ms
step:1880/2285 train_time:114494ms step_avg:60.90ms
step:1881/2285 train_time:114556ms step_avg:60.90ms
step:1882/2285 train_time:114617ms step_avg:60.90ms
step:1883/2285 train_time:114679ms step_avg:60.90ms
step:1884/2285 train_time:114739ms step_avg:60.90ms
step:1885/2285 train_time:114804ms step_avg:60.90ms
step:1886/2285 train_time:114864ms step_avg:60.90ms
step:1887/2285 train_time:114927ms step_avg:60.90ms
step:1888/2285 train_time:114988ms step_avg:60.90ms
step:1889/2285 train_time:115050ms step_avg:60.91ms
step:1890/2285 train_time:115110ms step_avg:60.91ms
step:1891/2285 train_time:115173ms step_avg:60.91ms
step:1892/2285 train_time:115233ms step_avg:60.91ms
step:1893/2285 train_time:115296ms step_avg:60.91ms
step:1894/2285 train_time:115356ms step_avg:60.91ms
step:1895/2285 train_time:115418ms step_avg:60.91ms
step:1896/2285 train_time:115478ms step_avg:60.91ms
step:1897/2285 train_time:115541ms step_avg:60.91ms
step:1898/2285 train_time:115602ms step_avg:60.91ms
step:1899/2285 train_time:115665ms step_avg:60.91ms
step:1900/2285 train_time:115725ms step_avg:60.91ms
step:1901/2285 train_time:115788ms step_avg:60.91ms
step:1902/2285 train_time:115849ms step_avg:60.91ms
step:1903/2285 train_time:115912ms step_avg:60.91ms
step:1904/2285 train_time:115972ms step_avg:60.91ms
step:1905/2285 train_time:116035ms step_avg:60.91ms
step:1906/2285 train_time:116095ms step_avg:60.91ms
step:1907/2285 train_time:116158ms step_avg:60.91ms
step:1908/2285 train_time:116218ms step_avg:60.91ms
step:1909/2285 train_time:116280ms step_avg:60.91ms
step:1910/2285 train_time:116340ms step_avg:60.91ms
step:1911/2285 train_time:116403ms step_avg:60.91ms
step:1912/2285 train_time:116464ms step_avg:60.91ms
step:1913/2285 train_time:116527ms step_avg:60.91ms
step:1914/2285 train_time:116588ms step_avg:60.91ms
step:1915/2285 train_time:116651ms step_avg:60.91ms
step:1916/2285 train_time:116711ms step_avg:60.91ms
step:1917/2285 train_time:116773ms step_avg:60.91ms
step:1918/2285 train_time:116833ms step_avg:60.91ms
step:1919/2285 train_time:116896ms step_avg:60.91ms
step:1920/2285 train_time:116956ms step_avg:60.91ms
step:1921/2285 train_time:117019ms step_avg:60.92ms
step:1922/2285 train_time:117080ms step_avg:60.92ms
step:1923/2285 train_time:117143ms step_avg:60.92ms
step:1924/2285 train_time:117204ms step_avg:60.92ms
step:1925/2285 train_time:117266ms step_avg:60.92ms
step:1926/2285 train_time:117327ms step_avg:60.92ms
step:1927/2285 train_time:117389ms step_avg:60.92ms
step:1928/2285 train_time:117449ms step_avg:60.92ms
step:1929/2285 train_time:117512ms step_avg:60.92ms
step:1930/2285 train_time:117572ms step_avg:60.92ms
step:1931/2285 train_time:117635ms step_avg:60.92ms
step:1932/2285 train_time:117695ms step_avg:60.92ms
step:1933/2285 train_time:117759ms step_avg:60.92ms
step:1934/2285 train_time:117820ms step_avg:60.92ms
step:1935/2285 train_time:117882ms step_avg:60.92ms
step:1936/2285 train_time:117944ms step_avg:60.92ms
step:1937/2285 train_time:118006ms step_avg:60.92ms
step:1938/2285 train_time:118066ms step_avg:60.92ms
step:1939/2285 train_time:118129ms step_avg:60.92ms
step:1940/2285 train_time:118189ms step_avg:60.92ms
step:1941/2285 train_time:118252ms step_avg:60.92ms
step:1942/2285 train_time:118312ms step_avg:60.92ms
step:1943/2285 train_time:118375ms step_avg:60.92ms
step:1944/2285 train_time:118435ms step_avg:60.92ms
step:1945/2285 train_time:118497ms step_avg:60.92ms
step:1946/2285 train_time:118558ms step_avg:60.92ms
step:1947/2285 train_time:118620ms step_avg:60.92ms
step:1948/2285 train_time:118681ms step_avg:60.92ms
step:1949/2285 train_time:118745ms step_avg:60.93ms
step:1950/2285 train_time:118805ms step_avg:60.93ms
step:1951/2285 train_time:118868ms step_avg:60.93ms
step:1952/2285 train_time:118929ms step_avg:60.93ms
step:1953/2285 train_time:118991ms step_avg:60.93ms
step:1954/2285 train_time:119052ms step_avg:60.93ms
step:1955/2285 train_time:119114ms step_avg:60.93ms
step:1956/2285 train_time:119174ms step_avg:60.93ms
step:1957/2285 train_time:119237ms step_avg:60.93ms
step:1958/2285 train_time:119297ms step_avg:60.93ms
step:1959/2285 train_time:119361ms step_avg:60.93ms
step:1960/2285 train_time:119421ms step_avg:60.93ms
step:1961/2285 train_time:119484ms step_avg:60.93ms
step:1962/2285 train_time:119544ms step_avg:60.93ms
step:1963/2285 train_time:119607ms step_avg:60.93ms
step:1964/2285 train_time:119667ms step_avg:60.93ms
step:1965/2285 train_time:119731ms step_avg:60.93ms
step:1966/2285 train_time:119792ms step_avg:60.93ms
step:1967/2285 train_time:119854ms step_avg:60.93ms
step:1968/2285 train_time:119915ms step_avg:60.93ms
step:1969/2285 train_time:119977ms step_avg:60.93ms
step:1970/2285 train_time:120038ms step_avg:60.93ms
step:1971/2285 train_time:120100ms step_avg:60.93ms
step:1972/2285 train_time:120161ms step_avg:60.93ms
step:1973/2285 train_time:120224ms step_avg:60.93ms
step:1974/2285 train_time:120285ms step_avg:60.93ms
step:1975/2285 train_time:120348ms step_avg:60.94ms
step:1976/2285 train_time:120409ms step_avg:60.94ms
step:1977/2285 train_time:120471ms step_avg:60.94ms
step:1978/2285 train_time:120531ms step_avg:60.94ms
step:1979/2285 train_time:120593ms step_avg:60.94ms
step:1980/2285 train_time:120653ms step_avg:60.94ms
step:1981/2285 train_time:120716ms step_avg:60.94ms
step:1982/2285 train_time:120776ms step_avg:60.94ms
step:1983/2285 train_time:120839ms step_avg:60.94ms
step:1984/2285 train_time:120899ms step_avg:60.94ms
step:1985/2285 train_time:120962ms step_avg:60.94ms
step:1986/2285 train_time:121023ms step_avg:60.94ms
step:1987/2285 train_time:121085ms step_avg:60.94ms
step:1988/2285 train_time:121147ms step_avg:60.94ms
step:1989/2285 train_time:121210ms step_avg:60.94ms
step:1990/2285 train_time:121270ms step_avg:60.94ms
step:1991/2285 train_time:121333ms step_avg:60.94ms
step:1992/2285 train_time:121393ms step_avg:60.94ms
step:1993/2285 train_time:121456ms step_avg:60.94ms
step:1994/2285 train_time:121517ms step_avg:60.94ms
step:1995/2285 train_time:121579ms step_avg:60.94ms
step:1996/2285 train_time:121640ms step_avg:60.94ms
step:1997/2285 train_time:121703ms step_avg:60.94ms
step:1998/2285 train_time:121763ms step_avg:60.94ms
step:1999/2285 train_time:121826ms step_avg:60.94ms
step:2000/2285 train_time:121886ms step_avg:60.94ms
step:2000/2285 val_loss:3.3213 train_time:121950ms step_avg:60.97ms
step:2001/2285 train_time:121968ms step_avg:60.95ms
step:2002/2285 train_time:122016ms step_avg:60.95ms
step:2003/2285 train_time:122081ms step_avg:60.95ms
step:2004/2285 train_time:122142ms step_avg:60.95ms
step:2005/2285 train_time:122205ms step_avg:60.95ms
step:2006/2285 train_time:122266ms step_avg:60.95ms
step:2007/2285 train_time:122328ms step_avg:60.95ms
step:2008/2285 train_time:122388ms step_avg:60.95ms
step:2009/2285 train_time:122450ms step_avg:60.95ms
step:2010/2285 train_time:122509ms step_avg:60.95ms
step:2011/2285 train_time:122571ms step_avg:60.95ms
step:2012/2285 train_time:122631ms step_avg:60.95ms
step:2013/2285 train_time:122693ms step_avg:60.95ms
step:2014/2285 train_time:122752ms step_avg:60.95ms
step:2015/2285 train_time:122814ms step_avg:60.95ms
step:2016/2285 train_time:122876ms step_avg:60.95ms
step:2017/2285 train_time:122939ms step_avg:60.95ms
step:2018/2285 train_time:123001ms step_avg:60.95ms
step:2019/2285 train_time:123065ms step_avg:60.95ms
step:2020/2285 train_time:123126ms step_avg:60.95ms
step:2021/2285 train_time:123190ms step_avg:60.95ms
step:2022/2285 train_time:123250ms step_avg:60.95ms
step:2023/2285 train_time:123313ms step_avg:60.96ms
step:2024/2285 train_time:123373ms step_avg:60.96ms
step:2025/2285 train_time:123436ms step_avg:60.96ms
step:2026/2285 train_time:123496ms step_avg:60.96ms
step:2027/2285 train_time:123558ms step_avg:60.96ms
step:2028/2285 train_time:123618ms step_avg:60.96ms
step:2029/2285 train_time:123680ms step_avg:60.96ms
step:2030/2285 train_time:123740ms step_avg:60.96ms
step:2031/2285 train_time:123803ms step_avg:60.96ms
step:2032/2285 train_time:123865ms step_avg:60.96ms
step:2033/2285 train_time:123928ms step_avg:60.96ms
step:2034/2285 train_time:123989ms step_avg:60.96ms
step:2035/2285 train_time:124053ms step_avg:60.96ms
step:2036/2285 train_time:124113ms step_avg:60.96ms
step:2037/2285 train_time:124176ms step_avg:60.96ms
step:2038/2285 train_time:124237ms step_avg:60.96ms
step:2039/2285 train_time:124300ms step_avg:60.96ms
step:2040/2285 train_time:124361ms step_avg:60.96ms
step:2041/2285 train_time:124423ms step_avg:60.96ms
step:2042/2285 train_time:124484ms step_avg:60.96ms
step:2043/2285 train_time:124547ms step_avg:60.96ms
step:2044/2285 train_time:124608ms step_avg:60.96ms
step:2045/2285 train_time:124670ms step_avg:60.96ms
step:2046/2285 train_time:124730ms step_avg:60.96ms
step:2047/2285 train_time:124793ms step_avg:60.96ms
step:2048/2285 train_time:124853ms step_avg:60.96ms
step:2049/2285 train_time:124917ms step_avg:60.96ms
step:2050/2285 train_time:124978ms step_avg:60.96ms
step:2051/2285 train_time:125041ms step_avg:60.97ms
step:2052/2285 train_time:125102ms step_avg:60.97ms
step:2053/2285 train_time:125166ms step_avg:60.97ms
step:2054/2285 train_time:125227ms step_avg:60.97ms
step:2055/2285 train_time:125290ms step_avg:60.97ms
step:2056/2285 train_time:125350ms step_avg:60.97ms
step:2057/2285 train_time:125412ms step_avg:60.97ms
step:2058/2285 train_time:125472ms step_avg:60.97ms
step:2059/2285 train_time:125535ms step_avg:60.97ms
step:2060/2285 train_time:125595ms step_avg:60.97ms
step:2061/2285 train_time:125658ms step_avg:60.97ms
step:2062/2285 train_time:125718ms step_avg:60.97ms
step:2063/2285 train_time:125781ms step_avg:60.97ms
step:2064/2285 train_time:125842ms step_avg:60.97ms
step:2065/2285 train_time:125904ms step_avg:60.97ms
step:2066/2285 train_time:125965ms step_avg:60.97ms
step:2067/2285 train_time:126028ms step_avg:60.97ms
step:2068/2285 train_time:126088ms step_avg:60.97ms
step:2069/2285 train_time:126151ms step_avg:60.97ms
step:2070/2285 train_time:126212ms step_avg:60.97ms
step:2071/2285 train_time:126275ms step_avg:60.97ms
step:2072/2285 train_time:126336ms step_avg:60.97ms
step:2073/2285 train_time:126399ms step_avg:60.97ms
step:2074/2285 train_time:126460ms step_avg:60.97ms
step:2075/2285 train_time:126523ms step_avg:60.97ms
step:2076/2285 train_time:126583ms step_avg:60.97ms
step:2077/2285 train_time:126646ms step_avg:60.98ms
step:2078/2285 train_time:126706ms step_avg:60.97ms
step:2079/2285 train_time:126770ms step_avg:60.98ms
step:2080/2285 train_time:126830ms step_avg:60.98ms
step:2081/2285 train_time:126893ms step_avg:60.98ms
step:2082/2285 train_time:126954ms step_avg:60.98ms
step:2083/2285 train_time:127017ms step_avg:60.98ms
step:2084/2285 train_time:127078ms step_avg:60.98ms
step:2085/2285 train_time:127141ms step_avg:60.98ms
step:2086/2285 train_time:127202ms step_avg:60.98ms
step:2087/2285 train_time:127265ms step_avg:60.98ms
step:2088/2285 train_time:127326ms step_avg:60.98ms
step:2089/2285 train_time:127389ms step_avg:60.98ms
step:2090/2285 train_time:127449ms step_avg:60.98ms
step:2091/2285 train_time:127512ms step_avg:60.98ms
step:2092/2285 train_time:127572ms step_avg:60.98ms
step:2093/2285 train_time:127634ms step_avg:60.98ms
step:2094/2285 train_time:127695ms step_avg:60.98ms
step:2095/2285 train_time:127757ms step_avg:60.98ms
step:2096/2285 train_time:127818ms step_avg:60.98ms
step:2097/2285 train_time:127880ms step_avg:60.98ms
step:2098/2285 train_time:127940ms step_avg:60.98ms
step:2099/2285 train_time:128003ms step_avg:60.98ms
step:2100/2285 train_time:128064ms step_avg:60.98ms
step:2101/2285 train_time:128127ms step_avg:60.98ms
step:2102/2285 train_time:128187ms step_avg:60.98ms
step:2103/2285 train_time:128251ms step_avg:60.98ms
step:2104/2285 train_time:128311ms step_avg:60.98ms
step:2105/2285 train_time:128374ms step_avg:60.99ms
step:2106/2285 train_time:128435ms step_avg:60.99ms
step:2107/2285 train_time:128497ms step_avg:60.99ms
step:2108/2285 train_time:128557ms step_avg:60.99ms
step:2109/2285 train_time:128621ms step_avg:60.99ms
step:2110/2285 train_time:128682ms step_avg:60.99ms
step:2111/2285 train_time:128745ms step_avg:60.99ms
step:2112/2285 train_time:128806ms step_avg:60.99ms
step:2113/2285 train_time:128869ms step_avg:60.99ms
step:2114/2285 train_time:128930ms step_avg:60.99ms
step:2115/2285 train_time:128993ms step_avg:60.99ms
step:2116/2285 train_time:129053ms step_avg:60.99ms
step:2117/2285 train_time:129116ms step_avg:60.99ms
step:2118/2285 train_time:129177ms step_avg:60.99ms
step:2119/2285 train_time:129241ms step_avg:60.99ms
step:2120/2285 train_time:129302ms step_avg:60.99ms
step:2121/2285 train_time:129365ms step_avg:60.99ms
step:2122/2285 train_time:129426ms step_avg:60.99ms
step:2123/2285 train_time:129489ms step_avg:60.99ms
step:2124/2285 train_time:129549ms step_avg:60.99ms
step:2125/2285 train_time:129612ms step_avg:60.99ms
step:2126/2285 train_time:129672ms step_avg:60.99ms
step:2127/2285 train_time:129734ms step_avg:60.99ms
step:2128/2285 train_time:129794ms step_avg:60.99ms
step:2129/2285 train_time:129857ms step_avg:60.99ms
step:2130/2285 train_time:129918ms step_avg:60.99ms
step:2131/2285 train_time:129981ms step_avg:61.00ms
step:2132/2285 train_time:130042ms step_avg:61.00ms
step:2133/2285 train_time:130105ms step_avg:61.00ms
step:2134/2285 train_time:130165ms step_avg:61.00ms
step:2135/2285 train_time:130229ms step_avg:61.00ms
step:2136/2285 train_time:130289ms step_avg:61.00ms
step:2137/2285 train_time:130352ms step_avg:61.00ms
step:2138/2285 train_time:130412ms step_avg:61.00ms
step:2139/2285 train_time:130474ms step_avg:61.00ms
step:2140/2285 train_time:130534ms step_avg:61.00ms
step:2141/2285 train_time:130597ms step_avg:61.00ms
step:2142/2285 train_time:130658ms step_avg:61.00ms
step:2143/2285 train_time:130721ms step_avg:61.00ms
step:2144/2285 train_time:130781ms step_avg:61.00ms
step:2145/2285 train_time:130844ms step_avg:61.00ms
step:2146/2285 train_time:130904ms step_avg:61.00ms
step:2147/2285 train_time:130967ms step_avg:61.00ms
step:2148/2285 train_time:131028ms step_avg:61.00ms
step:2149/2285 train_time:131090ms step_avg:61.00ms
step:2150/2285 train_time:131150ms step_avg:61.00ms
step:2151/2285 train_time:131214ms step_avg:61.00ms
step:2152/2285 train_time:131274ms step_avg:61.00ms
step:2153/2285 train_time:131336ms step_avg:61.00ms
step:2154/2285 train_time:131397ms step_avg:61.00ms
step:2155/2285 train_time:131460ms step_avg:61.00ms
step:2156/2285 train_time:131521ms step_avg:61.00ms
step:2157/2285 train_time:131583ms step_avg:61.00ms
step:2158/2285 train_time:131644ms step_avg:61.00ms
step:2159/2285 train_time:131708ms step_avg:61.00ms
step:2160/2285 train_time:131768ms step_avg:61.00ms
step:2161/2285 train_time:131831ms step_avg:61.00ms
step:2162/2285 train_time:131891ms step_avg:61.00ms
step:2163/2285 train_time:131954ms step_avg:61.00ms
step:2164/2285 train_time:132014ms step_avg:61.00ms
step:2165/2285 train_time:132077ms step_avg:61.01ms
step:2166/2285 train_time:132137ms step_avg:61.01ms
step:2167/2285 train_time:132200ms step_avg:61.01ms
step:2168/2285 train_time:132260ms step_avg:61.01ms
step:2169/2285 train_time:132323ms step_avg:61.01ms
step:2170/2285 train_time:132383ms step_avg:61.01ms
step:2171/2285 train_time:132447ms step_avg:61.01ms
step:2172/2285 train_time:132507ms step_avg:61.01ms
step:2173/2285 train_time:132569ms step_avg:61.01ms
step:2174/2285 train_time:132630ms step_avg:61.01ms
step:2175/2285 train_time:132693ms step_avg:61.01ms
step:2176/2285 train_time:132753ms step_avg:61.01ms
step:2177/2285 train_time:132816ms step_avg:61.01ms
step:2178/2285 train_time:132876ms step_avg:61.01ms
step:2179/2285 train_time:132939ms step_avg:61.01ms
step:2180/2285 train_time:133000ms step_avg:61.01ms
step:2181/2285 train_time:133063ms step_avg:61.01ms
step:2182/2285 train_time:133124ms step_avg:61.01ms
step:2183/2285 train_time:133187ms step_avg:61.01ms
step:2184/2285 train_time:133248ms step_avg:61.01ms
step:2185/2285 train_time:133310ms step_avg:61.01ms
step:2186/2285 train_time:133370ms step_avg:61.01ms
step:2187/2285 train_time:133433ms step_avg:61.01ms
step:2188/2285 train_time:133493ms step_avg:61.01ms
step:2189/2285 train_time:133557ms step_avg:61.01ms
step:2190/2285 train_time:133617ms step_avg:61.01ms
step:2191/2285 train_time:133681ms step_avg:61.01ms
step:2192/2285 train_time:133740ms step_avg:61.01ms
step:2193/2285 train_time:133804ms step_avg:61.01ms
step:2194/2285 train_time:133864ms step_avg:61.01ms
step:2195/2285 train_time:133927ms step_avg:61.01ms
step:2196/2285 train_time:133987ms step_avg:61.01ms
step:2197/2285 train_time:134050ms step_avg:61.01ms
step:2198/2285 train_time:134110ms step_avg:61.01ms
step:2199/2285 train_time:134173ms step_avg:61.02ms
step:2200/2285 train_time:134233ms step_avg:61.02ms
step:2201/2285 train_time:134297ms step_avg:61.02ms
step:2202/2285 train_time:134357ms step_avg:61.02ms
step:2203/2285 train_time:134420ms step_avg:61.02ms
step:2204/2285 train_time:134480ms step_avg:61.02ms
step:2205/2285 train_time:134544ms step_avg:61.02ms
step:2206/2285 train_time:134605ms step_avg:61.02ms
step:2207/2285 train_time:134669ms step_avg:61.02ms
step:2208/2285 train_time:134729ms step_avg:61.02ms
step:2209/2285 train_time:134791ms step_avg:61.02ms
step:2210/2285 train_time:134851ms step_avg:61.02ms
step:2211/2285 train_time:134914ms step_avg:61.02ms
step:2212/2285 train_time:134975ms step_avg:61.02ms
step:2213/2285 train_time:135038ms step_avg:61.02ms
step:2214/2285 train_time:135098ms step_avg:61.02ms
step:2215/2285 train_time:135161ms step_avg:61.02ms
step:2216/2285 train_time:135222ms step_avg:61.02ms
step:2217/2285 train_time:135285ms step_avg:61.02ms
step:2218/2285 train_time:135346ms step_avg:61.02ms
step:2219/2285 train_time:135409ms step_avg:61.02ms
step:2220/2285 train_time:135469ms step_avg:61.02ms
step:2221/2285 train_time:135532ms step_avg:61.02ms
step:2222/2285 train_time:135592ms step_avg:61.02ms
step:2223/2285 train_time:135655ms step_avg:61.02ms
step:2224/2285 train_time:135716ms step_avg:61.02ms
step:2225/2285 train_time:135778ms step_avg:61.02ms
step:2226/2285 train_time:135839ms step_avg:61.02ms
step:2227/2285 train_time:135902ms step_avg:61.02ms
step:2228/2285 train_time:135963ms step_avg:61.02ms
step:2229/2285 train_time:136026ms step_avg:61.03ms
step:2230/2285 train_time:136087ms step_avg:61.03ms
step:2231/2285 train_time:136150ms step_avg:61.03ms
step:2232/2285 train_time:136210ms step_avg:61.03ms
step:2233/2285 train_time:136273ms step_avg:61.03ms
step:2234/2285 train_time:136333ms step_avg:61.03ms
step:2235/2285 train_time:136396ms step_avg:61.03ms
step:2236/2285 train_time:136457ms step_avg:61.03ms
step:2237/2285 train_time:136520ms step_avg:61.03ms
step:2238/2285 train_time:136581ms step_avg:61.03ms
step:2239/2285 train_time:136644ms step_avg:61.03ms
step:2240/2285 train_time:136705ms step_avg:61.03ms
step:2241/2285 train_time:136768ms step_avg:61.03ms
step:2242/2285 train_time:136828ms step_avg:61.03ms
step:2243/2285 train_time:136891ms step_avg:61.03ms
step:2244/2285 train_time:136951ms step_avg:61.03ms
step:2245/2285 train_time:137014ms step_avg:61.03ms
step:2246/2285 train_time:137074ms step_avg:61.03ms
step:2247/2285 train_time:137138ms step_avg:61.03ms
step:2248/2285 train_time:137197ms step_avg:61.03ms
step:2249/2285 train_time:137260ms step_avg:61.03ms
step:2250/2285 train_time:137322ms step_avg:61.03ms
step:2250/2285 val_loss:3.2838 train_time:137386ms step_avg:61.06ms
step:2251/2285 train_time:137406ms step_avg:61.04ms
step:2252/2285 train_time:137449ms step_avg:61.03ms
step:2253/2285 train_time:137512ms step_avg:61.04ms
step:2254/2285 train_time:137574ms step_avg:61.04ms
step:2255/2285 train_time:137638ms step_avg:61.04ms
step:2256/2285 train_time:137698ms step_avg:61.04ms
step:2257/2285 train_time:137760ms step_avg:61.04ms
step:2258/2285 train_time:137819ms step_avg:61.04ms
step:2259/2285 train_time:137881ms step_avg:61.04ms
step:2260/2285 train_time:137941ms step_avg:61.04ms
step:2261/2285 train_time:138003ms step_avg:61.04ms
step:2262/2285 train_time:138062ms step_avg:61.04ms
step:2263/2285 train_time:138125ms step_avg:61.04ms
step:2264/2285 train_time:138184ms step_avg:61.04ms
step:2265/2285 train_time:138246ms step_avg:61.04ms
step:2266/2285 train_time:138312ms step_avg:61.04ms
step:2267/2285 train_time:138379ms step_avg:61.04ms
step:2268/2285 train_time:138442ms step_avg:61.04ms
step:2269/2285 train_time:138506ms step_avg:61.04ms
step:2270/2285 train_time:138567ms step_avg:61.04ms
step:2271/2285 train_time:138630ms step_avg:61.04ms
step:2272/2285 train_time:138690ms step_avg:61.04ms
step:2273/2285 train_time:138753ms step_avg:61.04ms
step:2274/2285 train_time:138813ms step_avg:61.04ms
step:2275/2285 train_time:138875ms step_avg:61.04ms
step:2276/2285 train_time:138936ms step_avg:61.04ms
step:2277/2285 train_time:138998ms step_avg:61.04ms
step:2278/2285 train_time:139058ms step_avg:61.04ms
step:2279/2285 train_time:139120ms step_avg:61.04ms
step:2280/2285 train_time:139180ms step_avg:61.04ms
step:2281/2285 train_time:139243ms step_avg:61.04ms
step:2282/2285 train_time:139305ms step_avg:61.04ms
step:2283/2285 train_time:139369ms step_avg:61.05ms
step:2284/2285 train_time:139430ms step_avg:61.05ms
step:2285/2285 train_time:139494ms step_avg:61.05ms
step:2285/2285 val_loss:3.2778 train_time:139555ms step_avg:61.07ms
peak memory allocated: 29248 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2285
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 02:04:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2285 train_time:119ms step_avg:119.32ms
step:2/2285 train_time:141ms step_avg:70.40ms
step:3/2285 train_time:178ms step_avg:59.44ms
step:4/2285 train_time:234ms step_avg:58.60ms
step:5/2285 train_time:294ms step_avg:58.77ms
step:6/2285 train_time:352ms step_avg:58.61ms
step:7/2285 train_time:412ms step_avg:58.87ms
step:8/2285 train_time:471ms step_avg:58.83ms
step:9/2285 train_time:531ms step_avg:59.02ms
step:10/2285 train_time:590ms step_avg:59.00ms
step:11/2285 train_time:651ms step_avg:59.15ms
step:12/2285 train_time:709ms step_avg:59.08ms
step:13/2285 train_time:770ms step_avg:59.20ms
step:14/2285 train_time:828ms step_avg:59.17ms
step:15/2285 train_time:889ms step_avg:59.27ms
step:16/2285 train_time:948ms step_avg:59.23ms
step:17/2285 train_time:1012ms step_avg:59.55ms
step:18/2285 train_time:1076ms step_avg:59.78ms
step:19/2285 train_time:1140ms step_avg:60.03ms
step:20/2285 train_time:1201ms step_avg:60.03ms
step:21/2285 train_time:1262ms step_avg:60.07ms
step:22/2285 train_time:1321ms step_avg:60.03ms
step:23/2285 train_time:1381ms step_avg:60.06ms
step:24/2285 train_time:1440ms step_avg:60.01ms
step:25/2285 train_time:1501ms step_avg:60.06ms
step:26/2285 train_time:1561ms step_avg:60.03ms
step:27/2285 train_time:1622ms step_avg:60.07ms
step:28/2285 train_time:1681ms step_avg:60.05ms
step:29/2285 train_time:1743ms step_avg:60.09ms
step:30/2285 train_time:1802ms step_avg:60.07ms
step:31/2285 train_time:1863ms step_avg:60.09ms
step:32/2285 train_time:1922ms step_avg:60.06ms
step:33/2285 train_time:1985ms step_avg:60.14ms
step:34/2285 train_time:2044ms step_avg:60.13ms
step:35/2285 train_time:2106ms step_avg:60.18ms
step:36/2285 train_time:2165ms step_avg:60.15ms
step:37/2285 train_time:2227ms step_avg:60.18ms
step:38/2285 train_time:2286ms step_avg:60.15ms
step:39/2285 train_time:2347ms step_avg:60.18ms
step:40/2285 train_time:2406ms step_avg:60.15ms
step:41/2285 train_time:2467ms step_avg:60.18ms
step:42/2285 train_time:2526ms step_avg:60.15ms
step:43/2285 train_time:2587ms step_avg:60.17ms
step:44/2285 train_time:2647ms step_avg:60.16ms
step:45/2285 train_time:2709ms step_avg:60.20ms
step:46/2285 train_time:2768ms step_avg:60.17ms
step:47/2285 train_time:2830ms step_avg:60.21ms
step:48/2285 train_time:2889ms step_avg:60.19ms
step:49/2285 train_time:2951ms step_avg:60.23ms
step:50/2285 train_time:3011ms step_avg:60.22ms
step:51/2285 train_time:3073ms step_avg:60.25ms
step:52/2285 train_time:3132ms step_avg:60.24ms
step:53/2285 train_time:3194ms step_avg:60.27ms
step:54/2285 train_time:3253ms step_avg:60.24ms
step:55/2285 train_time:3315ms step_avg:60.27ms
step:56/2285 train_time:3373ms step_avg:60.24ms
step:57/2285 train_time:3435ms step_avg:60.26ms
step:58/2285 train_time:3494ms step_avg:60.24ms
step:59/2285 train_time:3555ms step_avg:60.26ms
step:60/2285 train_time:3615ms step_avg:60.25ms
step:61/2285 train_time:3677ms step_avg:60.27ms
step:62/2285 train_time:3736ms step_avg:60.25ms
step:63/2285 train_time:3797ms step_avg:60.27ms
step:64/2285 train_time:3856ms step_avg:60.25ms
step:65/2285 train_time:3918ms step_avg:60.27ms
step:66/2285 train_time:3977ms step_avg:60.26ms
step:67/2285 train_time:4038ms step_avg:60.28ms
step:68/2285 train_time:4098ms step_avg:60.26ms
step:69/2285 train_time:4159ms step_avg:60.28ms
step:70/2285 train_time:4218ms step_avg:60.26ms
step:71/2285 train_time:4279ms step_avg:60.27ms
step:72/2285 train_time:4338ms step_avg:60.25ms
step:73/2285 train_time:4399ms step_avg:60.26ms
step:74/2285 train_time:4458ms step_avg:60.24ms
step:75/2285 train_time:4519ms step_avg:60.25ms
step:76/2285 train_time:4578ms step_avg:60.24ms
step:77/2285 train_time:4641ms step_avg:60.27ms
step:78/2285 train_time:4700ms step_avg:60.25ms
step:79/2285 train_time:4761ms step_avg:60.26ms
step:80/2285 train_time:4820ms step_avg:60.25ms
step:81/2285 train_time:4881ms step_avg:60.26ms
step:82/2285 train_time:4940ms step_avg:60.24ms
step:83/2285 train_time:5002ms step_avg:60.26ms
step:84/2285 train_time:5060ms step_avg:60.24ms
step:85/2285 train_time:5122ms step_avg:60.25ms
step:86/2285 train_time:5180ms step_avg:60.24ms
step:87/2285 train_time:5242ms step_avg:60.25ms
step:88/2285 train_time:5301ms step_avg:60.23ms
step:89/2285 train_time:5362ms step_avg:60.24ms
step:90/2285 train_time:5421ms step_avg:60.23ms
step:91/2285 train_time:5482ms step_avg:60.24ms
step:92/2285 train_time:5541ms step_avg:60.23ms
step:93/2285 train_time:5602ms step_avg:60.24ms
step:94/2285 train_time:5661ms step_avg:60.22ms
step:95/2285 train_time:5722ms step_avg:60.23ms
step:96/2285 train_time:5780ms step_avg:60.21ms
step:97/2285 train_time:5841ms step_avg:60.22ms
step:98/2285 train_time:5900ms step_avg:60.20ms
step:99/2285 train_time:5961ms step_avg:60.22ms
step:100/2285 train_time:6020ms step_avg:60.20ms
step:101/2285 train_time:6081ms step_avg:60.21ms
step:102/2285 train_time:6140ms step_avg:60.19ms
step:103/2285 train_time:6201ms step_avg:60.20ms
step:104/2285 train_time:6259ms step_avg:60.19ms
step:105/2285 train_time:6320ms step_avg:60.19ms
step:106/2285 train_time:6379ms step_avg:60.18ms
step:107/2285 train_time:6441ms step_avg:60.19ms
step:108/2285 train_time:6500ms step_avg:60.18ms
step:109/2285 train_time:6561ms step_avg:60.19ms
step:110/2285 train_time:6619ms step_avg:60.18ms
step:111/2285 train_time:6681ms step_avg:60.19ms
step:112/2285 train_time:6739ms step_avg:60.17ms
step:113/2285 train_time:6800ms step_avg:60.18ms
step:114/2285 train_time:6859ms step_avg:60.16ms
step:115/2285 train_time:6920ms step_avg:60.17ms
step:116/2285 train_time:6979ms step_avg:60.16ms
step:117/2285 train_time:7040ms step_avg:60.17ms
step:118/2285 train_time:7100ms step_avg:60.17ms
step:119/2285 train_time:7161ms step_avg:60.17ms
step:120/2285 train_time:7219ms step_avg:60.16ms
step:121/2285 train_time:7280ms step_avg:60.17ms
step:122/2285 train_time:7339ms step_avg:60.15ms
step:123/2285 train_time:7400ms step_avg:60.16ms
step:124/2285 train_time:7459ms step_avg:60.15ms
step:125/2285 train_time:7519ms step_avg:60.15ms
step:126/2285 train_time:7578ms step_avg:60.14ms
step:127/2285 train_time:7639ms step_avg:60.15ms
step:128/2285 train_time:7698ms step_avg:60.14ms
step:129/2285 train_time:7759ms step_avg:60.15ms
step:130/2285 train_time:7818ms step_avg:60.14ms
step:131/2285 train_time:7879ms step_avg:60.14ms
step:132/2285 train_time:7937ms step_avg:60.13ms
step:133/2285 train_time:7998ms step_avg:60.13ms
step:134/2285 train_time:8057ms step_avg:60.12ms
step:135/2285 train_time:8118ms step_avg:60.13ms
step:136/2285 train_time:8177ms step_avg:60.12ms
step:137/2285 train_time:8238ms step_avg:60.13ms
step:138/2285 train_time:8297ms step_avg:60.13ms
step:139/2285 train_time:8358ms step_avg:60.13ms
step:140/2285 train_time:8417ms step_avg:60.12ms
step:141/2285 train_time:8478ms step_avg:60.13ms
step:142/2285 train_time:8537ms step_avg:60.12ms
step:143/2285 train_time:8598ms step_avg:60.13ms
step:144/2285 train_time:8657ms step_avg:60.12ms
step:145/2285 train_time:8717ms step_avg:60.12ms
step:146/2285 train_time:8776ms step_avg:60.11ms
step:147/2285 train_time:8838ms step_avg:60.12ms
step:148/2285 train_time:8896ms step_avg:60.11ms
step:149/2285 train_time:8958ms step_avg:60.12ms
step:150/2285 train_time:9016ms step_avg:60.11ms
step:151/2285 train_time:9078ms step_avg:60.12ms
step:152/2285 train_time:9136ms step_avg:60.11ms
step:153/2285 train_time:9197ms step_avg:60.11ms
step:154/2285 train_time:9256ms step_avg:60.10ms
step:155/2285 train_time:9317ms step_avg:60.11ms
step:156/2285 train_time:9376ms step_avg:60.10ms
step:157/2285 train_time:9437ms step_avg:60.11ms
step:158/2285 train_time:9496ms step_avg:60.10ms
step:159/2285 train_time:9557ms step_avg:60.11ms
step:160/2285 train_time:9615ms step_avg:60.10ms
step:161/2285 train_time:9677ms step_avg:60.10ms
step:162/2285 train_time:9736ms step_avg:60.10ms
step:163/2285 train_time:9797ms step_avg:60.10ms
step:164/2285 train_time:9855ms step_avg:60.09ms
step:165/2285 train_time:9916ms step_avg:60.10ms
step:166/2285 train_time:9975ms step_avg:60.09ms
step:167/2285 train_time:10036ms step_avg:60.10ms
step:168/2285 train_time:10095ms step_avg:60.09ms
step:169/2285 train_time:10156ms step_avg:60.09ms
step:170/2285 train_time:10215ms step_avg:60.09ms
step:171/2285 train_time:10276ms step_avg:60.09ms
step:172/2285 train_time:10334ms step_avg:60.08ms
step:173/2285 train_time:10395ms step_avg:60.09ms
step:174/2285 train_time:10454ms step_avg:60.08ms
step:175/2285 train_time:10515ms step_avg:60.09ms
step:176/2285 train_time:10574ms step_avg:60.08ms
step:177/2285 train_time:10635ms step_avg:60.09ms
step:178/2285 train_time:10694ms step_avg:60.08ms
step:179/2285 train_time:10755ms step_avg:60.08ms
step:180/2285 train_time:10814ms step_avg:60.08ms
step:181/2285 train_time:10875ms step_avg:60.08ms
step:182/2285 train_time:10934ms step_avg:60.08ms
step:183/2285 train_time:10996ms step_avg:60.09ms
step:184/2285 train_time:11055ms step_avg:60.08ms
step:185/2285 train_time:11116ms step_avg:60.09ms
step:186/2285 train_time:11175ms step_avg:60.08ms
step:187/2285 train_time:11235ms step_avg:60.08ms
step:188/2285 train_time:11294ms step_avg:60.08ms
step:189/2285 train_time:11355ms step_avg:60.08ms
step:190/2285 train_time:11414ms step_avg:60.07ms
step:191/2285 train_time:11474ms step_avg:60.08ms
step:192/2285 train_time:11533ms step_avg:60.07ms
step:193/2285 train_time:11595ms step_avg:60.08ms
step:194/2285 train_time:11654ms step_avg:60.07ms
step:195/2285 train_time:11715ms step_avg:60.08ms
step:196/2285 train_time:11773ms step_avg:60.07ms
step:197/2285 train_time:11834ms step_avg:60.07ms
step:198/2285 train_time:11893ms step_avg:60.07ms
step:199/2285 train_time:11954ms step_avg:60.07ms
step:200/2285 train_time:12013ms step_avg:60.07ms
step:201/2285 train_time:12074ms step_avg:60.07ms
step:202/2285 train_time:12133ms step_avg:60.06ms
step:203/2285 train_time:12195ms step_avg:60.07ms
step:204/2285 train_time:12253ms step_avg:60.06ms
step:205/2285 train_time:12315ms step_avg:60.07ms
step:206/2285 train_time:12373ms step_avg:60.07ms
step:207/2285 train_time:12434ms step_avg:60.07ms
step:208/2285 train_time:12493ms step_avg:60.06ms
step:209/2285 train_time:12555ms step_avg:60.07ms
step:210/2285 train_time:12614ms step_avg:60.06ms
step:211/2285 train_time:12674ms step_avg:60.07ms
step:212/2285 train_time:12733ms step_avg:60.06ms
step:213/2285 train_time:12794ms step_avg:60.07ms
step:214/2285 train_time:12853ms step_avg:60.06ms
step:215/2285 train_time:12914ms step_avg:60.06ms
step:216/2285 train_time:12973ms step_avg:60.06ms
step:217/2285 train_time:13034ms step_avg:60.06ms
step:218/2285 train_time:13094ms step_avg:60.06ms
step:219/2285 train_time:13155ms step_avg:60.07ms
step:220/2285 train_time:13214ms step_avg:60.06ms
step:221/2285 train_time:13275ms step_avg:60.07ms
step:222/2285 train_time:13334ms step_avg:60.06ms
step:223/2285 train_time:13395ms step_avg:60.07ms
step:224/2285 train_time:13454ms step_avg:60.06ms
step:225/2285 train_time:13515ms step_avg:60.07ms
step:226/2285 train_time:13573ms step_avg:60.06ms
step:227/2285 train_time:13634ms step_avg:60.06ms
step:228/2285 train_time:13694ms step_avg:60.06ms
step:229/2285 train_time:13755ms step_avg:60.07ms
step:230/2285 train_time:13814ms step_avg:60.06ms
step:231/2285 train_time:13875ms step_avg:60.07ms
step:232/2285 train_time:13934ms step_avg:60.06ms
step:233/2285 train_time:13994ms step_avg:60.06ms
step:234/2285 train_time:14053ms step_avg:60.05ms
step:235/2285 train_time:14114ms step_avg:60.06ms
step:236/2285 train_time:14172ms step_avg:60.05ms
step:237/2285 train_time:14233ms step_avg:60.06ms
step:238/2285 train_time:14292ms step_avg:60.05ms
step:239/2285 train_time:14353ms step_avg:60.05ms
step:240/2285 train_time:14412ms step_avg:60.05ms
step:241/2285 train_time:14472ms step_avg:60.05ms
step:242/2285 train_time:14531ms step_avg:60.05ms
step:243/2285 train_time:14592ms step_avg:60.05ms
step:244/2285 train_time:14651ms step_avg:60.04ms
step:245/2285 train_time:14711ms step_avg:60.05ms
step:246/2285 train_time:14770ms step_avg:60.04ms
step:247/2285 train_time:14831ms step_avg:60.04ms
step:248/2285 train_time:14890ms step_avg:60.04ms
step:249/2285 train_time:14951ms step_avg:60.04ms
step:250/2285 train_time:15009ms step_avg:60.04ms
step:250/2285 val_loss:4.0723 train_time:15071ms step_avg:60.29ms
step:251/2285 train_time:15089ms step_avg:60.12ms
step:252/2285 train_time:15130ms step_avg:60.04ms
step:253/2285 train_time:15197ms step_avg:60.07ms
step:254/2285 train_time:15259ms step_avg:60.08ms
step:255/2285 train_time:15321ms step_avg:60.08ms
step:256/2285 train_time:15380ms step_avg:60.08ms
step:257/2285 train_time:15440ms step_avg:60.08ms
step:258/2285 train_time:15499ms step_avg:60.07ms
step:259/2285 train_time:15559ms step_avg:60.07ms
step:260/2285 train_time:15618ms step_avg:60.07ms
step:261/2285 train_time:15678ms step_avg:60.07ms
step:262/2285 train_time:15735ms step_avg:60.06ms
step:263/2285 train_time:15796ms step_avg:60.06ms
step:264/2285 train_time:15853ms step_avg:60.05ms
step:265/2285 train_time:15913ms step_avg:60.05ms
step:266/2285 train_time:15971ms step_avg:60.04ms
step:267/2285 train_time:16031ms step_avg:60.04ms
step:268/2285 train_time:16090ms step_avg:60.04ms
step:269/2285 train_time:16152ms step_avg:60.04ms
step:270/2285 train_time:16212ms step_avg:60.04ms
step:271/2285 train_time:16273ms step_avg:60.05ms
step:272/2285 train_time:16333ms step_avg:60.05ms
step:273/2285 train_time:16394ms step_avg:60.05ms
step:274/2285 train_time:16453ms step_avg:60.05ms
step:275/2285 train_time:16514ms step_avg:60.05ms
step:276/2285 train_time:16572ms step_avg:60.04ms
step:277/2285 train_time:16633ms step_avg:60.05ms
step:278/2285 train_time:16692ms step_avg:60.04ms
step:279/2285 train_time:16752ms step_avg:60.04ms
step:280/2285 train_time:16810ms step_avg:60.04ms
step:281/2285 train_time:16870ms step_avg:60.04ms
step:282/2285 train_time:16928ms step_avg:60.03ms
step:283/2285 train_time:16988ms step_avg:60.03ms
step:284/2285 train_time:17046ms step_avg:60.02ms
step:285/2285 train_time:17107ms step_avg:60.03ms
step:286/2285 train_time:17166ms step_avg:60.02ms
step:287/2285 train_time:17228ms step_avg:60.03ms
step:288/2285 train_time:17287ms step_avg:60.02ms
step:289/2285 train_time:17349ms step_avg:60.03ms
step:290/2285 train_time:17407ms step_avg:60.03ms
step:291/2285 train_time:17469ms step_avg:60.03ms
step:292/2285 train_time:17528ms step_avg:60.03ms
step:293/2285 train_time:17589ms step_avg:60.03ms
step:294/2285 train_time:17648ms step_avg:60.03ms
step:295/2285 train_time:17709ms step_avg:60.03ms
step:296/2285 train_time:17767ms step_avg:60.02ms
step:297/2285 train_time:17828ms step_avg:60.03ms
step:298/2285 train_time:17886ms step_avg:60.02ms
step:299/2285 train_time:17946ms step_avg:60.02ms
step:300/2285 train_time:18005ms step_avg:60.02ms
step:301/2285 train_time:18065ms step_avg:60.02ms
step:302/2285 train_time:18124ms step_avg:60.01ms
step:303/2285 train_time:18185ms step_avg:60.02ms
step:304/2285 train_time:18244ms step_avg:60.01ms
step:305/2285 train_time:18306ms step_avg:60.02ms
step:306/2285 train_time:18365ms step_avg:60.02ms
step:307/2285 train_time:18427ms step_avg:60.02ms
step:308/2285 train_time:18486ms step_avg:60.02ms
step:309/2285 train_time:18547ms step_avg:60.02ms
step:310/2285 train_time:18606ms step_avg:60.02ms
step:311/2285 train_time:18666ms step_avg:60.02ms
step:312/2285 train_time:18724ms step_avg:60.01ms
step:313/2285 train_time:18785ms step_avg:60.02ms
step:314/2285 train_time:18843ms step_avg:60.01ms
step:315/2285 train_time:18904ms step_avg:60.01ms
step:316/2285 train_time:18962ms step_avg:60.01ms
step:317/2285 train_time:19023ms step_avg:60.01ms
step:318/2285 train_time:19081ms step_avg:60.00ms
step:319/2285 train_time:19142ms step_avg:60.01ms
step:320/2285 train_time:19200ms step_avg:60.00ms
step:321/2285 train_time:19262ms step_avg:60.01ms
step:322/2285 train_time:19320ms step_avg:60.00ms
step:323/2285 train_time:19381ms step_avg:60.00ms
step:324/2285 train_time:19440ms step_avg:60.00ms
step:325/2285 train_time:19502ms step_avg:60.01ms
step:326/2285 train_time:19560ms step_avg:60.00ms
step:327/2285 train_time:19621ms step_avg:60.00ms
step:328/2285 train_time:19680ms step_avg:60.00ms
step:329/2285 train_time:19741ms step_avg:60.00ms
step:330/2285 train_time:19800ms step_avg:60.00ms
step:331/2285 train_time:19860ms step_avg:60.00ms
step:332/2285 train_time:19918ms step_avg:59.99ms
step:333/2285 train_time:19979ms step_avg:60.00ms
step:334/2285 train_time:20037ms step_avg:59.99ms
step:335/2285 train_time:20097ms step_avg:59.99ms
step:336/2285 train_time:20156ms step_avg:59.99ms
step:337/2285 train_time:20216ms step_avg:59.99ms
step:338/2285 train_time:20275ms step_avg:59.98ms
step:339/2285 train_time:20336ms step_avg:59.99ms
step:340/2285 train_time:20394ms step_avg:59.98ms
step:341/2285 train_time:20456ms step_avg:59.99ms
step:342/2285 train_time:20514ms step_avg:59.98ms
step:343/2285 train_time:20575ms step_avg:59.99ms
step:344/2285 train_time:20634ms step_avg:59.98ms
step:345/2285 train_time:20695ms step_avg:59.98ms
step:346/2285 train_time:20753ms step_avg:59.98ms
step:347/2285 train_time:20814ms step_avg:59.98ms
step:348/2285 train_time:20872ms step_avg:59.98ms
step:349/2285 train_time:20933ms step_avg:59.98ms
step:350/2285 train_time:20991ms step_avg:59.98ms
step:351/2285 train_time:21052ms step_avg:59.98ms
step:352/2285 train_time:21110ms step_avg:59.97ms
step:353/2285 train_time:21171ms step_avg:59.97ms
step:354/2285 train_time:21229ms step_avg:59.97ms
step:355/2285 train_time:21290ms step_avg:59.97ms
step:356/2285 train_time:21349ms step_avg:59.97ms
step:357/2285 train_time:21410ms step_avg:59.97ms
step:358/2285 train_time:21469ms step_avg:59.97ms
step:359/2285 train_time:21529ms step_avg:59.97ms
step:360/2285 train_time:21588ms step_avg:59.97ms
step:361/2285 train_time:21649ms step_avg:59.97ms
step:362/2285 train_time:21707ms step_avg:59.97ms
step:363/2285 train_time:21768ms step_avg:59.97ms
step:364/2285 train_time:21827ms step_avg:59.96ms
step:365/2285 train_time:21888ms step_avg:59.97ms
step:366/2285 train_time:21946ms step_avg:59.96ms
step:367/2285 train_time:22007ms step_avg:59.96ms
step:368/2285 train_time:22065ms step_avg:59.96ms
step:369/2285 train_time:22126ms step_avg:59.96ms
step:370/2285 train_time:22185ms step_avg:59.96ms
step:371/2285 train_time:22247ms step_avg:59.97ms
step:372/2285 train_time:22306ms step_avg:59.96ms
step:373/2285 train_time:22367ms step_avg:59.97ms
step:374/2285 train_time:22426ms step_avg:59.96ms
step:375/2285 train_time:22487ms step_avg:59.96ms
step:376/2285 train_time:22546ms step_avg:59.96ms
step:377/2285 train_time:22607ms step_avg:59.97ms
step:378/2285 train_time:22666ms step_avg:59.96ms
step:379/2285 train_time:22727ms step_avg:59.97ms
step:380/2285 train_time:22786ms step_avg:59.96ms
step:381/2285 train_time:22847ms step_avg:59.97ms
step:382/2285 train_time:22906ms step_avg:59.96ms
step:383/2285 train_time:22967ms step_avg:59.97ms
step:384/2285 train_time:23026ms step_avg:59.96ms
step:385/2285 train_time:23087ms step_avg:59.97ms
step:386/2285 train_time:23146ms step_avg:59.96ms
step:387/2285 train_time:23208ms step_avg:59.97ms
step:388/2285 train_time:23267ms step_avg:59.97ms
step:389/2285 train_time:23328ms step_avg:59.97ms
step:390/2285 train_time:23387ms step_avg:59.97ms
step:391/2285 train_time:23449ms step_avg:59.97ms
step:392/2285 train_time:23508ms step_avg:59.97ms
step:393/2285 train_time:23570ms step_avg:59.97ms
step:394/2285 train_time:23628ms step_avg:59.97ms
step:395/2285 train_time:23690ms step_avg:59.97ms
step:396/2285 train_time:23749ms step_avg:59.97ms
step:397/2285 train_time:23810ms step_avg:59.98ms
step:398/2285 train_time:23869ms step_avg:59.97ms
step:399/2285 train_time:23930ms step_avg:59.98ms
step:400/2285 train_time:23989ms step_avg:59.97ms
step:401/2285 train_time:24051ms step_avg:59.98ms
step:402/2285 train_time:24110ms step_avg:59.98ms
step:403/2285 train_time:24172ms step_avg:59.98ms
step:404/2285 train_time:24231ms step_avg:59.98ms
step:405/2285 train_time:24292ms step_avg:59.98ms
step:406/2285 train_time:24350ms step_avg:59.98ms
step:407/2285 train_time:24411ms step_avg:59.98ms
step:408/2285 train_time:24470ms step_avg:59.98ms
step:409/2285 train_time:24532ms step_avg:59.98ms
step:410/2285 train_time:24592ms step_avg:59.98ms
step:411/2285 train_time:24653ms step_avg:59.98ms
step:412/2285 train_time:24712ms step_avg:59.98ms
step:413/2285 train_time:24773ms step_avg:59.98ms
step:414/2285 train_time:24832ms step_avg:59.98ms
step:415/2285 train_time:24893ms step_avg:59.98ms
step:416/2285 train_time:24952ms step_avg:59.98ms
step:417/2285 train_time:25014ms step_avg:59.98ms
step:418/2285 train_time:25073ms step_avg:59.98ms
step:419/2285 train_time:25133ms step_avg:59.98ms
step:420/2285 train_time:25192ms step_avg:59.98ms
step:421/2285 train_time:25253ms step_avg:59.98ms
step:422/2285 train_time:25313ms step_avg:59.98ms
step:423/2285 train_time:25374ms step_avg:59.99ms
step:424/2285 train_time:25433ms step_avg:59.98ms
step:425/2285 train_time:25494ms step_avg:59.99ms
step:426/2285 train_time:25553ms step_avg:59.98ms
step:427/2285 train_time:25614ms step_avg:59.99ms
step:428/2285 train_time:25673ms step_avg:59.98ms
step:429/2285 train_time:25734ms step_avg:59.99ms
step:430/2285 train_time:25794ms step_avg:59.99ms
step:431/2285 train_time:25856ms step_avg:59.99ms
step:432/2285 train_time:25915ms step_avg:59.99ms
step:433/2285 train_time:25976ms step_avg:59.99ms
step:434/2285 train_time:26035ms step_avg:59.99ms
step:435/2285 train_time:26096ms step_avg:59.99ms
step:436/2285 train_time:26155ms step_avg:59.99ms
step:437/2285 train_time:26217ms step_avg:59.99ms
step:438/2285 train_time:26276ms step_avg:59.99ms
step:439/2285 train_time:26337ms step_avg:59.99ms
step:440/2285 train_time:26396ms step_avg:59.99ms
step:441/2285 train_time:26458ms step_avg:59.99ms
step:442/2285 train_time:26517ms step_avg:59.99ms
step:443/2285 train_time:26578ms step_avg:59.99ms
step:444/2285 train_time:26636ms step_avg:59.99ms
step:445/2285 train_time:26697ms step_avg:59.99ms
step:446/2285 train_time:26756ms step_avg:59.99ms
step:447/2285 train_time:26818ms step_avg:59.99ms
step:448/2285 train_time:26876ms step_avg:59.99ms
step:449/2285 train_time:26937ms step_avg:59.99ms
step:450/2285 train_time:26996ms step_avg:59.99ms
step:451/2285 train_time:27057ms step_avg:59.99ms
step:452/2285 train_time:27117ms step_avg:59.99ms
step:453/2285 train_time:27178ms step_avg:59.99ms
step:454/2285 train_time:27236ms step_avg:59.99ms
step:455/2285 train_time:27297ms step_avg:59.99ms
step:456/2285 train_time:27357ms step_avg:59.99ms
step:457/2285 train_time:27418ms step_avg:60.00ms
step:458/2285 train_time:27477ms step_avg:59.99ms
step:459/2285 train_time:27538ms step_avg:60.00ms
step:460/2285 train_time:27596ms step_avg:59.99ms
step:461/2285 train_time:27658ms step_avg:59.99ms
step:462/2285 train_time:27717ms step_avg:59.99ms
step:463/2285 train_time:27778ms step_avg:60.00ms
step:464/2285 train_time:27837ms step_avg:59.99ms
step:465/2285 train_time:27898ms step_avg:59.99ms
step:466/2285 train_time:27957ms step_avg:59.99ms
step:467/2285 train_time:28018ms step_avg:60.00ms
step:468/2285 train_time:28077ms step_avg:59.99ms
step:469/2285 train_time:28138ms step_avg:60.00ms
step:470/2285 train_time:28197ms step_avg:59.99ms
step:471/2285 train_time:28258ms step_avg:60.00ms
step:472/2285 train_time:28318ms step_avg:59.99ms
step:473/2285 train_time:28378ms step_avg:60.00ms
step:474/2285 train_time:28437ms step_avg:59.99ms
step:475/2285 train_time:28498ms step_avg:60.00ms
step:476/2285 train_time:28557ms step_avg:59.99ms
step:477/2285 train_time:28618ms step_avg:60.00ms
step:478/2285 train_time:28677ms step_avg:59.99ms
step:479/2285 train_time:28738ms step_avg:60.00ms
step:480/2285 train_time:28798ms step_avg:60.00ms
step:481/2285 train_time:28859ms step_avg:60.00ms
step:482/2285 train_time:28918ms step_avg:60.00ms
step:483/2285 train_time:28979ms step_avg:60.00ms
step:484/2285 train_time:29037ms step_avg:59.99ms
step:485/2285 train_time:29099ms step_avg:60.00ms
step:486/2285 train_time:29158ms step_avg:60.00ms
step:487/2285 train_time:29219ms step_avg:60.00ms
step:488/2285 train_time:29277ms step_avg:59.99ms
step:489/2285 train_time:29338ms step_avg:60.00ms
step:490/2285 train_time:29397ms step_avg:59.99ms
step:491/2285 train_time:29458ms step_avg:60.00ms
step:492/2285 train_time:29517ms step_avg:59.99ms
step:493/2285 train_time:29578ms step_avg:60.00ms
step:494/2285 train_time:29637ms step_avg:59.99ms
step:495/2285 train_time:29699ms step_avg:60.00ms
step:496/2285 train_time:29758ms step_avg:60.00ms
step:497/2285 train_time:29819ms step_avg:60.00ms
step:498/2285 train_time:29877ms step_avg:59.99ms
step:499/2285 train_time:29938ms step_avg:60.00ms
step:500/2285 train_time:29997ms step_avg:59.99ms
step:500/2285 val_loss:3.7842 train_time:30060ms step_avg:60.12ms
step:501/2285 train_time:30083ms step_avg:60.05ms
step:502/2285 train_time:30120ms step_avg:60.00ms
step:503/2285 train_time:30180ms step_avg:60.00ms
step:504/2285 train_time:30238ms step_avg:60.00ms
step:505/2285 train_time:30299ms step_avg:60.00ms
step:506/2285 train_time:30358ms step_avg:60.00ms
step:507/2285 train_time:30418ms step_avg:60.00ms
step:508/2285 train_time:30476ms step_avg:59.99ms
step:509/2285 train_time:30537ms step_avg:59.99ms
step:510/2285 train_time:30595ms step_avg:59.99ms
step:511/2285 train_time:30656ms step_avg:59.99ms
step:512/2285 train_time:30714ms step_avg:59.99ms
step:513/2285 train_time:30774ms step_avg:59.99ms
step:514/2285 train_time:30833ms step_avg:59.99ms
step:515/2285 train_time:30893ms step_avg:59.99ms
step:516/2285 train_time:30955ms step_avg:59.99ms
step:517/2285 train_time:31023ms step_avg:60.01ms
step:518/2285 train_time:31085ms step_avg:60.01ms
step:519/2285 train_time:31146ms step_avg:60.01ms
step:520/2285 train_time:31205ms step_avg:60.01ms
step:521/2285 train_time:31266ms step_avg:60.01ms
step:522/2285 train_time:31325ms step_avg:60.01ms
step:523/2285 train_time:31386ms step_avg:60.01ms
step:524/2285 train_time:31445ms step_avg:60.01ms
step:525/2285 train_time:31506ms step_avg:60.01ms
step:526/2285 train_time:31565ms step_avg:60.01ms
step:527/2285 train_time:31627ms step_avg:60.01ms
step:528/2285 train_time:31686ms step_avg:60.01ms
step:529/2285 train_time:31747ms step_avg:60.01ms
step:530/2285 train_time:31806ms step_avg:60.01ms
step:531/2285 train_time:31867ms step_avg:60.01ms
step:532/2285 train_time:31927ms step_avg:60.01ms
step:533/2285 train_time:31990ms step_avg:60.02ms
step:534/2285 train_time:32049ms step_avg:60.02ms
step:535/2285 train_time:32111ms step_avg:60.02ms
step:536/2285 train_time:32171ms step_avg:60.02ms
step:537/2285 train_time:32233ms step_avg:60.02ms
step:538/2285 train_time:32292ms step_avg:60.02ms
step:539/2285 train_time:32353ms step_avg:60.02ms
step:540/2285 train_time:32412ms step_avg:60.02ms
step:541/2285 train_time:32473ms step_avg:60.02ms
step:542/2285 train_time:32533ms step_avg:60.02ms
step:543/2285 train_time:32594ms step_avg:60.03ms
step:544/2285 train_time:32653ms step_avg:60.02ms
step:545/2285 train_time:32714ms step_avg:60.03ms
step:546/2285 train_time:32773ms step_avg:60.02ms
step:547/2285 train_time:32834ms step_avg:60.03ms
step:548/2285 train_time:32893ms step_avg:60.02ms
step:549/2285 train_time:32955ms step_avg:60.03ms
step:550/2285 train_time:33014ms step_avg:60.02ms
step:551/2285 train_time:33076ms step_avg:60.03ms
step:552/2285 train_time:33135ms step_avg:60.03ms
step:553/2285 train_time:33197ms step_avg:60.03ms
step:554/2285 train_time:33257ms step_avg:60.03ms
step:555/2285 train_time:33319ms step_avg:60.03ms
step:556/2285 train_time:33378ms step_avg:60.03ms
step:557/2285 train_time:33439ms step_avg:60.03ms
step:558/2285 train_time:33498ms step_avg:60.03ms
step:559/2285 train_time:33560ms step_avg:60.04ms
step:560/2285 train_time:33619ms step_avg:60.03ms
step:561/2285 train_time:33679ms step_avg:60.03ms
step:562/2285 train_time:33739ms step_avg:60.03ms
step:563/2285 train_time:33800ms step_avg:60.04ms
step:564/2285 train_time:33859ms step_avg:60.03ms
step:565/2285 train_time:33920ms step_avg:60.04ms
step:566/2285 train_time:33980ms step_avg:60.04ms
step:567/2285 train_time:34042ms step_avg:60.04ms
step:568/2285 train_time:34101ms step_avg:60.04ms
step:569/2285 train_time:34163ms step_avg:60.04ms
step:570/2285 train_time:34222ms step_avg:60.04ms
step:571/2285 train_time:34284ms step_avg:60.04ms
step:572/2285 train_time:34342ms step_avg:60.04ms
step:573/2285 train_time:34404ms step_avg:60.04ms
step:574/2285 train_time:34463ms step_avg:60.04ms
step:575/2285 train_time:34525ms step_avg:60.04ms
step:576/2285 train_time:34584ms step_avg:60.04ms
step:577/2285 train_time:34645ms step_avg:60.04ms
step:578/2285 train_time:34704ms step_avg:60.04ms
step:579/2285 train_time:34765ms step_avg:60.04ms
step:580/2285 train_time:34824ms step_avg:60.04ms
step:581/2285 train_time:34886ms step_avg:60.04ms
step:582/2285 train_time:34945ms step_avg:60.04ms
step:583/2285 train_time:35006ms step_avg:60.04ms
step:584/2285 train_time:35065ms step_avg:60.04ms
step:585/2285 train_time:35126ms step_avg:60.04ms
step:586/2285 train_time:35185ms step_avg:60.04ms
step:587/2285 train_time:35246ms step_avg:60.04ms
step:588/2285 train_time:35305ms step_avg:60.04ms
step:589/2285 train_time:35367ms step_avg:60.05ms
step:590/2285 train_time:35426ms step_avg:60.04ms
step:591/2285 train_time:35487ms step_avg:60.05ms
step:592/2285 train_time:35546ms step_avg:60.04ms
step:593/2285 train_time:35607ms step_avg:60.05ms
step:594/2285 train_time:35667ms step_avg:60.04ms
step:595/2285 train_time:35728ms step_avg:60.05ms
step:596/2285 train_time:35787ms step_avg:60.05ms
step:597/2285 train_time:35849ms step_avg:60.05ms
step:598/2285 train_time:35908ms step_avg:60.05ms
step:599/2285 train_time:35970ms step_avg:60.05ms
step:600/2285 train_time:36029ms step_avg:60.05ms
step:601/2285 train_time:36091ms step_avg:60.05ms
step:602/2285 train_time:36150ms step_avg:60.05ms
step:603/2285 train_time:36211ms step_avg:60.05ms
step:604/2285 train_time:36270ms step_avg:60.05ms
step:605/2285 train_time:36331ms step_avg:60.05ms
step:606/2285 train_time:36391ms step_avg:60.05ms
step:607/2285 train_time:36452ms step_avg:60.05ms
step:608/2285 train_time:36511ms step_avg:60.05ms
step:609/2285 train_time:36572ms step_avg:60.05ms
step:610/2285 train_time:36631ms step_avg:60.05ms
step:611/2285 train_time:36692ms step_avg:60.05ms
step:612/2285 train_time:36751ms step_avg:60.05ms
step:613/2285 train_time:36813ms step_avg:60.05ms
step:614/2285 train_time:36871ms step_avg:60.05ms
step:615/2285 train_time:36933ms step_avg:60.05ms
step:616/2285 train_time:36992ms step_avg:60.05ms
step:617/2285 train_time:37054ms step_avg:60.05ms
step:618/2285 train_time:37113ms step_avg:60.05ms
step:619/2285 train_time:37175ms step_avg:60.06ms
step:620/2285 train_time:37234ms step_avg:60.06ms
step:621/2285 train_time:37296ms step_avg:60.06ms
step:622/2285 train_time:37355ms step_avg:60.06ms
step:623/2285 train_time:37416ms step_avg:60.06ms
step:624/2285 train_time:37476ms step_avg:60.06ms
step:625/2285 train_time:37538ms step_avg:60.06ms
step:626/2285 train_time:37597ms step_avg:60.06ms
step:627/2285 train_time:37659ms step_avg:60.06ms
step:628/2285 train_time:37718ms step_avg:60.06ms
step:629/2285 train_time:37780ms step_avg:60.06ms
step:630/2285 train_time:37839ms step_avg:60.06ms
step:631/2285 train_time:37901ms step_avg:60.07ms
step:632/2285 train_time:37960ms step_avg:60.06ms
step:633/2285 train_time:38022ms step_avg:60.07ms
step:634/2285 train_time:38081ms step_avg:60.06ms
step:635/2285 train_time:38143ms step_avg:60.07ms
step:636/2285 train_time:38202ms step_avg:60.07ms
step:637/2285 train_time:38263ms step_avg:60.07ms
step:638/2285 train_time:38322ms step_avg:60.07ms
step:639/2285 train_time:38384ms step_avg:60.07ms
step:640/2285 train_time:38444ms step_avg:60.07ms
step:641/2285 train_time:38505ms step_avg:60.07ms
step:642/2285 train_time:38564ms step_avg:60.07ms
step:643/2285 train_time:38626ms step_avg:60.07ms
step:644/2285 train_time:38685ms step_avg:60.07ms
step:645/2285 train_time:38747ms step_avg:60.07ms
step:646/2285 train_time:38806ms step_avg:60.07ms
step:647/2285 train_time:38867ms step_avg:60.07ms
step:648/2285 train_time:38926ms step_avg:60.07ms
step:649/2285 train_time:38988ms step_avg:60.07ms
step:650/2285 train_time:39047ms step_avg:60.07ms
step:651/2285 train_time:39108ms step_avg:60.07ms
step:652/2285 train_time:39167ms step_avg:60.07ms
step:653/2285 train_time:39229ms step_avg:60.07ms
step:654/2285 train_time:39288ms step_avg:60.07ms
step:655/2285 train_time:39350ms step_avg:60.08ms
step:656/2285 train_time:39409ms step_avg:60.07ms
step:657/2285 train_time:39470ms step_avg:60.08ms
step:658/2285 train_time:39529ms step_avg:60.07ms
step:659/2285 train_time:39591ms step_avg:60.08ms
step:660/2285 train_time:39650ms step_avg:60.08ms
step:661/2285 train_time:39712ms step_avg:60.08ms
step:662/2285 train_time:39770ms step_avg:60.08ms
step:663/2285 train_time:39832ms step_avg:60.08ms
step:664/2285 train_time:39891ms step_avg:60.08ms
step:665/2285 train_time:39952ms step_avg:60.08ms
step:666/2285 train_time:40011ms step_avg:60.08ms
step:667/2285 train_time:40072ms step_avg:60.08ms
step:668/2285 train_time:40131ms step_avg:60.08ms
step:669/2285 train_time:40193ms step_avg:60.08ms
step:670/2285 train_time:40252ms step_avg:60.08ms
step:671/2285 train_time:40314ms step_avg:60.08ms
step:672/2285 train_time:40373ms step_avg:60.08ms
step:673/2285 train_time:40434ms step_avg:60.08ms
step:674/2285 train_time:40493ms step_avg:60.08ms
step:675/2285 train_time:40555ms step_avg:60.08ms
step:676/2285 train_time:40614ms step_avg:60.08ms
step:677/2285 train_time:40675ms step_avg:60.08ms
step:678/2285 train_time:40734ms step_avg:60.08ms
step:679/2285 train_time:40796ms step_avg:60.08ms
step:680/2285 train_time:40855ms step_avg:60.08ms
step:681/2285 train_time:40916ms step_avg:60.08ms
step:682/2285 train_time:40975ms step_avg:60.08ms
step:683/2285 train_time:41037ms step_avg:60.08ms
step:684/2285 train_time:41097ms step_avg:60.08ms
step:685/2285 train_time:41159ms step_avg:60.09ms
step:686/2285 train_time:41218ms step_avg:60.08ms
step:687/2285 train_time:41280ms step_avg:60.09ms
step:688/2285 train_time:41339ms step_avg:60.09ms
step:689/2285 train_time:41400ms step_avg:60.09ms
step:690/2285 train_time:41460ms step_avg:60.09ms
step:691/2285 train_time:41521ms step_avg:60.09ms
step:692/2285 train_time:41580ms step_avg:60.09ms
step:693/2285 train_time:41642ms step_avg:60.09ms
step:694/2285 train_time:41701ms step_avg:60.09ms
step:695/2285 train_time:41762ms step_avg:60.09ms
step:696/2285 train_time:41822ms step_avg:60.09ms
step:697/2285 train_time:41883ms step_avg:60.09ms
step:698/2285 train_time:41942ms step_avg:60.09ms
step:699/2285 train_time:42004ms step_avg:60.09ms
step:700/2285 train_time:42063ms step_avg:60.09ms
step:701/2285 train_time:42124ms step_avg:60.09ms
step:702/2285 train_time:42183ms step_avg:60.09ms
step:703/2285 train_time:42245ms step_avg:60.09ms
step:704/2285 train_time:42304ms step_avg:60.09ms
step:705/2285 train_time:42365ms step_avg:60.09ms
step:706/2285 train_time:42424ms step_avg:60.09ms
step:707/2285 train_time:42486ms step_avg:60.09ms
step:708/2285 train_time:42545ms step_avg:60.09ms
step:709/2285 train_time:42607ms step_avg:60.09ms
step:710/2285 train_time:42667ms step_avg:60.09ms
step:711/2285 train_time:42728ms step_avg:60.10ms
step:712/2285 train_time:42787ms step_avg:60.09ms
step:713/2285 train_time:42849ms step_avg:60.10ms
step:714/2285 train_time:42908ms step_avg:60.10ms
step:715/2285 train_time:42969ms step_avg:60.10ms
step:716/2285 train_time:43028ms step_avg:60.10ms
step:717/2285 train_time:43090ms step_avg:60.10ms
step:718/2285 train_time:43149ms step_avg:60.10ms
step:719/2285 train_time:43210ms step_avg:60.10ms
step:720/2285 train_time:43270ms step_avg:60.10ms
step:721/2285 train_time:43331ms step_avg:60.10ms
step:722/2285 train_time:43391ms step_avg:60.10ms
step:723/2285 train_time:43453ms step_avg:60.10ms
step:724/2285 train_time:43512ms step_avg:60.10ms
step:725/2285 train_time:43574ms step_avg:60.10ms
step:726/2285 train_time:43633ms step_avg:60.10ms
step:727/2285 train_time:43695ms step_avg:60.10ms
step:728/2285 train_time:43753ms step_avg:60.10ms
step:729/2285 train_time:43815ms step_avg:60.10ms
step:730/2285 train_time:43874ms step_avg:60.10ms
step:731/2285 train_time:43936ms step_avg:60.10ms
step:732/2285 train_time:43995ms step_avg:60.10ms
step:733/2285 train_time:44057ms step_avg:60.10ms
step:734/2285 train_time:44116ms step_avg:60.10ms
step:735/2285 train_time:44178ms step_avg:60.11ms
step:736/2285 train_time:44237ms step_avg:60.10ms
step:737/2285 train_time:44299ms step_avg:60.11ms
step:738/2285 train_time:44358ms step_avg:60.11ms
step:739/2285 train_time:44420ms step_avg:60.11ms
step:740/2285 train_time:44479ms step_avg:60.11ms
step:741/2285 train_time:44540ms step_avg:60.11ms
step:742/2285 train_time:44599ms step_avg:60.11ms
step:743/2285 train_time:44661ms step_avg:60.11ms
step:744/2285 train_time:44720ms step_avg:60.11ms
step:745/2285 train_time:44782ms step_avg:60.11ms
step:746/2285 train_time:44841ms step_avg:60.11ms
step:747/2285 train_time:44902ms step_avg:60.11ms
step:748/2285 train_time:44961ms step_avg:60.11ms
step:749/2285 train_time:45023ms step_avg:60.11ms
step:750/2285 train_time:45082ms step_avg:60.11ms
step:750/2285 val_loss:3.6546 train_time:45145ms step_avg:60.19ms
step:751/2285 train_time:45163ms step_avg:60.14ms
step:752/2285 train_time:45206ms step_avg:60.11ms
step:753/2285 train_time:45270ms step_avg:60.12ms
step:754/2285 train_time:45331ms step_avg:60.12ms
step:755/2285 train_time:45393ms step_avg:60.12ms
step:756/2285 train_time:45452ms step_avg:60.12ms
step:757/2285 train_time:45512ms step_avg:60.12ms
step:758/2285 train_time:45571ms step_avg:60.12ms
step:759/2285 train_time:45631ms step_avg:60.12ms
step:760/2285 train_time:45690ms step_avg:60.12ms
step:761/2285 train_time:45750ms step_avg:60.12ms
step:762/2285 train_time:45808ms step_avg:60.12ms
step:763/2285 train_time:45869ms step_avg:60.12ms
step:764/2285 train_time:45929ms step_avg:60.12ms
step:765/2285 train_time:45989ms step_avg:60.12ms
step:766/2285 train_time:46049ms step_avg:60.12ms
step:767/2285 train_time:46113ms step_avg:60.12ms
step:768/2285 train_time:46174ms step_avg:60.12ms
step:769/2285 train_time:46238ms step_avg:60.13ms
step:770/2285 train_time:46297ms step_avg:60.13ms
step:771/2285 train_time:46359ms step_avg:60.13ms
step:772/2285 train_time:46419ms step_avg:60.13ms
step:773/2285 train_time:46481ms step_avg:60.13ms
step:774/2285 train_time:46540ms step_avg:60.13ms
step:775/2285 train_time:46602ms step_avg:60.13ms
step:776/2285 train_time:46661ms step_avg:60.13ms
step:777/2285 train_time:46723ms step_avg:60.13ms
step:778/2285 train_time:46783ms step_avg:60.13ms
step:779/2285 train_time:46844ms step_avg:60.13ms
step:780/2285 train_time:46903ms step_avg:60.13ms
step:781/2285 train_time:46965ms step_avg:60.13ms
step:782/2285 train_time:47025ms step_avg:60.13ms
step:783/2285 train_time:47087ms step_avg:60.14ms
step:784/2285 train_time:47147ms step_avg:60.14ms
step:785/2285 train_time:47209ms step_avg:60.14ms
step:786/2285 train_time:47268ms step_avg:60.14ms
step:787/2285 train_time:47331ms step_avg:60.14ms
step:788/2285 train_time:47391ms step_avg:60.14ms
step:789/2285 train_time:47453ms step_avg:60.14ms
step:790/2285 train_time:47513ms step_avg:60.14ms
step:791/2285 train_time:47575ms step_avg:60.15ms
step:792/2285 train_time:47634ms step_avg:60.14ms
step:793/2285 train_time:47696ms step_avg:60.15ms
step:794/2285 train_time:47755ms step_avg:60.15ms
step:795/2285 train_time:47816ms step_avg:60.15ms
step:796/2285 train_time:47876ms step_avg:60.15ms
step:797/2285 train_time:47938ms step_avg:60.15ms
step:798/2285 train_time:47997ms step_avg:60.15ms
step:799/2285 train_time:48059ms step_avg:60.15ms
step:800/2285 train_time:48119ms step_avg:60.15ms
step:801/2285 train_time:48181ms step_avg:60.15ms
step:802/2285 train_time:48241ms step_avg:60.15ms
step:803/2285 train_time:48304ms step_avg:60.15ms
step:804/2285 train_time:48364ms step_avg:60.15ms
step:805/2285 train_time:48427ms step_avg:60.16ms
step:806/2285 train_time:48487ms step_avg:60.16ms
step:807/2285 train_time:48549ms step_avg:60.16ms
step:808/2285 train_time:48608ms step_avg:60.16ms
step:809/2285 train_time:48669ms step_avg:60.16ms
step:810/2285 train_time:48728ms step_avg:60.16ms
step:811/2285 train_time:48790ms step_avg:60.16ms
step:812/2285 train_time:48850ms step_avg:60.16ms
step:813/2285 train_time:48912ms step_avg:60.16ms
step:814/2285 train_time:48972ms step_avg:60.16ms
step:815/2285 train_time:49033ms step_avg:60.16ms
step:816/2285 train_time:49093ms step_avg:60.16ms
step:817/2285 train_time:49155ms step_avg:60.16ms
step:818/2285 train_time:49214ms step_avg:60.16ms
step:819/2285 train_time:49276ms step_avg:60.17ms
step:820/2285 train_time:49336ms step_avg:60.17ms
step:821/2285 train_time:49399ms step_avg:60.17ms
step:822/2285 train_time:49458ms step_avg:60.17ms
step:823/2285 train_time:49520ms step_avg:60.17ms
step:824/2285 train_time:49579ms step_avg:60.17ms
step:825/2285 train_time:49642ms step_avg:60.17ms
step:826/2285 train_time:49701ms step_avg:60.17ms
step:827/2285 train_time:49763ms step_avg:60.17ms
step:828/2285 train_time:49823ms step_avg:60.17ms
step:829/2285 train_time:49885ms step_avg:60.17ms
step:830/2285 train_time:49944ms step_avg:60.17ms
step:831/2285 train_time:50006ms step_avg:60.18ms
step:832/2285 train_time:50066ms step_avg:60.17ms
step:833/2285 train_time:50127ms step_avg:60.18ms
step:834/2285 train_time:50187ms step_avg:60.18ms
step:835/2285 train_time:50249ms step_avg:60.18ms
step:836/2285 train_time:50309ms step_avg:60.18ms
step:837/2285 train_time:50371ms step_avg:60.18ms
step:838/2285 train_time:50432ms step_avg:60.18ms
step:839/2285 train_time:50494ms step_avg:60.18ms
step:840/2285 train_time:50553ms step_avg:60.18ms
step:841/2285 train_time:50614ms step_avg:60.18ms
step:842/2285 train_time:50674ms step_avg:60.18ms
step:843/2285 train_time:50736ms step_avg:60.19ms
step:844/2285 train_time:50795ms step_avg:60.18ms
step:845/2285 train_time:50857ms step_avg:60.19ms
step:846/2285 train_time:50916ms step_avg:60.18ms
step:847/2285 train_time:50978ms step_avg:60.19ms
step:848/2285 train_time:51037ms step_avg:60.18ms
step:849/2285 train_time:51098ms step_avg:60.19ms
step:850/2285 train_time:51158ms step_avg:60.19ms
step:851/2285 train_time:51220ms step_avg:60.19ms
step:852/2285 train_time:51280ms step_avg:60.19ms
step:853/2285 train_time:51343ms step_avg:60.19ms
step:854/2285 train_time:51403ms step_avg:60.19ms
step:855/2285 train_time:51466ms step_avg:60.19ms
step:856/2285 train_time:51525ms step_avg:60.19ms
step:857/2285 train_time:51587ms step_avg:60.19ms
step:858/2285 train_time:51646ms step_avg:60.19ms
step:859/2285 train_time:51708ms step_avg:60.20ms
step:860/2285 train_time:51767ms step_avg:60.19ms
step:861/2285 train_time:51829ms step_avg:60.20ms
step:862/2285 train_time:51889ms step_avg:60.20ms
step:863/2285 train_time:51951ms step_avg:60.20ms
step:864/2285 train_time:52010ms step_avg:60.20ms
step:865/2285 train_time:52072ms step_avg:60.20ms
step:866/2285 train_time:52132ms step_avg:60.20ms
step:867/2285 train_time:52194ms step_avg:60.20ms
step:868/2285 train_time:52253ms step_avg:60.20ms
step:869/2285 train_time:52315ms step_avg:60.20ms
step:870/2285 train_time:52375ms step_avg:60.20ms
step:871/2285 train_time:52438ms step_avg:60.20ms
step:872/2285 train_time:52497ms step_avg:60.20ms
step:873/2285 train_time:52558ms step_avg:60.20ms
step:874/2285 train_time:52617ms step_avg:60.20ms
step:875/2285 train_time:52679ms step_avg:60.21ms
step:876/2285 train_time:52739ms step_avg:60.20ms
step:877/2285 train_time:52801ms step_avg:60.21ms
step:878/2285 train_time:52861ms step_avg:60.21ms
step:879/2285 train_time:52923ms step_avg:60.21ms
step:880/2285 train_time:52983ms step_avg:60.21ms
step:881/2285 train_time:53045ms step_avg:60.21ms
step:882/2285 train_time:53104ms step_avg:60.21ms
step:883/2285 train_time:53166ms step_avg:60.21ms
step:884/2285 train_time:53226ms step_avg:60.21ms
step:885/2285 train_time:53288ms step_avg:60.21ms
step:886/2285 train_time:53347ms step_avg:60.21ms
step:887/2285 train_time:53409ms step_avg:60.21ms
step:888/2285 train_time:53469ms step_avg:60.21ms
step:889/2285 train_time:53531ms step_avg:60.21ms
step:890/2285 train_time:53591ms step_avg:60.21ms
step:891/2285 train_time:53652ms step_avg:60.22ms
step:892/2285 train_time:53712ms step_avg:60.22ms
step:893/2285 train_time:53774ms step_avg:60.22ms
step:894/2285 train_time:53834ms step_avg:60.22ms
step:895/2285 train_time:53895ms step_avg:60.22ms
step:896/2285 train_time:53955ms step_avg:60.22ms
step:897/2285 train_time:54016ms step_avg:60.22ms
step:898/2285 train_time:54075ms step_avg:60.22ms
step:899/2285 train_time:54137ms step_avg:60.22ms
step:900/2285 train_time:54196ms step_avg:60.22ms
step:901/2285 train_time:54258ms step_avg:60.22ms
step:902/2285 train_time:54317ms step_avg:60.22ms
step:903/2285 train_time:54379ms step_avg:60.22ms
step:904/2285 train_time:54439ms step_avg:60.22ms
step:905/2285 train_time:54501ms step_avg:60.22ms
step:906/2285 train_time:54560ms step_avg:60.22ms
step:907/2285 train_time:54622ms step_avg:60.22ms
step:908/2285 train_time:54682ms step_avg:60.22ms
step:909/2285 train_time:54744ms step_avg:60.22ms
step:910/2285 train_time:54804ms step_avg:60.22ms
step:911/2285 train_time:54866ms step_avg:60.23ms
step:912/2285 train_time:54925ms step_avg:60.23ms
step:913/2285 train_time:54987ms step_avg:60.23ms
step:914/2285 train_time:55046ms step_avg:60.23ms
step:915/2285 train_time:55108ms step_avg:60.23ms
step:916/2285 train_time:55167ms step_avg:60.23ms
step:917/2285 train_time:55229ms step_avg:60.23ms
step:918/2285 train_time:55288ms step_avg:60.23ms
step:919/2285 train_time:55351ms step_avg:60.23ms
step:920/2285 train_time:55410ms step_avg:60.23ms
step:921/2285 train_time:55472ms step_avg:60.23ms
step:922/2285 train_time:55532ms step_avg:60.23ms
step:923/2285 train_time:55593ms step_avg:60.23ms
step:924/2285 train_time:55653ms step_avg:60.23ms
step:925/2285 train_time:55715ms step_avg:60.23ms
step:926/2285 train_time:55775ms step_avg:60.23ms
step:927/2285 train_time:55837ms step_avg:60.23ms
step:928/2285 train_time:55897ms step_avg:60.23ms
step:929/2285 train_time:55958ms step_avg:60.23ms
step:930/2285 train_time:56017ms step_avg:60.23ms
step:931/2285 train_time:56079ms step_avg:60.23ms
step:932/2285 train_time:56138ms step_avg:60.23ms
step:933/2285 train_time:56200ms step_avg:60.24ms
step:934/2285 train_time:56259ms step_avg:60.23ms
step:935/2285 train_time:56322ms step_avg:60.24ms
step:936/2285 train_time:56382ms step_avg:60.24ms
step:937/2285 train_time:56444ms step_avg:60.24ms
step:938/2285 train_time:56504ms step_avg:60.24ms
step:939/2285 train_time:56566ms step_avg:60.24ms
step:940/2285 train_time:56625ms step_avg:60.24ms
step:941/2285 train_time:56688ms step_avg:60.24ms
step:942/2285 train_time:56747ms step_avg:60.24ms
step:943/2285 train_time:56808ms step_avg:60.24ms
step:944/2285 train_time:56868ms step_avg:60.24ms
step:945/2285 train_time:56931ms step_avg:60.24ms
step:946/2285 train_time:56991ms step_avg:60.24ms
step:947/2285 train_time:57052ms step_avg:60.25ms
step:948/2285 train_time:57112ms step_avg:60.24ms
step:949/2285 train_time:57174ms step_avg:60.25ms
step:950/2285 train_time:57234ms step_avg:60.25ms
step:951/2285 train_time:57295ms step_avg:60.25ms
step:952/2285 train_time:57355ms step_avg:60.25ms
step:953/2285 train_time:57417ms step_avg:60.25ms
step:954/2285 train_time:57476ms step_avg:60.25ms
step:955/2285 train_time:57538ms step_avg:60.25ms
step:956/2285 train_time:57597ms step_avg:60.25ms
step:957/2285 train_time:57659ms step_avg:60.25ms
step:958/2285 train_time:57718ms step_avg:60.25ms
step:959/2285 train_time:57781ms step_avg:60.25ms
step:960/2285 train_time:57841ms step_avg:60.25ms
step:961/2285 train_time:57902ms step_avg:60.25ms
step:962/2285 train_time:57962ms step_avg:60.25ms
step:963/2285 train_time:58025ms step_avg:60.25ms
step:964/2285 train_time:58085ms step_avg:60.25ms
step:965/2285 train_time:58147ms step_avg:60.26ms
step:966/2285 train_time:58206ms step_avg:60.25ms
step:967/2285 train_time:58268ms step_avg:60.26ms
step:968/2285 train_time:58327ms step_avg:60.26ms
step:969/2285 train_time:58390ms step_avg:60.26ms
step:970/2285 train_time:58449ms step_avg:60.26ms
step:971/2285 train_time:58511ms step_avg:60.26ms
step:972/2285 train_time:58571ms step_avg:60.26ms
step:973/2285 train_time:58633ms step_avg:60.26ms
step:974/2285 train_time:58693ms step_avg:60.26ms
step:975/2285 train_time:58755ms step_avg:60.26ms
step:976/2285 train_time:58815ms step_avg:60.26ms
step:977/2285 train_time:58877ms step_avg:60.26ms
step:978/2285 train_time:58936ms step_avg:60.26ms
step:979/2285 train_time:58997ms step_avg:60.26ms
step:980/2285 train_time:59057ms step_avg:60.26ms
step:981/2285 train_time:59119ms step_avg:60.26ms
step:982/2285 train_time:59179ms step_avg:60.26ms
step:983/2285 train_time:59241ms step_avg:60.27ms
step:984/2285 train_time:59301ms step_avg:60.27ms
step:985/2285 train_time:59363ms step_avg:60.27ms
step:986/2285 train_time:59423ms step_avg:60.27ms
step:987/2285 train_time:59485ms step_avg:60.27ms
step:988/2285 train_time:59545ms step_avg:60.27ms
step:989/2285 train_time:59607ms step_avg:60.27ms
step:990/2285 train_time:59666ms step_avg:60.27ms
step:991/2285 train_time:59728ms step_avg:60.27ms
step:992/2285 train_time:59787ms step_avg:60.27ms
step:993/2285 train_time:59849ms step_avg:60.27ms
step:994/2285 train_time:59909ms step_avg:60.27ms
step:995/2285 train_time:59971ms step_avg:60.27ms
step:996/2285 train_time:60031ms step_avg:60.27ms
step:997/2285 train_time:60093ms step_avg:60.27ms
step:998/2285 train_time:60152ms step_avg:60.27ms
step:999/2285 train_time:60214ms step_avg:60.27ms
step:1000/2285 train_time:60274ms step_avg:60.27ms
step:1000/2285 val_loss:3.5659 train_time:60338ms step_avg:60.34ms
step:1001/2285 train_time:60357ms step_avg:60.30ms
step:1002/2285 train_time:60400ms step_avg:60.28ms
step:1003/2285 train_time:60461ms step_avg:60.28ms
step:1004/2285 train_time:60521ms step_avg:60.28ms
step:1005/2285 train_time:60583ms step_avg:60.28ms
step:1006/2285 train_time:60643ms step_avg:60.28ms
step:1007/2285 train_time:60704ms step_avg:60.28ms
step:1008/2285 train_time:60763ms step_avg:60.28ms
step:1009/2285 train_time:60824ms step_avg:60.28ms
step:1010/2285 train_time:60883ms step_avg:60.28ms
step:1011/2285 train_time:60944ms step_avg:60.28ms
step:1012/2285 train_time:61003ms step_avg:60.28ms
step:1013/2285 train_time:61064ms step_avg:60.28ms
step:1014/2285 train_time:61123ms step_avg:60.28ms
step:1015/2285 train_time:61184ms step_avg:60.28ms
step:1016/2285 train_time:61243ms step_avg:60.28ms
step:1017/2285 train_time:61309ms step_avg:60.28ms
step:1018/2285 train_time:61371ms step_avg:60.29ms
step:1019/2285 train_time:61433ms step_avg:60.29ms
step:1020/2285 train_time:61492ms step_avg:60.29ms
step:1021/2285 train_time:61554ms step_avg:60.29ms
step:1022/2285 train_time:61614ms step_avg:60.29ms
step:1023/2285 train_time:61676ms step_avg:60.29ms
step:1024/2285 train_time:61735ms step_avg:60.29ms
step:1025/2285 train_time:61797ms step_avg:60.29ms
step:1026/2285 train_time:61857ms step_avg:60.29ms
step:1027/2285 train_time:61918ms step_avg:60.29ms
step:1028/2285 train_time:61977ms step_avg:60.29ms
step:1029/2285 train_time:62038ms step_avg:60.29ms
step:1030/2285 train_time:62097ms step_avg:60.29ms
step:1031/2285 train_time:62159ms step_avg:60.29ms
step:1032/2285 train_time:62219ms step_avg:60.29ms
step:1033/2285 train_time:62281ms step_avg:60.29ms
step:1034/2285 train_time:62341ms step_avg:60.29ms
step:1035/2285 train_time:62403ms step_avg:60.29ms
step:1036/2285 train_time:62462ms step_avg:60.29ms
step:1037/2285 train_time:62524ms step_avg:60.29ms
step:1038/2285 train_time:62585ms step_avg:60.29ms
step:1039/2285 train_time:62647ms step_avg:60.30ms
step:1040/2285 train_time:62706ms step_avg:60.29ms
step:1041/2285 train_time:62768ms step_avg:60.30ms
step:1042/2285 train_time:62828ms step_avg:60.30ms
step:1043/2285 train_time:62891ms step_avg:60.30ms
step:1044/2285 train_time:62950ms step_avg:60.30ms
step:1045/2285 train_time:63012ms step_avg:60.30ms
step:1046/2285 train_time:63071ms step_avg:60.30ms
step:1047/2285 train_time:63133ms step_avg:60.30ms
step:1048/2285 train_time:63192ms step_avg:60.30ms
step:1049/2285 train_time:63254ms step_avg:60.30ms
step:1050/2285 train_time:63314ms step_avg:60.30ms
step:1051/2285 train_time:63376ms step_avg:60.30ms
step:1052/2285 train_time:63435ms step_avg:60.30ms
step:1053/2285 train_time:63497ms step_avg:60.30ms
step:1054/2285 train_time:63557ms step_avg:60.30ms
step:1055/2285 train_time:63619ms step_avg:60.30ms
step:1056/2285 train_time:63678ms step_avg:60.30ms
step:1057/2285 train_time:63740ms step_avg:60.30ms
step:1058/2285 train_time:63799ms step_avg:60.30ms
step:1059/2285 train_time:63860ms step_avg:60.30ms
step:1060/2285 train_time:63920ms step_avg:60.30ms
step:1061/2285 train_time:63982ms step_avg:60.30ms
step:1062/2285 train_time:64041ms step_avg:60.30ms
step:1063/2285 train_time:64102ms step_avg:60.30ms
step:1064/2285 train_time:64162ms step_avg:60.30ms
step:1065/2285 train_time:64224ms step_avg:60.30ms
step:1066/2285 train_time:64284ms step_avg:60.30ms
step:1067/2285 train_time:64345ms step_avg:60.30ms
step:1068/2285 train_time:64405ms step_avg:60.30ms
step:1069/2285 train_time:64467ms step_avg:60.31ms
step:1070/2285 train_time:64527ms step_avg:60.31ms
step:1071/2285 train_time:64589ms step_avg:60.31ms
step:1072/2285 train_time:64649ms step_avg:60.31ms
step:1073/2285 train_time:64710ms step_avg:60.31ms
step:1074/2285 train_time:64770ms step_avg:60.31ms
step:1075/2285 train_time:64831ms step_avg:60.31ms
step:1076/2285 train_time:64891ms step_avg:60.31ms
step:1077/2285 train_time:64953ms step_avg:60.31ms
step:1078/2285 train_time:65012ms step_avg:60.31ms
step:1079/2285 train_time:65074ms step_avg:60.31ms
step:1080/2285 train_time:65134ms step_avg:60.31ms
step:1081/2285 train_time:65195ms step_avg:60.31ms
step:1082/2285 train_time:65255ms step_avg:60.31ms
step:1083/2285 train_time:65317ms step_avg:60.31ms
step:1084/2285 train_time:65377ms step_avg:60.31ms
step:1085/2285 train_time:65440ms step_avg:60.31ms
step:1086/2285 train_time:65499ms step_avg:60.31ms
step:1087/2285 train_time:65561ms step_avg:60.31ms
step:1088/2285 train_time:65620ms step_avg:60.31ms
step:1089/2285 train_time:65682ms step_avg:60.31ms
step:1090/2285 train_time:65742ms step_avg:60.31ms
step:1091/2285 train_time:65804ms step_avg:60.31ms
step:1092/2285 train_time:65863ms step_avg:60.31ms
step:1093/2285 train_time:65925ms step_avg:60.32ms
step:1094/2285 train_time:65985ms step_avg:60.32ms
step:1095/2285 train_time:66047ms step_avg:60.32ms
step:1096/2285 train_time:66106ms step_avg:60.32ms
step:1097/2285 train_time:66169ms step_avg:60.32ms
step:1098/2285 train_time:66228ms step_avg:60.32ms
step:1099/2285 train_time:66290ms step_avg:60.32ms
step:1100/2285 train_time:66350ms step_avg:60.32ms
step:1101/2285 train_time:66412ms step_avg:60.32ms
step:1102/2285 train_time:66471ms step_avg:60.32ms
step:1103/2285 train_time:66533ms step_avg:60.32ms
step:1104/2285 train_time:66593ms step_avg:60.32ms
step:1105/2285 train_time:66654ms step_avg:60.32ms
step:1106/2285 train_time:66714ms step_avg:60.32ms
step:1107/2285 train_time:66776ms step_avg:60.32ms
step:1108/2285 train_time:66836ms step_avg:60.32ms
step:1109/2285 train_time:66898ms step_avg:60.32ms
step:1110/2285 train_time:66958ms step_avg:60.32ms
step:1111/2285 train_time:67020ms step_avg:60.32ms
step:1112/2285 train_time:67079ms step_avg:60.32ms
step:1113/2285 train_time:67141ms step_avg:60.32ms
step:1114/2285 train_time:67201ms step_avg:60.32ms
step:1115/2285 train_time:67263ms step_avg:60.33ms
step:1116/2285 train_time:67322ms step_avg:60.32ms
step:1117/2285 train_time:67384ms step_avg:60.33ms
step:1118/2285 train_time:67443ms step_avg:60.33ms
step:1119/2285 train_time:67506ms step_avg:60.33ms
step:1120/2285 train_time:67565ms step_avg:60.33ms
step:1121/2285 train_time:67627ms step_avg:60.33ms
step:1122/2285 train_time:67687ms step_avg:60.33ms
step:1123/2285 train_time:67750ms step_avg:60.33ms
step:1124/2285 train_time:67809ms step_avg:60.33ms
step:1125/2285 train_time:67870ms step_avg:60.33ms
step:1126/2285 train_time:67930ms step_avg:60.33ms
step:1127/2285 train_time:67992ms step_avg:60.33ms
step:1128/2285 train_time:68052ms step_avg:60.33ms
step:1129/2285 train_time:68115ms step_avg:60.33ms
step:1130/2285 train_time:68174ms step_avg:60.33ms
step:1131/2285 train_time:68236ms step_avg:60.33ms
step:1132/2285 train_time:68295ms step_avg:60.33ms
step:1133/2285 train_time:68357ms step_avg:60.33ms
step:1134/2285 train_time:68417ms step_avg:60.33ms
step:1135/2285 train_time:68480ms step_avg:60.34ms
step:1136/2285 train_time:68539ms step_avg:60.33ms
step:1137/2285 train_time:68601ms step_avg:60.34ms
step:1138/2285 train_time:68660ms step_avg:60.33ms
step:1139/2285 train_time:68722ms step_avg:60.34ms
step:1140/2285 train_time:68781ms step_avg:60.33ms
step:1141/2285 train_time:68843ms step_avg:60.34ms
step:1142/2285 train_time:68902ms step_avg:60.33ms
step:1143/2285 train_time:68964ms step_avg:60.34ms
step:1144/2285 train_time:69024ms step_avg:60.34ms
step:1145/2285 train_time:69086ms step_avg:60.34ms
step:1146/2285 train_time:69146ms step_avg:60.34ms
step:1147/2285 train_time:69209ms step_avg:60.34ms
step:1148/2285 train_time:69270ms step_avg:60.34ms
step:1149/2285 train_time:69332ms step_avg:60.34ms
step:1150/2285 train_time:69392ms step_avg:60.34ms
step:1151/2285 train_time:69454ms step_avg:60.34ms
step:1152/2285 train_time:69513ms step_avg:60.34ms
step:1153/2285 train_time:69576ms step_avg:60.34ms
step:1154/2285 train_time:69635ms step_avg:60.34ms
step:1155/2285 train_time:69698ms step_avg:60.34ms
step:1156/2285 train_time:69758ms step_avg:60.34ms
step:1157/2285 train_time:69820ms step_avg:60.35ms
step:1158/2285 train_time:69880ms step_avg:60.35ms
step:1159/2285 train_time:69942ms step_avg:60.35ms
step:1160/2285 train_time:70002ms step_avg:60.35ms
step:1161/2285 train_time:70064ms step_avg:60.35ms
step:1162/2285 train_time:70123ms step_avg:60.35ms
step:1163/2285 train_time:70185ms step_avg:60.35ms
step:1164/2285 train_time:70245ms step_avg:60.35ms
step:1165/2285 train_time:70307ms step_avg:60.35ms
step:1166/2285 train_time:70368ms step_avg:60.35ms
step:1167/2285 train_time:70431ms step_avg:60.35ms
step:1168/2285 train_time:70491ms step_avg:60.35ms
step:1169/2285 train_time:70553ms step_avg:60.35ms
step:1170/2285 train_time:70613ms step_avg:60.35ms
step:1171/2285 train_time:70674ms step_avg:60.35ms
step:1172/2285 train_time:70734ms step_avg:60.35ms
step:1173/2285 train_time:70796ms step_avg:60.35ms
step:1174/2285 train_time:70856ms step_avg:60.35ms
step:1175/2285 train_time:70918ms step_avg:60.36ms
step:1176/2285 train_time:70978ms step_avg:60.36ms
step:1177/2285 train_time:71040ms step_avg:60.36ms
step:1178/2285 train_time:71099ms step_avg:60.36ms
step:1179/2285 train_time:71161ms step_avg:60.36ms
step:1180/2285 train_time:71221ms step_avg:60.36ms
step:1181/2285 train_time:71284ms step_avg:60.36ms
step:1182/2285 train_time:71344ms step_avg:60.36ms
step:1183/2285 train_time:71406ms step_avg:60.36ms
step:1184/2285 train_time:71467ms step_avg:60.36ms
step:1185/2285 train_time:71530ms step_avg:60.36ms
step:1186/2285 train_time:71591ms step_avg:60.36ms
step:1187/2285 train_time:71653ms step_avg:60.36ms
step:1188/2285 train_time:71712ms step_avg:60.36ms
step:1189/2285 train_time:71775ms step_avg:60.37ms
step:1190/2285 train_time:71834ms step_avg:60.36ms
step:1191/2285 train_time:71896ms step_avg:60.37ms
step:1192/2285 train_time:71956ms step_avg:60.37ms
step:1193/2285 train_time:72018ms step_avg:60.37ms
step:1194/2285 train_time:72078ms step_avg:60.37ms
step:1195/2285 train_time:72141ms step_avg:60.37ms
step:1196/2285 train_time:72200ms step_avg:60.37ms
step:1197/2285 train_time:72262ms step_avg:60.37ms
step:1198/2285 train_time:72323ms step_avg:60.37ms
step:1199/2285 train_time:72385ms step_avg:60.37ms
step:1200/2285 train_time:72444ms step_avg:60.37ms
step:1201/2285 train_time:72506ms step_avg:60.37ms
step:1202/2285 train_time:72566ms step_avg:60.37ms
step:1203/2285 train_time:72629ms step_avg:60.37ms
step:1204/2285 train_time:72689ms step_avg:60.37ms
step:1205/2285 train_time:72751ms step_avg:60.37ms
step:1206/2285 train_time:72811ms step_avg:60.37ms
step:1207/2285 train_time:72873ms step_avg:60.38ms
step:1208/2285 train_time:72933ms step_avg:60.37ms
step:1209/2285 train_time:72994ms step_avg:60.38ms
step:1210/2285 train_time:73054ms step_avg:60.38ms
step:1211/2285 train_time:73117ms step_avg:60.38ms
step:1212/2285 train_time:73177ms step_avg:60.38ms
step:1213/2285 train_time:73240ms step_avg:60.38ms
step:1214/2285 train_time:73300ms step_avg:60.38ms
step:1215/2285 train_time:73362ms step_avg:60.38ms
step:1216/2285 train_time:73422ms step_avg:60.38ms
step:1217/2285 train_time:73484ms step_avg:60.38ms
step:1218/2285 train_time:73544ms step_avg:60.38ms
step:1219/2285 train_time:73606ms step_avg:60.38ms
step:1220/2285 train_time:73666ms step_avg:60.38ms
step:1221/2285 train_time:73728ms step_avg:60.38ms
step:1222/2285 train_time:73789ms step_avg:60.38ms
step:1223/2285 train_time:73851ms step_avg:60.39ms
step:1224/2285 train_time:73911ms step_avg:60.38ms
step:1225/2285 train_time:73973ms step_avg:60.39ms
step:1226/2285 train_time:74032ms step_avg:60.39ms
step:1227/2285 train_time:74094ms step_avg:60.39ms
step:1228/2285 train_time:74154ms step_avg:60.39ms
step:1229/2285 train_time:74216ms step_avg:60.39ms
step:1230/2285 train_time:74277ms step_avg:60.39ms
step:1231/2285 train_time:74340ms step_avg:60.39ms
step:1232/2285 train_time:74399ms step_avg:60.39ms
step:1233/2285 train_time:74461ms step_avg:60.39ms
step:1234/2285 train_time:74521ms step_avg:60.39ms
step:1235/2285 train_time:74584ms step_avg:60.39ms
step:1236/2285 train_time:74644ms step_avg:60.39ms
step:1237/2285 train_time:74706ms step_avg:60.39ms
step:1238/2285 train_time:74766ms step_avg:60.39ms
step:1239/2285 train_time:74829ms step_avg:60.39ms
step:1240/2285 train_time:74888ms step_avg:60.39ms
step:1241/2285 train_time:74950ms step_avg:60.40ms
step:1242/2285 train_time:75010ms step_avg:60.39ms
step:1243/2285 train_time:75073ms step_avg:60.40ms
step:1244/2285 train_time:75133ms step_avg:60.40ms
step:1245/2285 train_time:75195ms step_avg:60.40ms
step:1246/2285 train_time:75254ms step_avg:60.40ms
step:1247/2285 train_time:75316ms step_avg:60.40ms
step:1248/2285 train_time:75376ms step_avg:60.40ms
step:1249/2285 train_time:75439ms step_avg:60.40ms
step:1250/2285 train_time:75500ms step_avg:60.40ms
step:1250/2285 val_loss:3.4966 train_time:75563ms step_avg:60.45ms
step:1251/2285 train_time:75586ms step_avg:60.42ms
step:1252/2285 train_time:75626ms step_avg:60.40ms
step:1253/2285 train_time:75687ms step_avg:60.40ms
step:1254/2285 train_time:75745ms step_avg:60.40ms
step:1255/2285 train_time:75807ms step_avg:60.40ms
step:1256/2285 train_time:75866ms step_avg:60.40ms
step:1257/2285 train_time:75927ms step_avg:60.40ms
step:1258/2285 train_time:75986ms step_avg:60.40ms
step:1259/2285 train_time:76047ms step_avg:60.40ms
step:1260/2285 train_time:76106ms step_avg:60.40ms
step:1261/2285 train_time:76167ms step_avg:60.40ms
step:1262/2285 train_time:76226ms step_avg:60.40ms
step:1263/2285 train_time:76287ms step_avg:60.40ms
step:1264/2285 train_time:76346ms step_avg:60.40ms
step:1265/2285 train_time:76407ms step_avg:60.40ms
step:1266/2285 train_time:76476ms step_avg:60.41ms
step:1267/2285 train_time:76544ms step_avg:60.41ms
step:1268/2285 train_time:76604ms step_avg:60.41ms
step:1269/2285 train_time:76667ms step_avg:60.41ms
step:1270/2285 train_time:76726ms step_avg:60.41ms
step:1271/2285 train_time:76787ms step_avg:60.42ms
step:1272/2285 train_time:76847ms step_avg:60.41ms
step:1273/2285 train_time:76908ms step_avg:60.42ms
step:1274/2285 train_time:76967ms step_avg:60.41ms
step:1275/2285 train_time:77028ms step_avg:60.41ms
step:1276/2285 train_time:77087ms step_avg:60.41ms
step:1277/2285 train_time:77148ms step_avg:60.41ms
step:1278/2285 train_time:77208ms step_avg:60.41ms
step:1279/2285 train_time:77269ms step_avg:60.41ms
step:1280/2285 train_time:77328ms step_avg:60.41ms
step:1281/2285 train_time:77391ms step_avg:60.41ms
step:1282/2285 train_time:77454ms step_avg:60.42ms
step:1283/2285 train_time:77518ms step_avg:60.42ms
step:1284/2285 train_time:77579ms step_avg:60.42ms
step:1285/2285 train_time:77641ms step_avg:60.42ms
step:1286/2285 train_time:77701ms step_avg:60.42ms
step:1287/2285 train_time:77763ms step_avg:60.42ms
step:1288/2285 train_time:77822ms step_avg:60.42ms
step:1289/2285 train_time:77884ms step_avg:60.42ms
step:1290/2285 train_time:77943ms step_avg:60.42ms
step:1291/2285 train_time:78004ms step_avg:60.42ms
step:1292/2285 train_time:78063ms step_avg:60.42ms
step:1293/2285 train_time:78125ms step_avg:60.42ms
step:1294/2285 train_time:78185ms step_avg:60.42ms
step:1295/2285 train_time:78246ms step_avg:60.42ms
step:1296/2285 train_time:78305ms step_avg:60.42ms
step:1297/2285 train_time:78367ms step_avg:60.42ms
step:1298/2285 train_time:78428ms step_avg:60.42ms
step:1299/2285 train_time:78491ms step_avg:60.42ms
step:1300/2285 train_time:78553ms step_avg:60.43ms
step:1301/2285 train_time:78617ms step_avg:60.43ms
step:1302/2285 train_time:78676ms step_avg:60.43ms
step:1303/2285 train_time:78738ms step_avg:60.43ms
step:1304/2285 train_time:78798ms step_avg:60.43ms
step:1305/2285 train_time:78859ms step_avg:60.43ms
step:1306/2285 train_time:78919ms step_avg:60.43ms
step:1307/2285 train_time:78982ms step_avg:60.43ms
step:1308/2285 train_time:79041ms step_avg:60.43ms
step:1309/2285 train_time:79103ms step_avg:60.43ms
step:1310/2285 train_time:79162ms step_avg:60.43ms
step:1311/2285 train_time:79224ms step_avg:60.43ms
step:1312/2285 train_time:79283ms step_avg:60.43ms
step:1313/2285 train_time:79345ms step_avg:60.43ms
step:1314/2285 train_time:79405ms step_avg:60.43ms
step:1315/2285 train_time:79468ms step_avg:60.43ms
step:1316/2285 train_time:79529ms step_avg:60.43ms
step:1317/2285 train_time:79592ms step_avg:60.43ms
step:1318/2285 train_time:79653ms step_avg:60.44ms
step:1319/2285 train_time:79716ms step_avg:60.44ms
step:1320/2285 train_time:79775ms step_avg:60.44ms
step:1321/2285 train_time:79837ms step_avg:60.44ms
step:1322/2285 train_time:79897ms step_avg:60.44ms
step:1323/2285 train_time:79959ms step_avg:60.44ms
step:1324/2285 train_time:80019ms step_avg:60.44ms
step:1325/2285 train_time:80081ms step_avg:60.44ms
step:1326/2285 train_time:80141ms step_avg:60.44ms
step:1327/2285 train_time:80203ms step_avg:60.44ms
step:1328/2285 train_time:80263ms step_avg:60.44ms
step:1329/2285 train_time:80324ms step_avg:60.44ms
step:1330/2285 train_time:80384ms step_avg:60.44ms
step:1331/2285 train_time:80446ms step_avg:60.44ms
step:1332/2285 train_time:80506ms step_avg:60.44ms
step:1333/2285 train_time:80569ms step_avg:60.44ms
step:1334/2285 train_time:80628ms step_avg:60.44ms
step:1335/2285 train_time:80691ms step_avg:60.44ms
step:1336/2285 train_time:80752ms step_avg:60.44ms
step:1337/2285 train_time:80815ms step_avg:60.44ms
step:1338/2285 train_time:80874ms step_avg:60.44ms
step:1339/2285 train_time:80936ms step_avg:60.45ms
step:1340/2285 train_time:80996ms step_avg:60.45ms
step:1341/2285 train_time:81058ms step_avg:60.45ms
step:1342/2285 train_time:81117ms step_avg:60.44ms
step:1343/2285 train_time:81179ms step_avg:60.45ms
step:1344/2285 train_time:81238ms step_avg:60.45ms
step:1345/2285 train_time:81300ms step_avg:60.45ms
step:1346/2285 train_time:81362ms step_avg:60.45ms
step:1347/2285 train_time:81424ms step_avg:60.45ms
step:1348/2285 train_time:81483ms step_avg:60.45ms
step:1349/2285 train_time:81546ms step_avg:60.45ms
step:1350/2285 train_time:81606ms step_avg:60.45ms
step:1351/2285 train_time:81668ms step_avg:60.45ms
step:1352/2285 train_time:81727ms step_avg:60.45ms
step:1353/2285 train_time:81790ms step_avg:60.45ms
step:1354/2285 train_time:81850ms step_avg:60.45ms
step:1355/2285 train_time:81913ms step_avg:60.45ms
step:1356/2285 train_time:81973ms step_avg:60.45ms
step:1357/2285 train_time:82035ms step_avg:60.45ms
step:1358/2285 train_time:82094ms step_avg:60.45ms
step:1359/2285 train_time:82157ms step_avg:60.45ms
step:1360/2285 train_time:82216ms step_avg:60.45ms
step:1361/2285 train_time:82278ms step_avg:60.45ms
step:1362/2285 train_time:82338ms step_avg:60.45ms
step:1363/2285 train_time:82400ms step_avg:60.45ms
step:1364/2285 train_time:82460ms step_avg:60.45ms
step:1365/2285 train_time:82523ms step_avg:60.46ms
step:1366/2285 train_time:82583ms step_avg:60.46ms
step:1367/2285 train_time:82645ms step_avg:60.46ms
step:1368/2285 train_time:82705ms step_avg:60.46ms
step:1369/2285 train_time:82767ms step_avg:60.46ms
step:1370/2285 train_time:82827ms step_avg:60.46ms
step:1371/2285 train_time:82889ms step_avg:60.46ms
step:1372/2285 train_time:82949ms step_avg:60.46ms
step:1373/2285 train_time:83011ms step_avg:60.46ms
step:1374/2285 train_time:83071ms step_avg:60.46ms
step:1375/2285 train_time:83133ms step_avg:60.46ms
step:1376/2285 train_time:83193ms step_avg:60.46ms
step:1377/2285 train_time:83255ms step_avg:60.46ms
step:1378/2285 train_time:83315ms step_avg:60.46ms
step:1379/2285 train_time:83377ms step_avg:60.46ms
step:1380/2285 train_time:83436ms step_avg:60.46ms
step:1381/2285 train_time:83499ms step_avg:60.46ms
step:1382/2285 train_time:83559ms step_avg:60.46ms
step:1383/2285 train_time:83621ms step_avg:60.46ms
step:1384/2285 train_time:83681ms step_avg:60.46ms
step:1385/2285 train_time:83744ms step_avg:60.46ms
step:1386/2285 train_time:83804ms step_avg:60.46ms
step:1387/2285 train_time:83866ms step_avg:60.47ms
step:1388/2285 train_time:83926ms step_avg:60.47ms
step:1389/2285 train_time:83987ms step_avg:60.47ms
step:1390/2285 train_time:84047ms step_avg:60.47ms
step:1391/2285 train_time:84110ms step_avg:60.47ms
step:1392/2285 train_time:84170ms step_avg:60.47ms
step:1393/2285 train_time:84232ms step_avg:60.47ms
step:1394/2285 train_time:84292ms step_avg:60.47ms
step:1395/2285 train_time:84355ms step_avg:60.47ms
step:1396/2285 train_time:84415ms step_avg:60.47ms
step:1397/2285 train_time:84477ms step_avg:60.47ms
step:1398/2285 train_time:84536ms step_avg:60.47ms
step:1399/2285 train_time:84599ms step_avg:60.47ms
step:1400/2285 train_time:84659ms step_avg:60.47ms
step:1401/2285 train_time:84721ms step_avg:60.47ms
step:1402/2285 train_time:84781ms step_avg:60.47ms
step:1403/2285 train_time:84843ms step_avg:60.47ms
step:1404/2285 train_time:84903ms step_avg:60.47ms
step:1405/2285 train_time:84965ms step_avg:60.47ms
step:1406/2285 train_time:85025ms step_avg:60.47ms
step:1407/2285 train_time:85087ms step_avg:60.47ms
step:1408/2285 train_time:85146ms step_avg:60.47ms
step:1409/2285 train_time:85208ms step_avg:60.47ms
step:1410/2285 train_time:85268ms step_avg:60.47ms
step:1411/2285 train_time:85330ms step_avg:60.47ms
step:1412/2285 train_time:85390ms step_avg:60.47ms
step:1413/2285 train_time:85454ms step_avg:60.48ms
step:1414/2285 train_time:85514ms step_avg:60.48ms
step:1415/2285 train_time:85576ms step_avg:60.48ms
step:1416/2285 train_time:85636ms step_avg:60.48ms
step:1417/2285 train_time:85698ms step_avg:60.48ms
step:1418/2285 train_time:85758ms step_avg:60.48ms
step:1419/2285 train_time:85820ms step_avg:60.48ms
step:1420/2285 train_time:85880ms step_avg:60.48ms
step:1421/2285 train_time:85943ms step_avg:60.48ms
step:1422/2285 train_time:86003ms step_avg:60.48ms
step:1423/2285 train_time:86064ms step_avg:60.48ms
step:1424/2285 train_time:86124ms step_avg:60.48ms
step:1425/2285 train_time:86186ms step_avg:60.48ms
step:1426/2285 train_time:86245ms step_avg:60.48ms
step:1427/2285 train_time:86307ms step_avg:60.48ms
step:1428/2285 train_time:86367ms step_avg:60.48ms
step:1429/2285 train_time:86430ms step_avg:60.48ms
step:1430/2285 train_time:86491ms step_avg:60.48ms
step:1431/2285 train_time:86554ms step_avg:60.49ms
step:1432/2285 train_time:86614ms step_avg:60.48ms
step:1433/2285 train_time:86676ms step_avg:60.49ms
step:1434/2285 train_time:86736ms step_avg:60.49ms
step:1435/2285 train_time:86798ms step_avg:60.49ms
step:1436/2285 train_time:86858ms step_avg:60.49ms
step:1437/2285 train_time:86920ms step_avg:60.49ms
step:1438/2285 train_time:86979ms step_avg:60.49ms
step:1439/2285 train_time:87041ms step_avg:60.49ms
step:1440/2285 train_time:87101ms step_avg:60.49ms
step:1441/2285 train_time:87163ms step_avg:60.49ms
step:1442/2285 train_time:87223ms step_avg:60.49ms
step:1443/2285 train_time:87285ms step_avg:60.49ms
step:1444/2285 train_time:87345ms step_avg:60.49ms
step:1445/2285 train_time:87407ms step_avg:60.49ms
step:1446/2285 train_time:87467ms step_avg:60.49ms
step:1447/2285 train_time:87529ms step_avg:60.49ms
step:1448/2285 train_time:87590ms step_avg:60.49ms
step:1449/2285 train_time:87654ms step_avg:60.49ms
step:1450/2285 train_time:87714ms step_avg:60.49ms
step:1451/2285 train_time:87776ms step_avg:60.49ms
step:1452/2285 train_time:87835ms step_avg:60.49ms
step:1453/2285 train_time:87897ms step_avg:60.49ms
step:1454/2285 train_time:87957ms step_avg:60.49ms
step:1455/2285 train_time:88019ms step_avg:60.49ms
step:1456/2285 train_time:88079ms step_avg:60.49ms
step:1457/2285 train_time:88141ms step_avg:60.49ms
step:1458/2285 train_time:88201ms step_avg:60.49ms
step:1459/2285 train_time:88263ms step_avg:60.50ms
step:1460/2285 train_time:88323ms step_avg:60.50ms
step:1461/2285 train_time:88385ms step_avg:60.50ms
step:1462/2285 train_time:88445ms step_avg:60.50ms
step:1463/2285 train_time:88508ms step_avg:60.50ms
step:1464/2285 train_time:88568ms step_avg:60.50ms
step:1465/2285 train_time:88630ms step_avg:60.50ms
step:1466/2285 train_time:88690ms step_avg:60.50ms
step:1467/2285 train_time:88753ms step_avg:60.50ms
step:1468/2285 train_time:88813ms step_avg:60.50ms
step:1469/2285 train_time:88875ms step_avg:60.50ms
step:1470/2285 train_time:88935ms step_avg:60.50ms
step:1471/2285 train_time:88997ms step_avg:60.50ms
step:1472/2285 train_time:89057ms step_avg:60.50ms
step:1473/2285 train_time:89119ms step_avg:60.50ms
step:1474/2285 train_time:89179ms step_avg:60.50ms
step:1475/2285 train_time:89241ms step_avg:60.50ms
step:1476/2285 train_time:89301ms step_avg:60.50ms
step:1477/2285 train_time:89364ms step_avg:60.50ms
step:1478/2285 train_time:89424ms step_avg:60.50ms
step:1479/2285 train_time:89486ms step_avg:60.50ms
step:1480/2285 train_time:89545ms step_avg:60.50ms
step:1481/2285 train_time:89608ms step_avg:60.50ms
step:1482/2285 train_time:89667ms step_avg:60.50ms
step:1483/2285 train_time:89729ms step_avg:60.51ms
step:1484/2285 train_time:89789ms step_avg:60.50ms
step:1485/2285 train_time:89853ms step_avg:60.51ms
step:1486/2285 train_time:89913ms step_avg:60.51ms
step:1487/2285 train_time:89975ms step_avg:60.51ms
step:1488/2285 train_time:90035ms step_avg:60.51ms
step:1489/2285 train_time:90097ms step_avg:60.51ms
step:1490/2285 train_time:90156ms step_avg:60.51ms
step:1491/2285 train_time:90218ms step_avg:60.51ms
step:1492/2285 train_time:90278ms step_avg:60.51ms
step:1493/2285 train_time:90340ms step_avg:60.51ms
step:1494/2285 train_time:90401ms step_avg:60.51ms
step:1495/2285 train_time:90465ms step_avg:60.51ms
step:1496/2285 train_time:90524ms step_avg:60.51ms
step:1497/2285 train_time:90586ms step_avg:60.51ms
step:1498/2285 train_time:90645ms step_avg:60.51ms
step:1499/2285 train_time:90707ms step_avg:60.51ms
step:1500/2285 train_time:90767ms step_avg:60.51ms
step:1500/2285 val_loss:3.4283 train_time:90830ms step_avg:60.55ms
step:1501/2285 train_time:90848ms step_avg:60.53ms
step:1502/2285 train_time:90892ms step_avg:60.51ms
step:1503/2285 train_time:90957ms step_avg:60.52ms
step:1504/2285 train_time:91017ms step_avg:60.52ms
step:1505/2285 train_time:91079ms step_avg:60.52ms
step:1506/2285 train_time:91138ms step_avg:60.52ms
step:1507/2285 train_time:91200ms step_avg:60.52ms
step:1508/2285 train_time:91259ms step_avg:60.52ms
step:1509/2285 train_time:91320ms step_avg:60.52ms
step:1510/2285 train_time:91380ms step_avg:60.52ms
step:1511/2285 train_time:91443ms step_avg:60.52ms
step:1512/2285 train_time:91502ms step_avg:60.52ms
step:1513/2285 train_time:91563ms step_avg:60.52ms
step:1514/2285 train_time:91623ms step_avg:60.52ms
step:1515/2285 train_time:91684ms step_avg:60.52ms
step:1516/2285 train_time:91745ms step_avg:60.52ms
step:1517/2285 train_time:91809ms step_avg:60.52ms
step:1518/2285 train_time:91870ms step_avg:60.52ms
step:1519/2285 train_time:91933ms step_avg:60.52ms
step:1520/2285 train_time:91994ms step_avg:60.52ms
step:1521/2285 train_time:92056ms step_avg:60.52ms
step:1522/2285 train_time:92116ms step_avg:60.52ms
step:1523/2285 train_time:92177ms step_avg:60.52ms
step:1524/2285 train_time:92236ms step_avg:60.52ms
step:1525/2285 train_time:92298ms step_avg:60.52ms
step:1526/2285 train_time:92358ms step_avg:60.52ms
step:1527/2285 train_time:92421ms step_avg:60.52ms
step:1528/2285 train_time:92480ms step_avg:60.52ms
step:1529/2285 train_time:92542ms step_avg:60.52ms
step:1530/2285 train_time:92601ms step_avg:60.52ms
step:1531/2285 train_time:92664ms step_avg:60.52ms
step:1532/2285 train_time:92724ms step_avg:60.52ms
step:1533/2285 train_time:92788ms step_avg:60.53ms
step:1534/2285 train_time:92849ms step_avg:60.53ms
step:1535/2285 train_time:92913ms step_avg:60.53ms
step:1536/2285 train_time:92973ms step_avg:60.53ms
step:1537/2285 train_time:93035ms step_avg:60.53ms
step:1538/2285 train_time:93095ms step_avg:60.53ms
step:1539/2285 train_time:93157ms step_avg:60.53ms
step:1540/2285 train_time:93217ms step_avg:60.53ms
step:1541/2285 train_time:93279ms step_avg:60.53ms
step:1542/2285 train_time:93339ms step_avg:60.53ms
step:1543/2285 train_time:93401ms step_avg:60.53ms
step:1544/2285 train_time:93461ms step_avg:60.53ms
step:1545/2285 train_time:93522ms step_avg:60.53ms
step:1546/2285 train_time:93582ms step_avg:60.53ms
step:1547/2285 train_time:93643ms step_avg:60.53ms
step:1548/2285 train_time:93704ms step_avg:60.53ms
step:1549/2285 train_time:93767ms step_avg:60.53ms
step:1550/2285 train_time:93828ms step_avg:60.53ms
step:1551/2285 train_time:93891ms step_avg:60.54ms
step:1552/2285 train_time:93950ms step_avg:60.53ms
step:1553/2285 train_time:94013ms step_avg:60.54ms
step:1554/2285 train_time:94073ms step_avg:60.54ms
step:1555/2285 train_time:94135ms step_avg:60.54ms
step:1556/2285 train_time:94196ms step_avg:60.54ms
step:1557/2285 train_time:94258ms step_avg:60.54ms
step:1558/2285 train_time:94317ms step_avg:60.54ms
step:1559/2285 train_time:94380ms step_avg:60.54ms
step:1560/2285 train_time:94439ms step_avg:60.54ms
step:1561/2285 train_time:94501ms step_avg:60.54ms
step:1562/2285 train_time:94560ms step_avg:60.54ms
step:1563/2285 train_time:94622ms step_avg:60.54ms
step:1564/2285 train_time:94682ms step_avg:60.54ms
step:1565/2285 train_time:94745ms step_avg:60.54ms
step:1566/2285 train_time:94807ms step_avg:60.54ms
step:1567/2285 train_time:94870ms step_avg:60.54ms
step:1568/2285 train_time:94930ms step_avg:60.54ms
step:1569/2285 train_time:94993ms step_avg:60.54ms
step:1570/2285 train_time:95053ms step_avg:60.54ms
step:1571/2285 train_time:95116ms step_avg:60.54ms
step:1572/2285 train_time:95176ms step_avg:60.54ms
step:1573/2285 train_time:95238ms step_avg:60.55ms
step:1574/2285 train_time:95298ms step_avg:60.55ms
step:1575/2285 train_time:95360ms step_avg:60.55ms
step:1576/2285 train_time:95419ms step_avg:60.55ms
step:1577/2285 train_time:95482ms step_avg:60.55ms
step:1578/2285 train_time:95542ms step_avg:60.55ms
step:1579/2285 train_time:95603ms step_avg:60.55ms
step:1580/2285 train_time:95663ms step_avg:60.55ms
step:1581/2285 train_time:95726ms step_avg:60.55ms
step:1582/2285 train_time:95788ms step_avg:60.55ms
step:1583/2285 train_time:95850ms step_avg:60.55ms
step:1584/2285 train_time:95910ms step_avg:60.55ms
step:1585/2285 train_time:95973ms step_avg:60.55ms
step:1586/2285 train_time:96033ms step_avg:60.55ms
step:1587/2285 train_time:96095ms step_avg:60.55ms
step:1588/2285 train_time:96155ms step_avg:60.55ms
step:1589/2285 train_time:96217ms step_avg:60.55ms
step:1590/2285 train_time:96277ms step_avg:60.55ms
step:1591/2285 train_time:96339ms step_avg:60.55ms
step:1592/2285 train_time:96398ms step_avg:60.55ms
step:1593/2285 train_time:96460ms step_avg:60.55ms
step:1594/2285 train_time:96520ms step_avg:60.55ms
step:1595/2285 train_time:96583ms step_avg:60.55ms
step:1596/2285 train_time:96642ms step_avg:60.55ms
step:1597/2285 train_time:96705ms step_avg:60.55ms
step:1598/2285 train_time:96766ms step_avg:60.55ms
step:1599/2285 train_time:96830ms step_avg:60.56ms
step:1600/2285 train_time:96890ms step_avg:60.56ms
step:1601/2285 train_time:96951ms step_avg:60.56ms
step:1602/2285 train_time:97011ms step_avg:60.56ms
step:1603/2285 train_time:97073ms step_avg:60.56ms
step:1604/2285 train_time:97133ms step_avg:60.56ms
step:1605/2285 train_time:97195ms step_avg:60.56ms
step:1606/2285 train_time:97255ms step_avg:60.56ms
step:1607/2285 train_time:97317ms step_avg:60.56ms
step:1608/2285 train_time:97377ms step_avg:60.56ms
step:1609/2285 train_time:97440ms step_avg:60.56ms
step:1610/2285 train_time:97500ms step_avg:60.56ms
step:1611/2285 train_time:97561ms step_avg:60.56ms
step:1612/2285 train_time:97621ms step_avg:60.56ms
step:1613/2285 train_time:97683ms step_avg:60.56ms
step:1614/2285 train_time:97744ms step_avg:60.56ms
step:1615/2285 train_time:97808ms step_avg:60.56ms
step:1616/2285 train_time:97869ms step_avg:60.56ms
step:1617/2285 train_time:97931ms step_avg:60.56ms
step:1618/2285 train_time:97992ms step_avg:60.56ms
step:1619/2285 train_time:98054ms step_avg:60.56ms
step:1620/2285 train_time:98114ms step_avg:60.56ms
step:1621/2285 train_time:98176ms step_avg:60.56ms
step:1622/2285 train_time:98235ms step_avg:60.56ms
step:1623/2285 train_time:98297ms step_avg:60.57ms
step:1624/2285 train_time:98357ms step_avg:60.56ms
step:1625/2285 train_time:98419ms step_avg:60.57ms
step:1626/2285 train_time:98479ms step_avg:60.57ms
step:1627/2285 train_time:98541ms step_avg:60.57ms
step:1628/2285 train_time:98601ms step_avg:60.57ms
step:1629/2285 train_time:98663ms step_avg:60.57ms
step:1630/2285 train_time:98723ms step_avg:60.57ms
step:1631/2285 train_time:98786ms step_avg:60.57ms
step:1632/2285 train_time:98848ms step_avg:60.57ms
step:1633/2285 train_time:98911ms step_avg:60.57ms
step:1634/2285 train_time:98970ms step_avg:60.57ms
step:1635/2285 train_time:99033ms step_avg:60.57ms
step:1636/2285 train_time:99093ms step_avg:60.57ms
step:1637/2285 train_time:99156ms step_avg:60.57ms
step:1638/2285 train_time:99216ms step_avg:60.57ms
step:1639/2285 train_time:99278ms step_avg:60.57ms
step:1640/2285 train_time:99338ms step_avg:60.57ms
step:1641/2285 train_time:99400ms step_avg:60.57ms
step:1642/2285 train_time:99460ms step_avg:60.57ms
step:1643/2285 train_time:99522ms step_avg:60.57ms
step:1644/2285 train_time:99582ms step_avg:60.57ms
step:1645/2285 train_time:99643ms step_avg:60.57ms
step:1646/2285 train_time:99703ms step_avg:60.57ms
step:1647/2285 train_time:99766ms step_avg:60.57ms
step:1648/2285 train_time:99827ms step_avg:60.57ms
step:1649/2285 train_time:99890ms step_avg:60.58ms
step:1650/2285 train_time:99950ms step_avg:60.58ms
step:1651/2285 train_time:100013ms step_avg:60.58ms
step:1652/2285 train_time:100073ms step_avg:60.58ms
step:1653/2285 train_time:100135ms step_avg:60.58ms
step:1654/2285 train_time:100195ms step_avg:60.58ms
step:1655/2285 train_time:100257ms step_avg:60.58ms
step:1656/2285 train_time:100317ms step_avg:60.58ms
step:1657/2285 train_time:100379ms step_avg:60.58ms
step:1658/2285 train_time:100439ms step_avg:60.58ms
step:1659/2285 train_time:100501ms step_avg:60.58ms
step:1660/2285 train_time:100561ms step_avg:60.58ms
step:1661/2285 train_time:100624ms step_avg:60.58ms
step:1662/2285 train_time:100684ms step_avg:60.58ms
step:1663/2285 train_time:100747ms step_avg:60.58ms
step:1664/2285 train_time:100807ms step_avg:60.58ms
step:1665/2285 train_time:100870ms step_avg:60.58ms
step:1666/2285 train_time:100930ms step_avg:60.58ms
step:1667/2285 train_time:100992ms step_avg:60.58ms
step:1668/2285 train_time:101052ms step_avg:60.58ms
step:1669/2285 train_time:101114ms step_avg:60.58ms
step:1670/2285 train_time:101174ms step_avg:60.58ms
step:1671/2285 train_time:101236ms step_avg:60.58ms
step:1672/2285 train_time:101297ms step_avg:60.58ms
step:1673/2285 train_time:101359ms step_avg:60.59ms
step:1674/2285 train_time:101419ms step_avg:60.58ms
step:1675/2285 train_time:101481ms step_avg:60.59ms
step:1676/2285 train_time:101541ms step_avg:60.59ms
step:1677/2285 train_time:101603ms step_avg:60.59ms
step:1678/2285 train_time:101664ms step_avg:60.59ms
step:1679/2285 train_time:101726ms step_avg:60.59ms
step:1680/2285 train_time:101787ms step_avg:60.59ms
step:1681/2285 train_time:101849ms step_avg:60.59ms
step:1682/2285 train_time:101909ms step_avg:60.59ms
step:1683/2285 train_time:101971ms step_avg:60.59ms
step:1684/2285 train_time:102031ms step_avg:60.59ms
step:1685/2285 train_time:102093ms step_avg:60.59ms
step:1686/2285 train_time:102153ms step_avg:60.59ms
step:1687/2285 train_time:102216ms step_avg:60.59ms
step:1688/2285 train_time:102276ms step_avg:60.59ms
step:1689/2285 train_time:102338ms step_avg:60.59ms
step:1690/2285 train_time:102398ms step_avg:60.59ms
step:1691/2285 train_time:102460ms step_avg:60.59ms
step:1692/2285 train_time:102520ms step_avg:60.59ms
step:1693/2285 train_time:102582ms step_avg:60.59ms
step:1694/2285 train_time:102642ms step_avg:60.59ms
step:1695/2285 train_time:102705ms step_avg:60.59ms
step:1696/2285 train_time:102765ms step_avg:60.59ms
step:1697/2285 train_time:102828ms step_avg:60.59ms
step:1698/2285 train_time:102888ms step_avg:60.59ms
step:1699/2285 train_time:102950ms step_avg:60.59ms
step:1700/2285 train_time:103011ms step_avg:60.59ms
step:1701/2285 train_time:103073ms step_avg:60.60ms
step:1702/2285 train_time:103133ms step_avg:60.60ms
step:1703/2285 train_time:103195ms step_avg:60.60ms
step:1704/2285 train_time:103255ms step_avg:60.60ms
step:1705/2285 train_time:103317ms step_avg:60.60ms
step:1706/2285 train_time:103377ms step_avg:60.60ms
step:1707/2285 train_time:103440ms step_avg:60.60ms
step:1708/2285 train_time:103500ms step_avg:60.60ms
step:1709/2285 train_time:103562ms step_avg:60.60ms
step:1710/2285 train_time:103621ms step_avg:60.60ms
step:1711/2285 train_time:103684ms step_avg:60.60ms
step:1712/2285 train_time:103743ms step_avg:60.60ms
step:1713/2285 train_time:103805ms step_avg:60.60ms
step:1714/2285 train_time:103866ms step_avg:60.60ms
step:1715/2285 train_time:103929ms step_avg:60.60ms
step:1716/2285 train_time:103990ms step_avg:60.60ms
step:1717/2285 train_time:104052ms step_avg:60.60ms
step:1718/2285 train_time:104112ms step_avg:60.60ms
step:1719/2285 train_time:104175ms step_avg:60.60ms
step:1720/2285 train_time:104235ms step_avg:60.60ms
step:1721/2285 train_time:104297ms step_avg:60.60ms
step:1722/2285 train_time:104356ms step_avg:60.60ms
step:1723/2285 train_time:104419ms step_avg:60.60ms
step:1724/2285 train_time:104478ms step_avg:60.60ms
step:1725/2285 train_time:104540ms step_avg:60.60ms
step:1726/2285 train_time:104601ms step_avg:60.60ms
step:1727/2285 train_time:104663ms step_avg:60.60ms
step:1728/2285 train_time:104723ms step_avg:60.60ms
step:1729/2285 train_time:104786ms step_avg:60.60ms
step:1730/2285 train_time:104846ms step_avg:60.60ms
step:1731/2285 train_time:104909ms step_avg:60.61ms
step:1732/2285 train_time:104969ms step_avg:60.61ms
step:1733/2285 train_time:105031ms step_avg:60.61ms
step:1734/2285 train_time:105092ms step_avg:60.61ms
step:1735/2285 train_time:105154ms step_avg:60.61ms
step:1736/2285 train_time:105215ms step_avg:60.61ms
step:1737/2285 train_time:105277ms step_avg:60.61ms
step:1738/2285 train_time:105336ms step_avg:60.61ms
step:1739/2285 train_time:105399ms step_avg:60.61ms
step:1740/2285 train_time:105459ms step_avg:60.61ms
step:1741/2285 train_time:105521ms step_avg:60.61ms
step:1742/2285 train_time:105580ms step_avg:60.61ms
step:1743/2285 train_time:105642ms step_avg:60.61ms
step:1744/2285 train_time:105703ms step_avg:60.61ms
step:1745/2285 train_time:105765ms step_avg:60.61ms
step:1746/2285 train_time:105826ms step_avg:60.61ms
step:1747/2285 train_time:105889ms step_avg:60.61ms
step:1748/2285 train_time:105949ms step_avg:60.61ms
step:1749/2285 train_time:106011ms step_avg:60.61ms
step:1750/2285 train_time:106071ms step_avg:60.61ms
step:1750/2285 val_loss:3.3684 train_time:106135ms step_avg:60.65ms
step:1751/2285 train_time:106157ms step_avg:60.63ms
step:1752/2285 train_time:106196ms step_avg:60.61ms
step:1753/2285 train_time:106259ms step_avg:60.62ms
step:1754/2285 train_time:106321ms step_avg:60.62ms
step:1755/2285 train_time:106386ms step_avg:60.62ms
step:1756/2285 train_time:106447ms step_avg:60.62ms
step:1757/2285 train_time:106508ms step_avg:60.62ms
step:1758/2285 train_time:106567ms step_avg:60.62ms
step:1759/2285 train_time:106628ms step_avg:60.62ms
step:1760/2285 train_time:106687ms step_avg:60.62ms
step:1761/2285 train_time:106749ms step_avg:60.62ms
step:1762/2285 train_time:106808ms step_avg:60.62ms
step:1763/2285 train_time:106870ms step_avg:60.62ms
step:1764/2285 train_time:106930ms step_avg:60.62ms
step:1765/2285 train_time:106991ms step_avg:60.62ms
step:1766/2285 train_time:107052ms step_avg:60.62ms
step:1767/2285 train_time:107115ms step_avg:60.62ms
step:1768/2285 train_time:107176ms step_avg:60.62ms
step:1769/2285 train_time:107239ms step_avg:60.62ms
step:1770/2285 train_time:107300ms step_avg:60.62ms
step:1771/2285 train_time:107362ms step_avg:60.62ms
step:1772/2285 train_time:107423ms step_avg:60.62ms
step:1773/2285 train_time:107485ms step_avg:60.62ms
step:1774/2285 train_time:107545ms step_avg:60.62ms
step:1775/2285 train_time:107607ms step_avg:60.62ms
step:1776/2285 train_time:107666ms step_avg:60.62ms
step:1777/2285 train_time:107727ms step_avg:60.62ms
step:1778/2285 train_time:107786ms step_avg:60.62ms
step:1779/2285 train_time:107848ms step_avg:60.62ms
step:1780/2285 train_time:107907ms step_avg:60.62ms
step:1781/2285 train_time:107969ms step_avg:60.62ms
step:1782/2285 train_time:108029ms step_avg:60.62ms
step:1783/2285 train_time:108093ms step_avg:60.62ms
step:1784/2285 train_time:108153ms step_avg:60.62ms
step:1785/2285 train_time:108216ms step_avg:60.63ms
step:1786/2285 train_time:108276ms step_avg:60.63ms
step:1787/2285 train_time:108339ms step_avg:60.63ms
step:1788/2285 train_time:108399ms step_avg:60.63ms
step:1789/2285 train_time:108462ms step_avg:60.63ms
step:1790/2285 train_time:108522ms step_avg:60.63ms
step:1791/2285 train_time:108584ms step_avg:60.63ms
step:1792/2285 train_time:108644ms step_avg:60.63ms
step:1793/2285 train_time:108706ms step_avg:60.63ms
step:1794/2285 train_time:108766ms step_avg:60.63ms
step:1795/2285 train_time:108828ms step_avg:60.63ms
step:1796/2285 train_time:108887ms step_avg:60.63ms
step:1797/2285 train_time:108949ms step_avg:60.63ms
step:1798/2285 train_time:109008ms step_avg:60.63ms
step:1799/2285 train_time:109072ms step_avg:60.63ms
step:1800/2285 train_time:109133ms step_avg:60.63ms
step:1801/2285 train_time:109196ms step_avg:60.63ms
step:1802/2285 train_time:109256ms step_avg:60.63ms
step:1803/2285 train_time:109319ms step_avg:60.63ms
step:1804/2285 train_time:109378ms step_avg:60.63ms
step:1805/2285 train_time:109441ms step_avg:60.63ms
step:1806/2285 train_time:109500ms step_avg:60.63ms
step:1807/2285 train_time:109562ms step_avg:60.63ms
step:1808/2285 train_time:109622ms step_avg:60.63ms
step:1809/2285 train_time:109684ms step_avg:60.63ms
step:1810/2285 train_time:109744ms step_avg:60.63ms
step:1811/2285 train_time:109806ms step_avg:60.63ms
step:1812/2285 train_time:109866ms step_avg:60.63ms
step:1813/2285 train_time:109929ms step_avg:60.63ms
step:1814/2285 train_time:109988ms step_avg:60.63ms
step:1815/2285 train_time:110051ms step_avg:60.63ms
step:1816/2285 train_time:110111ms step_avg:60.63ms
step:1817/2285 train_time:110174ms step_avg:60.63ms
step:1818/2285 train_time:110235ms step_avg:60.64ms
step:1819/2285 train_time:110297ms step_avg:60.64ms
step:1820/2285 train_time:110357ms step_avg:60.64ms
step:1821/2285 train_time:110420ms step_avg:60.64ms
step:1822/2285 train_time:110479ms step_avg:60.64ms
step:1823/2285 train_time:110541ms step_avg:60.64ms
step:1824/2285 train_time:110601ms step_avg:60.64ms
step:1825/2285 train_time:110662ms step_avg:60.64ms
step:1826/2285 train_time:110722ms step_avg:60.64ms
step:1827/2285 train_time:110784ms step_avg:60.64ms
step:1828/2285 train_time:110844ms step_avg:60.64ms
step:1829/2285 train_time:110907ms step_avg:60.64ms
step:1830/2285 train_time:110967ms step_avg:60.64ms
step:1831/2285 train_time:111029ms step_avg:60.64ms
step:1832/2285 train_time:111089ms step_avg:60.64ms
step:1833/2285 train_time:111151ms step_avg:60.64ms
step:1834/2285 train_time:111211ms step_avg:60.64ms
step:1835/2285 train_time:111274ms step_avg:60.64ms
step:1836/2285 train_time:111335ms step_avg:60.64ms
step:1837/2285 train_time:111398ms step_avg:60.64ms
step:1838/2285 train_time:111458ms step_avg:60.64ms
step:1839/2285 train_time:111520ms step_avg:60.64ms
step:1840/2285 train_time:111580ms step_avg:60.64ms
step:1841/2285 train_time:111642ms step_avg:60.64ms
step:1842/2285 train_time:111702ms step_avg:60.64ms
step:1843/2285 train_time:111764ms step_avg:60.64ms
step:1844/2285 train_time:111825ms step_avg:60.64ms
step:1845/2285 train_time:111888ms step_avg:60.64ms
step:1846/2285 train_time:111948ms step_avg:60.64ms
step:1847/2285 train_time:112010ms step_avg:60.64ms
step:1848/2285 train_time:112069ms step_avg:60.64ms
step:1849/2285 train_time:112132ms step_avg:60.64ms
step:1850/2285 train_time:112192ms step_avg:60.64ms
step:1851/2285 train_time:112254ms step_avg:60.65ms
step:1852/2285 train_time:112313ms step_avg:60.64ms
step:1853/2285 train_time:112376ms step_avg:60.65ms
step:1854/2285 train_time:112436ms step_avg:60.64ms
step:1855/2285 train_time:112498ms step_avg:60.65ms
step:1856/2285 train_time:112558ms step_avg:60.65ms
step:1857/2285 train_time:112620ms step_avg:60.65ms
step:1858/2285 train_time:112680ms step_avg:60.65ms
step:1859/2285 train_time:112742ms step_avg:60.65ms
step:1860/2285 train_time:112801ms step_avg:60.65ms
step:1861/2285 train_time:112865ms step_avg:60.65ms
step:1862/2285 train_time:112925ms step_avg:60.65ms
step:1863/2285 train_time:112987ms step_avg:60.65ms
step:1864/2285 train_time:113048ms step_avg:60.65ms
step:1865/2285 train_time:113110ms step_avg:60.65ms
step:1866/2285 train_time:113170ms step_avg:60.65ms
step:1867/2285 train_time:113232ms step_avg:60.65ms
step:1868/2285 train_time:113291ms step_avg:60.65ms
step:1869/2285 train_time:113354ms step_avg:60.65ms
step:1870/2285 train_time:113414ms step_avg:60.65ms
step:1871/2285 train_time:113476ms step_avg:60.65ms
step:1872/2285 train_time:113536ms step_avg:60.65ms
step:1873/2285 train_time:113598ms step_avg:60.65ms
step:1874/2285 train_time:113657ms step_avg:60.65ms
step:1875/2285 train_time:113720ms step_avg:60.65ms
step:1876/2285 train_time:113779ms step_avg:60.65ms
step:1877/2285 train_time:113842ms step_avg:60.65ms
step:1878/2285 train_time:113901ms step_avg:60.65ms
step:1879/2285 train_time:113964ms step_avg:60.65ms
step:1880/2285 train_time:114025ms step_avg:60.65ms
step:1881/2285 train_time:114088ms step_avg:60.65ms
step:1882/2285 train_time:114148ms step_avg:60.65ms
step:1883/2285 train_time:114210ms step_avg:60.65ms
step:1884/2285 train_time:114270ms step_avg:60.65ms
step:1885/2285 train_time:114332ms step_avg:60.65ms
step:1886/2285 train_time:114392ms step_avg:60.65ms
step:1887/2285 train_time:114454ms step_avg:60.65ms
step:1888/2285 train_time:114515ms step_avg:60.65ms
step:1889/2285 train_time:114577ms step_avg:60.65ms
step:1890/2285 train_time:114637ms step_avg:60.65ms
step:1891/2285 train_time:114699ms step_avg:60.66ms
step:1892/2285 train_time:114759ms step_avg:60.65ms
step:1893/2285 train_time:114821ms step_avg:60.66ms
step:1894/2285 train_time:114881ms step_avg:60.66ms
step:1895/2285 train_time:114943ms step_avg:60.66ms
step:1896/2285 train_time:115004ms step_avg:60.66ms
step:1897/2285 train_time:115066ms step_avg:60.66ms
step:1898/2285 train_time:115126ms step_avg:60.66ms
step:1899/2285 train_time:115189ms step_avg:60.66ms
step:1900/2285 train_time:115249ms step_avg:60.66ms
step:1901/2285 train_time:115312ms step_avg:60.66ms
step:1902/2285 train_time:115372ms step_avg:60.66ms
step:1903/2285 train_time:115434ms step_avg:60.66ms
step:1904/2285 train_time:115495ms step_avg:60.66ms
step:1905/2285 train_time:115557ms step_avg:60.66ms
step:1906/2285 train_time:115618ms step_avg:60.66ms
step:1907/2285 train_time:115679ms step_avg:60.66ms
step:1908/2285 train_time:115739ms step_avg:60.66ms
step:1909/2285 train_time:115801ms step_avg:60.66ms
step:1910/2285 train_time:115862ms step_avg:60.66ms
step:1911/2285 train_time:115924ms step_avg:60.66ms
step:1912/2285 train_time:115984ms step_avg:60.66ms
step:1913/2285 train_time:116047ms step_avg:60.66ms
step:1914/2285 train_time:116107ms step_avg:60.66ms
step:1915/2285 train_time:116170ms step_avg:60.66ms
step:1916/2285 train_time:116230ms step_avg:60.66ms
step:1917/2285 train_time:116293ms step_avg:60.66ms
step:1918/2285 train_time:116352ms step_avg:60.66ms
step:1919/2285 train_time:116414ms step_avg:60.66ms
step:1920/2285 train_time:116475ms step_avg:60.66ms
step:1921/2285 train_time:116537ms step_avg:60.66ms
step:1922/2285 train_time:116597ms step_avg:60.66ms
step:1923/2285 train_time:116659ms step_avg:60.67ms
step:1924/2285 train_time:116719ms step_avg:60.66ms
step:1925/2285 train_time:116781ms step_avg:60.67ms
step:1926/2285 train_time:116841ms step_avg:60.67ms
step:1927/2285 train_time:116904ms step_avg:60.67ms
step:1928/2285 train_time:116964ms step_avg:60.67ms
step:1929/2285 train_time:117027ms step_avg:60.67ms
step:1930/2285 train_time:117087ms step_avg:60.67ms
step:1931/2285 train_time:117150ms step_avg:60.67ms
step:1932/2285 train_time:117209ms step_avg:60.67ms
step:1933/2285 train_time:117272ms step_avg:60.67ms
step:1934/2285 train_time:117331ms step_avg:60.67ms
step:1935/2285 train_time:117394ms step_avg:60.67ms
step:1936/2285 train_time:117454ms step_avg:60.67ms
step:1937/2285 train_time:117517ms step_avg:60.67ms
step:1938/2285 train_time:117577ms step_avg:60.67ms
step:1939/2285 train_time:117639ms step_avg:60.67ms
step:1940/2285 train_time:117699ms step_avg:60.67ms
step:1941/2285 train_time:117762ms step_avg:60.67ms
step:1942/2285 train_time:117822ms step_avg:60.67ms
step:1943/2285 train_time:117884ms step_avg:60.67ms
step:1944/2285 train_time:117944ms step_avg:60.67ms
step:1945/2285 train_time:118006ms step_avg:60.67ms
step:1946/2285 train_time:118066ms step_avg:60.67ms
step:1947/2285 train_time:118129ms step_avg:60.67ms
step:1948/2285 train_time:118189ms step_avg:60.67ms
step:1949/2285 train_time:118252ms step_avg:60.67ms
step:1950/2285 train_time:118312ms step_avg:60.67ms
step:1951/2285 train_time:118374ms step_avg:60.67ms
step:1952/2285 train_time:118434ms step_avg:60.67ms
step:1953/2285 train_time:118496ms step_avg:60.67ms
step:1954/2285 train_time:118557ms step_avg:60.67ms
step:1955/2285 train_time:118619ms step_avg:60.67ms
step:1956/2285 train_time:118679ms step_avg:60.67ms
step:1957/2285 train_time:118742ms step_avg:60.68ms
step:1958/2285 train_time:118802ms step_avg:60.68ms
step:1959/2285 train_time:118864ms step_avg:60.68ms
step:1960/2285 train_time:118924ms step_avg:60.68ms
step:1961/2285 train_time:118987ms step_avg:60.68ms
step:1962/2285 train_time:119047ms step_avg:60.68ms
step:1963/2285 train_time:119109ms step_avg:60.68ms
step:1964/2285 train_time:119169ms step_avg:60.68ms
step:1965/2285 train_time:119232ms step_avg:60.68ms
step:1966/2285 train_time:119292ms step_avg:60.68ms
step:1967/2285 train_time:119355ms step_avg:60.68ms
step:1968/2285 train_time:119415ms step_avg:60.68ms
step:1969/2285 train_time:119477ms step_avg:60.68ms
step:1970/2285 train_time:119538ms step_avg:60.68ms
step:1971/2285 train_time:119600ms step_avg:60.68ms
step:1972/2285 train_time:119659ms step_avg:60.68ms
step:1973/2285 train_time:119722ms step_avg:60.68ms
step:1974/2285 train_time:119782ms step_avg:60.68ms
step:1975/2285 train_time:119844ms step_avg:60.68ms
step:1976/2285 train_time:119903ms step_avg:60.68ms
step:1977/2285 train_time:119966ms step_avg:60.68ms
step:1978/2285 train_time:120026ms step_avg:60.68ms
step:1979/2285 train_time:120088ms step_avg:60.68ms
step:1980/2285 train_time:120148ms step_avg:60.68ms
step:1981/2285 train_time:120210ms step_avg:60.68ms
step:1982/2285 train_time:120271ms step_avg:60.68ms
step:1983/2285 train_time:120334ms step_avg:60.68ms
step:1984/2285 train_time:120394ms step_avg:60.68ms
step:1985/2285 train_time:120456ms step_avg:60.68ms
step:1986/2285 train_time:120516ms step_avg:60.68ms
step:1987/2285 train_time:120578ms step_avg:60.68ms
step:1988/2285 train_time:120638ms step_avg:60.68ms
step:1989/2285 train_time:120701ms step_avg:60.68ms
step:1990/2285 train_time:120761ms step_avg:60.68ms
step:1991/2285 train_time:120823ms step_avg:60.68ms
step:1992/2285 train_time:120883ms step_avg:60.68ms
step:1993/2285 train_time:120946ms step_avg:60.69ms
step:1994/2285 train_time:121006ms step_avg:60.69ms
step:1995/2285 train_time:121069ms step_avg:60.69ms
step:1996/2285 train_time:121129ms step_avg:60.69ms
step:1997/2285 train_time:121191ms step_avg:60.69ms
step:1998/2285 train_time:121251ms step_avg:60.69ms
step:1999/2285 train_time:121314ms step_avg:60.69ms
step:2000/2285 train_time:121374ms step_avg:60.69ms
step:2000/2285 val_loss:3.3190 train_time:121438ms step_avg:60.72ms
step:2001/2285 train_time:121458ms step_avg:60.70ms
step:2002/2285 train_time:121499ms step_avg:60.69ms
step:2003/2285 train_time:121562ms step_avg:60.69ms
step:2004/2285 train_time:121624ms step_avg:60.69ms
step:2005/2285 train_time:121688ms step_avg:60.69ms
step:2006/2285 train_time:121749ms step_avg:60.69ms
step:2007/2285 train_time:121810ms step_avg:60.69ms
step:2008/2285 train_time:121870ms step_avg:60.69ms
step:2009/2285 train_time:121932ms step_avg:60.69ms
step:2010/2285 train_time:121991ms step_avg:60.69ms
step:2011/2285 train_time:122053ms step_avg:60.69ms
step:2012/2285 train_time:122112ms step_avg:60.69ms
step:2013/2285 train_time:122174ms step_avg:60.69ms
step:2014/2285 train_time:122233ms step_avg:60.69ms
step:2015/2285 train_time:122295ms step_avg:60.69ms
step:2016/2285 train_time:122356ms step_avg:60.69ms
step:2017/2285 train_time:122420ms step_avg:60.69ms
step:2018/2285 train_time:122481ms step_avg:60.69ms
step:2019/2285 train_time:122545ms step_avg:60.70ms
step:2020/2285 train_time:122606ms step_avg:60.70ms
step:2021/2285 train_time:122669ms step_avg:60.70ms
step:2022/2285 train_time:122730ms step_avg:60.70ms
step:2023/2285 train_time:122792ms step_avg:60.70ms
step:2024/2285 train_time:122852ms step_avg:60.70ms
step:2025/2285 train_time:122914ms step_avg:60.70ms
step:2026/2285 train_time:122973ms step_avg:60.70ms
step:2027/2285 train_time:123035ms step_avg:60.70ms
step:2028/2285 train_time:123095ms step_avg:60.70ms
step:2029/2285 train_time:123156ms step_avg:60.70ms
step:2030/2285 train_time:123216ms step_avg:60.70ms
step:2031/2285 train_time:123278ms step_avg:60.70ms
step:2032/2285 train_time:123338ms step_avg:60.70ms
step:2033/2285 train_time:123401ms step_avg:60.70ms
step:2034/2285 train_time:123461ms step_avg:60.70ms
step:2035/2285 train_time:123524ms step_avg:60.70ms
step:2036/2285 train_time:123584ms step_avg:60.70ms
step:2037/2285 train_time:123647ms step_avg:60.70ms
step:2038/2285 train_time:123707ms step_avg:60.70ms
step:2039/2285 train_time:123770ms step_avg:60.70ms
step:2040/2285 train_time:123830ms step_avg:60.70ms
step:2041/2285 train_time:123893ms step_avg:60.70ms
step:2042/2285 train_time:123952ms step_avg:60.70ms
step:2043/2285 train_time:124014ms step_avg:60.70ms
step:2044/2285 train_time:124075ms step_avg:60.70ms
step:2045/2285 train_time:124137ms step_avg:60.70ms
step:2046/2285 train_time:124196ms step_avg:60.70ms
step:2047/2285 train_time:124258ms step_avg:60.70ms
step:2048/2285 train_time:124319ms step_avg:60.70ms
step:2049/2285 train_time:124382ms step_avg:60.70ms
step:2050/2285 train_time:124442ms step_avg:60.70ms
step:2051/2285 train_time:124505ms step_avg:60.70ms
step:2052/2285 train_time:124566ms step_avg:60.70ms
step:2053/2285 train_time:124629ms step_avg:60.71ms
step:2054/2285 train_time:124689ms step_avg:60.71ms
step:2055/2285 train_time:124752ms step_avg:60.71ms
step:2056/2285 train_time:124812ms step_avg:60.71ms
step:2057/2285 train_time:124875ms step_avg:60.71ms
step:2058/2285 train_time:124934ms step_avg:60.71ms
step:2059/2285 train_time:124996ms step_avg:60.71ms
step:2060/2285 train_time:125056ms step_avg:60.71ms
step:2061/2285 train_time:125118ms step_avg:60.71ms
step:2062/2285 train_time:125178ms step_avg:60.71ms
step:2063/2285 train_time:125240ms step_avg:60.71ms
step:2064/2285 train_time:125300ms step_avg:60.71ms
step:2065/2285 train_time:125363ms step_avg:60.71ms
step:2066/2285 train_time:125423ms step_avg:60.71ms
step:2067/2285 train_time:125486ms step_avg:60.71ms
step:2068/2285 train_time:125546ms step_avg:60.71ms
step:2069/2285 train_time:125610ms step_avg:60.71ms
step:2070/2285 train_time:125670ms step_avg:60.71ms
step:2071/2285 train_time:125732ms step_avg:60.71ms
step:2072/2285 train_time:125792ms step_avg:60.71ms
step:2073/2285 train_time:125854ms step_avg:60.71ms
step:2074/2285 train_time:125914ms step_avg:60.71ms
step:2075/2285 train_time:125977ms step_avg:60.71ms
step:2076/2285 train_time:126037ms step_avg:60.71ms
step:2077/2285 train_time:126099ms step_avg:60.71ms
step:2078/2285 train_time:126159ms step_avg:60.71ms
step:2079/2285 train_time:126221ms step_avg:60.71ms
step:2080/2285 train_time:126282ms step_avg:60.71ms
step:2081/2285 train_time:126344ms step_avg:60.71ms
step:2082/2285 train_time:126404ms step_avg:60.71ms
step:2083/2285 train_time:126466ms step_avg:60.71ms
step:2084/2285 train_time:126526ms step_avg:60.71ms
step:2085/2285 train_time:126589ms step_avg:60.71ms
step:2086/2285 train_time:126649ms step_avg:60.71ms
step:2087/2285 train_time:126712ms step_avg:60.71ms
step:2088/2285 train_time:126772ms step_avg:60.71ms
step:2089/2285 train_time:126835ms step_avg:60.72ms
step:2090/2285 train_time:126894ms step_avg:60.71ms
step:2091/2285 train_time:126956ms step_avg:60.72ms
step:2092/2285 train_time:127016ms step_avg:60.72ms
step:2093/2285 train_time:127079ms step_avg:60.72ms
step:2094/2285 train_time:127138ms step_avg:60.72ms
step:2095/2285 train_time:127201ms step_avg:60.72ms
step:2096/2285 train_time:127261ms step_avg:60.72ms
step:2097/2285 train_time:127324ms step_avg:60.72ms
step:2098/2285 train_time:127384ms step_avg:60.72ms
step:2099/2285 train_time:127447ms step_avg:60.72ms
step:2100/2285 train_time:127507ms step_avg:60.72ms
step:2101/2285 train_time:127569ms step_avg:60.72ms
step:2102/2285 train_time:127630ms step_avg:60.72ms
step:2103/2285 train_time:127692ms step_avg:60.72ms
step:2104/2285 train_time:127752ms step_avg:60.72ms
step:2105/2285 train_time:127814ms step_avg:60.72ms
step:2106/2285 train_time:127875ms step_avg:60.72ms
step:2107/2285 train_time:127937ms step_avg:60.72ms
step:2108/2285 train_time:127997ms step_avg:60.72ms
step:2109/2285 train_time:128059ms step_avg:60.72ms
step:2110/2285 train_time:128120ms step_avg:60.72ms
step:2111/2285 train_time:128182ms step_avg:60.72ms
step:2112/2285 train_time:128241ms step_avg:60.72ms
step:2113/2285 train_time:128304ms step_avg:60.72ms
step:2114/2285 train_time:128364ms step_avg:60.72ms
step:2115/2285 train_time:128426ms step_avg:60.72ms
step:2116/2285 train_time:128486ms step_avg:60.72ms
step:2117/2285 train_time:128549ms step_avg:60.72ms
step:2118/2285 train_time:128609ms step_avg:60.72ms
step:2119/2285 train_time:128671ms step_avg:60.72ms
step:2120/2285 train_time:128731ms step_avg:60.72ms
step:2121/2285 train_time:128794ms step_avg:60.72ms
step:2122/2285 train_time:128854ms step_avg:60.72ms
step:2123/2285 train_time:128917ms step_avg:60.72ms
step:2124/2285 train_time:128977ms step_avg:60.72ms
step:2125/2285 train_time:129039ms step_avg:60.72ms
step:2126/2285 train_time:129100ms step_avg:60.72ms
step:2127/2285 train_time:129163ms step_avg:60.73ms
step:2128/2285 train_time:129223ms step_avg:60.72ms
step:2129/2285 train_time:129285ms step_avg:60.73ms
step:2130/2285 train_time:129344ms step_avg:60.72ms
step:2131/2285 train_time:129407ms step_avg:60.73ms
step:2132/2285 train_time:129467ms step_avg:60.73ms
step:2133/2285 train_time:129530ms step_avg:60.73ms
step:2134/2285 train_time:129591ms step_avg:60.73ms
step:2135/2285 train_time:129652ms step_avg:60.73ms
step:2136/2285 train_time:129713ms step_avg:60.73ms
step:2137/2285 train_time:129775ms step_avg:60.73ms
step:2138/2285 train_time:129835ms step_avg:60.73ms
step:2139/2285 train_time:129898ms step_avg:60.73ms
step:2140/2285 train_time:129958ms step_avg:60.73ms
step:2141/2285 train_time:130020ms step_avg:60.73ms
step:2142/2285 train_time:130080ms step_avg:60.73ms
step:2143/2285 train_time:130142ms step_avg:60.73ms
step:2144/2285 train_time:130202ms step_avg:60.73ms
step:2145/2285 train_time:130265ms step_avg:60.73ms
step:2146/2285 train_time:130325ms step_avg:60.73ms
step:2147/2285 train_time:130387ms step_avg:60.73ms
step:2148/2285 train_time:130447ms step_avg:60.73ms
step:2149/2285 train_time:130509ms step_avg:60.73ms
step:2150/2285 train_time:130569ms step_avg:60.73ms
step:2151/2285 train_time:130632ms step_avg:60.73ms
step:2152/2285 train_time:130692ms step_avg:60.73ms
step:2153/2285 train_time:130754ms step_avg:60.73ms
step:2154/2285 train_time:130815ms step_avg:60.73ms
step:2155/2285 train_time:130877ms step_avg:60.73ms
step:2156/2285 train_time:130937ms step_avg:60.73ms
step:2157/2285 train_time:130999ms step_avg:60.73ms
step:2158/2285 train_time:131060ms step_avg:60.73ms
step:2159/2285 train_time:131123ms step_avg:60.73ms
step:2160/2285 train_time:131182ms step_avg:60.73ms
step:2161/2285 train_time:131245ms step_avg:60.73ms
step:2162/2285 train_time:131305ms step_avg:60.73ms
step:2163/2285 train_time:131368ms step_avg:60.73ms
step:2164/2285 train_time:131428ms step_avg:60.73ms
step:2165/2285 train_time:131490ms step_avg:60.73ms
step:2166/2285 train_time:131550ms step_avg:60.73ms
step:2167/2285 train_time:131612ms step_avg:60.73ms
step:2168/2285 train_time:131672ms step_avg:60.73ms
step:2169/2285 train_time:131734ms step_avg:60.73ms
step:2170/2285 train_time:131794ms step_avg:60.73ms
step:2171/2285 train_time:131856ms step_avg:60.74ms
step:2172/2285 train_time:131917ms step_avg:60.74ms
step:2173/2285 train_time:131979ms step_avg:60.74ms
step:2174/2285 train_time:132039ms step_avg:60.74ms
step:2175/2285 train_time:132102ms step_avg:60.74ms
step:2176/2285 train_time:132163ms step_avg:60.74ms
step:2177/2285 train_time:132225ms step_avg:60.74ms
step:2178/2285 train_time:132285ms step_avg:60.74ms
step:2179/2285 train_time:132347ms step_avg:60.74ms
step:2180/2285 train_time:132407ms step_avg:60.74ms
step:2181/2285 train_time:132470ms step_avg:60.74ms
step:2182/2285 train_time:132530ms step_avg:60.74ms
step:2183/2285 train_time:132592ms step_avg:60.74ms
step:2184/2285 train_time:132652ms step_avg:60.74ms
step:2185/2285 train_time:132715ms step_avg:60.74ms
step:2186/2285 train_time:132775ms step_avg:60.74ms
step:2187/2285 train_time:132837ms step_avg:60.74ms
step:2188/2285 train_time:132898ms step_avg:60.74ms
step:2189/2285 train_time:132960ms step_avg:60.74ms
step:2190/2285 train_time:133020ms step_avg:60.74ms
step:2191/2285 train_time:133083ms step_avg:60.74ms
step:2192/2285 train_time:133143ms step_avg:60.74ms
step:2193/2285 train_time:133205ms step_avg:60.74ms
step:2194/2285 train_time:133266ms step_avg:60.74ms
step:2195/2285 train_time:133328ms step_avg:60.74ms
step:2196/2285 train_time:133388ms step_avg:60.74ms
step:2197/2285 train_time:133450ms step_avg:60.74ms
step:2198/2285 train_time:133510ms step_avg:60.74ms
step:2199/2285 train_time:133573ms step_avg:60.74ms
step:2200/2285 train_time:133632ms step_avg:60.74ms
step:2201/2285 train_time:133694ms step_avg:60.74ms
step:2202/2285 train_time:133754ms step_avg:60.74ms
step:2203/2285 train_time:133817ms step_avg:60.74ms
step:2204/2285 train_time:133878ms step_avg:60.74ms
step:2205/2285 train_time:133940ms step_avg:60.74ms
step:2206/2285 train_time:134000ms step_avg:60.74ms
step:2207/2285 train_time:134063ms step_avg:60.74ms
step:2208/2285 train_time:134123ms step_avg:60.74ms
step:2209/2285 train_time:134186ms step_avg:60.75ms
step:2210/2285 train_time:134246ms step_avg:60.74ms
step:2211/2285 train_time:134309ms step_avg:60.75ms
step:2212/2285 train_time:134369ms step_avg:60.75ms
step:2213/2285 train_time:134432ms step_avg:60.75ms
step:2214/2285 train_time:134492ms step_avg:60.75ms
step:2215/2285 train_time:134555ms step_avg:60.75ms
step:2216/2285 train_time:134614ms step_avg:60.75ms
step:2217/2285 train_time:134677ms step_avg:60.75ms
step:2218/2285 train_time:134737ms step_avg:60.75ms
step:2219/2285 train_time:134799ms step_avg:60.75ms
step:2220/2285 train_time:134859ms step_avg:60.75ms
step:2221/2285 train_time:134922ms step_avg:60.75ms
step:2222/2285 train_time:134982ms step_avg:60.75ms
step:2223/2285 train_time:135044ms step_avg:60.75ms
step:2224/2285 train_time:135105ms step_avg:60.75ms
step:2225/2285 train_time:135167ms step_avg:60.75ms
step:2226/2285 train_time:135227ms step_avg:60.75ms
step:2227/2285 train_time:135289ms step_avg:60.75ms
step:2228/2285 train_time:135349ms step_avg:60.75ms
step:2229/2285 train_time:135411ms step_avg:60.75ms
step:2230/2285 train_time:135472ms step_avg:60.75ms
step:2231/2285 train_time:135534ms step_avg:60.75ms
step:2232/2285 train_time:135594ms step_avg:60.75ms
step:2233/2285 train_time:135656ms step_avg:60.75ms
step:2234/2285 train_time:135716ms step_avg:60.75ms
step:2235/2285 train_time:135778ms step_avg:60.75ms
step:2236/2285 train_time:135838ms step_avg:60.75ms
step:2237/2285 train_time:135901ms step_avg:60.75ms
step:2238/2285 train_time:135961ms step_avg:60.75ms
step:2239/2285 train_time:136023ms step_avg:60.75ms
step:2240/2285 train_time:136083ms step_avg:60.75ms
step:2241/2285 train_time:136145ms step_avg:60.75ms
step:2242/2285 train_time:136206ms step_avg:60.75ms
step:2243/2285 train_time:136268ms step_avg:60.75ms
step:2244/2285 train_time:136328ms step_avg:60.75ms
step:2245/2285 train_time:136391ms step_avg:60.75ms
step:2246/2285 train_time:136450ms step_avg:60.75ms
step:2247/2285 train_time:136513ms step_avg:60.75ms
step:2248/2285 train_time:136573ms step_avg:60.75ms
step:2249/2285 train_time:136635ms step_avg:60.75ms
step:2250/2285 train_time:136695ms step_avg:60.75ms
step:2250/2285 val_loss:3.2836 train_time:136759ms step_avg:60.78ms
step:2251/2285 train_time:136778ms step_avg:60.76ms
step:2252/2285 train_time:136819ms step_avg:60.75ms
step:2253/2285 train_time:136884ms step_avg:60.76ms
step:2254/2285 train_time:136945ms step_avg:60.76ms
step:2255/2285 train_time:137007ms step_avg:60.76ms
step:2256/2285 train_time:137067ms step_avg:60.76ms
step:2257/2285 train_time:137128ms step_avg:60.76ms
step:2258/2285 train_time:137188ms step_avg:60.76ms
step:2259/2285 train_time:137250ms step_avg:60.76ms
step:2260/2285 train_time:137309ms step_avg:60.76ms
step:2261/2285 train_time:137372ms step_avg:60.76ms
step:2262/2285 train_time:137432ms step_avg:60.76ms
step:2263/2285 train_time:137494ms step_avg:60.76ms
step:2264/2285 train_time:137553ms step_avg:60.76ms
step:2265/2285 train_time:137615ms step_avg:60.76ms
step:2266/2285 train_time:137675ms step_avg:60.76ms
step:2267/2285 train_time:137740ms step_avg:60.76ms
step:2268/2285 train_time:137802ms step_avg:60.76ms
step:2269/2285 train_time:137864ms step_avg:60.76ms
step:2270/2285 train_time:137925ms step_avg:60.76ms
step:2271/2285 train_time:137988ms step_avg:60.76ms
step:2272/2285 train_time:138048ms step_avg:60.76ms
step:2273/2285 train_time:138110ms step_avg:60.76ms
step:2274/2285 train_time:138169ms step_avg:60.76ms
step:2275/2285 train_time:138231ms step_avg:60.76ms
step:2276/2285 train_time:138290ms step_avg:60.76ms
step:2277/2285 train_time:138352ms step_avg:60.76ms
step:2278/2285 train_time:138412ms step_avg:60.76ms
step:2279/2285 train_time:138474ms step_avg:60.76ms
step:2280/2285 train_time:138534ms step_avg:60.76ms
step:2281/2285 train_time:138596ms step_avg:60.76ms
step:2282/2285 train_time:138656ms step_avg:60.76ms
step:2283/2285 train_time:138720ms step_avg:60.76ms
step:2284/2285 train_time:138780ms step_avg:60.76ms
step:2285/2285 train_time:138843ms step_avg:60.76ms
step:2285/2285 val_loss:3.2776 train_time:138905ms step_avg:60.79ms
peak memory allocated: 29626 MiB reserved: 50528 MiB

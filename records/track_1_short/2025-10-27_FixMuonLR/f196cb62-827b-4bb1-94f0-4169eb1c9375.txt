import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2285
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 01:51:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:106ms step_avg:105.97ms
step:2/2285 train_time:128ms step_avg:64.03ms
step:3/2285 train_time:166ms step_avg:55.27ms
step:4/2285 train_time:222ms step_avg:55.49ms
step:5/2285 train_time:281ms step_avg:56.27ms
step:6/2285 train_time:339ms step_avg:56.55ms
step:7/2285 train_time:400ms step_avg:57.17ms
step:8/2285 train_time:458ms step_avg:57.31ms
step:9/2285 train_time:519ms step_avg:57.67ms
step:10/2285 train_time:578ms step_avg:57.76ms
step:11/2285 train_time:638ms step_avg:58.04ms
step:12/2285 train_time:697ms step_avg:58.08ms
step:13/2285 train_time:758ms step_avg:58.30ms
step:14/2285 train_time:816ms step_avg:58.32ms
step:15/2285 train_time:877ms step_avg:58.46ms
step:16/2285 train_time:935ms step_avg:58.47ms
step:17/2285 train_time:998ms step_avg:58.68ms
step:18/2285 train_time:1060ms step_avg:58.87ms
step:19/2285 train_time:1125ms step_avg:59.20ms
step:20/2285 train_time:1185ms step_avg:59.26ms
step:21/2285 train_time:1247ms step_avg:59.38ms
step:22/2285 train_time:1306ms step_avg:59.37ms
step:23/2285 train_time:1367ms step_avg:59.44ms
step:24/2285 train_time:1426ms step_avg:59.42ms
step:25/2285 train_time:1487ms step_avg:59.47ms
step:26/2285 train_time:1545ms step_avg:59.43ms
step:27/2285 train_time:1606ms step_avg:59.48ms
step:28/2285 train_time:1665ms step_avg:59.45ms
step:29/2285 train_time:1727ms step_avg:59.54ms
step:30/2285 train_time:1785ms step_avg:59.51ms
step:31/2285 train_time:1847ms step_avg:59.57ms
step:32/2285 train_time:1906ms step_avg:59.55ms
step:33/2285 train_time:1967ms step_avg:59.61ms
step:34/2285 train_time:2027ms step_avg:59.61ms
step:35/2285 train_time:2088ms step_avg:59.67ms
step:36/2285 train_time:2148ms step_avg:59.67ms
step:37/2285 train_time:2211ms step_avg:59.74ms
step:38/2285 train_time:2270ms step_avg:59.73ms
step:39/2285 train_time:2331ms step_avg:59.78ms
step:40/2285 train_time:2390ms step_avg:59.76ms
step:41/2285 train_time:2452ms step_avg:59.80ms
step:42/2285 train_time:2511ms step_avg:59.78ms
step:43/2285 train_time:2573ms step_avg:59.83ms
step:44/2285 train_time:2632ms step_avg:59.81ms
step:45/2285 train_time:2694ms step_avg:59.87ms
step:46/2285 train_time:2753ms step_avg:59.85ms
step:47/2285 train_time:2815ms step_avg:59.89ms
step:48/2285 train_time:2874ms step_avg:59.87ms
step:49/2285 train_time:2935ms step_avg:59.90ms
step:50/2285 train_time:2994ms step_avg:59.88ms
step:51/2285 train_time:3056ms step_avg:59.92ms
step:52/2285 train_time:3115ms step_avg:59.91ms
step:53/2285 train_time:3177ms step_avg:59.95ms
step:54/2285 train_time:3237ms step_avg:59.94ms
step:55/2285 train_time:3299ms step_avg:59.97ms
step:56/2285 train_time:3357ms step_avg:59.95ms
step:57/2285 train_time:3419ms step_avg:59.98ms
step:58/2285 train_time:3477ms step_avg:59.95ms
step:59/2285 train_time:3539ms step_avg:59.98ms
step:60/2285 train_time:3598ms step_avg:59.97ms
step:61/2285 train_time:3660ms step_avg:60.00ms
step:62/2285 train_time:3719ms step_avg:59.98ms
step:63/2285 train_time:3780ms step_avg:60.00ms
step:64/2285 train_time:3839ms step_avg:59.99ms
step:65/2285 train_time:3900ms step_avg:60.01ms
step:66/2285 train_time:3959ms step_avg:59.98ms
step:67/2285 train_time:4021ms step_avg:60.01ms
step:68/2285 train_time:4079ms step_avg:59.99ms
step:69/2285 train_time:4141ms step_avg:60.01ms
step:70/2285 train_time:4200ms step_avg:60.00ms
step:71/2285 train_time:4261ms step_avg:60.01ms
step:72/2285 train_time:4320ms step_avg:60.00ms
step:73/2285 train_time:4382ms step_avg:60.03ms
step:74/2285 train_time:4441ms step_avg:60.01ms
step:75/2285 train_time:4502ms step_avg:60.03ms
step:76/2285 train_time:4562ms step_avg:60.02ms
step:77/2285 train_time:4623ms step_avg:60.04ms
step:78/2285 train_time:4681ms step_avg:60.02ms
step:79/2285 train_time:4743ms step_avg:60.04ms
step:80/2285 train_time:4801ms step_avg:60.02ms
step:81/2285 train_time:4863ms step_avg:60.03ms
step:82/2285 train_time:4921ms step_avg:60.02ms
step:83/2285 train_time:4982ms step_avg:60.02ms
step:84/2285 train_time:5041ms step_avg:60.01ms
step:85/2285 train_time:5101ms step_avg:60.02ms
step:86/2285 train_time:5161ms step_avg:60.01ms
step:87/2285 train_time:5221ms step_avg:60.01ms
step:88/2285 train_time:5280ms step_avg:60.00ms
step:89/2285 train_time:5342ms step_avg:60.02ms
step:90/2285 train_time:5400ms step_avg:60.00ms
step:91/2285 train_time:5461ms step_avg:60.01ms
step:92/2285 train_time:5520ms step_avg:60.00ms
step:93/2285 train_time:5581ms step_avg:60.01ms
step:94/2285 train_time:5639ms step_avg:59.99ms
step:95/2285 train_time:5701ms step_avg:60.01ms
step:96/2285 train_time:5759ms step_avg:59.99ms
step:97/2285 train_time:5821ms step_avg:60.01ms
step:98/2285 train_time:5879ms step_avg:59.99ms
step:99/2285 train_time:5940ms step_avg:60.00ms
step:100/2285 train_time:5999ms step_avg:59.99ms
step:101/2285 train_time:6060ms step_avg:60.00ms
step:102/2285 train_time:6119ms step_avg:59.99ms
step:103/2285 train_time:6180ms step_avg:60.00ms
step:104/2285 train_time:6238ms step_avg:59.98ms
step:105/2285 train_time:6300ms step_avg:60.00ms
step:106/2285 train_time:6359ms step_avg:59.99ms
step:107/2285 train_time:6420ms step_avg:60.00ms
step:108/2285 train_time:6479ms step_avg:59.99ms
step:109/2285 train_time:6541ms step_avg:60.01ms
step:110/2285 train_time:6599ms step_avg:59.99ms
step:111/2285 train_time:6660ms step_avg:60.00ms
step:112/2285 train_time:6719ms step_avg:59.99ms
step:113/2285 train_time:6780ms step_avg:60.00ms
step:114/2285 train_time:6839ms step_avg:59.99ms
step:115/2285 train_time:6900ms step_avg:60.00ms
step:116/2285 train_time:6958ms step_avg:59.99ms
step:117/2285 train_time:7020ms step_avg:60.00ms
step:118/2285 train_time:7078ms step_avg:59.99ms
step:119/2285 train_time:7140ms step_avg:60.00ms
step:120/2285 train_time:7199ms step_avg:59.99ms
step:121/2285 train_time:7260ms step_avg:60.00ms
step:122/2285 train_time:7318ms step_avg:59.99ms
step:123/2285 train_time:7380ms step_avg:60.00ms
step:124/2285 train_time:7439ms step_avg:59.99ms
step:125/2285 train_time:7501ms step_avg:60.00ms
step:126/2285 train_time:7559ms step_avg:59.99ms
step:127/2285 train_time:7621ms step_avg:60.01ms
step:128/2285 train_time:7680ms step_avg:60.00ms
step:129/2285 train_time:7741ms step_avg:60.01ms
step:130/2285 train_time:7799ms step_avg:60.00ms
step:131/2285 train_time:7860ms step_avg:60.00ms
step:132/2285 train_time:7919ms step_avg:59.99ms
step:133/2285 train_time:7980ms step_avg:60.00ms
step:134/2285 train_time:8038ms step_avg:59.99ms
step:135/2285 train_time:8099ms step_avg:60.00ms
step:136/2285 train_time:8158ms step_avg:59.99ms
step:137/2285 train_time:8219ms step_avg:59.99ms
step:138/2285 train_time:8278ms step_avg:59.99ms
step:139/2285 train_time:8341ms step_avg:60.00ms
step:140/2285 train_time:8399ms step_avg:59.99ms
step:141/2285 train_time:8460ms step_avg:60.00ms
step:142/2285 train_time:8519ms step_avg:59.99ms
step:143/2285 train_time:8580ms step_avg:60.00ms
step:144/2285 train_time:8639ms step_avg:59.99ms
step:145/2285 train_time:8701ms step_avg:60.01ms
step:146/2285 train_time:8759ms step_avg:59.99ms
step:147/2285 train_time:8820ms step_avg:60.00ms
step:148/2285 train_time:8879ms step_avg:60.00ms
step:149/2285 train_time:8940ms step_avg:60.00ms
step:150/2285 train_time:8999ms step_avg:59.99ms
step:151/2285 train_time:9060ms step_avg:60.00ms
step:152/2285 train_time:9118ms step_avg:59.99ms
step:153/2285 train_time:9180ms step_avg:60.00ms
step:154/2285 train_time:9238ms step_avg:59.99ms
step:155/2285 train_time:9299ms step_avg:60.00ms
step:156/2285 train_time:9358ms step_avg:59.99ms
step:157/2285 train_time:9419ms step_avg:59.99ms
step:158/2285 train_time:9478ms step_avg:59.99ms
step:159/2285 train_time:9539ms step_avg:59.99ms
step:160/2285 train_time:9598ms step_avg:59.99ms
step:161/2285 train_time:9659ms step_avg:60.00ms
step:162/2285 train_time:9718ms step_avg:59.99ms
step:163/2285 train_time:9779ms step_avg:59.99ms
step:164/2285 train_time:9838ms step_avg:59.99ms
step:165/2285 train_time:9899ms step_avg:59.99ms
step:166/2285 train_time:9958ms step_avg:59.99ms
step:167/2285 train_time:10019ms step_avg:59.99ms
step:168/2285 train_time:10078ms step_avg:59.99ms
step:169/2285 train_time:10140ms step_avg:60.00ms
step:170/2285 train_time:10198ms step_avg:59.99ms
step:171/2285 train_time:10259ms step_avg:60.00ms
step:172/2285 train_time:10318ms step_avg:59.99ms
step:173/2285 train_time:10379ms step_avg:59.99ms
step:174/2285 train_time:10437ms step_avg:59.98ms
step:175/2285 train_time:10499ms step_avg:59.99ms
step:176/2285 train_time:10558ms step_avg:59.99ms
step:177/2285 train_time:10619ms step_avg:59.99ms
step:178/2285 train_time:10678ms step_avg:59.99ms
step:179/2285 train_time:10740ms step_avg:60.00ms
step:180/2285 train_time:10798ms step_avg:59.99ms
step:181/2285 train_time:10859ms step_avg:60.00ms
step:182/2285 train_time:10918ms step_avg:59.99ms
step:183/2285 train_time:10979ms step_avg:59.99ms
step:184/2285 train_time:11038ms step_avg:59.99ms
step:185/2285 train_time:11099ms step_avg:60.00ms
step:186/2285 train_time:11158ms step_avg:59.99ms
step:187/2285 train_time:11219ms step_avg:60.00ms
step:188/2285 train_time:11278ms step_avg:59.99ms
step:189/2285 train_time:11339ms step_avg:59.99ms
step:190/2285 train_time:11398ms step_avg:59.99ms
step:191/2285 train_time:11459ms step_avg:59.99ms
step:192/2285 train_time:11518ms step_avg:59.99ms
step:193/2285 train_time:11579ms step_avg:59.99ms
step:194/2285 train_time:11638ms step_avg:59.99ms
step:195/2285 train_time:11699ms step_avg:60.00ms
step:196/2285 train_time:11758ms step_avg:59.99ms
step:197/2285 train_time:11819ms step_avg:60.00ms
step:198/2285 train_time:11878ms step_avg:59.99ms
step:199/2285 train_time:11939ms step_avg:60.00ms
step:200/2285 train_time:11998ms step_avg:59.99ms
step:201/2285 train_time:12059ms step_avg:60.00ms
step:202/2285 train_time:12118ms step_avg:59.99ms
step:203/2285 train_time:12179ms step_avg:59.99ms
step:204/2285 train_time:12238ms step_avg:59.99ms
step:205/2285 train_time:12299ms step_avg:60.00ms
step:206/2285 train_time:12358ms step_avg:59.99ms
step:207/2285 train_time:12419ms step_avg:59.99ms
step:208/2285 train_time:12477ms step_avg:59.98ms
step:209/2285 train_time:12538ms step_avg:59.99ms
step:210/2285 train_time:12597ms step_avg:59.99ms
step:211/2285 train_time:12658ms step_avg:59.99ms
step:212/2285 train_time:12717ms step_avg:59.98ms
step:213/2285 train_time:12778ms step_avg:59.99ms
step:214/2285 train_time:12837ms step_avg:59.98ms
step:215/2285 train_time:12899ms step_avg:59.99ms
step:216/2285 train_time:12957ms step_avg:59.99ms
step:217/2285 train_time:13019ms step_avg:60.00ms
step:218/2285 train_time:13078ms step_avg:59.99ms
step:219/2285 train_time:13141ms step_avg:60.00ms
step:220/2285 train_time:13198ms step_avg:59.99ms
step:221/2285 train_time:13259ms step_avg:60.00ms
step:222/2285 train_time:13318ms step_avg:59.99ms
step:223/2285 train_time:13379ms step_avg:59.99ms
step:224/2285 train_time:13437ms step_avg:59.99ms
step:225/2285 train_time:13499ms step_avg:59.99ms
step:226/2285 train_time:13558ms step_avg:59.99ms
step:227/2285 train_time:13619ms step_avg:59.99ms
step:228/2285 train_time:13677ms step_avg:59.99ms
step:229/2285 train_time:13739ms step_avg:60.00ms
step:230/2285 train_time:13798ms step_avg:59.99ms
step:231/2285 train_time:13859ms step_avg:60.00ms
step:232/2285 train_time:13917ms step_avg:59.99ms
step:233/2285 train_time:13979ms step_avg:59.99ms
step:234/2285 train_time:14037ms step_avg:59.99ms
step:235/2285 train_time:14098ms step_avg:59.99ms
step:236/2285 train_time:14157ms step_avg:59.99ms
step:237/2285 train_time:14218ms step_avg:59.99ms
step:238/2285 train_time:14277ms step_avg:59.99ms
step:239/2285 train_time:14337ms step_avg:59.99ms
step:240/2285 train_time:14396ms step_avg:59.98ms
step:241/2285 train_time:14458ms step_avg:59.99ms
step:242/2285 train_time:14516ms step_avg:59.99ms
step:243/2285 train_time:14577ms step_avg:59.99ms
step:244/2285 train_time:14636ms step_avg:59.99ms
step:245/2285 train_time:14698ms step_avg:59.99ms
step:246/2285 train_time:14757ms step_avg:59.99ms
step:247/2285 train_time:14818ms step_avg:59.99ms
step:248/2285 train_time:14877ms step_avg:59.99ms
step:249/2285 train_time:14938ms step_avg:59.99ms
step:250/2285 train_time:14998ms step_avg:59.99ms
step:250/2285 val_loss:4.0701 train_time:15061ms step_avg:60.24ms
step:251/2285 train_time:15080ms step_avg:60.08ms
step:252/2285 train_time:15123ms step_avg:60.01ms
step:253/2285 train_time:15187ms step_avg:60.03ms
step:254/2285 train_time:15251ms step_avg:60.04ms
step:255/2285 train_time:15315ms step_avg:60.06ms
step:256/2285 train_time:15373ms step_avg:60.05ms
step:257/2285 train_time:15434ms step_avg:60.05ms
step:258/2285 train_time:15492ms step_avg:60.05ms
step:259/2285 train_time:15553ms step_avg:60.05ms
step:260/2285 train_time:15611ms step_avg:60.04ms
step:261/2285 train_time:15671ms step_avg:60.04ms
step:262/2285 train_time:15728ms step_avg:60.03ms
step:263/2285 train_time:15788ms step_avg:60.03ms
step:264/2285 train_time:15847ms step_avg:60.03ms
step:265/2285 train_time:15906ms step_avg:60.02ms
step:266/2285 train_time:15964ms step_avg:60.01ms
step:267/2285 train_time:16025ms step_avg:60.02ms
step:268/2285 train_time:16084ms step_avg:60.01ms
step:269/2285 train_time:16146ms step_avg:60.02ms
step:270/2285 train_time:16206ms step_avg:60.02ms
step:271/2285 train_time:16269ms step_avg:60.03ms
step:272/2285 train_time:16328ms step_avg:60.03ms
step:273/2285 train_time:16389ms step_avg:60.03ms
step:274/2285 train_time:16447ms step_avg:60.03ms
step:275/2285 train_time:16508ms step_avg:60.03ms
step:276/2285 train_time:16566ms step_avg:60.02ms
step:277/2285 train_time:16627ms step_avg:60.02ms
step:278/2285 train_time:16685ms step_avg:60.02ms
step:279/2285 train_time:16746ms step_avg:60.02ms
step:280/2285 train_time:16804ms step_avg:60.01ms
step:281/2285 train_time:16864ms step_avg:60.02ms
step:282/2285 train_time:16923ms step_avg:60.01ms
step:283/2285 train_time:16983ms step_avg:60.01ms
step:284/2285 train_time:17042ms step_avg:60.01ms
step:285/2285 train_time:17103ms step_avg:60.01ms
step:286/2285 train_time:17162ms step_avg:60.01ms
step:287/2285 train_time:17224ms step_avg:60.01ms
step:288/2285 train_time:17283ms step_avg:60.01ms
step:289/2285 train_time:17344ms step_avg:60.01ms
step:290/2285 train_time:17403ms step_avg:60.01ms
step:291/2285 train_time:17465ms step_avg:60.02ms
step:292/2285 train_time:17523ms step_avg:60.01ms
step:293/2285 train_time:17584ms step_avg:60.01ms
step:294/2285 train_time:17642ms step_avg:60.01ms
step:295/2285 train_time:17703ms step_avg:60.01ms
step:296/2285 train_time:17761ms step_avg:60.00ms
step:297/2285 train_time:17822ms step_avg:60.01ms
step:298/2285 train_time:17880ms step_avg:60.00ms
step:299/2285 train_time:17940ms step_avg:60.00ms
step:300/2285 train_time:17999ms step_avg:60.00ms
step:301/2285 train_time:18060ms step_avg:60.00ms
step:302/2285 train_time:18119ms step_avg:60.00ms
step:303/2285 train_time:18180ms step_avg:60.00ms
step:304/2285 train_time:18242ms step_avg:60.01ms
step:305/2285 train_time:18301ms step_avg:60.00ms
step:306/2285 train_time:18361ms step_avg:60.00ms
step:307/2285 train_time:18422ms step_avg:60.01ms
step:308/2285 train_time:18481ms step_avg:60.00ms
step:309/2285 train_time:18543ms step_avg:60.01ms
step:310/2285 train_time:18601ms step_avg:60.00ms
step:311/2285 train_time:18662ms step_avg:60.01ms
step:312/2285 train_time:18720ms step_avg:60.00ms
step:313/2285 train_time:18781ms step_avg:60.00ms
step:314/2285 train_time:18842ms step_avg:60.00ms
step:315/2285 train_time:18900ms step_avg:60.00ms
step:316/2285 train_time:18959ms step_avg:60.00ms
step:317/2285 train_time:19020ms step_avg:60.00ms
step:318/2285 train_time:19079ms step_avg:60.00ms
step:319/2285 train_time:19140ms step_avg:60.00ms
step:320/2285 train_time:19199ms step_avg:60.00ms
step:321/2285 train_time:19261ms step_avg:60.00ms
step:322/2285 train_time:19320ms step_avg:60.00ms
step:323/2285 train_time:19382ms step_avg:60.00ms
step:324/2285 train_time:19441ms step_avg:60.00ms
step:325/2285 train_time:19503ms step_avg:60.01ms
step:326/2285 train_time:19562ms step_avg:60.00ms
step:327/2285 train_time:19622ms step_avg:60.01ms
step:328/2285 train_time:19681ms step_avg:60.00ms
step:329/2285 train_time:19741ms step_avg:60.00ms
step:330/2285 train_time:19800ms step_avg:60.00ms
step:331/2285 train_time:19861ms step_avg:60.00ms
step:332/2285 train_time:19919ms step_avg:60.00ms
step:333/2285 train_time:19980ms step_avg:60.00ms
step:334/2285 train_time:20039ms step_avg:60.00ms
step:335/2285 train_time:20100ms step_avg:60.00ms
step:336/2285 train_time:20159ms step_avg:60.00ms
step:337/2285 train_time:20220ms step_avg:60.00ms
step:338/2285 train_time:20279ms step_avg:60.00ms
step:339/2285 train_time:20342ms step_avg:60.01ms
step:340/2285 train_time:20400ms step_avg:60.00ms
step:341/2285 train_time:20461ms step_avg:60.00ms
step:342/2285 train_time:20520ms step_avg:60.00ms
step:343/2285 train_time:20581ms step_avg:60.00ms
step:344/2285 train_time:20640ms step_avg:60.00ms
step:345/2285 train_time:20702ms step_avg:60.01ms
step:346/2285 train_time:20761ms step_avg:60.00ms
step:347/2285 train_time:20822ms step_avg:60.01ms
step:348/2285 train_time:20881ms step_avg:60.00ms
step:349/2285 train_time:20944ms step_avg:60.01ms
step:350/2285 train_time:21001ms step_avg:60.00ms
step:351/2285 train_time:21061ms step_avg:60.00ms
step:352/2285 train_time:21120ms step_avg:60.00ms
step:353/2285 train_time:21181ms step_avg:60.00ms
step:354/2285 train_time:21239ms step_avg:60.00ms
step:355/2285 train_time:21300ms step_avg:60.00ms
step:356/2285 train_time:21359ms step_avg:60.00ms
step:357/2285 train_time:21420ms step_avg:60.00ms
step:358/2285 train_time:21479ms step_avg:60.00ms
step:359/2285 train_time:21541ms step_avg:60.00ms
step:360/2285 train_time:21601ms step_avg:60.00ms
step:361/2285 train_time:21661ms step_avg:60.00ms
step:362/2285 train_time:21720ms step_avg:60.00ms
step:363/2285 train_time:21780ms step_avg:60.00ms
step:364/2285 train_time:21839ms step_avg:60.00ms
step:365/2285 train_time:21900ms step_avg:60.00ms
step:366/2285 train_time:21959ms step_avg:60.00ms
step:367/2285 train_time:22020ms step_avg:60.00ms
step:368/2285 train_time:22078ms step_avg:60.00ms
step:369/2285 train_time:22139ms step_avg:60.00ms
step:370/2285 train_time:22198ms step_avg:59.99ms
step:371/2285 train_time:22258ms step_avg:60.00ms
step:372/2285 train_time:22317ms step_avg:59.99ms
step:373/2285 train_time:22378ms step_avg:59.99ms
step:374/2285 train_time:22437ms step_avg:59.99ms
step:375/2285 train_time:22498ms step_avg:59.99ms
step:376/2285 train_time:22557ms step_avg:59.99ms
step:377/2285 train_time:22618ms step_avg:59.99ms
step:378/2285 train_time:22677ms step_avg:59.99ms
step:379/2285 train_time:22738ms step_avg:59.99ms
step:380/2285 train_time:22797ms step_avg:59.99ms
step:381/2285 train_time:22858ms step_avg:59.99ms
step:382/2285 train_time:22917ms step_avg:59.99ms
step:383/2285 train_time:22978ms step_avg:60.00ms
step:384/2285 train_time:23037ms step_avg:59.99ms
step:385/2285 train_time:23098ms step_avg:59.99ms
step:386/2285 train_time:23157ms step_avg:59.99ms
step:387/2285 train_time:23218ms step_avg:60.00ms
step:388/2285 train_time:23277ms step_avg:59.99ms
step:389/2285 train_time:23338ms step_avg:60.00ms
step:390/2285 train_time:23398ms step_avg:59.99ms
step:391/2285 train_time:23459ms step_avg:60.00ms
step:392/2285 train_time:23519ms step_avg:60.00ms
step:393/2285 train_time:23580ms step_avg:60.00ms
step:394/2285 train_time:23639ms step_avg:60.00ms
step:395/2285 train_time:23700ms step_avg:60.00ms
step:396/2285 train_time:23760ms step_avg:60.00ms
step:397/2285 train_time:23821ms step_avg:60.00ms
step:398/2285 train_time:23880ms step_avg:60.00ms
step:399/2285 train_time:23942ms step_avg:60.01ms
step:400/2285 train_time:24001ms step_avg:60.00ms
step:401/2285 train_time:24062ms step_avg:60.01ms
step:402/2285 train_time:24122ms step_avg:60.00ms
step:403/2285 train_time:24182ms step_avg:60.01ms
step:404/2285 train_time:24242ms step_avg:60.00ms
step:405/2285 train_time:24303ms step_avg:60.01ms
step:406/2285 train_time:24362ms step_avg:60.00ms
step:407/2285 train_time:24423ms step_avg:60.01ms
step:408/2285 train_time:24482ms step_avg:60.00ms
step:409/2285 train_time:24543ms step_avg:60.01ms
step:410/2285 train_time:24602ms step_avg:60.01ms
step:411/2285 train_time:24663ms step_avg:60.01ms
step:412/2285 train_time:24722ms step_avg:60.00ms
step:413/2285 train_time:24784ms step_avg:60.01ms
step:414/2285 train_time:24842ms step_avg:60.01ms
step:415/2285 train_time:24904ms step_avg:60.01ms
step:416/2285 train_time:24963ms step_avg:60.01ms
step:417/2285 train_time:25024ms step_avg:60.01ms
step:418/2285 train_time:25083ms step_avg:60.01ms
step:419/2285 train_time:25144ms step_avg:60.01ms
step:420/2285 train_time:25203ms step_avg:60.01ms
step:421/2285 train_time:25264ms step_avg:60.01ms
step:422/2285 train_time:25323ms step_avg:60.01ms
step:423/2285 train_time:25385ms step_avg:60.01ms
step:424/2285 train_time:25444ms step_avg:60.01ms
step:425/2285 train_time:25506ms step_avg:60.01ms
step:426/2285 train_time:25565ms step_avg:60.01ms
step:427/2285 train_time:25626ms step_avg:60.01ms
step:428/2285 train_time:25685ms step_avg:60.01ms
step:429/2285 train_time:25746ms step_avg:60.01ms
step:430/2285 train_time:25805ms step_avg:60.01ms
step:431/2285 train_time:25867ms step_avg:60.02ms
step:432/2285 train_time:25926ms step_avg:60.01ms
step:433/2285 train_time:25987ms step_avg:60.02ms
step:434/2285 train_time:26046ms step_avg:60.01ms
step:435/2285 train_time:26107ms step_avg:60.02ms
step:436/2285 train_time:26166ms step_avg:60.01ms
step:437/2285 train_time:26227ms step_avg:60.02ms
step:438/2285 train_time:26286ms step_avg:60.01ms
step:439/2285 train_time:26348ms step_avg:60.02ms
step:440/2285 train_time:26407ms step_avg:60.02ms
step:441/2285 train_time:26468ms step_avg:60.02ms
step:442/2285 train_time:26527ms step_avg:60.02ms
step:443/2285 train_time:26588ms step_avg:60.02ms
step:444/2285 train_time:26647ms step_avg:60.02ms
step:445/2285 train_time:26708ms step_avg:60.02ms
step:446/2285 train_time:26767ms step_avg:60.02ms
step:447/2285 train_time:26828ms step_avg:60.02ms
step:448/2285 train_time:26887ms step_avg:60.02ms
step:449/2285 train_time:26948ms step_avg:60.02ms
step:450/2285 train_time:27007ms step_avg:60.01ms
step:451/2285 train_time:27068ms step_avg:60.02ms
step:452/2285 train_time:27126ms step_avg:60.01ms
step:453/2285 train_time:27187ms step_avg:60.02ms
step:454/2285 train_time:27246ms step_avg:60.01ms
step:455/2285 train_time:27307ms step_avg:60.02ms
step:456/2285 train_time:27366ms step_avg:60.01ms
step:457/2285 train_time:27428ms step_avg:60.02ms
step:458/2285 train_time:27487ms step_avg:60.01ms
step:459/2285 train_time:27548ms step_avg:60.02ms
step:460/2285 train_time:27607ms step_avg:60.02ms
step:461/2285 train_time:27669ms step_avg:60.02ms
step:462/2285 train_time:27728ms step_avg:60.02ms
step:463/2285 train_time:27790ms step_avg:60.02ms
step:464/2285 train_time:27849ms step_avg:60.02ms
step:465/2285 train_time:27910ms step_avg:60.02ms
step:466/2285 train_time:27970ms step_avg:60.02ms
step:467/2285 train_time:28030ms step_avg:60.02ms
step:468/2285 train_time:28089ms step_avg:60.02ms
step:469/2285 train_time:28152ms step_avg:60.03ms
step:470/2285 train_time:28210ms step_avg:60.02ms
step:471/2285 train_time:28271ms step_avg:60.02ms
step:472/2285 train_time:28330ms step_avg:60.02ms
step:473/2285 train_time:28391ms step_avg:60.02ms
step:474/2285 train_time:28450ms step_avg:60.02ms
step:475/2285 train_time:28512ms step_avg:60.02ms
step:476/2285 train_time:28570ms step_avg:60.02ms
step:477/2285 train_time:28632ms step_avg:60.02ms
step:478/2285 train_time:28690ms step_avg:60.02ms
step:479/2285 train_time:28752ms step_avg:60.03ms
step:480/2285 train_time:28811ms step_avg:60.02ms
step:481/2285 train_time:28873ms step_avg:60.03ms
step:482/2285 train_time:28932ms step_avg:60.02ms
step:483/2285 train_time:28993ms step_avg:60.03ms
step:484/2285 train_time:29052ms step_avg:60.02ms
step:485/2285 train_time:29113ms step_avg:60.03ms
step:486/2285 train_time:29172ms step_avg:60.02ms
step:487/2285 train_time:29233ms step_avg:60.03ms
step:488/2285 train_time:29292ms step_avg:60.02ms
step:489/2285 train_time:29353ms step_avg:60.03ms
step:490/2285 train_time:29412ms step_avg:60.03ms
step:491/2285 train_time:29474ms step_avg:60.03ms
step:492/2285 train_time:29533ms step_avg:60.03ms
step:493/2285 train_time:29595ms step_avg:60.03ms
step:494/2285 train_time:29655ms step_avg:60.03ms
step:495/2285 train_time:29716ms step_avg:60.03ms
step:496/2285 train_time:29775ms step_avg:60.03ms
step:497/2285 train_time:29836ms step_avg:60.03ms
step:498/2285 train_time:29895ms step_avg:60.03ms
step:499/2285 train_time:29956ms step_avg:60.03ms
step:500/2285 train_time:30015ms step_avg:60.03ms
step:500/2285 val_loss:3.7855 train_time:30078ms step_avg:60.16ms
step:501/2285 train_time:30097ms step_avg:60.07ms
step:502/2285 train_time:30139ms step_avg:60.04ms
step:503/2285 train_time:30201ms step_avg:60.04ms
step:504/2285 train_time:30261ms step_avg:60.04ms
step:505/2285 train_time:30324ms step_avg:60.05ms
step:506/2285 train_time:30384ms step_avg:60.05ms
step:507/2285 train_time:30445ms step_avg:60.05ms
step:508/2285 train_time:30504ms step_avg:60.05ms
step:509/2285 train_time:30565ms step_avg:60.05ms
step:510/2285 train_time:30623ms step_avg:60.05ms
step:511/2285 train_time:30683ms step_avg:60.05ms
step:512/2285 train_time:30741ms step_avg:60.04ms
step:513/2285 train_time:30803ms step_avg:60.05ms
step:514/2285 train_time:30862ms step_avg:60.04ms
step:515/2285 train_time:30923ms step_avg:60.04ms
step:516/2285 train_time:30985ms step_avg:60.05ms
step:517/2285 train_time:31051ms step_avg:60.06ms
step:518/2285 train_time:31111ms step_avg:60.06ms
step:519/2285 train_time:31173ms step_avg:60.06ms
step:520/2285 train_time:31232ms step_avg:60.06ms
step:521/2285 train_time:31294ms step_avg:60.06ms
step:522/2285 train_time:31353ms step_avg:60.06ms
step:523/2285 train_time:31414ms step_avg:60.06ms
step:524/2285 train_time:31472ms step_avg:60.06ms
step:525/2285 train_time:31533ms step_avg:60.06ms
step:526/2285 train_time:31592ms step_avg:60.06ms
step:527/2285 train_time:31653ms step_avg:60.06ms
step:528/2285 train_time:31712ms step_avg:60.06ms
step:529/2285 train_time:31773ms step_avg:60.06ms
step:530/2285 train_time:31831ms step_avg:60.06ms
step:531/2285 train_time:31894ms step_avg:60.06ms
step:532/2285 train_time:31954ms step_avg:60.06ms
step:533/2285 train_time:32016ms step_avg:60.07ms
step:534/2285 train_time:32075ms step_avg:60.07ms
step:535/2285 train_time:32136ms step_avg:60.07ms
step:536/2285 train_time:32195ms step_avg:60.06ms
step:537/2285 train_time:32256ms step_avg:60.07ms
step:538/2285 train_time:32315ms step_avg:60.06ms
step:539/2285 train_time:32376ms step_avg:60.07ms
step:540/2285 train_time:32435ms step_avg:60.06ms
step:541/2285 train_time:32497ms step_avg:60.07ms
step:542/2285 train_time:32555ms step_avg:60.07ms
step:543/2285 train_time:32617ms step_avg:60.07ms
step:544/2285 train_time:32675ms step_avg:60.06ms
step:545/2285 train_time:32737ms step_avg:60.07ms
step:546/2285 train_time:32796ms step_avg:60.07ms
step:547/2285 train_time:32857ms step_avg:60.07ms
step:548/2285 train_time:32916ms step_avg:60.07ms
step:549/2285 train_time:32977ms step_avg:60.07ms
step:550/2285 train_time:33037ms step_avg:60.07ms
step:551/2285 train_time:33098ms step_avg:60.07ms
step:552/2285 train_time:33157ms step_avg:60.07ms
step:553/2285 train_time:33218ms step_avg:60.07ms
step:554/2285 train_time:33277ms step_avg:60.07ms
step:555/2285 train_time:33339ms step_avg:60.07ms
step:556/2285 train_time:33398ms step_avg:60.07ms
step:557/2285 train_time:33460ms step_avg:60.07ms
step:558/2285 train_time:33519ms step_avg:60.07ms
step:559/2285 train_time:33581ms step_avg:60.07ms
step:560/2285 train_time:33640ms step_avg:60.07ms
step:561/2285 train_time:33703ms step_avg:60.08ms
step:562/2285 train_time:33762ms step_avg:60.08ms
step:563/2285 train_time:33824ms step_avg:60.08ms
step:564/2285 train_time:33883ms step_avg:60.08ms
step:565/2285 train_time:33945ms step_avg:60.08ms
step:566/2285 train_time:34005ms step_avg:60.08ms
step:567/2285 train_time:34066ms step_avg:60.08ms
step:568/2285 train_time:34125ms step_avg:60.08ms
step:569/2285 train_time:34186ms step_avg:60.08ms
step:570/2285 train_time:34246ms step_avg:60.08ms
step:571/2285 train_time:34307ms step_avg:60.08ms
step:572/2285 train_time:34367ms step_avg:60.08ms
step:573/2285 train_time:34428ms step_avg:60.08ms
step:574/2285 train_time:34487ms step_avg:60.08ms
step:575/2285 train_time:34548ms step_avg:60.08ms
step:576/2285 train_time:34607ms step_avg:60.08ms
step:577/2285 train_time:34669ms step_avg:60.09ms
step:578/2285 train_time:34728ms step_avg:60.08ms
step:579/2285 train_time:34789ms step_avg:60.09ms
step:580/2285 train_time:34848ms step_avg:60.08ms
step:581/2285 train_time:34910ms step_avg:60.09ms
step:582/2285 train_time:34969ms step_avg:60.08ms
step:583/2285 train_time:35031ms step_avg:60.09ms
step:584/2285 train_time:35089ms step_avg:60.08ms
step:585/2285 train_time:35151ms step_avg:60.09ms
step:586/2285 train_time:35210ms step_avg:60.09ms
step:587/2285 train_time:35272ms step_avg:60.09ms
step:588/2285 train_time:35331ms step_avg:60.09ms
step:589/2285 train_time:35392ms step_avg:60.09ms
step:590/2285 train_time:35450ms step_avg:60.09ms
step:591/2285 train_time:35511ms step_avg:60.09ms
step:592/2285 train_time:35570ms step_avg:60.08ms
step:593/2285 train_time:35631ms step_avg:60.09ms
step:594/2285 train_time:35690ms step_avg:60.08ms
step:595/2285 train_time:35752ms step_avg:60.09ms
step:596/2285 train_time:35811ms step_avg:60.08ms
step:597/2285 train_time:35872ms step_avg:60.09ms
step:598/2285 train_time:35930ms step_avg:60.08ms
step:599/2285 train_time:35991ms step_avg:60.09ms
step:600/2285 train_time:36050ms step_avg:60.08ms
step:601/2285 train_time:36112ms step_avg:60.09ms
step:602/2285 train_time:36171ms step_avg:60.08ms
step:603/2285 train_time:36232ms step_avg:60.09ms
step:604/2285 train_time:36291ms step_avg:60.08ms
step:605/2285 train_time:36352ms step_avg:60.09ms
step:606/2285 train_time:36411ms step_avg:60.08ms
step:607/2285 train_time:36472ms step_avg:60.09ms
step:608/2285 train_time:36530ms step_avg:60.08ms
step:609/2285 train_time:36592ms step_avg:60.08ms
step:610/2285 train_time:36650ms step_avg:60.08ms
step:611/2285 train_time:36711ms step_avg:60.08ms
step:612/2285 train_time:36770ms step_avg:60.08ms
step:613/2285 train_time:36832ms step_avg:60.08ms
step:614/2285 train_time:36891ms step_avg:60.08ms
step:615/2285 train_time:36953ms step_avg:60.09ms
step:616/2285 train_time:37012ms step_avg:60.08ms
step:617/2285 train_time:37073ms step_avg:60.09ms
step:618/2285 train_time:37132ms step_avg:60.08ms
step:619/2285 train_time:37192ms step_avg:60.08ms
step:620/2285 train_time:37251ms step_avg:60.08ms
step:621/2285 train_time:37312ms step_avg:60.08ms
step:622/2285 train_time:37371ms step_avg:60.08ms
step:623/2285 train_time:37432ms step_avg:60.08ms
step:624/2285 train_time:37490ms step_avg:60.08ms
step:625/2285 train_time:37553ms step_avg:60.08ms
step:626/2285 train_time:37610ms step_avg:60.08ms
step:627/2285 train_time:37672ms step_avg:60.08ms
step:628/2285 train_time:37730ms step_avg:60.08ms
step:629/2285 train_time:37792ms step_avg:60.08ms
step:630/2285 train_time:37850ms step_avg:60.08ms
step:631/2285 train_time:37912ms step_avg:60.08ms
step:632/2285 train_time:37971ms step_avg:60.08ms
step:633/2285 train_time:38032ms step_avg:60.08ms
step:634/2285 train_time:38092ms step_avg:60.08ms
step:635/2285 train_time:38153ms step_avg:60.08ms
step:636/2285 train_time:38212ms step_avg:60.08ms
step:637/2285 train_time:38273ms step_avg:60.08ms
step:638/2285 train_time:38331ms step_avg:60.08ms
step:639/2285 train_time:38392ms step_avg:60.08ms
step:640/2285 train_time:38451ms step_avg:60.08ms
step:641/2285 train_time:38512ms step_avg:60.08ms
step:642/2285 train_time:38571ms step_avg:60.08ms
step:643/2285 train_time:38632ms step_avg:60.08ms
step:644/2285 train_time:38691ms step_avg:60.08ms
step:645/2285 train_time:38752ms step_avg:60.08ms
step:646/2285 train_time:38811ms step_avg:60.08ms
step:647/2285 train_time:38872ms step_avg:60.08ms
step:648/2285 train_time:38931ms step_avg:60.08ms
step:649/2285 train_time:38993ms step_avg:60.08ms
step:650/2285 train_time:39052ms step_avg:60.08ms
step:651/2285 train_time:39113ms step_avg:60.08ms
step:652/2285 train_time:39172ms step_avg:60.08ms
step:653/2285 train_time:39233ms step_avg:60.08ms
step:654/2285 train_time:39292ms step_avg:60.08ms
step:655/2285 train_time:39353ms step_avg:60.08ms
step:656/2285 train_time:39412ms step_avg:60.08ms
step:657/2285 train_time:39474ms step_avg:60.08ms
step:658/2285 train_time:39532ms step_avg:60.08ms
step:659/2285 train_time:39593ms step_avg:60.08ms
step:660/2285 train_time:39652ms step_avg:60.08ms
step:661/2285 train_time:39713ms step_avg:60.08ms
step:662/2285 train_time:39772ms step_avg:60.08ms
step:663/2285 train_time:39833ms step_avg:60.08ms
step:664/2285 train_time:39893ms step_avg:60.08ms
step:665/2285 train_time:39954ms step_avg:60.08ms
step:666/2285 train_time:40013ms step_avg:60.08ms
step:667/2285 train_time:40074ms step_avg:60.08ms
step:668/2285 train_time:40133ms step_avg:60.08ms
step:669/2285 train_time:40194ms step_avg:60.08ms
step:670/2285 train_time:40253ms step_avg:60.08ms
step:671/2285 train_time:40315ms step_avg:60.08ms
step:672/2285 train_time:40373ms step_avg:60.08ms
step:673/2285 train_time:40434ms step_avg:60.08ms
step:674/2285 train_time:40493ms step_avg:60.08ms
step:675/2285 train_time:40554ms step_avg:60.08ms
step:676/2285 train_time:40613ms step_avg:60.08ms
step:677/2285 train_time:40673ms step_avg:60.08ms
step:678/2285 train_time:40732ms step_avg:60.08ms
step:679/2285 train_time:40794ms step_avg:60.08ms
step:680/2285 train_time:40853ms step_avg:60.08ms
step:681/2285 train_time:40915ms step_avg:60.08ms
step:682/2285 train_time:40974ms step_avg:60.08ms
step:683/2285 train_time:41035ms step_avg:60.08ms
step:684/2285 train_time:41094ms step_avg:60.08ms
step:685/2285 train_time:41155ms step_avg:60.08ms
step:686/2285 train_time:41214ms step_avg:60.08ms
step:687/2285 train_time:41275ms step_avg:60.08ms
step:688/2285 train_time:41334ms step_avg:60.08ms
step:689/2285 train_time:41395ms step_avg:60.08ms
step:690/2285 train_time:41454ms step_avg:60.08ms
step:691/2285 train_time:41515ms step_avg:60.08ms
step:692/2285 train_time:41575ms step_avg:60.08ms
step:693/2285 train_time:41636ms step_avg:60.08ms
step:694/2285 train_time:41695ms step_avg:60.08ms
step:695/2285 train_time:41756ms step_avg:60.08ms
step:696/2285 train_time:41816ms step_avg:60.08ms
step:697/2285 train_time:41877ms step_avg:60.08ms
step:698/2285 train_time:41936ms step_avg:60.08ms
step:699/2285 train_time:41997ms step_avg:60.08ms
step:700/2285 train_time:42056ms step_avg:60.08ms
step:701/2285 train_time:42118ms step_avg:60.08ms
step:702/2285 train_time:42177ms step_avg:60.08ms
step:703/2285 train_time:42238ms step_avg:60.08ms
step:704/2285 train_time:42297ms step_avg:60.08ms
step:705/2285 train_time:42359ms step_avg:60.08ms
step:706/2285 train_time:42418ms step_avg:60.08ms
step:707/2285 train_time:42480ms step_avg:60.09ms
step:708/2285 train_time:42539ms step_avg:60.08ms
step:709/2285 train_time:42601ms step_avg:60.09ms
step:710/2285 train_time:42661ms step_avg:60.09ms
step:711/2285 train_time:42722ms step_avg:60.09ms
step:712/2285 train_time:42781ms step_avg:60.09ms
step:713/2285 train_time:42844ms step_avg:60.09ms
step:714/2285 train_time:42903ms step_avg:60.09ms
step:715/2285 train_time:42964ms step_avg:60.09ms
step:716/2285 train_time:43024ms step_avg:60.09ms
step:717/2285 train_time:43085ms step_avg:60.09ms
step:718/2285 train_time:43144ms step_avg:60.09ms
step:719/2285 train_time:43206ms step_avg:60.09ms
step:720/2285 train_time:43265ms step_avg:60.09ms
step:721/2285 train_time:43327ms step_avg:60.09ms
step:722/2285 train_time:43386ms step_avg:60.09ms
step:723/2285 train_time:43449ms step_avg:60.10ms
step:724/2285 train_time:43508ms step_avg:60.09ms
step:725/2285 train_time:43570ms step_avg:60.10ms
step:726/2285 train_time:43628ms step_avg:60.09ms
step:727/2285 train_time:43689ms step_avg:60.10ms
step:728/2285 train_time:43749ms step_avg:60.09ms
step:729/2285 train_time:43810ms step_avg:60.10ms
step:730/2285 train_time:43869ms step_avg:60.09ms
step:731/2285 train_time:43931ms step_avg:60.10ms
step:732/2285 train_time:43989ms step_avg:60.09ms
step:733/2285 train_time:44051ms step_avg:60.10ms
step:734/2285 train_time:44110ms step_avg:60.09ms
step:735/2285 train_time:44171ms step_avg:60.10ms
step:736/2285 train_time:44230ms step_avg:60.10ms
step:737/2285 train_time:44291ms step_avg:60.10ms
step:738/2285 train_time:44350ms step_avg:60.10ms
step:739/2285 train_time:44412ms step_avg:60.10ms
step:740/2285 train_time:44470ms step_avg:60.09ms
step:741/2285 train_time:44531ms step_avg:60.10ms
step:742/2285 train_time:44590ms step_avg:60.09ms
step:743/2285 train_time:44651ms step_avg:60.10ms
step:744/2285 train_time:44710ms step_avg:60.09ms
step:745/2285 train_time:44771ms step_avg:60.10ms
step:746/2285 train_time:44829ms step_avg:60.09ms
step:747/2285 train_time:44890ms step_avg:60.09ms
step:748/2285 train_time:44949ms step_avg:60.09ms
step:749/2285 train_time:45010ms step_avg:60.09ms
step:750/2285 train_time:45069ms step_avg:60.09ms
step:750/2285 val_loss:3.6572 train_time:45133ms step_avg:60.18ms
step:751/2285 train_time:45153ms step_avg:60.12ms
step:752/2285 train_time:45193ms step_avg:60.10ms
step:753/2285 train_time:45256ms step_avg:60.10ms
step:754/2285 train_time:45316ms step_avg:60.10ms
step:755/2285 train_time:45377ms step_avg:60.10ms
step:756/2285 train_time:45436ms step_avg:60.10ms
step:757/2285 train_time:45497ms step_avg:60.10ms
step:758/2285 train_time:45556ms step_avg:60.10ms
step:759/2285 train_time:45616ms step_avg:60.10ms
step:760/2285 train_time:45675ms step_avg:60.10ms
step:761/2285 train_time:45736ms step_avg:60.10ms
step:762/2285 train_time:45798ms step_avg:60.10ms
step:763/2285 train_time:45858ms step_avg:60.10ms
step:764/2285 train_time:45918ms step_avg:60.10ms
step:765/2285 train_time:45979ms step_avg:60.10ms
step:766/2285 train_time:46038ms step_avg:60.10ms
step:767/2285 train_time:46101ms step_avg:60.11ms
step:768/2285 train_time:46162ms step_avg:60.11ms
step:769/2285 train_time:46224ms step_avg:60.11ms
step:770/2285 train_time:46284ms step_avg:60.11ms
step:771/2285 train_time:46346ms step_avg:60.11ms
step:772/2285 train_time:46405ms step_avg:60.11ms
step:773/2285 train_time:46467ms step_avg:60.11ms
step:774/2285 train_time:46526ms step_avg:60.11ms
step:775/2285 train_time:46587ms step_avg:60.11ms
step:776/2285 train_time:46646ms step_avg:60.11ms
step:777/2285 train_time:46707ms step_avg:60.11ms
step:778/2285 train_time:46766ms step_avg:60.11ms
step:779/2285 train_time:46828ms step_avg:60.11ms
step:780/2285 train_time:46887ms step_avg:60.11ms
step:781/2285 train_time:46949ms step_avg:60.11ms
step:782/2285 train_time:47009ms step_avg:60.11ms
step:783/2285 train_time:47071ms step_avg:60.12ms
step:784/2285 train_time:47130ms step_avg:60.12ms
step:785/2285 train_time:47192ms step_avg:60.12ms
step:786/2285 train_time:47252ms step_avg:60.12ms
step:787/2285 train_time:47314ms step_avg:60.12ms
step:788/2285 train_time:47374ms step_avg:60.12ms
step:789/2285 train_time:47435ms step_avg:60.12ms
step:790/2285 train_time:47495ms step_avg:60.12ms
step:791/2285 train_time:47557ms step_avg:60.12ms
step:792/2285 train_time:47617ms step_avg:60.12ms
step:793/2285 train_time:47678ms step_avg:60.12ms
step:794/2285 train_time:47737ms step_avg:60.12ms
step:795/2285 train_time:47799ms step_avg:60.12ms
step:796/2285 train_time:47859ms step_avg:60.12ms
step:797/2285 train_time:47920ms step_avg:60.13ms
step:798/2285 train_time:47979ms step_avg:60.12ms
step:799/2285 train_time:48040ms step_avg:60.13ms
step:800/2285 train_time:48100ms step_avg:60.12ms
step:801/2285 train_time:48162ms step_avg:60.13ms
step:802/2285 train_time:48222ms step_avg:60.13ms
step:803/2285 train_time:48284ms step_avg:60.13ms
step:804/2285 train_time:48343ms step_avg:60.13ms
step:805/2285 train_time:48406ms step_avg:60.13ms
step:806/2285 train_time:48465ms step_avg:60.13ms
step:807/2285 train_time:48527ms step_avg:60.13ms
step:808/2285 train_time:48586ms step_avg:60.13ms
step:809/2285 train_time:48647ms step_avg:60.13ms
step:810/2285 train_time:48706ms step_avg:60.13ms
step:811/2285 train_time:48768ms step_avg:60.13ms
step:812/2285 train_time:48827ms step_avg:60.13ms
step:813/2285 train_time:48888ms step_avg:60.13ms
step:814/2285 train_time:48948ms step_avg:60.13ms
step:815/2285 train_time:49010ms step_avg:60.13ms
step:816/2285 train_time:49070ms step_avg:60.13ms
step:817/2285 train_time:49132ms step_avg:60.14ms
step:818/2285 train_time:49192ms step_avg:60.14ms
step:819/2285 train_time:49254ms step_avg:60.14ms
step:820/2285 train_time:49314ms step_avg:60.14ms
step:821/2285 train_time:49377ms step_avg:60.14ms
step:822/2285 train_time:49436ms step_avg:60.14ms
step:823/2285 train_time:49498ms step_avg:60.14ms
step:824/2285 train_time:49558ms step_avg:60.14ms
step:825/2285 train_time:49619ms step_avg:60.14ms
step:826/2285 train_time:49679ms step_avg:60.14ms
step:827/2285 train_time:49740ms step_avg:60.15ms
step:828/2285 train_time:49800ms step_avg:60.15ms
step:829/2285 train_time:49861ms step_avg:60.15ms
step:830/2285 train_time:49920ms step_avg:60.15ms
step:831/2285 train_time:49982ms step_avg:60.15ms
step:832/2285 train_time:50041ms step_avg:60.15ms
step:833/2285 train_time:50103ms step_avg:60.15ms
step:834/2285 train_time:50162ms step_avg:60.15ms
step:835/2285 train_time:50225ms step_avg:60.15ms
step:836/2285 train_time:50285ms step_avg:60.15ms
step:837/2285 train_time:50347ms step_avg:60.15ms
step:838/2285 train_time:50406ms step_avg:60.15ms
step:839/2285 train_time:50468ms step_avg:60.15ms
step:840/2285 train_time:50527ms step_avg:60.15ms
step:841/2285 train_time:50589ms step_avg:60.15ms
step:842/2285 train_time:50649ms step_avg:60.15ms
step:843/2285 train_time:50710ms step_avg:60.15ms
step:844/2285 train_time:50770ms step_avg:60.15ms
step:845/2285 train_time:50832ms step_avg:60.16ms
step:846/2285 train_time:50892ms step_avg:60.16ms
step:847/2285 train_time:50954ms step_avg:60.16ms
step:848/2285 train_time:51014ms step_avg:60.16ms
step:849/2285 train_time:51077ms step_avg:60.16ms
step:850/2285 train_time:51136ms step_avg:60.16ms
step:851/2285 train_time:51199ms step_avg:60.16ms
step:852/2285 train_time:51258ms step_avg:60.16ms
step:853/2285 train_time:51320ms step_avg:60.16ms
step:854/2285 train_time:51380ms step_avg:60.16ms
step:855/2285 train_time:51441ms step_avg:60.17ms
step:856/2285 train_time:51501ms step_avg:60.16ms
step:857/2285 train_time:51562ms step_avg:60.17ms
step:858/2285 train_time:51621ms step_avg:60.16ms
step:859/2285 train_time:51682ms step_avg:60.17ms
step:860/2285 train_time:51741ms step_avg:60.16ms
step:861/2285 train_time:51803ms step_avg:60.17ms
step:862/2285 train_time:51863ms step_avg:60.17ms
step:863/2285 train_time:51925ms step_avg:60.17ms
step:864/2285 train_time:51984ms step_avg:60.17ms
step:865/2285 train_time:52046ms step_avg:60.17ms
step:866/2285 train_time:52105ms step_avg:60.17ms
step:867/2285 train_time:52167ms step_avg:60.17ms
step:868/2285 train_time:52226ms step_avg:60.17ms
step:869/2285 train_time:52288ms step_avg:60.17ms
step:870/2285 train_time:52347ms step_avg:60.17ms
step:871/2285 train_time:52410ms step_avg:60.17ms
step:872/2285 train_time:52469ms step_avg:60.17ms
step:873/2285 train_time:52531ms step_avg:60.17ms
step:874/2285 train_time:52590ms step_avg:60.17ms
step:875/2285 train_time:52652ms step_avg:60.17ms
step:876/2285 train_time:52712ms step_avg:60.17ms
step:877/2285 train_time:52774ms step_avg:60.18ms
step:878/2285 train_time:52834ms step_avg:60.18ms
step:879/2285 train_time:52896ms step_avg:60.18ms
step:880/2285 train_time:52956ms step_avg:60.18ms
step:881/2285 train_time:53017ms step_avg:60.18ms
step:882/2285 train_time:53077ms step_avg:60.18ms
step:883/2285 train_time:53139ms step_avg:60.18ms
step:884/2285 train_time:53198ms step_avg:60.18ms
step:885/2285 train_time:53260ms step_avg:60.18ms
step:886/2285 train_time:53320ms step_avg:60.18ms
step:887/2285 train_time:53380ms step_avg:60.18ms
step:888/2285 train_time:53440ms step_avg:60.18ms
step:889/2285 train_time:53501ms step_avg:60.18ms
step:890/2285 train_time:53560ms step_avg:60.18ms
step:891/2285 train_time:53622ms step_avg:60.18ms
step:892/2285 train_time:53681ms step_avg:60.18ms
step:893/2285 train_time:53743ms step_avg:60.18ms
step:894/2285 train_time:53803ms step_avg:60.18ms
step:895/2285 train_time:53865ms step_avg:60.18ms
step:896/2285 train_time:53924ms step_avg:60.18ms
step:897/2285 train_time:53985ms step_avg:60.18ms
step:898/2285 train_time:54044ms step_avg:60.18ms
step:899/2285 train_time:54105ms step_avg:60.18ms
step:900/2285 train_time:54165ms step_avg:60.18ms
step:901/2285 train_time:54226ms step_avg:60.18ms
step:902/2285 train_time:54286ms step_avg:60.18ms
step:903/2285 train_time:54348ms step_avg:60.19ms
step:904/2285 train_time:54408ms step_avg:60.19ms
step:905/2285 train_time:54470ms step_avg:60.19ms
step:906/2285 train_time:54529ms step_avg:60.19ms
step:907/2285 train_time:54591ms step_avg:60.19ms
step:908/2285 train_time:54651ms step_avg:60.19ms
step:909/2285 train_time:54713ms step_avg:60.19ms
step:910/2285 train_time:54773ms step_avg:60.19ms
step:911/2285 train_time:54834ms step_avg:60.19ms
step:912/2285 train_time:54894ms step_avg:60.19ms
step:913/2285 train_time:54957ms step_avg:60.19ms
step:914/2285 train_time:55017ms step_avg:60.19ms
step:915/2285 train_time:55079ms step_avg:60.20ms
step:916/2285 train_time:55138ms step_avg:60.19ms
step:917/2285 train_time:55200ms step_avg:60.20ms
step:918/2285 train_time:55259ms step_avg:60.20ms
step:919/2285 train_time:55321ms step_avg:60.20ms
step:920/2285 train_time:55380ms step_avg:60.20ms
step:921/2285 train_time:55442ms step_avg:60.20ms
step:922/2285 train_time:55501ms step_avg:60.20ms
step:923/2285 train_time:55563ms step_avg:60.20ms
step:924/2285 train_time:55623ms step_avg:60.20ms
step:925/2285 train_time:55684ms step_avg:60.20ms
step:926/2285 train_time:55744ms step_avg:60.20ms
step:927/2285 train_time:55806ms step_avg:60.20ms
step:928/2285 train_time:55865ms step_avg:60.20ms
step:929/2285 train_time:55927ms step_avg:60.20ms
step:930/2285 train_time:55986ms step_avg:60.20ms
step:931/2285 train_time:56047ms step_avg:60.20ms
step:932/2285 train_time:56107ms step_avg:60.20ms
step:933/2285 train_time:56169ms step_avg:60.20ms
step:934/2285 train_time:56229ms step_avg:60.20ms
step:935/2285 train_time:56291ms step_avg:60.20ms
step:936/2285 train_time:56350ms step_avg:60.20ms
step:937/2285 train_time:56412ms step_avg:60.21ms
step:938/2285 train_time:56472ms step_avg:60.20ms
step:939/2285 train_time:56534ms step_avg:60.21ms
step:940/2285 train_time:56593ms step_avg:60.21ms
step:941/2285 train_time:56656ms step_avg:60.21ms
step:942/2285 train_time:56715ms step_avg:60.21ms
step:943/2285 train_time:56778ms step_avg:60.21ms
step:944/2285 train_time:56837ms step_avg:60.21ms
step:945/2285 train_time:56899ms step_avg:60.21ms
step:946/2285 train_time:56959ms step_avg:60.21ms
step:947/2285 train_time:57020ms step_avg:60.21ms
step:948/2285 train_time:57080ms step_avg:60.21ms
step:949/2285 train_time:57141ms step_avg:60.21ms
step:950/2285 train_time:57201ms step_avg:60.21ms
step:951/2285 train_time:57262ms step_avg:60.21ms
step:952/2285 train_time:57321ms step_avg:60.21ms
step:953/2285 train_time:57382ms step_avg:60.21ms
step:954/2285 train_time:57441ms step_avg:60.21ms
step:955/2285 train_time:57506ms step_avg:60.22ms
step:956/2285 train_time:57564ms step_avg:60.21ms
step:957/2285 train_time:57626ms step_avg:60.21ms
step:958/2285 train_time:57685ms step_avg:60.21ms
step:959/2285 train_time:57747ms step_avg:60.22ms
step:960/2285 train_time:57806ms step_avg:60.21ms
step:961/2285 train_time:57868ms step_avg:60.22ms
step:962/2285 train_time:57927ms step_avg:60.22ms
step:963/2285 train_time:57989ms step_avg:60.22ms
step:964/2285 train_time:58048ms step_avg:60.22ms
step:965/2285 train_time:58110ms step_avg:60.22ms
step:966/2285 train_time:58170ms step_avg:60.22ms
step:967/2285 train_time:58232ms step_avg:60.22ms
step:968/2285 train_time:58292ms step_avg:60.22ms
step:969/2285 train_time:58354ms step_avg:60.22ms
step:970/2285 train_time:58414ms step_avg:60.22ms
step:971/2285 train_time:58476ms step_avg:60.22ms
step:972/2285 train_time:58536ms step_avg:60.22ms
step:973/2285 train_time:58598ms step_avg:60.22ms
step:974/2285 train_time:58657ms step_avg:60.22ms
step:975/2285 train_time:58719ms step_avg:60.22ms
step:976/2285 train_time:58779ms step_avg:60.22ms
step:977/2285 train_time:58840ms step_avg:60.23ms
step:978/2285 train_time:58900ms step_avg:60.22ms
step:979/2285 train_time:58961ms step_avg:60.23ms
step:980/2285 train_time:59021ms step_avg:60.23ms
step:981/2285 train_time:59082ms step_avg:60.23ms
step:982/2285 train_time:59142ms step_avg:60.23ms
step:983/2285 train_time:59203ms step_avg:60.23ms
step:984/2285 train_time:59262ms step_avg:60.23ms
step:985/2285 train_time:59324ms step_avg:60.23ms
step:986/2285 train_time:59383ms step_avg:60.23ms
step:987/2285 train_time:59445ms step_avg:60.23ms
step:988/2285 train_time:59505ms step_avg:60.23ms
step:989/2285 train_time:59566ms step_avg:60.23ms
step:990/2285 train_time:59625ms step_avg:60.23ms
step:991/2285 train_time:59687ms step_avg:60.23ms
step:992/2285 train_time:59747ms step_avg:60.23ms
step:993/2285 train_time:59809ms step_avg:60.23ms
step:994/2285 train_time:59868ms step_avg:60.23ms
step:995/2285 train_time:59931ms step_avg:60.23ms
step:996/2285 train_time:59990ms step_avg:60.23ms
step:997/2285 train_time:60053ms step_avg:60.23ms
step:998/2285 train_time:60112ms step_avg:60.23ms
step:999/2285 train_time:60174ms step_avg:60.23ms
step:1000/2285 train_time:60234ms step_avg:60.23ms
step:1000/2285 val_loss:3.5694 train_time:60298ms step_avg:60.30ms
step:1001/2285 train_time:60318ms step_avg:60.26ms
step:1002/2285 train_time:60360ms step_avg:60.24ms
step:1003/2285 train_time:60422ms step_avg:60.24ms
step:1004/2285 train_time:60481ms step_avg:60.24ms
step:1005/2285 train_time:60543ms step_avg:60.24ms
step:1006/2285 train_time:60602ms step_avg:60.24ms
step:1007/2285 train_time:60663ms step_avg:60.24ms
step:1008/2285 train_time:60722ms step_avg:60.24ms
step:1009/2285 train_time:60783ms step_avg:60.24ms
step:1010/2285 train_time:60841ms step_avg:60.24ms
step:1011/2285 train_time:60902ms step_avg:60.24ms
step:1012/2285 train_time:60960ms step_avg:60.24ms
step:1013/2285 train_time:61022ms step_avg:60.24ms
step:1014/2285 train_time:61080ms step_avg:60.24ms
step:1015/2285 train_time:61141ms step_avg:60.24ms
step:1016/2285 train_time:61204ms step_avg:60.24ms
step:1017/2285 train_time:61271ms step_avg:60.25ms
step:1018/2285 train_time:61331ms step_avg:60.25ms
step:1019/2285 train_time:61393ms step_avg:60.25ms
step:1020/2285 train_time:61453ms step_avg:60.25ms
step:1021/2285 train_time:61515ms step_avg:60.25ms
step:1022/2285 train_time:61575ms step_avg:60.25ms
step:1023/2285 train_time:61637ms step_avg:60.25ms
step:1024/2285 train_time:61697ms step_avg:60.25ms
step:1025/2285 train_time:61759ms step_avg:60.25ms
step:1026/2285 train_time:61818ms step_avg:60.25ms
step:1027/2285 train_time:61879ms step_avg:60.25ms
step:1028/2285 train_time:61938ms step_avg:60.25ms
step:1029/2285 train_time:61999ms step_avg:60.25ms
step:1030/2285 train_time:62057ms step_avg:60.25ms
step:1031/2285 train_time:62119ms step_avg:60.25ms
step:1032/2285 train_time:62179ms step_avg:60.25ms
step:1033/2285 train_time:62243ms step_avg:60.25ms
step:1034/2285 train_time:62303ms step_avg:60.25ms
step:1035/2285 train_time:62366ms step_avg:60.26ms
step:1036/2285 train_time:62426ms step_avg:60.26ms
step:1037/2285 train_time:62489ms step_avg:60.26ms
step:1038/2285 train_time:62548ms step_avg:60.26ms
step:1039/2285 train_time:62610ms step_avg:60.26ms
step:1040/2285 train_time:62669ms step_avg:60.26ms
step:1041/2285 train_time:62731ms step_avg:60.26ms
step:1042/2285 train_time:62791ms step_avg:60.26ms
step:1043/2285 train_time:62853ms step_avg:60.26ms
step:1044/2285 train_time:62912ms step_avg:60.26ms
step:1045/2285 train_time:62974ms step_avg:60.26ms
step:1046/2285 train_time:63034ms step_avg:60.26ms
step:1047/2285 train_time:63097ms step_avg:60.26ms
step:1048/2285 train_time:63156ms step_avg:60.26ms
step:1049/2285 train_time:63218ms step_avg:60.27ms
step:1050/2285 train_time:63278ms step_avg:60.26ms
step:1051/2285 train_time:63341ms step_avg:60.27ms
step:1052/2285 train_time:63400ms step_avg:60.27ms
step:1053/2285 train_time:63462ms step_avg:60.27ms
step:1054/2285 train_time:63521ms step_avg:60.27ms
step:1055/2285 train_time:63583ms step_avg:60.27ms
step:1056/2285 train_time:63642ms step_avg:60.27ms
step:1057/2285 train_time:63704ms step_avg:60.27ms
step:1058/2285 train_time:63763ms step_avg:60.27ms
step:1059/2285 train_time:63825ms step_avg:60.27ms
step:1060/2285 train_time:63884ms step_avg:60.27ms
step:1061/2285 train_time:63946ms step_avg:60.27ms
step:1062/2285 train_time:64005ms step_avg:60.27ms
step:1063/2285 train_time:64067ms step_avg:60.27ms
step:1064/2285 train_time:64127ms step_avg:60.27ms
step:1065/2285 train_time:64189ms step_avg:60.27ms
step:1066/2285 train_time:64248ms step_avg:60.27ms
step:1067/2285 train_time:64310ms step_avg:60.27ms
step:1068/2285 train_time:64370ms step_avg:60.27ms
step:1069/2285 train_time:64433ms step_avg:60.27ms
step:1070/2285 train_time:64493ms step_avg:60.27ms
step:1071/2285 train_time:64555ms step_avg:60.28ms
step:1072/2285 train_time:64615ms step_avg:60.27ms
step:1073/2285 train_time:64677ms step_avg:60.28ms
step:1074/2285 train_time:64736ms step_avg:60.28ms
step:1075/2285 train_time:64798ms step_avg:60.28ms
step:1076/2285 train_time:64857ms step_avg:60.28ms
step:1077/2285 train_time:64919ms step_avg:60.28ms
step:1078/2285 train_time:64978ms step_avg:60.28ms
step:1079/2285 train_time:65040ms step_avg:60.28ms
step:1080/2285 train_time:65099ms step_avg:60.28ms
step:1081/2285 train_time:65161ms step_avg:60.28ms
step:1082/2285 train_time:65220ms step_avg:60.28ms
step:1083/2285 train_time:65282ms step_avg:60.28ms
step:1084/2285 train_time:65341ms step_avg:60.28ms
step:1085/2285 train_time:65402ms step_avg:60.28ms
step:1086/2285 train_time:65462ms step_avg:60.28ms
step:1087/2285 train_time:65524ms step_avg:60.28ms
step:1088/2285 train_time:65583ms step_avg:60.28ms
step:1089/2285 train_time:65646ms step_avg:60.28ms
step:1090/2285 train_time:65704ms step_avg:60.28ms
step:1091/2285 train_time:65766ms step_avg:60.28ms
step:1092/2285 train_time:65825ms step_avg:60.28ms
step:1093/2285 train_time:65887ms step_avg:60.28ms
step:1094/2285 train_time:65946ms step_avg:60.28ms
step:1095/2285 train_time:66008ms step_avg:60.28ms
step:1096/2285 train_time:66067ms step_avg:60.28ms
step:1097/2285 train_time:66129ms step_avg:60.28ms
step:1098/2285 train_time:66189ms step_avg:60.28ms
step:1099/2285 train_time:66251ms step_avg:60.28ms
step:1100/2285 train_time:66311ms step_avg:60.28ms
step:1101/2285 train_time:66374ms step_avg:60.28ms
step:1102/2285 train_time:66434ms step_avg:60.28ms
step:1103/2285 train_time:66496ms step_avg:60.29ms
step:1104/2285 train_time:66556ms step_avg:60.29ms
step:1105/2285 train_time:66618ms step_avg:60.29ms
step:1106/2285 train_time:66678ms step_avg:60.29ms
step:1107/2285 train_time:66740ms step_avg:60.29ms
step:1108/2285 train_time:66799ms step_avg:60.29ms
step:1109/2285 train_time:66861ms step_avg:60.29ms
step:1110/2285 train_time:66920ms step_avg:60.29ms
step:1111/2285 train_time:66981ms step_avg:60.29ms
step:1112/2285 train_time:67041ms step_avg:60.29ms
step:1113/2285 train_time:67102ms step_avg:60.29ms
step:1114/2285 train_time:67162ms step_avg:60.29ms
step:1115/2285 train_time:67223ms step_avg:60.29ms
step:1116/2285 train_time:67283ms step_avg:60.29ms
step:1117/2285 train_time:67344ms step_avg:60.29ms
step:1118/2285 train_time:67403ms step_avg:60.29ms
step:1119/2285 train_time:67465ms step_avg:60.29ms
step:1120/2285 train_time:67524ms step_avg:60.29ms
step:1121/2285 train_time:67586ms step_avg:60.29ms
step:1122/2285 train_time:67645ms step_avg:60.29ms
step:1123/2285 train_time:67708ms step_avg:60.29ms
step:1124/2285 train_time:67768ms step_avg:60.29ms
step:1125/2285 train_time:67829ms step_avg:60.29ms
step:1126/2285 train_time:67890ms step_avg:60.29ms
step:1127/2285 train_time:67952ms step_avg:60.29ms
step:1128/2285 train_time:68012ms step_avg:60.29ms
step:1129/2285 train_time:68074ms step_avg:60.30ms
step:1130/2285 train_time:68133ms step_avg:60.30ms
step:1131/2285 train_time:68196ms step_avg:60.30ms
step:1132/2285 train_time:68255ms step_avg:60.30ms
step:1133/2285 train_time:68317ms step_avg:60.30ms
step:1134/2285 train_time:68377ms step_avg:60.30ms
step:1135/2285 train_time:68439ms step_avg:60.30ms
step:1136/2285 train_time:68498ms step_avg:60.30ms
step:1137/2285 train_time:68560ms step_avg:60.30ms
step:1138/2285 train_time:68619ms step_avg:60.30ms
step:1139/2285 train_time:68681ms step_avg:60.30ms
step:1140/2285 train_time:68740ms step_avg:60.30ms
step:1141/2285 train_time:68802ms step_avg:60.30ms
step:1142/2285 train_time:68861ms step_avg:60.30ms
step:1143/2285 train_time:68923ms step_avg:60.30ms
step:1144/2285 train_time:68983ms step_avg:60.30ms
step:1145/2285 train_time:69044ms step_avg:60.30ms
step:1146/2285 train_time:69104ms step_avg:60.30ms
step:1147/2285 train_time:69167ms step_avg:60.30ms
step:1148/2285 train_time:69226ms step_avg:60.30ms
step:1149/2285 train_time:69288ms step_avg:60.30ms
step:1150/2285 train_time:69348ms step_avg:60.30ms
step:1151/2285 train_time:69410ms step_avg:60.30ms
step:1152/2285 train_time:69470ms step_avg:60.30ms
step:1153/2285 train_time:69533ms step_avg:60.31ms
step:1154/2285 train_time:69594ms step_avg:60.31ms
step:1155/2285 train_time:69656ms step_avg:60.31ms
step:1156/2285 train_time:69716ms step_avg:60.31ms
step:1157/2285 train_time:69778ms step_avg:60.31ms
step:1158/2285 train_time:69838ms step_avg:60.31ms
step:1159/2285 train_time:69900ms step_avg:60.31ms
step:1160/2285 train_time:69960ms step_avg:60.31ms
step:1161/2285 train_time:70021ms step_avg:60.31ms
step:1162/2285 train_time:70080ms step_avg:60.31ms
step:1163/2285 train_time:70142ms step_avg:60.31ms
step:1164/2285 train_time:70202ms step_avg:60.31ms
step:1165/2285 train_time:70264ms step_avg:60.31ms
step:1166/2285 train_time:70324ms step_avg:60.31ms
step:1167/2285 train_time:70386ms step_avg:60.31ms
step:1168/2285 train_time:70445ms step_avg:60.31ms
step:1169/2285 train_time:70507ms step_avg:60.31ms
step:1170/2285 train_time:70567ms step_avg:60.31ms
step:1171/2285 train_time:70630ms step_avg:60.32ms
step:1172/2285 train_time:70690ms step_avg:60.32ms
step:1173/2285 train_time:70752ms step_avg:60.32ms
step:1174/2285 train_time:70813ms step_avg:60.32ms
step:1175/2285 train_time:70876ms step_avg:60.32ms
step:1176/2285 train_time:70935ms step_avg:60.32ms
step:1177/2285 train_time:70998ms step_avg:60.32ms
step:1178/2285 train_time:71058ms step_avg:60.32ms
step:1179/2285 train_time:71120ms step_avg:60.32ms
step:1180/2285 train_time:71179ms step_avg:60.32ms
step:1181/2285 train_time:71241ms step_avg:60.32ms
step:1182/2285 train_time:71301ms step_avg:60.32ms
step:1183/2285 train_time:71362ms step_avg:60.32ms
step:1184/2285 train_time:71422ms step_avg:60.32ms
step:1185/2285 train_time:71485ms step_avg:60.32ms
step:1186/2285 train_time:71545ms step_avg:60.32ms
step:1187/2285 train_time:71607ms step_avg:60.33ms
step:1188/2285 train_time:71667ms step_avg:60.33ms
step:1189/2285 train_time:71729ms step_avg:60.33ms
step:1190/2285 train_time:71789ms step_avg:60.33ms
step:1191/2285 train_time:71851ms step_avg:60.33ms
step:1192/2285 train_time:71912ms step_avg:60.33ms
step:1193/2285 train_time:71975ms step_avg:60.33ms
step:1194/2285 train_time:72035ms step_avg:60.33ms
step:1195/2285 train_time:72097ms step_avg:60.33ms
step:1196/2285 train_time:72157ms step_avg:60.33ms
step:1197/2285 train_time:72219ms step_avg:60.33ms
step:1198/2285 train_time:72278ms step_avg:60.33ms
step:1199/2285 train_time:72341ms step_avg:60.33ms
step:1200/2285 train_time:72400ms step_avg:60.33ms
step:1201/2285 train_time:72463ms step_avg:60.34ms
step:1202/2285 train_time:72522ms step_avg:60.33ms
step:1203/2285 train_time:72584ms step_avg:60.34ms
step:1204/2285 train_time:72643ms step_avg:60.33ms
step:1205/2285 train_time:72705ms step_avg:60.34ms
step:1206/2285 train_time:72765ms step_avg:60.34ms
step:1207/2285 train_time:72827ms step_avg:60.34ms
step:1208/2285 train_time:72887ms step_avg:60.34ms
step:1209/2285 train_time:72949ms step_avg:60.34ms
step:1210/2285 train_time:73009ms step_avg:60.34ms
step:1211/2285 train_time:73072ms step_avg:60.34ms
step:1212/2285 train_time:73132ms step_avg:60.34ms
step:1213/2285 train_time:73195ms step_avg:60.34ms
step:1214/2285 train_time:73256ms step_avg:60.34ms
step:1215/2285 train_time:73318ms step_avg:60.34ms
step:1216/2285 train_time:73378ms step_avg:60.34ms
step:1217/2285 train_time:73440ms step_avg:60.35ms
step:1218/2285 train_time:73500ms step_avg:60.34ms
step:1219/2285 train_time:73562ms step_avg:60.35ms
step:1220/2285 train_time:73621ms step_avg:60.34ms
step:1221/2285 train_time:73683ms step_avg:60.35ms
step:1222/2285 train_time:73742ms step_avg:60.35ms
step:1223/2285 train_time:73805ms step_avg:60.35ms
step:1224/2285 train_time:73864ms step_avg:60.35ms
step:1225/2285 train_time:73927ms step_avg:60.35ms
step:1226/2285 train_time:73987ms step_avg:60.35ms
step:1227/2285 train_time:74049ms step_avg:60.35ms
step:1228/2285 train_time:74109ms step_avg:60.35ms
step:1229/2285 train_time:74171ms step_avg:60.35ms
step:1230/2285 train_time:74232ms step_avg:60.35ms
step:1231/2285 train_time:74294ms step_avg:60.35ms
step:1232/2285 train_time:74354ms step_avg:60.35ms
step:1233/2285 train_time:74417ms step_avg:60.35ms
step:1234/2285 train_time:74477ms step_avg:60.35ms
step:1235/2285 train_time:74538ms step_avg:60.35ms
step:1236/2285 train_time:74598ms step_avg:60.35ms
step:1237/2285 train_time:74660ms step_avg:60.36ms
step:1238/2285 train_time:74720ms step_avg:60.36ms
step:1239/2285 train_time:74782ms step_avg:60.36ms
step:1240/2285 train_time:74841ms step_avg:60.36ms
step:1241/2285 train_time:74903ms step_avg:60.36ms
step:1242/2285 train_time:74963ms step_avg:60.36ms
step:1243/2285 train_time:75025ms step_avg:60.36ms
step:1244/2285 train_time:75085ms step_avg:60.36ms
step:1245/2285 train_time:75148ms step_avg:60.36ms
step:1246/2285 train_time:75207ms step_avg:60.36ms
step:1247/2285 train_time:75269ms step_avg:60.36ms
step:1248/2285 train_time:75330ms step_avg:60.36ms
step:1249/2285 train_time:75392ms step_avg:60.36ms
step:1250/2285 train_time:75452ms step_avg:60.36ms
step:1250/2285 val_loss:3.4998 train_time:75516ms step_avg:60.41ms
step:1251/2285 train_time:75535ms step_avg:60.38ms
step:1252/2285 train_time:75578ms step_avg:60.37ms
step:1253/2285 train_time:75639ms step_avg:60.37ms
step:1254/2285 train_time:75699ms step_avg:60.37ms
step:1255/2285 train_time:75761ms step_avg:60.37ms
step:1256/2285 train_time:75822ms step_avg:60.37ms
step:1257/2285 train_time:75883ms step_avg:60.37ms
step:1258/2285 train_time:75941ms step_avg:60.37ms
step:1259/2285 train_time:76002ms step_avg:60.37ms
step:1260/2285 train_time:76061ms step_avg:60.37ms
step:1261/2285 train_time:76122ms step_avg:60.37ms
step:1262/2285 train_time:76181ms step_avg:60.37ms
step:1263/2285 train_time:76242ms step_avg:60.37ms
step:1264/2285 train_time:76301ms step_avg:60.36ms
step:1265/2285 train_time:76363ms step_avg:60.37ms
step:1266/2285 train_time:76428ms step_avg:60.37ms
step:1267/2285 train_time:76494ms step_avg:60.37ms
step:1268/2285 train_time:76555ms step_avg:60.37ms
step:1269/2285 train_time:76617ms step_avg:60.38ms
step:1270/2285 train_time:76677ms step_avg:60.38ms
step:1271/2285 train_time:76739ms step_avg:60.38ms
step:1272/2285 train_time:76798ms step_avg:60.38ms
step:1273/2285 train_time:76860ms step_avg:60.38ms
step:1274/2285 train_time:76919ms step_avg:60.38ms
step:1275/2285 train_time:76980ms step_avg:60.38ms
step:1276/2285 train_time:77039ms step_avg:60.38ms
step:1277/2285 train_time:77100ms step_avg:60.38ms
step:1278/2285 train_time:77159ms step_avg:60.38ms
step:1279/2285 train_time:77221ms step_avg:60.38ms
step:1280/2285 train_time:77280ms step_avg:60.38ms
step:1281/2285 train_time:77343ms step_avg:60.38ms
step:1282/2285 train_time:77405ms step_avg:60.38ms
step:1283/2285 train_time:77470ms step_avg:60.38ms
step:1284/2285 train_time:77530ms step_avg:60.38ms
step:1285/2285 train_time:77592ms step_avg:60.38ms
step:1286/2285 train_time:77651ms step_avg:60.38ms
step:1287/2285 train_time:77713ms step_avg:60.38ms
step:1288/2285 train_time:77773ms step_avg:60.38ms
step:1289/2285 train_time:77834ms step_avg:60.38ms
step:1290/2285 train_time:77893ms step_avg:60.38ms
step:1291/2285 train_time:77955ms step_avg:60.38ms
step:1292/2285 train_time:78015ms step_avg:60.38ms
step:1293/2285 train_time:78077ms step_avg:60.38ms
step:1294/2285 train_time:78136ms step_avg:60.38ms
step:1295/2285 train_time:78197ms step_avg:60.38ms
step:1296/2285 train_time:78257ms step_avg:60.38ms
step:1297/2285 train_time:78319ms step_avg:60.38ms
step:1298/2285 train_time:78380ms step_avg:60.39ms
step:1299/2285 train_time:78444ms step_avg:60.39ms
step:1300/2285 train_time:78504ms step_avg:60.39ms
step:1301/2285 train_time:78567ms step_avg:60.39ms
step:1302/2285 train_time:78627ms step_avg:60.39ms
step:1303/2285 train_time:78689ms step_avg:60.39ms
step:1304/2285 train_time:78749ms step_avg:60.39ms
step:1305/2285 train_time:78811ms step_avg:60.39ms
step:1306/2285 train_time:78870ms step_avg:60.39ms
step:1307/2285 train_time:78931ms step_avg:60.39ms
step:1308/2285 train_time:78991ms step_avg:60.39ms
step:1309/2285 train_time:79053ms step_avg:60.39ms
step:1310/2285 train_time:79113ms step_avg:60.39ms
step:1311/2285 train_time:79175ms step_avg:60.39ms
step:1312/2285 train_time:79235ms step_avg:60.39ms
step:1313/2285 train_time:79297ms step_avg:60.39ms
step:1314/2285 train_time:79356ms step_avg:60.39ms
step:1315/2285 train_time:79419ms step_avg:60.39ms
step:1316/2285 train_time:79479ms step_avg:60.39ms
step:1317/2285 train_time:79542ms step_avg:60.40ms
step:1318/2285 train_time:79602ms step_avg:60.40ms
step:1319/2285 train_time:79665ms step_avg:60.40ms
step:1320/2285 train_time:79724ms step_avg:60.40ms
step:1321/2285 train_time:79786ms step_avg:60.40ms
step:1322/2285 train_time:79846ms step_avg:60.40ms
step:1323/2285 train_time:79908ms step_avg:60.40ms
step:1324/2285 train_time:79968ms step_avg:60.40ms
step:1325/2285 train_time:80030ms step_avg:60.40ms
step:1326/2285 train_time:80089ms step_avg:60.40ms
step:1327/2285 train_time:80151ms step_avg:60.40ms
step:1328/2285 train_time:80211ms step_avg:60.40ms
step:1329/2285 train_time:80273ms step_avg:60.40ms
step:1330/2285 train_time:80333ms step_avg:60.40ms
step:1331/2285 train_time:80396ms step_avg:60.40ms
step:1332/2285 train_time:80455ms step_avg:60.40ms
step:1333/2285 train_time:80518ms step_avg:60.40ms
step:1334/2285 train_time:80577ms step_avg:60.40ms
step:1335/2285 train_time:80640ms step_avg:60.40ms
step:1336/2285 train_time:80700ms step_avg:60.40ms
step:1337/2285 train_time:80763ms step_avg:60.41ms
step:1338/2285 train_time:80822ms step_avg:60.41ms
step:1339/2285 train_time:80884ms step_avg:60.41ms
step:1340/2285 train_time:80944ms step_avg:60.41ms
step:1341/2285 train_time:81006ms step_avg:60.41ms
step:1342/2285 train_time:81066ms step_avg:60.41ms
step:1343/2285 train_time:81128ms step_avg:60.41ms
step:1344/2285 train_time:81187ms step_avg:60.41ms
step:1345/2285 train_time:81250ms step_avg:60.41ms
step:1346/2285 train_time:81309ms step_avg:60.41ms
step:1347/2285 train_time:81371ms step_avg:60.41ms
step:1348/2285 train_time:81431ms step_avg:60.41ms
step:1349/2285 train_time:81493ms step_avg:60.41ms
step:1350/2285 train_time:81553ms step_avg:60.41ms
step:1351/2285 train_time:81616ms step_avg:60.41ms
step:1352/2285 train_time:81675ms step_avg:60.41ms
step:1353/2285 train_time:81737ms step_avg:60.41ms
step:1354/2285 train_time:81797ms step_avg:60.41ms
step:1355/2285 train_time:81859ms step_avg:60.41ms
step:1356/2285 train_time:81919ms step_avg:60.41ms
step:1357/2285 train_time:81982ms step_avg:60.41ms
step:1358/2285 train_time:82042ms step_avg:60.41ms
step:1359/2285 train_time:82104ms step_avg:60.41ms
step:1360/2285 train_time:82164ms step_avg:60.41ms
step:1361/2285 train_time:82226ms step_avg:60.42ms
step:1362/2285 train_time:82286ms step_avg:60.42ms
step:1363/2285 train_time:82348ms step_avg:60.42ms
step:1364/2285 train_time:82408ms step_avg:60.42ms
step:1365/2285 train_time:82471ms step_avg:60.42ms
step:1366/2285 train_time:82530ms step_avg:60.42ms
step:1367/2285 train_time:82592ms step_avg:60.42ms
step:1368/2285 train_time:82652ms step_avg:60.42ms
step:1369/2285 train_time:82715ms step_avg:60.42ms
step:1370/2285 train_time:82774ms step_avg:60.42ms
step:1371/2285 train_time:82836ms step_avg:60.42ms
step:1372/2285 train_time:82896ms step_avg:60.42ms
step:1373/2285 train_time:82958ms step_avg:60.42ms
step:1374/2285 train_time:83017ms step_avg:60.42ms
step:1375/2285 train_time:83080ms step_avg:60.42ms
step:1376/2285 train_time:83140ms step_avg:60.42ms
step:1377/2285 train_time:83202ms step_avg:60.42ms
step:1378/2285 train_time:83262ms step_avg:60.42ms
step:1379/2285 train_time:83324ms step_avg:60.42ms
step:1380/2285 train_time:83384ms step_avg:60.42ms
step:1381/2285 train_time:83446ms step_avg:60.42ms
step:1382/2285 train_time:83506ms step_avg:60.42ms
step:1383/2285 train_time:83568ms step_avg:60.43ms
step:1384/2285 train_time:83632ms step_avg:60.43ms
step:1385/2285 train_time:83691ms step_avg:60.43ms
step:1386/2285 train_time:83750ms step_avg:60.43ms
step:1387/2285 train_time:83812ms step_avg:60.43ms
step:1388/2285 train_time:83871ms step_avg:60.43ms
step:1389/2285 train_time:83933ms step_avg:60.43ms
step:1390/2285 train_time:83993ms step_avg:60.43ms
step:1391/2285 train_time:84055ms step_avg:60.43ms
step:1392/2285 train_time:84115ms step_avg:60.43ms
step:1393/2285 train_time:84177ms step_avg:60.43ms
step:1394/2285 train_time:84237ms step_avg:60.43ms
step:1395/2285 train_time:84299ms step_avg:60.43ms
step:1396/2285 train_time:84359ms step_avg:60.43ms
step:1397/2285 train_time:84421ms step_avg:60.43ms
step:1398/2285 train_time:84482ms step_avg:60.43ms
step:1399/2285 train_time:84544ms step_avg:60.43ms
step:1400/2285 train_time:84604ms step_avg:60.43ms
step:1401/2285 train_time:84666ms step_avg:60.43ms
step:1402/2285 train_time:84726ms step_avg:60.43ms
step:1403/2285 train_time:84788ms step_avg:60.43ms
step:1404/2285 train_time:84848ms step_avg:60.43ms
step:1405/2285 train_time:84910ms step_avg:60.43ms
step:1406/2285 train_time:84969ms step_avg:60.43ms
step:1407/2285 train_time:85031ms step_avg:60.43ms
step:1408/2285 train_time:85091ms step_avg:60.43ms
step:1409/2285 train_time:85153ms step_avg:60.43ms
step:1410/2285 train_time:85213ms step_avg:60.43ms
step:1411/2285 train_time:85276ms step_avg:60.44ms
step:1412/2285 train_time:85336ms step_avg:60.44ms
step:1413/2285 train_time:85398ms step_avg:60.44ms
step:1414/2285 train_time:85458ms step_avg:60.44ms
step:1415/2285 train_time:85521ms step_avg:60.44ms
step:1416/2285 train_time:85581ms step_avg:60.44ms
step:1417/2285 train_time:85643ms step_avg:60.44ms
step:1418/2285 train_time:85703ms step_avg:60.44ms
step:1419/2285 train_time:85766ms step_avg:60.44ms
step:1420/2285 train_time:85826ms step_avg:60.44ms
step:1421/2285 train_time:85888ms step_avg:60.44ms
step:1422/2285 train_time:85947ms step_avg:60.44ms
step:1423/2285 train_time:86009ms step_avg:60.44ms
step:1424/2285 train_time:86069ms step_avg:60.44ms
step:1425/2285 train_time:86130ms step_avg:60.44ms
step:1426/2285 train_time:86190ms step_avg:60.44ms
step:1427/2285 train_time:86252ms step_avg:60.44ms
step:1428/2285 train_time:86312ms step_avg:60.44ms
step:1429/2285 train_time:86374ms step_avg:60.44ms
step:1430/2285 train_time:86435ms step_avg:60.44ms
step:1431/2285 train_time:86497ms step_avg:60.44ms
step:1432/2285 train_time:86556ms step_avg:60.44ms
step:1433/2285 train_time:86619ms step_avg:60.45ms
step:1434/2285 train_time:86679ms step_avg:60.45ms
step:1435/2285 train_time:86741ms step_avg:60.45ms
step:1436/2285 train_time:86802ms step_avg:60.45ms
step:1437/2285 train_time:86865ms step_avg:60.45ms
step:1438/2285 train_time:86925ms step_avg:60.45ms
step:1439/2285 train_time:86987ms step_avg:60.45ms
step:1440/2285 train_time:87046ms step_avg:60.45ms
step:1441/2285 train_time:87108ms step_avg:60.45ms
step:1442/2285 train_time:87169ms step_avg:60.45ms
step:1443/2285 train_time:87231ms step_avg:60.45ms
step:1444/2285 train_time:87290ms step_avg:60.45ms
step:1445/2285 train_time:87352ms step_avg:60.45ms
step:1446/2285 train_time:87412ms step_avg:60.45ms
step:1447/2285 train_time:87474ms step_avg:60.45ms
step:1448/2285 train_time:87533ms step_avg:60.45ms
step:1449/2285 train_time:87596ms step_avg:60.45ms
step:1450/2285 train_time:87655ms step_avg:60.45ms
step:1451/2285 train_time:87717ms step_avg:60.45ms
step:1452/2285 train_time:87777ms step_avg:60.45ms
step:1453/2285 train_time:87839ms step_avg:60.45ms
step:1454/2285 train_time:87899ms step_avg:60.45ms
step:1455/2285 train_time:87961ms step_avg:60.45ms
step:1456/2285 train_time:88022ms step_avg:60.45ms
step:1457/2285 train_time:88085ms step_avg:60.46ms
step:1458/2285 train_time:88145ms step_avg:60.46ms
step:1459/2285 train_time:88207ms step_avg:60.46ms
step:1460/2285 train_time:88268ms step_avg:60.46ms
step:1461/2285 train_time:88330ms step_avg:60.46ms
step:1462/2285 train_time:88389ms step_avg:60.46ms
step:1463/2285 train_time:88452ms step_avg:60.46ms
step:1464/2285 train_time:88511ms step_avg:60.46ms
step:1465/2285 train_time:88573ms step_avg:60.46ms
step:1466/2285 train_time:88633ms step_avg:60.46ms
step:1467/2285 train_time:88695ms step_avg:60.46ms
step:1468/2285 train_time:88755ms step_avg:60.46ms
step:1469/2285 train_time:88817ms step_avg:60.46ms
step:1470/2285 train_time:88876ms step_avg:60.46ms
step:1471/2285 train_time:88938ms step_avg:60.46ms
step:1472/2285 train_time:88998ms step_avg:60.46ms
step:1473/2285 train_time:89061ms step_avg:60.46ms
step:1474/2285 train_time:89121ms step_avg:60.46ms
step:1475/2285 train_time:89183ms step_avg:60.46ms
step:1476/2285 train_time:89243ms step_avg:60.46ms
step:1477/2285 train_time:89306ms step_avg:60.46ms
step:1478/2285 train_time:89366ms step_avg:60.46ms
step:1479/2285 train_time:89430ms step_avg:60.47ms
step:1480/2285 train_time:89488ms step_avg:60.47ms
step:1481/2285 train_time:89550ms step_avg:60.47ms
step:1482/2285 train_time:89610ms step_avg:60.47ms
step:1483/2285 train_time:89672ms step_avg:60.47ms
step:1484/2285 train_time:89731ms step_avg:60.47ms
step:1485/2285 train_time:89794ms step_avg:60.47ms
step:1486/2285 train_time:89853ms step_avg:60.47ms
step:1487/2285 train_time:89916ms step_avg:60.47ms
step:1488/2285 train_time:89976ms step_avg:60.47ms
step:1489/2285 train_time:90038ms step_avg:60.47ms
step:1490/2285 train_time:90097ms step_avg:60.47ms
step:1491/2285 train_time:90159ms step_avg:60.47ms
step:1492/2285 train_time:90220ms step_avg:60.47ms
step:1493/2285 train_time:90283ms step_avg:60.47ms
step:1494/2285 train_time:90343ms step_avg:60.47ms
step:1495/2285 train_time:90406ms step_avg:60.47ms
step:1496/2285 train_time:90466ms step_avg:60.47ms
step:1497/2285 train_time:90528ms step_avg:60.47ms
step:1498/2285 train_time:90588ms step_avg:60.47ms
step:1499/2285 train_time:90650ms step_avg:60.47ms
step:1500/2285 train_time:90710ms step_avg:60.47ms
step:1500/2285 val_loss:3.4305 train_time:90773ms step_avg:60.52ms
step:1501/2285 train_time:90791ms step_avg:60.49ms
step:1502/2285 train_time:90834ms step_avg:60.48ms
step:1503/2285 train_time:90898ms step_avg:60.48ms
step:1504/2285 train_time:90958ms step_avg:60.48ms
step:1505/2285 train_time:91020ms step_avg:60.48ms
step:1506/2285 train_time:91080ms step_avg:60.48ms
step:1507/2285 train_time:91141ms step_avg:60.48ms
step:1508/2285 train_time:91200ms step_avg:60.48ms
step:1509/2285 train_time:91261ms step_avg:60.48ms
step:1510/2285 train_time:91321ms step_avg:60.48ms
step:1511/2285 train_time:91383ms step_avg:60.48ms
step:1512/2285 train_time:91443ms step_avg:60.48ms
step:1513/2285 train_time:91504ms step_avg:60.48ms
step:1514/2285 train_time:91564ms step_avg:60.48ms
step:1515/2285 train_time:91626ms step_avg:60.48ms
step:1516/2285 train_time:91687ms step_avg:60.48ms
step:1517/2285 train_time:91751ms step_avg:60.48ms
step:1518/2285 train_time:91812ms step_avg:60.48ms
step:1519/2285 train_time:91875ms step_avg:60.48ms
step:1520/2285 train_time:91935ms step_avg:60.48ms
step:1521/2285 train_time:91997ms step_avg:60.48ms
step:1522/2285 train_time:92056ms step_avg:60.48ms
step:1523/2285 train_time:92118ms step_avg:60.48ms
step:1524/2285 train_time:92177ms step_avg:60.48ms
step:1525/2285 train_time:92239ms step_avg:60.48ms
step:1526/2285 train_time:92298ms step_avg:60.48ms
step:1527/2285 train_time:92360ms step_avg:60.48ms
step:1528/2285 train_time:92419ms step_avg:60.48ms
step:1529/2285 train_time:92481ms step_avg:60.48ms
step:1530/2285 train_time:92540ms step_avg:60.48ms
step:1531/2285 train_time:92602ms step_avg:60.48ms
step:1532/2285 train_time:92663ms step_avg:60.49ms
step:1533/2285 train_time:92727ms step_avg:60.49ms
step:1534/2285 train_time:92788ms step_avg:60.49ms
step:1535/2285 train_time:92851ms step_avg:60.49ms
step:1536/2285 train_time:92911ms step_avg:60.49ms
step:1537/2285 train_time:92973ms step_avg:60.49ms
step:1538/2285 train_time:93033ms step_avg:60.49ms
step:1539/2285 train_time:93095ms step_avg:60.49ms
step:1540/2285 train_time:93155ms step_avg:60.49ms
step:1541/2285 train_time:93217ms step_avg:60.49ms
step:1542/2285 train_time:93277ms step_avg:60.49ms
step:1543/2285 train_time:93339ms step_avg:60.49ms
step:1544/2285 train_time:93398ms step_avg:60.49ms
step:1545/2285 train_time:93461ms step_avg:60.49ms
step:1546/2285 train_time:93520ms step_avg:60.49ms
step:1547/2285 train_time:93582ms step_avg:60.49ms
step:1548/2285 train_time:93642ms step_avg:60.49ms
step:1549/2285 train_time:93706ms step_avg:60.49ms
step:1550/2285 train_time:93767ms step_avg:60.49ms
step:1551/2285 train_time:93830ms step_avg:60.50ms
step:1552/2285 train_time:93891ms step_avg:60.50ms
step:1553/2285 train_time:93953ms step_avg:60.50ms
step:1554/2285 train_time:94014ms step_avg:60.50ms
step:1555/2285 train_time:94076ms step_avg:60.50ms
step:1556/2285 train_time:94135ms step_avg:60.50ms
step:1557/2285 train_time:94197ms step_avg:60.50ms
step:1558/2285 train_time:94257ms step_avg:60.50ms
step:1559/2285 train_time:94319ms step_avg:60.50ms
step:1560/2285 train_time:94378ms step_avg:60.50ms
step:1561/2285 train_time:94440ms step_avg:60.50ms
step:1562/2285 train_time:94500ms step_avg:60.50ms
step:1563/2285 train_time:94563ms step_avg:60.50ms
step:1564/2285 train_time:94623ms step_avg:60.50ms
step:1565/2285 train_time:94686ms step_avg:60.50ms
step:1566/2285 train_time:94747ms step_avg:60.50ms
step:1567/2285 train_time:94810ms step_avg:60.50ms
step:1568/2285 train_time:94870ms step_avg:60.50ms
step:1569/2285 train_time:94933ms step_avg:60.51ms
step:1570/2285 train_time:94993ms step_avg:60.50ms
step:1571/2285 train_time:95054ms step_avg:60.51ms
step:1572/2285 train_time:95114ms step_avg:60.51ms
step:1573/2285 train_time:95177ms step_avg:60.51ms
step:1574/2285 train_time:95236ms step_avg:60.51ms
step:1575/2285 train_time:95298ms step_avg:60.51ms
step:1576/2285 train_time:95358ms step_avg:60.51ms
step:1577/2285 train_time:95420ms step_avg:60.51ms
step:1578/2285 train_time:95479ms step_avg:60.51ms
step:1579/2285 train_time:95541ms step_avg:60.51ms
step:1580/2285 train_time:95601ms step_avg:60.51ms
step:1581/2285 train_time:95663ms step_avg:60.51ms
step:1582/2285 train_time:95724ms step_avg:60.51ms
step:1583/2285 train_time:95787ms step_avg:60.51ms
step:1584/2285 train_time:95848ms step_avg:60.51ms
step:1585/2285 train_time:95911ms step_avg:60.51ms
step:1586/2285 train_time:95971ms step_avg:60.51ms
step:1587/2285 train_time:96034ms step_avg:60.51ms
step:1588/2285 train_time:96094ms step_avg:60.51ms
step:1589/2285 train_time:96156ms step_avg:60.51ms
step:1590/2285 train_time:96216ms step_avg:60.51ms
step:1591/2285 train_time:96278ms step_avg:60.51ms
step:1592/2285 train_time:96337ms step_avg:60.51ms
step:1593/2285 train_time:96399ms step_avg:60.51ms
step:1594/2285 train_time:96459ms step_avg:60.51ms
step:1595/2285 train_time:96521ms step_avg:60.51ms
step:1596/2285 train_time:96580ms step_avg:60.51ms
step:1597/2285 train_time:96643ms step_avg:60.52ms
step:1598/2285 train_time:96703ms step_avg:60.52ms
step:1599/2285 train_time:96767ms step_avg:60.52ms
step:1600/2285 train_time:96827ms step_avg:60.52ms
step:1601/2285 train_time:96890ms step_avg:60.52ms
step:1602/2285 train_time:96949ms step_avg:60.52ms
step:1603/2285 train_time:97012ms step_avg:60.52ms
step:1604/2285 train_time:97072ms step_avg:60.52ms
step:1605/2285 train_time:97135ms step_avg:60.52ms
step:1606/2285 train_time:97195ms step_avg:60.52ms
step:1607/2285 train_time:97256ms step_avg:60.52ms
step:1608/2285 train_time:97316ms step_avg:60.52ms
step:1609/2285 train_time:97378ms step_avg:60.52ms
step:1610/2285 train_time:97437ms step_avg:60.52ms
step:1611/2285 train_time:97500ms step_avg:60.52ms
step:1612/2285 train_time:97560ms step_avg:60.52ms
step:1613/2285 train_time:97622ms step_avg:60.52ms
step:1614/2285 train_time:97682ms step_avg:60.52ms
step:1615/2285 train_time:97745ms step_avg:60.52ms
step:1616/2285 train_time:97807ms step_avg:60.52ms
step:1617/2285 train_time:97869ms step_avg:60.53ms
step:1618/2285 train_time:97929ms step_avg:60.52ms
step:1619/2285 train_time:97992ms step_avg:60.53ms
step:1620/2285 train_time:98051ms step_avg:60.53ms
step:1621/2285 train_time:98114ms step_avg:60.53ms
step:1622/2285 train_time:98174ms step_avg:60.53ms
step:1623/2285 train_time:98235ms step_avg:60.53ms
step:1624/2285 train_time:98295ms step_avg:60.53ms
step:1625/2285 train_time:98357ms step_avg:60.53ms
step:1626/2285 train_time:98417ms step_avg:60.53ms
step:1627/2285 train_time:98479ms step_avg:60.53ms
step:1628/2285 train_time:98539ms step_avg:60.53ms
step:1629/2285 train_time:98601ms step_avg:60.53ms
step:1630/2285 train_time:98662ms step_avg:60.53ms
step:1631/2285 train_time:98724ms step_avg:60.53ms
step:1632/2285 train_time:98784ms step_avg:60.53ms
step:1633/2285 train_time:98847ms step_avg:60.53ms
step:1634/2285 train_time:98908ms step_avg:60.53ms
step:1635/2285 train_time:98970ms step_avg:60.53ms
step:1636/2285 train_time:99031ms step_avg:60.53ms
step:1637/2285 train_time:99093ms step_avg:60.53ms
step:1638/2285 train_time:99153ms step_avg:60.53ms
step:1639/2285 train_time:99215ms step_avg:60.53ms
step:1640/2285 train_time:99275ms step_avg:60.53ms
step:1641/2285 train_time:99337ms step_avg:60.53ms
step:1642/2285 train_time:99396ms step_avg:60.53ms
step:1643/2285 train_time:99458ms step_avg:60.53ms
step:1644/2285 train_time:99518ms step_avg:60.53ms
step:1645/2285 train_time:99580ms step_avg:60.53ms
step:1646/2285 train_time:99640ms step_avg:60.53ms
step:1647/2285 train_time:99702ms step_avg:60.54ms
step:1648/2285 train_time:99762ms step_avg:60.53ms
step:1649/2285 train_time:99825ms step_avg:60.54ms
step:1650/2285 train_time:99885ms step_avg:60.54ms
step:1651/2285 train_time:99949ms step_avg:60.54ms
step:1652/2285 train_time:100010ms step_avg:60.54ms
step:1653/2285 train_time:100072ms step_avg:60.54ms
step:1654/2285 train_time:100132ms step_avg:60.54ms
step:1655/2285 train_time:100194ms step_avg:60.54ms
step:1656/2285 train_time:100254ms step_avg:60.54ms
step:1657/2285 train_time:100317ms step_avg:60.54ms
step:1658/2285 train_time:100376ms step_avg:60.54ms
step:1659/2285 train_time:100438ms step_avg:60.54ms
step:1660/2285 train_time:100498ms step_avg:60.54ms
step:1661/2285 train_time:100560ms step_avg:60.54ms
step:1662/2285 train_time:100620ms step_avg:60.54ms
step:1663/2285 train_time:100682ms step_avg:60.54ms
step:1664/2285 train_time:100741ms step_avg:60.54ms
step:1665/2285 train_time:100804ms step_avg:60.54ms
step:1666/2285 train_time:100864ms step_avg:60.54ms
step:1667/2285 train_time:100926ms step_avg:60.54ms
step:1668/2285 train_time:100987ms step_avg:60.54ms
step:1669/2285 train_time:101049ms step_avg:60.54ms
step:1670/2285 train_time:101110ms step_avg:60.54ms
step:1671/2285 train_time:101172ms step_avg:60.55ms
step:1672/2285 train_time:101232ms step_avg:60.55ms
step:1673/2285 train_time:101295ms step_avg:60.55ms
step:1674/2285 train_time:101354ms step_avg:60.55ms
step:1675/2285 train_time:101416ms step_avg:60.55ms
step:1676/2285 train_time:101475ms step_avg:60.55ms
step:1677/2285 train_time:101538ms step_avg:60.55ms
step:1678/2285 train_time:101598ms step_avg:60.55ms
step:1679/2285 train_time:101660ms step_avg:60.55ms
step:1680/2285 train_time:101720ms step_avg:60.55ms
step:1681/2285 train_time:101782ms step_avg:60.55ms
step:1682/2285 train_time:101842ms step_avg:60.55ms
step:1683/2285 train_time:101905ms step_avg:60.55ms
step:1684/2285 train_time:101965ms step_avg:60.55ms
step:1685/2285 train_time:102028ms step_avg:60.55ms
step:1686/2285 train_time:102088ms step_avg:60.55ms
step:1687/2285 train_time:102152ms step_avg:60.55ms
step:1688/2285 train_time:102212ms step_avg:60.55ms
step:1689/2285 train_time:102275ms step_avg:60.55ms
step:1690/2285 train_time:102334ms step_avg:60.55ms
step:1691/2285 train_time:102396ms step_avg:60.55ms
step:1692/2285 train_time:102456ms step_avg:60.55ms
step:1693/2285 train_time:102518ms step_avg:60.55ms
step:1694/2285 train_time:102578ms step_avg:60.55ms
step:1695/2285 train_time:102640ms step_avg:60.55ms
step:1696/2285 train_time:102700ms step_avg:60.55ms
step:1697/2285 train_time:102762ms step_avg:60.56ms
step:1698/2285 train_time:102822ms step_avg:60.55ms
step:1699/2285 train_time:102885ms step_avg:60.56ms
step:1700/2285 train_time:102945ms step_avg:60.56ms
step:1701/2285 train_time:103008ms step_avg:60.56ms
step:1702/2285 train_time:103069ms step_avg:60.56ms
step:1703/2285 train_time:103132ms step_avg:60.56ms
step:1704/2285 train_time:103192ms step_avg:60.56ms
step:1705/2285 train_time:103254ms step_avg:60.56ms
step:1706/2285 train_time:103314ms step_avg:60.56ms
step:1707/2285 train_time:103378ms step_avg:60.56ms
step:1708/2285 train_time:103436ms step_avg:60.56ms
step:1709/2285 train_time:103499ms step_avg:60.56ms
step:1710/2285 train_time:103559ms step_avg:60.56ms
step:1711/2285 train_time:103621ms step_avg:60.56ms
step:1712/2285 train_time:103680ms step_avg:60.56ms
step:1713/2285 train_time:103742ms step_avg:60.56ms
step:1714/2285 train_time:103802ms step_avg:60.56ms
step:1715/2285 train_time:103865ms step_avg:60.56ms
step:1716/2285 train_time:103925ms step_avg:60.56ms
step:1717/2285 train_time:103988ms step_avg:60.56ms
step:1718/2285 train_time:104048ms step_avg:60.56ms
step:1719/2285 train_time:104112ms step_avg:60.57ms
step:1720/2285 train_time:104171ms step_avg:60.56ms
step:1721/2285 train_time:104234ms step_avg:60.57ms
step:1722/2285 train_time:104294ms step_avg:60.57ms
step:1723/2285 train_time:104356ms step_avg:60.57ms
step:1724/2285 train_time:104416ms step_avg:60.57ms
step:1725/2285 train_time:104478ms step_avg:60.57ms
step:1726/2285 train_time:104537ms step_avg:60.57ms
step:1727/2285 train_time:104599ms step_avg:60.57ms
step:1728/2285 train_time:104659ms step_avg:60.57ms
step:1729/2285 train_time:104721ms step_avg:60.57ms
step:1730/2285 train_time:104781ms step_avg:60.57ms
step:1731/2285 train_time:104843ms step_avg:60.57ms
step:1732/2285 train_time:104904ms step_avg:60.57ms
step:1733/2285 train_time:104967ms step_avg:60.57ms
step:1734/2285 train_time:105027ms step_avg:60.57ms
step:1735/2285 train_time:105090ms step_avg:60.57ms
step:1736/2285 train_time:105150ms step_avg:60.57ms
step:1737/2285 train_time:105212ms step_avg:60.57ms
step:1738/2285 train_time:105272ms step_avg:60.57ms
step:1739/2285 train_time:105334ms step_avg:60.57ms
step:1740/2285 train_time:105393ms step_avg:60.57ms
step:1741/2285 train_time:105456ms step_avg:60.57ms
step:1742/2285 train_time:105516ms step_avg:60.57ms
step:1743/2285 train_time:105578ms step_avg:60.57ms
step:1744/2285 train_time:105638ms step_avg:60.57ms
step:1745/2285 train_time:105700ms step_avg:60.57ms
step:1746/2285 train_time:105759ms step_avg:60.57ms
step:1747/2285 train_time:105821ms step_avg:60.57ms
step:1748/2285 train_time:105881ms step_avg:60.57ms
step:1749/2285 train_time:105944ms step_avg:60.57ms
step:1750/2285 train_time:106004ms step_avg:60.57ms
step:1750/2285 val_loss:3.3698 train_time:106069ms step_avg:60.61ms
step:1751/2285 train_time:106087ms step_avg:60.59ms
step:1752/2285 train_time:106132ms step_avg:60.58ms
step:1753/2285 train_time:106194ms step_avg:60.58ms
step:1754/2285 train_time:106254ms step_avg:60.58ms
step:1755/2285 train_time:106318ms step_avg:60.58ms
step:1756/2285 train_time:106378ms step_avg:60.58ms
step:1757/2285 train_time:106439ms step_avg:60.58ms
step:1758/2285 train_time:106498ms step_avg:60.58ms
step:1759/2285 train_time:106559ms step_avg:60.58ms
step:1760/2285 train_time:106618ms step_avg:60.58ms
step:1761/2285 train_time:106680ms step_avg:60.58ms
step:1762/2285 train_time:106740ms step_avg:60.58ms
step:1763/2285 train_time:106803ms step_avg:60.58ms
step:1764/2285 train_time:106862ms step_avg:60.58ms
step:1765/2285 train_time:106923ms step_avg:60.58ms
step:1766/2285 train_time:106986ms step_avg:60.58ms
step:1767/2285 train_time:107050ms step_avg:60.58ms
step:1768/2285 train_time:107110ms step_avg:60.58ms
step:1769/2285 train_time:107173ms step_avg:60.58ms
step:1770/2285 train_time:107233ms step_avg:60.58ms
step:1771/2285 train_time:107295ms step_avg:60.58ms
step:1772/2285 train_time:107355ms step_avg:60.58ms
step:1773/2285 train_time:107417ms step_avg:60.59ms
step:1774/2285 train_time:107477ms step_avg:60.58ms
step:1775/2285 train_time:107539ms step_avg:60.59ms
step:1776/2285 train_time:107598ms step_avg:60.58ms
step:1777/2285 train_time:107660ms step_avg:60.59ms
step:1778/2285 train_time:107719ms step_avg:60.58ms
step:1779/2285 train_time:107781ms step_avg:60.59ms
step:1780/2285 train_time:107841ms step_avg:60.58ms
step:1781/2285 train_time:107903ms step_avg:60.59ms
step:1782/2285 train_time:107964ms step_avg:60.59ms
step:1783/2285 train_time:108027ms step_avg:60.59ms
step:1784/2285 train_time:108087ms step_avg:60.59ms
step:1785/2285 train_time:108150ms step_avg:60.59ms
step:1786/2285 train_time:108209ms step_avg:60.59ms
step:1787/2285 train_time:108271ms step_avg:60.59ms
step:1788/2285 train_time:108331ms step_avg:60.59ms
step:1789/2285 train_time:108393ms step_avg:60.59ms
step:1790/2285 train_time:108453ms step_avg:60.59ms
step:1791/2285 train_time:108515ms step_avg:60.59ms
step:1792/2285 train_time:108575ms step_avg:60.59ms
step:1793/2285 train_time:108637ms step_avg:60.59ms
step:1794/2285 train_time:108697ms step_avg:60.59ms
step:1795/2285 train_time:108759ms step_avg:60.59ms
step:1796/2285 train_time:108818ms step_avg:60.59ms
step:1797/2285 train_time:108881ms step_avg:60.59ms
step:1798/2285 train_time:108942ms step_avg:60.59ms
step:1799/2285 train_time:109005ms step_avg:60.59ms
step:1800/2285 train_time:109066ms step_avg:60.59ms
step:1801/2285 train_time:109128ms step_avg:60.59ms
step:1802/2285 train_time:109188ms step_avg:60.59ms
step:1803/2285 train_time:109251ms step_avg:60.59ms
step:1804/2285 train_time:109310ms step_avg:60.59ms
step:1805/2285 train_time:109372ms step_avg:60.59ms
step:1806/2285 train_time:109432ms step_avg:60.59ms
step:1807/2285 train_time:109494ms step_avg:60.59ms
step:1808/2285 train_time:109554ms step_avg:60.59ms
step:1809/2285 train_time:109616ms step_avg:60.59ms
step:1810/2285 train_time:109676ms step_avg:60.59ms
step:1811/2285 train_time:109738ms step_avg:60.60ms
step:1812/2285 train_time:109798ms step_avg:60.60ms
step:1813/2285 train_time:109861ms step_avg:60.60ms
step:1814/2285 train_time:109921ms step_avg:60.60ms
step:1815/2285 train_time:109984ms step_avg:60.60ms
step:1816/2285 train_time:110044ms step_avg:60.60ms
step:1817/2285 train_time:110106ms step_avg:60.60ms
step:1818/2285 train_time:110166ms step_avg:60.60ms
step:1819/2285 train_time:110228ms step_avg:60.60ms
step:1820/2285 train_time:110289ms step_avg:60.60ms
step:1821/2285 train_time:110351ms step_avg:60.60ms
step:1822/2285 train_time:110414ms step_avg:60.60ms
step:1823/2285 train_time:110473ms step_avg:60.60ms
step:1824/2285 train_time:110532ms step_avg:60.60ms
step:1825/2285 train_time:110594ms step_avg:60.60ms
step:1826/2285 train_time:110654ms step_avg:60.60ms
step:1827/2285 train_time:110717ms step_avg:60.60ms
step:1828/2285 train_time:110777ms step_avg:60.60ms
step:1829/2285 train_time:110840ms step_avg:60.60ms
step:1830/2285 train_time:110900ms step_avg:60.60ms
step:1831/2285 train_time:110963ms step_avg:60.60ms
step:1832/2285 train_time:111023ms step_avg:60.60ms
step:1833/2285 train_time:111086ms step_avg:60.60ms
step:1834/2285 train_time:111146ms step_avg:60.60ms
step:1835/2285 train_time:111208ms step_avg:60.60ms
step:1836/2285 train_time:111268ms step_avg:60.60ms
step:1837/2285 train_time:111330ms step_avg:60.60ms
step:1838/2285 train_time:111391ms step_avg:60.60ms
step:1839/2285 train_time:111453ms step_avg:60.61ms
step:1840/2285 train_time:111512ms step_avg:60.60ms
step:1841/2285 train_time:111574ms step_avg:60.61ms
step:1842/2285 train_time:111634ms step_avg:60.60ms
step:1843/2285 train_time:111696ms step_avg:60.61ms
step:1844/2285 train_time:111756ms step_avg:60.61ms
step:1845/2285 train_time:111819ms step_avg:60.61ms
step:1846/2285 train_time:111880ms step_avg:60.61ms
step:1847/2285 train_time:111943ms step_avg:60.61ms
step:1848/2285 train_time:112002ms step_avg:60.61ms
step:1849/2285 train_time:112065ms step_avg:60.61ms
step:1850/2285 train_time:112125ms step_avg:60.61ms
step:1851/2285 train_time:112188ms step_avg:60.61ms
step:1852/2285 train_time:112248ms step_avg:60.61ms
step:1853/2285 train_time:112312ms step_avg:60.61ms
step:1854/2285 train_time:112370ms step_avg:60.61ms
step:1855/2285 train_time:112432ms step_avg:60.61ms
step:1856/2285 train_time:112492ms step_avg:60.61ms
step:1857/2285 train_time:112554ms step_avg:60.61ms
step:1858/2285 train_time:112613ms step_avg:60.61ms
step:1859/2285 train_time:112675ms step_avg:60.61ms
step:1860/2285 train_time:112735ms step_avg:60.61ms
step:1861/2285 train_time:112798ms step_avg:60.61ms
step:1862/2285 train_time:112859ms step_avg:60.61ms
step:1863/2285 train_time:112921ms step_avg:60.61ms
step:1864/2285 train_time:112981ms step_avg:60.61ms
step:1865/2285 train_time:113044ms step_avg:60.61ms
step:1866/2285 train_time:113104ms step_avg:60.61ms
step:1867/2285 train_time:113167ms step_avg:60.61ms
step:1868/2285 train_time:113226ms step_avg:60.61ms
step:1869/2285 train_time:113288ms step_avg:60.61ms
step:1870/2285 train_time:113348ms step_avg:60.61ms
step:1871/2285 train_time:113410ms step_avg:60.61ms
step:1872/2285 train_time:113470ms step_avg:60.61ms
step:1873/2285 train_time:113532ms step_avg:60.61ms
step:1874/2285 train_time:113591ms step_avg:60.61ms
step:1875/2285 train_time:113654ms step_avg:60.62ms
step:1876/2285 train_time:113713ms step_avg:60.61ms
step:1877/2285 train_time:113775ms step_avg:60.62ms
step:1878/2285 train_time:113835ms step_avg:60.62ms
step:1879/2285 train_time:113897ms step_avg:60.62ms
step:1880/2285 train_time:113958ms step_avg:60.62ms
step:1881/2285 train_time:114021ms step_avg:60.62ms
step:1882/2285 train_time:114081ms step_avg:60.62ms
step:1883/2285 train_time:114144ms step_avg:60.62ms
step:1884/2285 train_time:114204ms step_avg:60.62ms
step:1885/2285 train_time:114266ms step_avg:60.62ms
step:1886/2285 train_time:114326ms step_avg:60.62ms
step:1887/2285 train_time:114389ms step_avg:60.62ms
step:1888/2285 train_time:114448ms step_avg:60.62ms
step:1889/2285 train_time:114511ms step_avg:60.62ms
step:1890/2285 train_time:114570ms step_avg:60.62ms
step:1891/2285 train_time:114632ms step_avg:60.62ms
step:1892/2285 train_time:114691ms step_avg:60.62ms
step:1893/2285 train_time:114754ms step_avg:60.62ms
step:1894/2285 train_time:114813ms step_avg:60.62ms
step:1895/2285 train_time:114876ms step_avg:60.62ms
step:1896/2285 train_time:114936ms step_avg:60.62ms
step:1897/2285 train_time:114999ms step_avg:60.62ms
step:1898/2285 train_time:115060ms step_avg:60.62ms
step:1899/2285 train_time:115123ms step_avg:60.62ms
step:1900/2285 train_time:115183ms step_avg:60.62ms
step:1901/2285 train_time:115246ms step_avg:60.62ms
step:1902/2285 train_time:115305ms step_avg:60.62ms
step:1903/2285 train_time:115369ms step_avg:60.62ms
step:1904/2285 train_time:115428ms step_avg:60.62ms
step:1905/2285 train_time:115490ms step_avg:60.62ms
step:1906/2285 train_time:115550ms step_avg:60.62ms
step:1907/2285 train_time:115612ms step_avg:60.63ms
step:1908/2285 train_time:115671ms step_avg:60.62ms
step:1909/2285 train_time:115734ms step_avg:60.63ms
step:1910/2285 train_time:115794ms step_avg:60.63ms
step:1911/2285 train_time:115856ms step_avg:60.63ms
step:1912/2285 train_time:115917ms step_avg:60.63ms
step:1913/2285 train_time:115980ms step_avg:60.63ms
step:1914/2285 train_time:116041ms step_avg:60.63ms
step:1915/2285 train_time:116103ms step_avg:60.63ms
step:1916/2285 train_time:116164ms step_avg:60.63ms
step:1917/2285 train_time:116226ms step_avg:60.63ms
step:1918/2285 train_time:116287ms step_avg:60.63ms
step:1919/2285 train_time:116349ms step_avg:60.63ms
step:1920/2285 train_time:116409ms step_avg:60.63ms
step:1921/2285 train_time:116471ms step_avg:60.63ms
step:1922/2285 train_time:116531ms step_avg:60.63ms
step:1923/2285 train_time:116593ms step_avg:60.63ms
step:1924/2285 train_time:116653ms step_avg:60.63ms
step:1925/2285 train_time:116714ms step_avg:60.63ms
step:1926/2285 train_time:116774ms step_avg:60.63ms
step:1927/2285 train_time:116837ms step_avg:60.63ms
step:1928/2285 train_time:116897ms step_avg:60.63ms
step:1929/2285 train_time:116960ms step_avg:60.63ms
step:1930/2285 train_time:117021ms step_avg:60.63ms
step:1931/2285 train_time:117083ms step_avg:60.63ms
step:1932/2285 train_time:117144ms step_avg:60.63ms
step:1933/2285 train_time:117206ms step_avg:60.63ms
step:1934/2285 train_time:117266ms step_avg:60.63ms
step:1935/2285 train_time:117329ms step_avg:60.64ms
step:1936/2285 train_time:117389ms step_avg:60.63ms
step:1937/2285 train_time:117451ms step_avg:60.64ms
step:1938/2285 train_time:117511ms step_avg:60.64ms
step:1939/2285 train_time:117573ms step_avg:60.64ms
step:1940/2285 train_time:117632ms step_avg:60.64ms
step:1941/2285 train_time:117695ms step_avg:60.64ms
step:1942/2285 train_time:117754ms step_avg:60.64ms
step:1943/2285 train_time:117817ms step_avg:60.64ms
step:1944/2285 train_time:117877ms step_avg:60.64ms
step:1945/2285 train_time:117940ms step_avg:60.64ms
step:1946/2285 train_time:118001ms step_avg:60.64ms
step:1947/2285 train_time:118063ms step_avg:60.64ms
step:1948/2285 train_time:118123ms step_avg:60.64ms
step:1949/2285 train_time:118186ms step_avg:60.64ms
step:1950/2285 train_time:118247ms step_avg:60.64ms
step:1951/2285 train_time:118308ms step_avg:60.64ms
step:1952/2285 train_time:118368ms step_avg:60.64ms
step:1953/2285 train_time:118431ms step_avg:60.64ms
step:1954/2285 train_time:118491ms step_avg:60.64ms
step:1955/2285 train_time:118553ms step_avg:60.64ms
step:1956/2285 train_time:118612ms step_avg:60.64ms
step:1957/2285 train_time:118674ms step_avg:60.64ms
step:1958/2285 train_time:118735ms step_avg:60.64ms
step:1959/2285 train_time:118797ms step_avg:60.64ms
step:1960/2285 train_time:118858ms step_avg:60.64ms
step:1961/2285 train_time:118920ms step_avg:60.64ms
step:1962/2285 train_time:118981ms step_avg:60.64ms
step:1963/2285 train_time:119044ms step_avg:60.64ms
step:1964/2285 train_time:119103ms step_avg:60.64ms
step:1965/2285 train_time:119166ms step_avg:60.64ms
step:1966/2285 train_time:119226ms step_avg:60.64ms
step:1967/2285 train_time:119289ms step_avg:60.64ms
step:1968/2285 train_time:119349ms step_avg:60.64ms
step:1969/2285 train_time:119411ms step_avg:60.65ms
step:1970/2285 train_time:119471ms step_avg:60.65ms
step:1971/2285 train_time:119533ms step_avg:60.65ms
step:1972/2285 train_time:119592ms step_avg:60.65ms
step:1973/2285 train_time:119654ms step_avg:60.65ms
step:1974/2285 train_time:119714ms step_avg:60.65ms
step:1975/2285 train_time:119777ms step_avg:60.65ms
step:1976/2285 train_time:119837ms step_avg:60.65ms
step:1977/2285 train_time:119900ms step_avg:60.65ms
step:1978/2285 train_time:119960ms step_avg:60.65ms
step:1979/2285 train_time:120023ms step_avg:60.65ms
step:1980/2285 train_time:120082ms step_avg:60.65ms
step:1981/2285 train_time:120145ms step_avg:60.65ms
step:1982/2285 train_time:120205ms step_avg:60.65ms
step:1983/2285 train_time:120268ms step_avg:60.65ms
step:1984/2285 train_time:120328ms step_avg:60.65ms
step:1985/2285 train_time:120390ms step_avg:60.65ms
step:1986/2285 train_time:120451ms step_avg:60.65ms
step:1987/2285 train_time:120512ms step_avg:60.65ms
step:1988/2285 train_time:120572ms step_avg:60.65ms
step:1989/2285 train_time:120634ms step_avg:60.65ms
step:1990/2285 train_time:120693ms step_avg:60.65ms
step:1991/2285 train_time:120756ms step_avg:60.65ms
step:1992/2285 train_time:120819ms step_avg:60.65ms
step:1993/2285 train_time:120879ms step_avg:60.65ms
step:1994/2285 train_time:120940ms step_avg:60.65ms
step:1995/2285 train_time:121003ms step_avg:60.65ms
step:1996/2285 train_time:121063ms step_avg:60.65ms
step:1997/2285 train_time:121125ms step_avg:60.65ms
step:1998/2285 train_time:121185ms step_avg:60.65ms
step:1999/2285 train_time:121248ms step_avg:60.65ms
step:2000/2285 train_time:121308ms step_avg:60.65ms
step:2000/2285 val_loss:3.3212 train_time:121371ms step_avg:60.69ms
step:2001/2285 train_time:121390ms step_avg:60.66ms
step:2002/2285 train_time:121435ms step_avg:60.66ms
step:2003/2285 train_time:121497ms step_avg:60.66ms
step:2004/2285 train_time:121559ms step_avg:60.66ms
step:2005/2285 train_time:121622ms step_avg:60.66ms
step:2006/2285 train_time:121683ms step_avg:60.66ms
step:2007/2285 train_time:121745ms step_avg:60.66ms
step:2008/2285 train_time:121804ms step_avg:60.66ms
step:2009/2285 train_time:121866ms step_avg:60.66ms
step:2010/2285 train_time:121925ms step_avg:60.66ms
step:2011/2285 train_time:121987ms step_avg:60.66ms
step:2012/2285 train_time:122046ms step_avg:60.66ms
step:2013/2285 train_time:122108ms step_avg:60.66ms
step:2014/2285 train_time:122167ms step_avg:60.66ms
step:2015/2285 train_time:122229ms step_avg:60.66ms
step:2016/2285 train_time:122292ms step_avg:60.66ms
step:2017/2285 train_time:122357ms step_avg:60.66ms
step:2018/2285 train_time:122418ms step_avg:60.66ms
step:2019/2285 train_time:122481ms step_avg:60.66ms
step:2020/2285 train_time:122542ms step_avg:60.66ms
step:2021/2285 train_time:122606ms step_avg:60.67ms
step:2022/2285 train_time:122667ms step_avg:60.67ms
step:2023/2285 train_time:122729ms step_avg:60.67ms
step:2024/2285 train_time:122788ms step_avg:60.67ms
step:2025/2285 train_time:122850ms step_avg:60.67ms
step:2026/2285 train_time:122909ms step_avg:60.67ms
step:2027/2285 train_time:122970ms step_avg:60.67ms
step:2028/2285 train_time:123029ms step_avg:60.67ms
step:2029/2285 train_time:123091ms step_avg:60.67ms
step:2030/2285 train_time:123151ms step_avg:60.67ms
step:2031/2285 train_time:123213ms step_avg:60.67ms
step:2032/2285 train_time:123274ms step_avg:60.67ms
step:2033/2285 train_time:123337ms step_avg:60.67ms
step:2034/2285 train_time:123398ms step_avg:60.67ms
step:2035/2285 train_time:123461ms step_avg:60.67ms
step:2036/2285 train_time:123521ms step_avg:60.67ms
step:2037/2285 train_time:123585ms step_avg:60.67ms
step:2038/2285 train_time:123645ms step_avg:60.67ms
step:2039/2285 train_time:123707ms step_avg:60.67ms
step:2040/2285 train_time:123767ms step_avg:60.67ms
step:2041/2285 train_time:123829ms step_avg:60.67ms
step:2042/2285 train_time:123889ms step_avg:60.67ms
step:2043/2285 train_time:123950ms step_avg:60.67ms
step:2044/2285 train_time:124010ms step_avg:60.67ms
step:2045/2285 train_time:124072ms step_avg:60.67ms
step:2046/2285 train_time:124132ms step_avg:60.67ms
step:2047/2285 train_time:124194ms step_avg:60.67ms
step:2048/2285 train_time:124253ms step_avg:60.67ms
step:2049/2285 train_time:124316ms step_avg:60.67ms
step:2050/2285 train_time:124377ms step_avg:60.67ms
step:2051/2285 train_time:124439ms step_avg:60.67ms
step:2052/2285 train_time:124501ms step_avg:60.67ms
step:2053/2285 train_time:124565ms step_avg:60.67ms
step:2054/2285 train_time:124625ms step_avg:60.67ms
step:2055/2285 train_time:124687ms step_avg:60.67ms
step:2056/2285 train_time:124747ms step_avg:60.67ms
step:2057/2285 train_time:124809ms step_avg:60.68ms
step:2058/2285 train_time:124869ms step_avg:60.67ms
step:2059/2285 train_time:124931ms step_avg:60.68ms
step:2060/2285 train_time:124990ms step_avg:60.67ms
step:2061/2285 train_time:125052ms step_avg:60.68ms
step:2062/2285 train_time:125112ms step_avg:60.67ms
step:2063/2285 train_time:125174ms step_avg:60.68ms
step:2064/2285 train_time:125233ms step_avg:60.68ms
step:2065/2285 train_time:125296ms step_avg:60.68ms
step:2066/2285 train_time:125356ms step_avg:60.68ms
step:2067/2285 train_time:125419ms step_avg:60.68ms
step:2068/2285 train_time:125480ms step_avg:60.68ms
step:2069/2285 train_time:125543ms step_avg:60.68ms
step:2070/2285 train_time:125604ms step_avg:60.68ms
step:2071/2285 train_time:125667ms step_avg:60.68ms
step:2072/2285 train_time:125726ms step_avg:60.68ms
step:2073/2285 train_time:125788ms step_avg:60.68ms
step:2074/2285 train_time:125848ms step_avg:60.68ms
step:2075/2285 train_time:125911ms step_avg:60.68ms
step:2076/2285 train_time:125971ms step_avg:60.68ms
step:2077/2285 train_time:126033ms step_avg:60.68ms
step:2078/2285 train_time:126093ms step_avg:60.68ms
step:2079/2285 train_time:126155ms step_avg:60.68ms
step:2080/2285 train_time:126214ms step_avg:60.68ms
step:2081/2285 train_time:126277ms step_avg:60.68ms
step:2082/2285 train_time:126337ms step_avg:60.68ms
step:2083/2285 train_time:126401ms step_avg:60.68ms
step:2084/2285 train_time:126461ms step_avg:60.68ms
step:2085/2285 train_time:126524ms step_avg:60.68ms
step:2086/2285 train_time:126584ms step_avg:60.68ms
step:2087/2285 train_time:126646ms step_avg:60.68ms
step:2088/2285 train_time:126706ms step_avg:60.68ms
step:2089/2285 train_time:126768ms step_avg:60.68ms
step:2090/2285 train_time:126828ms step_avg:60.68ms
step:2091/2285 train_time:126890ms step_avg:60.68ms
step:2092/2285 train_time:126950ms step_avg:60.68ms
step:2093/2285 train_time:127012ms step_avg:60.68ms
step:2094/2285 train_time:127073ms step_avg:60.68ms
step:2095/2285 train_time:127135ms step_avg:60.68ms
step:2096/2285 train_time:127194ms step_avg:60.68ms
step:2097/2285 train_time:127257ms step_avg:60.69ms
step:2098/2285 train_time:127316ms step_avg:60.68ms
step:2099/2285 train_time:127379ms step_avg:60.69ms
step:2100/2285 train_time:127439ms step_avg:60.69ms
step:2101/2285 train_time:127502ms step_avg:60.69ms
step:2102/2285 train_time:127563ms step_avg:60.69ms
step:2103/2285 train_time:127625ms step_avg:60.69ms
step:2104/2285 train_time:127685ms step_avg:60.69ms
step:2105/2285 train_time:127748ms step_avg:60.69ms
step:2106/2285 train_time:127808ms step_avg:60.69ms
step:2107/2285 train_time:127870ms step_avg:60.69ms
step:2108/2285 train_time:127930ms step_avg:60.69ms
step:2109/2285 train_time:127992ms step_avg:60.69ms
step:2110/2285 train_time:128052ms step_avg:60.69ms
step:2111/2285 train_time:128114ms step_avg:60.69ms
step:2112/2285 train_time:128174ms step_avg:60.69ms
step:2113/2285 train_time:128236ms step_avg:60.69ms
step:2114/2285 train_time:128296ms step_avg:60.69ms
step:2115/2285 train_time:128359ms step_avg:60.69ms
step:2116/2285 train_time:128419ms step_avg:60.69ms
step:2117/2285 train_time:128482ms step_avg:60.69ms
step:2118/2285 train_time:128542ms step_avg:60.69ms
step:2119/2285 train_time:128605ms step_avg:60.69ms
step:2120/2285 train_time:128666ms step_avg:60.69ms
step:2121/2285 train_time:128728ms step_avg:60.69ms
step:2122/2285 train_time:128788ms step_avg:60.69ms
step:2123/2285 train_time:128850ms step_avg:60.69ms
step:2124/2285 train_time:128911ms step_avg:60.69ms
step:2125/2285 train_time:128973ms step_avg:60.69ms
step:2126/2285 train_time:129032ms step_avg:60.69ms
step:2127/2285 train_time:129095ms step_avg:60.69ms
step:2128/2285 train_time:129154ms step_avg:60.69ms
step:2129/2285 train_time:129217ms step_avg:60.69ms
step:2130/2285 train_time:129277ms step_avg:60.69ms
step:2131/2285 train_time:129340ms step_avg:60.69ms
step:2132/2285 train_time:129400ms step_avg:60.69ms
step:2133/2285 train_time:129463ms step_avg:60.70ms
step:2134/2285 train_time:129523ms step_avg:60.70ms
step:2135/2285 train_time:129586ms step_avg:60.70ms
step:2136/2285 train_time:129646ms step_avg:60.70ms
step:2137/2285 train_time:129708ms step_avg:60.70ms
step:2138/2285 train_time:129769ms step_avg:60.70ms
step:2139/2285 train_time:129831ms step_avg:60.70ms
step:2140/2285 train_time:129891ms step_avg:60.70ms
step:2141/2285 train_time:129953ms step_avg:60.70ms
step:2142/2285 train_time:130013ms step_avg:60.70ms
step:2143/2285 train_time:130075ms step_avg:60.70ms
step:2144/2285 train_time:130135ms step_avg:60.70ms
step:2145/2285 train_time:130197ms step_avg:60.70ms
step:2146/2285 train_time:130257ms step_avg:60.70ms
step:2147/2285 train_time:130320ms step_avg:60.70ms
step:2148/2285 train_time:130380ms step_avg:60.70ms
step:2149/2285 train_time:130443ms step_avg:60.70ms
step:2150/2285 train_time:130503ms step_avg:60.70ms
step:2151/2285 train_time:130567ms step_avg:60.70ms
step:2152/2285 train_time:130626ms step_avg:60.70ms
step:2153/2285 train_time:130689ms step_avg:60.70ms
step:2154/2285 train_time:130749ms step_avg:60.70ms
step:2155/2285 train_time:130811ms step_avg:60.70ms
step:2156/2285 train_time:130871ms step_avg:60.70ms
step:2157/2285 train_time:130933ms step_avg:60.70ms
step:2158/2285 train_time:130993ms step_avg:60.70ms
step:2159/2285 train_time:131055ms step_avg:60.70ms
step:2160/2285 train_time:131115ms step_avg:60.70ms
step:2161/2285 train_time:131177ms step_avg:60.70ms
step:2162/2285 train_time:131237ms step_avg:60.70ms
step:2163/2285 train_time:131300ms step_avg:60.70ms
step:2164/2285 train_time:131360ms step_avg:60.70ms
step:2165/2285 train_time:131423ms step_avg:60.70ms
step:2166/2285 train_time:131484ms step_avg:60.70ms
step:2167/2285 train_time:131546ms step_avg:60.70ms
step:2168/2285 train_time:131606ms step_avg:60.70ms
step:2169/2285 train_time:131670ms step_avg:60.71ms
step:2170/2285 train_time:131730ms step_avg:60.71ms
step:2171/2285 train_time:131793ms step_avg:60.71ms
step:2172/2285 train_time:131853ms step_avg:60.71ms
step:2173/2285 train_time:131915ms step_avg:60.71ms
step:2174/2285 train_time:131976ms step_avg:60.71ms
step:2175/2285 train_time:132038ms step_avg:60.71ms
step:2176/2285 train_time:132099ms step_avg:60.71ms
step:2177/2285 train_time:132161ms step_avg:60.71ms
step:2178/2285 train_time:132221ms step_avg:60.71ms
step:2179/2285 train_time:132284ms step_avg:60.71ms
step:2180/2285 train_time:132344ms step_avg:60.71ms
step:2181/2285 train_time:132407ms step_avg:60.71ms
step:2182/2285 train_time:132467ms step_avg:60.71ms
step:2183/2285 train_time:132529ms step_avg:60.71ms
step:2184/2285 train_time:132589ms step_avg:60.71ms
step:2185/2285 train_time:132652ms step_avg:60.71ms
step:2186/2285 train_time:132712ms step_avg:60.71ms
step:2187/2285 train_time:132776ms step_avg:60.71ms
step:2188/2285 train_time:132835ms step_avg:60.71ms
step:2189/2285 train_time:132898ms step_avg:60.71ms
step:2190/2285 train_time:132958ms step_avg:60.71ms
step:2191/2285 train_time:133021ms step_avg:60.71ms
step:2192/2285 train_time:133081ms step_avg:60.71ms
step:2193/2285 train_time:133143ms step_avg:60.71ms
step:2194/2285 train_time:133203ms step_avg:60.71ms
step:2195/2285 train_time:133265ms step_avg:60.71ms
step:2196/2285 train_time:133325ms step_avg:60.71ms
step:2197/2285 train_time:133387ms step_avg:60.71ms
step:2198/2285 train_time:133447ms step_avg:60.71ms
step:2199/2285 train_time:133510ms step_avg:60.71ms
step:2200/2285 train_time:133570ms step_avg:60.71ms
step:2201/2285 train_time:133633ms step_avg:60.71ms
step:2202/2285 train_time:133693ms step_avg:60.71ms
step:2203/2285 train_time:133756ms step_avg:60.72ms
step:2204/2285 train_time:133816ms step_avg:60.72ms
step:2205/2285 train_time:133878ms step_avg:60.72ms
step:2206/2285 train_time:133939ms step_avg:60.72ms
step:2207/2285 train_time:134002ms step_avg:60.72ms
step:2208/2285 train_time:134062ms step_avg:60.72ms
step:2209/2285 train_time:134124ms step_avg:60.72ms
step:2210/2285 train_time:134184ms step_avg:60.72ms
step:2211/2285 train_time:134247ms step_avg:60.72ms
step:2212/2285 train_time:134307ms step_avg:60.72ms
step:2213/2285 train_time:134369ms step_avg:60.72ms
step:2214/2285 train_time:134428ms step_avg:60.72ms
step:2215/2285 train_time:134491ms step_avg:60.72ms
step:2216/2285 train_time:134551ms step_avg:60.72ms
step:2217/2285 train_time:134614ms step_avg:60.72ms
step:2218/2285 train_time:134673ms step_avg:60.72ms
step:2219/2285 train_time:134736ms step_avg:60.72ms
step:2220/2285 train_time:134796ms step_avg:60.72ms
step:2221/2285 train_time:134858ms step_avg:60.72ms
step:2222/2285 train_time:134919ms step_avg:60.72ms
step:2223/2285 train_time:134982ms step_avg:60.72ms
step:2224/2285 train_time:135043ms step_avg:60.72ms
step:2225/2285 train_time:135104ms step_avg:60.72ms
step:2226/2285 train_time:135165ms step_avg:60.72ms
step:2227/2285 train_time:135227ms step_avg:60.72ms
step:2228/2285 train_time:135287ms step_avg:60.72ms
step:2229/2285 train_time:135349ms step_avg:60.72ms
step:2230/2285 train_time:135409ms step_avg:60.72ms
step:2231/2285 train_time:135472ms step_avg:60.72ms
step:2232/2285 train_time:135531ms step_avg:60.72ms
step:2233/2285 train_time:135594ms step_avg:60.72ms
step:2234/2285 train_time:135654ms step_avg:60.72ms
step:2235/2285 train_time:135717ms step_avg:60.72ms
step:2236/2285 train_time:135777ms step_avg:60.72ms
step:2237/2285 train_time:135839ms step_avg:60.72ms
step:2238/2285 train_time:135900ms step_avg:60.72ms
step:2239/2285 train_time:135963ms step_avg:60.72ms
step:2240/2285 train_time:136023ms step_avg:60.72ms
step:2241/2285 train_time:136085ms step_avg:60.73ms
step:2242/2285 train_time:136145ms step_avg:60.72ms
step:2243/2285 train_time:136207ms step_avg:60.73ms
step:2244/2285 train_time:136268ms step_avg:60.73ms
step:2245/2285 train_time:136330ms step_avg:60.73ms
step:2246/2285 train_time:136391ms step_avg:60.73ms
step:2247/2285 train_time:136453ms step_avg:60.73ms
step:2248/2285 train_time:136512ms step_avg:60.73ms
step:2249/2285 train_time:136574ms step_avg:60.73ms
step:2250/2285 train_time:136634ms step_avg:60.73ms
step:2250/2285 val_loss:3.2861 train_time:136698ms step_avg:60.75ms
step:2251/2285 train_time:136715ms step_avg:60.74ms
step:2252/2285 train_time:136760ms step_avg:60.73ms
step:2253/2285 train_time:136823ms step_avg:60.73ms
step:2254/2285 train_time:136883ms step_avg:60.73ms
step:2255/2285 train_time:136945ms step_avg:60.73ms
step:2256/2285 train_time:137005ms step_avg:60.73ms
step:2257/2285 train_time:137066ms step_avg:60.73ms
step:2258/2285 train_time:137126ms step_avg:60.73ms
step:2259/2285 train_time:137187ms step_avg:60.73ms
step:2260/2285 train_time:137247ms step_avg:60.73ms
step:2261/2285 train_time:137310ms step_avg:60.73ms
step:2262/2285 train_time:137370ms step_avg:60.73ms
step:2263/2285 train_time:137433ms step_avg:60.73ms
step:2264/2285 train_time:137492ms step_avg:60.73ms
step:2265/2285 train_time:137555ms step_avg:60.73ms
step:2266/2285 train_time:137616ms step_avg:60.73ms
step:2267/2285 train_time:137680ms step_avg:60.73ms
step:2268/2285 train_time:137741ms step_avg:60.73ms
step:2269/2285 train_time:137804ms step_avg:60.73ms
step:2270/2285 train_time:137864ms step_avg:60.73ms
step:2271/2285 train_time:137927ms step_avg:60.73ms
step:2272/2285 train_time:137987ms step_avg:60.73ms
step:2273/2285 train_time:138049ms step_avg:60.73ms
step:2274/2285 train_time:138109ms step_avg:60.73ms
step:2275/2285 train_time:138171ms step_avg:60.73ms
step:2276/2285 train_time:138231ms step_avg:60.73ms
step:2277/2285 train_time:138293ms step_avg:60.73ms
step:2278/2285 train_time:138353ms step_avg:60.73ms
step:2279/2285 train_time:138416ms step_avg:60.74ms
step:2280/2285 train_time:138476ms step_avg:60.74ms
step:2281/2285 train_time:138539ms step_avg:60.74ms
step:2282/2285 train_time:138599ms step_avg:60.74ms
step:2283/2285 train_time:138662ms step_avg:60.74ms
step:2284/2285 train_time:138722ms step_avg:60.74ms
step:2285/2285 train_time:138785ms step_avg:60.74ms
step:2285/2285 val_loss:3.2802 train_time:138846ms step_avg:60.76ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

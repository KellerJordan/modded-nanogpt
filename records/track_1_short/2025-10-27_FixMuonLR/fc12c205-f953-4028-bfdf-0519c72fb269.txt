import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2285
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 02:08:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:109ms step_avg:109.38ms
step:2/2285 train_time:130ms step_avg:64.89ms
step:3/2285 train_time:168ms step_avg:56.07ms
step:4/2285 train_time:224ms step_avg:56.11ms
step:5/2285 train_time:284ms step_avg:56.71ms
step:6/2285 train_time:342ms step_avg:56.96ms
step:7/2285 train_time:402ms step_avg:57.43ms
step:8/2285 train_time:460ms step_avg:57.54ms
step:9/2285 train_time:521ms step_avg:57.89ms
step:10/2285 train_time:580ms step_avg:57.95ms
step:11/2285 train_time:640ms step_avg:58.21ms
step:12/2285 train_time:699ms step_avg:58.22ms
step:13/2285 train_time:759ms step_avg:58.39ms
step:14/2285 train_time:818ms step_avg:58.41ms
step:15/2285 train_time:878ms step_avg:58.55ms
step:16/2285 train_time:937ms step_avg:58.54ms
step:17/2285 train_time:999ms step_avg:58.76ms
step:18/2285 train_time:1061ms step_avg:58.93ms
step:19/2285 train_time:1126ms step_avg:59.24ms
step:20/2285 train_time:1186ms step_avg:59.32ms
step:21/2285 train_time:1248ms step_avg:59.42ms
step:22/2285 train_time:1307ms step_avg:59.40ms
step:23/2285 train_time:1368ms step_avg:59.46ms
step:24/2285 train_time:1426ms step_avg:59.42ms
step:25/2285 train_time:1487ms step_avg:59.49ms
step:26/2285 train_time:1546ms step_avg:59.46ms
step:27/2285 train_time:1607ms step_avg:59.53ms
step:28/2285 train_time:1666ms step_avg:59.49ms
step:29/2285 train_time:1727ms step_avg:59.55ms
step:30/2285 train_time:1786ms step_avg:59.53ms
step:31/2285 train_time:1847ms step_avg:59.59ms
step:32/2285 train_time:1906ms step_avg:59.56ms
step:33/2285 train_time:1968ms step_avg:59.63ms
step:34/2285 train_time:2027ms step_avg:59.63ms
step:35/2285 train_time:2090ms step_avg:59.71ms
step:36/2285 train_time:2149ms step_avg:59.70ms
step:37/2285 train_time:2211ms step_avg:59.76ms
step:38/2285 train_time:2270ms step_avg:59.73ms
step:39/2285 train_time:2331ms step_avg:59.78ms
step:40/2285 train_time:2390ms step_avg:59.76ms
step:41/2285 train_time:2452ms step_avg:59.81ms
step:42/2285 train_time:2512ms step_avg:59.80ms
step:43/2285 train_time:2573ms step_avg:59.83ms
step:44/2285 train_time:2632ms step_avg:59.82ms
step:45/2285 train_time:2693ms step_avg:59.85ms
step:46/2285 train_time:2752ms step_avg:59.83ms
step:47/2285 train_time:2813ms step_avg:59.86ms
step:48/2285 train_time:2873ms step_avg:59.85ms
step:49/2285 train_time:2934ms step_avg:59.89ms
step:50/2285 train_time:2995ms step_avg:59.89ms
step:51/2285 train_time:3057ms step_avg:59.94ms
step:52/2285 train_time:3117ms step_avg:59.94ms
step:53/2285 train_time:3178ms step_avg:59.97ms
step:54/2285 train_time:3237ms step_avg:59.94ms
step:55/2285 train_time:3299ms step_avg:59.98ms
step:56/2285 train_time:3358ms step_avg:59.96ms
step:57/2285 train_time:3419ms step_avg:59.99ms
step:58/2285 train_time:3479ms step_avg:59.98ms
step:59/2285 train_time:3542ms step_avg:60.03ms
step:60/2285 train_time:3601ms step_avg:60.01ms
step:61/2285 train_time:3662ms step_avg:60.03ms
step:62/2285 train_time:3721ms step_avg:60.01ms
step:63/2285 train_time:3782ms step_avg:60.04ms
step:64/2285 train_time:3842ms step_avg:60.02ms
step:65/2285 train_time:3903ms step_avg:60.05ms
step:66/2285 train_time:3963ms step_avg:60.05ms
step:67/2285 train_time:4025ms step_avg:60.07ms
step:68/2285 train_time:4083ms step_avg:60.05ms
step:69/2285 train_time:4145ms step_avg:60.07ms
step:70/2285 train_time:4204ms step_avg:60.05ms
step:71/2285 train_time:4265ms step_avg:60.07ms
step:72/2285 train_time:4324ms step_avg:60.05ms
step:73/2285 train_time:4385ms step_avg:60.06ms
step:74/2285 train_time:4444ms step_avg:60.06ms
step:75/2285 train_time:4506ms step_avg:60.08ms
step:76/2285 train_time:4565ms step_avg:60.06ms
step:77/2285 train_time:4626ms step_avg:60.08ms
step:78/2285 train_time:4685ms step_avg:60.06ms
step:79/2285 train_time:4747ms step_avg:60.08ms
step:80/2285 train_time:4806ms step_avg:60.08ms
step:81/2285 train_time:4868ms step_avg:60.10ms
step:82/2285 train_time:4927ms step_avg:60.08ms
step:83/2285 train_time:4988ms step_avg:60.10ms
step:84/2285 train_time:5048ms step_avg:60.09ms
step:85/2285 train_time:5109ms step_avg:60.11ms
step:86/2285 train_time:5168ms step_avg:60.10ms
step:87/2285 train_time:5229ms step_avg:60.11ms
step:88/2285 train_time:5288ms step_avg:60.09ms
step:89/2285 train_time:5349ms step_avg:60.10ms
step:90/2285 train_time:5408ms step_avg:60.09ms
step:91/2285 train_time:5469ms step_avg:60.10ms
step:92/2285 train_time:5529ms step_avg:60.10ms
step:93/2285 train_time:5590ms step_avg:60.11ms
step:94/2285 train_time:5650ms step_avg:60.11ms
step:95/2285 train_time:5712ms step_avg:60.12ms
step:96/2285 train_time:5770ms step_avg:60.11ms
step:97/2285 train_time:5832ms step_avg:60.12ms
step:98/2285 train_time:5892ms step_avg:60.12ms
step:99/2285 train_time:5953ms step_avg:60.13ms
step:100/2285 train_time:6012ms step_avg:60.12ms
step:101/2285 train_time:6073ms step_avg:60.13ms
step:102/2285 train_time:6131ms step_avg:60.11ms
step:103/2285 train_time:6193ms step_avg:60.13ms
step:104/2285 train_time:6252ms step_avg:60.12ms
step:105/2285 train_time:6313ms step_avg:60.12ms
step:106/2285 train_time:6372ms step_avg:60.11ms
step:107/2285 train_time:6433ms step_avg:60.12ms
step:108/2285 train_time:6493ms step_avg:60.12ms
step:109/2285 train_time:6555ms step_avg:60.14ms
step:110/2285 train_time:6614ms step_avg:60.13ms
step:111/2285 train_time:6675ms step_avg:60.14ms
step:112/2285 train_time:6734ms step_avg:60.13ms
step:113/2285 train_time:6796ms step_avg:60.14ms
step:114/2285 train_time:6855ms step_avg:60.14ms
step:115/2285 train_time:6917ms step_avg:60.15ms
step:116/2285 train_time:6976ms step_avg:60.14ms
step:117/2285 train_time:7037ms step_avg:60.15ms
step:118/2285 train_time:7096ms step_avg:60.13ms
step:119/2285 train_time:7158ms step_avg:60.15ms
step:120/2285 train_time:7217ms step_avg:60.14ms
step:121/2285 train_time:7278ms step_avg:60.15ms
step:122/2285 train_time:7337ms step_avg:60.14ms
step:123/2285 train_time:7398ms step_avg:60.14ms
step:124/2285 train_time:7456ms step_avg:60.13ms
step:125/2285 train_time:7518ms step_avg:60.14ms
step:126/2285 train_time:7577ms step_avg:60.13ms
step:127/2285 train_time:7638ms step_avg:60.14ms
step:128/2285 train_time:7697ms step_avg:60.13ms
step:129/2285 train_time:7758ms step_avg:60.14ms
step:130/2285 train_time:7817ms step_avg:60.13ms
step:131/2285 train_time:7878ms step_avg:60.14ms
step:132/2285 train_time:7937ms step_avg:60.13ms
step:133/2285 train_time:7999ms step_avg:60.14ms
step:134/2285 train_time:8058ms step_avg:60.13ms
step:135/2285 train_time:8118ms step_avg:60.14ms
step:136/2285 train_time:8177ms step_avg:60.12ms
step:137/2285 train_time:8238ms step_avg:60.13ms
step:138/2285 train_time:8296ms step_avg:60.12ms
step:139/2285 train_time:8357ms step_avg:60.12ms
step:140/2285 train_time:8417ms step_avg:60.12ms
step:141/2285 train_time:8479ms step_avg:60.13ms
step:142/2285 train_time:8537ms step_avg:60.12ms
step:143/2285 train_time:8599ms step_avg:60.13ms
step:144/2285 train_time:8658ms step_avg:60.12ms
step:145/2285 train_time:8719ms step_avg:60.13ms
step:146/2285 train_time:8778ms step_avg:60.12ms
step:147/2285 train_time:8839ms step_avg:60.13ms
step:148/2285 train_time:8898ms step_avg:60.12ms
step:149/2285 train_time:8959ms step_avg:60.13ms
step:150/2285 train_time:9018ms step_avg:60.12ms
step:151/2285 train_time:9079ms step_avg:60.13ms
step:152/2285 train_time:9138ms step_avg:60.12ms
step:153/2285 train_time:9199ms step_avg:60.12ms
step:154/2285 train_time:9257ms step_avg:60.11ms
step:155/2285 train_time:9318ms step_avg:60.12ms
step:156/2285 train_time:9377ms step_avg:60.11ms
step:157/2285 train_time:9438ms step_avg:60.11ms
step:158/2285 train_time:9497ms step_avg:60.11ms
step:159/2285 train_time:9558ms step_avg:60.11ms
step:160/2285 train_time:9617ms step_avg:60.10ms
step:161/2285 train_time:9678ms step_avg:60.11ms
step:162/2285 train_time:9737ms step_avg:60.10ms
step:163/2285 train_time:9798ms step_avg:60.11ms
step:164/2285 train_time:9856ms step_avg:60.10ms
step:165/2285 train_time:9918ms step_avg:60.11ms
step:166/2285 train_time:9976ms step_avg:60.10ms
step:167/2285 train_time:10037ms step_avg:60.10ms
step:168/2285 train_time:10096ms step_avg:60.10ms
step:169/2285 train_time:10158ms step_avg:60.10ms
step:170/2285 train_time:10216ms step_avg:60.09ms
step:171/2285 train_time:10277ms step_avg:60.10ms
step:172/2285 train_time:10335ms step_avg:60.09ms
step:173/2285 train_time:10397ms step_avg:60.10ms
step:174/2285 train_time:10455ms step_avg:60.09ms
step:175/2285 train_time:10516ms step_avg:60.09ms
step:176/2285 train_time:10575ms step_avg:60.08ms
step:177/2285 train_time:10637ms step_avg:60.09ms
step:178/2285 train_time:10696ms step_avg:60.09ms
step:179/2285 train_time:10757ms step_avg:60.09ms
step:180/2285 train_time:10815ms step_avg:60.09ms
step:181/2285 train_time:10876ms step_avg:60.09ms
step:182/2285 train_time:10935ms step_avg:60.08ms
step:183/2285 train_time:10997ms step_avg:60.09ms
step:184/2285 train_time:11056ms step_avg:60.08ms
step:185/2285 train_time:11117ms step_avg:60.09ms
step:186/2285 train_time:11176ms step_avg:60.08ms
step:187/2285 train_time:11237ms step_avg:60.09ms
step:188/2285 train_time:11296ms step_avg:60.08ms
step:189/2285 train_time:11357ms step_avg:60.09ms
step:190/2285 train_time:11416ms step_avg:60.08ms
step:191/2285 train_time:11477ms step_avg:60.09ms
step:192/2285 train_time:11536ms step_avg:60.08ms
step:193/2285 train_time:11597ms step_avg:60.09ms
step:194/2285 train_time:11655ms step_avg:60.08ms
step:195/2285 train_time:11716ms step_avg:60.08ms
step:196/2285 train_time:11775ms step_avg:60.08ms
step:197/2285 train_time:11837ms step_avg:60.08ms
step:198/2285 train_time:11896ms step_avg:60.08ms
step:199/2285 train_time:11957ms step_avg:60.08ms
step:200/2285 train_time:12015ms step_avg:60.08ms
step:201/2285 train_time:12076ms step_avg:60.08ms
step:202/2285 train_time:12135ms step_avg:60.07ms
step:203/2285 train_time:12196ms step_avg:60.08ms
step:204/2285 train_time:12255ms step_avg:60.07ms
step:205/2285 train_time:12316ms step_avg:60.08ms
step:206/2285 train_time:12375ms step_avg:60.07ms
step:207/2285 train_time:12436ms step_avg:60.08ms
step:208/2285 train_time:12495ms step_avg:60.07ms
step:209/2285 train_time:12556ms step_avg:60.08ms
step:210/2285 train_time:12615ms step_avg:60.07ms
step:211/2285 train_time:12676ms step_avg:60.08ms
step:212/2285 train_time:12735ms step_avg:60.07ms
step:213/2285 train_time:12797ms step_avg:60.08ms
step:214/2285 train_time:12856ms step_avg:60.07ms
step:215/2285 train_time:12917ms step_avg:60.08ms
step:216/2285 train_time:12975ms step_avg:60.07ms
step:217/2285 train_time:13036ms step_avg:60.07ms
step:218/2285 train_time:13095ms step_avg:60.07ms
step:219/2285 train_time:13156ms step_avg:60.07ms
step:220/2285 train_time:13215ms step_avg:60.07ms
step:221/2285 train_time:13276ms step_avg:60.07ms
step:222/2285 train_time:13334ms step_avg:60.06ms
step:223/2285 train_time:13395ms step_avg:60.07ms
step:224/2285 train_time:13454ms step_avg:60.06ms
step:225/2285 train_time:13515ms step_avg:60.07ms
step:226/2285 train_time:13574ms step_avg:60.06ms
step:227/2285 train_time:13635ms step_avg:60.07ms
step:228/2285 train_time:13694ms step_avg:60.06ms
step:229/2285 train_time:13756ms step_avg:60.07ms
step:230/2285 train_time:13815ms step_avg:60.06ms
step:231/2285 train_time:13876ms step_avg:60.07ms
step:232/2285 train_time:13934ms step_avg:60.06ms
step:233/2285 train_time:13996ms step_avg:60.07ms
step:234/2285 train_time:14054ms step_avg:60.06ms
step:235/2285 train_time:14115ms step_avg:60.06ms
step:236/2285 train_time:14174ms step_avg:60.06ms
step:237/2285 train_time:14235ms step_avg:60.06ms
step:238/2285 train_time:14295ms step_avg:60.06ms
step:239/2285 train_time:14356ms step_avg:60.07ms
step:240/2285 train_time:14415ms step_avg:60.06ms
step:241/2285 train_time:14476ms step_avg:60.07ms
step:242/2285 train_time:14534ms step_avg:60.06ms
step:243/2285 train_time:14596ms step_avg:60.06ms
step:244/2285 train_time:14655ms step_avg:60.06ms
step:245/2285 train_time:14716ms step_avg:60.06ms
step:246/2285 train_time:14774ms step_avg:60.06ms
step:247/2285 train_time:14835ms step_avg:60.06ms
step:248/2285 train_time:14894ms step_avg:60.06ms
step:249/2285 train_time:14955ms step_avg:60.06ms
step:250/2285 train_time:15014ms step_avg:60.05ms
step:250/2285 val_loss:4.0722 train_time:15076ms step_avg:60.30ms
step:251/2285 train_time:15094ms step_avg:60.14ms
step:252/2285 train_time:15135ms step_avg:60.06ms
step:253/2285 train_time:15202ms step_avg:60.09ms
step:254/2285 train_time:15264ms step_avg:60.09ms
step:255/2285 train_time:15327ms step_avg:60.11ms
step:256/2285 train_time:15387ms step_avg:60.10ms
step:257/2285 train_time:15448ms step_avg:60.11ms
step:258/2285 train_time:15506ms step_avg:60.10ms
step:259/2285 train_time:15567ms step_avg:60.10ms
step:260/2285 train_time:15625ms step_avg:60.09ms
step:261/2285 train_time:15684ms step_avg:60.09ms
step:262/2285 train_time:15742ms step_avg:60.08ms
step:263/2285 train_time:15802ms step_avg:60.08ms
step:264/2285 train_time:15860ms step_avg:60.08ms
step:265/2285 train_time:15920ms step_avg:60.07ms
step:266/2285 train_time:15977ms step_avg:60.07ms
step:267/2285 train_time:16037ms step_avg:60.07ms
step:268/2285 train_time:16096ms step_avg:60.06ms
step:269/2285 train_time:16157ms step_avg:60.06ms
step:270/2285 train_time:16216ms step_avg:60.06ms
step:271/2285 train_time:16279ms step_avg:60.07ms
step:272/2285 train_time:16338ms step_avg:60.07ms
step:273/2285 train_time:16399ms step_avg:60.07ms
step:274/2285 train_time:16458ms step_avg:60.07ms
step:275/2285 train_time:16519ms step_avg:60.07ms
step:276/2285 train_time:16578ms step_avg:60.07ms
step:277/2285 train_time:16639ms step_avg:60.07ms
step:278/2285 train_time:16698ms step_avg:60.06ms
step:279/2285 train_time:16758ms step_avg:60.07ms
step:280/2285 train_time:16816ms step_avg:60.06ms
step:281/2285 train_time:16877ms step_avg:60.06ms
step:282/2285 train_time:16936ms step_avg:60.06ms
step:283/2285 train_time:16996ms step_avg:60.06ms
step:284/2285 train_time:17054ms step_avg:60.05ms
step:285/2285 train_time:17115ms step_avg:60.05ms
step:286/2285 train_time:17173ms step_avg:60.05ms
step:287/2285 train_time:17235ms step_avg:60.05ms
step:288/2285 train_time:17293ms step_avg:60.05ms
step:289/2285 train_time:17355ms step_avg:60.05ms
step:290/2285 train_time:17414ms step_avg:60.05ms
step:291/2285 train_time:17476ms step_avg:60.05ms
step:292/2285 train_time:17535ms step_avg:60.05ms
step:293/2285 train_time:17596ms step_avg:60.06ms
step:294/2285 train_time:17655ms step_avg:60.05ms
step:295/2285 train_time:17716ms step_avg:60.05ms
step:296/2285 train_time:17774ms step_avg:60.05ms
step:297/2285 train_time:17835ms step_avg:60.05ms
step:298/2285 train_time:17894ms step_avg:60.05ms
step:299/2285 train_time:17954ms step_avg:60.05ms
step:300/2285 train_time:18012ms step_avg:60.04ms
step:301/2285 train_time:18072ms step_avg:60.04ms
step:302/2285 train_time:18130ms step_avg:60.03ms
step:303/2285 train_time:18191ms step_avg:60.04ms
step:304/2285 train_time:18250ms step_avg:60.03ms
step:305/2285 train_time:18311ms step_avg:60.04ms
step:306/2285 train_time:18370ms step_avg:60.03ms
step:307/2285 train_time:18431ms step_avg:60.04ms
step:308/2285 train_time:18490ms step_avg:60.03ms
step:309/2285 train_time:18552ms step_avg:60.04ms
step:310/2285 train_time:18611ms step_avg:60.03ms
step:311/2285 train_time:18672ms step_avg:60.04ms
step:312/2285 train_time:18730ms step_avg:60.03ms
step:313/2285 train_time:18791ms step_avg:60.04ms
step:314/2285 train_time:18850ms step_avg:60.03ms
step:315/2285 train_time:18911ms step_avg:60.03ms
step:316/2285 train_time:18969ms step_avg:60.03ms
step:317/2285 train_time:19030ms step_avg:60.03ms
step:318/2285 train_time:19088ms step_avg:60.02ms
step:319/2285 train_time:19148ms step_avg:60.03ms
step:320/2285 train_time:19207ms step_avg:60.02ms
step:321/2285 train_time:19269ms step_avg:60.03ms
step:322/2285 train_time:19328ms step_avg:60.02ms
step:323/2285 train_time:19390ms step_avg:60.03ms
step:324/2285 train_time:19449ms step_avg:60.03ms
step:325/2285 train_time:19511ms step_avg:60.03ms
step:326/2285 train_time:19570ms step_avg:60.03ms
step:327/2285 train_time:19631ms step_avg:60.03ms
step:328/2285 train_time:19690ms step_avg:60.03ms
step:329/2285 train_time:19752ms step_avg:60.04ms
step:330/2285 train_time:19810ms step_avg:60.03ms
step:331/2285 train_time:19871ms step_avg:60.03ms
step:332/2285 train_time:19930ms step_avg:60.03ms
step:333/2285 train_time:19990ms step_avg:60.03ms
step:334/2285 train_time:20048ms step_avg:60.03ms
step:335/2285 train_time:20109ms step_avg:60.03ms
step:336/2285 train_time:20167ms step_avg:60.02ms
step:337/2285 train_time:20228ms step_avg:60.02ms
step:338/2285 train_time:20286ms step_avg:60.02ms
step:339/2285 train_time:20348ms step_avg:60.02ms
step:340/2285 train_time:20407ms step_avg:60.02ms
step:341/2285 train_time:20468ms step_avg:60.02ms
step:342/2285 train_time:20526ms step_avg:60.02ms
step:343/2285 train_time:20588ms step_avg:60.02ms
step:344/2285 train_time:20647ms step_avg:60.02ms
step:345/2285 train_time:20709ms step_avg:60.03ms
step:346/2285 train_time:20768ms step_avg:60.02ms
step:347/2285 train_time:20829ms step_avg:60.02ms
step:348/2285 train_time:20887ms step_avg:60.02ms
step:349/2285 train_time:20948ms step_avg:60.02ms
step:350/2285 train_time:21007ms step_avg:60.02ms
step:351/2285 train_time:21067ms step_avg:60.02ms
step:352/2285 train_time:21126ms step_avg:60.02ms
step:353/2285 train_time:21186ms step_avg:60.02ms
step:354/2285 train_time:21245ms step_avg:60.01ms
step:355/2285 train_time:21306ms step_avg:60.02ms
step:356/2285 train_time:21365ms step_avg:60.01ms
step:357/2285 train_time:21426ms step_avg:60.02ms
step:358/2285 train_time:21485ms step_avg:60.02ms
step:359/2285 train_time:21547ms step_avg:60.02ms
step:360/2285 train_time:21606ms step_avg:60.02ms
step:361/2285 train_time:21667ms step_avg:60.02ms
step:362/2285 train_time:21726ms step_avg:60.02ms
step:363/2285 train_time:21787ms step_avg:60.02ms
step:364/2285 train_time:21846ms step_avg:60.02ms
step:365/2285 train_time:21907ms step_avg:60.02ms
step:366/2285 train_time:21965ms step_avg:60.01ms
step:367/2285 train_time:22027ms step_avg:60.02ms
step:368/2285 train_time:22085ms step_avg:60.01ms
step:369/2285 train_time:22146ms step_avg:60.02ms
step:370/2285 train_time:22204ms step_avg:60.01ms
step:371/2285 train_time:22264ms step_avg:60.01ms
step:372/2285 train_time:22323ms step_avg:60.01ms
step:373/2285 train_time:22384ms step_avg:60.01ms
step:374/2285 train_time:22443ms step_avg:60.01ms
step:375/2285 train_time:22504ms step_avg:60.01ms
step:376/2285 train_time:22563ms step_avg:60.01ms
step:377/2285 train_time:22624ms step_avg:60.01ms
step:378/2285 train_time:22683ms step_avg:60.01ms
step:379/2285 train_time:22745ms step_avg:60.01ms
step:380/2285 train_time:22804ms step_avg:60.01ms
step:381/2285 train_time:22865ms step_avg:60.01ms
step:382/2285 train_time:22924ms step_avg:60.01ms
step:383/2285 train_time:22985ms step_avg:60.01ms
step:384/2285 train_time:23044ms step_avg:60.01ms
step:385/2285 train_time:23105ms step_avg:60.01ms
step:386/2285 train_time:23164ms step_avg:60.01ms
step:387/2285 train_time:23225ms step_avg:60.01ms
step:388/2285 train_time:23284ms step_avg:60.01ms
step:389/2285 train_time:23345ms step_avg:60.01ms
step:390/2285 train_time:23404ms step_avg:60.01ms
step:391/2285 train_time:23466ms step_avg:60.01ms
step:392/2285 train_time:23525ms step_avg:60.01ms
step:393/2285 train_time:23587ms step_avg:60.02ms
step:394/2285 train_time:23648ms step_avg:60.02ms
step:395/2285 train_time:23709ms step_avg:60.02ms
step:396/2285 train_time:23769ms step_avg:60.02ms
step:397/2285 train_time:23830ms step_avg:60.03ms
step:398/2285 train_time:23889ms step_avg:60.02ms
step:399/2285 train_time:23951ms step_avg:60.03ms
step:400/2285 train_time:24010ms step_avg:60.02ms
step:401/2285 train_time:24071ms step_avg:60.03ms
step:402/2285 train_time:24130ms step_avg:60.03ms
step:403/2285 train_time:24192ms step_avg:60.03ms
step:404/2285 train_time:24251ms step_avg:60.03ms
step:405/2285 train_time:24312ms step_avg:60.03ms
step:406/2285 train_time:24371ms step_avg:60.03ms
step:407/2285 train_time:24433ms step_avg:60.03ms
step:408/2285 train_time:24492ms step_avg:60.03ms
step:409/2285 train_time:24554ms step_avg:60.03ms
step:410/2285 train_time:24614ms step_avg:60.03ms
step:411/2285 train_time:24675ms step_avg:60.04ms
step:412/2285 train_time:24735ms step_avg:60.04ms
step:413/2285 train_time:24796ms step_avg:60.04ms
step:414/2285 train_time:24856ms step_avg:60.04ms
step:415/2285 train_time:24917ms step_avg:60.04ms
step:416/2285 train_time:24976ms step_avg:60.04ms
step:417/2285 train_time:25037ms step_avg:60.04ms
step:418/2285 train_time:25095ms step_avg:60.04ms
step:419/2285 train_time:25157ms step_avg:60.04ms
step:420/2285 train_time:25216ms step_avg:60.04ms
step:421/2285 train_time:25277ms step_avg:60.04ms
step:422/2285 train_time:25335ms step_avg:60.04ms
step:423/2285 train_time:25397ms step_avg:60.04ms
step:424/2285 train_time:25456ms step_avg:60.04ms
step:425/2285 train_time:25518ms step_avg:60.04ms
step:426/2285 train_time:25576ms step_avg:60.04ms
step:427/2285 train_time:25638ms step_avg:60.04ms
step:428/2285 train_time:25697ms step_avg:60.04ms
step:429/2285 train_time:25758ms step_avg:60.04ms
step:430/2285 train_time:25817ms step_avg:60.04ms
step:431/2285 train_time:25879ms step_avg:60.04ms
step:432/2285 train_time:25937ms step_avg:60.04ms
step:433/2285 train_time:25999ms step_avg:60.04ms
step:434/2285 train_time:26058ms step_avg:60.04ms
step:435/2285 train_time:26120ms step_avg:60.05ms
step:436/2285 train_time:26179ms step_avg:60.04ms
step:437/2285 train_time:26239ms step_avg:60.04ms
step:438/2285 train_time:26299ms step_avg:60.04ms
step:439/2285 train_time:26360ms step_avg:60.04ms
step:440/2285 train_time:26419ms step_avg:60.04ms
step:441/2285 train_time:26479ms step_avg:60.04ms
step:442/2285 train_time:26538ms step_avg:60.04ms
step:443/2285 train_time:26599ms step_avg:60.04ms
step:444/2285 train_time:26658ms step_avg:60.04ms
step:445/2285 train_time:26719ms step_avg:60.04ms
step:446/2285 train_time:26778ms step_avg:60.04ms
step:447/2285 train_time:26840ms step_avg:60.04ms
step:448/2285 train_time:26899ms step_avg:60.04ms
step:449/2285 train_time:26960ms step_avg:60.05ms
step:450/2285 train_time:27019ms step_avg:60.04ms
step:451/2285 train_time:27081ms step_avg:60.05ms
step:452/2285 train_time:27140ms step_avg:60.04ms
step:453/2285 train_time:27201ms step_avg:60.05ms
step:454/2285 train_time:27260ms step_avg:60.04ms
step:455/2285 train_time:27321ms step_avg:60.05ms
step:456/2285 train_time:27380ms step_avg:60.04ms
step:457/2285 train_time:27441ms step_avg:60.05ms
step:458/2285 train_time:27500ms step_avg:60.04ms
step:459/2285 train_time:27561ms step_avg:60.05ms
step:460/2285 train_time:27620ms step_avg:60.04ms
step:461/2285 train_time:27681ms step_avg:60.05ms
step:462/2285 train_time:27740ms step_avg:60.04ms
step:463/2285 train_time:27801ms step_avg:60.05ms
step:464/2285 train_time:27860ms step_avg:60.04ms
step:465/2285 train_time:27921ms step_avg:60.04ms
step:466/2285 train_time:27980ms step_avg:60.04ms
step:467/2285 train_time:28041ms step_avg:60.05ms
step:468/2285 train_time:28100ms step_avg:60.04ms
step:469/2285 train_time:28162ms step_avg:60.05ms
step:470/2285 train_time:28220ms step_avg:60.04ms
step:471/2285 train_time:28281ms step_avg:60.05ms
step:472/2285 train_time:28340ms step_avg:60.04ms
step:473/2285 train_time:28401ms step_avg:60.04ms
step:474/2285 train_time:28461ms step_avg:60.04ms
step:475/2285 train_time:28522ms step_avg:60.05ms
step:476/2285 train_time:28581ms step_avg:60.04ms
step:477/2285 train_time:28642ms step_avg:60.05ms
step:478/2285 train_time:28702ms step_avg:60.05ms
step:479/2285 train_time:28763ms step_avg:60.05ms
step:480/2285 train_time:28822ms step_avg:60.05ms
step:481/2285 train_time:28883ms step_avg:60.05ms
step:482/2285 train_time:28942ms step_avg:60.05ms
step:483/2285 train_time:29003ms step_avg:60.05ms
step:484/2285 train_time:29062ms step_avg:60.05ms
step:485/2285 train_time:29124ms step_avg:60.05ms
step:486/2285 train_time:29182ms step_avg:60.05ms
step:487/2285 train_time:29244ms step_avg:60.05ms
step:488/2285 train_time:29302ms step_avg:60.05ms
step:489/2285 train_time:29364ms step_avg:60.05ms
step:490/2285 train_time:29423ms step_avg:60.05ms
step:491/2285 train_time:29484ms step_avg:60.05ms
step:492/2285 train_time:29543ms step_avg:60.05ms
step:493/2285 train_time:29604ms step_avg:60.05ms
step:494/2285 train_time:29663ms step_avg:60.05ms
step:495/2285 train_time:29724ms step_avg:60.05ms
step:496/2285 train_time:29782ms step_avg:60.04ms
step:497/2285 train_time:29844ms step_avg:60.05ms
step:498/2285 train_time:29903ms step_avg:60.05ms
step:499/2285 train_time:29964ms step_avg:60.05ms
step:500/2285 train_time:30023ms step_avg:60.05ms
step:500/2285 val_loss:3.7848 train_time:30086ms step_avg:60.17ms
step:501/2285 train_time:30105ms step_avg:60.09ms
step:502/2285 train_time:30146ms step_avg:60.05ms
step:503/2285 train_time:30206ms step_avg:60.05ms
step:504/2285 train_time:30266ms step_avg:60.05ms
step:505/2285 train_time:30330ms step_avg:60.06ms
step:506/2285 train_time:30389ms step_avg:60.06ms
step:507/2285 train_time:30449ms step_avg:60.06ms
step:508/2285 train_time:30507ms step_avg:60.05ms
step:509/2285 train_time:30568ms step_avg:60.06ms
step:510/2285 train_time:30627ms step_avg:60.05ms
step:511/2285 train_time:30687ms step_avg:60.05ms
step:512/2285 train_time:30745ms step_avg:60.05ms
step:513/2285 train_time:30807ms step_avg:60.05ms
step:514/2285 train_time:30867ms step_avg:60.05ms
step:515/2285 train_time:30928ms step_avg:60.05ms
step:516/2285 train_time:30988ms step_avg:60.05ms
step:517/2285 train_time:31055ms step_avg:60.07ms
step:518/2285 train_time:31115ms step_avg:60.07ms
step:519/2285 train_time:31178ms step_avg:60.07ms
step:520/2285 train_time:31236ms step_avg:60.07ms
step:521/2285 train_time:31298ms step_avg:60.07ms
step:522/2285 train_time:31357ms step_avg:60.07ms
step:523/2285 train_time:31417ms step_avg:60.07ms
step:524/2285 train_time:31476ms step_avg:60.07ms
step:525/2285 train_time:31537ms step_avg:60.07ms
step:526/2285 train_time:31596ms step_avg:60.07ms
step:527/2285 train_time:31657ms step_avg:60.07ms
step:528/2285 train_time:31716ms step_avg:60.07ms
step:529/2285 train_time:31777ms step_avg:60.07ms
step:530/2285 train_time:31837ms step_avg:60.07ms
step:531/2285 train_time:31899ms step_avg:60.07ms
step:532/2285 train_time:31958ms step_avg:60.07ms
step:533/2285 train_time:32019ms step_avg:60.07ms
step:534/2285 train_time:32079ms step_avg:60.07ms
step:535/2285 train_time:32140ms step_avg:60.08ms
step:536/2285 train_time:32199ms step_avg:60.07ms
step:537/2285 train_time:32261ms step_avg:60.08ms
step:538/2285 train_time:32320ms step_avg:60.07ms
step:539/2285 train_time:32381ms step_avg:60.08ms
step:540/2285 train_time:32440ms step_avg:60.07ms
step:541/2285 train_time:32501ms step_avg:60.08ms
step:542/2285 train_time:32560ms step_avg:60.07ms
step:543/2285 train_time:32622ms step_avg:60.08ms
step:544/2285 train_time:32680ms step_avg:60.07ms
step:545/2285 train_time:32741ms step_avg:60.08ms
step:546/2285 train_time:32800ms step_avg:60.07ms
step:547/2285 train_time:32862ms step_avg:60.08ms
step:548/2285 train_time:32921ms step_avg:60.07ms
step:549/2285 train_time:32982ms step_avg:60.08ms
step:550/2285 train_time:33041ms step_avg:60.07ms
step:551/2285 train_time:33102ms step_avg:60.08ms
step:552/2285 train_time:33161ms step_avg:60.07ms
step:553/2285 train_time:33223ms step_avg:60.08ms
step:554/2285 train_time:33282ms step_avg:60.08ms
step:555/2285 train_time:33344ms step_avg:60.08ms
step:556/2285 train_time:33403ms step_avg:60.08ms
step:557/2285 train_time:33465ms step_avg:60.08ms
step:558/2285 train_time:33525ms step_avg:60.08ms
step:559/2285 train_time:33586ms step_avg:60.08ms
step:560/2285 train_time:33645ms step_avg:60.08ms
step:561/2285 train_time:33707ms step_avg:60.08ms
step:562/2285 train_time:33766ms step_avg:60.08ms
step:563/2285 train_time:33828ms step_avg:60.08ms
step:564/2285 train_time:33887ms step_avg:60.08ms
step:565/2285 train_time:33948ms step_avg:60.09ms
step:566/2285 train_time:34008ms step_avg:60.08ms
step:567/2285 train_time:34069ms step_avg:60.09ms
step:568/2285 train_time:34129ms step_avg:60.09ms
step:569/2285 train_time:34190ms step_avg:60.09ms
step:570/2285 train_time:34249ms step_avg:60.09ms
step:571/2285 train_time:34311ms step_avg:60.09ms
step:572/2285 train_time:34369ms step_avg:60.09ms
step:573/2285 train_time:34431ms step_avg:60.09ms
step:574/2285 train_time:34490ms step_avg:60.09ms
step:575/2285 train_time:34551ms step_avg:60.09ms
step:576/2285 train_time:34610ms step_avg:60.09ms
step:577/2285 train_time:34671ms step_avg:60.09ms
step:578/2285 train_time:34730ms step_avg:60.09ms
step:579/2285 train_time:34792ms step_avg:60.09ms
step:580/2285 train_time:34851ms step_avg:60.09ms
step:581/2285 train_time:34913ms step_avg:60.09ms
step:582/2285 train_time:34972ms step_avg:60.09ms
step:583/2285 train_time:35033ms step_avg:60.09ms
step:584/2285 train_time:35092ms step_avg:60.09ms
step:585/2285 train_time:35154ms step_avg:60.09ms
step:586/2285 train_time:35213ms step_avg:60.09ms
step:587/2285 train_time:35275ms step_avg:60.09ms
step:588/2285 train_time:35334ms step_avg:60.09ms
step:589/2285 train_time:35395ms step_avg:60.09ms
step:590/2285 train_time:35454ms step_avg:60.09ms
step:591/2285 train_time:35515ms step_avg:60.09ms
step:592/2285 train_time:35574ms step_avg:60.09ms
step:593/2285 train_time:35636ms step_avg:60.09ms
step:594/2285 train_time:35695ms step_avg:60.09ms
step:595/2285 train_time:35756ms step_avg:60.09ms
step:596/2285 train_time:35815ms step_avg:60.09ms
step:597/2285 train_time:35876ms step_avg:60.09ms
step:598/2285 train_time:35936ms step_avg:60.09ms
step:599/2285 train_time:35997ms step_avg:60.10ms
step:600/2285 train_time:36056ms step_avg:60.09ms
step:601/2285 train_time:36117ms step_avg:60.10ms
step:602/2285 train_time:36177ms step_avg:60.09ms
step:603/2285 train_time:36240ms step_avg:60.10ms
step:604/2285 train_time:36298ms step_avg:60.10ms
step:605/2285 train_time:36360ms step_avg:60.10ms
step:606/2285 train_time:36418ms step_avg:60.10ms
step:607/2285 train_time:36480ms step_avg:60.10ms
step:608/2285 train_time:36539ms step_avg:60.10ms
step:609/2285 train_time:36600ms step_avg:60.10ms
step:610/2285 train_time:36659ms step_avg:60.10ms
step:611/2285 train_time:36721ms step_avg:60.10ms
step:612/2285 train_time:36781ms step_avg:60.10ms
step:613/2285 train_time:36842ms step_avg:60.10ms
step:614/2285 train_time:36900ms step_avg:60.10ms
step:615/2285 train_time:36963ms step_avg:60.10ms
step:616/2285 train_time:37022ms step_avg:60.10ms
step:617/2285 train_time:37084ms step_avg:60.10ms
step:618/2285 train_time:37143ms step_avg:60.10ms
step:619/2285 train_time:37205ms step_avg:60.10ms
step:620/2285 train_time:37263ms step_avg:60.10ms
step:621/2285 train_time:37324ms step_avg:60.10ms
step:622/2285 train_time:37384ms step_avg:60.10ms
step:623/2285 train_time:37445ms step_avg:60.10ms
step:624/2285 train_time:37504ms step_avg:60.10ms
step:625/2285 train_time:37565ms step_avg:60.10ms
step:626/2285 train_time:37624ms step_avg:60.10ms
step:627/2285 train_time:37686ms step_avg:60.11ms
step:628/2285 train_time:37746ms step_avg:60.10ms
step:629/2285 train_time:37807ms step_avg:60.11ms
step:630/2285 train_time:37866ms step_avg:60.11ms
step:631/2285 train_time:37929ms step_avg:60.11ms
step:632/2285 train_time:37988ms step_avg:60.11ms
step:633/2285 train_time:38050ms step_avg:60.11ms
step:634/2285 train_time:38109ms step_avg:60.11ms
step:635/2285 train_time:38170ms step_avg:60.11ms
step:636/2285 train_time:38229ms step_avg:60.11ms
step:637/2285 train_time:38291ms step_avg:60.11ms
step:638/2285 train_time:38350ms step_avg:60.11ms
step:639/2285 train_time:38412ms step_avg:60.11ms
step:640/2285 train_time:38471ms step_avg:60.11ms
step:641/2285 train_time:38532ms step_avg:60.11ms
step:642/2285 train_time:38591ms step_avg:60.11ms
step:643/2285 train_time:38653ms step_avg:60.11ms
step:644/2285 train_time:38712ms step_avg:60.11ms
step:645/2285 train_time:38774ms step_avg:60.11ms
step:646/2285 train_time:38833ms step_avg:60.11ms
step:647/2285 train_time:38896ms step_avg:60.12ms
step:648/2285 train_time:38956ms step_avg:60.12ms
step:649/2285 train_time:39017ms step_avg:60.12ms
step:650/2285 train_time:39075ms step_avg:60.12ms
step:651/2285 train_time:39136ms step_avg:60.12ms
step:652/2285 train_time:39196ms step_avg:60.12ms
step:653/2285 train_time:39256ms step_avg:60.12ms
step:654/2285 train_time:39316ms step_avg:60.12ms
step:655/2285 train_time:39377ms step_avg:60.12ms
step:656/2285 train_time:39436ms step_avg:60.12ms
step:657/2285 train_time:39497ms step_avg:60.12ms
step:658/2285 train_time:39556ms step_avg:60.12ms
step:659/2285 train_time:39617ms step_avg:60.12ms
step:660/2285 train_time:39676ms step_avg:60.12ms
step:661/2285 train_time:39737ms step_avg:60.12ms
step:662/2285 train_time:39796ms step_avg:60.12ms
step:663/2285 train_time:39858ms step_avg:60.12ms
step:664/2285 train_time:39918ms step_avg:60.12ms
step:665/2285 train_time:39979ms step_avg:60.12ms
step:666/2285 train_time:40038ms step_avg:60.12ms
step:667/2285 train_time:40100ms step_avg:60.12ms
step:668/2285 train_time:40159ms step_avg:60.12ms
step:669/2285 train_time:40220ms step_avg:60.12ms
step:670/2285 train_time:40279ms step_avg:60.12ms
step:671/2285 train_time:40340ms step_avg:60.12ms
step:672/2285 train_time:40400ms step_avg:60.12ms
step:673/2285 train_time:40461ms step_avg:60.12ms
step:674/2285 train_time:40520ms step_avg:60.12ms
step:675/2285 train_time:40582ms step_avg:60.12ms
step:676/2285 train_time:40641ms step_avg:60.12ms
step:677/2285 train_time:40704ms step_avg:60.12ms
step:678/2285 train_time:40763ms step_avg:60.12ms
step:679/2285 train_time:40825ms step_avg:60.12ms
step:680/2285 train_time:40884ms step_avg:60.12ms
step:681/2285 train_time:40946ms step_avg:60.13ms
step:682/2285 train_time:41005ms step_avg:60.12ms
step:683/2285 train_time:41066ms step_avg:60.13ms
step:684/2285 train_time:41126ms step_avg:60.13ms
step:685/2285 train_time:41187ms step_avg:60.13ms
step:686/2285 train_time:41247ms step_avg:60.13ms
step:687/2285 train_time:41308ms step_avg:60.13ms
step:688/2285 train_time:41367ms step_avg:60.13ms
step:689/2285 train_time:41429ms step_avg:60.13ms
step:690/2285 train_time:41488ms step_avg:60.13ms
step:691/2285 train_time:41550ms step_avg:60.13ms
step:692/2285 train_time:41609ms step_avg:60.13ms
step:693/2285 train_time:41671ms step_avg:60.13ms
step:694/2285 train_time:41730ms step_avg:60.13ms
step:695/2285 train_time:41791ms step_avg:60.13ms
step:696/2285 train_time:41850ms step_avg:60.13ms
step:697/2285 train_time:41912ms step_avg:60.13ms
step:698/2285 train_time:41971ms step_avg:60.13ms
step:699/2285 train_time:42032ms step_avg:60.13ms
step:700/2285 train_time:42091ms step_avg:60.13ms
step:701/2285 train_time:42153ms step_avg:60.13ms
step:702/2285 train_time:42212ms step_avg:60.13ms
step:703/2285 train_time:42273ms step_avg:60.13ms
step:704/2285 train_time:42333ms step_avg:60.13ms
step:705/2285 train_time:42394ms step_avg:60.13ms
step:706/2285 train_time:42454ms step_avg:60.13ms
step:707/2285 train_time:42515ms step_avg:60.14ms
step:708/2285 train_time:42575ms step_avg:60.13ms
step:709/2285 train_time:42636ms step_avg:60.14ms
step:710/2285 train_time:42696ms step_avg:60.14ms
step:711/2285 train_time:42757ms step_avg:60.14ms
step:712/2285 train_time:42816ms step_avg:60.13ms
step:713/2285 train_time:42877ms step_avg:60.14ms
step:714/2285 train_time:42936ms step_avg:60.14ms
step:715/2285 train_time:42998ms step_avg:60.14ms
step:716/2285 train_time:43057ms step_avg:60.14ms
step:717/2285 train_time:43118ms step_avg:60.14ms
step:718/2285 train_time:43178ms step_avg:60.14ms
step:719/2285 train_time:43240ms step_avg:60.14ms
step:720/2285 train_time:43299ms step_avg:60.14ms
step:721/2285 train_time:43360ms step_avg:60.14ms
step:722/2285 train_time:43418ms step_avg:60.14ms
step:723/2285 train_time:43480ms step_avg:60.14ms
step:724/2285 train_time:43539ms step_avg:60.14ms
step:725/2285 train_time:43601ms step_avg:60.14ms
step:726/2285 train_time:43660ms step_avg:60.14ms
step:727/2285 train_time:43721ms step_avg:60.14ms
step:728/2285 train_time:43780ms step_avg:60.14ms
step:729/2285 train_time:43842ms step_avg:60.14ms
step:730/2285 train_time:43901ms step_avg:60.14ms
step:731/2285 train_time:43962ms step_avg:60.14ms
step:732/2285 train_time:44021ms step_avg:60.14ms
step:733/2285 train_time:44083ms step_avg:60.14ms
step:734/2285 train_time:44141ms step_avg:60.14ms
step:735/2285 train_time:44203ms step_avg:60.14ms
step:736/2285 train_time:44262ms step_avg:60.14ms
step:737/2285 train_time:44323ms step_avg:60.14ms
step:738/2285 train_time:44382ms step_avg:60.14ms
step:739/2285 train_time:44444ms step_avg:60.14ms
step:740/2285 train_time:44503ms step_avg:60.14ms
step:741/2285 train_time:44565ms step_avg:60.14ms
step:742/2285 train_time:44624ms step_avg:60.14ms
step:743/2285 train_time:44686ms step_avg:60.14ms
step:744/2285 train_time:44745ms step_avg:60.14ms
step:745/2285 train_time:44806ms step_avg:60.14ms
step:746/2285 train_time:44865ms step_avg:60.14ms
step:747/2285 train_time:44927ms step_avg:60.14ms
step:748/2285 train_time:44986ms step_avg:60.14ms
step:749/2285 train_time:45048ms step_avg:60.14ms
step:750/2285 train_time:45107ms step_avg:60.14ms
step:750/2285 val_loss:3.6533 train_time:45170ms step_avg:60.23ms
step:751/2285 train_time:45189ms step_avg:60.17ms
step:752/2285 train_time:45231ms step_avg:60.15ms
step:753/2285 train_time:45294ms step_avg:60.15ms
step:754/2285 train_time:45355ms step_avg:60.15ms
step:755/2285 train_time:45416ms step_avg:60.15ms
step:756/2285 train_time:45475ms step_avg:60.15ms
step:757/2285 train_time:45536ms step_avg:60.15ms
step:758/2285 train_time:45595ms step_avg:60.15ms
step:759/2285 train_time:45655ms step_avg:60.15ms
step:760/2285 train_time:45713ms step_avg:60.15ms
step:761/2285 train_time:45774ms step_avg:60.15ms
step:762/2285 train_time:45832ms step_avg:60.15ms
step:763/2285 train_time:45893ms step_avg:60.15ms
step:764/2285 train_time:45953ms step_avg:60.15ms
step:765/2285 train_time:46015ms step_avg:60.15ms
step:766/2285 train_time:46075ms step_avg:60.15ms
step:767/2285 train_time:46137ms step_avg:60.15ms
step:768/2285 train_time:46199ms step_avg:60.15ms
step:769/2285 train_time:46262ms step_avg:60.16ms
step:770/2285 train_time:46321ms step_avg:60.16ms
step:771/2285 train_time:46384ms step_avg:60.16ms
step:772/2285 train_time:46443ms step_avg:60.16ms
step:773/2285 train_time:46505ms step_avg:60.16ms
step:774/2285 train_time:46564ms step_avg:60.16ms
step:775/2285 train_time:46625ms step_avg:60.16ms
step:776/2285 train_time:46684ms step_avg:60.16ms
step:777/2285 train_time:46745ms step_avg:60.16ms
step:778/2285 train_time:46805ms step_avg:60.16ms
step:779/2285 train_time:46866ms step_avg:60.16ms
step:780/2285 train_time:46925ms step_avg:60.16ms
step:781/2285 train_time:46987ms step_avg:60.16ms
step:782/2285 train_time:47047ms step_avg:60.16ms
step:783/2285 train_time:47110ms step_avg:60.17ms
step:784/2285 train_time:47170ms step_avg:60.17ms
step:785/2285 train_time:47233ms step_avg:60.17ms
step:786/2285 train_time:47293ms step_avg:60.17ms
step:787/2285 train_time:47355ms step_avg:60.17ms
step:788/2285 train_time:47415ms step_avg:60.17ms
step:789/2285 train_time:47476ms step_avg:60.17ms
step:790/2285 train_time:47536ms step_avg:60.17ms
step:791/2285 train_time:47598ms step_avg:60.17ms
step:792/2285 train_time:47657ms step_avg:60.17ms
step:793/2285 train_time:47719ms step_avg:60.17ms
step:794/2285 train_time:47778ms step_avg:60.17ms
step:795/2285 train_time:47839ms step_avg:60.18ms
step:796/2285 train_time:47899ms step_avg:60.17ms
step:797/2285 train_time:47961ms step_avg:60.18ms
step:798/2285 train_time:48020ms step_avg:60.18ms
step:799/2285 train_time:48083ms step_avg:60.18ms
step:800/2285 train_time:48142ms step_avg:60.18ms
step:801/2285 train_time:48205ms step_avg:60.18ms
step:802/2285 train_time:48264ms step_avg:60.18ms
step:803/2285 train_time:48327ms step_avg:60.18ms
step:804/2285 train_time:48387ms step_avg:60.18ms
step:805/2285 train_time:48449ms step_avg:60.18ms
step:806/2285 train_time:48508ms step_avg:60.18ms
step:807/2285 train_time:48570ms step_avg:60.19ms
step:808/2285 train_time:48629ms step_avg:60.18ms
step:809/2285 train_time:48691ms step_avg:60.19ms
step:810/2285 train_time:48750ms step_avg:60.18ms
step:811/2285 train_time:48811ms step_avg:60.19ms
step:812/2285 train_time:48871ms step_avg:60.19ms
step:813/2285 train_time:48932ms step_avg:60.19ms
step:814/2285 train_time:48991ms step_avg:60.19ms
step:815/2285 train_time:49053ms step_avg:60.19ms
step:816/2285 train_time:49112ms step_avg:60.19ms
step:817/2285 train_time:49174ms step_avg:60.19ms
step:818/2285 train_time:49234ms step_avg:60.19ms
step:819/2285 train_time:49297ms step_avg:60.19ms
step:820/2285 train_time:49356ms step_avg:60.19ms
step:821/2285 train_time:49419ms step_avg:60.19ms
step:822/2285 train_time:49478ms step_avg:60.19ms
step:823/2285 train_time:49540ms step_avg:60.19ms
step:824/2285 train_time:49600ms step_avg:60.19ms
step:825/2285 train_time:49662ms step_avg:60.20ms
step:826/2285 train_time:49721ms step_avg:60.20ms
step:827/2285 train_time:49783ms step_avg:60.20ms
step:828/2285 train_time:49842ms step_avg:60.20ms
step:829/2285 train_time:49904ms step_avg:60.20ms
step:830/2285 train_time:49963ms step_avg:60.20ms
step:831/2285 train_time:50025ms step_avg:60.20ms
step:832/2285 train_time:50085ms step_avg:60.20ms
step:833/2285 train_time:50146ms step_avg:60.20ms
step:834/2285 train_time:50206ms step_avg:60.20ms
step:835/2285 train_time:50268ms step_avg:60.20ms
step:836/2285 train_time:50328ms step_avg:60.20ms
step:837/2285 train_time:50390ms step_avg:60.20ms
step:838/2285 train_time:50450ms step_avg:60.20ms
step:839/2285 train_time:50511ms step_avg:60.20ms
step:840/2285 train_time:50571ms step_avg:60.20ms
step:841/2285 train_time:50633ms step_avg:60.21ms
step:842/2285 train_time:50692ms step_avg:60.20ms
step:843/2285 train_time:50754ms step_avg:60.21ms
step:844/2285 train_time:50813ms step_avg:60.21ms
step:845/2285 train_time:50875ms step_avg:60.21ms
step:846/2285 train_time:50934ms step_avg:60.21ms
step:847/2285 train_time:50996ms step_avg:60.21ms
step:848/2285 train_time:51055ms step_avg:60.21ms
step:849/2285 train_time:51117ms step_avg:60.21ms
step:850/2285 train_time:51177ms step_avg:60.21ms
step:851/2285 train_time:51239ms step_avg:60.21ms
step:852/2285 train_time:51299ms step_avg:60.21ms
step:853/2285 train_time:51361ms step_avg:60.21ms
step:854/2285 train_time:51421ms step_avg:60.21ms
step:855/2285 train_time:51482ms step_avg:60.21ms
step:856/2285 train_time:51542ms step_avg:60.21ms
step:857/2285 train_time:51604ms step_avg:60.21ms
step:858/2285 train_time:51663ms step_avg:60.21ms
step:859/2285 train_time:51726ms step_avg:60.22ms
step:860/2285 train_time:51785ms step_avg:60.22ms
step:861/2285 train_time:51847ms step_avg:60.22ms
step:862/2285 train_time:51905ms step_avg:60.22ms
step:863/2285 train_time:51968ms step_avg:60.22ms
step:864/2285 train_time:52027ms step_avg:60.22ms
step:865/2285 train_time:52089ms step_avg:60.22ms
step:866/2285 train_time:52149ms step_avg:60.22ms
step:867/2285 train_time:52211ms step_avg:60.22ms
step:868/2285 train_time:52271ms step_avg:60.22ms
step:869/2285 train_time:52333ms step_avg:60.22ms
step:870/2285 train_time:52392ms step_avg:60.22ms
step:871/2285 train_time:52454ms step_avg:60.22ms
step:872/2285 train_time:52513ms step_avg:60.22ms
step:873/2285 train_time:52575ms step_avg:60.22ms
step:874/2285 train_time:52634ms step_avg:60.22ms
step:875/2285 train_time:52697ms step_avg:60.22ms
step:876/2285 train_time:52756ms step_avg:60.22ms
step:877/2285 train_time:52818ms step_avg:60.23ms
step:878/2285 train_time:52877ms step_avg:60.22ms
step:879/2285 train_time:52939ms step_avg:60.23ms
step:880/2285 train_time:52999ms step_avg:60.23ms
step:881/2285 train_time:53061ms step_avg:60.23ms
step:882/2285 train_time:53121ms step_avg:60.23ms
step:883/2285 train_time:53183ms step_avg:60.23ms
step:884/2285 train_time:53243ms step_avg:60.23ms
step:885/2285 train_time:53305ms step_avg:60.23ms
step:886/2285 train_time:53364ms step_avg:60.23ms
step:887/2285 train_time:53426ms step_avg:60.23ms
step:888/2285 train_time:53486ms step_avg:60.23ms
step:889/2285 train_time:53548ms step_avg:60.23ms
step:890/2285 train_time:53608ms step_avg:60.23ms
step:891/2285 train_time:53669ms step_avg:60.23ms
step:892/2285 train_time:53729ms step_avg:60.23ms
step:893/2285 train_time:53790ms step_avg:60.24ms
step:894/2285 train_time:53849ms step_avg:60.23ms
step:895/2285 train_time:53911ms step_avg:60.24ms
step:896/2285 train_time:53971ms step_avg:60.24ms
step:897/2285 train_time:54032ms step_avg:60.24ms
step:898/2285 train_time:54092ms step_avg:60.24ms
step:899/2285 train_time:54153ms step_avg:60.24ms
step:900/2285 train_time:54212ms step_avg:60.24ms
step:901/2285 train_time:54275ms step_avg:60.24ms
step:902/2285 train_time:54334ms step_avg:60.24ms
step:903/2285 train_time:54396ms step_avg:60.24ms
step:904/2285 train_time:54456ms step_avg:60.24ms
step:905/2285 train_time:54518ms step_avg:60.24ms
step:906/2285 train_time:54577ms step_avg:60.24ms
step:907/2285 train_time:54639ms step_avg:60.24ms
step:908/2285 train_time:54699ms step_avg:60.24ms
step:909/2285 train_time:54760ms step_avg:60.24ms
step:910/2285 train_time:54820ms step_avg:60.24ms
step:911/2285 train_time:54882ms step_avg:60.24ms
step:912/2285 train_time:54942ms step_avg:60.24ms
step:913/2285 train_time:55004ms step_avg:60.25ms
step:914/2285 train_time:55063ms step_avg:60.24ms
step:915/2285 train_time:55125ms step_avg:60.25ms
step:916/2285 train_time:55184ms step_avg:60.24ms
step:917/2285 train_time:55246ms step_avg:60.25ms
step:918/2285 train_time:55306ms step_avg:60.25ms
step:919/2285 train_time:55368ms step_avg:60.25ms
step:920/2285 train_time:55428ms step_avg:60.25ms
step:921/2285 train_time:55490ms step_avg:60.25ms
step:922/2285 train_time:55550ms step_avg:60.25ms
step:923/2285 train_time:55613ms step_avg:60.25ms
step:924/2285 train_time:55672ms step_avg:60.25ms
step:925/2285 train_time:55734ms step_avg:60.25ms
step:926/2285 train_time:55793ms step_avg:60.25ms
step:927/2285 train_time:55855ms step_avg:60.25ms
step:928/2285 train_time:55914ms step_avg:60.25ms
step:929/2285 train_time:55977ms step_avg:60.25ms
step:930/2285 train_time:56037ms step_avg:60.25ms
step:931/2285 train_time:56099ms step_avg:60.26ms
step:932/2285 train_time:56158ms step_avg:60.26ms
step:933/2285 train_time:56220ms step_avg:60.26ms
step:934/2285 train_time:56279ms step_avg:60.26ms
step:935/2285 train_time:56340ms step_avg:60.26ms
step:936/2285 train_time:56400ms step_avg:60.26ms
step:937/2285 train_time:56462ms step_avg:60.26ms
step:938/2285 train_time:56522ms step_avg:60.26ms
step:939/2285 train_time:56583ms step_avg:60.26ms
step:940/2285 train_time:56643ms step_avg:60.26ms
step:941/2285 train_time:56704ms step_avg:60.26ms
step:942/2285 train_time:56764ms step_avg:60.26ms
step:943/2285 train_time:56826ms step_avg:60.26ms
step:944/2285 train_time:56886ms step_avg:60.26ms
step:945/2285 train_time:56948ms step_avg:60.26ms
step:946/2285 train_time:57008ms step_avg:60.26ms
step:947/2285 train_time:57070ms step_avg:60.26ms
step:948/2285 train_time:57130ms step_avg:60.26ms
step:949/2285 train_time:57192ms step_avg:60.27ms
step:950/2285 train_time:57251ms step_avg:60.26ms
step:951/2285 train_time:57312ms step_avg:60.26ms
step:952/2285 train_time:57372ms step_avg:60.26ms
step:953/2285 train_time:57434ms step_avg:60.27ms
step:954/2285 train_time:57493ms step_avg:60.27ms
step:955/2285 train_time:57555ms step_avg:60.27ms
step:956/2285 train_time:57614ms step_avg:60.27ms
step:957/2285 train_time:57676ms step_avg:60.27ms
step:958/2285 train_time:57736ms step_avg:60.27ms
step:959/2285 train_time:57798ms step_avg:60.27ms
step:960/2285 train_time:57857ms step_avg:60.27ms
step:961/2285 train_time:57920ms step_avg:60.27ms
step:962/2285 train_time:57980ms step_avg:60.27ms
step:963/2285 train_time:58042ms step_avg:60.27ms
step:964/2285 train_time:58101ms step_avg:60.27ms
step:965/2285 train_time:58163ms step_avg:60.27ms
step:966/2285 train_time:58223ms step_avg:60.27ms
step:967/2285 train_time:58284ms step_avg:60.27ms
step:968/2285 train_time:58344ms step_avg:60.27ms
step:969/2285 train_time:58405ms step_avg:60.27ms
step:970/2285 train_time:58465ms step_avg:60.27ms
step:971/2285 train_time:58527ms step_avg:60.27ms
step:972/2285 train_time:58586ms step_avg:60.27ms
step:973/2285 train_time:58648ms step_avg:60.28ms
step:974/2285 train_time:58707ms step_avg:60.27ms
step:975/2285 train_time:58769ms step_avg:60.28ms
step:976/2285 train_time:58829ms step_avg:60.28ms
step:977/2285 train_time:58891ms step_avg:60.28ms
step:978/2285 train_time:58951ms step_avg:60.28ms
step:979/2285 train_time:59012ms step_avg:60.28ms
step:980/2285 train_time:59072ms step_avg:60.28ms
step:981/2285 train_time:59134ms step_avg:60.28ms
step:982/2285 train_time:59193ms step_avg:60.28ms
step:983/2285 train_time:59255ms step_avg:60.28ms
step:984/2285 train_time:59314ms step_avg:60.28ms
step:985/2285 train_time:59376ms step_avg:60.28ms
step:986/2285 train_time:59436ms step_avg:60.28ms
step:987/2285 train_time:59497ms step_avg:60.28ms
step:988/2285 train_time:59557ms step_avg:60.28ms
step:989/2285 train_time:59618ms step_avg:60.28ms
step:990/2285 train_time:59678ms step_avg:60.28ms
step:991/2285 train_time:59740ms step_avg:60.28ms
step:992/2285 train_time:59801ms step_avg:60.28ms
step:993/2285 train_time:59862ms step_avg:60.28ms
step:994/2285 train_time:59922ms step_avg:60.28ms
step:995/2285 train_time:59985ms step_avg:60.29ms
step:996/2285 train_time:60044ms step_avg:60.29ms
step:997/2285 train_time:60106ms step_avg:60.29ms
step:998/2285 train_time:60165ms step_avg:60.29ms
step:999/2285 train_time:60227ms step_avg:60.29ms
step:1000/2285 train_time:60286ms step_avg:60.29ms
step:1000/2285 val_loss:3.5663 train_time:60349ms step_avg:60.35ms
step:1001/2285 train_time:60368ms step_avg:60.31ms
step:1002/2285 train_time:60411ms step_avg:60.29ms
step:1003/2285 train_time:60472ms step_avg:60.29ms
step:1004/2285 train_time:60532ms step_avg:60.29ms
step:1005/2285 train_time:60595ms step_avg:60.29ms
step:1006/2285 train_time:60655ms step_avg:60.29ms
step:1007/2285 train_time:60716ms step_avg:60.29ms
step:1008/2285 train_time:60775ms step_avg:60.29ms
step:1009/2285 train_time:60836ms step_avg:60.29ms
step:1010/2285 train_time:60894ms step_avg:60.29ms
step:1011/2285 train_time:60955ms step_avg:60.29ms
step:1012/2285 train_time:61014ms step_avg:60.29ms
step:1013/2285 train_time:61074ms step_avg:60.29ms
step:1014/2285 train_time:61134ms step_avg:60.29ms
step:1015/2285 train_time:61195ms step_avg:60.29ms
step:1016/2285 train_time:61257ms step_avg:60.29ms
step:1017/2285 train_time:61325ms step_avg:60.30ms
step:1018/2285 train_time:61385ms step_avg:60.30ms
step:1019/2285 train_time:61447ms step_avg:60.30ms
step:1020/2285 train_time:61507ms step_avg:60.30ms
step:1021/2285 train_time:61568ms step_avg:60.30ms
step:1022/2285 train_time:61628ms step_avg:60.30ms
step:1023/2285 train_time:61690ms step_avg:60.30ms
step:1024/2285 train_time:61749ms step_avg:60.30ms
step:1025/2285 train_time:61810ms step_avg:60.30ms
step:1026/2285 train_time:61870ms step_avg:60.30ms
step:1027/2285 train_time:61931ms step_avg:60.30ms
step:1028/2285 train_time:61990ms step_avg:60.30ms
step:1029/2285 train_time:62051ms step_avg:60.30ms
step:1030/2285 train_time:62110ms step_avg:60.30ms
step:1031/2285 train_time:62172ms step_avg:60.30ms
step:1032/2285 train_time:62232ms step_avg:60.30ms
step:1033/2285 train_time:62295ms step_avg:60.31ms
step:1034/2285 train_time:62357ms step_avg:60.31ms
step:1035/2285 train_time:62420ms step_avg:60.31ms
step:1036/2285 train_time:62480ms step_avg:60.31ms
step:1037/2285 train_time:62542ms step_avg:60.31ms
step:1038/2285 train_time:62601ms step_avg:60.31ms
step:1039/2285 train_time:62663ms step_avg:60.31ms
step:1040/2285 train_time:62722ms step_avg:60.31ms
step:1041/2285 train_time:62783ms step_avg:60.31ms
step:1042/2285 train_time:62843ms step_avg:60.31ms
step:1043/2285 train_time:62904ms step_avg:60.31ms
step:1044/2285 train_time:62963ms step_avg:60.31ms
step:1045/2285 train_time:63025ms step_avg:60.31ms
step:1046/2285 train_time:63084ms step_avg:60.31ms
step:1047/2285 train_time:63146ms step_avg:60.31ms
step:1048/2285 train_time:63206ms step_avg:60.31ms
step:1049/2285 train_time:63268ms step_avg:60.31ms
step:1050/2285 train_time:63329ms step_avg:60.31ms
step:1051/2285 train_time:63391ms step_avg:60.31ms
step:1052/2285 train_time:63451ms step_avg:60.31ms
step:1053/2285 train_time:63513ms step_avg:60.32ms
step:1054/2285 train_time:63573ms step_avg:60.32ms
step:1055/2285 train_time:63635ms step_avg:60.32ms
step:1056/2285 train_time:63694ms step_avg:60.32ms
step:1057/2285 train_time:63756ms step_avg:60.32ms
step:1058/2285 train_time:63816ms step_avg:60.32ms
step:1059/2285 train_time:63878ms step_avg:60.32ms
step:1060/2285 train_time:63938ms step_avg:60.32ms
step:1061/2285 train_time:63999ms step_avg:60.32ms
step:1062/2285 train_time:64059ms step_avg:60.32ms
step:1063/2285 train_time:64121ms step_avg:60.32ms
step:1064/2285 train_time:64180ms step_avg:60.32ms
step:1065/2285 train_time:64242ms step_avg:60.32ms
step:1066/2285 train_time:64301ms step_avg:60.32ms
step:1067/2285 train_time:64363ms step_avg:60.32ms
step:1068/2285 train_time:64423ms step_avg:60.32ms
step:1069/2285 train_time:64485ms step_avg:60.32ms
step:1070/2285 train_time:64545ms step_avg:60.32ms
step:1071/2285 train_time:64606ms step_avg:60.32ms
step:1072/2285 train_time:64666ms step_avg:60.32ms
step:1073/2285 train_time:64728ms step_avg:60.32ms
step:1074/2285 train_time:64788ms step_avg:60.32ms
step:1075/2285 train_time:64850ms step_avg:60.33ms
step:1076/2285 train_time:64909ms step_avg:60.32ms
step:1077/2285 train_time:64971ms step_avg:60.33ms
step:1078/2285 train_time:65030ms step_avg:60.33ms
step:1079/2285 train_time:65092ms step_avg:60.33ms
step:1080/2285 train_time:65152ms step_avg:60.33ms
step:1081/2285 train_time:65214ms step_avg:60.33ms
step:1082/2285 train_time:65273ms step_avg:60.33ms
step:1083/2285 train_time:65336ms step_avg:60.33ms
step:1084/2285 train_time:65396ms step_avg:60.33ms
step:1085/2285 train_time:65458ms step_avg:60.33ms
step:1086/2285 train_time:65518ms step_avg:60.33ms
step:1087/2285 train_time:65580ms step_avg:60.33ms
step:1088/2285 train_time:65639ms step_avg:60.33ms
step:1089/2285 train_time:65701ms step_avg:60.33ms
step:1090/2285 train_time:65760ms step_avg:60.33ms
step:1091/2285 train_time:65822ms step_avg:60.33ms
step:1092/2285 train_time:65882ms step_avg:60.33ms
step:1093/2285 train_time:65943ms step_avg:60.33ms
step:1094/2285 train_time:66002ms step_avg:60.33ms
step:1095/2285 train_time:66065ms step_avg:60.33ms
step:1096/2285 train_time:66124ms step_avg:60.33ms
step:1097/2285 train_time:66186ms step_avg:60.33ms
step:1098/2285 train_time:66246ms step_avg:60.33ms
step:1099/2285 train_time:66308ms step_avg:60.33ms
step:1100/2285 train_time:66367ms step_avg:60.33ms
step:1101/2285 train_time:66429ms step_avg:60.34ms
step:1102/2285 train_time:66489ms step_avg:60.33ms
step:1103/2285 train_time:66550ms step_avg:60.34ms
step:1104/2285 train_time:66610ms step_avg:60.34ms
step:1105/2285 train_time:66673ms step_avg:60.34ms
step:1106/2285 train_time:66733ms step_avg:60.34ms
step:1107/2285 train_time:66795ms step_avg:60.34ms
step:1108/2285 train_time:66855ms step_avg:60.34ms
step:1109/2285 train_time:66917ms step_avg:60.34ms
step:1110/2285 train_time:66977ms step_avg:60.34ms
step:1111/2285 train_time:67039ms step_avg:60.34ms
step:1112/2285 train_time:67098ms step_avg:60.34ms
step:1113/2285 train_time:67160ms step_avg:60.34ms
step:1114/2285 train_time:67220ms step_avg:60.34ms
step:1115/2285 train_time:67281ms step_avg:60.34ms
step:1116/2285 train_time:67341ms step_avg:60.34ms
step:1117/2285 train_time:67403ms step_avg:60.34ms
step:1118/2285 train_time:67463ms step_avg:60.34ms
step:1119/2285 train_time:67525ms step_avg:60.34ms
step:1120/2285 train_time:67585ms step_avg:60.34ms
step:1121/2285 train_time:67646ms step_avg:60.34ms
step:1122/2285 train_time:67706ms step_avg:60.34ms
step:1123/2285 train_time:67768ms step_avg:60.35ms
step:1124/2285 train_time:67829ms step_avg:60.35ms
step:1125/2285 train_time:67890ms step_avg:60.35ms
step:1126/2285 train_time:67950ms step_avg:60.35ms
step:1127/2285 train_time:68011ms step_avg:60.35ms
step:1128/2285 train_time:68071ms step_avg:60.35ms
step:1129/2285 train_time:68133ms step_avg:60.35ms
step:1130/2285 train_time:68193ms step_avg:60.35ms
step:1131/2285 train_time:68255ms step_avg:60.35ms
step:1132/2285 train_time:68316ms step_avg:60.35ms
step:1133/2285 train_time:68378ms step_avg:60.35ms
step:1134/2285 train_time:68438ms step_avg:60.35ms
step:1135/2285 train_time:68500ms step_avg:60.35ms
step:1136/2285 train_time:68559ms step_avg:60.35ms
step:1137/2285 train_time:68621ms step_avg:60.35ms
step:1138/2285 train_time:68681ms step_avg:60.35ms
step:1139/2285 train_time:68743ms step_avg:60.35ms
step:1140/2285 train_time:68803ms step_avg:60.35ms
step:1141/2285 train_time:68865ms step_avg:60.35ms
step:1142/2285 train_time:68924ms step_avg:60.35ms
step:1143/2285 train_time:68986ms step_avg:60.36ms
step:1144/2285 train_time:69046ms step_avg:60.36ms
step:1145/2285 train_time:69108ms step_avg:60.36ms
step:1146/2285 train_time:69168ms step_avg:60.36ms
step:1147/2285 train_time:69231ms step_avg:60.36ms
step:1148/2285 train_time:69290ms step_avg:60.36ms
step:1149/2285 train_time:69352ms step_avg:60.36ms
step:1150/2285 train_time:69412ms step_avg:60.36ms
step:1151/2285 train_time:69474ms step_avg:60.36ms
step:1152/2285 train_time:69534ms step_avg:60.36ms
step:1153/2285 train_time:69595ms step_avg:60.36ms
step:1154/2285 train_time:69656ms step_avg:60.36ms
step:1155/2285 train_time:69719ms step_avg:60.36ms
step:1156/2285 train_time:69778ms step_avg:60.36ms
step:1157/2285 train_time:69840ms step_avg:60.36ms
step:1158/2285 train_time:69901ms step_avg:60.36ms
step:1159/2285 train_time:69963ms step_avg:60.37ms
step:1160/2285 train_time:70024ms step_avg:60.37ms
step:1161/2285 train_time:70086ms step_avg:60.37ms
step:1162/2285 train_time:70145ms step_avg:60.37ms
step:1163/2285 train_time:70207ms step_avg:60.37ms
step:1164/2285 train_time:70267ms step_avg:60.37ms
step:1165/2285 train_time:70329ms step_avg:60.37ms
step:1166/2285 train_time:70389ms step_avg:60.37ms
step:1167/2285 train_time:70451ms step_avg:60.37ms
step:1168/2285 train_time:70511ms step_avg:60.37ms
step:1169/2285 train_time:70573ms step_avg:60.37ms
step:1170/2285 train_time:70633ms step_avg:60.37ms
step:1171/2285 train_time:70694ms step_avg:60.37ms
step:1172/2285 train_time:70755ms step_avg:60.37ms
step:1173/2285 train_time:70818ms step_avg:60.37ms
step:1174/2285 train_time:70878ms step_avg:60.37ms
step:1175/2285 train_time:70940ms step_avg:60.37ms
step:1176/2285 train_time:71000ms step_avg:60.37ms
step:1177/2285 train_time:71062ms step_avg:60.38ms
step:1178/2285 train_time:71121ms step_avg:60.37ms
step:1179/2285 train_time:71183ms step_avg:60.38ms
step:1180/2285 train_time:71242ms step_avg:60.37ms
step:1181/2285 train_time:71305ms step_avg:60.38ms
step:1182/2285 train_time:71364ms step_avg:60.38ms
step:1183/2285 train_time:71427ms step_avg:60.38ms
step:1184/2285 train_time:71487ms step_avg:60.38ms
step:1185/2285 train_time:71549ms step_avg:60.38ms
step:1186/2285 train_time:71609ms step_avg:60.38ms
step:1187/2285 train_time:71672ms step_avg:60.38ms
step:1188/2285 train_time:71731ms step_avg:60.38ms
step:1189/2285 train_time:71794ms step_avg:60.38ms
step:1190/2285 train_time:71854ms step_avg:60.38ms
step:1191/2285 train_time:71917ms step_avg:60.38ms
step:1192/2285 train_time:71977ms step_avg:60.38ms
step:1193/2285 train_time:72040ms step_avg:60.39ms
step:1194/2285 train_time:72099ms step_avg:60.38ms
step:1195/2285 train_time:72161ms step_avg:60.39ms
step:1196/2285 train_time:72221ms step_avg:60.39ms
step:1197/2285 train_time:72283ms step_avg:60.39ms
step:1198/2285 train_time:72343ms step_avg:60.39ms
step:1199/2285 train_time:72405ms step_avg:60.39ms
step:1200/2285 train_time:72464ms step_avg:60.39ms
step:1201/2285 train_time:72526ms step_avg:60.39ms
step:1202/2285 train_time:72587ms step_avg:60.39ms
step:1203/2285 train_time:72649ms step_avg:60.39ms
step:1204/2285 train_time:72709ms step_avg:60.39ms
step:1205/2285 train_time:72771ms step_avg:60.39ms
step:1206/2285 train_time:72830ms step_avg:60.39ms
step:1207/2285 train_time:72893ms step_avg:60.39ms
step:1208/2285 train_time:72953ms step_avg:60.39ms
step:1209/2285 train_time:73015ms step_avg:60.39ms
step:1210/2285 train_time:73075ms step_avg:60.39ms
step:1211/2285 train_time:73138ms step_avg:60.39ms
step:1212/2285 train_time:73198ms step_avg:60.39ms
step:1213/2285 train_time:73261ms step_avg:60.40ms
step:1214/2285 train_time:73321ms step_avg:60.40ms
step:1215/2285 train_time:73382ms step_avg:60.40ms
step:1216/2285 train_time:73442ms step_avg:60.40ms
step:1217/2285 train_time:73504ms step_avg:60.40ms
step:1218/2285 train_time:73564ms step_avg:60.40ms
step:1219/2285 train_time:73626ms step_avg:60.40ms
step:1220/2285 train_time:73686ms step_avg:60.40ms
step:1221/2285 train_time:73748ms step_avg:60.40ms
step:1222/2285 train_time:73808ms step_avg:60.40ms
step:1223/2285 train_time:73871ms step_avg:60.40ms
step:1224/2285 train_time:73931ms step_avg:60.40ms
step:1225/2285 train_time:73993ms step_avg:60.40ms
step:1226/2285 train_time:74053ms step_avg:60.40ms
step:1227/2285 train_time:74116ms step_avg:60.40ms
step:1228/2285 train_time:74176ms step_avg:60.40ms
step:1229/2285 train_time:74239ms step_avg:60.41ms
step:1230/2285 train_time:74298ms step_avg:60.41ms
step:1231/2285 train_time:74360ms step_avg:60.41ms
step:1232/2285 train_time:74420ms step_avg:60.41ms
step:1233/2285 train_time:74483ms step_avg:60.41ms
step:1234/2285 train_time:74542ms step_avg:60.41ms
step:1235/2285 train_time:74604ms step_avg:60.41ms
step:1236/2285 train_time:74664ms step_avg:60.41ms
step:1237/2285 train_time:74726ms step_avg:60.41ms
step:1238/2285 train_time:74786ms step_avg:60.41ms
step:1239/2285 train_time:74848ms step_avg:60.41ms
step:1240/2285 train_time:74908ms step_avg:60.41ms
step:1241/2285 train_time:74970ms step_avg:60.41ms
step:1242/2285 train_time:75030ms step_avg:60.41ms
step:1243/2285 train_time:75093ms step_avg:60.41ms
step:1244/2285 train_time:75153ms step_avg:60.41ms
step:1245/2285 train_time:75215ms step_avg:60.41ms
step:1246/2285 train_time:75274ms step_avg:60.41ms
step:1247/2285 train_time:75336ms step_avg:60.41ms
step:1248/2285 train_time:75396ms step_avg:60.41ms
step:1249/2285 train_time:75459ms step_avg:60.42ms
step:1250/2285 train_time:75520ms step_avg:60.42ms
step:1250/2285 val_loss:3.4929 train_time:75583ms step_avg:60.47ms
step:1251/2285 train_time:75602ms step_avg:60.43ms
step:1252/2285 train_time:75645ms step_avg:60.42ms
step:1253/2285 train_time:75706ms step_avg:60.42ms
step:1254/2285 train_time:75765ms step_avg:60.42ms
step:1255/2285 train_time:75828ms step_avg:60.42ms
step:1256/2285 train_time:75887ms step_avg:60.42ms
step:1257/2285 train_time:75948ms step_avg:60.42ms
step:1258/2285 train_time:76007ms step_avg:60.42ms
step:1259/2285 train_time:76067ms step_avg:60.42ms
step:1260/2285 train_time:76126ms step_avg:60.42ms
step:1261/2285 train_time:76188ms step_avg:60.42ms
step:1262/2285 train_time:76247ms step_avg:60.42ms
step:1263/2285 train_time:76308ms step_avg:60.42ms
step:1264/2285 train_time:76366ms step_avg:60.42ms
step:1265/2285 train_time:76428ms step_avg:60.42ms
step:1266/2285 train_time:76496ms step_avg:60.42ms
step:1267/2285 train_time:76563ms step_avg:60.43ms
step:1268/2285 train_time:76624ms step_avg:60.43ms
step:1269/2285 train_time:76686ms step_avg:60.43ms
step:1270/2285 train_time:76745ms step_avg:60.43ms
step:1271/2285 train_time:76808ms step_avg:60.43ms
step:1272/2285 train_time:76867ms step_avg:60.43ms
step:1273/2285 train_time:76928ms step_avg:60.43ms
step:1274/2285 train_time:76987ms step_avg:60.43ms
step:1275/2285 train_time:77048ms step_avg:60.43ms
step:1276/2285 train_time:77107ms step_avg:60.43ms
step:1277/2285 train_time:77168ms step_avg:60.43ms
step:1278/2285 train_time:77227ms step_avg:60.43ms
step:1279/2285 train_time:77288ms step_avg:60.43ms
step:1280/2285 train_time:77348ms step_avg:60.43ms
step:1281/2285 train_time:77412ms step_avg:60.43ms
step:1282/2285 train_time:77475ms step_avg:60.43ms
step:1283/2285 train_time:77539ms step_avg:60.44ms
step:1284/2285 train_time:77599ms step_avg:60.44ms
step:1285/2285 train_time:77662ms step_avg:60.44ms
step:1286/2285 train_time:77722ms step_avg:60.44ms
step:1287/2285 train_time:77783ms step_avg:60.44ms
step:1288/2285 train_time:77843ms step_avg:60.44ms
step:1289/2285 train_time:77905ms step_avg:60.44ms
step:1290/2285 train_time:77964ms step_avg:60.44ms
step:1291/2285 train_time:78025ms step_avg:60.44ms
step:1292/2285 train_time:78084ms step_avg:60.44ms
step:1293/2285 train_time:78146ms step_avg:60.44ms
step:1294/2285 train_time:78205ms step_avg:60.44ms
step:1295/2285 train_time:78266ms step_avg:60.44ms
step:1296/2285 train_time:78326ms step_avg:60.44ms
step:1297/2285 train_time:78389ms step_avg:60.44ms
step:1298/2285 train_time:78451ms step_avg:60.44ms
step:1299/2285 train_time:78515ms step_avg:60.44ms
step:1300/2285 train_time:78575ms step_avg:60.44ms
step:1301/2285 train_time:78637ms step_avg:60.44ms
step:1302/2285 train_time:78697ms step_avg:60.44ms
step:1303/2285 train_time:78758ms step_avg:60.44ms
step:1304/2285 train_time:78818ms step_avg:60.44ms
step:1305/2285 train_time:78880ms step_avg:60.44ms
step:1306/2285 train_time:78940ms step_avg:60.44ms
step:1307/2285 train_time:79001ms step_avg:60.44ms
step:1308/2285 train_time:79061ms step_avg:60.44ms
step:1309/2285 train_time:79123ms step_avg:60.45ms
step:1310/2285 train_time:79182ms step_avg:60.44ms
step:1311/2285 train_time:79243ms step_avg:60.45ms
step:1312/2285 train_time:79303ms step_avg:60.44ms
step:1313/2285 train_time:79366ms step_avg:60.45ms
step:1314/2285 train_time:79426ms step_avg:60.45ms
step:1315/2285 train_time:79489ms step_avg:60.45ms
step:1316/2285 train_time:79549ms step_avg:60.45ms
step:1317/2285 train_time:79612ms step_avg:60.45ms
step:1318/2285 train_time:79672ms step_avg:60.45ms
step:1319/2285 train_time:79734ms step_avg:60.45ms
step:1320/2285 train_time:79794ms step_avg:60.45ms
step:1321/2285 train_time:79856ms step_avg:60.45ms
step:1322/2285 train_time:79915ms step_avg:60.45ms
step:1323/2285 train_time:79977ms step_avg:60.45ms
step:1324/2285 train_time:80037ms step_avg:60.45ms
step:1325/2285 train_time:80099ms step_avg:60.45ms
step:1326/2285 train_time:80159ms step_avg:60.45ms
step:1327/2285 train_time:80221ms step_avg:60.45ms
step:1328/2285 train_time:80281ms step_avg:60.45ms
step:1329/2285 train_time:80344ms step_avg:60.45ms
step:1330/2285 train_time:80404ms step_avg:60.45ms
step:1331/2285 train_time:80466ms step_avg:60.46ms
step:1332/2285 train_time:80525ms step_avg:60.45ms
step:1333/2285 train_time:80588ms step_avg:60.46ms
step:1334/2285 train_time:80648ms step_avg:60.46ms
step:1335/2285 train_time:80710ms step_avg:60.46ms
step:1336/2285 train_time:80769ms step_avg:60.46ms
step:1337/2285 train_time:80832ms step_avg:60.46ms
step:1338/2285 train_time:80892ms step_avg:60.46ms
step:1339/2285 train_time:80954ms step_avg:60.46ms
step:1340/2285 train_time:81014ms step_avg:60.46ms
step:1341/2285 train_time:81076ms step_avg:60.46ms
step:1342/2285 train_time:81136ms step_avg:60.46ms
step:1343/2285 train_time:81198ms step_avg:60.46ms
step:1344/2285 train_time:81257ms step_avg:60.46ms
step:1345/2285 train_time:81319ms step_avg:60.46ms
step:1346/2285 train_time:81380ms step_avg:60.46ms
step:1347/2285 train_time:81443ms step_avg:60.46ms
step:1348/2285 train_time:81503ms step_avg:60.46ms
step:1349/2285 train_time:81565ms step_avg:60.46ms
step:1350/2285 train_time:81624ms step_avg:60.46ms
step:1351/2285 train_time:81687ms step_avg:60.46ms
step:1352/2285 train_time:81747ms step_avg:60.46ms
step:1353/2285 train_time:81809ms step_avg:60.46ms
step:1354/2285 train_time:81868ms step_avg:60.46ms
step:1355/2285 train_time:81930ms step_avg:60.47ms
step:1356/2285 train_time:81991ms step_avg:60.47ms
step:1357/2285 train_time:82053ms step_avg:60.47ms
step:1358/2285 train_time:82113ms step_avg:60.47ms
step:1359/2285 train_time:82175ms step_avg:60.47ms
step:1360/2285 train_time:82235ms step_avg:60.47ms
step:1361/2285 train_time:82297ms step_avg:60.47ms
step:1362/2285 train_time:82356ms step_avg:60.47ms
step:1363/2285 train_time:82418ms step_avg:60.47ms
step:1364/2285 train_time:82478ms step_avg:60.47ms
step:1365/2285 train_time:82541ms step_avg:60.47ms
step:1366/2285 train_time:82601ms step_avg:60.47ms
step:1367/2285 train_time:82664ms step_avg:60.47ms
step:1368/2285 train_time:82724ms step_avg:60.47ms
step:1369/2285 train_time:82786ms step_avg:60.47ms
step:1370/2285 train_time:82846ms step_avg:60.47ms
step:1371/2285 train_time:82908ms step_avg:60.47ms
step:1372/2285 train_time:82968ms step_avg:60.47ms
step:1373/2285 train_time:83030ms step_avg:60.47ms
step:1374/2285 train_time:83090ms step_avg:60.47ms
step:1375/2285 train_time:83152ms step_avg:60.47ms
step:1376/2285 train_time:83212ms step_avg:60.47ms
step:1377/2285 train_time:83275ms step_avg:60.48ms
step:1378/2285 train_time:83335ms step_avg:60.47ms
step:1379/2285 train_time:83397ms step_avg:60.48ms
step:1380/2285 train_time:83457ms step_avg:60.48ms
step:1381/2285 train_time:83519ms step_avg:60.48ms
step:1382/2285 train_time:83579ms step_avg:60.48ms
step:1383/2285 train_time:83641ms step_avg:60.48ms
step:1384/2285 train_time:83702ms step_avg:60.48ms
step:1385/2285 train_time:83765ms step_avg:60.48ms
step:1386/2285 train_time:83824ms step_avg:60.48ms
step:1387/2285 train_time:83886ms step_avg:60.48ms
step:1388/2285 train_time:83945ms step_avg:60.48ms
step:1389/2285 train_time:84007ms step_avg:60.48ms
step:1390/2285 train_time:84067ms step_avg:60.48ms
step:1391/2285 train_time:84130ms step_avg:60.48ms
step:1392/2285 train_time:84191ms step_avg:60.48ms
step:1393/2285 train_time:84253ms step_avg:60.48ms
step:1394/2285 train_time:84313ms step_avg:60.48ms
step:1395/2285 train_time:84375ms step_avg:60.48ms
step:1396/2285 train_time:84435ms step_avg:60.48ms
step:1397/2285 train_time:84497ms step_avg:60.48ms
step:1398/2285 train_time:84557ms step_avg:60.48ms
step:1399/2285 train_time:84619ms step_avg:60.49ms
step:1400/2285 train_time:84679ms step_avg:60.48ms
step:1401/2285 train_time:84741ms step_avg:60.49ms
step:1402/2285 train_time:84801ms step_avg:60.49ms
step:1403/2285 train_time:84864ms step_avg:60.49ms
step:1404/2285 train_time:84924ms step_avg:60.49ms
step:1405/2285 train_time:84986ms step_avg:60.49ms
step:1406/2285 train_time:85046ms step_avg:60.49ms
step:1407/2285 train_time:85108ms step_avg:60.49ms
step:1408/2285 train_time:85167ms step_avg:60.49ms
step:1409/2285 train_time:85230ms step_avg:60.49ms
step:1410/2285 train_time:85290ms step_avg:60.49ms
step:1411/2285 train_time:85352ms step_avg:60.49ms
step:1412/2285 train_time:85412ms step_avg:60.49ms
step:1413/2285 train_time:85475ms step_avg:60.49ms
step:1414/2285 train_time:85534ms step_avg:60.49ms
step:1415/2285 train_time:85597ms step_avg:60.49ms
step:1416/2285 train_time:85656ms step_avg:60.49ms
step:1417/2285 train_time:85718ms step_avg:60.49ms
step:1418/2285 train_time:85778ms step_avg:60.49ms
step:1419/2285 train_time:85840ms step_avg:60.49ms
step:1420/2285 train_time:85901ms step_avg:60.49ms
step:1421/2285 train_time:85963ms step_avg:60.50ms
step:1422/2285 train_time:86024ms step_avg:60.49ms
step:1423/2285 train_time:86086ms step_avg:60.50ms
step:1424/2285 train_time:86145ms step_avg:60.50ms
step:1425/2285 train_time:86208ms step_avg:60.50ms
step:1426/2285 train_time:86267ms step_avg:60.50ms
step:1427/2285 train_time:86329ms step_avg:60.50ms
step:1428/2285 train_time:86390ms step_avg:60.50ms
step:1429/2285 train_time:86453ms step_avg:60.50ms
step:1430/2285 train_time:86513ms step_avg:60.50ms
step:1431/2285 train_time:86576ms step_avg:60.50ms
step:1432/2285 train_time:86635ms step_avg:60.50ms
step:1433/2285 train_time:86697ms step_avg:60.50ms
step:1434/2285 train_time:86757ms step_avg:60.50ms
step:1435/2285 train_time:86819ms step_avg:60.50ms
step:1436/2285 train_time:86879ms step_avg:60.50ms
step:1437/2285 train_time:86941ms step_avg:60.50ms
step:1438/2285 train_time:87001ms step_avg:60.50ms
step:1439/2285 train_time:87063ms step_avg:60.50ms
step:1440/2285 train_time:87124ms step_avg:60.50ms
step:1441/2285 train_time:87186ms step_avg:60.50ms
step:1442/2285 train_time:87246ms step_avg:60.50ms
step:1443/2285 train_time:87308ms step_avg:60.50ms
step:1444/2285 train_time:87368ms step_avg:60.50ms
step:1445/2285 train_time:87431ms step_avg:60.51ms
step:1446/2285 train_time:87491ms step_avg:60.51ms
step:1447/2285 train_time:87554ms step_avg:60.51ms
step:1448/2285 train_time:87613ms step_avg:60.51ms
step:1449/2285 train_time:87675ms step_avg:60.51ms
step:1450/2285 train_time:87735ms step_avg:60.51ms
step:1451/2285 train_time:87797ms step_avg:60.51ms
step:1452/2285 train_time:87856ms step_avg:60.51ms
step:1453/2285 train_time:87918ms step_avg:60.51ms
step:1454/2285 train_time:87978ms step_avg:60.51ms
step:1455/2285 train_time:88040ms step_avg:60.51ms
step:1456/2285 train_time:88101ms step_avg:60.51ms
step:1457/2285 train_time:88164ms step_avg:60.51ms
step:1458/2285 train_time:88223ms step_avg:60.51ms
step:1459/2285 train_time:88285ms step_avg:60.51ms
step:1460/2285 train_time:88345ms step_avg:60.51ms
step:1461/2285 train_time:88407ms step_avg:60.51ms
step:1462/2285 train_time:88467ms step_avg:60.51ms
step:1463/2285 train_time:88531ms step_avg:60.51ms
step:1464/2285 train_time:88591ms step_avg:60.51ms
step:1465/2285 train_time:88653ms step_avg:60.51ms
step:1466/2285 train_time:88713ms step_avg:60.51ms
step:1467/2285 train_time:88775ms step_avg:60.51ms
step:1468/2285 train_time:88835ms step_avg:60.51ms
step:1469/2285 train_time:88897ms step_avg:60.52ms
step:1470/2285 train_time:88957ms step_avg:60.51ms
step:1471/2285 train_time:89019ms step_avg:60.52ms
step:1472/2285 train_time:89078ms step_avg:60.52ms
step:1473/2285 train_time:89141ms step_avg:60.52ms
step:1474/2285 train_time:89201ms step_avg:60.52ms
step:1475/2285 train_time:89264ms step_avg:60.52ms
step:1476/2285 train_time:89324ms step_avg:60.52ms
step:1477/2285 train_time:89386ms step_avg:60.52ms
step:1478/2285 train_time:89446ms step_avg:60.52ms
step:1479/2285 train_time:89508ms step_avg:60.52ms
step:1480/2285 train_time:89568ms step_avg:60.52ms
step:1481/2285 train_time:89631ms step_avg:60.52ms
step:1482/2285 train_time:89691ms step_avg:60.52ms
step:1483/2285 train_time:89754ms step_avg:60.52ms
step:1484/2285 train_time:89813ms step_avg:60.52ms
step:1485/2285 train_time:89875ms step_avg:60.52ms
step:1486/2285 train_time:89935ms step_avg:60.52ms
step:1487/2285 train_time:89997ms step_avg:60.52ms
step:1488/2285 train_time:90056ms step_avg:60.52ms
step:1489/2285 train_time:90119ms step_avg:60.52ms
step:1490/2285 train_time:90178ms step_avg:60.52ms
step:1491/2285 train_time:90241ms step_avg:60.52ms
step:1492/2285 train_time:90301ms step_avg:60.52ms
step:1493/2285 train_time:90363ms step_avg:60.52ms
step:1494/2285 train_time:90423ms step_avg:60.52ms
step:1495/2285 train_time:90486ms step_avg:60.53ms
step:1496/2285 train_time:90545ms step_avg:60.53ms
step:1497/2285 train_time:90608ms step_avg:60.53ms
step:1498/2285 train_time:90668ms step_avg:60.53ms
step:1499/2285 train_time:90731ms step_avg:60.53ms
step:1500/2285 train_time:90791ms step_avg:60.53ms
step:1500/2285 val_loss:3.4260 train_time:90854ms step_avg:60.57ms
step:1501/2285 train_time:90873ms step_avg:60.54ms
step:1502/2285 train_time:90917ms step_avg:60.53ms
step:1503/2285 train_time:90981ms step_avg:60.53ms
step:1504/2285 train_time:91042ms step_avg:60.53ms
step:1505/2285 train_time:91103ms step_avg:60.53ms
step:1506/2285 train_time:91163ms step_avg:60.53ms
step:1507/2285 train_time:91225ms step_avg:60.53ms
step:1508/2285 train_time:91284ms step_avg:60.53ms
step:1509/2285 train_time:91346ms step_avg:60.53ms
step:1510/2285 train_time:91405ms step_avg:60.53ms
step:1511/2285 train_time:91467ms step_avg:60.53ms
step:1512/2285 train_time:91527ms step_avg:60.53ms
step:1513/2285 train_time:91588ms step_avg:60.53ms
step:1514/2285 train_time:91648ms step_avg:60.53ms
step:1515/2285 train_time:91709ms step_avg:60.53ms
step:1516/2285 train_time:91770ms step_avg:60.53ms
step:1517/2285 train_time:91834ms step_avg:60.54ms
step:1518/2285 train_time:91894ms step_avg:60.54ms
step:1519/2285 train_time:91957ms step_avg:60.54ms
step:1520/2285 train_time:92017ms step_avg:60.54ms
step:1521/2285 train_time:92079ms step_avg:60.54ms
step:1522/2285 train_time:92139ms step_avg:60.54ms
step:1523/2285 train_time:92201ms step_avg:60.54ms
step:1524/2285 train_time:92261ms step_avg:60.54ms
step:1525/2285 train_time:92322ms step_avg:60.54ms
step:1526/2285 train_time:92382ms step_avg:60.54ms
step:1527/2285 train_time:92444ms step_avg:60.54ms
step:1528/2285 train_time:92504ms step_avg:60.54ms
step:1529/2285 train_time:92565ms step_avg:60.54ms
step:1530/2285 train_time:92624ms step_avg:60.54ms
step:1531/2285 train_time:92687ms step_avg:60.54ms
step:1532/2285 train_time:92748ms step_avg:60.54ms
step:1533/2285 train_time:92811ms step_avg:60.54ms
step:1534/2285 train_time:92872ms step_avg:60.54ms
step:1535/2285 train_time:92936ms step_avg:60.54ms
step:1536/2285 train_time:92996ms step_avg:60.54ms
step:1537/2285 train_time:93058ms step_avg:60.55ms
step:1538/2285 train_time:93118ms step_avg:60.54ms
step:1539/2285 train_time:93180ms step_avg:60.55ms
step:1540/2285 train_time:93240ms step_avg:60.55ms
step:1541/2285 train_time:93302ms step_avg:60.55ms
step:1542/2285 train_time:93362ms step_avg:60.55ms
step:1543/2285 train_time:93423ms step_avg:60.55ms
step:1544/2285 train_time:93483ms step_avg:60.55ms
step:1545/2285 train_time:93544ms step_avg:60.55ms
step:1546/2285 train_time:93604ms step_avg:60.55ms
step:1547/2285 train_time:93666ms step_avg:60.55ms
step:1548/2285 train_time:93727ms step_avg:60.55ms
step:1549/2285 train_time:93789ms step_avg:60.55ms
step:1550/2285 train_time:93850ms step_avg:60.55ms
step:1551/2285 train_time:93913ms step_avg:60.55ms
step:1552/2285 train_time:93973ms step_avg:60.55ms
step:1553/2285 train_time:94036ms step_avg:60.55ms
step:1554/2285 train_time:94096ms step_avg:60.55ms
step:1555/2285 train_time:94159ms step_avg:60.55ms
step:1556/2285 train_time:94218ms step_avg:60.55ms
step:1557/2285 train_time:94281ms step_avg:60.55ms
step:1558/2285 train_time:94341ms step_avg:60.55ms
step:1559/2285 train_time:94402ms step_avg:60.55ms
step:1560/2285 train_time:94462ms step_avg:60.55ms
step:1561/2285 train_time:94524ms step_avg:60.55ms
step:1562/2285 train_time:94584ms step_avg:60.55ms
step:1563/2285 train_time:94646ms step_avg:60.55ms
step:1564/2285 train_time:94706ms step_avg:60.55ms
step:1565/2285 train_time:94769ms step_avg:60.56ms
step:1566/2285 train_time:94830ms step_avg:60.56ms
step:1567/2285 train_time:94893ms step_avg:60.56ms
step:1568/2285 train_time:94953ms step_avg:60.56ms
step:1569/2285 train_time:95015ms step_avg:60.56ms
step:1570/2285 train_time:95075ms step_avg:60.56ms
step:1571/2285 train_time:95138ms step_avg:60.56ms
step:1572/2285 train_time:95197ms step_avg:60.56ms
step:1573/2285 train_time:95260ms step_avg:60.56ms
step:1574/2285 train_time:95319ms step_avg:60.56ms
step:1575/2285 train_time:95381ms step_avg:60.56ms
step:1576/2285 train_time:95442ms step_avg:60.56ms
step:1577/2285 train_time:95504ms step_avg:60.56ms
step:1578/2285 train_time:95565ms step_avg:60.56ms
step:1579/2285 train_time:95627ms step_avg:60.56ms
step:1580/2285 train_time:95687ms step_avg:60.56ms
step:1581/2285 train_time:95750ms step_avg:60.56ms
step:1582/2285 train_time:95810ms step_avg:60.56ms
step:1583/2285 train_time:95873ms step_avg:60.56ms
step:1584/2285 train_time:95933ms step_avg:60.56ms
step:1585/2285 train_time:95996ms step_avg:60.57ms
step:1586/2285 train_time:96055ms step_avg:60.56ms
step:1587/2285 train_time:96118ms step_avg:60.57ms
step:1588/2285 train_time:96178ms step_avg:60.57ms
step:1589/2285 train_time:96240ms step_avg:60.57ms
step:1590/2285 train_time:96300ms step_avg:60.57ms
step:1591/2285 train_time:96361ms step_avg:60.57ms
step:1592/2285 train_time:96421ms step_avg:60.57ms
step:1593/2285 train_time:96483ms step_avg:60.57ms
step:1594/2285 train_time:96544ms step_avg:60.57ms
step:1595/2285 train_time:96607ms step_avg:60.57ms
step:1596/2285 train_time:96666ms step_avg:60.57ms
step:1597/2285 train_time:96728ms step_avg:60.57ms
step:1598/2285 train_time:96789ms step_avg:60.57ms
step:1599/2285 train_time:96852ms step_avg:60.57ms
step:1600/2285 train_time:96912ms step_avg:60.57ms
step:1601/2285 train_time:96975ms step_avg:60.57ms
step:1602/2285 train_time:97035ms step_avg:60.57ms
step:1603/2285 train_time:97096ms step_avg:60.57ms
step:1604/2285 train_time:97156ms step_avg:60.57ms
step:1605/2285 train_time:97218ms step_avg:60.57ms
step:1606/2285 train_time:97279ms step_avg:60.57ms
step:1607/2285 train_time:97341ms step_avg:60.57ms
step:1608/2285 train_time:97401ms step_avg:60.57ms
step:1609/2285 train_time:97463ms step_avg:60.57ms
step:1610/2285 train_time:97523ms step_avg:60.57ms
step:1611/2285 train_time:97585ms step_avg:60.57ms
step:1612/2285 train_time:97645ms step_avg:60.57ms
step:1613/2285 train_time:97708ms step_avg:60.58ms
step:1614/2285 train_time:97768ms step_avg:60.57ms
step:1615/2285 train_time:97830ms step_avg:60.58ms
step:1616/2285 train_time:97890ms step_avg:60.58ms
step:1617/2285 train_time:97953ms step_avg:60.58ms
step:1618/2285 train_time:98013ms step_avg:60.58ms
step:1619/2285 train_time:98075ms step_avg:60.58ms
step:1620/2285 train_time:98135ms step_avg:60.58ms
step:1621/2285 train_time:98198ms step_avg:60.58ms
step:1622/2285 train_time:98258ms step_avg:60.58ms
step:1623/2285 train_time:98320ms step_avg:60.58ms
step:1624/2285 train_time:98380ms step_avg:60.58ms
step:1625/2285 train_time:98442ms step_avg:60.58ms
step:1626/2285 train_time:98502ms step_avg:60.58ms
step:1627/2285 train_time:98564ms step_avg:60.58ms
step:1628/2285 train_time:98625ms step_avg:60.58ms
step:1629/2285 train_time:98687ms step_avg:60.58ms
step:1630/2285 train_time:98747ms step_avg:60.58ms
step:1631/2285 train_time:98809ms step_avg:60.58ms
step:1632/2285 train_time:98869ms step_avg:60.58ms
step:1633/2285 train_time:98932ms step_avg:60.58ms
step:1634/2285 train_time:98993ms step_avg:60.58ms
step:1635/2285 train_time:99055ms step_avg:60.58ms
step:1636/2285 train_time:99115ms step_avg:60.58ms
step:1637/2285 train_time:99177ms step_avg:60.58ms
step:1638/2285 train_time:99237ms step_avg:60.58ms
step:1639/2285 train_time:99300ms step_avg:60.59ms
step:1640/2285 train_time:99360ms step_avg:60.59ms
step:1641/2285 train_time:99422ms step_avg:60.59ms
step:1642/2285 train_time:99482ms step_avg:60.59ms
step:1643/2285 train_time:99544ms step_avg:60.59ms
step:1644/2285 train_time:99604ms step_avg:60.59ms
step:1645/2285 train_time:99666ms step_avg:60.59ms
step:1646/2285 train_time:99726ms step_avg:60.59ms
step:1647/2285 train_time:99789ms step_avg:60.59ms
step:1648/2285 train_time:99849ms step_avg:60.59ms
step:1649/2285 train_time:99912ms step_avg:60.59ms
step:1650/2285 train_time:99973ms step_avg:60.59ms
step:1651/2285 train_time:100036ms step_avg:60.59ms
step:1652/2285 train_time:100095ms step_avg:60.59ms
step:1653/2285 train_time:100157ms step_avg:60.59ms
step:1654/2285 train_time:100217ms step_avg:60.59ms
step:1655/2285 train_time:100279ms step_avg:60.59ms
step:1656/2285 train_time:100340ms step_avg:60.59ms
step:1657/2285 train_time:100402ms step_avg:60.59ms
step:1658/2285 train_time:100462ms step_avg:60.59ms
step:1659/2285 train_time:100524ms step_avg:60.59ms
step:1660/2285 train_time:100583ms step_avg:60.59ms
step:1661/2285 train_time:100646ms step_avg:60.59ms
step:1662/2285 train_time:100706ms step_avg:60.59ms
step:1663/2285 train_time:100768ms step_avg:60.59ms
step:1664/2285 train_time:100827ms step_avg:60.59ms
step:1665/2285 train_time:100890ms step_avg:60.59ms
step:1666/2285 train_time:100951ms step_avg:60.59ms
step:1667/2285 train_time:101014ms step_avg:60.60ms
step:1668/2285 train_time:101074ms step_avg:60.60ms
step:1669/2285 train_time:101136ms step_avg:60.60ms
step:1670/2285 train_time:101195ms step_avg:60.60ms
step:1671/2285 train_time:101258ms step_avg:60.60ms
step:1672/2285 train_time:101318ms step_avg:60.60ms
step:1673/2285 train_time:101380ms step_avg:60.60ms
step:1674/2285 train_time:101440ms step_avg:60.60ms
step:1675/2285 train_time:101502ms step_avg:60.60ms
step:1676/2285 train_time:101562ms step_avg:60.60ms
step:1677/2285 train_time:101624ms step_avg:60.60ms
step:1678/2285 train_time:101685ms step_avg:60.60ms
step:1679/2285 train_time:101746ms step_avg:60.60ms
step:1680/2285 train_time:101806ms step_avg:60.60ms
step:1681/2285 train_time:101869ms step_avg:60.60ms
step:1682/2285 train_time:101930ms step_avg:60.60ms
step:1683/2285 train_time:101992ms step_avg:60.60ms
step:1684/2285 train_time:102052ms step_avg:60.60ms
step:1685/2285 train_time:102115ms step_avg:60.60ms
step:1686/2285 train_time:102174ms step_avg:60.60ms
step:1687/2285 train_time:102236ms step_avg:60.60ms
step:1688/2285 train_time:102297ms step_avg:60.60ms
step:1689/2285 train_time:102359ms step_avg:60.60ms
step:1690/2285 train_time:102419ms step_avg:60.60ms
step:1691/2285 train_time:102481ms step_avg:60.60ms
step:1692/2285 train_time:102541ms step_avg:60.60ms
step:1693/2285 train_time:102604ms step_avg:60.60ms
step:1694/2285 train_time:102664ms step_avg:60.60ms
step:1695/2285 train_time:102726ms step_avg:60.61ms
step:1696/2285 train_time:102786ms step_avg:60.61ms
step:1697/2285 train_time:102849ms step_avg:60.61ms
step:1698/2285 train_time:102909ms step_avg:60.61ms
step:1699/2285 train_time:102972ms step_avg:60.61ms
step:1700/2285 train_time:103033ms step_avg:60.61ms
step:1701/2285 train_time:103095ms step_avg:60.61ms
step:1702/2285 train_time:103155ms step_avg:60.61ms
step:1703/2285 train_time:103217ms step_avg:60.61ms
step:1704/2285 train_time:103277ms step_avg:60.61ms
step:1705/2285 train_time:103340ms step_avg:60.61ms
step:1706/2285 train_time:103400ms step_avg:60.61ms
step:1707/2285 train_time:103463ms step_avg:60.61ms
step:1708/2285 train_time:103522ms step_avg:60.61ms
step:1709/2285 train_time:103584ms step_avg:60.61ms
step:1710/2285 train_time:103644ms step_avg:60.61ms
step:1711/2285 train_time:103706ms step_avg:60.61ms
step:1712/2285 train_time:103766ms step_avg:60.61ms
step:1713/2285 train_time:103828ms step_avg:60.61ms
step:1714/2285 train_time:103888ms step_avg:60.61ms
step:1715/2285 train_time:103951ms step_avg:60.61ms
step:1716/2285 train_time:104010ms step_avg:60.61ms
step:1717/2285 train_time:104074ms step_avg:60.61ms
step:1718/2285 train_time:104134ms step_avg:60.61ms
step:1719/2285 train_time:104196ms step_avg:60.61ms
step:1720/2285 train_time:104257ms step_avg:60.61ms
step:1721/2285 train_time:104320ms step_avg:60.62ms
step:1722/2285 train_time:104380ms step_avg:60.62ms
step:1723/2285 train_time:104442ms step_avg:60.62ms
step:1724/2285 train_time:104503ms step_avg:60.62ms
step:1725/2285 train_time:104565ms step_avg:60.62ms
step:1726/2285 train_time:104625ms step_avg:60.62ms
step:1727/2285 train_time:104686ms step_avg:60.62ms
step:1728/2285 train_time:104746ms step_avg:60.62ms
step:1729/2285 train_time:104808ms step_avg:60.62ms
step:1730/2285 train_time:104868ms step_avg:60.62ms
step:1731/2285 train_time:104931ms step_avg:60.62ms
step:1732/2285 train_time:104992ms step_avg:60.62ms
step:1733/2285 train_time:105054ms step_avg:60.62ms
step:1734/2285 train_time:105114ms step_avg:60.62ms
step:1735/2285 train_time:105176ms step_avg:60.62ms
step:1736/2285 train_time:105237ms step_avg:60.62ms
step:1737/2285 train_time:105299ms step_avg:60.62ms
step:1738/2285 train_time:105359ms step_avg:60.62ms
step:1739/2285 train_time:105421ms step_avg:60.62ms
step:1740/2285 train_time:105481ms step_avg:60.62ms
step:1741/2285 train_time:105543ms step_avg:60.62ms
step:1742/2285 train_time:105602ms step_avg:60.62ms
step:1743/2285 train_time:105664ms step_avg:60.62ms
step:1744/2285 train_time:105724ms step_avg:60.62ms
step:1745/2285 train_time:105787ms step_avg:60.62ms
step:1746/2285 train_time:105847ms step_avg:60.62ms
step:1747/2285 train_time:105910ms step_avg:60.62ms
step:1748/2285 train_time:105970ms step_avg:60.62ms
step:1749/2285 train_time:106033ms step_avg:60.63ms
step:1750/2285 train_time:106093ms step_avg:60.62ms
step:1750/2285 val_loss:3.3655 train_time:106157ms step_avg:60.66ms
step:1751/2285 train_time:106175ms step_avg:60.64ms
step:1752/2285 train_time:106221ms step_avg:60.63ms
step:1753/2285 train_time:106283ms step_avg:60.63ms
step:1754/2285 train_time:106344ms step_avg:60.63ms
step:1755/2285 train_time:106408ms step_avg:60.63ms
step:1756/2285 train_time:106468ms step_avg:60.63ms
step:1757/2285 train_time:106530ms step_avg:60.63ms
step:1758/2285 train_time:106588ms step_avg:60.63ms
step:1759/2285 train_time:106650ms step_avg:60.63ms
step:1760/2285 train_time:106709ms step_avg:60.63ms
step:1761/2285 train_time:106770ms step_avg:60.63ms
step:1762/2285 train_time:106830ms step_avg:60.63ms
step:1763/2285 train_time:106891ms step_avg:60.63ms
step:1764/2285 train_time:106951ms step_avg:60.63ms
step:1765/2285 train_time:107013ms step_avg:60.63ms
step:1766/2285 train_time:107075ms step_avg:60.63ms
step:1767/2285 train_time:107141ms step_avg:60.63ms
step:1768/2285 train_time:107201ms step_avg:60.63ms
step:1769/2285 train_time:107264ms step_avg:60.64ms
step:1770/2285 train_time:107325ms step_avg:60.64ms
step:1771/2285 train_time:107387ms step_avg:60.64ms
step:1772/2285 train_time:107447ms step_avg:60.64ms
step:1773/2285 train_time:107510ms step_avg:60.64ms
step:1774/2285 train_time:107569ms step_avg:60.64ms
step:1775/2285 train_time:107630ms step_avg:60.64ms
step:1776/2285 train_time:107689ms step_avg:60.64ms
step:1777/2285 train_time:107751ms step_avg:60.64ms
step:1778/2285 train_time:107810ms step_avg:60.64ms
step:1779/2285 train_time:107872ms step_avg:60.64ms
step:1780/2285 train_time:107931ms step_avg:60.64ms
step:1781/2285 train_time:107993ms step_avg:60.64ms
step:1782/2285 train_time:108055ms step_avg:60.64ms
step:1783/2285 train_time:108119ms step_avg:60.64ms
step:1784/2285 train_time:108179ms step_avg:60.64ms
step:1785/2285 train_time:108243ms step_avg:60.64ms
step:1786/2285 train_time:108303ms step_avg:60.64ms
step:1787/2285 train_time:108365ms step_avg:60.64ms
step:1788/2285 train_time:108425ms step_avg:60.64ms
step:1789/2285 train_time:108487ms step_avg:60.64ms
step:1790/2285 train_time:108546ms step_avg:60.64ms
step:1791/2285 train_time:108608ms step_avg:60.64ms
step:1792/2285 train_time:108667ms step_avg:60.64ms
step:1793/2285 train_time:108729ms step_avg:60.64ms
step:1794/2285 train_time:108788ms step_avg:60.64ms
step:1795/2285 train_time:108850ms step_avg:60.64ms
step:1796/2285 train_time:108909ms step_avg:60.64ms
step:1797/2285 train_time:108972ms step_avg:60.64ms
step:1798/2285 train_time:109032ms step_avg:60.64ms
step:1799/2285 train_time:109096ms step_avg:60.64ms
step:1800/2285 train_time:109157ms step_avg:60.64ms
step:1801/2285 train_time:109219ms step_avg:60.64ms
step:1802/2285 train_time:109280ms step_avg:60.64ms
step:1803/2285 train_time:109342ms step_avg:60.64ms
step:1804/2285 train_time:109402ms step_avg:60.64ms
step:1805/2285 train_time:109464ms step_avg:60.64ms
step:1806/2285 train_time:109525ms step_avg:60.64ms
step:1807/2285 train_time:109587ms step_avg:60.65ms
step:1808/2285 train_time:109646ms step_avg:60.64ms
step:1809/2285 train_time:109708ms step_avg:60.65ms
step:1810/2285 train_time:109767ms step_avg:60.64ms
step:1811/2285 train_time:109829ms step_avg:60.65ms
step:1812/2285 train_time:109888ms step_avg:60.64ms
step:1813/2285 train_time:109951ms step_avg:60.65ms
step:1814/2285 train_time:110011ms step_avg:60.65ms
step:1815/2285 train_time:110074ms step_avg:60.65ms
step:1816/2285 train_time:110135ms step_avg:60.65ms
step:1817/2285 train_time:110198ms step_avg:60.65ms
step:1818/2285 train_time:110259ms step_avg:60.65ms
step:1819/2285 train_time:110321ms step_avg:60.65ms
step:1820/2285 train_time:110381ms step_avg:60.65ms
step:1821/2285 train_time:110443ms step_avg:60.65ms
step:1822/2285 train_time:110503ms step_avg:60.65ms
step:1823/2285 train_time:110565ms step_avg:60.65ms
step:1824/2285 train_time:110624ms step_avg:60.65ms
step:1825/2285 train_time:110686ms step_avg:60.65ms
step:1826/2285 train_time:110746ms step_avg:60.65ms
step:1827/2285 train_time:110808ms step_avg:60.65ms
step:1828/2285 train_time:110868ms step_avg:60.65ms
step:1829/2285 train_time:110931ms step_avg:60.65ms
step:1830/2285 train_time:110990ms step_avg:60.65ms
step:1831/2285 train_time:111053ms step_avg:60.65ms
step:1832/2285 train_time:111113ms step_avg:60.65ms
step:1833/2285 train_time:111176ms step_avg:60.65ms
step:1834/2285 train_time:111236ms step_avg:60.65ms
step:1835/2285 train_time:111299ms step_avg:60.65ms
step:1836/2285 train_time:111359ms step_avg:60.65ms
step:1837/2285 train_time:111421ms step_avg:60.65ms
step:1838/2285 train_time:111481ms step_avg:60.65ms
step:1839/2285 train_time:111543ms step_avg:60.65ms
step:1840/2285 train_time:111603ms step_avg:60.65ms
step:1841/2285 train_time:111665ms step_avg:60.65ms
step:1842/2285 train_time:111725ms step_avg:60.65ms
step:1843/2285 train_time:111787ms step_avg:60.66ms
step:1844/2285 train_time:111847ms step_avg:60.65ms
step:1845/2285 train_time:111910ms step_avg:60.66ms
step:1846/2285 train_time:111970ms step_avg:60.66ms
step:1847/2285 train_time:112032ms step_avg:60.66ms
step:1848/2285 train_time:112092ms step_avg:60.66ms
step:1849/2285 train_time:112155ms step_avg:60.66ms
step:1850/2285 train_time:112215ms step_avg:60.66ms
step:1851/2285 train_time:112278ms step_avg:60.66ms
step:1852/2285 train_time:112338ms step_avg:60.66ms
step:1853/2285 train_time:112401ms step_avg:60.66ms
step:1854/2285 train_time:112461ms step_avg:60.66ms
step:1855/2285 train_time:112523ms step_avg:60.66ms
step:1856/2285 train_time:112583ms step_avg:60.66ms
step:1857/2285 train_time:112644ms step_avg:60.66ms
step:1858/2285 train_time:112704ms step_avg:60.66ms
step:1859/2285 train_time:112767ms step_avg:60.66ms
step:1860/2285 train_time:112827ms step_avg:60.66ms
step:1861/2285 train_time:112890ms step_avg:60.66ms
step:1862/2285 train_time:112949ms step_avg:60.66ms
step:1863/2285 train_time:113011ms step_avg:60.66ms
step:1864/2285 train_time:113072ms step_avg:60.66ms
step:1865/2285 train_time:113134ms step_avg:60.66ms
step:1866/2285 train_time:113194ms step_avg:60.66ms
step:1867/2285 train_time:113257ms step_avg:60.66ms
step:1868/2285 train_time:113318ms step_avg:60.66ms
step:1869/2285 train_time:113380ms step_avg:60.66ms
step:1870/2285 train_time:113439ms step_avg:60.66ms
step:1871/2285 train_time:113501ms step_avg:60.66ms
step:1872/2285 train_time:113561ms step_avg:60.66ms
step:1873/2285 train_time:113623ms step_avg:60.66ms
step:1874/2285 train_time:113683ms step_avg:60.66ms
step:1875/2285 train_time:113745ms step_avg:60.66ms
step:1876/2285 train_time:113805ms step_avg:60.66ms
step:1877/2285 train_time:113868ms step_avg:60.66ms
step:1878/2285 train_time:113928ms step_avg:60.66ms
step:1879/2285 train_time:113990ms step_avg:60.67ms
step:1880/2285 train_time:114050ms step_avg:60.66ms
step:1881/2285 train_time:114112ms step_avg:60.67ms
step:1882/2285 train_time:114173ms step_avg:60.67ms
step:1883/2285 train_time:114236ms step_avg:60.67ms
step:1884/2285 train_time:114296ms step_avg:60.67ms
step:1885/2285 train_time:114358ms step_avg:60.67ms
step:1886/2285 train_time:114418ms step_avg:60.67ms
step:1887/2285 train_time:114481ms step_avg:60.67ms
step:1888/2285 train_time:114540ms step_avg:60.67ms
step:1889/2285 train_time:114602ms step_avg:60.67ms
step:1890/2285 train_time:114662ms step_avg:60.67ms
step:1891/2285 train_time:114725ms step_avg:60.67ms
step:1892/2285 train_time:114785ms step_avg:60.67ms
step:1893/2285 train_time:114847ms step_avg:60.67ms
step:1894/2285 train_time:114907ms step_avg:60.67ms
step:1895/2285 train_time:114970ms step_avg:60.67ms
step:1896/2285 train_time:115030ms step_avg:60.67ms
step:1897/2285 train_time:115092ms step_avg:60.67ms
step:1898/2285 train_time:115153ms step_avg:60.67ms
step:1899/2285 train_time:115216ms step_avg:60.67ms
step:1900/2285 train_time:115276ms step_avg:60.67ms
step:1901/2285 train_time:115338ms step_avg:60.67ms
step:1902/2285 train_time:115397ms step_avg:60.67ms
step:1903/2285 train_time:115461ms step_avg:60.67ms
step:1904/2285 train_time:115520ms step_avg:60.67ms
step:1905/2285 train_time:115583ms step_avg:60.67ms
step:1906/2285 train_time:115642ms step_avg:60.67ms
step:1907/2285 train_time:115705ms step_avg:60.67ms
step:1908/2285 train_time:115765ms step_avg:60.67ms
step:1909/2285 train_time:115827ms step_avg:60.67ms
step:1910/2285 train_time:115888ms step_avg:60.67ms
step:1911/2285 train_time:115950ms step_avg:60.68ms
step:1912/2285 train_time:116010ms step_avg:60.67ms
step:1913/2285 train_time:116073ms step_avg:60.68ms
step:1914/2285 train_time:116133ms step_avg:60.68ms
step:1915/2285 train_time:116196ms step_avg:60.68ms
step:1916/2285 train_time:116256ms step_avg:60.68ms
step:1917/2285 train_time:116318ms step_avg:60.68ms
step:1918/2285 train_time:116379ms step_avg:60.68ms
step:1919/2285 train_time:116441ms step_avg:60.68ms
step:1920/2285 train_time:116501ms step_avg:60.68ms
step:1921/2285 train_time:116564ms step_avg:60.68ms
step:1922/2285 train_time:116625ms step_avg:60.68ms
step:1923/2285 train_time:116687ms step_avg:60.68ms
step:1924/2285 train_time:116747ms step_avg:60.68ms
step:1925/2285 train_time:116809ms step_avg:60.68ms
step:1926/2285 train_time:116869ms step_avg:60.68ms
step:1927/2285 train_time:116932ms step_avg:60.68ms
step:1928/2285 train_time:116992ms step_avg:60.68ms
step:1929/2285 train_time:117054ms step_avg:60.68ms
step:1930/2285 train_time:117115ms step_avg:60.68ms
step:1931/2285 train_time:117177ms step_avg:60.68ms
step:1932/2285 train_time:117237ms step_avg:60.68ms
step:1933/2285 train_time:117300ms step_avg:60.68ms
step:1934/2285 train_time:117359ms step_avg:60.68ms
step:1935/2285 train_time:117421ms step_avg:60.68ms
step:1936/2285 train_time:117482ms step_avg:60.68ms
step:1937/2285 train_time:117544ms step_avg:60.68ms
step:1938/2285 train_time:117604ms step_avg:60.68ms
step:1939/2285 train_time:117666ms step_avg:60.68ms
step:1940/2285 train_time:117726ms step_avg:60.68ms
step:1941/2285 train_time:117789ms step_avg:60.68ms
step:1942/2285 train_time:117849ms step_avg:60.68ms
step:1943/2285 train_time:117911ms step_avg:60.69ms
step:1944/2285 train_time:117971ms step_avg:60.68ms
step:1945/2285 train_time:118033ms step_avg:60.69ms
step:1946/2285 train_time:118093ms step_avg:60.69ms
step:1947/2285 train_time:118156ms step_avg:60.69ms
step:1948/2285 train_time:118216ms step_avg:60.69ms
step:1949/2285 train_time:118279ms step_avg:60.69ms
step:1950/2285 train_time:118339ms step_avg:60.69ms
step:1951/2285 train_time:118401ms step_avg:60.69ms
step:1952/2285 train_time:118461ms step_avg:60.69ms
step:1953/2285 train_time:118523ms step_avg:60.69ms
step:1954/2285 train_time:118583ms step_avg:60.69ms
step:1955/2285 train_time:118645ms step_avg:60.69ms
step:1956/2285 train_time:118706ms step_avg:60.69ms
step:1957/2285 train_time:118769ms step_avg:60.69ms
step:1958/2285 train_time:118829ms step_avg:60.69ms
step:1959/2285 train_time:118891ms step_avg:60.69ms
step:1960/2285 train_time:118951ms step_avg:60.69ms
step:1961/2285 train_time:119013ms step_avg:60.69ms
step:1962/2285 train_time:119073ms step_avg:60.69ms
step:1963/2285 train_time:119136ms step_avg:60.69ms
step:1964/2285 train_time:119196ms step_avg:60.69ms
step:1965/2285 train_time:119258ms step_avg:60.69ms
step:1966/2285 train_time:119319ms step_avg:60.69ms
step:1967/2285 train_time:119381ms step_avg:60.69ms
step:1968/2285 train_time:119440ms step_avg:60.69ms
step:1969/2285 train_time:119503ms step_avg:60.69ms
step:1970/2285 train_time:119564ms step_avg:60.69ms
step:1971/2285 train_time:119627ms step_avg:60.69ms
step:1972/2285 train_time:119687ms step_avg:60.69ms
step:1973/2285 train_time:119749ms step_avg:60.69ms
step:1974/2285 train_time:119809ms step_avg:60.69ms
step:1975/2285 train_time:119871ms step_avg:60.69ms
step:1976/2285 train_time:119932ms step_avg:60.69ms
step:1977/2285 train_time:119994ms step_avg:60.69ms
step:1978/2285 train_time:120053ms step_avg:60.69ms
step:1979/2285 train_time:120116ms step_avg:60.70ms
step:1980/2285 train_time:120176ms step_avg:60.69ms
step:1981/2285 train_time:120238ms step_avg:60.70ms
step:1982/2285 train_time:120298ms step_avg:60.70ms
step:1983/2285 train_time:120361ms step_avg:60.70ms
step:1984/2285 train_time:120420ms step_avg:60.70ms
step:1985/2285 train_time:120483ms step_avg:60.70ms
step:1986/2285 train_time:120543ms step_avg:60.70ms
step:1987/2285 train_time:120605ms step_avg:60.70ms
step:1988/2285 train_time:120665ms step_avg:60.70ms
step:1989/2285 train_time:120728ms step_avg:60.70ms
step:1990/2285 train_time:120788ms step_avg:60.70ms
step:1991/2285 train_time:120850ms step_avg:60.70ms
step:1992/2285 train_time:120911ms step_avg:60.70ms
step:1993/2285 train_time:120973ms step_avg:60.70ms
step:1994/2285 train_time:121033ms step_avg:60.70ms
step:1995/2285 train_time:121096ms step_avg:60.70ms
step:1996/2285 train_time:121156ms step_avg:60.70ms
step:1997/2285 train_time:121219ms step_avg:60.70ms
step:1998/2285 train_time:121279ms step_avg:60.70ms
step:1999/2285 train_time:121341ms step_avg:60.70ms
step:2000/2285 train_time:121401ms step_avg:60.70ms
step:2000/2285 val_loss:3.3166 train_time:121465ms step_avg:60.73ms
step:2001/2285 train_time:121483ms step_avg:60.71ms
step:2002/2285 train_time:121526ms step_avg:60.70ms
step:2003/2285 train_time:121588ms step_avg:60.70ms
step:2004/2285 train_time:121648ms step_avg:60.70ms
step:2005/2285 train_time:121712ms step_avg:60.70ms
step:2006/2285 train_time:121771ms step_avg:60.70ms
step:2007/2285 train_time:121833ms step_avg:60.70ms
step:2008/2285 train_time:121892ms step_avg:60.70ms
step:2009/2285 train_time:121954ms step_avg:60.70ms
step:2010/2285 train_time:122013ms step_avg:60.70ms
step:2011/2285 train_time:122074ms step_avg:60.70ms
step:2012/2285 train_time:122133ms step_avg:60.70ms
step:2013/2285 train_time:122195ms step_avg:60.70ms
step:2014/2285 train_time:122255ms step_avg:60.70ms
step:2015/2285 train_time:122316ms step_avg:60.70ms
step:2016/2285 train_time:122379ms step_avg:60.70ms
step:2017/2285 train_time:122447ms step_avg:60.71ms
step:2018/2285 train_time:122508ms step_avg:60.71ms
step:2019/2285 train_time:122571ms step_avg:60.71ms
step:2020/2285 train_time:122631ms step_avg:60.71ms
step:2021/2285 train_time:122694ms step_avg:60.71ms
step:2022/2285 train_time:122754ms step_avg:60.71ms
step:2023/2285 train_time:122816ms step_avg:60.71ms
step:2024/2285 train_time:122876ms step_avg:60.71ms
step:2025/2285 train_time:122937ms step_avg:60.71ms
step:2026/2285 train_time:122998ms step_avg:60.71ms
step:2027/2285 train_time:123059ms step_avg:60.71ms
step:2028/2285 train_time:123118ms step_avg:60.71ms
step:2029/2285 train_time:123180ms step_avg:60.71ms
step:2030/2285 train_time:123239ms step_avg:60.71ms
step:2031/2285 train_time:123302ms step_avg:60.71ms
step:2032/2285 train_time:123363ms step_avg:60.71ms
step:2033/2285 train_time:123426ms step_avg:60.71ms
step:2034/2285 train_time:123487ms step_avg:60.71ms
step:2035/2285 train_time:123549ms step_avg:60.71ms
step:2036/2285 train_time:123610ms step_avg:60.71ms
step:2037/2285 train_time:123672ms step_avg:60.71ms
step:2038/2285 train_time:123733ms step_avg:60.71ms
step:2039/2285 train_time:123796ms step_avg:60.71ms
step:2040/2285 train_time:123856ms step_avg:60.71ms
step:2041/2285 train_time:123918ms step_avg:60.71ms
step:2042/2285 train_time:123978ms step_avg:60.71ms
step:2043/2285 train_time:124039ms step_avg:60.71ms
step:2044/2285 train_time:124099ms step_avg:60.71ms
step:2045/2285 train_time:124161ms step_avg:60.71ms
step:2046/2285 train_time:124221ms step_avg:60.71ms
step:2047/2285 train_time:124284ms step_avg:60.72ms
step:2048/2285 train_time:124344ms step_avg:60.72ms
step:2049/2285 train_time:124408ms step_avg:60.72ms
step:2050/2285 train_time:124468ms step_avg:60.72ms
step:2051/2285 train_time:124531ms step_avg:60.72ms
step:2052/2285 train_time:124591ms step_avg:60.72ms
step:2053/2285 train_time:124653ms step_avg:60.72ms
step:2054/2285 train_time:124714ms step_avg:60.72ms
step:2055/2285 train_time:124776ms step_avg:60.72ms
step:2056/2285 train_time:124836ms step_avg:60.72ms
step:2057/2285 train_time:124898ms step_avg:60.72ms
step:2058/2285 train_time:124958ms step_avg:60.72ms
step:2059/2285 train_time:125020ms step_avg:60.72ms
step:2060/2285 train_time:125079ms step_avg:60.72ms
step:2061/2285 train_time:125142ms step_avg:60.72ms
step:2062/2285 train_time:125202ms step_avg:60.72ms
step:2063/2285 train_time:125264ms step_avg:60.72ms
step:2064/2285 train_time:125324ms step_avg:60.72ms
step:2065/2285 train_time:125386ms step_avg:60.72ms
step:2066/2285 train_time:125447ms step_avg:60.72ms
step:2067/2285 train_time:125509ms step_avg:60.72ms
step:2068/2285 train_time:125570ms step_avg:60.72ms
step:2069/2285 train_time:125633ms step_avg:60.72ms
step:2070/2285 train_time:125693ms step_avg:60.72ms
step:2071/2285 train_time:125755ms step_avg:60.72ms
step:2072/2285 train_time:125815ms step_avg:60.72ms
step:2073/2285 train_time:125878ms step_avg:60.72ms
step:2074/2285 train_time:125938ms step_avg:60.72ms
step:2075/2285 train_time:126001ms step_avg:60.72ms
step:2076/2285 train_time:126061ms step_avg:60.72ms
step:2077/2285 train_time:126123ms step_avg:60.72ms
step:2078/2285 train_time:126183ms step_avg:60.72ms
step:2079/2285 train_time:126245ms step_avg:60.72ms
step:2080/2285 train_time:126304ms step_avg:60.72ms
step:2081/2285 train_time:126366ms step_avg:60.72ms
step:2082/2285 train_time:126427ms step_avg:60.72ms
step:2083/2285 train_time:126489ms step_avg:60.72ms
step:2084/2285 train_time:126549ms step_avg:60.72ms
step:2085/2285 train_time:126612ms step_avg:60.73ms
step:2086/2285 train_time:126673ms step_avg:60.73ms
step:2087/2285 train_time:126735ms step_avg:60.73ms
step:2088/2285 train_time:126796ms step_avg:60.73ms
step:2089/2285 train_time:126857ms step_avg:60.73ms
step:2090/2285 train_time:126918ms step_avg:60.73ms
step:2091/2285 train_time:126980ms step_avg:60.73ms
step:2092/2285 train_time:127040ms step_avg:60.73ms
step:2093/2285 train_time:127103ms step_avg:60.73ms
step:2094/2285 train_time:127163ms step_avg:60.73ms
step:2095/2285 train_time:127224ms step_avg:60.73ms
step:2096/2285 train_time:127284ms step_avg:60.73ms
step:2097/2285 train_time:127346ms step_avg:60.73ms
step:2098/2285 train_time:127406ms step_avg:60.73ms
step:2099/2285 train_time:127468ms step_avg:60.73ms
step:2100/2285 train_time:127529ms step_avg:60.73ms
step:2101/2285 train_time:127591ms step_avg:60.73ms
step:2102/2285 train_time:127652ms step_avg:60.73ms
step:2103/2285 train_time:127715ms step_avg:60.73ms
step:2104/2285 train_time:127775ms step_avg:60.73ms
step:2105/2285 train_time:127837ms step_avg:60.73ms
step:2106/2285 train_time:127898ms step_avg:60.73ms
step:2107/2285 train_time:127960ms step_avg:60.73ms
step:2108/2285 train_time:128020ms step_avg:60.73ms
step:2109/2285 train_time:128082ms step_avg:60.73ms
step:2110/2285 train_time:128142ms step_avg:60.73ms
step:2111/2285 train_time:128205ms step_avg:60.73ms
step:2112/2285 train_time:128265ms step_avg:60.73ms
step:2113/2285 train_time:128327ms step_avg:60.73ms
step:2114/2285 train_time:128387ms step_avg:60.73ms
step:2115/2285 train_time:128450ms step_avg:60.73ms
step:2116/2285 train_time:128510ms step_avg:60.73ms
step:2117/2285 train_time:128572ms step_avg:60.73ms
step:2118/2285 train_time:128632ms step_avg:60.73ms
step:2119/2285 train_time:128695ms step_avg:60.73ms
step:2120/2285 train_time:128755ms step_avg:60.73ms
step:2121/2285 train_time:128818ms step_avg:60.73ms
step:2122/2285 train_time:128878ms step_avg:60.73ms
step:2123/2285 train_time:128940ms step_avg:60.73ms
step:2124/2285 train_time:129000ms step_avg:60.73ms
step:2125/2285 train_time:129063ms step_avg:60.74ms
step:2126/2285 train_time:129123ms step_avg:60.74ms
step:2127/2285 train_time:129185ms step_avg:60.74ms
step:2128/2285 train_time:129245ms step_avg:60.74ms
step:2129/2285 train_time:129307ms step_avg:60.74ms
step:2130/2285 train_time:129367ms step_avg:60.74ms
step:2131/2285 train_time:129429ms step_avg:60.74ms
step:2132/2285 train_time:129490ms step_avg:60.74ms
step:2133/2285 train_time:129552ms step_avg:60.74ms
step:2134/2285 train_time:129612ms step_avg:60.74ms
step:2135/2285 train_time:129675ms step_avg:60.74ms
step:2136/2285 train_time:129736ms step_avg:60.74ms
step:2137/2285 train_time:129799ms step_avg:60.74ms
step:2138/2285 train_time:129859ms step_avg:60.74ms
step:2139/2285 train_time:129921ms step_avg:60.74ms
step:2140/2285 train_time:129981ms step_avg:60.74ms
step:2141/2285 train_time:130044ms step_avg:60.74ms
step:2142/2285 train_time:130104ms step_avg:60.74ms
step:2143/2285 train_time:130166ms step_avg:60.74ms
step:2144/2285 train_time:130226ms step_avg:60.74ms
step:2145/2285 train_time:130288ms step_avg:60.74ms
step:2146/2285 train_time:130348ms step_avg:60.74ms
step:2147/2285 train_time:130411ms step_avg:60.74ms
step:2148/2285 train_time:130471ms step_avg:60.74ms
step:2149/2285 train_time:130533ms step_avg:60.74ms
step:2150/2285 train_time:130593ms step_avg:60.74ms
step:2151/2285 train_time:130656ms step_avg:60.74ms
step:2152/2285 train_time:130716ms step_avg:60.74ms
step:2153/2285 train_time:130778ms step_avg:60.74ms
step:2154/2285 train_time:130838ms step_avg:60.74ms
step:2155/2285 train_time:130901ms step_avg:60.74ms
step:2156/2285 train_time:130961ms step_avg:60.74ms
step:2157/2285 train_time:131024ms step_avg:60.74ms
step:2158/2285 train_time:131084ms step_avg:60.74ms
step:2159/2285 train_time:131147ms step_avg:60.74ms
step:2160/2285 train_time:131207ms step_avg:60.74ms
step:2161/2285 train_time:131269ms step_avg:60.74ms
step:2162/2285 train_time:131329ms step_avg:60.74ms
step:2163/2285 train_time:131392ms step_avg:60.75ms
step:2164/2285 train_time:131452ms step_avg:60.74ms
step:2165/2285 train_time:131514ms step_avg:60.75ms
step:2166/2285 train_time:131574ms step_avg:60.75ms
step:2167/2285 train_time:131636ms step_avg:60.75ms
step:2168/2285 train_time:131697ms step_avg:60.75ms
step:2169/2285 train_time:131759ms step_avg:60.75ms
step:2170/2285 train_time:131819ms step_avg:60.75ms
step:2171/2285 train_time:131882ms step_avg:60.75ms
step:2172/2285 train_time:131942ms step_avg:60.75ms
step:2173/2285 train_time:132004ms step_avg:60.75ms
step:2174/2285 train_time:132064ms step_avg:60.75ms
step:2175/2285 train_time:132127ms step_avg:60.75ms
step:2176/2285 train_time:132187ms step_avg:60.75ms
step:2177/2285 train_time:132250ms step_avg:60.75ms
step:2178/2285 train_time:132310ms step_avg:60.75ms
step:2179/2285 train_time:132372ms step_avg:60.75ms
step:2180/2285 train_time:132432ms step_avg:60.75ms
step:2181/2285 train_time:132495ms step_avg:60.75ms
step:2182/2285 train_time:132554ms step_avg:60.75ms
step:2183/2285 train_time:132617ms step_avg:60.75ms
step:2184/2285 train_time:132677ms step_avg:60.75ms
step:2185/2285 train_time:132739ms step_avg:60.75ms
step:2186/2285 train_time:132799ms step_avg:60.75ms
step:2187/2285 train_time:132861ms step_avg:60.75ms
step:2188/2285 train_time:132922ms step_avg:60.75ms
step:2189/2285 train_time:132984ms step_avg:60.75ms
step:2190/2285 train_time:133044ms step_avg:60.75ms
step:2191/2285 train_time:133106ms step_avg:60.75ms
step:2192/2285 train_time:133166ms step_avg:60.75ms
step:2193/2285 train_time:133229ms step_avg:60.75ms
step:2194/2285 train_time:133289ms step_avg:60.75ms
step:2195/2285 train_time:133351ms step_avg:60.75ms
step:2196/2285 train_time:133411ms step_avg:60.75ms
step:2197/2285 train_time:133473ms step_avg:60.75ms
step:2198/2285 train_time:133533ms step_avg:60.75ms
step:2199/2285 train_time:133597ms step_avg:60.75ms
step:2200/2285 train_time:133657ms step_avg:60.75ms
step:2201/2285 train_time:133719ms step_avg:60.75ms
step:2202/2285 train_time:133779ms step_avg:60.75ms
step:2203/2285 train_time:133842ms step_avg:60.75ms
step:2204/2285 train_time:133902ms step_avg:60.75ms
step:2205/2285 train_time:133965ms step_avg:60.75ms
step:2206/2285 train_time:134025ms step_avg:60.75ms
step:2207/2285 train_time:134087ms step_avg:60.76ms
step:2208/2285 train_time:134148ms step_avg:60.76ms
step:2209/2285 train_time:134210ms step_avg:60.76ms
step:2210/2285 train_time:134270ms step_avg:60.76ms
step:2211/2285 train_time:134333ms step_avg:60.76ms
step:2212/2285 train_time:134393ms step_avg:60.76ms
step:2213/2285 train_time:134455ms step_avg:60.76ms
step:2214/2285 train_time:134515ms step_avg:60.76ms
step:2215/2285 train_time:134578ms step_avg:60.76ms
step:2216/2285 train_time:134638ms step_avg:60.76ms
step:2217/2285 train_time:134701ms step_avg:60.76ms
step:2218/2285 train_time:134760ms step_avg:60.76ms
step:2219/2285 train_time:134823ms step_avg:60.76ms
step:2220/2285 train_time:134883ms step_avg:60.76ms
step:2221/2285 train_time:134945ms step_avg:60.76ms
step:2222/2285 train_time:135005ms step_avg:60.76ms
step:2223/2285 train_time:135068ms step_avg:60.76ms
step:2224/2285 train_time:135128ms step_avg:60.76ms
step:2225/2285 train_time:135190ms step_avg:60.76ms
step:2226/2285 train_time:135250ms step_avg:60.76ms
step:2227/2285 train_time:135313ms step_avg:60.76ms
step:2228/2285 train_time:135373ms step_avg:60.76ms
step:2229/2285 train_time:135435ms step_avg:60.76ms
step:2230/2285 train_time:135495ms step_avg:60.76ms
step:2231/2285 train_time:135557ms step_avg:60.76ms
step:2232/2285 train_time:135617ms step_avg:60.76ms
step:2233/2285 train_time:135679ms step_avg:60.76ms
step:2234/2285 train_time:135739ms step_avg:60.76ms
step:2235/2285 train_time:135801ms step_avg:60.76ms
step:2236/2285 train_time:135861ms step_avg:60.76ms
step:2237/2285 train_time:135924ms step_avg:60.76ms
step:2238/2285 train_time:135984ms step_avg:60.76ms
step:2239/2285 train_time:136046ms step_avg:60.76ms
step:2240/2285 train_time:136106ms step_avg:60.76ms
step:2241/2285 train_time:136168ms step_avg:60.76ms
step:2242/2285 train_time:136229ms step_avg:60.76ms
step:2243/2285 train_time:136293ms step_avg:60.76ms
step:2244/2285 train_time:136353ms step_avg:60.76ms
step:2245/2285 train_time:136416ms step_avg:60.76ms
step:2246/2285 train_time:136476ms step_avg:60.76ms
step:2247/2285 train_time:136538ms step_avg:60.76ms
step:2248/2285 train_time:136598ms step_avg:60.76ms
step:2249/2285 train_time:136660ms step_avg:60.76ms
step:2250/2285 train_time:136720ms step_avg:60.76ms
step:2250/2285 val_loss:3.2816 train_time:136784ms step_avg:60.79ms
step:2251/2285 train_time:136802ms step_avg:60.77ms
step:2252/2285 train_time:136847ms step_avg:60.77ms
step:2253/2285 train_time:136910ms step_avg:60.77ms
step:2254/2285 train_time:136971ms step_avg:60.77ms
step:2255/2285 train_time:137034ms step_avg:60.77ms
step:2256/2285 train_time:137094ms step_avg:60.77ms
step:2257/2285 train_time:137156ms step_avg:60.77ms
step:2258/2285 train_time:137217ms step_avg:60.77ms
step:2259/2285 train_time:137279ms step_avg:60.77ms
step:2260/2285 train_time:137339ms step_avg:60.77ms
step:2261/2285 train_time:137402ms step_avg:60.77ms
step:2262/2285 train_time:137461ms step_avg:60.77ms
step:2263/2285 train_time:137523ms step_avg:60.77ms
step:2264/2285 train_time:137584ms step_avg:60.77ms
step:2265/2285 train_time:137646ms step_avg:60.77ms
step:2266/2285 train_time:137706ms step_avg:60.77ms
step:2267/2285 train_time:137771ms step_avg:60.77ms
step:2268/2285 train_time:137832ms step_avg:60.77ms
step:2269/2285 train_time:137895ms step_avg:60.77ms
step:2270/2285 train_time:137956ms step_avg:60.77ms
step:2271/2285 train_time:138018ms step_avg:60.77ms
step:2272/2285 train_time:138078ms step_avg:60.77ms
step:2273/2285 train_time:138141ms step_avg:60.77ms
step:2274/2285 train_time:138202ms step_avg:60.77ms
step:2275/2285 train_time:138264ms step_avg:60.78ms
step:2276/2285 train_time:138323ms step_avg:60.77ms
step:2277/2285 train_time:138385ms step_avg:60.78ms
step:2278/2285 train_time:138445ms step_avg:60.77ms
step:2279/2285 train_time:138507ms step_avg:60.78ms
step:2280/2285 train_time:138567ms step_avg:60.77ms
step:2281/2285 train_time:138629ms step_avg:60.78ms
step:2282/2285 train_time:138690ms step_avg:60.78ms
step:2283/2285 train_time:138753ms step_avg:60.78ms
step:2284/2285 train_time:138813ms step_avg:60.78ms
step:2285/2285 train_time:138876ms step_avg:60.78ms
step:2285/2285 val_loss:3.2757 train_time:138937ms step_avg:60.80ms
peak memory allocated: 29626 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2285
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 02:13:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:120ms step_avg:119.52ms
step:2/2285 train_time:140ms step_avg:70.10ms
step:3/2285 train_time:179ms step_avg:59.70ms
step:4/2285 train_time:235ms step_avg:58.79ms
step:5/2285 train_time:294ms step_avg:58.87ms
step:6/2285 train_time:353ms step_avg:58.79ms
step:7/2285 train_time:414ms step_avg:59.12ms
step:8/2285 train_time:472ms step_avg:59.00ms
step:9/2285 train_time:532ms step_avg:59.16ms
step:10/2285 train_time:591ms step_avg:59.06ms
step:11/2285 train_time:651ms step_avg:59.21ms
step:12/2285 train_time:710ms step_avg:59.14ms
step:13/2285 train_time:770ms step_avg:59.22ms
step:14/2285 train_time:828ms step_avg:59.16ms
step:15/2285 train_time:889ms step_avg:59.24ms
step:16/2285 train_time:947ms step_avg:59.19ms
step:17/2285 train_time:1009ms step_avg:59.33ms
step:18/2285 train_time:1071ms step_avg:59.49ms
step:19/2285 train_time:1137ms step_avg:59.82ms
step:20/2285 train_time:1198ms step_avg:59.89ms
step:21/2285 train_time:1259ms step_avg:59.96ms
step:22/2285 train_time:1318ms step_avg:59.91ms
step:23/2285 train_time:1379ms step_avg:59.97ms
step:24/2285 train_time:1438ms step_avg:59.91ms
step:25/2285 train_time:1499ms step_avg:59.95ms
step:26/2285 train_time:1558ms step_avg:59.92ms
step:27/2285 train_time:1619ms step_avg:59.95ms
step:28/2285 train_time:1678ms step_avg:59.91ms
step:29/2285 train_time:1739ms step_avg:59.95ms
step:30/2285 train_time:1797ms step_avg:59.91ms
step:31/2285 train_time:1858ms step_avg:59.95ms
step:32/2285 train_time:1917ms step_avg:59.90ms
step:33/2285 train_time:1978ms step_avg:59.94ms
step:34/2285 train_time:2038ms step_avg:59.93ms
step:35/2285 train_time:2101ms step_avg:60.03ms
step:36/2285 train_time:2162ms step_avg:60.04ms
step:37/2285 train_time:2223ms step_avg:60.09ms
step:38/2285 train_time:2283ms step_avg:60.08ms
step:39/2285 train_time:2345ms step_avg:60.12ms
step:40/2285 train_time:2403ms step_avg:60.08ms
step:41/2285 train_time:2465ms step_avg:60.13ms
step:42/2285 train_time:2524ms step_avg:60.10ms
step:43/2285 train_time:2586ms step_avg:60.13ms
step:44/2285 train_time:2645ms step_avg:60.11ms
step:45/2285 train_time:2706ms step_avg:60.13ms
step:46/2285 train_time:2765ms step_avg:60.11ms
step:47/2285 train_time:2826ms step_avg:60.14ms
step:48/2285 train_time:2886ms step_avg:60.12ms
step:49/2285 train_time:2948ms step_avg:60.16ms
step:50/2285 train_time:3007ms step_avg:60.14ms
step:51/2285 train_time:3070ms step_avg:60.20ms
step:52/2285 train_time:3130ms step_avg:60.19ms
step:53/2285 train_time:3191ms step_avg:60.21ms
step:54/2285 train_time:3250ms step_avg:60.19ms
step:55/2285 train_time:3312ms step_avg:60.21ms
step:56/2285 train_time:3371ms step_avg:60.19ms
step:57/2285 train_time:3432ms step_avg:60.21ms
step:58/2285 train_time:3491ms step_avg:60.19ms
step:59/2285 train_time:3553ms step_avg:60.23ms
step:60/2285 train_time:3612ms step_avg:60.20ms
step:61/2285 train_time:3673ms step_avg:60.21ms
step:62/2285 train_time:3731ms step_avg:60.18ms
step:63/2285 train_time:3792ms step_avg:60.20ms
step:64/2285 train_time:3852ms step_avg:60.18ms
step:65/2285 train_time:3913ms step_avg:60.20ms
step:66/2285 train_time:3972ms step_avg:60.18ms
step:67/2285 train_time:4033ms step_avg:60.19ms
step:68/2285 train_time:4092ms step_avg:60.18ms
step:69/2285 train_time:4154ms step_avg:60.20ms
step:70/2285 train_time:4213ms step_avg:60.18ms
step:71/2285 train_time:4274ms step_avg:60.20ms
step:72/2285 train_time:4332ms step_avg:60.17ms
step:73/2285 train_time:4394ms step_avg:60.19ms
step:74/2285 train_time:4454ms step_avg:60.19ms
step:75/2285 train_time:4514ms step_avg:60.18ms
step:76/2285 train_time:4572ms step_avg:60.16ms
step:77/2285 train_time:4634ms step_avg:60.18ms
step:78/2285 train_time:4693ms step_avg:60.17ms
step:79/2285 train_time:4755ms step_avg:60.19ms
step:80/2285 train_time:4814ms step_avg:60.17ms
step:81/2285 train_time:4876ms step_avg:60.20ms
step:82/2285 train_time:4935ms step_avg:60.18ms
step:83/2285 train_time:4996ms step_avg:60.19ms
step:84/2285 train_time:5055ms step_avg:60.18ms
step:85/2285 train_time:5116ms step_avg:60.19ms
step:86/2285 train_time:5174ms step_avg:60.17ms
step:87/2285 train_time:5236ms step_avg:60.18ms
step:88/2285 train_time:5294ms step_avg:60.16ms
step:89/2285 train_time:5355ms step_avg:60.17ms
step:90/2285 train_time:5414ms step_avg:60.16ms
step:91/2285 train_time:5475ms step_avg:60.16ms
step:92/2285 train_time:5533ms step_avg:60.15ms
step:93/2285 train_time:5595ms step_avg:60.16ms
step:94/2285 train_time:5653ms step_avg:60.14ms
step:95/2285 train_time:5715ms step_avg:60.16ms
step:96/2285 train_time:5774ms step_avg:60.14ms
step:97/2285 train_time:5835ms step_avg:60.15ms
step:98/2285 train_time:5894ms step_avg:60.14ms
step:99/2285 train_time:5955ms step_avg:60.15ms
step:100/2285 train_time:6014ms step_avg:60.14ms
step:101/2285 train_time:6074ms step_avg:60.14ms
step:102/2285 train_time:6133ms step_avg:60.13ms
step:103/2285 train_time:6194ms step_avg:60.14ms
step:104/2285 train_time:6253ms step_avg:60.12ms
step:105/2285 train_time:6314ms step_avg:60.14ms
step:106/2285 train_time:6373ms step_avg:60.12ms
step:107/2285 train_time:6434ms step_avg:60.13ms
step:108/2285 train_time:6492ms step_avg:60.11ms
step:109/2285 train_time:6553ms step_avg:60.12ms
step:110/2285 train_time:6611ms step_avg:60.10ms
step:111/2285 train_time:6673ms step_avg:60.12ms
step:112/2285 train_time:6731ms step_avg:60.10ms
step:113/2285 train_time:6793ms step_avg:60.12ms
step:114/2285 train_time:6852ms step_avg:60.11ms
step:115/2285 train_time:6913ms step_avg:60.11ms
step:116/2285 train_time:6971ms step_avg:60.10ms
step:117/2285 train_time:7032ms step_avg:60.11ms
step:118/2285 train_time:7091ms step_avg:60.09ms
step:119/2285 train_time:7152ms step_avg:60.10ms
step:120/2285 train_time:7210ms step_avg:60.09ms
step:121/2285 train_time:7272ms step_avg:60.10ms
step:122/2285 train_time:7331ms step_avg:60.09ms
step:123/2285 train_time:7392ms step_avg:60.10ms
step:124/2285 train_time:7451ms step_avg:60.09ms
step:125/2285 train_time:7512ms step_avg:60.09ms
step:126/2285 train_time:7570ms step_avg:60.08ms
step:127/2285 train_time:7631ms step_avg:60.08ms
step:128/2285 train_time:7690ms step_avg:60.08ms
step:129/2285 train_time:7751ms step_avg:60.08ms
step:130/2285 train_time:7809ms step_avg:60.07ms
step:131/2285 train_time:7870ms step_avg:60.08ms
step:132/2285 train_time:7929ms step_avg:60.07ms
step:133/2285 train_time:7990ms step_avg:60.07ms
step:134/2285 train_time:8048ms step_avg:60.06ms
step:135/2285 train_time:8109ms step_avg:60.07ms
step:136/2285 train_time:8168ms step_avg:60.06ms
step:137/2285 train_time:8229ms step_avg:60.07ms
step:138/2285 train_time:8288ms step_avg:60.06ms
step:139/2285 train_time:8349ms step_avg:60.06ms
step:140/2285 train_time:8407ms step_avg:60.05ms
step:141/2285 train_time:8468ms step_avg:60.06ms
step:142/2285 train_time:8527ms step_avg:60.05ms
step:143/2285 train_time:8588ms step_avg:60.06ms
step:144/2285 train_time:8647ms step_avg:60.05ms
step:145/2285 train_time:8708ms step_avg:60.06ms
step:146/2285 train_time:8767ms step_avg:60.05ms
step:147/2285 train_time:8827ms step_avg:60.05ms
step:148/2285 train_time:8886ms step_avg:60.04ms
step:149/2285 train_time:8948ms step_avg:60.05ms
step:150/2285 train_time:9007ms step_avg:60.04ms
step:151/2285 train_time:9068ms step_avg:60.06ms
step:152/2285 train_time:9127ms step_avg:60.04ms
step:153/2285 train_time:9188ms step_avg:60.06ms
step:154/2285 train_time:9247ms step_avg:60.05ms
step:155/2285 train_time:9308ms step_avg:60.05ms
step:156/2285 train_time:9367ms step_avg:60.04ms
step:157/2285 train_time:9428ms step_avg:60.05ms
step:158/2285 train_time:9486ms step_avg:60.04ms
step:159/2285 train_time:9548ms step_avg:60.05ms
step:160/2285 train_time:9607ms step_avg:60.04ms
step:161/2285 train_time:9669ms step_avg:60.05ms
step:162/2285 train_time:9727ms step_avg:60.04ms
step:163/2285 train_time:9788ms step_avg:60.05ms
step:164/2285 train_time:9847ms step_avg:60.04ms
step:165/2285 train_time:9908ms step_avg:60.05ms
step:166/2285 train_time:9966ms step_avg:60.04ms
step:167/2285 train_time:10027ms step_avg:60.04ms
step:168/2285 train_time:10086ms step_avg:60.04ms
step:169/2285 train_time:10148ms step_avg:60.05ms
step:170/2285 train_time:10206ms step_avg:60.04ms
step:171/2285 train_time:10268ms step_avg:60.05ms
step:172/2285 train_time:10327ms step_avg:60.04ms
step:173/2285 train_time:10388ms step_avg:60.04ms
step:174/2285 train_time:10447ms step_avg:60.04ms
step:175/2285 train_time:10508ms step_avg:60.05ms
step:176/2285 train_time:10567ms step_avg:60.04ms
step:177/2285 train_time:10627ms step_avg:60.04ms
step:178/2285 train_time:10686ms step_avg:60.03ms
step:179/2285 train_time:10748ms step_avg:60.04ms
step:180/2285 train_time:10807ms step_avg:60.04ms
step:181/2285 train_time:10867ms step_avg:60.04ms
step:182/2285 train_time:10926ms step_avg:60.03ms
step:183/2285 train_time:10987ms step_avg:60.04ms
step:184/2285 train_time:11046ms step_avg:60.03ms
step:185/2285 train_time:11107ms step_avg:60.04ms
step:186/2285 train_time:11166ms step_avg:60.03ms
step:187/2285 train_time:11227ms step_avg:60.04ms
step:188/2285 train_time:11285ms step_avg:60.03ms
step:189/2285 train_time:11346ms step_avg:60.03ms
step:190/2285 train_time:11405ms step_avg:60.03ms
step:191/2285 train_time:11467ms step_avg:60.04ms
step:192/2285 train_time:11525ms step_avg:60.03ms
step:193/2285 train_time:11586ms step_avg:60.03ms
step:194/2285 train_time:11645ms step_avg:60.03ms
step:195/2285 train_time:11706ms step_avg:60.03ms
step:196/2285 train_time:11765ms step_avg:60.03ms
step:197/2285 train_time:11826ms step_avg:60.03ms
step:198/2285 train_time:11885ms step_avg:60.03ms
step:199/2285 train_time:11946ms step_avg:60.03ms
step:200/2285 train_time:12005ms step_avg:60.03ms
step:201/2285 train_time:12067ms step_avg:60.04ms
step:202/2285 train_time:12126ms step_avg:60.03ms
step:203/2285 train_time:12187ms step_avg:60.04ms
step:204/2285 train_time:12246ms step_avg:60.03ms
step:205/2285 train_time:12307ms step_avg:60.04ms
step:206/2285 train_time:12366ms step_avg:60.03ms
step:207/2285 train_time:12427ms step_avg:60.04ms
step:208/2285 train_time:12486ms step_avg:60.03ms
step:209/2285 train_time:12548ms step_avg:60.04ms
step:210/2285 train_time:12607ms step_avg:60.03ms
step:211/2285 train_time:12668ms step_avg:60.04ms
step:212/2285 train_time:12727ms step_avg:60.03ms
step:213/2285 train_time:12788ms step_avg:60.04ms
step:214/2285 train_time:12847ms step_avg:60.03ms
step:215/2285 train_time:12908ms step_avg:60.04ms
step:216/2285 train_time:12966ms step_avg:60.03ms
step:217/2285 train_time:13028ms step_avg:60.04ms
step:218/2285 train_time:13087ms step_avg:60.03ms
step:219/2285 train_time:13149ms step_avg:60.04ms
step:220/2285 train_time:13207ms step_avg:60.03ms
step:221/2285 train_time:13268ms step_avg:60.04ms
step:222/2285 train_time:13327ms step_avg:60.03ms
step:223/2285 train_time:13388ms step_avg:60.03ms
step:224/2285 train_time:13447ms step_avg:60.03ms
step:225/2285 train_time:13508ms step_avg:60.04ms
step:226/2285 train_time:13567ms step_avg:60.03ms
step:227/2285 train_time:13628ms step_avg:60.03ms
step:228/2285 train_time:13686ms step_avg:60.03ms
step:229/2285 train_time:13747ms step_avg:60.03ms
step:230/2285 train_time:13806ms step_avg:60.03ms
step:231/2285 train_time:13868ms step_avg:60.03ms
step:232/2285 train_time:13926ms step_avg:60.03ms
step:233/2285 train_time:13987ms step_avg:60.03ms
step:234/2285 train_time:14046ms step_avg:60.02ms
step:235/2285 train_time:14107ms step_avg:60.03ms
step:236/2285 train_time:14166ms step_avg:60.02ms
step:237/2285 train_time:14227ms step_avg:60.03ms
step:238/2285 train_time:14286ms step_avg:60.02ms
step:239/2285 train_time:14348ms step_avg:60.03ms
step:240/2285 train_time:14406ms step_avg:60.03ms
step:241/2285 train_time:14468ms step_avg:60.03ms
step:242/2285 train_time:14526ms step_avg:60.03ms
step:243/2285 train_time:14588ms step_avg:60.03ms
step:244/2285 train_time:14647ms step_avg:60.03ms
step:245/2285 train_time:14707ms step_avg:60.03ms
step:246/2285 train_time:14766ms step_avg:60.02ms
step:247/2285 train_time:14828ms step_avg:60.03ms
step:248/2285 train_time:14886ms step_avg:60.03ms
step:249/2285 train_time:14948ms step_avg:60.03ms
step:250/2285 train_time:15007ms step_avg:60.03ms
step:250/2285 val_loss:4.0735 train_time:15069ms step_avg:60.28ms
step:251/2285 train_time:15089ms step_avg:60.11ms
step:252/2285 train_time:15129ms step_avg:60.04ms
step:253/2285 train_time:15194ms step_avg:60.05ms
step:254/2285 train_time:15257ms step_avg:60.07ms
step:255/2285 train_time:15319ms step_avg:60.07ms
step:256/2285 train_time:15378ms step_avg:60.07ms
step:257/2285 train_time:15438ms step_avg:60.07ms
step:258/2285 train_time:15497ms step_avg:60.06ms
step:259/2285 train_time:15556ms step_avg:60.06ms
step:260/2285 train_time:15614ms step_avg:60.05ms
step:261/2285 train_time:15675ms step_avg:60.06ms
step:262/2285 train_time:15732ms step_avg:60.05ms
step:263/2285 train_time:15792ms step_avg:60.05ms
step:264/2285 train_time:15850ms step_avg:60.04ms
step:265/2285 train_time:15910ms step_avg:60.04ms
step:266/2285 train_time:15968ms step_avg:60.03ms
step:267/2285 train_time:16029ms step_avg:60.03ms
step:268/2285 train_time:16088ms step_avg:60.03ms
step:269/2285 train_time:16152ms step_avg:60.04ms
step:270/2285 train_time:16213ms step_avg:60.05ms
step:271/2285 train_time:16276ms step_avg:60.06ms
step:272/2285 train_time:16335ms step_avg:60.06ms
step:273/2285 train_time:16397ms step_avg:60.06ms
step:274/2285 train_time:16455ms step_avg:60.05ms
step:275/2285 train_time:16516ms step_avg:60.06ms
step:276/2285 train_time:16574ms step_avg:60.05ms
step:277/2285 train_time:16634ms step_avg:60.05ms
step:278/2285 train_time:16692ms step_avg:60.04ms
step:279/2285 train_time:16752ms step_avg:60.04ms
step:280/2285 train_time:16810ms step_avg:60.04ms
step:281/2285 train_time:16871ms step_avg:60.04ms
step:282/2285 train_time:16929ms step_avg:60.03ms
step:283/2285 train_time:16989ms step_avg:60.03ms
step:284/2285 train_time:17048ms step_avg:60.03ms
step:285/2285 train_time:17110ms step_avg:60.03ms
step:286/2285 train_time:17169ms step_avg:60.03ms
step:287/2285 train_time:17232ms step_avg:60.04ms
step:288/2285 train_time:17292ms step_avg:60.04ms
step:289/2285 train_time:17353ms step_avg:60.05ms
step:290/2285 train_time:17413ms step_avg:60.04ms
step:291/2285 train_time:17474ms step_avg:60.05ms
step:292/2285 train_time:17532ms step_avg:60.04ms
step:293/2285 train_time:17593ms step_avg:60.04ms
step:294/2285 train_time:17651ms step_avg:60.04ms
step:295/2285 train_time:17712ms step_avg:60.04ms
step:296/2285 train_time:17769ms step_avg:60.03ms
step:297/2285 train_time:17830ms step_avg:60.03ms
step:298/2285 train_time:17888ms step_avg:60.03ms
step:299/2285 train_time:17949ms step_avg:60.03ms
step:300/2285 train_time:18008ms step_avg:60.03ms
step:301/2285 train_time:18069ms step_avg:60.03ms
step:302/2285 train_time:18128ms step_avg:60.03ms
step:303/2285 train_time:18189ms step_avg:60.03ms
step:304/2285 train_time:18249ms step_avg:60.03ms
step:305/2285 train_time:18310ms step_avg:60.03ms
step:306/2285 train_time:18370ms step_avg:60.03ms
step:307/2285 train_time:18432ms step_avg:60.04ms
step:308/2285 train_time:18491ms step_avg:60.04ms
step:309/2285 train_time:18552ms step_avg:60.04ms
step:310/2285 train_time:18611ms step_avg:60.04ms
step:311/2285 train_time:18671ms step_avg:60.04ms
step:312/2285 train_time:18729ms step_avg:60.03ms
step:313/2285 train_time:18790ms step_avg:60.03ms
step:314/2285 train_time:18848ms step_avg:60.03ms
step:315/2285 train_time:18908ms step_avg:60.03ms
step:316/2285 train_time:18966ms step_avg:60.02ms
step:317/2285 train_time:19027ms step_avg:60.02ms
step:318/2285 train_time:19085ms step_avg:60.02ms
step:319/2285 train_time:19147ms step_avg:60.02ms
step:320/2285 train_time:19205ms step_avg:60.02ms
step:321/2285 train_time:19266ms step_avg:60.02ms
step:322/2285 train_time:19325ms step_avg:60.02ms
step:323/2285 train_time:19386ms step_avg:60.02ms
step:324/2285 train_time:19446ms step_avg:60.02ms
step:325/2285 train_time:19507ms step_avg:60.02ms
step:326/2285 train_time:19565ms step_avg:60.02ms
step:327/2285 train_time:19626ms step_avg:60.02ms
step:328/2285 train_time:19684ms step_avg:60.01ms
step:329/2285 train_time:19744ms step_avg:60.01ms
step:330/2285 train_time:19802ms step_avg:60.01ms
step:331/2285 train_time:19863ms step_avg:60.01ms
step:332/2285 train_time:19921ms step_avg:60.00ms
step:333/2285 train_time:19982ms step_avg:60.01ms
step:334/2285 train_time:20040ms step_avg:60.00ms
step:335/2285 train_time:20101ms step_avg:60.00ms
step:336/2285 train_time:20159ms step_avg:60.00ms
step:337/2285 train_time:20220ms step_avg:60.00ms
step:338/2285 train_time:20279ms step_avg:60.00ms
step:339/2285 train_time:20340ms step_avg:60.00ms
step:340/2285 train_time:20400ms step_avg:60.00ms
step:341/2285 train_time:20461ms step_avg:60.00ms
step:342/2285 train_time:20519ms step_avg:60.00ms
step:343/2285 train_time:20580ms step_avg:60.00ms
step:344/2285 train_time:20639ms step_avg:60.00ms
step:345/2285 train_time:20700ms step_avg:60.00ms
step:346/2285 train_time:20758ms step_avg:60.00ms
step:347/2285 train_time:20820ms step_avg:60.00ms
step:348/2285 train_time:20878ms step_avg:59.99ms
step:349/2285 train_time:20938ms step_avg:60.00ms
step:350/2285 train_time:20997ms step_avg:59.99ms
step:351/2285 train_time:21057ms step_avg:59.99ms
step:352/2285 train_time:21115ms step_avg:59.99ms
step:353/2285 train_time:21176ms step_avg:59.99ms
step:354/2285 train_time:21234ms step_avg:59.98ms
step:355/2285 train_time:21295ms step_avg:59.99ms
step:356/2285 train_time:21354ms step_avg:59.98ms
step:357/2285 train_time:21416ms step_avg:59.99ms
step:358/2285 train_time:21475ms step_avg:59.99ms
step:359/2285 train_time:21536ms step_avg:59.99ms
step:360/2285 train_time:21595ms step_avg:59.99ms
step:361/2285 train_time:21656ms step_avg:59.99ms
step:362/2285 train_time:21714ms step_avg:59.98ms
step:363/2285 train_time:21775ms step_avg:59.99ms
step:364/2285 train_time:21833ms step_avg:59.98ms
step:365/2285 train_time:21894ms step_avg:59.98ms
step:366/2285 train_time:21953ms step_avg:59.98ms
step:367/2285 train_time:22013ms step_avg:59.98ms
step:368/2285 train_time:22072ms step_avg:59.98ms
step:369/2285 train_time:22132ms step_avg:59.98ms
step:370/2285 train_time:22191ms step_avg:59.98ms
step:371/2285 train_time:22252ms step_avg:59.98ms
step:372/2285 train_time:22310ms step_avg:59.97ms
step:373/2285 train_time:22372ms step_avg:59.98ms
step:374/2285 train_time:22431ms step_avg:59.97ms
step:375/2285 train_time:22492ms step_avg:59.98ms
step:376/2285 train_time:22551ms step_avg:59.98ms
step:377/2285 train_time:22612ms step_avg:59.98ms
step:378/2285 train_time:22671ms step_avg:59.98ms
step:379/2285 train_time:22732ms step_avg:59.98ms
step:380/2285 train_time:22791ms step_avg:59.98ms
step:381/2285 train_time:22852ms step_avg:59.98ms
step:382/2285 train_time:22910ms step_avg:59.97ms
step:383/2285 train_time:22971ms step_avg:59.98ms
step:384/2285 train_time:23030ms step_avg:59.97ms
step:385/2285 train_time:23091ms step_avg:59.98ms
step:386/2285 train_time:23150ms step_avg:59.97ms
step:387/2285 train_time:23211ms step_avg:59.98ms
step:388/2285 train_time:23270ms step_avg:59.97ms
step:389/2285 train_time:23332ms step_avg:59.98ms
step:390/2285 train_time:23391ms step_avg:59.98ms
step:391/2285 train_time:23453ms step_avg:59.98ms
step:392/2285 train_time:23512ms step_avg:59.98ms
step:393/2285 train_time:23574ms step_avg:59.98ms
step:394/2285 train_time:23633ms step_avg:59.98ms
step:395/2285 train_time:23694ms step_avg:59.99ms
step:396/2285 train_time:23754ms step_avg:59.98ms
step:397/2285 train_time:23815ms step_avg:59.99ms
step:398/2285 train_time:23874ms step_avg:59.98ms
step:399/2285 train_time:23935ms step_avg:59.99ms
step:400/2285 train_time:23994ms step_avg:59.99ms
step:401/2285 train_time:24055ms step_avg:59.99ms
step:402/2285 train_time:24114ms step_avg:59.99ms
step:403/2285 train_time:24176ms step_avg:59.99ms
step:404/2285 train_time:24235ms step_avg:59.99ms
step:405/2285 train_time:24298ms step_avg:59.99ms
step:406/2285 train_time:24355ms step_avg:59.99ms
step:407/2285 train_time:24416ms step_avg:59.99ms
step:408/2285 train_time:24475ms step_avg:59.99ms
step:409/2285 train_time:24537ms step_avg:59.99ms
step:410/2285 train_time:24596ms step_avg:59.99ms
step:411/2285 train_time:24657ms step_avg:59.99ms
step:412/2285 train_time:24716ms step_avg:59.99ms
step:413/2285 train_time:24778ms step_avg:59.99ms
step:414/2285 train_time:24837ms step_avg:59.99ms
step:415/2285 train_time:24898ms step_avg:59.99ms
step:416/2285 train_time:24956ms step_avg:59.99ms
step:417/2285 train_time:25018ms step_avg:59.99ms
step:418/2285 train_time:25077ms step_avg:59.99ms
step:419/2285 train_time:25138ms step_avg:60.00ms
step:420/2285 train_time:25197ms step_avg:59.99ms
step:421/2285 train_time:25259ms step_avg:60.00ms
step:422/2285 train_time:25317ms step_avg:59.99ms
step:423/2285 train_time:25379ms step_avg:60.00ms
step:424/2285 train_time:25437ms step_avg:59.99ms
step:425/2285 train_time:25499ms step_avg:60.00ms
step:426/2285 train_time:25558ms step_avg:60.00ms
step:427/2285 train_time:25620ms step_avg:60.00ms
step:428/2285 train_time:25679ms step_avg:60.00ms
step:429/2285 train_time:25740ms step_avg:60.00ms
step:430/2285 train_time:25800ms step_avg:60.00ms
step:431/2285 train_time:25860ms step_avg:60.00ms
step:432/2285 train_time:25919ms step_avg:60.00ms
step:433/2285 train_time:25980ms step_avg:60.00ms
step:434/2285 train_time:26039ms step_avg:60.00ms
step:435/2285 train_time:26100ms step_avg:60.00ms
step:436/2285 train_time:26159ms step_avg:60.00ms
step:437/2285 train_time:26220ms step_avg:60.00ms
step:438/2285 train_time:26279ms step_avg:60.00ms
step:439/2285 train_time:26340ms step_avg:60.00ms
step:440/2285 train_time:26399ms step_avg:60.00ms
step:441/2285 train_time:26460ms step_avg:60.00ms
step:442/2285 train_time:26519ms step_avg:60.00ms
step:443/2285 train_time:26580ms step_avg:60.00ms
step:444/2285 train_time:26640ms step_avg:60.00ms
step:445/2285 train_time:26701ms step_avg:60.00ms
step:446/2285 train_time:26760ms step_avg:60.00ms
step:447/2285 train_time:26822ms step_avg:60.00ms
step:448/2285 train_time:26880ms step_avg:60.00ms
step:449/2285 train_time:26941ms step_avg:60.00ms
step:450/2285 train_time:27000ms step_avg:60.00ms
step:451/2285 train_time:27062ms step_avg:60.00ms
step:452/2285 train_time:27121ms step_avg:60.00ms
step:453/2285 train_time:27182ms step_avg:60.00ms
step:454/2285 train_time:27241ms step_avg:60.00ms
step:455/2285 train_time:27302ms step_avg:60.00ms
step:456/2285 train_time:27361ms step_avg:60.00ms
step:457/2285 train_time:27423ms step_avg:60.01ms
step:458/2285 train_time:27481ms step_avg:60.00ms
step:459/2285 train_time:27542ms step_avg:60.01ms
step:460/2285 train_time:27602ms step_avg:60.00ms
step:461/2285 train_time:27663ms step_avg:60.01ms
step:462/2285 train_time:27722ms step_avg:60.00ms
step:463/2285 train_time:27782ms step_avg:60.01ms
step:464/2285 train_time:27841ms step_avg:60.00ms
step:465/2285 train_time:27902ms step_avg:60.00ms
step:466/2285 train_time:27961ms step_avg:60.00ms
step:467/2285 train_time:28022ms step_avg:60.00ms
step:468/2285 train_time:28081ms step_avg:60.00ms
step:469/2285 train_time:28142ms step_avg:60.00ms
step:470/2285 train_time:28201ms step_avg:60.00ms
step:471/2285 train_time:28263ms step_avg:60.01ms
step:472/2285 train_time:28322ms step_avg:60.00ms
step:473/2285 train_time:28383ms step_avg:60.01ms
step:474/2285 train_time:28442ms step_avg:60.00ms
step:475/2285 train_time:28503ms step_avg:60.01ms
step:476/2285 train_time:28562ms step_avg:60.01ms
step:477/2285 train_time:28624ms step_avg:60.01ms
step:478/2285 train_time:28683ms step_avg:60.01ms
step:479/2285 train_time:28744ms step_avg:60.01ms
step:480/2285 train_time:28803ms step_avg:60.01ms
step:481/2285 train_time:28864ms step_avg:60.01ms
step:482/2285 train_time:28922ms step_avg:60.01ms
step:483/2285 train_time:28983ms step_avg:60.01ms
step:484/2285 train_time:29042ms step_avg:60.00ms
step:485/2285 train_time:29104ms step_avg:60.01ms
step:486/2285 train_time:29163ms step_avg:60.01ms
step:487/2285 train_time:29224ms step_avg:60.01ms
step:488/2285 train_time:29282ms step_avg:60.00ms
step:489/2285 train_time:29344ms step_avg:60.01ms
step:490/2285 train_time:29403ms step_avg:60.01ms
step:491/2285 train_time:29464ms step_avg:60.01ms
step:492/2285 train_time:29523ms step_avg:60.01ms
step:493/2285 train_time:29584ms step_avg:60.01ms
step:494/2285 train_time:29644ms step_avg:60.01ms
step:495/2285 train_time:29705ms step_avg:60.01ms
step:496/2285 train_time:29764ms step_avg:60.01ms
step:497/2285 train_time:29825ms step_avg:60.01ms
step:498/2285 train_time:29884ms step_avg:60.01ms
step:499/2285 train_time:29945ms step_avg:60.01ms
step:500/2285 train_time:30004ms step_avg:60.01ms
step:500/2285 val_loss:3.7835 train_time:30067ms step_avg:60.13ms
step:501/2285 train_time:30085ms step_avg:60.05ms
step:502/2285 train_time:30127ms step_avg:60.01ms
step:503/2285 train_time:30189ms step_avg:60.02ms
step:504/2285 train_time:30251ms step_avg:60.02ms
step:505/2285 train_time:30314ms step_avg:60.03ms
step:506/2285 train_time:30374ms step_avg:60.03ms
step:507/2285 train_time:30435ms step_avg:60.03ms
step:508/2285 train_time:30494ms step_avg:60.03ms
step:509/2285 train_time:30555ms step_avg:60.03ms
step:510/2285 train_time:30613ms step_avg:60.03ms
step:511/2285 train_time:30674ms step_avg:60.03ms
step:512/2285 train_time:30733ms step_avg:60.02ms
step:513/2285 train_time:30794ms step_avg:60.03ms
step:514/2285 train_time:30853ms step_avg:60.03ms
step:515/2285 train_time:30914ms step_avg:60.03ms
step:516/2285 train_time:30974ms step_avg:60.03ms
step:517/2285 train_time:31038ms step_avg:60.04ms
step:518/2285 train_time:31098ms step_avg:60.03ms
step:519/2285 train_time:31160ms step_avg:60.04ms
step:520/2285 train_time:31220ms step_avg:60.04ms
step:521/2285 train_time:31281ms step_avg:60.04ms
step:522/2285 train_time:31340ms step_avg:60.04ms
step:523/2285 train_time:31402ms step_avg:60.04ms
step:524/2285 train_time:31462ms step_avg:60.04ms
step:525/2285 train_time:31523ms step_avg:60.04ms
step:526/2285 train_time:31581ms step_avg:60.04ms
step:527/2285 train_time:31643ms step_avg:60.04ms
step:528/2285 train_time:31702ms step_avg:60.04ms
step:529/2285 train_time:31764ms step_avg:60.05ms
step:530/2285 train_time:31823ms step_avg:60.04ms
step:531/2285 train_time:31885ms step_avg:60.05ms
step:532/2285 train_time:31944ms step_avg:60.05ms
step:533/2285 train_time:32005ms step_avg:60.05ms
step:534/2285 train_time:32065ms step_avg:60.05ms
step:535/2285 train_time:32127ms step_avg:60.05ms
step:536/2285 train_time:32187ms step_avg:60.05ms
step:537/2285 train_time:32249ms step_avg:60.05ms
step:538/2285 train_time:32309ms step_avg:60.05ms
step:539/2285 train_time:32371ms step_avg:60.06ms
step:540/2285 train_time:32430ms step_avg:60.06ms
step:541/2285 train_time:32492ms step_avg:60.06ms
step:542/2285 train_time:32551ms step_avg:60.06ms
step:543/2285 train_time:32613ms step_avg:60.06ms
step:544/2285 train_time:32672ms step_avg:60.06ms
step:545/2285 train_time:32733ms step_avg:60.06ms
step:546/2285 train_time:32792ms step_avg:60.06ms
step:547/2285 train_time:32854ms step_avg:60.06ms
step:548/2285 train_time:32913ms step_avg:60.06ms
step:549/2285 train_time:32974ms step_avg:60.06ms
step:550/2285 train_time:33033ms step_avg:60.06ms
step:551/2285 train_time:33095ms step_avg:60.06ms
step:552/2285 train_time:33154ms step_avg:60.06ms
step:553/2285 train_time:33216ms step_avg:60.06ms
step:554/2285 train_time:33275ms step_avg:60.06ms
step:555/2285 train_time:33337ms step_avg:60.07ms
step:556/2285 train_time:33395ms step_avg:60.06ms
step:557/2285 train_time:33457ms step_avg:60.07ms
step:558/2285 train_time:33516ms step_avg:60.06ms
step:559/2285 train_time:33577ms step_avg:60.07ms
step:560/2285 train_time:33636ms step_avg:60.06ms
step:561/2285 train_time:33697ms step_avg:60.07ms
step:562/2285 train_time:33756ms step_avg:60.06ms
step:563/2285 train_time:33817ms step_avg:60.07ms
step:564/2285 train_time:33876ms step_avg:60.06ms
step:565/2285 train_time:33938ms step_avg:60.07ms
step:566/2285 train_time:33997ms step_avg:60.07ms
step:567/2285 train_time:34058ms step_avg:60.07ms
step:568/2285 train_time:34117ms step_avg:60.07ms
step:569/2285 train_time:34179ms step_avg:60.07ms
step:570/2285 train_time:34238ms step_avg:60.07ms
step:571/2285 train_time:34300ms step_avg:60.07ms
step:572/2285 train_time:34359ms step_avg:60.07ms
step:573/2285 train_time:34420ms step_avg:60.07ms
step:574/2285 train_time:34479ms step_avg:60.07ms
step:575/2285 train_time:34540ms step_avg:60.07ms
step:576/2285 train_time:34599ms step_avg:60.07ms
step:577/2285 train_time:34661ms step_avg:60.07ms
step:578/2285 train_time:34720ms step_avg:60.07ms
step:579/2285 train_time:34782ms step_avg:60.07ms
step:580/2285 train_time:34841ms step_avg:60.07ms
step:581/2285 train_time:34903ms step_avg:60.07ms
step:582/2285 train_time:34962ms step_avg:60.07ms
step:583/2285 train_time:35023ms step_avg:60.07ms
step:584/2285 train_time:35082ms step_avg:60.07ms
step:585/2285 train_time:35144ms step_avg:60.07ms
step:586/2285 train_time:35203ms step_avg:60.07ms
step:587/2285 train_time:35265ms step_avg:60.08ms
step:588/2285 train_time:35324ms step_avg:60.08ms
step:589/2285 train_time:35386ms step_avg:60.08ms
step:590/2285 train_time:35446ms step_avg:60.08ms
step:591/2285 train_time:35508ms step_avg:60.08ms
step:592/2285 train_time:35567ms step_avg:60.08ms
step:593/2285 train_time:35628ms step_avg:60.08ms
step:594/2285 train_time:35687ms step_avg:60.08ms
step:595/2285 train_time:35749ms step_avg:60.08ms
step:596/2285 train_time:35808ms step_avg:60.08ms
step:597/2285 train_time:35870ms step_avg:60.08ms
step:598/2285 train_time:35929ms step_avg:60.08ms
step:599/2285 train_time:35990ms step_avg:60.08ms
step:600/2285 train_time:36049ms step_avg:60.08ms
step:601/2285 train_time:36111ms step_avg:60.08ms
step:602/2285 train_time:36170ms step_avg:60.08ms
step:603/2285 train_time:36232ms step_avg:60.09ms
step:604/2285 train_time:36291ms step_avg:60.08ms
step:605/2285 train_time:36352ms step_avg:60.09ms
step:606/2285 train_time:36412ms step_avg:60.08ms
step:607/2285 train_time:36473ms step_avg:60.09ms
step:608/2285 train_time:36532ms step_avg:60.09ms
step:609/2285 train_time:36594ms step_avg:60.09ms
step:610/2285 train_time:36653ms step_avg:60.09ms
step:611/2285 train_time:36715ms step_avg:60.09ms
step:612/2285 train_time:36774ms step_avg:60.09ms
step:613/2285 train_time:36835ms step_avg:60.09ms
step:614/2285 train_time:36894ms step_avg:60.09ms
step:615/2285 train_time:36955ms step_avg:60.09ms
step:616/2285 train_time:37014ms step_avg:60.09ms
step:617/2285 train_time:37075ms step_avg:60.09ms
step:618/2285 train_time:37134ms step_avg:60.09ms
step:619/2285 train_time:37195ms step_avg:60.09ms
step:620/2285 train_time:37254ms step_avg:60.09ms
step:621/2285 train_time:37316ms step_avg:60.09ms
step:622/2285 train_time:37375ms step_avg:60.09ms
step:623/2285 train_time:37436ms step_avg:60.09ms
step:624/2285 train_time:37496ms step_avg:60.09ms
step:625/2285 train_time:37558ms step_avg:60.09ms
step:626/2285 train_time:37616ms step_avg:60.09ms
step:627/2285 train_time:37678ms step_avg:60.09ms
step:628/2285 train_time:37737ms step_avg:60.09ms
step:629/2285 train_time:37798ms step_avg:60.09ms
step:630/2285 train_time:37858ms step_avg:60.09ms
step:631/2285 train_time:37918ms step_avg:60.09ms
step:632/2285 train_time:37976ms step_avg:60.09ms
step:633/2285 train_time:38038ms step_avg:60.09ms
step:634/2285 train_time:38096ms step_avg:60.09ms
step:635/2285 train_time:38158ms step_avg:60.09ms
step:636/2285 train_time:38217ms step_avg:60.09ms
step:637/2285 train_time:38278ms step_avg:60.09ms
step:638/2285 train_time:38337ms step_avg:60.09ms
step:639/2285 train_time:38399ms step_avg:60.09ms
step:640/2285 train_time:38458ms step_avg:60.09ms
step:641/2285 train_time:38519ms step_avg:60.09ms
step:642/2285 train_time:38578ms step_avg:60.09ms
step:643/2285 train_time:38639ms step_avg:60.09ms
step:644/2285 train_time:38698ms step_avg:60.09ms
step:645/2285 train_time:38759ms step_avg:60.09ms
step:646/2285 train_time:38818ms step_avg:60.09ms
step:647/2285 train_time:38879ms step_avg:60.09ms
step:648/2285 train_time:38938ms step_avg:60.09ms
step:649/2285 train_time:38999ms step_avg:60.09ms
step:650/2285 train_time:39058ms step_avg:60.09ms
step:651/2285 train_time:39120ms step_avg:60.09ms
step:652/2285 train_time:39179ms step_avg:60.09ms
step:653/2285 train_time:39240ms step_avg:60.09ms
step:654/2285 train_time:39299ms step_avg:60.09ms
step:655/2285 train_time:39360ms step_avg:60.09ms
step:656/2285 train_time:39420ms step_avg:60.09ms
step:657/2285 train_time:39481ms step_avg:60.09ms
step:658/2285 train_time:39539ms step_avg:60.09ms
step:659/2285 train_time:39601ms step_avg:60.09ms
step:660/2285 train_time:39660ms step_avg:60.09ms
step:661/2285 train_time:39722ms step_avg:60.09ms
step:662/2285 train_time:39781ms step_avg:60.09ms
step:663/2285 train_time:39842ms step_avg:60.09ms
step:664/2285 train_time:39902ms step_avg:60.09ms
step:665/2285 train_time:39963ms step_avg:60.09ms
step:666/2285 train_time:40022ms step_avg:60.09ms
step:667/2285 train_time:40083ms step_avg:60.09ms
step:668/2285 train_time:40142ms step_avg:60.09ms
step:669/2285 train_time:40203ms step_avg:60.09ms
step:670/2285 train_time:40263ms step_avg:60.09ms
step:671/2285 train_time:40325ms step_avg:60.10ms
step:672/2285 train_time:40384ms step_avg:60.10ms
step:673/2285 train_time:40445ms step_avg:60.10ms
step:674/2285 train_time:40505ms step_avg:60.10ms
step:675/2285 train_time:40567ms step_avg:60.10ms
step:676/2285 train_time:40625ms step_avg:60.10ms
step:677/2285 train_time:40687ms step_avg:60.10ms
step:678/2285 train_time:40747ms step_avg:60.10ms
step:679/2285 train_time:40809ms step_avg:60.10ms
step:680/2285 train_time:40868ms step_avg:60.10ms
step:681/2285 train_time:40930ms step_avg:60.10ms
step:682/2285 train_time:40989ms step_avg:60.10ms
step:683/2285 train_time:41051ms step_avg:60.10ms
step:684/2285 train_time:41111ms step_avg:60.10ms
step:685/2285 train_time:41173ms step_avg:60.11ms
step:686/2285 train_time:41233ms step_avg:60.11ms
step:687/2285 train_time:41294ms step_avg:60.11ms
step:688/2285 train_time:41353ms step_avg:60.11ms
step:689/2285 train_time:41416ms step_avg:60.11ms
step:690/2285 train_time:41475ms step_avg:60.11ms
step:691/2285 train_time:41536ms step_avg:60.11ms
step:692/2285 train_time:41595ms step_avg:60.11ms
step:693/2285 train_time:41656ms step_avg:60.11ms
step:694/2285 train_time:41715ms step_avg:60.11ms
step:695/2285 train_time:41777ms step_avg:60.11ms
step:696/2285 train_time:41835ms step_avg:60.11ms
step:697/2285 train_time:41897ms step_avg:60.11ms
step:698/2285 train_time:41956ms step_avg:60.11ms
step:699/2285 train_time:42018ms step_avg:60.11ms
step:700/2285 train_time:42076ms step_avg:60.11ms
step:701/2285 train_time:42138ms step_avg:60.11ms
step:702/2285 train_time:42197ms step_avg:60.11ms
step:703/2285 train_time:42258ms step_avg:60.11ms
step:704/2285 train_time:42317ms step_avg:60.11ms
step:705/2285 train_time:42379ms step_avg:60.11ms
step:706/2285 train_time:42437ms step_avg:60.11ms
step:707/2285 train_time:42499ms step_avg:60.11ms
step:708/2285 train_time:42558ms step_avg:60.11ms
step:709/2285 train_time:42619ms step_avg:60.11ms
step:710/2285 train_time:42678ms step_avg:60.11ms
step:711/2285 train_time:42739ms step_avg:60.11ms
step:712/2285 train_time:42798ms step_avg:60.11ms
step:713/2285 train_time:42860ms step_avg:60.11ms
step:714/2285 train_time:42919ms step_avg:60.11ms
step:715/2285 train_time:42981ms step_avg:60.11ms
step:716/2285 train_time:43040ms step_avg:60.11ms
step:717/2285 train_time:43101ms step_avg:60.11ms
step:718/2285 train_time:43161ms step_avg:60.11ms
step:719/2285 train_time:43223ms step_avg:60.12ms
step:720/2285 train_time:43282ms step_avg:60.11ms
step:721/2285 train_time:43343ms step_avg:60.12ms
step:722/2285 train_time:43402ms step_avg:60.11ms
step:723/2285 train_time:43464ms step_avg:60.12ms
step:724/2285 train_time:43523ms step_avg:60.11ms
step:725/2285 train_time:43584ms step_avg:60.12ms
step:726/2285 train_time:43643ms step_avg:60.11ms
step:727/2285 train_time:43704ms step_avg:60.12ms
step:728/2285 train_time:43765ms step_avg:60.12ms
step:729/2285 train_time:43826ms step_avg:60.12ms
step:730/2285 train_time:43885ms step_avg:60.12ms
step:731/2285 train_time:43947ms step_avg:60.12ms
step:732/2285 train_time:44006ms step_avg:60.12ms
step:733/2285 train_time:44067ms step_avg:60.12ms
step:734/2285 train_time:44126ms step_avg:60.12ms
step:735/2285 train_time:44189ms step_avg:60.12ms
step:736/2285 train_time:44248ms step_avg:60.12ms
step:737/2285 train_time:44309ms step_avg:60.12ms
step:738/2285 train_time:44369ms step_avg:60.12ms
step:739/2285 train_time:44430ms step_avg:60.12ms
step:740/2285 train_time:44490ms step_avg:60.12ms
step:741/2285 train_time:44552ms step_avg:60.12ms
step:742/2285 train_time:44611ms step_avg:60.12ms
step:743/2285 train_time:44672ms step_avg:60.12ms
step:744/2285 train_time:44731ms step_avg:60.12ms
step:745/2285 train_time:44793ms step_avg:60.12ms
step:746/2285 train_time:44852ms step_avg:60.12ms
step:747/2285 train_time:44914ms step_avg:60.13ms
step:748/2285 train_time:44973ms step_avg:60.12ms
step:749/2285 train_time:45035ms step_avg:60.13ms
step:750/2285 train_time:45095ms step_avg:60.13ms
step:750/2285 val_loss:3.6604 train_time:45158ms step_avg:60.21ms
step:751/2285 train_time:45177ms step_avg:60.16ms
step:752/2285 train_time:45218ms step_avg:60.13ms
step:753/2285 train_time:45282ms step_avg:60.14ms
step:754/2285 train_time:45343ms step_avg:60.14ms
step:755/2285 train_time:45405ms step_avg:60.14ms
step:756/2285 train_time:45464ms step_avg:60.14ms
step:757/2285 train_time:45525ms step_avg:60.14ms
step:758/2285 train_time:45583ms step_avg:60.14ms
step:759/2285 train_time:45644ms step_avg:60.14ms
step:760/2285 train_time:45703ms step_avg:60.14ms
step:761/2285 train_time:45764ms step_avg:60.14ms
step:762/2285 train_time:45823ms step_avg:60.13ms
step:763/2285 train_time:45884ms step_avg:60.14ms
step:764/2285 train_time:45943ms step_avg:60.13ms
step:765/2285 train_time:46004ms step_avg:60.14ms
step:766/2285 train_time:46064ms step_avg:60.14ms
step:767/2285 train_time:46127ms step_avg:60.14ms
step:768/2285 train_time:46188ms step_avg:60.14ms
step:769/2285 train_time:46251ms step_avg:60.14ms
step:770/2285 train_time:46311ms step_avg:60.14ms
step:771/2285 train_time:46374ms step_avg:60.15ms
step:772/2285 train_time:46433ms step_avg:60.15ms
step:773/2285 train_time:46494ms step_avg:60.15ms
step:774/2285 train_time:46554ms step_avg:60.15ms
step:775/2285 train_time:46615ms step_avg:60.15ms
step:776/2285 train_time:46674ms step_avg:60.15ms
step:777/2285 train_time:46736ms step_avg:60.15ms
step:778/2285 train_time:46795ms step_avg:60.15ms
step:779/2285 train_time:46857ms step_avg:60.15ms
step:780/2285 train_time:46916ms step_avg:60.15ms
step:781/2285 train_time:46978ms step_avg:60.15ms
step:782/2285 train_time:47037ms step_avg:60.15ms
step:783/2285 train_time:47099ms step_avg:60.15ms
step:784/2285 train_time:47160ms step_avg:60.15ms
step:785/2285 train_time:47222ms step_avg:60.16ms
step:786/2285 train_time:47282ms step_avg:60.16ms
step:787/2285 train_time:47344ms step_avg:60.16ms
step:788/2285 train_time:47405ms step_avg:60.16ms
step:789/2285 train_time:47467ms step_avg:60.16ms
step:790/2285 train_time:47526ms step_avg:60.16ms
step:791/2285 train_time:47588ms step_avg:60.16ms
step:792/2285 train_time:47647ms step_avg:60.16ms
step:793/2285 train_time:47709ms step_avg:60.16ms
step:794/2285 train_time:47768ms step_avg:60.16ms
step:795/2285 train_time:47830ms step_avg:60.16ms
step:796/2285 train_time:47889ms step_avg:60.16ms
step:797/2285 train_time:47951ms step_avg:60.16ms
step:798/2285 train_time:48010ms step_avg:60.16ms
step:799/2285 train_time:48072ms step_avg:60.17ms
step:800/2285 train_time:48132ms step_avg:60.17ms
step:801/2285 train_time:48194ms step_avg:60.17ms
step:802/2285 train_time:48253ms step_avg:60.17ms
step:803/2285 train_time:48315ms step_avg:60.17ms
step:804/2285 train_time:48374ms step_avg:60.17ms
step:805/2285 train_time:48436ms step_avg:60.17ms
step:806/2285 train_time:48496ms step_avg:60.17ms
step:807/2285 train_time:48557ms step_avg:60.17ms
step:808/2285 train_time:48617ms step_avg:60.17ms
step:809/2285 train_time:48679ms step_avg:60.17ms
step:810/2285 train_time:48738ms step_avg:60.17ms
step:811/2285 train_time:48800ms step_avg:60.17ms
step:812/2285 train_time:48859ms step_avg:60.17ms
step:813/2285 train_time:48920ms step_avg:60.17ms
step:814/2285 train_time:48980ms step_avg:60.17ms
step:815/2285 train_time:49042ms step_avg:60.17ms
step:816/2285 train_time:49103ms step_avg:60.17ms
step:817/2285 train_time:49165ms step_avg:60.18ms
step:818/2285 train_time:49225ms step_avg:60.18ms
step:819/2285 train_time:49287ms step_avg:60.18ms
step:820/2285 train_time:49347ms step_avg:60.18ms
step:821/2285 train_time:49409ms step_avg:60.18ms
step:822/2285 train_time:49469ms step_avg:60.18ms
step:823/2285 train_time:49531ms step_avg:60.18ms
step:824/2285 train_time:49590ms step_avg:60.18ms
step:825/2285 train_time:49652ms step_avg:60.18ms
step:826/2285 train_time:49712ms step_avg:60.18ms
step:827/2285 train_time:49774ms step_avg:60.19ms
step:828/2285 train_time:49833ms step_avg:60.18ms
step:829/2285 train_time:49894ms step_avg:60.19ms
step:830/2285 train_time:49954ms step_avg:60.19ms
step:831/2285 train_time:50016ms step_avg:60.19ms
step:832/2285 train_time:50075ms step_avg:60.19ms
step:833/2285 train_time:50137ms step_avg:60.19ms
step:834/2285 train_time:50197ms step_avg:60.19ms
step:835/2285 train_time:50258ms step_avg:60.19ms
step:836/2285 train_time:50318ms step_avg:60.19ms
step:837/2285 train_time:50380ms step_avg:60.19ms
step:838/2285 train_time:50439ms step_avg:60.19ms
step:839/2285 train_time:50501ms step_avg:60.19ms
step:840/2285 train_time:50561ms step_avg:60.19ms
step:841/2285 train_time:50623ms step_avg:60.19ms
step:842/2285 train_time:50683ms step_avg:60.19ms
step:843/2285 train_time:50745ms step_avg:60.20ms
step:844/2285 train_time:50804ms step_avg:60.19ms
step:845/2285 train_time:50866ms step_avg:60.20ms
step:846/2285 train_time:50927ms step_avg:60.20ms
step:847/2285 train_time:50988ms step_avg:60.20ms
step:848/2285 train_time:51047ms step_avg:60.20ms
step:849/2285 train_time:51110ms step_avg:60.20ms
step:850/2285 train_time:51169ms step_avg:60.20ms
step:851/2285 train_time:51231ms step_avg:60.20ms
step:852/2285 train_time:51291ms step_avg:60.20ms
step:853/2285 train_time:51352ms step_avg:60.20ms
step:854/2285 train_time:51412ms step_avg:60.20ms
step:855/2285 train_time:51473ms step_avg:60.20ms
step:856/2285 train_time:51533ms step_avg:60.20ms
step:857/2285 train_time:51595ms step_avg:60.20ms
step:858/2285 train_time:51654ms step_avg:60.20ms
step:859/2285 train_time:51716ms step_avg:60.20ms
step:860/2285 train_time:51775ms step_avg:60.20ms
step:861/2285 train_time:51837ms step_avg:60.21ms
step:862/2285 train_time:51897ms step_avg:60.20ms
step:863/2285 train_time:51958ms step_avg:60.21ms
step:864/2285 train_time:52018ms step_avg:60.21ms
step:865/2285 train_time:52079ms step_avg:60.21ms
step:866/2285 train_time:52139ms step_avg:60.21ms
step:867/2285 train_time:52201ms step_avg:60.21ms
step:868/2285 train_time:52260ms step_avg:60.21ms
step:869/2285 train_time:52322ms step_avg:60.21ms
step:870/2285 train_time:52382ms step_avg:60.21ms
step:871/2285 train_time:52444ms step_avg:60.21ms
step:872/2285 train_time:52505ms step_avg:60.21ms
step:873/2285 train_time:52567ms step_avg:60.21ms
step:874/2285 train_time:52627ms step_avg:60.21ms
step:875/2285 train_time:52689ms step_avg:60.22ms
step:876/2285 train_time:52748ms step_avg:60.21ms
step:877/2285 train_time:52810ms step_avg:60.22ms
step:878/2285 train_time:52869ms step_avg:60.22ms
step:879/2285 train_time:52931ms step_avg:60.22ms
step:880/2285 train_time:52991ms step_avg:60.22ms
step:881/2285 train_time:53054ms step_avg:60.22ms
step:882/2285 train_time:53113ms step_avg:60.22ms
step:883/2285 train_time:53175ms step_avg:60.22ms
step:884/2285 train_time:53234ms step_avg:60.22ms
step:885/2285 train_time:53296ms step_avg:60.22ms
step:886/2285 train_time:53356ms step_avg:60.22ms
step:887/2285 train_time:53417ms step_avg:60.22ms
step:888/2285 train_time:53477ms step_avg:60.22ms
step:889/2285 train_time:53539ms step_avg:60.22ms
step:890/2285 train_time:53598ms step_avg:60.22ms
step:891/2285 train_time:53660ms step_avg:60.22ms
step:892/2285 train_time:53720ms step_avg:60.22ms
step:893/2285 train_time:53781ms step_avg:60.23ms
step:894/2285 train_time:53841ms step_avg:60.22ms
step:895/2285 train_time:53904ms step_avg:60.23ms
step:896/2285 train_time:53963ms step_avg:60.23ms
step:897/2285 train_time:54025ms step_avg:60.23ms
step:898/2285 train_time:54085ms step_avg:60.23ms
step:899/2285 train_time:54147ms step_avg:60.23ms
step:900/2285 train_time:54207ms step_avg:60.23ms
step:901/2285 train_time:54269ms step_avg:60.23ms
step:902/2285 train_time:54329ms step_avg:60.23ms
step:903/2285 train_time:54390ms step_avg:60.23ms
step:904/2285 train_time:54450ms step_avg:60.23ms
step:905/2285 train_time:54511ms step_avg:60.23ms
step:906/2285 train_time:54571ms step_avg:60.23ms
step:907/2285 train_time:54632ms step_avg:60.23ms
step:908/2285 train_time:54692ms step_avg:60.23ms
step:909/2285 train_time:54753ms step_avg:60.23ms
step:910/2285 train_time:54813ms step_avg:60.23ms
step:911/2285 train_time:54875ms step_avg:60.24ms
step:912/2285 train_time:54935ms step_avg:60.24ms
step:913/2285 train_time:54997ms step_avg:60.24ms
step:914/2285 train_time:55056ms step_avg:60.24ms
step:915/2285 train_time:55118ms step_avg:60.24ms
step:916/2285 train_time:55178ms step_avg:60.24ms
step:917/2285 train_time:55240ms step_avg:60.24ms
step:918/2285 train_time:55299ms step_avg:60.24ms
step:919/2285 train_time:55361ms step_avg:60.24ms
step:920/2285 train_time:55421ms step_avg:60.24ms
step:921/2285 train_time:55483ms step_avg:60.24ms
step:922/2285 train_time:55542ms step_avg:60.24ms
step:923/2285 train_time:55604ms step_avg:60.24ms
step:924/2285 train_time:55663ms step_avg:60.24ms
step:925/2285 train_time:55725ms step_avg:60.24ms
step:926/2285 train_time:55784ms step_avg:60.24ms
step:927/2285 train_time:55846ms step_avg:60.24ms
step:928/2285 train_time:55907ms step_avg:60.24ms
step:929/2285 train_time:55969ms step_avg:60.25ms
step:930/2285 train_time:56028ms step_avg:60.25ms
step:931/2285 train_time:56090ms step_avg:60.25ms
step:932/2285 train_time:56150ms step_avg:60.25ms
step:933/2285 train_time:56212ms step_avg:60.25ms
step:934/2285 train_time:56271ms step_avg:60.25ms
step:935/2285 train_time:56334ms step_avg:60.25ms
step:936/2285 train_time:56393ms step_avg:60.25ms
step:937/2285 train_time:56455ms step_avg:60.25ms
step:938/2285 train_time:56514ms step_avg:60.25ms
step:939/2285 train_time:56576ms step_avg:60.25ms
step:940/2285 train_time:56635ms step_avg:60.25ms
step:941/2285 train_time:56696ms step_avg:60.25ms
step:942/2285 train_time:56756ms step_avg:60.25ms
step:943/2285 train_time:56818ms step_avg:60.25ms
step:944/2285 train_time:56878ms step_avg:60.25ms
step:945/2285 train_time:56939ms step_avg:60.25ms
step:946/2285 train_time:56999ms step_avg:60.25ms
step:947/2285 train_time:57061ms step_avg:60.25ms
step:948/2285 train_time:57121ms step_avg:60.25ms
step:949/2285 train_time:57183ms step_avg:60.26ms
step:950/2285 train_time:57243ms step_avg:60.26ms
step:951/2285 train_time:57305ms step_avg:60.26ms
step:952/2285 train_time:57365ms step_avg:60.26ms
step:953/2285 train_time:57427ms step_avg:60.26ms
step:954/2285 train_time:57487ms step_avg:60.26ms
step:955/2285 train_time:57548ms step_avg:60.26ms
step:956/2285 train_time:57608ms step_avg:60.26ms
step:957/2285 train_time:57670ms step_avg:60.26ms
step:958/2285 train_time:57730ms step_avg:60.26ms
step:959/2285 train_time:57792ms step_avg:60.26ms
step:960/2285 train_time:57852ms step_avg:60.26ms
step:961/2285 train_time:57913ms step_avg:60.26ms
step:962/2285 train_time:57973ms step_avg:60.26ms
step:963/2285 train_time:58034ms step_avg:60.26ms
step:964/2285 train_time:58094ms step_avg:60.26ms
step:965/2285 train_time:58156ms step_avg:60.27ms
step:966/2285 train_time:58216ms step_avg:60.27ms
step:967/2285 train_time:58278ms step_avg:60.27ms
step:968/2285 train_time:58337ms step_avg:60.27ms
step:969/2285 train_time:58399ms step_avg:60.27ms
step:970/2285 train_time:58459ms step_avg:60.27ms
step:971/2285 train_time:58521ms step_avg:60.27ms
step:972/2285 train_time:58580ms step_avg:60.27ms
step:973/2285 train_time:58641ms step_avg:60.27ms
step:974/2285 train_time:58701ms step_avg:60.27ms
step:975/2285 train_time:58763ms step_avg:60.27ms
step:976/2285 train_time:58823ms step_avg:60.27ms
step:977/2285 train_time:58885ms step_avg:60.27ms
step:978/2285 train_time:58945ms step_avg:60.27ms
step:979/2285 train_time:59008ms step_avg:60.27ms
step:980/2285 train_time:59067ms step_avg:60.27ms
step:981/2285 train_time:59129ms step_avg:60.27ms
step:982/2285 train_time:59189ms step_avg:60.27ms
step:983/2285 train_time:59251ms step_avg:60.28ms
step:984/2285 train_time:59310ms step_avg:60.27ms
step:985/2285 train_time:59372ms step_avg:60.28ms
step:986/2285 train_time:59432ms step_avg:60.28ms
step:987/2285 train_time:59494ms step_avg:60.28ms
step:988/2285 train_time:59553ms step_avg:60.28ms
step:989/2285 train_time:59615ms step_avg:60.28ms
step:990/2285 train_time:59675ms step_avg:60.28ms
step:991/2285 train_time:59737ms step_avg:60.28ms
step:992/2285 train_time:59796ms step_avg:60.28ms
step:993/2285 train_time:59858ms step_avg:60.28ms
step:994/2285 train_time:59918ms step_avg:60.28ms
step:995/2285 train_time:59980ms step_avg:60.28ms
step:996/2285 train_time:60039ms step_avg:60.28ms
step:997/2285 train_time:60101ms step_avg:60.28ms
step:998/2285 train_time:60161ms step_avg:60.28ms
step:999/2285 train_time:60222ms step_avg:60.28ms
step:1000/2285 train_time:60281ms step_avg:60.28ms
step:1000/2285 val_loss:3.5649 train_time:60345ms step_avg:60.35ms
step:1001/2285 train_time:60364ms step_avg:60.30ms
step:1002/2285 train_time:60406ms step_avg:60.29ms
step:1003/2285 train_time:60468ms step_avg:60.29ms
step:1004/2285 train_time:60528ms step_avg:60.29ms
step:1005/2285 train_time:60593ms step_avg:60.29ms
step:1006/2285 train_time:60654ms step_avg:60.29ms
step:1007/2285 train_time:60715ms step_avg:60.29ms
step:1008/2285 train_time:60774ms step_avg:60.29ms
step:1009/2285 train_time:60835ms step_avg:60.29ms
step:1010/2285 train_time:60894ms step_avg:60.29ms
step:1011/2285 train_time:60955ms step_avg:60.29ms
step:1012/2285 train_time:61013ms step_avg:60.29ms
step:1013/2285 train_time:61074ms step_avg:60.29ms
step:1014/2285 train_time:61133ms step_avg:60.29ms
step:1015/2285 train_time:61194ms step_avg:60.29ms
step:1016/2285 train_time:61254ms step_avg:60.29ms
step:1017/2285 train_time:61317ms step_avg:60.29ms
step:1018/2285 train_time:61377ms step_avg:60.29ms
step:1019/2285 train_time:61440ms step_avg:60.29ms
step:1020/2285 train_time:61501ms step_avg:60.29ms
step:1021/2285 train_time:61563ms step_avg:60.30ms
step:1022/2285 train_time:61623ms step_avg:60.30ms
step:1023/2285 train_time:61685ms step_avg:60.30ms
step:1024/2285 train_time:61744ms step_avg:60.30ms
step:1025/2285 train_time:61806ms step_avg:60.30ms
step:1026/2285 train_time:61866ms step_avg:60.30ms
step:1027/2285 train_time:61927ms step_avg:60.30ms
step:1028/2285 train_time:61986ms step_avg:60.30ms
step:1029/2285 train_time:62048ms step_avg:60.30ms
step:1030/2285 train_time:62107ms step_avg:60.30ms
step:1031/2285 train_time:62168ms step_avg:60.30ms
step:1032/2285 train_time:62228ms step_avg:60.30ms
step:1033/2285 train_time:62290ms step_avg:60.30ms
step:1034/2285 train_time:62351ms step_avg:60.30ms
step:1035/2285 train_time:62413ms step_avg:60.30ms
step:1036/2285 train_time:62474ms step_avg:60.30ms
step:1037/2285 train_time:62536ms step_avg:60.30ms
step:1038/2285 train_time:62596ms step_avg:60.30ms
step:1039/2285 train_time:62658ms step_avg:60.31ms
step:1040/2285 train_time:62717ms step_avg:60.31ms
step:1041/2285 train_time:62780ms step_avg:60.31ms
step:1042/2285 train_time:62838ms step_avg:60.31ms
step:1043/2285 train_time:62900ms step_avg:60.31ms
step:1044/2285 train_time:62959ms step_avg:60.31ms
step:1045/2285 train_time:63021ms step_avg:60.31ms
step:1046/2285 train_time:63081ms step_avg:60.31ms
step:1047/2285 train_time:63142ms step_avg:60.31ms
step:1048/2285 train_time:63202ms step_avg:60.31ms
step:1049/2285 train_time:63264ms step_avg:60.31ms
step:1050/2285 train_time:63324ms step_avg:60.31ms
step:1051/2285 train_time:63386ms step_avg:60.31ms
step:1052/2285 train_time:63446ms step_avg:60.31ms
step:1053/2285 train_time:63508ms step_avg:60.31ms
step:1054/2285 train_time:63568ms step_avg:60.31ms
step:1055/2285 train_time:63630ms step_avg:60.31ms
step:1056/2285 train_time:63691ms step_avg:60.31ms
step:1057/2285 train_time:63754ms step_avg:60.32ms
step:1058/2285 train_time:63814ms step_avg:60.32ms
step:1059/2285 train_time:63875ms step_avg:60.32ms
step:1060/2285 train_time:63935ms step_avg:60.32ms
step:1061/2285 train_time:63996ms step_avg:60.32ms
step:1062/2285 train_time:64056ms step_avg:60.32ms
step:1063/2285 train_time:64117ms step_avg:60.32ms
step:1064/2285 train_time:64177ms step_avg:60.32ms
step:1065/2285 train_time:64239ms step_avg:60.32ms
step:1066/2285 train_time:64299ms step_avg:60.32ms
step:1067/2285 train_time:64361ms step_avg:60.32ms
step:1068/2285 train_time:64420ms step_avg:60.32ms
step:1069/2285 train_time:64482ms step_avg:60.32ms
step:1070/2285 train_time:64541ms step_avg:60.32ms
step:1071/2285 train_time:64603ms step_avg:60.32ms
step:1072/2285 train_time:64663ms step_avg:60.32ms
step:1073/2285 train_time:64726ms step_avg:60.32ms
step:1074/2285 train_time:64786ms step_avg:60.32ms
step:1075/2285 train_time:64848ms step_avg:60.32ms
step:1076/2285 train_time:64907ms step_avg:60.32ms
step:1077/2285 train_time:64969ms step_avg:60.32ms
step:1078/2285 train_time:65028ms step_avg:60.32ms
step:1079/2285 train_time:65091ms step_avg:60.32ms
step:1080/2285 train_time:65150ms step_avg:60.32ms
step:1081/2285 train_time:65212ms step_avg:60.33ms
step:1082/2285 train_time:65272ms step_avg:60.33ms
step:1083/2285 train_time:65334ms step_avg:60.33ms
step:1084/2285 train_time:65393ms step_avg:60.33ms
step:1085/2285 train_time:65456ms step_avg:60.33ms
step:1086/2285 train_time:65515ms step_avg:60.33ms
step:1087/2285 train_time:65577ms step_avg:60.33ms
step:1088/2285 train_time:65637ms step_avg:60.33ms
step:1089/2285 train_time:65699ms step_avg:60.33ms
step:1090/2285 train_time:65758ms step_avg:60.33ms
step:1091/2285 train_time:65821ms step_avg:60.33ms
step:1092/2285 train_time:65880ms step_avg:60.33ms
step:1093/2285 train_time:65942ms step_avg:60.33ms
step:1094/2285 train_time:66001ms step_avg:60.33ms
step:1095/2285 train_time:66063ms step_avg:60.33ms
step:1096/2285 train_time:66123ms step_avg:60.33ms
step:1097/2285 train_time:66184ms step_avg:60.33ms
step:1098/2285 train_time:66244ms step_avg:60.33ms
step:1099/2285 train_time:66305ms step_avg:60.33ms
step:1100/2285 train_time:66365ms step_avg:60.33ms
step:1101/2285 train_time:66427ms step_avg:60.33ms
step:1102/2285 train_time:66486ms step_avg:60.33ms
step:1103/2285 train_time:66548ms step_avg:60.33ms
step:1104/2285 train_time:66608ms step_avg:60.33ms
step:1105/2285 train_time:66672ms step_avg:60.34ms
step:1106/2285 train_time:66730ms step_avg:60.33ms
step:1107/2285 train_time:66792ms step_avg:60.34ms
step:1108/2285 train_time:66852ms step_avg:60.34ms
step:1109/2285 train_time:66914ms step_avg:60.34ms
step:1110/2285 train_time:66974ms step_avg:60.34ms
step:1111/2285 train_time:67035ms step_avg:60.34ms
step:1112/2285 train_time:67095ms step_avg:60.34ms
step:1113/2285 train_time:67157ms step_avg:60.34ms
step:1114/2285 train_time:67216ms step_avg:60.34ms
step:1115/2285 train_time:67277ms step_avg:60.34ms
step:1116/2285 train_time:67336ms step_avg:60.34ms
step:1117/2285 train_time:67398ms step_avg:60.34ms
step:1118/2285 train_time:67458ms step_avg:60.34ms
step:1119/2285 train_time:67520ms step_avg:60.34ms
step:1120/2285 train_time:67579ms step_avg:60.34ms
step:1121/2285 train_time:67642ms step_avg:60.34ms
step:1122/2285 train_time:67701ms step_avg:60.34ms
step:1123/2285 train_time:67763ms step_avg:60.34ms
step:1124/2285 train_time:67822ms step_avg:60.34ms
step:1125/2285 train_time:67884ms step_avg:60.34ms
step:1126/2285 train_time:67943ms step_avg:60.34ms
step:1127/2285 train_time:68005ms step_avg:60.34ms
step:1128/2285 train_time:68066ms step_avg:60.34ms
step:1129/2285 train_time:68128ms step_avg:60.34ms
step:1130/2285 train_time:68187ms step_avg:60.34ms
step:1131/2285 train_time:68249ms step_avg:60.34ms
step:1132/2285 train_time:68309ms step_avg:60.34ms
step:1133/2285 train_time:68371ms step_avg:60.34ms
step:1134/2285 train_time:68430ms step_avg:60.34ms
step:1135/2285 train_time:68492ms step_avg:60.35ms
step:1136/2285 train_time:68553ms step_avg:60.35ms
step:1137/2285 train_time:68614ms step_avg:60.35ms
step:1138/2285 train_time:68674ms step_avg:60.35ms
step:1139/2285 train_time:68736ms step_avg:60.35ms
step:1140/2285 train_time:68795ms step_avg:60.35ms
step:1141/2285 train_time:68857ms step_avg:60.35ms
step:1142/2285 train_time:68917ms step_avg:60.35ms
step:1143/2285 train_time:68980ms step_avg:60.35ms
step:1144/2285 train_time:69039ms step_avg:60.35ms
step:1145/2285 train_time:69101ms step_avg:60.35ms
step:1146/2285 train_time:69160ms step_avg:60.35ms
step:1147/2285 train_time:69222ms step_avg:60.35ms
step:1148/2285 train_time:69282ms step_avg:60.35ms
step:1149/2285 train_time:69345ms step_avg:60.35ms
step:1150/2285 train_time:69404ms step_avg:60.35ms
step:1151/2285 train_time:69467ms step_avg:60.35ms
step:1152/2285 train_time:69526ms step_avg:60.35ms
step:1153/2285 train_time:69588ms step_avg:60.35ms
step:1154/2285 train_time:69649ms step_avg:60.35ms
step:1155/2285 train_time:69711ms step_avg:60.36ms
step:1156/2285 train_time:69771ms step_avg:60.36ms
step:1157/2285 train_time:69834ms step_avg:60.36ms
step:1158/2285 train_time:69894ms step_avg:60.36ms
step:1159/2285 train_time:69956ms step_avg:60.36ms
step:1160/2285 train_time:70016ms step_avg:60.36ms
step:1161/2285 train_time:70078ms step_avg:60.36ms
step:1162/2285 train_time:70138ms step_avg:60.36ms
step:1163/2285 train_time:70200ms step_avg:60.36ms
step:1164/2285 train_time:70260ms step_avg:60.36ms
step:1165/2285 train_time:70322ms step_avg:60.36ms
step:1166/2285 train_time:70382ms step_avg:60.36ms
step:1167/2285 train_time:70444ms step_avg:60.36ms
step:1168/2285 train_time:70504ms step_avg:60.36ms
step:1169/2285 train_time:70567ms step_avg:60.37ms
step:1170/2285 train_time:70627ms step_avg:60.37ms
step:1171/2285 train_time:70689ms step_avg:60.37ms
step:1172/2285 train_time:70749ms step_avg:60.37ms
step:1173/2285 train_time:70811ms step_avg:60.37ms
step:1174/2285 train_time:70871ms step_avg:60.37ms
step:1175/2285 train_time:70934ms step_avg:60.37ms
step:1176/2285 train_time:70994ms step_avg:60.37ms
step:1177/2285 train_time:71056ms step_avg:60.37ms
step:1178/2285 train_time:71116ms step_avg:60.37ms
step:1179/2285 train_time:71178ms step_avg:60.37ms
step:1180/2285 train_time:71238ms step_avg:60.37ms
step:1181/2285 train_time:71300ms step_avg:60.37ms
step:1182/2285 train_time:71360ms step_avg:60.37ms
step:1183/2285 train_time:71422ms step_avg:60.37ms
step:1184/2285 train_time:71481ms step_avg:60.37ms
step:1185/2285 train_time:71543ms step_avg:60.37ms
step:1186/2285 train_time:71603ms step_avg:60.37ms
step:1187/2285 train_time:71666ms step_avg:60.38ms
step:1188/2285 train_time:71726ms step_avg:60.38ms
step:1189/2285 train_time:71789ms step_avg:60.38ms
step:1190/2285 train_time:71850ms step_avg:60.38ms
step:1191/2285 train_time:71913ms step_avg:60.38ms
step:1192/2285 train_time:71973ms step_avg:60.38ms
step:1193/2285 train_time:72035ms step_avg:60.38ms
step:1194/2285 train_time:72094ms step_avg:60.38ms
step:1195/2285 train_time:72156ms step_avg:60.38ms
step:1196/2285 train_time:72216ms step_avg:60.38ms
step:1197/2285 train_time:72278ms step_avg:60.38ms
step:1198/2285 train_time:72338ms step_avg:60.38ms
step:1199/2285 train_time:72400ms step_avg:60.38ms
step:1200/2285 train_time:72460ms step_avg:60.38ms
step:1201/2285 train_time:72522ms step_avg:60.38ms
step:1202/2285 train_time:72583ms step_avg:60.38ms
step:1203/2285 train_time:72644ms step_avg:60.39ms
step:1204/2285 train_time:72703ms step_avg:60.38ms
step:1205/2285 train_time:72766ms step_avg:60.39ms
step:1206/2285 train_time:72826ms step_avg:60.39ms
step:1207/2285 train_time:72888ms step_avg:60.39ms
step:1208/2285 train_time:72948ms step_avg:60.39ms
step:1209/2285 train_time:73010ms step_avg:60.39ms
step:1210/2285 train_time:73070ms step_avg:60.39ms
step:1211/2285 train_time:73133ms step_avg:60.39ms
step:1212/2285 train_time:73193ms step_avg:60.39ms
step:1213/2285 train_time:73256ms step_avg:60.39ms
step:1214/2285 train_time:73316ms step_avg:60.39ms
step:1215/2285 train_time:73378ms step_avg:60.39ms
step:1216/2285 train_time:73438ms step_avg:60.39ms
step:1217/2285 train_time:73500ms step_avg:60.39ms
step:1218/2285 train_time:73560ms step_avg:60.39ms
step:1219/2285 train_time:73622ms step_avg:60.40ms
step:1220/2285 train_time:73682ms step_avg:60.39ms
step:1221/2285 train_time:73745ms step_avg:60.40ms
step:1222/2285 train_time:73805ms step_avg:60.40ms
step:1223/2285 train_time:73867ms step_avg:60.40ms
step:1224/2285 train_time:73927ms step_avg:60.40ms
step:1225/2285 train_time:73989ms step_avg:60.40ms
step:1226/2285 train_time:74050ms step_avg:60.40ms
step:1227/2285 train_time:74113ms step_avg:60.40ms
step:1228/2285 train_time:74173ms step_avg:60.40ms
step:1229/2285 train_time:74235ms step_avg:60.40ms
step:1230/2285 train_time:74295ms step_avg:60.40ms
step:1231/2285 train_time:74357ms step_avg:60.40ms
step:1232/2285 train_time:74418ms step_avg:60.40ms
step:1233/2285 train_time:74481ms step_avg:60.41ms
step:1234/2285 train_time:74540ms step_avg:60.40ms
step:1235/2285 train_time:74601ms step_avg:60.41ms
step:1236/2285 train_time:74661ms step_avg:60.41ms
step:1237/2285 train_time:74723ms step_avg:60.41ms
step:1238/2285 train_time:74782ms step_avg:60.41ms
step:1239/2285 train_time:74844ms step_avg:60.41ms
step:1240/2285 train_time:74904ms step_avg:60.41ms
step:1241/2285 train_time:74966ms step_avg:60.41ms
step:1242/2285 train_time:75026ms step_avg:60.41ms
step:1243/2285 train_time:75089ms step_avg:60.41ms
step:1244/2285 train_time:75149ms step_avg:60.41ms
step:1245/2285 train_time:75211ms step_avg:60.41ms
step:1246/2285 train_time:75272ms step_avg:60.41ms
step:1247/2285 train_time:75335ms step_avg:60.41ms
step:1248/2285 train_time:75395ms step_avg:60.41ms
step:1249/2285 train_time:75458ms step_avg:60.41ms
step:1250/2285 train_time:75517ms step_avg:60.41ms
step:1250/2285 val_loss:3.4939 train_time:75581ms step_avg:60.46ms
step:1251/2285 train_time:75600ms step_avg:60.43ms
step:1252/2285 train_time:75641ms step_avg:60.42ms
step:1253/2285 train_time:75703ms step_avg:60.42ms
step:1254/2285 train_time:75762ms step_avg:60.42ms
step:1255/2285 train_time:75823ms step_avg:60.42ms
step:1256/2285 train_time:75882ms step_avg:60.42ms
step:1257/2285 train_time:75943ms step_avg:60.42ms
step:1258/2285 train_time:76002ms step_avg:60.41ms
step:1259/2285 train_time:76063ms step_avg:60.42ms
step:1260/2285 train_time:76122ms step_avg:60.41ms
step:1261/2285 train_time:76183ms step_avg:60.41ms
step:1262/2285 train_time:76241ms step_avg:60.41ms
step:1263/2285 train_time:76302ms step_avg:60.41ms
step:1264/2285 train_time:76363ms step_avg:60.41ms
step:1265/2285 train_time:76422ms step_avg:60.41ms
step:1266/2285 train_time:76485ms step_avg:60.41ms
step:1267/2285 train_time:76553ms step_avg:60.42ms
step:1268/2285 train_time:76614ms step_avg:60.42ms
step:1269/2285 train_time:76676ms step_avg:60.42ms
step:1270/2285 train_time:76735ms step_avg:60.42ms
step:1271/2285 train_time:76798ms step_avg:60.42ms
step:1272/2285 train_time:76857ms step_avg:60.42ms
step:1273/2285 train_time:76918ms step_avg:60.42ms
step:1274/2285 train_time:76978ms step_avg:60.42ms
step:1275/2285 train_time:77040ms step_avg:60.42ms
step:1276/2285 train_time:77098ms step_avg:60.42ms
step:1277/2285 train_time:77160ms step_avg:60.42ms
step:1278/2285 train_time:77219ms step_avg:60.42ms
step:1279/2285 train_time:77280ms step_avg:60.42ms
step:1280/2285 train_time:77339ms step_avg:60.42ms
step:1281/2285 train_time:77401ms step_avg:60.42ms
step:1282/2285 train_time:77463ms step_avg:60.42ms
step:1283/2285 train_time:77527ms step_avg:60.43ms
step:1284/2285 train_time:77587ms step_avg:60.43ms
step:1285/2285 train_time:77649ms step_avg:60.43ms
step:1286/2285 train_time:77709ms step_avg:60.43ms
step:1287/2285 train_time:77771ms step_avg:60.43ms
step:1288/2285 train_time:77830ms step_avg:60.43ms
step:1289/2285 train_time:77893ms step_avg:60.43ms
step:1290/2285 train_time:77953ms step_avg:60.43ms
step:1291/2285 train_time:78014ms step_avg:60.43ms
step:1292/2285 train_time:78073ms step_avg:60.43ms
step:1293/2285 train_time:78135ms step_avg:60.43ms
step:1294/2285 train_time:78194ms step_avg:60.43ms
step:1295/2285 train_time:78256ms step_avg:60.43ms
step:1296/2285 train_time:78316ms step_avg:60.43ms
step:1297/2285 train_time:78380ms step_avg:60.43ms
step:1298/2285 train_time:78440ms step_avg:60.43ms
step:1299/2285 train_time:78503ms step_avg:60.43ms
step:1300/2285 train_time:78564ms step_avg:60.43ms
step:1301/2285 train_time:78626ms step_avg:60.43ms
step:1302/2285 train_time:78685ms step_avg:60.43ms
step:1303/2285 train_time:78747ms step_avg:60.44ms
step:1304/2285 train_time:78806ms step_avg:60.43ms
step:1305/2285 train_time:78868ms step_avg:60.44ms
step:1306/2285 train_time:78927ms step_avg:60.43ms
step:1307/2285 train_time:78988ms step_avg:60.43ms
step:1308/2285 train_time:79048ms step_avg:60.43ms
step:1309/2285 train_time:79110ms step_avg:60.44ms
step:1310/2285 train_time:79170ms step_avg:60.44ms
step:1311/2285 train_time:79232ms step_avg:60.44ms
step:1312/2285 train_time:79292ms step_avg:60.44ms
step:1313/2285 train_time:79354ms step_avg:60.44ms
step:1314/2285 train_time:79415ms step_avg:60.44ms
step:1315/2285 train_time:79477ms step_avg:60.44ms
step:1316/2285 train_time:79538ms step_avg:60.44ms
step:1317/2285 train_time:79601ms step_avg:60.44ms
step:1318/2285 train_time:79661ms step_avg:60.44ms
step:1319/2285 train_time:79723ms step_avg:60.44ms
step:1320/2285 train_time:79782ms step_avg:60.44ms
step:1321/2285 train_time:79844ms step_avg:60.44ms
step:1322/2285 train_time:79903ms step_avg:60.44ms
step:1323/2285 train_time:79965ms step_avg:60.44ms
step:1324/2285 train_time:80025ms step_avg:60.44ms
step:1325/2285 train_time:80087ms step_avg:60.44ms
step:1326/2285 train_time:80147ms step_avg:60.44ms
step:1327/2285 train_time:80208ms step_avg:60.44ms
step:1328/2285 train_time:80272ms step_avg:60.45ms
step:1329/2285 train_time:80331ms step_avg:60.44ms
step:1330/2285 train_time:80391ms step_avg:60.44ms
step:1331/2285 train_time:80453ms step_avg:60.45ms
step:1332/2285 train_time:80513ms step_avg:60.44ms
step:1333/2285 train_time:80575ms step_avg:60.45ms
step:1334/2285 train_time:80636ms step_avg:60.45ms
step:1335/2285 train_time:80699ms step_avg:60.45ms
step:1336/2285 train_time:80758ms step_avg:60.45ms
step:1337/2285 train_time:80820ms step_avg:60.45ms
step:1338/2285 train_time:80880ms step_avg:60.45ms
step:1339/2285 train_time:80943ms step_avg:60.45ms
step:1340/2285 train_time:81002ms step_avg:60.45ms
step:1341/2285 train_time:81064ms step_avg:60.45ms
step:1342/2285 train_time:81124ms step_avg:60.45ms
step:1343/2285 train_time:81186ms step_avg:60.45ms
step:1344/2285 train_time:81245ms step_avg:60.45ms
step:1345/2285 train_time:81308ms step_avg:60.45ms
step:1346/2285 train_time:81369ms step_avg:60.45ms
step:1347/2285 train_time:81430ms step_avg:60.45ms
step:1348/2285 train_time:81490ms step_avg:60.45ms
step:1349/2285 train_time:81553ms step_avg:60.45ms
step:1350/2285 train_time:81613ms step_avg:60.45ms
step:1351/2285 train_time:81676ms step_avg:60.46ms
step:1352/2285 train_time:81737ms step_avg:60.46ms
step:1353/2285 train_time:81799ms step_avg:60.46ms
step:1354/2285 train_time:81859ms step_avg:60.46ms
step:1355/2285 train_time:81920ms step_avg:60.46ms
step:1356/2285 train_time:81980ms step_avg:60.46ms
step:1357/2285 train_time:82042ms step_avg:60.46ms
step:1358/2285 train_time:82101ms step_avg:60.46ms
step:1359/2285 train_time:82163ms step_avg:60.46ms
step:1360/2285 train_time:82223ms step_avg:60.46ms
step:1361/2285 train_time:82285ms step_avg:60.46ms
step:1362/2285 train_time:82345ms step_avg:60.46ms
step:1363/2285 train_time:82407ms step_avg:60.46ms
step:1364/2285 train_time:82467ms step_avg:60.46ms
step:1365/2285 train_time:82529ms step_avg:60.46ms
step:1366/2285 train_time:82589ms step_avg:60.46ms
step:1367/2285 train_time:82652ms step_avg:60.46ms
step:1368/2285 train_time:82712ms step_avg:60.46ms
step:1369/2285 train_time:82774ms step_avg:60.46ms
step:1370/2285 train_time:82835ms step_avg:60.46ms
step:1371/2285 train_time:82897ms step_avg:60.46ms
step:1372/2285 train_time:82957ms step_avg:60.46ms
step:1373/2285 train_time:83020ms step_avg:60.47ms
step:1374/2285 train_time:83079ms step_avg:60.47ms
step:1375/2285 train_time:83141ms step_avg:60.47ms
step:1376/2285 train_time:83201ms step_avg:60.47ms
step:1377/2285 train_time:83264ms step_avg:60.47ms
step:1378/2285 train_time:83323ms step_avg:60.47ms
step:1379/2285 train_time:83385ms step_avg:60.47ms
step:1380/2285 train_time:83445ms step_avg:60.47ms
step:1381/2285 train_time:83507ms step_avg:60.47ms
step:1382/2285 train_time:83567ms step_avg:60.47ms
step:1383/2285 train_time:83629ms step_avg:60.47ms
step:1384/2285 train_time:83690ms step_avg:60.47ms
step:1385/2285 train_time:83753ms step_avg:60.47ms
step:1386/2285 train_time:83813ms step_avg:60.47ms
step:1387/2285 train_time:83875ms step_avg:60.47ms
step:1388/2285 train_time:83936ms step_avg:60.47ms
step:1389/2285 train_time:83998ms step_avg:60.47ms
step:1390/2285 train_time:84058ms step_avg:60.47ms
step:1391/2285 train_time:84120ms step_avg:60.47ms
step:1392/2285 train_time:84180ms step_avg:60.47ms
step:1393/2285 train_time:84242ms step_avg:60.47ms
step:1394/2285 train_time:84302ms step_avg:60.47ms
step:1395/2285 train_time:84364ms step_avg:60.48ms
step:1396/2285 train_time:84424ms step_avg:60.48ms
step:1397/2285 train_time:84486ms step_avg:60.48ms
step:1398/2285 train_time:84546ms step_avg:60.48ms
step:1399/2285 train_time:84608ms step_avg:60.48ms
step:1400/2285 train_time:84668ms step_avg:60.48ms
step:1401/2285 train_time:84730ms step_avg:60.48ms
step:1402/2285 train_time:84790ms step_avg:60.48ms
step:1403/2285 train_time:84853ms step_avg:60.48ms
step:1404/2285 train_time:84913ms step_avg:60.48ms
step:1405/2285 train_time:84974ms step_avg:60.48ms
step:1406/2285 train_time:85034ms step_avg:60.48ms
step:1407/2285 train_time:85097ms step_avg:60.48ms
step:1408/2285 train_time:85156ms step_avg:60.48ms
step:1409/2285 train_time:85219ms step_avg:60.48ms
step:1410/2285 train_time:85278ms step_avg:60.48ms
step:1411/2285 train_time:85341ms step_avg:60.48ms
step:1412/2285 train_time:85401ms step_avg:60.48ms
step:1413/2285 train_time:85463ms step_avg:60.48ms
step:1414/2285 train_time:85523ms step_avg:60.48ms
step:1415/2285 train_time:85585ms step_avg:60.48ms
step:1416/2285 train_time:85645ms step_avg:60.48ms
step:1417/2285 train_time:85707ms step_avg:60.49ms
step:1418/2285 train_time:85767ms step_avg:60.48ms
step:1419/2285 train_time:85829ms step_avg:60.49ms
step:1420/2285 train_time:85889ms step_avg:60.49ms
step:1421/2285 train_time:85952ms step_avg:60.49ms
step:1422/2285 train_time:86012ms step_avg:60.49ms
step:1423/2285 train_time:86074ms step_avg:60.49ms
step:1424/2285 train_time:86134ms step_avg:60.49ms
step:1425/2285 train_time:86196ms step_avg:60.49ms
step:1426/2285 train_time:86256ms step_avg:60.49ms
step:1427/2285 train_time:86318ms step_avg:60.49ms
step:1428/2285 train_time:86378ms step_avg:60.49ms
step:1429/2285 train_time:86440ms step_avg:60.49ms
step:1430/2285 train_time:86500ms step_avg:60.49ms
step:1431/2285 train_time:86563ms step_avg:60.49ms
step:1432/2285 train_time:86622ms step_avg:60.49ms
step:1433/2285 train_time:86685ms step_avg:60.49ms
step:1434/2285 train_time:86744ms step_avg:60.49ms
step:1435/2285 train_time:86806ms step_avg:60.49ms
step:1436/2285 train_time:86866ms step_avg:60.49ms
step:1437/2285 train_time:86928ms step_avg:60.49ms
step:1438/2285 train_time:86988ms step_avg:60.49ms
step:1439/2285 train_time:87050ms step_avg:60.49ms
step:1440/2285 train_time:87111ms step_avg:60.49ms
step:1441/2285 train_time:87173ms step_avg:60.49ms
step:1442/2285 train_time:87233ms step_avg:60.49ms
step:1443/2285 train_time:87295ms step_avg:60.50ms
step:1444/2285 train_time:87355ms step_avg:60.50ms
step:1445/2285 train_time:87417ms step_avg:60.50ms
step:1446/2285 train_time:87477ms step_avg:60.50ms
step:1447/2285 train_time:87540ms step_avg:60.50ms
step:1448/2285 train_time:87600ms step_avg:60.50ms
step:1449/2285 train_time:87663ms step_avg:60.50ms
step:1450/2285 train_time:87723ms step_avg:60.50ms
step:1451/2285 train_time:87785ms step_avg:60.50ms
step:1452/2285 train_time:87844ms step_avg:60.50ms
step:1453/2285 train_time:87906ms step_avg:60.50ms
step:1454/2285 train_time:87966ms step_avg:60.50ms
step:1455/2285 train_time:88027ms step_avg:60.50ms
step:1456/2285 train_time:88088ms step_avg:60.50ms
step:1457/2285 train_time:88150ms step_avg:60.50ms
step:1458/2285 train_time:88210ms step_avg:60.50ms
step:1459/2285 train_time:88273ms step_avg:60.50ms
step:1460/2285 train_time:88332ms step_avg:60.50ms
step:1461/2285 train_time:88395ms step_avg:60.50ms
step:1462/2285 train_time:88456ms step_avg:60.50ms
step:1463/2285 train_time:88519ms step_avg:60.51ms
step:1464/2285 train_time:88579ms step_avg:60.50ms
step:1465/2285 train_time:88641ms step_avg:60.51ms
step:1466/2285 train_time:88701ms step_avg:60.51ms
step:1467/2285 train_time:88763ms step_avg:60.51ms
step:1468/2285 train_time:88823ms step_avg:60.51ms
step:1469/2285 train_time:88886ms step_avg:60.51ms
step:1470/2285 train_time:88945ms step_avg:60.51ms
step:1471/2285 train_time:89007ms step_avg:60.51ms
step:1472/2285 train_time:89067ms step_avg:60.51ms
step:1473/2285 train_time:89129ms step_avg:60.51ms
step:1474/2285 train_time:89188ms step_avg:60.51ms
step:1475/2285 train_time:89250ms step_avg:60.51ms
step:1476/2285 train_time:89311ms step_avg:60.51ms
step:1477/2285 train_time:89374ms step_avg:60.51ms
step:1478/2285 train_time:89434ms step_avg:60.51ms
step:1479/2285 train_time:89496ms step_avg:60.51ms
step:1480/2285 train_time:89557ms step_avg:60.51ms
step:1481/2285 train_time:89620ms step_avg:60.51ms
step:1482/2285 train_time:89679ms step_avg:60.51ms
step:1483/2285 train_time:89742ms step_avg:60.51ms
step:1484/2285 train_time:89801ms step_avg:60.51ms
step:1485/2285 train_time:89863ms step_avg:60.51ms
step:1486/2285 train_time:89922ms step_avg:60.51ms
step:1487/2285 train_time:89985ms step_avg:60.51ms
step:1488/2285 train_time:90045ms step_avg:60.51ms
step:1489/2285 train_time:90107ms step_avg:60.51ms
step:1490/2285 train_time:90168ms step_avg:60.52ms
step:1491/2285 train_time:90228ms step_avg:60.52ms
step:1492/2285 train_time:90288ms step_avg:60.51ms
step:1493/2285 train_time:90350ms step_avg:60.52ms
step:1494/2285 train_time:90410ms step_avg:60.52ms
step:1495/2285 train_time:90473ms step_avg:60.52ms
step:1496/2285 train_time:90533ms step_avg:60.52ms
step:1497/2285 train_time:90595ms step_avg:60.52ms
step:1498/2285 train_time:90656ms step_avg:60.52ms
step:1499/2285 train_time:90718ms step_avg:60.52ms
step:1500/2285 train_time:90778ms step_avg:60.52ms
step:1500/2285 val_loss:3.4262 train_time:90841ms step_avg:60.56ms
step:1501/2285 train_time:90860ms step_avg:60.53ms
step:1502/2285 train_time:90902ms step_avg:60.52ms
step:1503/2285 train_time:90968ms step_avg:60.52ms
step:1504/2285 train_time:91027ms step_avg:60.52ms
step:1505/2285 train_time:91089ms step_avg:60.52ms
step:1506/2285 train_time:91148ms step_avg:60.52ms
step:1507/2285 train_time:91211ms step_avg:60.52ms
step:1508/2285 train_time:91268ms step_avg:60.52ms
step:1509/2285 train_time:91330ms step_avg:60.52ms
step:1510/2285 train_time:91389ms step_avg:60.52ms
step:1511/2285 train_time:91451ms step_avg:60.52ms
step:1512/2285 train_time:91510ms step_avg:60.52ms
step:1513/2285 train_time:91572ms step_avg:60.52ms
step:1514/2285 train_time:91632ms step_avg:60.52ms
step:1515/2285 train_time:91694ms step_avg:60.52ms
step:1516/2285 train_time:91755ms step_avg:60.52ms
step:1517/2285 train_time:91819ms step_avg:60.53ms
step:1518/2285 train_time:91879ms step_avg:60.53ms
step:1519/2285 train_time:91943ms step_avg:60.53ms
step:1520/2285 train_time:92004ms step_avg:60.53ms
step:1521/2285 train_time:92067ms step_avg:60.53ms
step:1522/2285 train_time:92126ms step_avg:60.53ms
step:1523/2285 train_time:92188ms step_avg:60.53ms
step:1524/2285 train_time:92247ms step_avg:60.53ms
step:1525/2285 train_time:92310ms step_avg:60.53ms
step:1526/2285 train_time:92369ms step_avg:60.53ms
step:1527/2285 train_time:92431ms step_avg:60.53ms
step:1528/2285 train_time:92491ms step_avg:60.53ms
step:1529/2285 train_time:92553ms step_avg:60.53ms
step:1530/2285 train_time:92612ms step_avg:60.53ms
step:1531/2285 train_time:92674ms step_avg:60.53ms
step:1532/2285 train_time:92735ms step_avg:60.53ms
step:1533/2285 train_time:92799ms step_avg:60.53ms
step:1534/2285 train_time:92859ms step_avg:60.53ms
step:1535/2285 train_time:92922ms step_avg:60.54ms
step:1536/2285 train_time:92983ms step_avg:60.54ms
step:1537/2285 train_time:93047ms step_avg:60.54ms
step:1538/2285 train_time:93107ms step_avg:60.54ms
step:1539/2285 train_time:93168ms step_avg:60.54ms
step:1540/2285 train_time:93228ms step_avg:60.54ms
step:1541/2285 train_time:93290ms step_avg:60.54ms
step:1542/2285 train_time:93350ms step_avg:60.54ms
step:1543/2285 train_time:93412ms step_avg:60.54ms
step:1544/2285 train_time:93471ms step_avg:60.54ms
step:1545/2285 train_time:93533ms step_avg:60.54ms
step:1546/2285 train_time:93593ms step_avg:60.54ms
step:1547/2285 train_time:93655ms step_avg:60.54ms
step:1548/2285 train_time:93715ms step_avg:60.54ms
step:1549/2285 train_time:93778ms step_avg:60.54ms
step:1550/2285 train_time:93838ms step_avg:60.54ms
step:1551/2285 train_time:93901ms step_avg:60.54ms
step:1552/2285 train_time:93962ms step_avg:60.54ms
step:1553/2285 train_time:94026ms step_avg:60.54ms
step:1554/2285 train_time:94086ms step_avg:60.54ms
step:1555/2285 train_time:94148ms step_avg:60.55ms
step:1556/2285 train_time:94207ms step_avg:60.54ms
step:1557/2285 train_time:94270ms step_avg:60.55ms
step:1558/2285 train_time:94330ms step_avg:60.55ms
step:1559/2285 train_time:94391ms step_avg:60.55ms
step:1560/2285 train_time:94451ms step_avg:60.55ms
step:1561/2285 train_time:94512ms step_avg:60.55ms
step:1562/2285 train_time:94572ms step_avg:60.55ms
step:1563/2285 train_time:94634ms step_avg:60.55ms
step:1564/2285 train_time:94694ms step_avg:60.55ms
step:1565/2285 train_time:94756ms step_avg:60.55ms
step:1566/2285 train_time:94816ms step_avg:60.55ms
step:1567/2285 train_time:94880ms step_avg:60.55ms
step:1568/2285 train_time:94940ms step_avg:60.55ms
step:1569/2285 train_time:95003ms step_avg:60.55ms
step:1570/2285 train_time:95064ms step_avg:60.55ms
step:1571/2285 train_time:95126ms step_avg:60.55ms
step:1572/2285 train_time:95186ms step_avg:60.55ms
step:1573/2285 train_time:95248ms step_avg:60.55ms
step:1574/2285 train_time:95308ms step_avg:60.55ms
step:1575/2285 train_time:95371ms step_avg:60.55ms
step:1576/2285 train_time:95430ms step_avg:60.55ms
step:1577/2285 train_time:95492ms step_avg:60.55ms
step:1578/2285 train_time:95552ms step_avg:60.55ms
step:1579/2285 train_time:95614ms step_avg:60.55ms
step:1580/2285 train_time:95674ms step_avg:60.55ms
step:1581/2285 train_time:95736ms step_avg:60.55ms
step:1582/2285 train_time:95796ms step_avg:60.55ms
step:1583/2285 train_time:95859ms step_avg:60.56ms
step:1584/2285 train_time:95919ms step_avg:60.56ms
step:1585/2285 train_time:95982ms step_avg:60.56ms
step:1586/2285 train_time:96043ms step_avg:60.56ms
step:1587/2285 train_time:96105ms step_avg:60.56ms
step:1588/2285 train_time:96164ms step_avg:60.56ms
step:1589/2285 train_time:96227ms step_avg:60.56ms
step:1590/2285 train_time:96286ms step_avg:60.56ms
step:1591/2285 train_time:96348ms step_avg:60.56ms
step:1592/2285 train_time:96408ms step_avg:60.56ms
step:1593/2285 train_time:96471ms step_avg:60.56ms
step:1594/2285 train_time:96530ms step_avg:60.56ms
step:1595/2285 train_time:96593ms step_avg:60.56ms
step:1596/2285 train_time:96652ms step_avg:60.56ms
step:1597/2285 train_time:96714ms step_avg:60.56ms
step:1598/2285 train_time:96774ms step_avg:60.56ms
step:1599/2285 train_time:96837ms step_avg:60.56ms
step:1600/2285 train_time:96898ms step_avg:60.56ms
step:1601/2285 train_time:96961ms step_avg:60.56ms
step:1602/2285 train_time:97020ms step_avg:60.56ms
step:1603/2285 train_time:97083ms step_avg:60.56ms
step:1604/2285 train_time:97144ms step_avg:60.56ms
step:1605/2285 train_time:97207ms step_avg:60.56ms
step:1606/2285 train_time:97266ms step_avg:60.56ms
step:1607/2285 train_time:97329ms step_avg:60.57ms
step:1608/2285 train_time:97388ms step_avg:60.56ms
step:1609/2285 train_time:97450ms step_avg:60.57ms
step:1610/2285 train_time:97510ms step_avg:60.57ms
step:1611/2285 train_time:97572ms step_avg:60.57ms
step:1612/2285 train_time:97632ms step_avg:60.57ms
step:1613/2285 train_time:97694ms step_avg:60.57ms
step:1614/2285 train_time:97754ms step_avg:60.57ms
step:1615/2285 train_time:97817ms step_avg:60.57ms
step:1616/2285 train_time:97877ms step_avg:60.57ms
step:1617/2285 train_time:97939ms step_avg:60.57ms
step:1618/2285 train_time:97999ms step_avg:60.57ms
step:1619/2285 train_time:98062ms step_avg:60.57ms
step:1620/2285 train_time:98122ms step_avg:60.57ms
step:1621/2285 train_time:98185ms step_avg:60.57ms
step:1622/2285 train_time:98245ms step_avg:60.57ms
step:1623/2285 train_time:98307ms step_avg:60.57ms
step:1624/2285 train_time:98367ms step_avg:60.57ms
step:1625/2285 train_time:98430ms step_avg:60.57ms
step:1626/2285 train_time:98490ms step_avg:60.57ms
step:1627/2285 train_time:98552ms step_avg:60.57ms
step:1628/2285 train_time:98611ms step_avg:60.57ms
step:1629/2285 train_time:98673ms step_avg:60.57ms
step:1630/2285 train_time:98733ms step_avg:60.57ms
step:1631/2285 train_time:98795ms step_avg:60.57ms
step:1632/2285 train_time:98855ms step_avg:60.57ms
step:1633/2285 train_time:98918ms step_avg:60.57ms
step:1634/2285 train_time:98977ms step_avg:60.57ms
step:1635/2285 train_time:99039ms step_avg:60.57ms
step:1636/2285 train_time:99100ms step_avg:60.57ms
step:1637/2285 train_time:99162ms step_avg:60.58ms
step:1638/2285 train_time:99224ms step_avg:60.58ms
step:1639/2285 train_time:99287ms step_avg:60.58ms
step:1640/2285 train_time:99346ms step_avg:60.58ms
step:1641/2285 train_time:99408ms step_avg:60.58ms
step:1642/2285 train_time:99468ms step_avg:60.58ms
step:1643/2285 train_time:99531ms step_avg:60.58ms
step:1644/2285 train_time:99591ms step_avg:60.58ms
step:1645/2285 train_time:99653ms step_avg:60.58ms
step:1646/2285 train_time:99712ms step_avg:60.58ms
step:1647/2285 train_time:99774ms step_avg:60.58ms
step:1648/2285 train_time:99834ms step_avg:60.58ms
step:1649/2285 train_time:99896ms step_avg:60.58ms
step:1650/2285 train_time:99957ms step_avg:60.58ms
step:1651/2285 train_time:100019ms step_avg:60.58ms
step:1652/2285 train_time:100079ms step_avg:60.58ms
step:1653/2285 train_time:100142ms step_avg:60.58ms
step:1654/2285 train_time:100202ms step_avg:60.58ms
step:1655/2285 train_time:100265ms step_avg:60.58ms
step:1656/2285 train_time:100325ms step_avg:60.58ms
step:1657/2285 train_time:100389ms step_avg:60.58ms
step:1658/2285 train_time:100449ms step_avg:60.58ms
step:1659/2285 train_time:100511ms step_avg:60.59ms
step:1660/2285 train_time:100571ms step_avg:60.59ms
step:1661/2285 train_time:100633ms step_avg:60.59ms
step:1662/2285 train_time:100693ms step_avg:60.59ms
step:1663/2285 train_time:100755ms step_avg:60.59ms
step:1664/2285 train_time:100816ms step_avg:60.59ms
step:1665/2285 train_time:100877ms step_avg:60.59ms
step:1666/2285 train_time:100937ms step_avg:60.59ms
step:1667/2285 train_time:101000ms step_avg:60.59ms
step:1668/2285 train_time:101059ms step_avg:60.59ms
step:1669/2285 train_time:101122ms step_avg:60.59ms
step:1670/2285 train_time:101182ms step_avg:60.59ms
step:1671/2285 train_time:101245ms step_avg:60.59ms
step:1672/2285 train_time:101304ms step_avg:60.59ms
step:1673/2285 train_time:101367ms step_avg:60.59ms
step:1674/2285 train_time:101427ms step_avg:60.59ms
step:1675/2285 train_time:101490ms step_avg:60.59ms
step:1676/2285 train_time:101550ms step_avg:60.59ms
step:1677/2285 train_time:101612ms step_avg:60.59ms
step:1678/2285 train_time:101672ms step_avg:60.59ms
step:1679/2285 train_time:101734ms step_avg:60.59ms
step:1680/2285 train_time:101793ms step_avg:60.59ms
step:1681/2285 train_time:101856ms step_avg:60.59ms
step:1682/2285 train_time:101916ms step_avg:60.59ms
step:1683/2285 train_time:101978ms step_avg:60.59ms
step:1684/2285 train_time:102038ms step_avg:60.59ms
step:1685/2285 train_time:102102ms step_avg:60.59ms
step:1686/2285 train_time:102162ms step_avg:60.59ms
step:1687/2285 train_time:102224ms step_avg:60.60ms
step:1688/2285 train_time:102285ms step_avg:60.60ms
step:1689/2285 train_time:102348ms step_avg:60.60ms
step:1690/2285 train_time:102407ms step_avg:60.60ms
step:1691/2285 train_time:102470ms step_avg:60.60ms
step:1692/2285 train_time:102530ms step_avg:60.60ms
step:1693/2285 train_time:102592ms step_avg:60.60ms
step:1694/2285 train_time:102651ms step_avg:60.60ms
step:1695/2285 train_time:102714ms step_avg:60.60ms
step:1696/2285 train_time:102773ms step_avg:60.60ms
step:1697/2285 train_time:102835ms step_avg:60.60ms
step:1698/2285 train_time:102895ms step_avg:60.60ms
step:1699/2285 train_time:102957ms step_avg:60.60ms
step:1700/2285 train_time:103019ms step_avg:60.60ms
step:1701/2285 train_time:103080ms step_avg:60.60ms
step:1702/2285 train_time:103140ms step_avg:60.60ms
step:1703/2285 train_time:103203ms step_avg:60.60ms
step:1704/2285 train_time:103263ms step_avg:60.60ms
step:1705/2285 train_time:103327ms step_avg:60.60ms
step:1706/2285 train_time:103387ms step_avg:60.60ms
step:1707/2285 train_time:103450ms step_avg:60.60ms
step:1708/2285 train_time:103510ms step_avg:60.60ms
step:1709/2285 train_time:103573ms step_avg:60.60ms
step:1710/2285 train_time:103633ms step_avg:60.60ms
step:1711/2285 train_time:103695ms step_avg:60.60ms
step:1712/2285 train_time:103754ms step_avg:60.60ms
step:1713/2285 train_time:103817ms step_avg:60.61ms
step:1714/2285 train_time:103877ms step_avg:60.60ms
step:1715/2285 train_time:103939ms step_avg:60.61ms
step:1716/2285 train_time:104000ms step_avg:60.61ms
step:1717/2285 train_time:104061ms step_avg:60.61ms
step:1718/2285 train_time:104121ms step_avg:60.61ms
step:1719/2285 train_time:104184ms step_avg:60.61ms
step:1720/2285 train_time:104244ms step_avg:60.61ms
step:1721/2285 train_time:104307ms step_avg:60.61ms
step:1722/2285 train_time:104367ms step_avg:60.61ms
step:1723/2285 train_time:104429ms step_avg:60.61ms
step:1724/2285 train_time:104490ms step_avg:60.61ms
step:1725/2285 train_time:104552ms step_avg:60.61ms
step:1726/2285 train_time:104612ms step_avg:60.61ms
step:1727/2285 train_time:104674ms step_avg:60.61ms
step:1728/2285 train_time:104733ms step_avg:60.61ms
step:1729/2285 train_time:104795ms step_avg:60.61ms
step:1730/2285 train_time:104854ms step_avg:60.61ms
step:1731/2285 train_time:104917ms step_avg:60.61ms
step:1732/2285 train_time:104977ms step_avg:60.61ms
step:1733/2285 train_time:105039ms step_avg:60.61ms
step:1734/2285 train_time:105100ms step_avg:60.61ms
step:1735/2285 train_time:105162ms step_avg:60.61ms
step:1736/2285 train_time:105222ms step_avg:60.61ms
step:1737/2285 train_time:105286ms step_avg:60.61ms
step:1738/2285 train_time:105346ms step_avg:60.61ms
step:1739/2285 train_time:105408ms step_avg:60.61ms
step:1740/2285 train_time:105468ms step_avg:60.61ms
step:1741/2285 train_time:105530ms step_avg:60.61ms
step:1742/2285 train_time:105591ms step_avg:60.61ms
step:1743/2285 train_time:105653ms step_avg:60.62ms
step:1744/2285 train_time:105712ms step_avg:60.61ms
step:1745/2285 train_time:105774ms step_avg:60.62ms
step:1746/2285 train_time:105834ms step_avg:60.61ms
step:1747/2285 train_time:105896ms step_avg:60.62ms
step:1748/2285 train_time:105956ms step_avg:60.62ms
step:1749/2285 train_time:106019ms step_avg:60.62ms
step:1750/2285 train_time:106079ms step_avg:60.62ms
step:1750/2285 val_loss:3.3662 train_time:106143ms step_avg:60.65ms
step:1751/2285 train_time:106162ms step_avg:60.63ms
step:1752/2285 train_time:106206ms step_avg:60.62ms
step:1753/2285 train_time:106269ms step_avg:60.62ms
step:1754/2285 train_time:106330ms step_avg:60.62ms
step:1755/2285 train_time:106393ms step_avg:60.62ms
step:1756/2285 train_time:106453ms step_avg:60.62ms
step:1757/2285 train_time:106514ms step_avg:60.62ms
step:1758/2285 train_time:106574ms step_avg:60.62ms
step:1759/2285 train_time:106635ms step_avg:60.62ms
step:1760/2285 train_time:106694ms step_avg:60.62ms
step:1761/2285 train_time:106756ms step_avg:60.62ms
step:1762/2285 train_time:106815ms step_avg:60.62ms
step:1763/2285 train_time:106876ms step_avg:60.62ms
step:1764/2285 train_time:106936ms step_avg:60.62ms
step:1765/2285 train_time:106998ms step_avg:60.62ms
step:1766/2285 train_time:107060ms step_avg:60.62ms
step:1767/2285 train_time:107125ms step_avg:60.63ms
step:1768/2285 train_time:107185ms step_avg:60.63ms
step:1769/2285 train_time:107247ms step_avg:60.63ms
step:1770/2285 train_time:107307ms step_avg:60.63ms
step:1771/2285 train_time:107370ms step_avg:60.63ms
step:1772/2285 train_time:107430ms step_avg:60.63ms
step:1773/2285 train_time:107492ms step_avg:60.63ms
step:1774/2285 train_time:107551ms step_avg:60.63ms
step:1775/2285 train_time:107614ms step_avg:60.63ms
step:1776/2285 train_time:107673ms step_avg:60.63ms
step:1777/2285 train_time:107735ms step_avg:60.63ms
step:1778/2285 train_time:107794ms step_avg:60.63ms
step:1779/2285 train_time:107856ms step_avg:60.63ms
step:1780/2285 train_time:107915ms step_avg:60.63ms
step:1781/2285 train_time:107977ms step_avg:60.63ms
step:1782/2285 train_time:108038ms step_avg:60.63ms
step:1783/2285 train_time:108102ms step_avg:60.63ms
step:1784/2285 train_time:108162ms step_avg:60.63ms
step:1785/2285 train_time:108225ms step_avg:60.63ms
step:1786/2285 train_time:108285ms step_avg:60.63ms
step:1787/2285 train_time:108347ms step_avg:60.63ms
step:1788/2285 train_time:108407ms step_avg:60.63ms
step:1789/2285 train_time:108469ms step_avg:60.63ms
step:1790/2285 train_time:108529ms step_avg:60.63ms
step:1791/2285 train_time:108591ms step_avg:60.63ms
step:1792/2285 train_time:108651ms step_avg:60.63ms
step:1793/2285 train_time:108713ms step_avg:60.63ms
step:1794/2285 train_time:108772ms step_avg:60.63ms
step:1795/2285 train_time:108834ms step_avg:60.63ms
step:1796/2285 train_time:108894ms step_avg:60.63ms
step:1797/2285 train_time:108956ms step_avg:60.63ms
step:1798/2285 train_time:109016ms step_avg:60.63ms
step:1799/2285 train_time:109079ms step_avg:60.63ms
step:1800/2285 train_time:109139ms step_avg:60.63ms
step:1801/2285 train_time:109203ms step_avg:60.63ms
step:1802/2285 train_time:109263ms step_avg:60.63ms
step:1803/2285 train_time:109325ms step_avg:60.63ms
step:1804/2285 train_time:109385ms step_avg:60.63ms
step:1805/2285 train_time:109446ms step_avg:60.64ms
step:1806/2285 train_time:109506ms step_avg:60.63ms
step:1807/2285 train_time:109568ms step_avg:60.64ms
step:1808/2285 train_time:109628ms step_avg:60.63ms
step:1809/2285 train_time:109690ms step_avg:60.64ms
step:1810/2285 train_time:109750ms step_avg:60.64ms
step:1811/2285 train_time:109813ms step_avg:60.64ms
step:1812/2285 train_time:109873ms step_avg:60.64ms
step:1813/2285 train_time:109936ms step_avg:60.64ms
step:1814/2285 train_time:109995ms step_avg:60.64ms
step:1815/2285 train_time:110058ms step_avg:60.64ms
step:1816/2285 train_time:110119ms step_avg:60.64ms
step:1817/2285 train_time:110182ms step_avg:60.64ms
step:1818/2285 train_time:110241ms step_avg:60.64ms
step:1819/2285 train_time:110304ms step_avg:60.64ms
step:1820/2285 train_time:110363ms step_avg:60.64ms
step:1821/2285 train_time:110425ms step_avg:60.64ms
step:1822/2285 train_time:110485ms step_avg:60.64ms
step:1823/2285 train_time:110547ms step_avg:60.64ms
step:1824/2285 train_time:110607ms step_avg:60.64ms
step:1825/2285 train_time:110669ms step_avg:60.64ms
step:1826/2285 train_time:110730ms step_avg:60.64ms
step:1827/2285 train_time:110792ms step_avg:60.64ms
step:1828/2285 train_time:110852ms step_avg:60.64ms
step:1829/2285 train_time:110915ms step_avg:60.64ms
step:1830/2285 train_time:110975ms step_avg:60.64ms
step:1831/2285 train_time:111037ms step_avg:60.64ms
step:1832/2285 train_time:111097ms step_avg:60.64ms
step:1833/2285 train_time:111160ms step_avg:60.64ms
step:1834/2285 train_time:111220ms step_avg:60.64ms
step:1835/2285 train_time:111282ms step_avg:60.64ms
step:1836/2285 train_time:111342ms step_avg:60.64ms
step:1837/2285 train_time:111404ms step_avg:60.64ms
step:1838/2285 train_time:111464ms step_avg:60.64ms
step:1839/2285 train_time:111526ms step_avg:60.64ms
step:1840/2285 train_time:111586ms step_avg:60.64ms
step:1841/2285 train_time:111648ms step_avg:60.65ms
step:1842/2285 train_time:111709ms step_avg:60.65ms
step:1843/2285 train_time:111771ms step_avg:60.65ms
step:1844/2285 train_time:111832ms step_avg:60.65ms
step:1845/2285 train_time:111894ms step_avg:60.65ms
step:1846/2285 train_time:111954ms step_avg:60.65ms
step:1847/2285 train_time:112017ms step_avg:60.65ms
step:1848/2285 train_time:112077ms step_avg:60.65ms
step:1849/2285 train_time:112140ms step_avg:60.65ms
step:1850/2285 train_time:112199ms step_avg:60.65ms
step:1851/2285 train_time:112262ms step_avg:60.65ms
step:1852/2285 train_time:112321ms step_avg:60.65ms
step:1853/2285 train_time:112384ms step_avg:60.65ms
step:1854/2285 train_time:112443ms step_avg:60.65ms
step:1855/2285 train_time:112505ms step_avg:60.65ms
step:1856/2285 train_time:112565ms step_avg:60.65ms
step:1857/2285 train_time:112627ms step_avg:60.65ms
step:1858/2285 train_time:112687ms step_avg:60.65ms
step:1859/2285 train_time:112750ms step_avg:60.65ms
step:1860/2285 train_time:112810ms step_avg:60.65ms
step:1861/2285 train_time:112872ms step_avg:60.65ms
step:1862/2285 train_time:112933ms step_avg:60.65ms
step:1863/2285 train_time:112995ms step_avg:60.65ms
step:1864/2285 train_time:113055ms step_avg:60.65ms
step:1865/2285 train_time:113117ms step_avg:60.65ms
step:1866/2285 train_time:113177ms step_avg:60.65ms
step:1867/2285 train_time:113240ms step_avg:60.65ms
step:1868/2285 train_time:113300ms step_avg:60.65ms
step:1869/2285 train_time:113362ms step_avg:60.65ms
step:1870/2285 train_time:113422ms step_avg:60.65ms
step:1871/2285 train_time:113484ms step_avg:60.65ms
step:1872/2285 train_time:113543ms step_avg:60.65ms
step:1873/2285 train_time:113605ms step_avg:60.65ms
step:1874/2285 train_time:113666ms step_avg:60.65ms
step:1875/2285 train_time:113728ms step_avg:60.66ms
step:1876/2285 train_time:113788ms step_avg:60.65ms
step:1877/2285 train_time:113850ms step_avg:60.66ms
step:1878/2285 train_time:113911ms step_avg:60.66ms
step:1879/2285 train_time:113975ms step_avg:60.66ms
step:1880/2285 train_time:114035ms step_avg:60.66ms
step:1881/2285 train_time:114097ms step_avg:60.66ms
step:1882/2285 train_time:114157ms step_avg:60.66ms
step:1883/2285 train_time:114220ms step_avg:60.66ms
step:1884/2285 train_time:114280ms step_avg:60.66ms
step:1885/2285 train_time:114342ms step_avg:60.66ms
step:1886/2285 train_time:114401ms step_avg:60.66ms
step:1887/2285 train_time:114463ms step_avg:60.66ms
step:1888/2285 train_time:114523ms step_avg:60.66ms
step:1889/2285 train_time:114586ms step_avg:60.66ms
step:1890/2285 train_time:114646ms step_avg:60.66ms
step:1891/2285 train_time:114708ms step_avg:60.66ms
step:1892/2285 train_time:114769ms step_avg:60.66ms
step:1893/2285 train_time:114831ms step_avg:60.66ms
step:1894/2285 train_time:114891ms step_avg:60.66ms
step:1895/2285 train_time:114954ms step_avg:60.66ms
step:1896/2285 train_time:115015ms step_avg:60.66ms
step:1897/2285 train_time:115077ms step_avg:60.66ms
step:1898/2285 train_time:115137ms step_avg:60.66ms
step:1899/2285 train_time:115200ms step_avg:60.66ms
step:1900/2285 train_time:115260ms step_avg:60.66ms
step:1901/2285 train_time:115322ms step_avg:60.66ms
step:1902/2285 train_time:115382ms step_avg:60.66ms
step:1903/2285 train_time:115443ms step_avg:60.66ms
step:1904/2285 train_time:115503ms step_avg:60.66ms
step:1905/2285 train_time:115565ms step_avg:60.66ms
step:1906/2285 train_time:115625ms step_avg:60.66ms
step:1907/2285 train_time:115687ms step_avg:60.66ms
step:1908/2285 train_time:115748ms step_avg:60.66ms
step:1909/2285 train_time:115810ms step_avg:60.67ms
step:1910/2285 train_time:115871ms step_avg:60.67ms
step:1911/2285 train_time:115933ms step_avg:60.67ms
step:1912/2285 train_time:115993ms step_avg:60.67ms
step:1913/2285 train_time:116056ms step_avg:60.67ms
step:1914/2285 train_time:116116ms step_avg:60.67ms
step:1915/2285 train_time:116178ms step_avg:60.67ms
step:1916/2285 train_time:116238ms step_avg:60.67ms
step:1917/2285 train_time:116301ms step_avg:60.67ms
step:1918/2285 train_time:116361ms step_avg:60.67ms
step:1919/2285 train_time:116423ms step_avg:60.67ms
step:1920/2285 train_time:116483ms step_avg:60.67ms
step:1921/2285 train_time:116545ms step_avg:60.67ms
step:1922/2285 train_time:116605ms step_avg:60.67ms
step:1923/2285 train_time:116667ms step_avg:60.67ms
step:1924/2285 train_time:116728ms step_avg:60.67ms
step:1925/2285 train_time:116790ms step_avg:60.67ms
step:1926/2285 train_time:116850ms step_avg:60.67ms
step:1927/2285 train_time:116913ms step_avg:60.67ms
step:1928/2285 train_time:116974ms step_avg:60.67ms
step:1929/2285 train_time:117037ms step_avg:60.67ms
step:1930/2285 train_time:117097ms step_avg:60.67ms
step:1931/2285 train_time:117159ms step_avg:60.67ms
step:1932/2285 train_time:117220ms step_avg:60.67ms
step:1933/2285 train_time:117282ms step_avg:60.67ms
step:1934/2285 train_time:117342ms step_avg:60.67ms
step:1935/2285 train_time:117404ms step_avg:60.67ms
step:1936/2285 train_time:117464ms step_avg:60.67ms
step:1937/2285 train_time:117526ms step_avg:60.67ms
step:1938/2285 train_time:117585ms step_avg:60.67ms
step:1939/2285 train_time:117648ms step_avg:60.67ms
step:1940/2285 train_time:117708ms step_avg:60.67ms
step:1941/2285 train_time:117771ms step_avg:60.68ms
step:1942/2285 train_time:117831ms step_avg:60.67ms
step:1943/2285 train_time:117893ms step_avg:60.68ms
step:1944/2285 train_time:117953ms step_avg:60.68ms
step:1945/2285 train_time:118015ms step_avg:60.68ms
step:1946/2285 train_time:118075ms step_avg:60.68ms
step:1947/2285 train_time:118138ms step_avg:60.68ms
step:1948/2285 train_time:118198ms step_avg:60.68ms
step:1949/2285 train_time:118262ms step_avg:60.68ms
step:1950/2285 train_time:118322ms step_avg:60.68ms
step:1951/2285 train_time:118383ms step_avg:60.68ms
step:1952/2285 train_time:118443ms step_avg:60.68ms
step:1953/2285 train_time:118505ms step_avg:60.68ms
step:1954/2285 train_time:118565ms step_avg:60.68ms
step:1955/2285 train_time:118627ms step_avg:60.68ms
step:1956/2285 train_time:118687ms step_avg:60.68ms
step:1957/2285 train_time:118750ms step_avg:60.68ms
step:1958/2285 train_time:118810ms step_avg:60.68ms
step:1959/2285 train_time:118873ms step_avg:60.68ms
step:1960/2285 train_time:118933ms step_avg:60.68ms
step:1961/2285 train_time:118996ms step_avg:60.68ms
step:1962/2285 train_time:119056ms step_avg:60.68ms
step:1963/2285 train_time:119119ms step_avg:60.68ms
step:1964/2285 train_time:119179ms step_avg:60.68ms
step:1965/2285 train_time:119242ms step_avg:60.68ms
step:1966/2285 train_time:119302ms step_avg:60.68ms
step:1967/2285 train_time:119364ms step_avg:60.68ms
step:1968/2285 train_time:119425ms step_avg:60.68ms
step:1969/2285 train_time:119486ms step_avg:60.68ms
step:1970/2285 train_time:119546ms step_avg:60.68ms
step:1971/2285 train_time:119609ms step_avg:60.68ms
step:1972/2285 train_time:119669ms step_avg:60.68ms
step:1973/2285 train_time:119731ms step_avg:60.68ms
step:1974/2285 train_time:119791ms step_avg:60.68ms
step:1975/2285 train_time:119853ms step_avg:60.69ms
step:1976/2285 train_time:119914ms step_avg:60.69ms
step:1977/2285 train_time:119976ms step_avg:60.69ms
step:1978/2285 train_time:120036ms step_avg:60.69ms
step:1979/2285 train_time:120098ms step_avg:60.69ms
step:1980/2285 train_time:120158ms step_avg:60.69ms
step:1981/2285 train_time:120220ms step_avg:60.69ms
step:1982/2285 train_time:120280ms step_avg:60.69ms
step:1983/2285 train_time:120343ms step_avg:60.69ms
step:1984/2285 train_time:120402ms step_avg:60.69ms
step:1985/2285 train_time:120464ms step_avg:60.69ms
step:1986/2285 train_time:120525ms step_avg:60.69ms
step:1987/2285 train_time:120587ms step_avg:60.69ms
step:1988/2285 train_time:120647ms step_avg:60.69ms
step:1989/2285 train_time:120709ms step_avg:60.69ms
step:1990/2285 train_time:120769ms step_avg:60.69ms
step:1991/2285 train_time:120832ms step_avg:60.69ms
step:1992/2285 train_time:120892ms step_avg:60.69ms
step:1993/2285 train_time:120955ms step_avg:60.69ms
step:1994/2285 train_time:121015ms step_avg:60.69ms
step:1995/2285 train_time:121078ms step_avg:60.69ms
step:1996/2285 train_time:121137ms step_avg:60.69ms
step:1997/2285 train_time:121200ms step_avg:60.69ms
step:1998/2285 train_time:121260ms step_avg:60.69ms
step:1999/2285 train_time:121323ms step_avg:60.69ms
step:2000/2285 train_time:121382ms step_avg:60.69ms
step:2000/2285 val_loss:3.3172 train_time:121446ms step_avg:60.72ms
step:2001/2285 train_time:121464ms step_avg:60.70ms
step:2002/2285 train_time:121507ms step_avg:60.69ms
step:2003/2285 train_time:121569ms step_avg:60.69ms
step:2004/2285 train_time:121630ms step_avg:60.69ms
step:2005/2285 train_time:121693ms step_avg:60.69ms
step:2006/2285 train_time:121754ms step_avg:60.69ms
step:2007/2285 train_time:121815ms step_avg:60.70ms
step:2008/2285 train_time:121874ms step_avg:60.69ms
step:2009/2285 train_time:121936ms step_avg:60.69ms
step:2010/2285 train_time:121995ms step_avg:60.69ms
step:2011/2285 train_time:122057ms step_avg:60.69ms
step:2012/2285 train_time:122116ms step_avg:60.69ms
step:2013/2285 train_time:122178ms step_avg:60.69ms
step:2014/2285 train_time:122238ms step_avg:60.69ms
step:2015/2285 train_time:122300ms step_avg:60.69ms
step:2016/2285 train_time:122362ms step_avg:60.70ms
step:2017/2285 train_time:122426ms step_avg:60.70ms
step:2018/2285 train_time:122488ms step_avg:60.70ms
step:2019/2285 train_time:122551ms step_avg:60.70ms
step:2020/2285 train_time:122611ms step_avg:60.70ms
step:2021/2285 train_time:122674ms step_avg:60.70ms
step:2022/2285 train_time:122734ms step_avg:60.70ms
step:2023/2285 train_time:122795ms step_avg:60.70ms
step:2024/2285 train_time:122855ms step_avg:60.70ms
step:2025/2285 train_time:122916ms step_avg:60.70ms
step:2026/2285 train_time:122979ms step_avg:60.70ms
step:2027/2285 train_time:123038ms step_avg:60.70ms
step:2028/2285 train_time:123098ms step_avg:60.70ms
step:2029/2285 train_time:123159ms step_avg:60.70ms
step:2030/2285 train_time:123219ms step_avg:60.70ms
step:2031/2285 train_time:123281ms step_avg:60.70ms
step:2032/2285 train_time:123342ms step_avg:60.70ms
step:2033/2285 train_time:123406ms step_avg:60.70ms
step:2034/2285 train_time:123467ms step_avg:60.70ms
step:2035/2285 train_time:123530ms step_avg:60.70ms
step:2036/2285 train_time:123590ms step_avg:60.70ms
step:2037/2285 train_time:123654ms step_avg:60.70ms
step:2038/2285 train_time:123713ms step_avg:60.70ms
step:2039/2285 train_time:123776ms step_avg:60.70ms
step:2040/2285 train_time:123836ms step_avg:60.70ms
step:2041/2285 train_time:123897ms step_avg:60.70ms
step:2042/2285 train_time:123957ms step_avg:60.70ms
step:2043/2285 train_time:124019ms step_avg:60.70ms
step:2044/2285 train_time:124079ms step_avg:60.70ms
step:2045/2285 train_time:124141ms step_avg:60.70ms
step:2046/2285 train_time:124201ms step_avg:60.70ms
step:2047/2285 train_time:124264ms step_avg:60.71ms
step:2048/2285 train_time:124325ms step_avg:60.71ms
step:2049/2285 train_time:124388ms step_avg:60.71ms
step:2050/2285 train_time:124448ms step_avg:60.71ms
step:2051/2285 train_time:124511ms step_avg:60.71ms
step:2052/2285 train_time:124571ms step_avg:60.71ms
step:2053/2285 train_time:124634ms step_avg:60.71ms
step:2054/2285 train_time:124694ms step_avg:60.71ms
step:2055/2285 train_time:124756ms step_avg:60.71ms
step:2056/2285 train_time:124816ms step_avg:60.71ms
step:2057/2285 train_time:124878ms step_avg:60.71ms
step:2058/2285 train_time:124937ms step_avg:60.71ms
step:2059/2285 train_time:124999ms step_avg:60.71ms
step:2060/2285 train_time:125059ms step_avg:60.71ms
step:2061/2285 train_time:125121ms step_avg:60.71ms
step:2062/2285 train_time:125182ms step_avg:60.71ms
step:2063/2285 train_time:125245ms step_avg:60.71ms
step:2064/2285 train_time:125305ms step_avg:60.71ms
step:2065/2285 train_time:125368ms step_avg:60.71ms
step:2066/2285 train_time:125428ms step_avg:60.71ms
step:2067/2285 train_time:125491ms step_avg:60.71ms
step:2068/2285 train_time:125551ms step_avg:60.71ms
step:2069/2285 train_time:125613ms step_avg:60.71ms
step:2070/2285 train_time:125673ms step_avg:60.71ms
step:2071/2285 train_time:125735ms step_avg:60.71ms
step:2072/2285 train_time:125796ms step_avg:60.71ms
step:2073/2285 train_time:125858ms step_avg:60.71ms
step:2074/2285 train_time:125918ms step_avg:60.71ms
step:2075/2285 train_time:125980ms step_avg:60.71ms
step:2076/2285 train_time:126039ms step_avg:60.71ms
step:2077/2285 train_time:126102ms step_avg:60.71ms
step:2078/2285 train_time:126162ms step_avg:60.71ms
step:2079/2285 train_time:126225ms step_avg:60.71ms
step:2080/2285 train_time:126286ms step_avg:60.71ms
step:2081/2285 train_time:126348ms step_avg:60.71ms
step:2082/2285 train_time:126408ms step_avg:60.71ms
step:2083/2285 train_time:126471ms step_avg:60.72ms
step:2084/2285 train_time:126531ms step_avg:60.72ms
step:2085/2285 train_time:126593ms step_avg:60.72ms
step:2086/2285 train_time:126653ms step_avg:60.72ms
step:2087/2285 train_time:126715ms step_avg:60.72ms
step:2088/2285 train_time:126775ms step_avg:60.72ms
step:2089/2285 train_time:126837ms step_avg:60.72ms
step:2090/2285 train_time:126897ms step_avg:60.72ms
step:2091/2285 train_time:126960ms step_avg:60.72ms
step:2092/2285 train_time:127019ms step_avg:60.72ms
step:2093/2285 train_time:127081ms step_avg:60.72ms
step:2094/2285 train_time:127142ms step_avg:60.72ms
step:2095/2285 train_time:127205ms step_avg:60.72ms
step:2096/2285 train_time:127265ms step_avg:60.72ms
step:2097/2285 train_time:127328ms step_avg:60.72ms
step:2098/2285 train_time:127387ms step_avg:60.72ms
step:2099/2285 train_time:127449ms step_avg:60.72ms
step:2100/2285 train_time:127509ms step_avg:60.72ms
step:2101/2285 train_time:127572ms step_avg:60.72ms
step:2102/2285 train_time:127632ms step_avg:60.72ms
step:2103/2285 train_time:127694ms step_avg:60.72ms
step:2104/2285 train_time:127754ms step_avg:60.72ms
step:2105/2285 train_time:127817ms step_avg:60.72ms
step:2106/2285 train_time:127876ms step_avg:60.72ms
step:2107/2285 train_time:127939ms step_avg:60.72ms
step:2108/2285 train_time:127999ms step_avg:60.72ms
step:2109/2285 train_time:128061ms step_avg:60.72ms
step:2110/2285 train_time:128120ms step_avg:60.72ms
step:2111/2285 train_time:128184ms step_avg:60.72ms
step:2112/2285 train_time:128244ms step_avg:60.72ms
step:2113/2285 train_time:128307ms step_avg:60.72ms
step:2114/2285 train_time:128367ms step_avg:60.72ms
step:2115/2285 train_time:128430ms step_avg:60.72ms
step:2116/2285 train_time:128490ms step_avg:60.72ms
step:2117/2285 train_time:128552ms step_avg:60.72ms
step:2118/2285 train_time:128612ms step_avg:60.72ms
step:2119/2285 train_time:128677ms step_avg:60.73ms
step:2120/2285 train_time:128735ms step_avg:60.72ms
step:2121/2285 train_time:128797ms step_avg:60.72ms
step:2122/2285 train_time:128857ms step_avg:60.72ms
step:2123/2285 train_time:128920ms step_avg:60.73ms
step:2124/2285 train_time:128979ms step_avg:60.72ms
step:2125/2285 train_time:129041ms step_avg:60.73ms
step:2126/2285 train_time:129102ms step_avg:60.73ms
step:2127/2285 train_time:129164ms step_avg:60.73ms
step:2128/2285 train_time:129224ms step_avg:60.73ms
step:2129/2285 train_time:129287ms step_avg:60.73ms
step:2130/2285 train_time:129348ms step_avg:60.73ms
step:2131/2285 train_time:129410ms step_avg:60.73ms
step:2132/2285 train_time:129471ms step_avg:60.73ms
step:2133/2285 train_time:129533ms step_avg:60.73ms
step:2134/2285 train_time:129593ms step_avg:60.73ms
step:2135/2285 train_time:129655ms step_avg:60.73ms
step:2136/2285 train_time:129715ms step_avg:60.73ms
step:2137/2285 train_time:129777ms step_avg:60.73ms
step:2138/2285 train_time:129837ms step_avg:60.73ms
step:2139/2285 train_time:129900ms step_avg:60.73ms
step:2140/2285 train_time:129960ms step_avg:60.73ms
step:2141/2285 train_time:130022ms step_avg:60.73ms
step:2142/2285 train_time:130082ms step_avg:60.73ms
step:2143/2285 train_time:130145ms step_avg:60.73ms
step:2144/2285 train_time:130205ms step_avg:60.73ms
step:2145/2285 train_time:130268ms step_avg:60.73ms
step:2146/2285 train_time:130328ms step_avg:60.73ms
step:2147/2285 train_time:130390ms step_avg:60.73ms
step:2148/2285 train_time:130451ms step_avg:60.73ms
step:2149/2285 train_time:130513ms step_avg:60.73ms
step:2150/2285 train_time:130573ms step_avg:60.73ms
step:2151/2285 train_time:130635ms step_avg:60.73ms
step:2152/2285 train_time:130695ms step_avg:60.73ms
step:2153/2285 train_time:130757ms step_avg:60.73ms
step:2154/2285 train_time:130818ms step_avg:60.73ms
step:2155/2285 train_time:130881ms step_avg:60.73ms
step:2156/2285 train_time:130940ms step_avg:60.73ms
step:2157/2285 train_time:131002ms step_avg:60.73ms
step:2158/2285 train_time:131063ms step_avg:60.73ms
step:2159/2285 train_time:131125ms step_avg:60.73ms
step:2160/2285 train_time:131185ms step_avg:60.73ms
step:2161/2285 train_time:131248ms step_avg:60.73ms
step:2162/2285 train_time:131308ms step_avg:60.73ms
step:2163/2285 train_time:131371ms step_avg:60.74ms
step:2164/2285 train_time:131431ms step_avg:60.74ms
step:2165/2285 train_time:131493ms step_avg:60.74ms
step:2166/2285 train_time:131553ms step_avg:60.74ms
step:2167/2285 train_time:131615ms step_avg:60.74ms
step:2168/2285 train_time:131676ms step_avg:60.74ms
step:2169/2285 train_time:131738ms step_avg:60.74ms
step:2170/2285 train_time:131798ms step_avg:60.74ms
step:2171/2285 train_time:131861ms step_avg:60.74ms
step:2172/2285 train_time:131921ms step_avg:60.74ms
step:2173/2285 train_time:131983ms step_avg:60.74ms
step:2174/2285 train_time:132043ms step_avg:60.74ms
step:2175/2285 train_time:132105ms step_avg:60.74ms
step:2176/2285 train_time:132165ms step_avg:60.74ms
step:2177/2285 train_time:132228ms step_avg:60.74ms
step:2178/2285 train_time:132288ms step_avg:60.74ms
step:2179/2285 train_time:132351ms step_avg:60.74ms
step:2180/2285 train_time:132412ms step_avg:60.74ms
step:2181/2285 train_time:132474ms step_avg:60.74ms
step:2182/2285 train_time:132535ms step_avg:60.74ms
step:2183/2285 train_time:132597ms step_avg:60.74ms
step:2184/2285 train_time:132657ms step_avg:60.74ms
step:2185/2285 train_time:132720ms step_avg:60.74ms
step:2186/2285 train_time:132780ms step_avg:60.74ms
step:2187/2285 train_time:132843ms step_avg:60.74ms
step:2188/2285 train_time:132903ms step_avg:60.74ms
step:2189/2285 train_time:132965ms step_avg:60.74ms
step:2190/2285 train_time:133025ms step_avg:60.74ms
step:2191/2285 train_time:133087ms step_avg:60.74ms
step:2192/2285 train_time:133147ms step_avg:60.74ms
step:2193/2285 train_time:133210ms step_avg:60.74ms
step:2194/2285 train_time:133270ms step_avg:60.74ms
step:2195/2285 train_time:133332ms step_avg:60.74ms
step:2196/2285 train_time:133392ms step_avg:60.74ms
step:2197/2285 train_time:133454ms step_avg:60.74ms
step:2198/2285 train_time:133516ms step_avg:60.74ms
step:2199/2285 train_time:133578ms step_avg:60.74ms
step:2200/2285 train_time:133638ms step_avg:60.74ms
step:2201/2285 train_time:133701ms step_avg:60.75ms
step:2202/2285 train_time:133761ms step_avg:60.75ms
step:2203/2285 train_time:133823ms step_avg:60.75ms
step:2204/2285 train_time:133882ms step_avg:60.75ms
step:2205/2285 train_time:133945ms step_avg:60.75ms
step:2206/2285 train_time:134005ms step_avg:60.75ms
step:2207/2285 train_time:134067ms step_avg:60.75ms
step:2208/2285 train_time:134127ms step_avg:60.75ms
step:2209/2285 train_time:134190ms step_avg:60.75ms
step:2210/2285 train_time:134251ms step_avg:60.75ms
step:2211/2285 train_time:134313ms step_avg:60.75ms
step:2212/2285 train_time:134373ms step_avg:60.75ms
step:2213/2285 train_time:134436ms step_avg:60.75ms
step:2214/2285 train_time:134496ms step_avg:60.75ms
step:2215/2285 train_time:134558ms step_avg:60.75ms
step:2216/2285 train_time:134618ms step_avg:60.75ms
step:2217/2285 train_time:134680ms step_avg:60.75ms
step:2218/2285 train_time:134740ms step_avg:60.75ms
step:2219/2285 train_time:134803ms step_avg:60.75ms
step:2220/2285 train_time:134863ms step_avg:60.75ms
step:2221/2285 train_time:134926ms step_avg:60.75ms
step:2222/2285 train_time:134986ms step_avg:60.75ms
step:2223/2285 train_time:135048ms step_avg:60.75ms
step:2224/2285 train_time:135109ms step_avg:60.75ms
step:2225/2285 train_time:135171ms step_avg:60.75ms
step:2226/2285 train_time:135231ms step_avg:60.75ms
step:2227/2285 train_time:135293ms step_avg:60.75ms
step:2228/2285 train_time:135354ms step_avg:60.75ms
step:2229/2285 train_time:135416ms step_avg:60.75ms
step:2230/2285 train_time:135477ms step_avg:60.75ms
step:2231/2285 train_time:135539ms step_avg:60.75ms
step:2232/2285 train_time:135600ms step_avg:60.75ms
step:2233/2285 train_time:135662ms step_avg:60.75ms
step:2234/2285 train_time:135721ms step_avg:60.75ms
step:2235/2285 train_time:135783ms step_avg:60.75ms
step:2236/2285 train_time:135844ms step_avg:60.75ms
step:2237/2285 train_time:135906ms step_avg:60.75ms
step:2238/2285 train_time:135966ms step_avg:60.75ms
step:2239/2285 train_time:136029ms step_avg:60.75ms
step:2240/2285 train_time:136089ms step_avg:60.75ms
step:2241/2285 train_time:136151ms step_avg:60.75ms
step:2242/2285 train_time:136211ms step_avg:60.75ms
step:2243/2285 train_time:136273ms step_avg:60.75ms
step:2244/2285 train_time:136333ms step_avg:60.75ms
step:2245/2285 train_time:136396ms step_avg:60.76ms
step:2246/2285 train_time:136456ms step_avg:60.76ms
step:2247/2285 train_time:136519ms step_avg:60.76ms
step:2248/2285 train_time:136579ms step_avg:60.76ms
step:2249/2285 train_time:136642ms step_avg:60.76ms
step:2250/2285 train_time:136702ms step_avg:60.76ms
step:2250/2285 val_loss:3.2821 train_time:136766ms step_avg:60.78ms
step:2251/2285 train_time:136784ms step_avg:60.77ms
step:2252/2285 train_time:136830ms step_avg:60.76ms
step:2253/2285 train_time:136894ms step_avg:60.76ms
step:2254/2285 train_time:136955ms step_avg:60.76ms
step:2255/2285 train_time:137016ms step_avg:60.76ms
step:2256/2285 train_time:137076ms step_avg:60.76ms
step:2257/2285 train_time:137138ms step_avg:60.76ms
step:2258/2285 train_time:137198ms step_avg:60.76ms
step:2259/2285 train_time:137260ms step_avg:60.76ms
step:2260/2285 train_time:137320ms step_avg:60.76ms
step:2261/2285 train_time:137383ms step_avg:60.76ms
step:2262/2285 train_time:137442ms step_avg:60.76ms
step:2263/2285 train_time:137504ms step_avg:60.76ms
step:2264/2285 train_time:137564ms step_avg:60.76ms
step:2265/2285 train_time:137626ms step_avg:60.76ms
step:2266/2285 train_time:137686ms step_avg:60.76ms
step:2267/2285 train_time:137749ms step_avg:60.76ms
step:2268/2285 train_time:137811ms step_avg:60.76ms
step:2269/2285 train_time:137874ms step_avg:60.76ms
step:2270/2285 train_time:137935ms step_avg:60.76ms
step:2271/2285 train_time:137998ms step_avg:60.77ms
step:2272/2285 train_time:138058ms step_avg:60.76ms
step:2273/2285 train_time:138120ms step_avg:60.77ms
step:2274/2285 train_time:138180ms step_avg:60.77ms
step:2275/2285 train_time:138242ms step_avg:60.77ms
step:2276/2285 train_time:138302ms step_avg:60.77ms
step:2277/2285 train_time:138363ms step_avg:60.77ms
step:2278/2285 train_time:138423ms step_avg:60.77ms
step:2279/2285 train_time:138485ms step_avg:60.77ms
step:2280/2285 train_time:138545ms step_avg:60.77ms
step:2281/2285 train_time:138607ms step_avg:60.77ms
step:2282/2285 train_time:138667ms step_avg:60.77ms
step:2283/2285 train_time:138730ms step_avg:60.77ms
step:2284/2285 train_time:138791ms step_avg:60.77ms
step:2285/2285 train_time:138854ms step_avg:60.77ms
step:2285/2285 val_loss:3.2760 train_time:138916ms step_avg:60.79ms
peak memory allocated: 29626 MiB reserved: 50528 MiB
